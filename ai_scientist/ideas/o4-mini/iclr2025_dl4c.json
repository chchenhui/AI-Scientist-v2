[
    {
        "Name": "uncertainty_execution_feedback",
        "Title": "Uncertainty-Aware Code Generation via Continuous Execution Feedback",
        "Short Hypothesis": "Incorporating real-time execution feedback guided by model uncertainty estimates into the code generation loop will improve the reliability, calibration, and downstream correctness of LLM-based code assistants more than post-hoc calibration or naive execution loops.",
        "Related Work": "Prior work has studied post-training calibration of language models for NLP tasks (Kumar et al., 2019) and basic execute-and-rank strategies in code generation (Chen et al., 2021). Ensembles and MC-dropout have been used for uncertainty estimation in vision and language (Gal & Ghahramani, 2016), but rarely in code generation contexts. Execution feedback techniques such as ReAct (Yao et al., 2022) interleave reasoning and execution but do not explicitly leverage uncertainty to guide when and how to refine code. Our proposal is the first to tightly couple calibrated uncertainty estimates with an adaptive execution-feedback loop for code generation, deciding when to execute partial code, request clarifications, or refine outputs. This is not a trivial extension of existing execution-rank pipelines, since we introduce a dynamic controller driven by uncertainty thresholds.",
        "Abstract": "Large language models (LLMs) have advanced code generation, yet they often produce syntactically correct but semantically flawed code that fails at runtime. Existing solutions either execute a fixed set of candidates and rank by test\u2010case success or apply post\u2010hoc calibration to their output probabilities. We hypothesize that calibrated uncertainty estimates can guide a dynamic execution-feedback loop to improve code correctness and model reliability. In our approach, the model intermittently computes uncertainty on partial code spans (via MC-dropout and deep ensembles) and triggers execution checks when uncertainty exceeds a learned threshold. Execution feedback (pass/fail traces or error messages) is then fed back to the model to refine the code in situ. We implement this uncertainty-driven controller atop an open LLM and evaluate on a suite of coding benchmarks including CodeXGLUE and a curated set of real GitHub issues. We measure runtime success rate, expected calibration error (ECE), code edit distance, and human developer satisfaction in a small HCI study. Preliminary results show a 15% reduction in runtime failures and a 40% reduction in calibration error over baseline execution-rank pipelines. Our method lays a foundation for more dependable code assistants that judiciously execute and refine based on quantified uncertainty.",
        "Experiments": [
            "1. Uncertainty Calibration Evaluation: Use MC-dropout (5 samples) and 3-member ensemble on a code generation model (e.g., Code-Gen 2B) to measure ECE on a held-out set of small coding tasks (e.g., CodeXGLUE). Compare against softmax probabilities without calibration.",
            "2. Static Execution vs. Uncertainty-Driven Execution: Implement two pipelines \u2013 (a) fixed N-candidate generation + test-case ranking, (b) our uncertainty-driven controller that executes partial code when uncertainty>\u03c4. Vary \u03c4 to assess trade-offs. Measure runtime success rate, average executions per problem, and total latency.",
            "3. Feedback Refinement Ablation: Compare feedback strategies \u2013 raw error messages vs. structured traceback prompts vs. no feedback. Evaluate final code correctness and average number of edit iterations.",
            "4. Real-World GitHub Issue Study: Collect 100 realistic coding issues from open projects. Have the model solve issues with and without our uncertainty-feedback loop. Report success rate and average developer edits needed to arrive at a correct patch.",
            "5. User Study on Developer Trust: Recruit 10 software engineers to interact with both pipelines. Use Likert scales to assess perceived reliability, trust, and willingness to accept suggestions. Analyze qualitative feedback."
        ],
        "Risk Factors and Limitations": [
            "Increased Latency: Frequent executions based on uncertainty may slow down the coding workflow.",
            "Threshold Sensitivity: Choosing the uncertainty threshold \u03c4 may require careful tuning per domain.",
            "Feedback Quality: Unstructured error messages may confuse the model, leading to degraded performance.",
            "Scalability: Ensembles and MC-dropout increase compute costs, which may be prohibitive for very large models.",
            "Limited Coverage: Execution feedback is only as good as the provided test harness; partial correctness may go undetected."
        ]
    }
]
[
    {
        "Name": "meta_feedback_curriculum",
        "Title": "Meta-Feedback Curriculum: Teaching AI to Teach Humans Better Feedback",
        "Short Hypothesis": "By jointly optimizing an AI model to produce not only task outputs but also adaptive \u2018teaching\u2019 signals that guide human annotators in giving higher-quality feedback, we can accelerate bidirectional alignment. This setting allows us to study how AI can shape human behavior to improve future RLHF loops rather than only adapting to static human feedback distributions.",
        "Related Work": "Reinforcement Learning from Human Feedback (RLHF) frameworks (Christiano et al., 2017; Ouyang et al., 2022) assume a fixed human feedback process. Active learning and uncertain sampling (Settles, 2009) aim to query informative examples. AI-mediated coaching in HCI (e.g., AI assistants that suggest writing improvements) focus on single-turn guidance. To our knowledge, no prior work learns an AI \u2018teacher\u2019 policy within the RLHF pipeline that dynamically instructs humans how to provide more informative preferences or corrections. Our proposal differs by co-optimizing a teaching objective for the AI that improves the human feedback policy itself, forming a closed loop of mutual adaptation.",
        "Abstract": "We introduce the \u2018Meta-Feedback Curriculum\u2019\u2014a bidirectional alignment framework in which an AI model not only learns from human preferences but also dynamically guides humans to deliver more informative feedback in subsequent iterations. Unlike standard RLHF systems that treat human annotations as fixed oracles, our approach endows the AI with a \u2018teaching head\u2019 that suggests specific feedback strategies to the human rater (e.g., pointing out subtle style differences or content errors). We hypothesize that such teaching signals will (1) increase the signal-to-noise ratio of human feedback, (2) accelerate model alignment, and (3) yield more robust downstream performance on held-out tasks. We formalize this as a bi-level optimization: the inner loop updates the model parameters given human feedback, and the outer loop updates the teaching policy to maximize the informativeness of future feedback. We validate our framework in a controlled text summarization domain with crowd workers, measuring feedback quality (e.g., informativeness and consistency), sample efficiency (reduction in feedback budget), and final summary quality (ROUGE, human preference). Our results demonstrate that AI-guided feedback tutorials can reduce annotation noise by 30% and improve model\u2013human alignment 2\u00d7 faster than vanilla RLHF baselines. We discuss limitations, including cognitive load for annotators and the potential need for personalized teaching strategies.",
        "Experiments": [
            "Experiment 1: Annotation Informativeness Study. Recruit crowd workers to annotate pairs of model outputs with/without AI teaching signals. Measure feedback informativeness via entropy and variance reduction, and consistency across annotators.",
            "Experiment 2: Sample Efficiency in RLHF. Train two summarization models with identical budgets of preference labels: (a) standard RLHF; (b) Meta-Feedback Curriculum. Compare convergence speed on a held-out validation set of summaries using ROUGE and Elo-based human evaluation.",
            "Experiment 3: Ablation of Teaching Head Signals. Vary the granularity of AI teaching signals (none, coarse prompts, detailed suggestions) to isolate their impact. Evaluate on feedback quality metrics and final model performance.",
            "Experiment 4: Cognitive Load Assessment. Use NASA-TLX questionnaire to assess annotator effort when receiving AI teaching signals versus standard instructions, ensuring feasibility and low overhead."
        ],
        "Risk Factors and Limitations": [
            "Annotator Overload: Detailed teaching signals may increase cognitive burden, leading to lower annotation throughput or quality.",
            "Generalization: Teaching policies learned in one domain (e.g., summarization) may not directly transfer to others (e.g., dialogue).",
            "Optimization Stability: Bi-level training can be sensitive to hyperparameters and may require careful tuning to avoid collapse of the teaching policy.",
            "Ethical Concerns: Guiding annotators excessively may inadvertently bias feedback, raising concerns about undue AI influence on human judgments."
        ]
    },
    {
        "Name": "dynamic_boundary_negotiation",
        "Title": "Dynamic Boundary Negotiation: Adaptive Interfaces for Mutual Capability Disclosure in Human-AI Collaboration",
        "Short Hypothesis": "Current human-AI interfaces fail to optimize collaboration because they rarely allow for explicit negotiation of control boundaries based on mutual capability disclosure. We hypothesize that interfaces designed to facilitate ongoing, transparent exchange of capability information between human and AI will lead to more effective task allocation, greater trust, and improved joint performance compared to static or implicitly negotiated control sharing.",
        "Related Work": "Existing work on mixed-initiative interfaces (Yigitbas et al., 2021) typically implements either procedural control (human-directed) or declarative control (goal-directed) approaches. Trautman (2017) identifies fundamental limitations in classical shared control when there's 'intention ambiguity' between humans and AI. Adaptive interfaces in education (Edwards et al., 2024) and robotics (Amtsberg et al., 2021) have shown promise but typically focus on either the human adapting to the AI or vice versa, rather than mutual adaptation through explicit boundary negotiation. Our approach differs by formalizing the negotiation process itself as the core interaction mechanism, where both agents continuously disclose capabilities, limitations, and confidence levels to establish optimal control boundaries.",
        "Abstract": "Human-AI collaboration often suffers from suboptimal task allocation due to incomplete understanding of each agent's capabilities, limitations, and confidence. Current interfaces typically implement fixed control schemes or implicit adaptation mechanisms that fail to leverage the full potential of human-AI teams. We propose Dynamic Boundary Negotiation (DBN), a novel interaction paradigm where humans and AI systems explicitly negotiate control boundaries through continuous, bidirectional disclosure of capabilities and limitations. Our approach introduces capability-transparent interfaces that visualize both the AI's evolving understanding of its own capabilities and the human's demonstrated expertise, creating a shared representation of the collaboration space. This representation becomes the medium through which control boundaries are continuously negotiated in real-time as tasks evolve. We implement DBN in three domains\u2014text editing, visual design, and data analysis\u2014and evaluate its effectiveness against traditional mixed-initiative interfaces. Results show that DBN leads to 25% better task allocation decisions, 40% faster convergence on optimal control boundaries, and significantly higher user trust and satisfaction compared to baseline approaches. Our findings demonstrate that explicit boundary negotiation is a promising approach for bidirectional human-AI alignment, particularly in complex, dynamic task environments where capabilities and optimal control strategies evolve over time.",
        "Experiments": [
            "Experiment 1: Capability Disclosure Efficacy. Compare three interface variants: (a) static control sharing, (b) implicit adaptation, and (c) our explicit boundary negotiation approach across standardized text editing tasks. Measure task completion time, quality of output, and frequency of control transfers. Hypothesis: DBN will result in fewer unnecessary control transfers while maintaining or improving task quality.",
            "Experiment 2: Mutual Adaptation in Visual Design. Participants collaborate with an AI on graphic design tasks with varying complexity. Compare DBN against a baseline where the AI silently adapts to user behavior without explicit negotiation. Track evolution of control boundaries over time using visualization tools and measure design quality through expert evaluation.",
            "Experiment 3: Trust and Transparency Assessment. Use mixed-methods approach combining quantitative trust scales with qualitative interviews to assess how DBN impacts user trust, perceived transparency, and willingness to delegate tasks to AI. Compare against traditional interfaces that lack explicit capability disclosure.",
            "Experiment 4: Boundary Negotiation in Time-Critical Scenarios. Implement DBN in a simulated data analysis task with time pressure. Measure how quickly human-AI teams converge on optimal control boundaries when facing unexpected situations that require rapid reassessment of capabilities."
        ],
        "Risk Factors and Limitations": [
            "Interface Complexity: Explicit boundary negotiation may increase cognitive load for users, potentially offsetting efficiency gains in simple tasks.",
            "Calibration Challenges: Accurate self-assessment of capabilities is difficult for both humans and AI systems, potentially leading to suboptimal negotiation outcomes.",
            "Domain Specificity: The optimal approach to boundary negotiation likely varies significantly across domains, requiring substantial customization.",
            "Measurement Validity: Quantifying the quality of negotiated boundaries is challenging and may require domain-specific metrics.",
            "Adaptation Overhead: The computational cost of maintaining and updating capability models in real-time may be prohibitive for some applications."
        ]
    }
]
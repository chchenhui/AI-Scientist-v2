{"edges": [[0, 12], [0, 5], [0, 2], [0, 1], [0, 6], [0, 3], [0, 4], [1, 13], [1, 15], [1, 14], [1, 16], [4, 8], [4, 9], [4, 7], [5, 11], [8, 10]], "layout": [[0.5793650793650794, 0.0], [0.16666666666666666, 0.33333333333333337], [0.2777777777777778, 0.33333333333333337], [0.3888888888888889, 0.33333333333333337], [0.5555555555555556, 0.33333333333333337], [0.7777777777777778, 0.33333333333333337], [0.8888888888888888, 0.33333333333333337], [0.4444444444444444, 0.6666666666666667], [0.5555555555555556, 0.6666666666666667], [0.6666666666666666, 0.6666666666666667], [0.5555555555555556, 1.0], [0.7777777777777778, 0.6666666666666667], [1.0, 0.33333333333333337], [0.0, 0.6666666666666667], [0.1111111111111111, 0.6666666666666667], [0.2222222222222222, 0.6666666666666667], [0.3333333333333333, 0.6666666666666667]], "plan": ["We load MNIST with normalization and create a rotated test split to simulate\nrejuvenation. Two small networks\u2014a simple MLP and a CNN\u2014are defined and moved to\nGPU if available. Each model is trained for a few epochs using Adam, and we\nrecord the average training loss. After every epoch we evaluate both models on\nthe original and rotated test sets, computing and printing validation losses. We\nthen calculate Challenge Gap Recovery (CGR) as the relative increase in inter-\nmodel accuracy variance between the rotated and original splits, printing CGR\neach epoch. All per-epoch losses, accuracies, CGR values, and final predictions\nwith ground truths are stored in an `experiment_data` dictionary. At the end, we\nsave `experiment_data` as a NumPy file in the working directory.", "Hyperparam tuning name: n_epochs. I will iterate over a list of epoch counts\n(e.g., [5, 10, 15, 20]), reinitialize the MLP and CNN for each run, and train\nfor the specified number of epochs while collecting training/validation losses,\noriginal/augmented accuracies, and CGR each epoch. After each run I gather final\npredictions and ground truths for both splits. All results are stored in a\nnested experiment_data['n_epochs'] dictionary keyed by epoch count and then\nsaved to 'experiment_data.npy'.", "Hyperparam tuning name: learning_rate. I will loop over the specified learning\nrates, reinitialize both MLP and CNN for each rate and train for a fixed number\nof epochs, recording per-epoch training/validation losses, clean and augmented\nvalidation accuracy, and computing CGR across the two models. After each run, I\ncapture the final CNN predictions and ground truths on both splits. All\ncollected lists are then converted to NumPy arrays and saved in a single\n`experiment_data.npy` file under the key `\"learning_rate\"`.", "Hyperparam tuning name: batch_size. We sweep over batch sizes [32,64,128,256],\nand for each batch size we reinitialize both MLP and CNN models and\ncorresponding optimizers. We train each for n_epochs, recording per\u2010epoch\ntrain/val losses and original/augmented accuracies into a nested\n`experiment_data` dictionary keyed by batch size and model name. After training,\nwe evaluate on the original test set to save final predictions and ground truth.\nFinally, we save the entire `experiment_data` dict as `experiment_data.npy` for\nlater analysis.", "Hyperparam tuning name: weight_decay. We sweep weight_decay values [0, 1e-5,\n1e-4, 1e-3] by reinitializing MLP and CNN models and Adam optimizers with each\ndecay. For each setting we train both models for `n_epochs`, computing average\ntrain/validation losses, original and rotated-test accuracies, and the CGR\nacross the two models at each epoch. After each hyperparam setting we evaluate\nthe CNN on both splits to collect final predictions and ground-truth labels. All\nresults are stored under `experiment_data['weight_decay'][str(wd)]` and saved to\n`experiment_data.npy`.", "Hyperparam tuning name: dropout_rate. We extend the MLP and CNN classes to\naccept a `dropout_rate` and insert `nn.Dropout` layers after each hidden\nactivation. Then we loop over a grid of dropout rates from 0.0 to 0.5,\ninstantiating new models for each value, training them for a fixed number of\nepochs, and evaluating on the original and rotated test sets to collect\ntrain/val losses, accuracies, and predictions. We compute the CGR for each\ndropout setting based on the standard deviation of original vs.\\ rotated\naccuracies across both models, and store all results in a structured\n`experiment_data` dict that is finally saved to `experiment_data.npy`.", "Hyperparam tuning name: label_smoothing. Below is a one\u2010file script that loops\nover \u03b5\u2208{0.0,0.05,0.1,0.2}, trains a fresh CNN with a custom\nsmoothed\u2010cross\u2010entropy loss, logs per\u2010epoch train/val losses and\noriginal/rotated accuracies, and finally saves predictions and ground truth on\nthe original test split. All data are stored in\nexperiment_data['label_smoothing'][f'eps_{\u03b5}'] and dumped to\n'experiment_data.npy'.", "I refactored training to loop over a grid of learning rates and batch sizes\n(updating to 10 epochs) and fixed the training function to accept a DataLoader\nargument so each config rebuilds its loaders. I added two HuggingFace image\ndatasets (Fashion-MNIST and KMNIST) for extra evaluation, applying the same\nnormalization and collating them into DataLoaders. At each epoch I compute and\nprint the validation loss, track accuracy and a new Benchmark Discriminative\nPower (BDP) metric (the std of model accuracies per dataset), and finally save\nall losses, metrics, BDPs, predictions, and ground truths into a structured\nnumpy file.", "We add two HuggingFace datasets (Fashion-MNIST and Kuzushiji-MNIST), map them\ninto PyTorch DataLoaders with the same MNIST normalization, and extend our\ntraining loop to evaluate and record their accuracies alongside the original and\naugmented MNIST.  We compute the Benchmark Discriminative Power (BDP) for each\ndataset as the standard deviation of accuracies across the MLP and CNN at every\nepoch and store it in `experiment_data`.  The `evaluate` function is generalized\nto handle both tuple and dict batches so that HF datasets integrate seamlessly,\nand at the end we save per-dataset predictions and ground truths for the CNN as\nwell as all metrics.", "We can fix the missing-datasets bug by loading two additional\nimage\u2010classification test sets (fashion_mnist and kuzushiji_mnist) from\nHuggingFace, wrapping them into PyTorch DataLoaders with the same normalization\npipeline, and evaluating both our MLP and CNN on these alongside the\noriginal/augmented MNIST. We also compute the Benchmark Discriminative Power\n(BDP) at each epoch as the standard deviation of the two model accuracies\n(normalized by [0,1]) for each test set. All metrics\u2014including\norig/aug/hug1/hug2 accuracies, BDPs, and the original CGR\u2014are recorded per epoch\nand saved in experiment_data.npy.", "We correct the HuggingFace dataset name to \u201ckuzushiji_mnist\u201d and unify all HF\ntest\u2010set transforms by resizing to 28\u00d728, converting to one channel,\ntensorizing, and normalizing so that we can also load USPS and SVHN_CROPPED for\nout\u2010of\u2010domain evaluation.  We bump the baseline hyperparameters to 10 epochs,\nlearning rate 1e\u22122 and train\u2010batch size 128 to improve performance, then compute\nper\u2010domain BDP and CGR metrics across the two models as before, and finally save\nall results to \u201cexperiment_data.npy.\u201d", "I updated the hyperparameters to a lower learning rate, larger batch size, and\nmore epochs to improve stability, and kept the MLP/CNN architectures unchanged.\nI added two new out-of-distribution test sets (Fashion-MNIST and Kuzushiji-\nMNIST) from HuggingFace with the same normalization. During training I now\nevaluate all four test sets each epoch, printing validation loss and computing a\nBenchmark Discriminative Power (BDP) metric as the standard deviation of their\naccuracies. All per-epoch losses and metrics (including orig, aug, fashion,\nkmnist accuracies, and BDP) are tracked in `experiment_data` and saved at the\nend.", "Hyperparam tuning name: adam_beta1. Below is a script that loops over several\n`beta1` settings for the Adam optimizer, reinitializes and trains both an MLP\nand a CNN for each setting, collects training/validation losses and\noriginal/augmented accuracy each epoch, computes CGR across the two models, and\nsaves final CNN predictions and ground truths. All results are stored in a\nnested `experiment_data` dict under `\"adam_beta1\"` and then saved via\n`np.save()` to `\"working/experiment_data.npy\"`.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Data preparation\nnormalize = transforms.Normalize((0.1307,), (0.3081,))\ntrain_transform = transforms.Compose([transforms.ToTensor(), normalize])\ntest_transform = train_transform\naug_transform = transforms.Compose(\n    [transforms.RandomRotation(30), transforms.ToTensor(), normalize]\n)\n\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=train_transform\n)\norig_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=test_transform\n)\naug_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=aug_transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\norig_test_loader = DataLoader(orig_test_dataset, batch_size=1000, shuffle=False)\naug_test_loader = DataLoader(aug_test_dataset, batch_size=1000, shuffle=False)\n\n\n# Model definitions\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(nn.Conv2d(1, 16, 3, 1), nn.ReLU(), nn.MaxPool2d(2))\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(16 * 13 * 13, 64), nn.ReLU(), nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(self.conv(x))\n\n\nmodels = {\"MLP\": MLP().to(device), \"CNN\": CNN().to(device)}\noptimizers = {name: optim.Adam(m.parameters(), lr=1e-3) for name, m in models.items()}\ncriterion = nn.CrossEntropyLoss()\n\n# Experiment data\nexperiment_data = {\n    \"original\": {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"orig_acc\": [], \"aug_acc\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    },\n    \"CGR\": [],\n}\n\n\ndef train_one_epoch(model, optimizer):\n    model.train()\n    total_loss = 0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n    return total_loss / len(train_loader.dataset)\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    all_preds = []\n    all_true = []\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            total_loss += loss.item() * x.size(0)\n            pred = out.argmax(dim=1)\n            correct += pred.eq(y).sum().item()\n            all_preds.append(pred.cpu().numpy())\n            all_true.append(y.cpu().numpy())\n    return (\n        total_loss / len(loader.dataset),\n        correct / len(loader.dataset),\n        np.concatenate(all_preds),\n        np.concatenate(all_true),\n    )\n\n\n# Training loop\nn_epochs = 5\nfor epoch in range(1, n_epochs + 1):\n    # Train both models\n    for name in models:\n        train_loss = train_one_epoch(models[name], optimizers[name])\n        experiment_data[\"original\"][\"losses\"][\"train\"].append(train_loss)\n    # Evaluate\n    orig_accs = []\n    aug_accs = []\n    for name in models:\n        val_loss, orig_acc, _, _ = evaluate(models[name], orig_test_loader)\n        experiment_data[\"original\"][\"losses\"][\"val\"].append(val_loss)\n        print(f\"Model {name} Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n        _, aug_acc, _, _ = evaluate(models[name], aug_test_loader)\n        experiment_data[\"original\"][\"metrics\"][\"orig_acc\"].append(orig_acc)\n        experiment_data[\"original\"][\"metrics\"][\"aug_acc\"].append(aug_acc)\n        orig_accs.append(orig_acc)\n        aug_accs.append(aug_acc)\n    # Compute CGR\n    std_orig = np.std(orig_accs)\n    std_aug = np.std(aug_accs)\n    cgr = (std_aug - std_orig) / (std_orig + 1e-8)\n    experiment_data[\"CGR\"].append(cgr)\n    print(f\"Epoch {epoch}: CGR = {cgr:.4f}\")\n\n# Save final predictions and ground truth for both splits (using CNN)\n_, _, preds, gts = evaluate(models[\"CNN\"], orig_test_loader)\nexperiment_data[\"original\"][\"predictions\"] = preds\nexperiment_data[\"original\"][\"ground_truth\"] = gts\n_, _, preds_aug, gts_aug = evaluate(models[\"CNN\"], aug_test_loader)\nexperiment_data[\"augmented\"] = {\"predictions\": preds_aug, \"ground_truth\": gts_aug}\n\n# Save all metrics and data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Transforms and Datasets\nnormalize = transforms.Normalize((0.1307,), (0.3081,))\ntrain_transform = transforms.Compose([transforms.ToTensor(), normalize])\ntest_transform = train_transform\naug_transform = transforms.Compose(\n    [transforms.RandomRotation(30), transforms.ToTensor(), normalize]\n)\n\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=train_transform\n)\norig_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=test_transform\n)\naug_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=aug_transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\norig_test_loader = DataLoader(orig_test_dataset, batch_size=1000, shuffle=False)\naug_test_loader = DataLoader(aug_test_dataset, batch_size=1000, shuffle=False)\n\n\n# Model definitions\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(nn.Conv2d(1, 16, 3, 1), nn.ReLU(), nn.MaxPool2d(2))\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(16 * 13 * 13, 64), nn.ReLU(), nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(self.conv(x))\n\n\n# Loss & eval functions\ncriterion = nn.CrossEntropyLoss()\n\n\ndef train_one_epoch(model, optimizer):\n    model.train()\n    total_loss = 0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n    return total_loss / len(train_loader.dataset)\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, correct = 0, 0\n    preds, trues = [], []\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            total_loss += loss.item() * x.size(0)\n            pred = out.argmax(dim=1)\n            correct += pred.eq(y).sum().item()\n            preds.append(pred.cpu().numpy())\n            trues.append(y.cpu().numpy())\n    return (\n        total_loss / len(loader.dataset),\n        correct / len(loader.dataset),\n        np.concatenate(preds),\n        np.concatenate(trues),\n    )\n\n\n# Hyperparameter tuning for n_epochs\nn_epochs_list = [5, 10, 15, 20]\nexperiment_data = {\"n_epochs\": {}}\n\nfor n_epochs in n_epochs_list:\n    print(f\"\\n=== Running with n_epochs = {n_epochs} ===\")\n    # Initialize models and optimizers\n    models = {\"MLP\": MLP().to(device), \"CNN\": CNN().to(device)}\n    optimizers = {\n        name: optim.Adam(m.parameters(), lr=1e-3) for name, m in models.items()\n    }\n    # Data structure for this run\n    run_data = {\"models\": {}, \"cgr\": []}\n    for name in models:\n        run_data[\"models\"][name] = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"orig_acc\": [], \"aug_acc\": []},\n            \"predictions\": None,\n            \"ground_truth\": None,\n            \"aug_predictions\": None,\n            \"aug_ground_truth\": None,\n        }\n    # Training loop\n    for epoch in range(1, n_epochs + 1):\n        # train\n        for name in models:\n            tl = train_one_epoch(models[name], optimizers[name])\n            run_data[\"models\"][name][\"losses\"][\"train\"].append(tl)\n        # eval and metrics\n        orig_accs, aug_accs = [], []\n        for name in models:\n            vl, oa, _, _ = evaluate(models[name], orig_test_loader)\n            run_data[\"models\"][name][\"losses\"][\"val\"].append(vl)\n            run_data[\"models\"][name][\"metrics\"][\"orig_acc\"].append(oa)\n            _, aa, _, _ = evaluate(models[name], aug_test_loader)\n            run_data[\"models\"][name][\"metrics\"][\"aug_acc\"].append(aa)\n            orig_accs.append(oa)\n            aug_accs.append(aa)\n            print(\n                f\"{name} Epoch {epoch}: val_loss={vl:.4f}, orig_acc={oa:.4f}, aug_acc={aa:.4f}\"\n            )\n        cgr = (np.std(aug_accs) - np.std(orig_accs)) / (np.std(orig_accs) + 1e-8)\n        run_data[\"cgr\"].append(cgr)\n        print(f\"Epoch {epoch}: CGR={cgr:.4f}\")\n    # Final predictions & ground truth\n    for name in models:\n        _, _, p, gt = evaluate(models[name], orig_test_loader)\n        run_data[\"models\"][name][\"predictions\"] = p\n        run_data[\"models\"][name][\"ground_truth\"] = gt\n        _, _, pa, gta = evaluate(models[name], aug_test_loader)\n        run_data[\"models\"][name][\"aug_predictions\"] = pa\n        run_data[\"models\"][name][\"aug_ground_truth\"] = gta\n    # Store run data\n    experiment_data[\"n_epochs\"][str(n_epochs)] = run_data\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\n\n# create working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# data\nnormalize = transforms.Normalize((0.1307,), (0.3081,))\ntrain_transform = transforms.Compose([transforms.ToTensor(), normalize])\ntest_transform = train_transform\naug_transform = transforms.Compose(\n    [transforms.RandomRotation(30), transforms.ToTensor(), normalize]\n)\n\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=train_transform\n)\norig_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=test_transform\n)\naug_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=aug_transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\norig_test_loader = DataLoader(orig_test_dataset, batch_size=1000, shuffle=False)\naug_test_loader = DataLoader(aug_test_dataset, batch_size=1000, shuffle=False)\n\n\n# models\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(nn.Conv2d(1, 16, 3, 1), nn.ReLU(), nn.MaxPool2d(2))\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(16 * 13 * 13, 64), nn.ReLU(), nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(self.conv(x))\n\n\n# loss / utils\ncriterion = nn.CrossEntropyLoss()\n\n\ndef train_one_epoch(model, optimizer):\n    model.train()\n    total_loss = 0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n    return total_loss / len(train_loader.dataset)\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, correct = 0, 0\n    preds, trues = [], []\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            total_loss += loss.item() * x.size(0)\n            p = out.argmax(dim=1)\n            correct += p.eq(y).sum().item()\n            preds.append(p.cpu().numpy())\n            trues.append(y.cpu().numpy())\n    return (\n        total_loss / len(loader.dataset),\n        correct / len(loader.dataset),\n        np.concatenate(preds),\n        np.concatenate(trues),\n    )\n\n\n# hyperparameter grid\nlearning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\nn_epochs = 5\n\n# container\nexperiment_data = {\"learning_rate\": {}}\n\nfor lr in learning_rates:\n    lr_str = str(lr)\n    # init storage\n    experiment_data[\"learning_rate\"][lr_str] = {\n        \"MLP\": {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"orig_acc\": [], \"aug_acc\": []},\n        },\n        \"CNN\": {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"orig_acc\": [], \"aug_acc\": []},\n        },\n        \"CGR\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"predictions_aug\": [],\n        \"ground_truth_aug\": [],\n    }\n    # reinit models & optimizers\n    models = {\"MLP\": MLP().to(device), \"CNN\": CNN().to(device)}\n    optimizers = {name: optim.Adam(m.parameters(), lr=lr) for name, m in models.items()}\n\n    # training loop\n    for epoch in range(1, n_epochs + 1):\n        # train\n        for name in models:\n            tl = train_one_epoch(models[name], optimizers[name])\n            experiment_data[\"learning_rate\"][lr_str][name][\"losses\"][\"train\"].append(tl)\n        # eval\n        orig_accs, aug_accs = [], []\n        for name in models:\n            vl, oa, _, _ = evaluate(models[name], orig_test_loader)\n            _, aa, _, _ = evaluate(models[name], aug_test_loader)\n            experiment_data[\"learning_rate\"][lr_str][name][\"losses\"][\"val\"].append(vl)\n            experiment_data[\"learning_rate\"][lr_str][name][\"metrics\"][\n                \"orig_acc\"\n            ].append(oa)\n            experiment_data[\"learning_rate\"][lr_str][name][\"metrics\"][\"aug_acc\"].append(\n                aa\n            )\n            orig_accs.append(oa)\n            aug_accs.append(aa)\n            print(\n                f\"LR {lr_str} Model {name} Epoch {epoch}: val_loss={vl:.4f}, orig_acc={oa:.4f}, aug_acc={aa:.4f}\"\n            )\n        # CGR across models\n        std_o = np.std(orig_accs)\n        std_a = np.std(aug_accs)\n        cgr = (std_a - std_o) / (std_o + 1e-8)\n        experiment_data[\"learning_rate\"][lr_str][\"CGR\"].append(cgr)\n        print(f\"LR {lr_str} Epoch {epoch}: CGR = {cgr:.4f}\")\n\n    # final CNN preds\n    _, _, p, gt = evaluate(models[\"CNN\"], orig_test_loader)\n    experiment_data[\"learning_rate\"][lr_str][\"predictions\"] = p\n    experiment_data[\"learning_rate\"][lr_str][\"ground_truth\"] = gt\n    _, _, pa, ga = evaluate(models[\"CNN\"], aug_test_loader)\n    experiment_data[\"learning_rate\"][lr_str][\"predictions_aug\"] = pa\n    experiment_data[\"learning_rate\"][lr_str][\"ground_truth_aug\"] = ga\n\n# convert lists to arrays\nfor lr_str, data in experiment_data[\"learning_rate\"].items():\n    for m in [\"MLP\", \"CNN\"]:\n        data[m][\"losses\"][\"train\"] = np.array(data[m][\"losses\"][\"train\"])\n        data[m][\"losses\"][\"val\"] = np.array(data[m][\"losses\"][\"val\"])\n        data[m][\"metrics\"][\"orig_acc\"] = np.array(data[m][\"metrics\"][\"orig_acc\"])\n        data[m][\"metrics\"][\"aug_acc\"] = np.array(data[m][\"metrics\"][\"aug_acc\"])\n    data[\"CGR\"] = np.array(data[\"CGR\"])\n    data[\"predictions\"] = np.array(data[\"predictions\"])\n    data[\"ground_truth\"] = np.array(data[\"ground_truth\"])\n    data[\"predictions_aug\"] = np.array(data[\"predictions_aug\"])\n    data[\"ground_truth_aug\"] = np.array(data[\"ground_truth_aug\"])\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# prepare working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# transforms and datasets\nnormalize = transforms.Normalize((0.1307,), (0.3081,))\ntrain_transform = transforms.Compose([transforms.ToTensor(), normalize])\ntest_transform = train_transform\naug_transform = transforms.Compose(\n    [transforms.RandomRotation(30), transforms.ToTensor(), normalize]\n)\n\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=train_transform\n)\norig_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=test_transform\n)\naug_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=aug_transform\n)\n\norig_test_loader = DataLoader(orig_test_dataset, batch_size=1000, shuffle=False)\naug_test_loader = DataLoader(aug_test_dataset, batch_size=1000, shuffle=False)\n\n\n# model definitions\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(nn.Conv2d(1, 16, 3, 1), nn.ReLU(), nn.MaxPool2d(2))\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(16 * 13 * 13, 64), nn.ReLU(), nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(self.conv(x))\n\n\n# utility functions\ncriterion = nn.CrossEntropyLoss()\n\n\ndef train_one_epoch(model, opt, loader):\n    model.train()\n    tot = 0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        opt.zero_grad()\n        l = criterion(model(x), y)\n        l.backward()\n        opt.step()\n        tot += l.item() * x.size(0)\n    return tot / len(loader.dataset)\n\n\ndef evaluate(model, loader):\n    model.eval()\n    tot = 0\n    corr = 0\n    preds = []\n    gts = []\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            tot += criterion(out, y).item() * x.size(0)\n            p = out.argmax(1)\n            corr += p.eq(y).sum().item()\n            preds.append(p.cpu().numpy())\n            gts.append(y.cpu().numpy())\n    return (\n        tot / len(loader.dataset),\n        corr / len(loader.dataset),\n        np.concatenate(preds),\n        np.concatenate(gts),\n    )\n\n\n# hyperparameter sweep\nbatch_sizes = [32, 64, 128, 256]\nn_epochs = 5\nexperiment_data = {\"batch_size_sweep\": {}}\n\nfor bs in batch_sizes:\n    key = str(bs)\n    experiment_data[\"batch_size_sweep\"][key] = {}\n    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n    for model_name, ModelClass in [(\"MLP\", MLP), (\"CNN\", CNN)]:\n        # initialize\n        model = ModelClass().to(device)\n        opt = optim.Adam(model.parameters(), lr=1e-3)\n        # containers\n        mdata = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"orig_acc\": [], \"aug_acc\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        # train\n        for epoch in range(1, n_epochs + 1):\n            trl = train_one_epoch(model, opt, train_loader)\n            mdata[\"losses\"][\"train\"].append(trl)\n            vl, oa, _, _ = evaluate(model, orig_test_loader)\n            _, aa, _, _ = evaluate(model, aug_test_loader)\n            mdata[\"losses\"][\"val\"].append(vl)\n            mdata[\"metrics\"][\"orig_acc\"].append(oa)\n            mdata[\"metrics\"][\"aug_acc\"].append(aa)\n            print(\n                f\"BS {bs} {model_name} E{epoch}: tr_loss {trl:.4f}, val_loss {vl:.4f}, orig_acc {oa:.4f}, aug_acc {aa:.4f}\"\n            )\n        # final preds\n        _, _, preds, gts = evaluate(model, orig_test_loader)\n        mdata[\"predictions\"] = preds\n        mdata[\"ground_truth\"] = gts\n        # store\n        experiment_data[\"batch_size_sweep\"][key][model_name] = mdata\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Data preparation\nnormalize = transforms.Normalize((0.1307,), (0.3081,))\ntrain_transform = transforms.Compose([transforms.ToTensor(), normalize])\ntest_transform = train_transform\naug_transform = transforms.Compose(\n    [transforms.RandomRotation(30), transforms.ToTensor(), normalize]\n)\n\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=train_transform\n)\norig_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=test_transform\n)\naug_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=aug_transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\norig_test_loader = DataLoader(orig_test_dataset, batch_size=1000, shuffle=False)\naug_test_loader = DataLoader(aug_test_dataset, batch_size=1000, shuffle=False)\n\n\n# Model definitions\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(nn.Conv2d(1, 16, 3, 1), nn.ReLU(), nn.MaxPool2d(2))\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(16 * 13 * 13, 64), nn.ReLU(), nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(self.conv(x))\n\n\n# Training and evaluation functions\ncriterion = nn.CrossEntropyLoss()\n\n\ndef train_one_epoch(model, optimizer):\n    model.train()\n    total_loss = 0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n    return total_loss / len(train_loader.dataset)\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, correct = 0, 0\n    all_preds, all_true = [], []\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            total_loss += loss.item() * x.size(0)\n            pred = out.argmax(dim=1)\n            correct += pred.eq(y).sum().item()\n            all_preds.append(pred.cpu().numpy())\n            all_true.append(y.cpu().numpy())\n    all_preds = np.concatenate(all_preds)\n    all_true = np.concatenate(all_true)\n    return (\n        total_loss / len(loader.dataset),\n        correct / len(loader.dataset),\n        all_preds,\n        all_true,\n    )\n\n\n# Hyperparameter tuning over weight_decay\nweight_decay_values = [0, 1e-5, 1e-4, 1e-3]\nn_epochs = 5\nexperiment_data = {\"weight_decay\": {}}\n\nfor wd in weight_decay_values:\n    key = str(wd)\n    experiment_data[\"weight_decay\"][key] = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"orig_acc\": [], \"aug_acc\": []},\n        \"CGR\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"predictions_aug\": [],\n    }\n    # Initialize models and optimizers\n    models = {\"MLP\": MLP().to(device), \"CNN\": CNN().to(device)}\n    optimizers = {\n        name: optim.Adam(m.parameters(), lr=1e-3, weight_decay=wd)\n        for name, m in models.items()\n    }\n    # Training loop\n    for epoch in range(1, n_epochs + 1):\n        train_losses, val_losses, orig_accs, aug_accs = [], [], [], []\n        for name in models:\n            tl = train_one_epoch(models[name], optimizers[name])\n            train_losses.append(tl)\n        for name in models:\n            vl, oacc, _, _ = evaluate(models[name], orig_test_loader)\n            _, aacc, _, _ = evaluate(models[name], aug_test_loader)\n            val_losses.append(vl)\n            orig_accs.append(oacc)\n            aug_accs.append(aacc)\n        avg_tl = float(np.mean(train_losses))\n        avg_vl = float(np.mean(val_losses))\n        avg_oacc = float(np.mean(orig_accs))\n        avg_aacc = float(np.mean(aug_accs))\n        std_o, std_a = float(np.std(orig_accs)), float(np.std(aug_accs))\n        cgr = float((std_a - std_o) / (std_o + 1e-8))\n        ed = experiment_data[\"weight_decay\"][key]\n        ed[\"losses\"][\"train\"].append(avg_tl)\n        ed[\"losses\"][\"val\"].append(avg_vl)\n        ed[\"metrics\"][\"orig_acc\"].append(avg_oacc)\n        ed[\"metrics\"][\"aug_acc\"].append(avg_aacc)\n        ed[\"CGR\"].append(cgr)\n        print(\n            f\"WD {wd} Epoch {epoch}: train_loss={avg_tl:.4f}, val_loss={avg_vl:.4f}, \"\n            f\"orig_acc={avg_oacc:.4f}, aug_acc={avg_aacc:.4f}, CGR={cgr:.4f}\"\n        )\n    # Final predictions using CNN\n    _, _, preds_o, gts_o = evaluate(models[\"CNN\"], orig_test_loader)\n    _, _, preds_a, _ = evaluate(models[\"CNN\"], aug_test_loader)\n    ed[\"predictions\"] = preds_o\n    ed[\"ground_truth\"] = gts_o\n    ed[\"predictions_aug\"] = preds_a\n\n# Save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\n\n# Setup working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Data transforms and loaders\nnormalize = transforms.Normalize((0.1307,), (0.3081,))\ntrain_transform = transforms.Compose([transforms.ToTensor(), normalize])\ntest_transform = train_transform\naug_transform = transforms.Compose(\n    [transforms.RandomRotation(30), transforms.ToTensor(), normalize]\n)\n\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=train_transform\n)\norig_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=test_transform\n)\naug_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=aug_transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\norig_test_loader = DataLoader(orig_test_dataset, batch_size=1000, shuffle=False)\naug_test_loader = DataLoader(aug_test_dataset, batch_size=1000, shuffle=False)\n\n# Training and evaluation helpers\ncriterion = nn.CrossEntropyLoss()\n\n\ndef train_one_epoch(model, optimizer):\n    model.train()\n    total = 0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total += loss.item() * x.size(0)\n    return total / len(train_loader.dataset)\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, correct = 0, 0\n    preds, truths = [], []\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            total_loss += loss.item() * x.size(0)\n            pred = out.argmax(1)\n            correct += pred.eq(y).sum().item()\n            preds.append(pred.cpu().numpy())\n            truths.append(y.cpu().numpy())\n    return (\n        total_loss / len(loader.dataset),\n        correct / len(loader.dataset),\n        np.concatenate(preds),\n        np.concatenate(truths),\n    )\n\n\n# Model definitions with dropout\nclass MLP(nn.Module):\n    def __init__(self, dropout_rate):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(28 * 28, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(128, 10),\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self, dropout_rate):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 16, 3, 1), nn.ReLU(), nn.Dropout(dropout_rate), nn.MaxPool2d(2)\n        )\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(16 * 13 * 13, 64),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(64, 10),\n        )\n\n    def forward(self, x):\n        return self.fc(self.conv(x))\n\n\n# Hyperparameter sweep over dropout_rate\ndropout_rates = np.linspace(0.0, 0.5, 6)\nn_epochs = 5\n\n# Initialize experiment_data structure\nexperiment_data = {\"dropout\": {}}\nfor name in [\"MLP\", \"CNN\"]:\n    experiment_data[\"dropout\"][name] = {\n        \"dropout_rates\": [],\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"orig_acc\": [], \"aug_acc\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"predictions_aug\": [],\n        \"ground_truth_aug\": [],\n    }\nexperiment_data[\"dropout\"][\"CGR\"] = []\n\n# Loop over dropout values\nfor dr in dropout_rates:\n    orig_accs, aug_accs = [], []\n    for name in [\"MLP\", \"CNN\"]:\n        # instantiate\n        model = (MLP if name == \"MLP\" else CNN)(dr).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n        # train\n        for _ in range(n_epochs):\n            train_one_epoch(model, optimizer)\n        # eval\n        train_loss, _, _, _ = evaluate(model, train_loader)\n        val_loss, orig_acc, preds, gts = evaluate(model, orig_test_loader)\n        _, aug_acc, preds_aug, gts_aug = evaluate(model, aug_test_loader)\n        # store\n        d = experiment_data[\"dropout\"][name]\n        d[\"dropout_rates\"].append(dr)\n        d[\"losses\"][\"train\"].append(train_loss)\n        d[\"losses\"][\"val\"].append(val_loss)\n        d[\"metrics\"][\"orig_acc\"].append(orig_acc)\n        d[\"metrics\"][\"aug_acc\"].append(aug_acc)\n        d[\"predictions\"].append(preds)\n        d[\"ground_truth\"].append(gts)\n        d[\"predictions_aug\"].append(preds_aug)\n        d[\"ground_truth_aug\"].append(gts_aug)\n        orig_accs.append(orig_acc)\n        aug_accs.append(aug_acc)\n    # compute CGR across models\n    std_orig = np.std(orig_accs)\n    std_aug = np.std(aug_accs)\n    cgr = (std_aug - std_orig) / (std_orig + 1e-8)\n    experiment_data[\"dropout\"][\"CGR\"].append(cgr)\n    print(f\"Dropout {dr:.2f}: CGR = {cgr:.4f}\")\n\n# Save all data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# data\nnormalize = transforms.Normalize((0.1307,), (0.3081,))\ntrain_transform = transforms.Compose([transforms.ToTensor(), normalize])\ntest_transform = train_transform\naug_transform = transforms.Compose(\n    [transforms.RandomRotation(30), transforms.ToTensor(), normalize]\n)\n\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=train_transform\n)\norig_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=test_transform\n)\naug_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=aug_transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\norig_test_loader = DataLoader(orig_test_dataset, batch_size=1000, shuffle=False)\naug_test_loader = DataLoader(aug_test_dataset, batch_size=1000, shuffle=False)\n\n\n# model\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(nn.Conv2d(1, 16, 3, 1), nn.ReLU(), nn.MaxPool2d(2))\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(16 * 13 * 13, 64), nn.ReLU(), nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(self.conv(x))\n\n\n# smoothed cross\u2010entropy\ndef smooth_ce(logits, target, epsilon):\n    logp = F.log_softmax(logits, dim=1)\n    n = logits.size(1)\n    with torch.no_grad():\n        t = torch.zeros_like(logp).scatter_(1, target.unsqueeze(1), 1)\n        t = t * (1 - epsilon) + (1 - t) * (epsilon / (n - 1))\n    loss = -(t * logp).sum(dim=1)\n    return loss.mean()\n\n\ndef train_one_epoch(model, optimizer, epsilon):\n    model.train()\n    total = 0.0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = smooth_ce(out, y, epsilon)\n        loss.backward()\n        optimizer.step()\n        total += loss.item() * x.size(0)\n    return total / len(train_loader.dataset)\n\n\ndef evaluate(model, loader, epsilon):\n    model.eval()\n    total_loss = 0.0\n    correct = 0\n    preds, trues = [], []\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            loss = smooth_ce(out, y, epsilon)\n            total_loss += loss.item() * x.size(0)\n            p = out.argmax(1)\n            correct += p.eq(y).sum().item()\n            preds.append(p.cpu().numpy())\n            trues.append(y.cpu().numpy())\n    preds = np.concatenate(preds)\n    trues = np.concatenate(trues)\n    return total_loss / len(loader.dataset), correct / len(loader.dataset), preds, trues\n\n\n# hyperparam tuning\nepsilons = [0.0, 0.05, 0.1, 0.2]\nn_epochs = 5\nexperiment_data = {\"label_smoothing\": {}}\n\nfor eps in epsilons:\n    key = f\"eps_{eps}\"\n    experiment_data[\"label_smoothing\"][key] = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"orig_acc\": [], \"aug_acc\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    model = CNN().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    for epoch in range(1, n_epochs + 1):\n        tr_loss = train_one_epoch(model, optimizer, eps)\n        vl_loss, orig_acc, _, _ = evaluate(model, orig_test_loader, eps)\n        _, aug_acc, _, _ = evaluate(model, aug_test_loader, eps)\n        experiment_data[\"label_smoothing\"][key][\"losses\"][\"train\"].append(tr_loss)\n        experiment_data[\"label_smoothing\"][key][\"losses\"][\"val\"].append(vl_loss)\n        experiment_data[\"label_smoothing\"][key][\"metrics\"][\"orig_acc\"].append(orig_acc)\n        experiment_data[\"label_smoothing\"][key][\"metrics\"][\"aug_acc\"].append(aug_acc)\n        print(\n            f\"[\u03b5={eps}] Epoch {epoch}/{n_epochs} - tr_loss:{tr_loss:.4f}, val_loss:{vl_loss:.4f}, orig_acc:{orig_acc:.4f}, aug_acc:{aug_acc:.4f}\"\n        )\n    # final preds & gts on original test\n    _, _, pr, gt = evaluate(model, orig_test_loader, eps)\n    experiment_data[\"label_smoothing\"][key][\"predictions\"] = pr\n    experiment_data[\"label_smoothing\"][key][\"ground_truth\"] = gt\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\nfrom datasets import load_dataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Transforms\nnormalize = transforms.Normalize((0.1307,), (0.3081,))\ntrain_transform = transforms.Compose([transforms.ToTensor(), normalize])\ntest_transform = transforms.Compose([transforms.ToTensor(), normalize])\naug_transform = transforms.Compose(\n    [transforms.RandomRotation(30), transforms.ToTensor(), normalize]\n)\n\n# MNIST datasets\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=train_transform\n)\norig_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=test_transform\n)\naug_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=aug_transform\n)\n\n# HuggingFace datasets: Fashion-MNIST and KMNIST\nfashion_ds = load_dataset(\"fashion_mnist\", split=\"test\")\nkmnist_ds = load_dataset(\"kmnist\", split=\"test\")\n\n\ndef hf_collate(batch):\n    imgs = torch.stack([test_transform(item[\"image\"]) for item in batch])\n    labels = torch.tensor([item[\"label\"] for item in batch], dtype=torch.long)\n    return imgs, labels\n\n\n# Model definitions\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(nn.Conv2d(1, 16, 3, 1), nn.ReLU(), nn.MaxPool2d(2))\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(16 * 13 * 13, 64), nn.ReLU(), nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(self.conv(x))\n\n\n# Training and evaluation\ncriterion = nn.CrossEntropyLoss()\n\n\ndef train_one_epoch(model, optimizer, loader):\n    model.train()\n    total_loss = 0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n    return total_loss / len(loader.dataset)\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, correct = 0, 0\n    all_preds, all_true = [], []\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            total_loss += loss.item() * x.size(0)\n            pred = out.argmax(dim=1)\n            correct += pred.eq(y).sum().item()\n            all_preds.append(pred.cpu().numpy())\n            all_true.append(y.cpu().numpy())\n    all_preds = np.concatenate(all_preds)\n    all_true = np.concatenate(all_true)\n    return (\n        total_loss / len(loader.dataset),\n        correct / len(loader.dataset),\n        all_preds,\n        all_true,\n    )\n\n\n# Hyperparameter grid\nlearning_rates = [1e-3, 5e-4]\nbatch_sizes = [64, 128]\nn_epochs = 10\n\nexperiment_data = {\"hyperparams\": {}}\n\nfor lr in learning_rates:\n    for bs in batch_sizes:\n        key = f\"lr{lr}_bs{bs}\"\n        experiment_data[\"hyperparams\"][key] = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\n                \"orig_acc\": [],\n                \"aug_acc\": [],\n                \"fashion_acc\": [],\n                \"kmnist_acc\": [],\n            },\n            \"BDP\": {\"orig\": [], \"aug\": [], \"fashion\": [], \"kmnist\": []},\n            \"predictions\": {},\n            \"ground_truth\": {},\n        }\n        # DataLoaders per config\n        train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n        orig_loader = DataLoader(orig_test_dataset, batch_size=bs, shuffle=False)\n        aug_loader = DataLoader(aug_test_dataset, batch_size=bs, shuffle=False)\n        fashion_loader = DataLoader(\n            fashion_ds, batch_size=bs, shuffle=False, collate_fn=hf_collate\n        )\n        kmnist_loader = DataLoader(\n            kmnist_ds, batch_size=bs, shuffle=False, collate_fn=hf_collate\n        )\n        # Models & optimizers\n        models = {\"MLP\": MLP().to(device), \"CNN\": CNN().to(device)}\n        optimizers = {\n            name: optim.Adam(m.parameters(), lr=lr, weight_decay=1e-4)\n            for name, m in models.items()\n        }\n        # Training loop\n        for epoch in range(1, n_epochs + 1):\n            train_losses = []\n            for name in models:\n                tl = train_one_epoch(models[name], optimizers[name], train_loader)\n                train_losses.append(tl)\n            avg_tl = float(np.mean(train_losses))\n            # Evaluate on all datasets\n            orig_res = [evaluate(models[n], orig_loader) for n in models]\n            aug_res = [evaluate(models[n], aug_loader) for n in models]\n            fash_res = [evaluate(models[n], fashion_loader) for n in models]\n            kmn_res = [evaluate(models[n], kmnist_loader) for n in models]\n            # Gather stats\n            avg_vl = float(np.mean([r[0] for r in orig_res]))\n            avg_orig = float(np.mean([r[1] for r in orig_res]))\n            avg_aug = float(np.mean([r[1] for r in aug_res]))\n            avg_fash = float(np.mean([r[1] for r in fash_res]))\n            avg_kmn = float(np.mean([r[1] for r in kmn_res]))\n            std_orig = float(np.std([r[1] for r in orig_res]))\n            std_aug = float(np.std([r[1] for r in aug_res]))\n            std_fash = float(np.std([r[1] for r in fash_res]))\n            std_kmn = float(np.std([r[1] for r in kmn_res]))\n            ed = experiment_data[\"hyperparams\"][key]\n            ed[\"losses\"][\"train\"].append(avg_tl)\n            ed[\"losses\"][\"val\"].append(avg_vl)\n            ed[\"metrics\"][\"orig_acc\"].append(avg_orig)\n            ed[\"metrics\"][\"aug_acc\"].append(avg_aug)\n            ed[\"metrics\"][\"fashion_acc\"].append(avg_fash)\n            ed[\"metrics\"][\"kmnist_acc\"].append(avg_kmn)\n            ed[\"BDP\"][\"orig\"].append(std_orig)\n            ed[\"BDP\"][\"aug\"].append(std_aug)\n            ed[\"BDP\"][\"fashion\"].append(std_fash)\n            ed[\"BDP\"][\"kmnist\"].append(std_kmn)\n            print(\n                f\"Params {key} Epoch {epoch}: train_loss={avg_tl:.4f}, \"\n                f\"validation_loss = {avg_vl:.4f}, orig_acc={avg_orig:.4f}, \"\n                f\"aug_acc={avg_aug:.4f}, fashion_acc={avg_fash:.4f}, \"\n                f\"kmnist_acc={avg_kmn:.4f}, BDP_orig={std_orig:.4f}\"\n            )\n        # Final CNN predictions\n        cnn = models[\"CNN\"]\n        for name, loader in [\n            (\"orig\", orig_loader),\n            (\"aug\", aug_loader),\n            (\"fashion\", fashion_loader),\n            (\"kmnist\", kmnist_loader),\n        ]:\n            _, _, preds, truths = evaluate(cnn, loader)\n            ed[\"predictions\"][name] = preds\n            ed[\"ground_truth\"][name] = truths\n\n# Save all collected data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\nfrom datasets import load_dataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Transforms\nnormalize = transforms.Normalize((0.1307,), (0.3081,))\ntrain_transform = transforms.Compose([transforms.ToTensor(), normalize])\ntest_transform = train_transform\naug_transform = transforms.Compose(\n    [transforms.RandomRotation(30), transforms.ToTensor(), normalize]\n)\n\n# Torchvision MNIST\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=train_transform\n)\norig_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=test_transform\n)\naug_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=aug_transform\n)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\norig_test_loader = DataLoader(orig_test_dataset, batch_size=1000, shuffle=False)\naug_test_loader = DataLoader(aug_test_dataset, batch_size=1000, shuffle=False)\n\n\n# HuggingFace datasets: Fashion-MNIST and Kuzushiji-MNIST\ndef hf_transform(ex):\n    img = ex[\"image\"]\n    x = transforms.ToTensor()(img)\n    x = normalize(x)\n    return {\"pixel_values\": x, \"label\": ex[\"label\"]}\n\n\nfashion_ds = load_dataset(\"fashion_mnist\", split=\"test\").map(hf_transform)\nfashion_ds.set_format(type=\"torch\", columns=[\"pixel_values\", \"label\"])\nkmnist_ds = load_dataset(\"kmnist\", split=\"test\").map(hf_transform)\nkmnist_ds.set_format(type=\"torch\", columns=[\"pixel_values\", \"label\"])\n\nfashion_loader = DataLoader(fashion_ds, batch_size=1000, shuffle=False)\nkmnist_loader = DataLoader(kmnist_ds, batch_size=1000, shuffle=False)\n\n\n# Model definitions\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(nn.Conv2d(1, 16, 3, 1), nn.ReLU(), nn.MaxPool2d(2))\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(16 * 13 * 13, 64), nn.ReLU(), nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(self.conv(x))\n\n\n# Training and evaluation\ncriterion = nn.CrossEntropyLoss()\n\n\ndef train_one_epoch(model, optimizer):\n    model.train()\n    total_loss = 0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n    return total_loss / len(train_loader.dataset)\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, correct = 0, 0\n    all_preds, all_true = [], []\n    with torch.no_grad():\n        for batch in loader:\n            if isinstance(batch, dict):\n                x = batch[\"pixel_values\"].to(device)\n                y = batch[\"label\"].to(device)\n            else:\n                x, y = batch\n                x = x.to(device)\n                y = y.to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            total_loss += loss.item() * x.size(0)\n            pred = out.argmax(dim=1)\n            correct += pred.eq(y).sum().item()\n            all_preds.append(pred.cpu().numpy())\n            all_true.append(y.cpu().numpy())\n    all_preds = np.concatenate(all_preds)\n    all_true = np.concatenate(all_true)\n    return (\n        total_loss / len(loader.dataset),\n        correct / len(loader.dataset),\n        all_preds,\n        all_true,\n    )\n\n\n# Hyperparameter tuning\nweight_decay_values = [0, 1e-5, 1e-4, 1e-3]\nn_epochs = 5\nexperiment_data = {\"weight_decay\": {}}\n\nfor wd in weight_decay_values:\n    key = str(wd)\n    experiment_data[\"weight_decay\"][key] = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"orig_acc\": [], \"aug_acc\": [], \"fashion_acc\": [], \"kmnist_acc\": []},\n        \"BDP\": {\"orig\": [], \"aug\": [], \"fashion\": [], \"kmnist\": []},\n        \"CGR\": [],\n        \"predictions\": {},\n        \"ground_truth\": {},\n    }\n    # Init models & optimizers\n    models = {\"MLP\": MLP().to(device), \"CNN\": CNN().to(device)}\n    optimizers = {\n        name: optim.Adam(m.parameters(), lr=1e-3, weight_decay=wd)\n        for name, m in models.items()\n    }\n    # Epoch loop\n    for epoch in range(1, n_epochs + 1):\n        train_losses = []\n        for name, model in models.items():\n            train_losses.append(train_one_epoch(model, optimizers[name]))\n        val_losses, orig_accs, aug_accs, f_accs, k_accs = [], [], [], [], []\n        for name, model in models.items():\n            vl, oa, _, _ = evaluate(model, orig_test_loader)\n            _, aa, _, _ = evaluate(model, aug_test_loader)\n            _, fa, _, _ = evaluate(model, fashion_loader)\n            _, ka, _, _ = evaluate(model, kmnist_loader)\n            val_losses.append(vl)\n            orig_accs.append(oa)\n            aug_accs.append(aa)\n            f_accs.append(fa)\n            k_accs.append(ka)\n        avg_tl = float(np.mean(train_losses))\n        avg_vl = float(np.mean(val_losses))\n        avg_oa, avg_aa = float(np.mean(orig_accs)), float(np.mean(aug_accs))\n        avg_fa, avg_ka = float(np.mean(f_accs)), float(np.mean(k_accs))\n        std_oa, std_aa = float(np.std(orig_accs)), float(np.std(aug_accs))\n        std_fa, std_ka = float(np.std(f_accs)), float(np.std(k_accs))\n        cgr = float((std_aa - std_oa) / (std_oa + 1e-8))\n\n        ed = experiment_data[\"weight_decay\"][key]\n        ed[\"losses\"][\"train\"].append(avg_tl)\n        ed[\"losses\"][\"val\"].append(avg_vl)\n        ed[\"metrics\"][\"orig_acc\"].append(avg_oa)\n        ed[\"metrics\"][\"aug_acc\"].append(avg_aa)\n        ed[\"metrics\"][\"fashion_acc\"].append(avg_fa)\n        ed[\"metrics\"][\"kmnist_acc\"].append(avg_ka)\n        ed[\"BDP\"][\"orig\"].append(std_oa)\n        ed[\"BDP\"][\"aug\"].append(std_aa)\n        ed[\"BDP\"][\"fashion\"].append(std_fa)\n        ed[\"BDP\"][\"kmnist\"].append(std_ka)\n        ed[\"CGR\"].append(cgr)\n\n        print(\n            f\"WD {wd} Epoch {epoch}: train_loss={avg_tl:.4f}, validation_loss={avg_vl:.4f}, \"\n            f\"orig_acc={avg_oa:.4f}, aug_acc={avg_aa:.4f}, fashion_acc={avg_fa:.4f}, kmnist_acc={avg_ka:.4f}, \"\n            f\"BDP_orig={std_oa:.4f}, BDP_aug={std_aa:.4f}, BDP_fashion={std_fa:.4f}, BDP_kmnist={std_ka:.4f}, CGR={cgr:.4f}\"\n        )\n    # Final CNN predictions & ground\u2010truths\n    preds, gts = {}, {}\n    for name, loader in [\n        (\"orig\", orig_test_loader),\n        (\"aug\", aug_test_loader),\n        (\"fashion\", fashion_loader),\n        (\"kmnist\", kmnist_loader),\n    ]:\n        _, _, p, t = evaluate(models[\"CNN\"], loader)\n        preds[name] = p\n        gts[name] = t\n    ed[\"predictions\"] = preds\n    ed[\"ground_truth\"] = gts\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms\nimport numpy as np\nfrom datasets import load_dataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Data transforms\nnormalize = transforms.Normalize((0.1307,), (0.3081,))\ntrain_transform = transforms.Compose([transforms.ToTensor(), normalize])\ntest_transform = transforms.Compose([transforms.ToTensor(), normalize])\naug_transform = transforms.Compose(\n    [transforms.RandomRotation(30), transforms.ToTensor(), normalize]\n)\n\n\n# HuggingFace dataset wrapper\nclass HFDataset(Dataset):\n    def __init__(self, hf_ds, transform):\n        self.ds = hf_ds\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        item = self.ds[idx]\n        img = item[\"image\"]\n        img = self.transform(img)\n        lbl = torch.tensor(item[\"label\"], dtype=torch.long)\n        return img, lbl\n\n\n# Load HF test sets\nfashion_ds = load_dataset(\"fashion_mnist\", split=\"test\")\nkuz_ds = load_dataset(\"kuzushiji_mnist\", split=\"test\")\nhug1_dataset = HFDataset(fashion_ds, test_transform)\nhug2_dataset = HFDataset(kuz_ds, test_transform)\n\n# TorchVision MNIST datasets\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=train_transform\n)\norig_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=test_transform\n)\naug_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=aug_transform\n)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\norig_test_loader = DataLoader(orig_test_dataset, batch_size=1000, shuffle=False)\naug_test_loader = DataLoader(aug_test_dataset, batch_size=1000, shuffle=False)\nhug1_loader = DataLoader(hug1_dataset, batch_size=1000, shuffle=False)\nhug2_loader = DataLoader(hug2_dataset, batch_size=1000, shuffle=False)\n\n\n# Model definitions\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(nn.Conv2d(1, 16, 3, 1), nn.ReLU(), nn.MaxPool2d(2))\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(16 * 13 * 13, 64), nn.ReLU(), nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(self.conv(x))\n\n\n# Training/evaluation\ncriterion = nn.CrossEntropyLoss()\n\n\ndef train_one_epoch(model, optimizer):\n    model.train()\n    total_loss = 0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n    return total_loss / len(train_loader.dataset)\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, correct = 0, 0\n    all_preds, all_true = [], []\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            total_loss += loss.item() * x.size(0)\n            pred = out.argmax(dim=1)\n            correct += pred.eq(y).sum().item()\n            all_preds.append(pred.cpu().numpy())\n            all_true.append(y.cpu().numpy())\n    all_preds = np.concatenate(all_preds)\n    all_true = np.concatenate(all_true)\n    acc = correct / len(loader.dataset)\n    return total_loss / len(loader.dataset), acc, all_preds, all_true\n\n\n# Hyperparameter grid\nweight_decay_values = [0, 1e-5, 1e-4, 1e-3]\nn_epochs = 5\nexperiment_data = {\"weight_decay\": {}}\n\nfor wd in weight_decay_values:\n    key = str(wd)\n    experiment_data[\"weight_decay\"][key] = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\n            \"orig_acc\": [],\n            \"aug_acc\": [],\n            \"hug1_acc\": [],\n            \"hug2_acc\": [],\n            \"orig_bdp\": [],\n            \"aug_bdp\": [],\n            \"hug1_bdp\": [],\n            \"hug2_bdp\": [],\n        },\n        \"CGR\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"predictions_aug\": [],\n        \"predictions_hug1\": [],\n        \"ground_truth_hug1\": [],\n        \"predictions_hug2\": [],\n        \"ground_truth_hug2\": [],\n    }\n    # initialize\n    models = {\"MLP\": MLP().to(device), \"CNN\": CNN().to(device)}\n    optimizers = {\n        name: optim.Adam(m.parameters(), lr=1e-3, weight_decay=wd)\n        for name, m in models.items()\n    }\n    for epoch in range(1, n_epochs + 1):\n        # train\n        train_losses = [train_one_epoch(models[n], optimizers[n]) for n in models]\n        # eval\n        orig_accs, aug_accs, h1_accs, h2_accs, val_losses = [], [], [], [], []\n        for name, m in models.items():\n            vl, oacc, _, _ = evaluate(m, orig_test_loader)\n            _, aacc, _, _ = evaluate(m, aug_test_loader)\n            _, h1acc, _, _ = evaluate(m, hug1_loader)\n            _, h2acc, _, _ = evaluate(m, hug2_loader)\n            val_losses.append(vl)\n            orig_accs.append(oacc)\n            aug_accs.append(aacc)\n            h1_accs.append(h1acc)\n            h2_accs.append(h2acc)\n        avg_tl, avg_vl = float(np.mean(train_losses)), float(np.mean(val_losses))\n        avg_o, avg_a, avg_h1, avg_h2 = (\n            float(np.mean(orig_accs)),\n            float(np.mean(aug_accs)),\n            float(np.mean(h1_accs)),\n            float(np.mean(h2_accs)),\n        )\n        std_o, std_a, std_h1, std_h2 = (\n            float(np.std(orig_accs)),\n            float(np.std(aug_accs)),\n            float(np.std(h1_accs)),\n            float(np.std(h2_accs)),\n        )\n        cgr = (std_a - std_o) / (std_o + 1e-8)\n        bdp_o, bdp_a, bdp_h1, bdp_h2 = std_o, std_a, std_h1, std_h2\n        ed = experiment_data[\"weight_decay\"][key]\n        ed[\"losses\"][\"train\"].append(avg_tl)\n        ed[\"losses\"][\"val\"].append(avg_vl)\n        ed[\"metrics\"][\"orig_acc\"].append(avg_o)\n        ed[\"metrics\"][\"aug_acc\"].append(avg_a)\n        ed[\"metrics\"][\"hug1_acc\"].append(avg_h1)\n        ed[\"metrics\"][\"hug2_acc\"].append(avg_h2)\n        ed[\"metrics\"][\"orig_bdp\"].append(bdp_o)\n        ed[\"metrics\"][\"aug_bdp\"].append(bdp_a)\n        ed[\"metrics\"][\"hug1_bdp\"].append(bdp_h1)\n        ed[\"metrics\"][\"hug2_bdp\"].append(bdp_h2)\n        ed[\"CGR\"].append(float(cgr))\n        print(\n            f\"WD {wd} Epoch {epoch}: validation_loss = {avg_vl:.4f}, orig_acc = {avg_o:.4f}, aug_acc = {avg_a:.4f}, \"\n            f\"hug1_acc = {avg_h1:.4f}, hug2_acc = {avg_h2:.4f}, \"\n            f\"orig_bdp = {bdp_o:.4f}, aug_bdp = {bdp_a:.4f}, hug1_bdp = {bdp_h1:.4f}, hug2_bdp = {bdp_h2:.4f}, CGR = {cgr:.4f}\"\n        )\n    # final CNN preds\n    _, _, preds_o, gts_o = evaluate(models[\"CNN\"], orig_test_loader)\n    _, _, preds_a, _ = evaluate(models[\"CNN\"], aug_test_loader)\n    _, _, preds_h1, gts_h1 = evaluate(models[\"CNN\"], hug1_loader)\n    _, _, preds_h2, gts_h2 = evaluate(models[\"CNN\"], hug2_loader)\n    ed = experiment_data[\"weight_decay\"][key]\n    ed[\"predictions\"] = preds_o\n    ed[\"ground_truth\"] = gts_o\n    ed[\"predictions_aug\"] = preds_a\n    ed[\"predictions_hug1\"] = preds_h1\n    ed[\"ground_truth_hug1\"] = gts_h1\n    ed[\"predictions_hug2\"] = preds_h2\n    ed[\"ground_truth_hug2\"] = gts_h2\n\n# Save all metrics and data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\nfrom datasets import load_dataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Transforms\nnormalize = transforms.Normalize((0.1307,), (0.3081,))\ntrain_transform = transforms.Compose([transforms.ToTensor(), normalize])\ntest_transform = train_transform\naug_transform = transforms.Compose(\n    [transforms.RandomRotation(30), transforms.ToTensor(), normalize]\n)\nhf_transform = transforms.Compose(\n    [\n        transforms.Resize((28, 28)),\n        transforms.Grayscale(num_output_channels=1),\n        transforms.ToTensor(),\n        normalize,\n    ]\n)\n\n# MNIST loaders\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=train_transform\n)\norig_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=test_transform\n)\naug_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=aug_transform\n)\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\norig_test_loader = DataLoader(orig_test_dataset, batch_size=1000, shuffle=False)\naug_test_loader = DataLoader(aug_test_dataset, batch_size=1000, shuffle=False)\n\n\n# Helper to map HF images\ndef map_hf(example):\n    return {\n        \"pixel_values\": hf_transform(example[\"image\"]),\n        \"label\": example[\"label\"],\n    }\n\n\n# Fashion\u2010MNIST\nfashion_ds = load_dataset(\"fashion_mnist\", split=\"test\").map(map_hf)\nfashion_ds.set_format(type=\"torch\", columns=[\"pixel_values\", \"label\"])\nfashion_loader = DataLoader(fashion_ds, batch_size=1000, shuffle=False)\n\n# Kuzushiji\u2010MNIST (correct name)\nkuzushiji_ds = load_dataset(\"kuzushiji_mnist\", split=\"test\").map(map_hf)\nkuzushiji_ds.set_format(type=\"torch\", columns=[\"pixel_values\", \"label\"])\nkuzushiji_loader = DataLoader(kuzushiji_ds, batch_size=1000, shuffle=False)\n\n# USPS\nusps_ds = load_dataset(\"usps\", split=\"test\").map(map_hf)\nusps_ds.set_format(type=\"torch\", columns=[\"pixel_values\", \"label\"])\nusps_loader = DataLoader(usps_ds, batch_size=1000, shuffle=False)\n\n# SVHN\u2010CROPPED\nsvhn_ds = load_dataset(\"svhn_cropped\", split=\"test\").map(map_hf)\nsvhn_ds.set_format(type=\"torch\", columns=[\"pixel_values\", \"label\"])\nsvhn_loader = DataLoader(svhn_ds, batch_size=1000, shuffle=False)\n\n\n# Model definitions\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(28 * 28, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10),\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 16, 3, 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n        )\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(16 * 13 * 13, 64),\n            nn.ReLU(),\n            nn.Linear(64, 10),\n        )\n\n    def forward(self, x):\n        return self.fc(self.conv(x))\n\n\n# Training & evaluation\ncriterion = nn.CrossEntropyLoss()\n\n\ndef train_one_epoch(model, optimizer):\n    model.train()\n    total_loss = 0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n    return total_loss / len(train_loader.dataset)\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    all_preds, all_true = [], []\n    with torch.no_grad():\n        for batch in loader:\n            x = batch[\"pixel_values\"].to(device)\n            y = batch[\"label\"].to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            total_loss += loss.item() * x.size(0)\n            pred = out.argmax(dim=1)\n            correct += pred.eq(y).sum().item()\n            all_preds.append(pred.cpu().numpy())\n            all_true.append(y.cpu().numpy())\n    all_preds = np.concatenate(all_preds)\n    all_true = np.concatenate(all_true)\n    return (\n        total_loss / len(loader.dataset),\n        correct / len(loader.dataset),\n        all_preds,\n        all_true,\n    )\n\n\n# Hyperparameters\nlearning_rate = 1e-2\nweight_decay = 1e-4\nn_epochs = 10\n\n# Initialize models & optimizers\nmodels = {\"MLP\": MLP().to(device), \"CNN\": CNN().to(device)}\noptimizers = {\n    name: optim.Adam(m.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    for name, m in models.items()\n}\n\n# Prepare experiment_data container\ndomains = [\"orig\", \"aug\", \"fashion\", \"kuzushiji\", \"usps\", \"svhn\"]\nloaders = {\n    \"orig\": orig_test_loader,\n    \"aug\": aug_test_loader,\n    \"fashion\": fashion_loader,\n    \"kuzushiji\": kuzushiji_loader,\n    \"usps\": usps_loader,\n    \"svhn\": svhn_loader,\n}\nexperiment_data = {\n    d: {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"acc\": []},\n        \"BDP\": [],\n        \"predictions\": None,\n        \"ground_truth\": None,\n    }\n    for d in domains\n}\nexperiment_data[\"CGR\"] = []\n\n# Training loop\nfor epoch in range(1, n_epochs + 1):\n    # train\n    train_losses = [\n        train_one_epoch(model, optimizers[name]) for name, model in models.items()\n    ]\n    avg_train = float(np.mean(train_losses))\n    # eval per domain\n    losses_per_domain = {d: [] for d in domains}\n    accs_per_domain = {d: [] for d in domains}\n    for name, model in models.items():\n        for d, loader in loaders.items():\n            vl, acc, _, _ = evaluate(model, loader)\n            losses_per_domain[d].append(vl)\n            accs_per_domain[d].append(acc)\n    avg_val_orig = float(np.mean(losses_per_domain[\"orig\"]))\n    print(f\"Epoch {epoch}: validation_loss = {avg_val_orig:.4f}\")\n    # record\n    for d in domains:\n        avg_val = float(np.mean(losses_per_domain[d]))\n        avg_acc = float(np.mean(accs_per_domain[d]))\n        bdp = float(np.std(accs_per_domain[d]))\n        experiment_data[d][\"losses\"][\"train\"].append(avg_train)\n        experiment_data[d][\"losses\"][\"val\"].append(avg_val)\n        experiment_data[d][\"metrics\"][\"acc\"].append(avg_acc)\n        experiment_data[d][\"BDP\"].append(bdp)\n    cgr = (experiment_data[\"aug\"][\"BDP\"][-1] - experiment_data[\"orig\"][\"BDP\"][-1]) / (\n        experiment_data[\"orig\"][\"BDP\"][-1] + 1e-8\n    )\n    experiment_data[\"CGR\"].append(cgr)\n\n# final CNN preds & GT\nfor d, loader in loaders.items():\n    _, _, preds, gts = evaluate(models[\"CNN\"], loader)\n    experiment_data[d][\"predictions\"] = preds\n    experiment_data[d][\"ground_truth\"] = gts\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms\nimport numpy as np\nfrom datasets import load_dataset\n\n# Setup working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Transforms\nnormalize = transforms.Normalize((0.1307,), (0.3081,))\ntrain_transform = transforms.Compose([transforms.ToTensor(), normalize])\ntest_transform = train_transform\naug_transform = transforms.Compose(\n    [transforms.RandomRotation(30), transforms.ToTensor(), normalize]\n)\n\n# Load MNIST\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=train_transform\n)\norig_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=test_transform\n)\naug_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=aug_transform\n)\n\n# Load HuggingFace datasets\nhf_fmnist = load_dataset(\"fashion_mnist\")\nhf_kmnist = load_dataset(\"kmnist\", \"balanced\")\n\n\nclass HFDataset(Dataset):\n    def __init__(self, hf_split, transform):\n        self.ds = hf_split\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        item = self.ds[idx]\n        img = item[\"image\"]\n        label = item[\"label\"]\n        return self.transform(img), label\n\n\nfashion_test_dataset = HFDataset(hf_fmnist[\"test\"], test_transform)\nkmnist_test_dataset = HFDataset(hf_kmnist[\"test\"], test_transform)\n\n# Hyperparameters\nlearning_rate = 5e-4\nbatch_size = 128\nn_epochs = 8\ndropout_rates = np.linspace(0.0, 0.5, 6)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\norig_test_loader = DataLoader(orig_test_dataset, batch_size=1000, shuffle=False)\naug_test_loader = DataLoader(aug_test_dataset, batch_size=1000, shuffle=False)\nfmnist_loader = DataLoader(fashion_test_dataset, batch_size=1000, shuffle=False)\nkmnist_loader = DataLoader(kmnist_test_dataset, batch_size=1000, shuffle=False)\n\n# Training and evaluation helpers\ncriterion = nn.CrossEntropyLoss()\n\n\ndef train_one_epoch(model, optimizer):\n    model.train()\n    total_loss = 0.0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n    return total_loss / len(train_loader.dataset)\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss = 0.0\n    correct = 0\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            total_loss += loss.item() * x.size(0)\n            correct += out.argmax(1).eq(y).sum().item()\n    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n\n\n# Model definitions\nclass MLP(nn.Module):\n    def __init__(self, dropout_rate):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(28 * 28, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(128, 10),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self, dropout_rate):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 16, 3, 1), nn.ReLU(), nn.Dropout(dropout_rate), nn.MaxPool2d(2)\n        )\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(16 * 13 * 13, 64),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(64, 10),\n        )\n\n    def forward(self, x):\n        return self.fc(self.conv(x))\n\n\n# Initialize experiment data\nexperiment_data = {\"dropout\": {}}\nfor name in [\"MLP\", \"CNN\"]:\n    experiment_data[\"dropout\"][name] = {\n        \"dropout_rates\": [],\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\n            \"orig_acc\": [],\n            \"aug_acc\": [],\n            \"fashion_acc\": [],\n            \"kmnist_acc\": [],\n            \"BDP\": [],\n        },\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"predictions_aug\": [],\n        \"ground_truth_aug\": [],\n    }\nexperiment_data[\"dropout\"][\"CGR\"] = []\n\n# Run experiments\nfor dr in dropout_rates:\n    orig_accs_final, aug_accs_final = [], []\n    for name in [\"MLP\", \"CNN\"]:\n        model = (MLP if name == \"MLP\" else CNN)(dr).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n        d = experiment_data[\"dropout\"][name]\n        d[\"dropout_rates\"].append(dr)\n        # per-epoch loop\n        for epoch in range(1, n_epochs + 1):\n            train_loss = train_one_epoch(model, optimizer)\n            val_loss, orig_acc = evaluate(model, orig_test_loader)\n            _, aug_acc = evaluate(model, aug_test_loader)\n            _, fashion_acc = evaluate(model, fmnist_loader)\n            _, kmnist_acc = evaluate(model, kmnist_loader)\n            bdp = np.std([orig_acc, aug_acc, fashion_acc, kmnist_acc])\n            print(\n                f\"Dropout {dr:.2f}, Model {name}, Epoch {epoch}: validation_loss = {val_loss:.4f}, BDP = {bdp:.4f}\"\n            )\n            d[\"losses\"][\"train\"].append(train_loss)\n            d[\"losses\"][\"val\"].append(val_loss)\n            d[\"metrics\"][\"orig_acc\"].append(orig_acc)\n            d[\"metrics\"][\"aug_acc\"].append(aug_acc)\n            d[\"metrics\"][\"fashion_acc\"].append(fashion_acc)\n            d[\"metrics\"][\"kmnist_acc\"].append(kmnist_acc)\n            d[\"metrics\"][\"BDP\"].append(bdp)\n        # store final predictions\n        _, _, orig_preds = evaluate(model, orig_test_loader)[:3]\n        _, _, aug_preds = evaluate(model, aug_test_loader)[:3]\n        d[\"predictions\"].append(orig_preds)\n        d[\"ground_truth\"].append(evaluate(model, orig_test_loader)[1])\n        d[\"predictions_aug\"].append(aug_preds)\n        d[\"ground_truth_aug\"].append(evaluate(model, aug_test_loader)[1])\n        orig_accs_final.append(d[\"metrics\"][\"orig_acc\"][-1])\n        aug_accs_final.append(d[\"metrics\"][\"aug_acc\"][-1])\n    # compute CGR\n    std_orig = np.std(orig_accs_final)\n    std_aug = np.std(aug_accs_final)\n    cgr = (std_aug - std_orig) / (std_orig + 1e-8)\n    experiment_data[\"dropout\"][\"CGR\"].append(cgr)\n    print(f\"Dropout {dr:.2f}: CGR = {cgr:.4f}\")\n\n# Save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\n\n# Setup working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Data\nnormalize = transforms.Normalize((0.1307,), (0.3081,))\ntrain_transform = transforms.Compose([transforms.ToTensor(), normalize])\ntest_transform = train_transform\naug_transform = transforms.Compose(\n    [transforms.RandomRotation(30), transforms.ToTensor(), normalize]\n)\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=train_transform\n)\norig_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=test_transform\n)\naug_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=aug_transform\n)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\norig_test_loader = DataLoader(orig_test_dataset, batch_size=1000, shuffle=False)\naug_test_loader = DataLoader(aug_test_dataset, batch_size=1000, shuffle=False)\n\n\n# Models\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(nn.Conv2d(1, 16, 3, 1), nn.ReLU(), nn.MaxPool2d(2))\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(16 * 13 * 13, 64), nn.ReLU(), nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(self.conv(x))\n\n\n# Criterion\ncriterion = nn.CrossEntropyLoss()\n\n\n# Helpers\ndef train_one_epoch(model, optimizer):\n    model.train()\n    total = 0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total += loss.item() * x.size(0)\n    return total / len(train_loader.dataset)\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total, corr = 0, 0\n    preds, trues = [], []\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            total += loss.item() * x.size(0)\n            p = out.argmax(dim=1)\n            corr += p.eq(y).sum().item()\n            preds.append(p.cpu().numpy())\n            trues.append(y.cpu().numpy())\n    return (\n        total / len(loader.dataset),\n        corr / len(loader.dataset),\n        np.concatenate(preds),\n        np.concatenate(trues),\n    )\n\n\n# Hyperparam tuning over beta1\nbetas1 = [0.8, 0.85, 0.95]\nn_epochs = 5\nexperiment_data = {\"adam_beta1\": {}}\n\nfor b1 in betas1:\n    key = f\"beta1_{b1}\"\n    experiment_data[\"adam_beta1\"][key] = {\n        \"MLP\": {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"orig_acc\": [], \"aug_acc\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n        \"CNN\": {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"orig_acc\": [], \"aug_acc\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"predictions_aug\": [],\n            \"ground_truth_aug\": [],\n        },\n        \"CGR\": [],\n    }\n    # init models & optimizers\n    models = {\"MLP\": MLP().to(device), \"CNN\": CNN().to(device)}\n    opts = {\n        n: optim.Adam(m.parameters(), lr=1e-3, betas=(b1, 0.999))\n        for n, m in models.items()\n    }\n    # train epochs\n    for epoch in range(n_epochs):\n        orig_accs, aug_accs = [], []\n        for name, m in models.items():\n            tr_loss = train_one_epoch(m, opts[name])\n            experiment_data[\"adam_beta1\"][key][name][\"losses\"][\"train\"].append(tr_loss)\n        for name, m in models.items():\n            v_loss, o_acc, _, _ = evaluate(m, orig_test_loader)\n            _, a_acc, _, _ = evaluate(m, aug_test_loader)\n            experiment_data[\"adam_beta1\"][key][name][\"losses\"][\"val\"].append(v_loss)\n            experiment_data[\"adam_beta1\"][key][name][\"metrics\"][\"orig_acc\"].append(\n                o_acc\n            )\n            experiment_data[\"adam_beta1\"][key][name][\"metrics\"][\"aug_acc\"].append(a_acc)\n            orig_accs.append(o_acc)\n            aug_accs.append(a_acc)\n        std_o = np.std(orig_accs)\n        std_a = np.std(aug_accs)\n        cgr = (std_a - std_o) / (std_o + 1e-8)\n        experiment_data[\"adam_beta1\"][key][\"CGR\"].append(cgr)\n        print(f\"beta1={b1} Epoch {epoch+1}/{n_epochs} CGR={cgr:.4f}\")\n    # save final CNN predictions\n    _, _, p_o, g_o = evaluate(models[\"CNN\"], orig_test_loader)\n    _, _, p_a, g_a = evaluate(models[\"CNN\"], aug_test_loader)\n    cd = experiment_data[\"adam_beta1\"][key][\"CNN\"]\n    cd[\"predictions\"] = p_o\n    cd[\"ground_truth\"] = g_o\n    cd[\"predictions_aug\"] = p_a\n    cd[\"ground_truth_aug\"] = g_a\n\n# Save all data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Transforms and Datasets\nnormalize = transforms.Normalize((0.1307,), (0.3081,))\ntrain_transform = transforms.Compose([transforms.ToTensor(), normalize])\ntest_transform = train_transform\naug_transform = transforms.Compose(\n    [transforms.RandomRotation(30), transforms.ToTensor(), normalize]\n)\n\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=train_transform\n)\norig_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=test_transform\n)\naug_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=aug_transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\norig_test_loader = DataLoader(orig_test_dataset, batch_size=1000, shuffle=False)\naug_test_loader = DataLoader(aug_test_dataset, batch_size=1000, shuffle=False)\n\n\n# Model definitions\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(nn.Conv2d(1, 16, 3, 1), nn.ReLU(), nn.MaxPool2d(2))\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(16 * 13 * 13, 64), nn.ReLU(), nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(self.conv(x))\n\n\n# Loss & eval functions\ncriterion = nn.CrossEntropyLoss()\n\n\ndef train_one_epoch(model, optimizer):\n    model.train()\n    total_loss = 0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n    return total_loss / len(train_loader.dataset)\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, correct = 0, 0\n    preds, trues = [], []\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            total_loss += loss.item() * x.size(0)\n            pred = out.argmax(dim=1)\n            correct += pred.eq(y).sum().item()\n            preds.append(pred.cpu().numpy())\n            trues.append(y.cpu().numpy())\n    return (\n        total_loss / len(loader.dataset),\n        correct / len(loader.dataset),\n        np.concatenate(preds),\n        np.concatenate(trues),\n    )\n\n\n# Hyperparameter tuning for n_epochs\nn_epochs_list = [5, 10, 15, 20]\nexperiment_data = {\"n_epochs\": {}}\n\nfor n_epochs in n_epochs_list:\n    print(f\"\\n=== Running with n_epochs = {n_epochs} ===\")\n    # Initialize models and optimizers\n    models = {\"MLP\": MLP().to(device), \"CNN\": CNN().to(device)}\n    optimizers = {\n        name: optim.Adam(m.parameters(), lr=1e-3) for name, m in models.items()\n    }\n    # Data structure for this run\n    run_data = {\"models\": {}, \"cgr\": []}\n    for name in models:\n        run_data[\"models\"][name] = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"orig_acc\": [], \"aug_acc\": []},\n            \"predictions\": None,\n            \"ground_truth\": None,\n            \"aug_predictions\": None,\n            \"aug_ground_truth\": None,\n        }\n    # Training loop\n    for epoch in range(1, n_epochs + 1):\n        # train\n        for name in models:\n            tl = train_one_epoch(models[name], optimizers[name])\n            run_data[\"models\"][name][\"losses\"][\"train\"].append(tl)\n        # eval and metrics\n        orig_accs, aug_accs = [], []\n        for name in models:\n            vl, oa, _, _ = evaluate(models[name], orig_test_loader)\n            run_data[\"models\"][name][\"losses\"][\"val\"].append(vl)\n            run_data[\"models\"][name][\"metrics\"][\"orig_acc\"].append(oa)\n            _, aa, _, _ = evaluate(models[name], aug_test_loader)\n            run_data[\"models\"][name][\"metrics\"][\"aug_acc\"].append(aa)\n            orig_accs.append(oa)\n            aug_accs.append(aa)\n            print(\n                f\"{name} Epoch {epoch}: val_loss={vl:.4f}, orig_acc={oa:.4f}, aug_acc={aa:.4f}\"\n            )\n        cgr = (np.std(aug_accs) - np.std(orig_accs)) / (np.std(orig_accs) + 1e-8)\n        run_data[\"cgr\"].append(cgr)\n        print(f\"Epoch {epoch}: CGR={cgr:.4f}\")\n    # Final predictions & ground truth\n    for name in models:\n        _, _, p, gt = evaluate(models[name], orig_test_loader)\n        run_data[\"models\"][name][\"predictions\"] = p\n        run_data[\"models\"][name][\"ground_truth\"] = gt\n        _, _, pa, gta = evaluate(models[name], aug_test_loader)\n        run_data[\"models\"][name][\"aug_predictions\"] = pa\n        run_data[\"models\"][name][\"aug_ground_truth\"] = gta\n    # Store run data\n    experiment_data[\"n_epochs\"][str(n_epochs)] = run_data\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Transforms and Datasets\nnormalize = transforms.Normalize((0.1307,), (0.3081,))\ntrain_transform = transforms.Compose([transforms.ToTensor(), normalize])\ntest_transform = train_transform\naug_transform = transforms.Compose(\n    [transforms.RandomRotation(30), transforms.ToTensor(), normalize]\n)\n\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=train_transform\n)\norig_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=test_transform\n)\naug_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=aug_transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\norig_test_loader = DataLoader(orig_test_dataset, batch_size=1000, shuffle=False)\naug_test_loader = DataLoader(aug_test_dataset, batch_size=1000, shuffle=False)\n\n\n# Model definitions\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(nn.Conv2d(1, 16, 3, 1), nn.ReLU(), nn.MaxPool2d(2))\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(16 * 13 * 13, 64), nn.ReLU(), nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(self.conv(x))\n\n\n# Loss & eval functions\ncriterion = nn.CrossEntropyLoss()\n\n\ndef train_one_epoch(model, optimizer):\n    model.train()\n    total_loss = 0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n    return total_loss / len(train_loader.dataset)\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, correct = 0, 0\n    preds, trues = [], []\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            total_loss += loss.item() * x.size(0)\n            pred = out.argmax(dim=1)\n            correct += pred.eq(y).sum().item()\n            preds.append(pred.cpu().numpy())\n            trues.append(y.cpu().numpy())\n    return (\n        total_loss / len(loader.dataset),\n        correct / len(loader.dataset),\n        np.concatenate(preds),\n        np.concatenate(trues),\n    )\n\n\n# Hyperparameter tuning for n_epochs\nn_epochs_list = [5, 10, 15, 20]\nexperiment_data = {\"n_epochs\": {}}\n\nfor n_epochs in n_epochs_list:\n    print(f\"\\n=== Running with n_epochs = {n_epochs} ===\")\n    # Initialize models and optimizers\n    models = {\"MLP\": MLP().to(device), \"CNN\": CNN().to(device)}\n    optimizers = {\n        name: optim.Adam(m.parameters(), lr=1e-3) for name, m in models.items()\n    }\n    # Data structure for this run\n    run_data = {\"models\": {}, \"cgr\": []}\n    for name in models:\n        run_data[\"models\"][name] = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"orig_acc\": [], \"aug_acc\": []},\n            \"predictions\": None,\n            \"ground_truth\": None,\n            \"aug_predictions\": None,\n            \"aug_ground_truth\": None,\n        }\n    # Training loop\n    for epoch in range(1, n_epochs + 1):\n        # train\n        for name in models:\n            tl = train_one_epoch(models[name], optimizers[name])\n            run_data[\"models\"][name][\"losses\"][\"train\"].append(tl)\n        # eval and metrics\n        orig_accs, aug_accs = [], []\n        for name in models:\n            vl, oa, _, _ = evaluate(models[name], orig_test_loader)\n            run_data[\"models\"][name][\"losses\"][\"val\"].append(vl)\n            run_data[\"models\"][name][\"metrics\"][\"orig_acc\"].append(oa)\n            _, aa, _, _ = evaluate(models[name], aug_test_loader)\n            run_data[\"models\"][name][\"metrics\"][\"aug_acc\"].append(aa)\n            orig_accs.append(oa)\n            aug_accs.append(aa)\n            print(\n                f\"{name} Epoch {epoch}: val_loss={vl:.4f}, orig_acc={oa:.4f}, aug_acc={aa:.4f}\"\n            )\n        cgr = (np.std(aug_accs) - np.std(orig_accs)) / (np.std(orig_accs) + 1e-8)\n        run_data[\"cgr\"].append(cgr)\n        print(f\"Epoch {epoch}: CGR={cgr:.4f}\")\n    # Final predictions & ground truth\n    for name in models:\n        _, _, p, gt = evaluate(models[name], orig_test_loader)\n        run_data[\"models\"][name][\"predictions\"] = p\n        run_data[\"models\"][name][\"ground_truth\"] = gt\n        _, _, pa, gta = evaluate(models[name], aug_test_loader)\n        run_data[\"models\"][name][\"aug_predictions\"] = pa\n        run_data[\"models\"][name][\"aug_ground_truth\"] = gta\n    # Store run data\n    experiment_data[\"n_epochs\"][str(n_epochs)] = run_data\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Transforms and Datasets\nnormalize = transforms.Normalize((0.1307,), (0.3081,))\ntrain_transform = transforms.Compose([transforms.ToTensor(), normalize])\ntest_transform = train_transform\naug_transform = transforms.Compose(\n    [transforms.RandomRotation(30), transforms.ToTensor(), normalize]\n)\n\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=train_transform\n)\norig_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=test_transform\n)\naug_test_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=aug_transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\norig_test_loader = DataLoader(orig_test_dataset, batch_size=1000, shuffle=False)\naug_test_loader = DataLoader(aug_test_dataset, batch_size=1000, shuffle=False)\n\n\n# Model definitions\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(nn.Conv2d(1, 16, 3, 1), nn.ReLU(), nn.MaxPool2d(2))\n        self.fc = nn.Sequential(\n            nn.Flatten(), nn.Linear(16 * 13 * 13, 64), nn.ReLU(), nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        return self.fc(self.conv(x))\n\n\n# Loss & eval functions\ncriterion = nn.CrossEntropyLoss()\n\n\ndef train_one_epoch(model, optimizer):\n    model.train()\n    total_loss = 0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n    return total_loss / len(train_loader.dataset)\n\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, correct = 0, 0\n    preds, trues = [], []\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            total_loss += loss.item() * x.size(0)\n            pred = out.argmax(dim=1)\n            correct += pred.eq(y).sum().item()\n            preds.append(pred.cpu().numpy())\n            trues.append(y.cpu().numpy())\n    return (\n        total_loss / len(loader.dataset),\n        correct / len(loader.dataset),\n        np.concatenate(preds),\n        np.concatenate(trues),\n    )\n\n\n# Hyperparameter tuning for n_epochs\nn_epochs_list = [5, 10, 15, 20]\nexperiment_data = {\"n_epochs\": {}}\n\nfor n_epochs in n_epochs_list:\n    print(f\"\\n=== Running with n_epochs = {n_epochs} ===\")\n    # Initialize models and optimizers\n    models = {\"MLP\": MLP().to(device), \"CNN\": CNN().to(device)}\n    optimizers = {\n        name: optim.Adam(m.parameters(), lr=1e-3) for name, m in models.items()\n    }\n    # Data structure for this run\n    run_data = {\"models\": {}, \"cgr\": []}\n    for name in models:\n        run_data[\"models\"][name] = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"orig_acc\": [], \"aug_acc\": []},\n            \"predictions\": None,\n            \"ground_truth\": None,\n            \"aug_predictions\": None,\n            \"aug_ground_truth\": None,\n        }\n    # Training loop\n    for epoch in range(1, n_epochs + 1):\n        # train\n        for name in models:\n            tl = train_one_epoch(models[name], optimizers[name])\n            run_data[\"models\"][name][\"losses\"][\"train\"].append(tl)\n        # eval and metrics\n        orig_accs, aug_accs = [], []\n        for name in models:\n            vl, oa, _, _ = evaluate(models[name], orig_test_loader)\n            run_data[\"models\"][name][\"losses\"][\"val\"].append(vl)\n            run_data[\"models\"][name][\"metrics\"][\"orig_acc\"].append(oa)\n            _, aa, _, _ = evaluate(models[name], aug_test_loader)\n            run_data[\"models\"][name][\"metrics\"][\"aug_acc\"].append(aa)\n            orig_accs.append(oa)\n            aug_accs.append(aa)\n            print(\n                f\"{name} Epoch {epoch}: val_loss={vl:.4f}, orig_acc={oa:.4f}, aug_acc={aa:.4f}\"\n            )\n        cgr = (np.std(aug_accs) - np.std(orig_accs)) / (np.std(orig_accs) + 1e-8)\n        run_data[\"cgr\"].append(cgr)\n        print(f\"Epoch {epoch}: CGR={cgr:.4f}\")\n    # Final predictions & ground truth\n    for name in models:\n        _, _, p, gt = evaluate(models[name], orig_test_loader)\n        run_data[\"models\"][name][\"predictions\"] = p\n        run_data[\"models\"][name][\"ground_truth\"] = gt\n        _, _, pa, gta = evaluate(models[name], aug_test_loader)\n        run_data[\"models\"][name][\"aug_predictions\"] = pa\n        run_data[\"models\"][name][\"aug_ground_truth\"] = gta\n    # Store run data\n    experiment_data[\"n_epochs\"][str(n_epochs)] = run_data\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Downloading\nhttp://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', '\\n', 'Failed to\ndownload (trying next):\\nHTTP Error 404: Not Found', '\\n', '\\n', 'Downloading\nhttps://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz', '\\n',\n'Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-\nidx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz', '\\n', '\\r  0%|\n| 0.00/9.91M [00:00<?, ?B/s]', '\\r  0%|          | 32.8k/9.91M [00:00<02:16,\n72.5kB/s]', '\\r  1%|          | 65.5k/9.91M [00:00<01:38, 99.9kB/s]', '\\r  1%|1\n| 131k/9.91M [00:00<00:50, 194kB/s]  ', '\\r  2%|2         | 229k/9.91M\n[00:00<00:28, 343kB/s]', '\\r  3%|2         | 295k/9.91M [00:01<00:29, 331kB/s]',\n'\\r  6%|5         | 557k/9.91M [00:01<00:12, 724kB/s]', '\\r  7%|6         |\n655k/9.91M [00:01<00:15, 602kB/s]', '\\r  8%|7         | 754k/9.91M [00:01<00:14,\n637kB/s]', '\\r  9%|8         | 852k/9.91M [00:01<00:13, 685kB/s]', '\\r 10%|9\n| 950k/9.91M [00:01<00:12, 705kB/s]', '\\r 11%|#         | 1.05M/9.91M\n[00:02<00:15, 586kB/s]', '\\r 12%|#1        | 1.15M/9.91M [00:02<00:13,\n647kB/s]', '\\r 13%|#2        | 1.25M/9.91M [00:02<00:12, 677kB/s]', '\\r 14%|#3\n| 1.34M/9.91M [00:02<00:14, 607kB/s]', '\\r 15%|#4        | 1.44M/9.91M\n[00:02<00:13, 622kB/s]', '\\r 16%|#5        | 1.57M/9.91M [00:02<00:11,\n719kB/s]', '\\r 17%|#6        | 1.67M/9.91M [00:03<00:10, 754kB/s]', '\\r 18%|#7\n| 1.77M/9.91M [00:03<00:13, 614kB/s]', '\\r 19%|#8        | 1.87M/9.91M\n[00:03<00:12, 647kB/s]', '\\r 20%|#9        | 1.97M/9.91M [00:03<00:11,\n697kB/s]', '\\r 21%|##        | 2.06M/9.91M [00:03<00:10, 715kB/s]', '\\r 22%|##1\n| 2.16M/9.91M [00:03<00:10, 753kB/s]', '\\r 23%|##2       | 2.26M/9.91M\n[00:03<00:12, 608kB/s]', '\\r 24%|##4       | 2.39M/9.91M [00:04<00:10,\n708kB/s]', '\\r 25%|##5       | 2.49M/9.91M [00:04<00:09, 744kB/s]', '\\r 26%|##6\n| 2.59M/9.91M [00:04<00:12, 609kB/s]', '\\r 27%|##7       | 2.72M/9.91M\n[00:04<00:10, 707kB/s]', '\\r 28%|##8       | 2.82M/9.91M [00:04<00:09,\n742kB/s]', '\\r 29%|##9       | 2.92M/9.91M [00:04<00:09, 747kB/s]', '\\r 30%|###\n| 3.01M/9.91M [00:04<00:08, 777kB/s]', '\\r 31%|###1      | 3.11M/9.91M\n[00:05<00:10, 621kB/s]', '\\r 33%|###2      | 3.24M/9.91M [00:05<00:09,\n720kB/s]', '\\r 34%|###3      | 3.34M/9.91M [00:05<00:08, 751kB/s]', '\\r 35%|###4\n| 3.44M/9.91M [00:05<00:08, 753kB/s]', '\\r 36%|###5      | 3.54M/9.91M\n[00:05<00:08, 784kB/s]', '\\r 37%|###6      | 3.64M/9.91M [00:05<00:10,\n625kB/s]', '\\r 38%|###8      | 3.77M/9.91M [00:06<00:08, 723kB/s]', '\\r 39%|###9\n| 3.87M/9.91M [00:06<00:08, 753kB/s]', '\\r 40%|###9      | 3.96M/9.91M\n[00:06<00:09, 615kB/s]', '\\r 41%|####1     | 4.10M/9.91M [00:06<00:08,\n712kB/s]', '\\r 42%|####2     | 4.19M/9.91M [00:06<00:07, 744kB/s]', '\\r\n43%|####3     | 4.29M/9.91M [00:06<00:07, 751kB/s]', '\\r 44%|####4     |\n4.39M/9.91M [00:06<00:07, 778kB/s]', '\\r 45%|####5     | 4.49M/9.91M\n[00:07<00:08, 624kB/s]', '\\r 47%|####6     | 4.62M/9.91M [00:07<00:07,\n721kB/s]', '\\r 48%|####7     | 4.72M/9.91M [00:07<00:06, 751kB/s]', '\\r\n49%|####8     | 4.82M/9.91M [00:07<00:06, 753kB/s]', '\\r 50%|####9     |\n4.92M/9.91M [00:07<00:06, 783kB/s]', '\\r 51%|#####     | 5.01M/9.91M\n[00:07<00:07, 625kB/s]', '\\r 52%|#####1    | 5.14M/9.91M [00:07<00:06,\n723kB/s]', '\\r 53%|#####2    | 5.24M/9.91M [00:08<00:06, 754kB/s]', '\\r\n54%|#####3    | 5.34M/9.91M [00:08<00:06, 755kB/s]', '\\r 55%|#####4    |\n5.44M/9.91M [00:08<00:05, 785kB/s]', '\\r 56%|#####5    | 5.54M/9.91M\n[00:08<00:06, 626kB/s]', '\\r 57%|#####6    | 5.64M/9.91M [00:08<00:06,\n663kB/s]', '\\r 58%|#####7    | 5.73M/9.91M [00:08<00:05, 707kB/s]', '\\r\n59%|#####8    | 5.83M/9.91M [00:08<00:05, 724kB/s]', '\\r 60%|#####9    |\n5.93M/9.91M [00:09<00:06, 635kB/s]', '\\r 61%|######    | 6.03M/9.91M\n[00:09<00:06, 638kB/s]', '\\r 62%|######1   | 6.13M/9.91M [00:09<00:05,\n671kB/s]', '\\r 63%|######2   | 6.23M/9.91M [00:09<00:05, 716kB/s]', '\\r\n64%|######3   | 6.32M/9.91M [00:09<00:04, 729kB/s]', '\\r 65%|######4   |\n6.42M/9.91M [00:09<00:04, 765kB/s]', '\\r 66%|######5   | 6.52M/9.91M\n[00:10<00:05, 614kB/s]', '\\r 67%|######7   | 6.65M/9.91M [00:10<00:04,\n712kB/s]', '\\r 68%|######8   | 6.75M/9.91M [00:10<00:04, 748kB/s]', '\\r\n69%|######9   | 6.85M/9.91M [00:10<00:04, 751kB/s]', '\\r 70%|#######   |\n6.95M/9.91M [00:10<00:03, 781kB/s]', '\\r 71%|#######1  | 7.05M/9.91M\n[00:10<00:04, 622kB/s]', '\\r 72%|#######2  | 7.18M/9.91M [00:10<00:03,\n721kB/s]', '\\r 73%|#######3  | 7.27M/9.91M [00:10<00:03, 753kB/s]', '\\r\n74%|#######4  | 7.37M/9.91M [00:11<00:04, 616kB/s]', '\\r 76%|#######5  |\n7.50M/9.91M [00:11<00:03, 712kB/s]', '\\r 77%|#######6  | 7.60M/9.91M\n[00:11<00:03, 743kB/s]', '\\r 78%|#######7  | 7.70M/9.91M [00:11<00:02,\n751kB/s]', '\\r 79%|#######8  | 7.80M/9.91M [00:11<00:02, 777kB/s]', '\\r\n80%|#######9  | 7.90M/9.91M [00:11<00:03, 622kB/s]', '\\r 81%|########  |\n8.03M/9.91M [00:12<00:02, 721kB/s]', '\\r 82%|########1 | 8.13M/9.91M\n[00:12<00:02, 752kB/s]', '\\r 83%|########2 | 8.22M/9.91M [00:12<00:02,\n756kB/s]', '\\r 84%|########3 | 8.32M/9.91M [00:12<00:02, 783kB/s]', '\\r\n85%|########4 | 8.42M/9.91M [00:12<00:02, 624kB/s]', '\\r 86%|########6 |\n8.55M/9.91M [00:12<00:01, 722kB/s]', '\\r 87%|########7 | 8.65M/9.91M\n[00:12<00:01, 752kB/s]', '\\r 88%|########8 | 8.75M/9.91M [00:13<00:01,\n616kB/s]', '\\r 90%|########9 | 8.88M/9.91M [00:13<00:01, 716kB/s]', '\\r\n91%|######### | 8.98M/9.91M [00:13<00:01, 740kB/s]', '\\r 92%|#########1|\n9.08M/9.91M [00:13<00:01, 752kB/s]', '\\r 93%|#########2| 9.18M/9.91M\n[00:13<00:00, 773kB/s]', '\\r 94%|#########3| 9.27M/9.91M [00:13<00:01,\n623kB/s]', '\\r 95%|#########4| 9.40M/9.91M [00:13<00:00, 724kB/s]', '\\r\n96%|#########5| 9.50M/9.91M [00:14<00:00, 748kB/s]', '\\r 97%|#########6|\n9.60M/9.91M [00:14<00:00, 758kB/s]', '\\r 98%|#########7| 9.70M/9.91M\n[00:14<00:00, 779kB/s]', '\\r 99%|#########8| 9.80M/9.91M [00:14<00:00,\n624kB/s]', '', '\\r100%|##########| 9.91M/9.91M [00:14<00:00, 675kB/s]', '\\n',\n'Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw',\n'\\n', '\\n', 'Downloading http://yann.lecun.com/exdb/mnist/train-labels-\nidx1-ubyte.gz', '\\n', 'Failed to download (trying next):\\nHTTP Error 404: Not\nFound', '\\n', '\\n', 'Downloading https://ossci-\ndatasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz', '\\n', 'Downloading\nhttps://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to\n./data/MNIST/raw/train-labels-idx1-ubyte.gz', '\\n', '\\r  0%|          |\n0.00/28.9k [00:00<?, ?B/s]', '\\r100%|##########| 28.9k/28.9k [00:00<00:00,\n123kB/s]', '', '\\r100%|##########| 28.9k/28.9k [00:00<00:00, 123kB/s]', '\\n',\n'Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw',\n'\\n', '\\n', 'Downloading http://yann.lecun.com/exdb/mnist/t10k-images-\nidx3-ubyte.gz', '\\n', 'Failed to download (trying next):\\nHTTP Error 404: Not\nFound', '\\n', '\\n', 'Downloading https://ossci-\ndatasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz', '\\n', 'Downloading\nhttps://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to\n./data/MNIST/raw/t10k-images-idx3-ubyte.gz', '\\n', '\\r  0%|          |\n0.00/1.65M [00:00<?, ?B/s]', '\\r  2%|1         | 32.8k/1.65M [00:00<00:21,\n75.5kB/s]', '\\r  6%|5         | 98.3k/1.65M [00:00<00:09, 158kB/s] ', '\\r 12%|#1\n| 197k/1.65M [00:00<00:05, 268kB/s] ', '\\r 22%|##1       | 360k/1.65M\n[00:01<00:03, 426kB/s]', '\\r 48%|####7     | 786k/1.65M [00:01<00:00, 904kB/s]',\n'\\r 93%|#########3| 1.54M/1.65M [00:01<00:00, 1.68MB/s]', '',\n'\\r100%|##########| 1.65M/1.65M [00:01<00:00, 1.03MB/s]', '\\n', 'Extracting\n./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw', '\\n', '\\n',\n'Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', '\\n',\n'Failed to download (trying next):\\nHTTP Error 404: Not Found', '\\n', '\\n',\n'Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-\nidx1-ubyte.gz', '\\n', 'Downloading https://ossci-\ndatasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to\n./data/MNIST/raw/t10k-labels-idx1-ubyte.gz', '\\n', '\\r  0%|          |\n0.00/4.54k [00:00<?, ?B/s]', '', '\\r100%|##########| 4.54k/4.54k [00:00<00:00,\n1.63MB/s]', '\\n', 'Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to\n./data/MNIST/raw', '\\n', '\\n', 'Model MLP Epoch 1: validation_loss = 0.1408',\n'\\n', 'Model CNN Epoch 1: validation_loss = 0.0761', '\\n', 'Epoch 1: CGR =\n0.9448', '\\n', 'Model MLP Epoch 2: validation_loss = 0.1023', '\\n', 'Model CNN\nEpoch 2: validation_loss = 0.0561', '\\n', 'Epoch 2: CGR = -0.1935', '\\n', 'Model\nMLP Epoch 3: validation_loss = 0.0826', '\\n', 'Model CNN Epoch 3:\nvalidation_loss = 0.0527', '\\n', 'Epoch 3: CGR = 0.0000', '\\n', 'Model MLP Epoch\n4: validation_loss = 0.0822', '\\n', 'Model CNN Epoch 4: validation_loss =\n0.0516', '\\n', 'Epoch 4: CGR = -0.9425', '\\n', 'Model MLP Epoch 5:\nvalidation_loss = 0.0808', '\\n', 'Model CNN Epoch 5: validation_loss = 0.0592',\n'\\n', 'Epoch 5: CGR = 0.5231', '\\n', 'Execution time: 3 minutes seconds (time\nlimit is an hour).']", "['Using device: cuda', '\\n', 'Downloading\nhttp://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', '\\n', 'Failed to\ndownload (trying next):\\nHTTP Error 404: Not Found', '\\n', '\\n', 'Downloading\nhttps://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz', '\\n',\n'Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-\nidx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz', '\\n', '\\r  0%|\n| 0.00/9.91M [00:00<?, ?B/s]', '\\r  0%|          | 32.8k/9.91M [00:00<01:08,\n143kB/s]', '\\r  1%|          | 98.3k/9.91M [00:00<00:43, 226kB/s]', '\\r  2%|1\n| 164k/9.91M [00:00<00:38, 254kB/s] ', '\\r  4%|3         | 360k/9.91M\n[00:00<00:19, 491kB/s]', '\\r  8%|7         | 754k/9.91M [00:01<00:09, 928kB/s]',\n'\\r 15%|#5        | 1.51M/9.91M [00:01<00:04, 1.73MB/s]', '\\r 27%|##7       |\n2.72M/9.91M [00:01<00:02, 3.52MB/s]', '\\r 33%|###2      | 3.24M/9.91M\n[00:01<00:01, 3.51MB/s]', '\\r 45%|####4     | 4.46M/9.91M [00:01<00:01,\n5.27MB/s]', '\\r 57%|#####7    | 5.67M/9.91M [00:01<00:00, 6.79MB/s]', '\\r\n69%|######9   | 6.88M/9.91M [00:01<00:00, 7.73MB/s]', '\\r 82%|########1 |\n8.09M/9.91M [00:02<00:00, 8.80MB/s]', '\\r 93%|#########3| 9.24M/9.91M\n[00:02<00:00, 9.45MB/s]', '', '\\r100%|##########| 9.91M/9.91M [00:02<00:00,\n4.44MB/s]', '\\n', 'Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to\n./data/MNIST/raw', '\\n', '\\n', 'Downloading\nhttp://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', '\\n', 'Failed to\ndownload (trying next):\\nHTTP Error 404: Not Found', '\\n', '\\n', 'Downloading\nhttps://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz', '\\n',\n'Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-\nidx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz', '\\n', '\\r  0%|\n| 0.00/28.9k [00:00<?, ?B/s]', '\\r100%|##########| 28.9k/28.9k [00:00<00:00,\n121kB/s]', '', '\\r100%|##########| 28.9k/28.9k [00:00<00:00, 121kB/s]', '\\n',\n'Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw',\n'\\n', '\\n', 'Downloading http://yann.lecun.com/exdb/mnist/t10k-images-\nidx3-ubyte.gz', '\\n', 'Failed to download (trying next):\\nHTTP Error 404: Not\nFound', '\\n', '\\n', 'Downloading https://ossci-\ndatasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz', '\\n', 'Downloading\nhttps://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to\n./data/MNIST/raw/t10k-images-idx3-ubyte.gz', '\\n', '\\r  0%|          |\n0.00/1.65M [00:00<?, ?B/s]', '\\r  2%|1         | 32.8k/1.65M [00:00<00:22,\n72.3kB/s]', '\\r  4%|3         | 65.5k/1.65M [00:00<00:15, 99.7kB/s]', '\\r 10%|9\n| 164k/1.65M [00:00<00:06, 217kB/s]  ', '\\r 18%|#7        | 295k/1.65M\n[00:01<00:03, 382kB/s]', '\\r 26%|##5       | 426k/1.65M [00:01<00:02, 541kB/s]',\n'\\r 38%|###7      | 623k/1.65M [00:01<00:01, 801kB/s]', '\\r 50%|####9     |\n819k/1.65M [00:01<00:00, 1.06MB/s]', '\\r 74%|#######3  | 1.21M/1.65M\n[00:01<00:00, 1.60MB/s]', '\\r 89%|########9 | 1.47M/1.65M [00:01<00:00,\n1.84MB/s]', '', '\\r100%|##########| 1.65M/1.65M [00:01<00:00, 976kB/s] ', '\\n',\n'Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw',\n'\\n', '\\n', 'Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-\nidx1-ubyte.gz', '\\n', 'Failed to download (trying next):\\nHTTP Error 404: Not\nFound', '\\n', '\\n', 'Downloading https://ossci-\ndatasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz', '\\n', 'Downloading\nhttps://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to\n./data/MNIST/raw/t10k-labels-idx1-ubyte.gz', '\\n', '\\r  0%|          |\n0.00/4.54k [00:00<?, ?B/s]', '', '\\r100%|##########| 4.54k/4.54k [00:00<00:00,\n1.92MB/s]', '\\n', 'Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to\n./data/MNIST/raw', '\\n', '\\n', '\\n=== Running with n_epochs = 5 ===', '\\n', 'MLP\nEpoch 1: val_loss=0.1408, orig_acc=0.9573, aug_acc=0.8664', '\\n', 'CNN Epoch 1:\nval_loss=0.0754, orig_acc=0.9757, aug_acc=0.9025', '\\n', 'Epoch 1: CGR=0.9620',\n'\\n', 'MLP Epoch 2: val_loss=0.1023, orig_acc=0.9693, aug_acc=0.8909', '\\n',\n'CNN Epoch 2: val_loss=0.0576, orig_acc=0.9802, aug_acc=0.8979', '\\n', 'Epoch 2:\nCGR=-0.3578', '\\n', 'MLP Epoch 3: val_loss=0.0826, orig_acc=0.9747,\naug_acc=0.8994', '\\n', 'CNN Epoch 3: val_loss=0.0559, orig_acc=0.9814,\naug_acc=0.9029', '\\n', 'Epoch 3: CGR=-0.4776', '\\n', 'MLP Epoch 4:\nval_loss=0.0822, orig_acc=0.9748, aug_acc=0.8961', '\\n', 'CNN Epoch 4:\nval_loss=0.0510, orig_acc=0.9833, aug_acc=0.8956', '\\n', 'Epoch 4: CGR=-0.9412',\n'\\n', 'MLP Epoch 5: val_loss=0.0808, orig_acc=0.9758, aug_acc=0.9023', '\\n',\n'CNN Epoch 5: val_loss=0.0587, orig_acc=0.9821, aug_acc=0.9167', '\\n', 'Epoch 5:\nCGR=1.2857', '\\n', '\\n=== Running with n_epochs = 10 ===', '\\n', 'MLP Epoch 1:\nval_loss=0.1379, orig_acc=0.9578, aug_acc=0.8680', '\\n', 'CNN Epoch 1:\nval_loss=0.0912, orig_acc=0.9722, aug_acc=0.8770', '\\n', 'Epoch 1: CGR=-0.3750',\n'\\n', 'MLP Epoch 2: val_loss=0.1009, orig_acc=0.9694, aug_acc=0.8893', '\\n',\n'CNN Epoch 2: val_loss=0.0618, orig_acc=0.9795, aug_acc=0.9030', '\\n', 'Epoch 2:\nCGR=0.3564', '\\n', 'MLP Epoch 3: val_loss=0.0952, orig_acc=0.9717,\naug_acc=0.8899', '\\n', 'CNN Epoch 3: val_loss=0.0562, orig_acc=0.9812,\naug_acc=0.9091', '\\n', 'Epoch 3: CGR=1.0211', '\\n', 'MLP Epoch 4:\nval_loss=0.0788, orig_acc=0.9771, aug_acc=0.8961', '\\n', 'CNN Epoch 4:\nval_loss=0.0598, orig_acc=0.9805, aug_acc=0.8977', '\\n', 'Epoch 4: CGR=-0.5294',\n'\\n', 'MLP Epoch 5: val_loss=0.0769, orig_acc=0.9788, aug_acc=0.9065', '\\n',\n'CNN Epoch 5: val_loss=0.0548, orig_acc=0.9826, aug_acc=0.9204', '\\n', 'Epoch 5:\nCGR=2.6579', '\\n', 'MLP Epoch 6: val_loss=0.0855, orig_acc=0.9760,\naug_acc=0.8938', '\\n', 'CNN Epoch 6: val_loss=0.0488, orig_acc=0.9854,\naug_acc=0.9185', '\\n', 'Epoch 6: CGR=1.6277', '\\n', 'MLP Epoch 7:\nval_loss=0.0823, orig_acc=0.9764, aug_acc=0.8928', '\\n', 'CNN Epoch 7:\nval_loss=0.0482, orig_acc=0.9851, aug_acc=0.9076', '\\n', 'Epoch 7: CGR=0.7011',\n'\\n', 'MLP Epoch 8: val_loss=0.0852, orig_acc=0.9766, aug_acc=0.8999', '\\n',\n'CNN Epoch 8: val_loss=0.0646, orig_acc=0.9814, aug_acc=0.8993', '\\n', 'Epoch 8:\nCGR=-0.8750', '\\n', 'MLP Epoch 9: val_loss=0.0965, orig_acc=0.9753,\naug_acc=0.8950', '\\n', 'CNN Epoch 9: val_loss=0.0547, orig_acc=0.9845,\naug_acc=0.9082', '\\n', 'Epoch 9: CGR=0.4348', '\\n', 'MLP Epoch 10:\nval_loss=0.0893, orig_acc=0.9779, aug_acc=0.8993', '\\n', 'CNN Epoch 10:\nval_loss=0.0604, orig_acc=0.9855, aug_acc=0.9047', '\\n', 'Epoch 10:\nCGR=-0.2895', '\\n', '\\n=== Running with n_epochs = 15 ===', '\\n', 'MLP Epoch 1:\nval_loss=0.1399, orig_acc=0.9582, aug_acc=0.8630', '\\n', 'CNN Epoch 1:\nval_loss=0.0932, orig_acc=0.9718, aug_acc=0.8819', '\\n', 'Epoch 1: CGR=0.3897',\n'\\n', 'MLP Epoch 2: val_loss=0.0987, orig_acc=0.9708, aug_acc=0.8884', '\\n',\n'CNN Epoch 2: val_loss=0.0680, orig_acc=0.9785, aug_acc=0.8986', '\\n', 'Epoch 2:\nCGR=0.3247', '\\n', 'MLP Epoch 3: val_loss=0.0916, orig_acc=0.9735,\naug_acc=0.8812', '\\n', 'CNN Epoch 3: val_loss=0.0578, orig_acc=0.9824,\naug_acc=0.9127', '\\n', 'Epoch 3: CGR=2.5393', '\\n', 'MLP Epoch 4:\nval_loss=0.0821, orig_acc=0.9754, aug_acc=0.8918', '\\n', 'CNN Epoch 4:\nval_loss=0.0481, orig_acc=0.9834, aug_acc=0.9036', '\\n', 'Epoch 4: CGR=0.4750',\n'\\n', 'MLP Epoch 5: val_loss=0.0760, orig_acc=0.9799, aug_acc=0.8929', '\\n',\n'CNN Epoch 5: val_loss=0.0528, orig_acc=0.9814, aug_acc=0.9065', '\\n', 'Epoch 5:\nCGR=8.0666', '\\n', 'MLP Epoch 6: val_loss=0.0927, orig_acc=0.9747,\naug_acc=0.8912', '\\n', 'CNN Epoch 6: val_loss=0.0456, orig_acc=0.9847,\naug_acc=0.9234', '\\n', 'Epoch 6: CGR=2.2200', '\\n', 'MLP Epoch 7:\nval_loss=0.0829, orig_acc=0.9773, aug_acc=0.8996', '\\n', 'CNN Epoch 7:\nval_loss=0.0471, orig_acc=0.9851, aug_acc=0.9134', '\\n', 'Epoch 7: CGR=0.7692',\n'\\n', 'MLP Epoch 8: val_loss=0.0781, orig_acc=0.9777, aug_acc=0.9045', '\\n',\n'CNN Epoch 8: val_loss=0.0568, orig_acc=0.9835, aug_acc=0.9163', '\\n', 'Epoch 8:\nCGR=1.0345', '\\n', 'MLP Epoch 9: val_loss=0.0881, orig_acc=0.9779,\naug_acc=0.8999', '\\n', 'CNN Epoch 9: val_loss=0.0464, orig_acc=0.9858,\naug_acc=0.9153', '\\n', 'Epoch 9: CGR=0.9494', '\\n', 'MLP Epoch 10:\nval_loss=0.0814, orig_acc=0.9792, aug_acc=0.8993', '\\n', 'CNN Epoch 10:\nval_loss=0.0938, orig_acc=0.9764, aug_acc=0.9115', '\\n', 'Epoch 10: CGR=3.3571',\n'\\n', 'MLP Epoch 11: val_loss=0.1073, orig_acc=0.9737, aug_acc=0.8883', '\\n',\n'CNN Epoch 11: val_loss=0.0532, orig_acc=0.9850, aug_acc=0.9098', '\\n', 'Epoch\n11: CGR=0.9027', '\\n', 'MLP Epoch 12: val_loss=0.1031, orig_acc=0.9764,\naug_acc=0.9017', '\\n', 'CNN Epoch 12: val_loss=0.0617, orig_acc=0.9840,\naug_acc=0.9034', '\\n', 'Epoch 12: CGR=-0.7763', '\\n', 'MLP Epoch 13:\nval_loss=0.0990, orig_acc=0.9755, aug_acc=0.8959', '\\n', 'CNN Epoch 13:\nval_loss=0.0681, orig_acc=0.9840, aug_acc=0.9216', '\\n', 'Epoch 13: CGR=2.0235',\n'\\n', 'MLP Epoch 14: val_loss=0.1195, orig_acc=0.9759, aug_acc=0.8950', '\\n',\n'CNN Epoch 14: val_loss=0.0661, orig_acc=0.9845, aug_acc=0.9224', '\\n', 'Epoch\n14: CGR=2.1860', '\\n', 'MLP Epoch 15: val_loss=0.1061, orig_acc=0.9799,\naug_acc=0.9013', '\\n', 'CNN Epoch 15: val_loss=0.0608, orig_acc=0.9855,\naug_acc=0.9114', '\\n', 'Epoch 15: CGR=0.8036', '\\n', '\\n=== Running with\nn_epochs = 20 ===', '\\n', 'MLP Epoch 1: val_loss=0.1449, orig_acc=0.9582,\naug_acc=0.8576', '\\n', 'CNN Epoch 1: val_loss=0.0856, orig_acc=0.9726,\naug_acc=0.8910', '\\n', 'Epoch 1: CGR=1.3194', '\\n', 'MLP Epoch 2:\nval_loss=0.1019, orig_acc=0.9669, aug_acc=0.8822', '\\n', 'CNN Epoch 2:\nval_loss=0.0589, orig_acc=0.9806, aug_acc=0.9042', '\\n', 'Epoch 2: CGR=0.6058',\n'\\n', 'MLP Epoch 3: val_loss=0.0830, orig_acc=0.9747, aug_acc=0.8963', '\\n',\n'CNN Epoch 3: val_loss=0.0638, orig_acc=0.9794, aug_acc=0.9025', '\\n', 'Epoch 3:\nCGR=0.3191', '\\n', 'MLP Epoch 4: val_loss=0.0759, orig_acc=0.9764,\naug_acc=0.9027', '\\n', 'CNN Epoch 4: val_loss=0.0460, orig_acc=0.9858,\naug_acc=0.9207', '\\n', 'Epoch 4: CGR=0.9149', '\\n', 'MLP Epoch 5:\nval_loss=0.0780, orig_acc=0.9754, aug_acc=0.8924', '\\n', 'CNN Epoch 5:\nval_loss=0.0494, orig_acc=0.9840, aug_acc=0.9096', '\\n', 'Epoch 5: CGR=1.0000',\n'\\n', 'MLP Epoch 6: val_loss=0.0779, orig_acc=0.9771, aug_acc=0.8983', '\\n',\n'CNN Epoch 6: val_loss=0.0498, orig_acc=0.9851, aug_acc=0.9189', '\\n', 'Epoch 6:\nCGR=1.5750', '\\n', 'MLP Epoch 7: val_loss=0.0888, orig_acc=0.9743,\naug_acc=0.8949', '\\n', 'CNN Epoch 7: val_loss=0.0509, orig_acc=0.9855,\naug_acc=0.9163', '\\n', 'Epoch 7: CGR=0.9107', '\\n', 'MLP Epoch 8:\nval_loss=0.0862, orig_acc=0.9759, aug_acc=0.9080', '\\n', 'CNN Epoch 8:\nval_loss=0.0533, orig_acc=0.9851, aug_acc=0.9215', '\\n', 'Epoch 8: CGR=0.4674',\n'\\n', 'MLP Epoch 9: val_loss=0.0841, orig_acc=0.9767, aug_acc=0.8949', '\\n',\n'CNN Epoch 9: val_loss=0.0610, orig_acc=0.9851, aug_acc=0.9194', '\\n', 'Epoch 9:\nCGR=1.9167', '\\n', 'MLP Epoch 10: val_loss=0.0891, orig_acc=0.9766,\naug_acc=0.8941', '\\n', 'CNN Epoch 10: val_loss=0.0490, orig_acc=0.9868,\naug_acc=0.9082', '\\n', 'Epoch 10: CGR=0.3824', '\\n', 'MLP Epoch 11:\nval_loss=0.0742, orig_acc=0.9798, aug_acc=0.9058', '\\n', 'CNN Epoch 11:\nval_loss=0.0610, orig_acc=0.9854, aug_acc=0.9039', '\\n', 'Epoch 11:\nCGR=-0.6607', '\\n', 'MLP Epoch 12: val_loss=0.1050, orig_acc=0.9750,\naug_acc=0.8963', '\\n', 'CNN Epoch 12: val_loss=0.0655, orig_acc=0.9847,\naug_acc=0.9236', '\\n', 'Epoch 12: CGR=1.8144', '\\n', 'MLP Epoch 13:\nval_loss=0.0919, orig_acc=0.9785, aug_acc=0.9039', '\\n', 'CNN Epoch 13:\nval_loss=0.0556, orig_acc=0.9869, aug_acc=0.9269', '\\n', 'Epoch 13: CGR=1.7381',\n'\\n', 'MLP Epoch 14: val_loss=0.1042, orig_acc=0.9764, aug_acc=0.8950', '\\n',\n'CNN Epoch 14: val_loss=0.0668, orig_acc=0.9862, aug_acc=0.9251', '\\n', 'Epoch\n14: CGR=2.0714', '\\n', 'MLP Epoch 15: val_loss=0.1112, orig_acc=0.9736,\naug_acc=0.8944', '\\n', 'CNN Epoch 15: val_loss=0.0589, orig_acc=0.9876,\naug_acc=0.9166', '\\n', 'Epoch 15: CGR=0.5857', '\\n', 'MLP Epoch 16:\nval_loss=0.0958, orig_acc=0.9788, aug_acc=0.9058', '\\n', 'CNN Epoch 16:\nval_loss=0.0647, orig_acc=0.9871, aug_acc=0.9221', '\\n', 'Epoch 16: CGR=0.9639',\n'\\n', 'MLP Epoch 17: val_loss=0.1098, orig_acc=0.9765, aug_acc=0.9003', '\\n',\n'CNN Epoch 17: val_loss=0.0775, orig_acc=0.9848, aug_acc=0.9151', '\\n', 'Epoch\n17: CGR=0.7831', '\\n', 'MLP Epoch 18: val_loss=0.1147, orig_acc=0.9763,\naug_acc=0.8962', '\\n', 'CNN Epoch 18: val_loss=0.0671, orig_acc=0.9851,\naug_acc=0.9154', '\\n', 'Epoch 18: CGR=1.1818', '\\n', 'MLP Epoch 19:\nval_loss=0.1328, orig_acc=0.9764, aug_acc=0.9010', '\\n', 'CNN Epoch 19:\nval_loss=0.0739, orig_acc=0.9877, aug_acc=0.9239', '\\n', 'Epoch 19: CGR=1.0265',\n'\\n', 'MLP Epoch 20: val_loss=0.1257, orig_acc=0.9760, aug_acc=0.8928', '\\n',\n'CNN Epoch 20: val_loss=0.0772, orig_acc=0.9861, aug_acc=0.9200', '\\n', 'Epoch\n20: CGR=1.6931', '\\n', 'Execution time: 22 minutes seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Downloading\nhttp://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', '\\n', 'Failed to\ndownload (trying next):\\nHTTP Error 404: Not Found', '\\n', '\\n', 'Downloading\nhttps://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz', '\\n',\n'Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-\nidx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz', '\\n', '\\r  0%|\n| 0.00/9.91M [00:00<?, ?B/s]', '\\r  0%|          | 32.8k/9.91M [00:00<01:11,\n138kB/s]', '\\r  1%|          | 65.5k/9.91M [00:00<01:11, 138kB/s]', '\\r  2%|1\n| 164k/9.91M [00:00<00:37, 263kB/s] ', '\\r  4%|3         | 360k/9.91M\n[00:00<00:19, 485kB/s]', '\\r  7%|7         | 721k/9.91M [00:01<00:10, 855kB/s]',\n'\\r 14%|#4        | 1.41M/9.91M [00:01<00:05, 1.55MB/s]', '\\r 26%|##6       |\n2.62M/9.91M [00:01<00:02, 3.33MB/s]', '\\r 31%|###1      | 3.11M/9.91M\n[00:01<00:02, 3.25MB/s]', '\\r 44%|####3     | 4.33M/9.91M [00:01<00:01,\n5.00MB/s]', '\\r 56%|#####5    | 5.54M/9.91M [00:01<00:00, 6.54MB/s]', '\\r\n66%|######6   | 6.59M/9.91M [00:02<00:00, 7.31MB/s]', '\\r 79%|#######8  |\n7.80M/9.91M [00:02<00:00, 8.47MB/s]', '\\r 91%|######### | 9.01M/9.91M\n[00:02<00:00, 9.40MB/s]', '', '\\r100%|##########| 9.91M/9.91M [00:02<00:00,\n4.29MB/s]', '\\n', 'Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to\n./data/MNIST/raw', '\\n', '\\n', 'Downloading\nhttp://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', '\\n', 'Failed to\ndownload (trying next):\\nHTTP Error 404: Not Found', '\\n', '\\n', 'Downloading\nhttps://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz', '\\n',\n'Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-\nidx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz', '\\n', '\\r  0%|\n| 0.00/28.9k [00:00<?, ?B/s]', '\\r100%|##########| 28.9k/28.9k [00:00<00:00,\n121kB/s]', '', '\\r100%|##########| 28.9k/28.9k [00:00<00:00, 120kB/s]', '\\n',\n'Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw',\n'\\n', '\\n', 'Downloading http://yann.lecun.com/exdb/mnist/t10k-images-\nidx3-ubyte.gz', '\\n', 'Failed to download (trying next):\\nHTTP Error 404: Not\nFound', '\\n', '\\n', 'Downloading https://ossci-\ndatasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz', '\\n', 'Downloading\nhttps://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to\n./data/MNIST/raw/t10k-images-idx3-ubyte.gz', '\\n', '\\r  0%|          |\n0.00/1.65M [00:00<?, ?B/s]', '\\r  2%|1         | 32.8k/1.65M [00:00<00:11,\n139kB/s]', '\\r  6%|5         | 98.3k/1.65M [00:00<00:07, 220kB/s]', '\\r 10%|9\n| 164k/1.65M [00:00<00:05, 248kB/s] ', '\\r 22%|##1       | 360k/1.65M\n[00:00<00:02, 480kB/s]', '\\r 44%|####3     | 721k/1.65M [00:01<00:01, 858kB/s]',\n'\\r 87%|########7 | 1.44M/1.65M [00:01<00:00, 1.61MB/s]', '',\n'\\r100%|##########| 1.65M/1.65M [00:01<00:00, 1.15MB/s]', '\\n', 'Extracting\n./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw', '\\n', '\\n',\n'Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', '\\n',\n'Failed to download (trying next):\\nHTTP Error 404: Not Found', '\\n', '\\n',\n'Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-\nidx1-ubyte.gz', '\\n', 'Downloading https://ossci-\ndatasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to\n./data/MNIST/raw/t10k-labels-idx1-ubyte.gz', '\\n', '\\r  0%|          |\n0.00/4.54k [00:00<?, ?B/s]', '', '\\r100%|##########| 4.54k/4.54k [00:00<00:00,\n1.59MB/s]', '\\n', 'Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to\n./data/MNIST/raw', '\\n', '\\n', 'LR 0.0001 Model MLP Epoch 1: val_loss=0.3106,\norig_acc=0.9161, aug_acc=0.7942', '\\n', 'LR 0.0001 Model CNN Epoch 1:\nval_loss=0.2259, orig_acc=0.9386, aug_acc=0.8316', '\\n', 'LR 0.0001 Epoch 1: CGR\n= 0.6622', '\\n', 'LR 0.0001 Model MLP Epoch 2: val_loss=0.2518, orig_acc=0.9282,\naug_acc=0.8098', '\\n', 'LR 0.0001 Model CNN Epoch 2: val_loss=0.1514,\norig_acc=0.9574, aug_acc=0.8555', '\\n', 'LR 0.0001 Epoch 2: CGR = 0.5651', '\\n',\n'LR 0.0001 Model MLP Epoch 3: val_loss=0.2092, orig_acc=0.9384, aug_acc=0.8293',\n'\\n', 'LR 0.0001 Model CNN Epoch 3: val_loss=0.1165, orig_acc=0.9662,\naug_acc=0.8780', '\\n', 'LR 0.0001 Epoch 3: CGR = 0.7518', '\\n', 'LR 0.0001 Model\nMLP Epoch 4: val_loss=0.1854, orig_acc=0.9471, aug_acc=0.8373', '\\n', 'LR 0.0001\nModel CNN Epoch 4: val_loss=0.0994, orig_acc=0.9703, aug_acc=0.8760', '\\n', 'LR\n0.0001 Epoch 4: CGR = 0.6681', '\\n', 'LR 0.0001 Model MLP Epoch 5:\nval_loss=0.1662, orig_acc=0.9522, aug_acc=0.8464', '\\n', 'LR 0.0001 Model CNN\nEpoch 5: val_loss=0.0822, orig_acc=0.9742, aug_acc=0.8995', '\\n', 'LR 0.0001\nEpoch 5: CGR = 1.4136', '\\n', 'LR 0.0005 Model MLP Epoch 1: val_loss=0.1753,\norig_acc=0.9484, aug_acc=0.8514', '\\n', 'LR 0.0005 Model CNN Epoch 1:\nval_loss=0.1009, orig_acc=0.9710, aug_acc=0.8778', '\\n', 'LR 0.0005 Epoch 1: CGR\n= 0.1681', '\\n', 'LR 0.0005 Model MLP Epoch 2: val_loss=0.1322, orig_acc=0.9606,\naug_acc=0.8643', '\\n', 'LR 0.0005 Model CNN Epoch 2: val_loss=0.0818,\norig_acc=0.9761, aug_acc=0.8914', '\\n', 'LR 0.0005 Epoch 2: CGR = 0.7484', '\\n',\n'LR 0.0005 Model MLP Epoch 3: val_loss=0.0988, orig_acc=0.9705, aug_acc=0.8827',\n'\\n', 'LR 0.0005 Model CNN Epoch 3: val_loss=0.0633, orig_acc=0.9806,\naug_acc=0.9024', '\\n', 'LR 0.0005 Epoch 3: CGR = 0.9505', '\\n', 'LR 0.0005 Model\nMLP Epoch 4: val_loss=0.0918, orig_acc=0.9727, aug_acc=0.8737', '\\n', 'LR 0.0005\nModel CNN Epoch 4: val_loss=0.0526, orig_acc=0.9830, aug_acc=0.9012', '\\n', 'LR\n0.0005 Epoch 4: CGR = 1.6699', '\\n', 'LR 0.0005 Model MLP Epoch 5:\nval_loss=0.0830, orig_acc=0.9753, aug_acc=0.8881', '\\n', 'LR 0.0005 Model CNN\nEpoch 5: val_loss=0.0580, orig_acc=0.9807, aug_acc=0.8948', '\\n', 'LR 0.0005\nEpoch 5: CGR = 0.2407', '\\n', 'LR 0.001 Model MLP Epoch 1: val_loss=0.1276,\norig_acc=0.9612, aug_acc=0.8666', '\\n', 'LR 0.001 Model CNN Epoch 1:\nval_loss=0.0899, orig_acc=0.9720, aug_acc=0.8984', '\\n', 'LR 0.001 Epoch 1: CGR\n= 1.9444', '\\n', 'LR 0.001 Model MLP Epoch 2: val_loss=0.1011, orig_acc=0.9683,\naug_acc=0.8881', '\\n', 'LR 0.001 Model CNN Epoch 2: val_loss=0.0572,\norig_acc=0.9809, aug_acc=0.9085', '\\n', 'LR 0.001 Epoch 2: CGR = 0.6190', '\\n',\n'LR 0.001 Model MLP Epoch 3: val_loss=0.0948, orig_acc=0.9705, aug_acc=0.8876',\n'\\n', 'LR 0.001 Model CNN Epoch 3: val_loss=0.0522, orig_acc=0.9818,\naug_acc=0.8951', '\\n', 'LR 0.001 Epoch 3: CGR = -0.3363', '\\n', 'LR 0.001 Model\nMLP Epoch 4: val_loss=0.0825, orig_acc=0.9740, aug_acc=0.8952', '\\n', 'LR 0.001\nModel CNN Epoch 4: val_loss=0.0494, orig_acc=0.9835, aug_acc=0.9079', '\\n', 'LR\n0.001 Epoch 4: CGR = 0.3368', '\\n', 'LR 0.001 Model MLP Epoch 5:\nval_loss=0.0804, orig_acc=0.9760, aug_acc=0.8936', '\\n', 'LR 0.001 Model CNN\nEpoch 5: val_loss=0.0421, orig_acc=0.9852, aug_acc=0.9115', '\\n', 'LR 0.001\nEpoch 5: CGR = 0.9457', '\\n', 'LR 0.005 Model MLP Epoch 1: val_loss=0.1777,\norig_acc=0.9466, aug_acc=0.8450', '\\n', 'LR 0.005 Model CNN Epoch 1:\nval_loss=0.0751, orig_acc=0.9754, aug_acc=0.8907', '\\n', 'LR 0.005 Epoch 1: CGR\n= 0.5868', '\\n', 'LR 0.005 Model MLP Epoch 2: val_loss=0.1418, orig_acc=0.9612,\naug_acc=0.8773', '\\n', 'LR 0.005 Model CNN Epoch 2: val_loss=0.0695,\norig_acc=0.9786, aug_acc=0.9031', '\\n', 'LR 0.005 Epoch 2: CGR = 0.4828', '\\n',\n'LR 0.005 Model MLP Epoch 3: val_loss=0.1384, orig_acc=0.9626, aug_acc=0.8696',\n'\\n', 'LR 0.005 Model CNN Epoch 3: val_loss=0.0599, orig_acc=0.9824,\naug_acc=0.9008', '\\n', 'LR 0.005 Epoch 3: CGR = 0.5758', '\\n', 'LR 0.005 Model\nMLP Epoch 4: val_loss=0.1730, orig_acc=0.9593, aug_acc=0.8748', '\\n', 'LR 0.005\nModel CNN Epoch 4: val_loss=0.0661, orig_acc=0.9812, aug_acc=0.8940', '\\n', 'LR\n0.005 Epoch 4: CGR = -0.1233', '\\n', 'LR 0.005 Model MLP Epoch 5:\nval_loss=0.1495, orig_acc=0.9661, aug_acc=0.8796', '\\n', 'LR 0.005 Model CNN\nEpoch 5: val_loss=0.0756, orig_acc=0.9807, aug_acc=0.9026', '\\n', 'LR 0.005\nEpoch 5: CGR = 0.5753', '\\n', 'LR 0.01 Model MLP Epoch 1: val_loss=0.1984,\norig_acc=0.9461, aug_acc=0.8431', '\\n', 'LR 0.01 Model CNN Epoch 1:\nval_loss=0.0774, orig_acc=0.9750, aug_acc=0.9007', '\\n', 'LR 0.01 Epoch 1: CGR =\n0.9931', '\\n', 'LR 0.01 Model MLP Epoch 2: val_loss=0.2041, orig_acc=0.9491,\naug_acc=0.8546', '\\n', 'LR 0.01 Model CNN Epoch 2: val_loss=0.0717,\norig_acc=0.9777, aug_acc=0.8964', '\\n', 'LR 0.01 Epoch 2: CGR = 0.4615', '\\n',\n'LR 0.01 Model MLP Epoch 3: val_loss=0.3087, orig_acc=0.9340, aug_acc=0.8351',\n'\\n', 'LR 0.01 Model CNN Epoch 3: val_loss=0.0677, orig_acc=0.9796,\naug_acc=0.9124', '\\n', 'LR 0.01 Epoch 3: CGR = 0.6952', '\\n', 'LR 0.01 Model MLP\nEpoch 4: val_loss=0.2077, orig_acc=0.9467, aug_acc=0.8446', '\\n', 'LR 0.01 Model\nCNN Epoch 4: val_loss=0.0928, orig_acc=0.9752, aug_acc=0.9049', '\\n', 'LR 0.01\nEpoch 4: CGR = 1.1158', '\\n', 'LR 0.01 Model MLP Epoch 5: val_loss=0.2000,\norig_acc=0.9529, aug_acc=0.8530', '\\n', 'LR 0.01 Model CNN Epoch 5:\nval_loss=0.0949, orig_acc=0.9747, aug_acc=0.8903', '\\n', 'LR 0.01 Epoch 5: CGR =\n0.7110', '\\n', 'Execution time: 11 minutes seconds (time limit is an hour).']", "['Using device:', ' ', 'cuda', '\\n', 'Downloading\nhttp://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', '\\n', 'Failed to\ndownload (trying next):\\nHTTP Error 404: Not Found', '\\n', '\\n', 'Downloading\nhttps://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz', '\\n',\n'Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-\nidx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz', '\\n', '\\r  0%|\n| 0.00/9.91M [00:00<?, ?B/s]', '\\r  0%|          | 32.8k/9.91M [00:00<03:30,\n46.9kB/s]', '\\r  1%|          | 65.5k/9.91M [00:01<03:30, 46.9kB/s]', '\\r  1%|\n| 98.3k/9.91M [00:01<02:58, 55.0kB/s]', '\\r  1%|1         | 131k/9.91M\n[00:02<02:42, 60.2kB/s] ', '\\r  2%|1         | 164k/9.91M [00:02<02:33,\n63.6kB/s]', '\\r  2%|1         | 197k/9.91M [00:03<02:27, 65.8kB/s]', '\\r  2%|2\n| 229k/9.91M [00:03<02:01, 79.5kB/s]', '\\r  3%|2         | 262k/9.91M\n[00:03<01:44, 92.2kB/s]', '\\r  3%|3         | 328k/9.91M [00:03<01:10, 136kB/s]\n', '\\r  4%|3         | 360k/9.91M [00:04<01:09, 137kB/s]', '\\r  4%|3         |\n393k/9.91M [00:04<01:09, 138kB/s]', '\\r  5%|4         | 459k/9.91M [00:04<00:53,\n177kB/s]', '\\r  5%|5         | 524k/9.91M [00:04<00:45, 206kB/s]', '\\r  6%|6\n| 623k/9.91M [00:05<00:34, 267kB/s]', '\\r  7%|6         | 688k/9.91M\n[00:05<00:34, 271kB/s]', '\\r  8%|7         | 786k/9.91M [00:05<00:28, 315kB/s]',\n'\\r  9%|8         | 885k/9.91M [00:05<00:26, 347kB/s]', '\\r 10%|#         |\n1.02M/9.91M [00:06<00:21, 409kB/s]', '\\r 11%|#1        | 1.11M/9.91M\n[00:06<00:21, 414kB/s]', '\\r 13%|#2        | 1.28M/9.91M [00:06<00:17,\n500kB/s]', '\\r 15%|#4        | 1.44M/9.91M [00:06<00:15, 560kB/s]', '\\r 16%|#6\n| 1.61M/9.91M [00:07<00:14, 592kB/s]', '\\r 18%|#7        | 1.74M/9.91M\n[00:07<00:13, 588kB/s]', '\\r 19%|#8        | 1.87M/9.91M [00:07<00:13,\n583kB/s]', '\\r 20%|##        | 2.03M/9.91M [00:07<00:12, 618kB/s]', '\\r 22%|##1\n| 2.16M/9.91M [00:07<00:12, 605kB/s]', '\\r 23%|##3       | 2.33M/9.91M\n[00:08<00:11, 634kB/s]', '\\r 25%|##5       | 2.49M/9.91M [00:08<00:11,\n651kB/s]', '\\r 27%|##6       | 2.65M/9.91M [00:08<00:10, 666kB/s]', '\\r 28%|##8\n| 2.79M/9.91M [00:08<00:09, 716kB/s]', '\\r 29%|##9       | 2.88M/9.91M\n[00:08<00:09, 756kB/s]', '\\r 30%|###       | 2.98M/9.91M [00:09<00:10,\n649kB/s]', '\\r 32%|###1      | 3.15M/9.91M [00:09<00:10, 669kB/s]', '\\r 33%|###3\n| 3.28M/9.91M [00:09<00:09, 722kB/s]', '\\r 34%|###4      | 3.38M/9.91M\n[00:09<00:08, 767kB/s]', '\\r 35%|###5      | 3.47M/9.91M [00:09<00:09,\n650kB/s]', '\\r 36%|###6      | 3.60M/9.91M [00:09<00:08, 718kB/s]', '\\r 37%|###7\n| 3.70M/9.91M [00:10<00:08, 765kB/s]', '\\r 38%|###8      | 3.80M/9.91M\n[00:10<00:09, 644kB/s]', '\\r 40%|###9      | 3.96M/9.91M [00:10<00:08,\n669kB/s]', '\\r 42%|####1     | 4.13M/9.91M [00:10<00:08, 657kB/s]', '\\r\n43%|####2     | 4.23M/9.91M [00:10<00:09, 576kB/s]', '\\r 43%|####3     |\n4.29M/9.91M [00:11<00:11, 487kB/s]', '\\r 44%|####3     | 4.36M/9.91M\n[00:11<00:13, 420kB/s]', '\\r 45%|####4     | 4.42M/9.91M [00:11<00:18,\n289kB/s]', '\\r 45%|####5     | 4.49M/9.91M [00:12<00:18, 287kB/s]', '\\r\n46%|####5     | 4.55M/9.91M [00:12<00:18, 284kB/s]', '\\r 46%|####6     |\n4.59M/9.91M [00:12<00:21, 245kB/s]', '\\r 47%|####6     | 4.62M/9.91M\n[00:12<00:24, 216kB/s]', '\\r 47%|####7     | 4.69M/9.91M [00:13<00:22,\n234kB/s]', '\\r 48%|####7     | 4.72M/9.91M [00:13<00:25, 207kB/s]', '\\r\n48%|####8     | 4.78M/9.91M [00:13<00:22, 229kB/s]', '\\r 49%|####8     |\n4.82M/9.91M [00:13<00:24, 204kB/s]', '\\r 49%|####9     | 4.88M/9.91M\n[00:14<00:22, 225kB/s]', '\\r 50%|####9     | 4.92M/9.91M [00:14<00:24,\n200kB/s]', '\\r 50%|#####     | 4.98M/9.91M [00:14<00:22, 224kB/s]', '\\r\n51%|#####     | 5.01M/9.91M [00:14<00:24, 199kB/s]', '\\r 51%|#####1    |\n5.08M/9.91M [00:14<00:21, 223kB/s]', '\\r 52%|#####1    | 5.11M/9.91M\n[00:15<00:23, 200kB/s]', '\\r 52%|#####2    | 5.18M/9.91M [00:15<00:21,\n223kB/s]', '\\r 53%|#####2    | 5.21M/9.91M [00:15<00:23, 200kB/s]', '\\r\n53%|#####3    | 5.28M/9.91M [00:15<00:20, 222kB/s]', '\\r 54%|#####3    |\n5.34M/9.91M [00:16<00:18, 241kB/s]', '\\r 55%|#####4    | 5.41M/9.91M\n[00:16<00:17, 251kB/s]', '\\r 55%|#####4    | 5.44M/9.91M [00:16<00:20,\n220kB/s]', '\\r 56%|#####5    | 5.51M/9.91M [00:16<00:18, 238kB/s]', '\\r\n56%|#####6    | 5.57M/9.91M [00:17<00:17, 251kB/s]', '\\r 57%|#####7    |\n5.67M/9.91M [00:17<00:14, 300kB/s]', '\\r 58%|#####7    | 5.73M/9.91M\n[00:17<00:14, 294kB/s]', '\\r 59%|#####8    | 5.80M/9.91M [00:17<00:14,\n292kB/s]', '\\r 60%|#####9    | 5.90M/9.91M [00:17<00:12, 331kB/s]', '\\r\n60%|######    | 6.00M/9.91M [00:18<00:11, 356kB/s]', '\\r 61%|######1   |\n6.06M/9.91M [00:18<00:11, 337kB/s]', '\\r 62%|######2   | 6.19M/9.91M\n[00:18<00:09, 404kB/s]', '\\r 63%|######3   | 6.29M/9.91M [00:18<00:08,\n410kB/s]', '\\r 65%|######4   | 6.42M/9.91M [00:19<00:07, 456kB/s]', '\\r\n66%|######6   | 6.55M/9.91M [00:19<00:06, 487kB/s]', '\\r 68%|######7   |\n6.72M/9.91M [00:19<00:05, 551kB/s]', '\\r 69%|######9   | 6.88M/9.91M\n[00:19<00:05, 595kB/s]', '\\r 71%|#######1  | 7.05M/9.91M [00:20<00:04,\n713kB/s]', '\\r 72%|#######2  | 7.14M/9.91M [00:20<00:03, 755kB/s]', '\\r\n73%|#######3  | 7.24M/9.91M [00:20<00:04, 644kB/s]', '\\r 75%|#######5  |\n7.44M/9.91M [00:20<00:03, 714kB/s]', '\\r 77%|#######7  | 7.67M/9.91M\n[00:20<00:02, 803kB/s]', '\\r 80%|#######9  | 7.93M/9.91M [00:21<00:02,\n904kB/s]', '\\r 83%|########2 | 8.19M/9.91M [00:21<00:01, 974kB/s]', '\\r\n86%|########5 | 8.49M/9.91M [00:21<00:01, 1.06MB/s]', '\\r 89%|########8 |\n8.78M/9.91M [00:21<00:01, 1.13MB/s]', '\\r 92%|#########1| 9.11M/9.91M\n[00:21<00:00, 1.21MB/s]', '\\r 96%|#########5| 9.47M/9.91M [00:22<00:00,\n1.31MB/s]', '\\r100%|#########9| 9.86M/9.91M [00:22<00:00, 1.43MB/s]', '',\n'\\r100%|##########| 9.91M/9.91M [00:22<00:00, 442kB/s] ', '\\n', 'Extracting\n./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw', '\\n', '\\n',\n'Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', '\\n',\n'Failed to download (trying next):\\nHTTP Error 404: Not Found', '\\n', '\\n',\n'Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-\nidx1-ubyte.gz', '\\n', 'Downloading https://ossci-\ndatasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to\n./data/MNIST/raw/train-labels-idx1-ubyte.gz', '\\n', '\\r  0%|          |\n0.00/28.9k [00:00<?, ?B/s]', '\\r100%|##########| 28.9k/28.9k [00:00<00:00,\n122kB/s]', '', '\\r100%|##########| 28.9k/28.9k [00:00<00:00, 121kB/s]', '\\n',\n'Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw',\n'\\n', '\\n', 'Downloading http://yann.lecun.com/exdb/mnist/t10k-images-\nidx3-ubyte.gz', '\\n', 'Failed to download (trying next):\\nHTTP Error 404: Not\nFound', '\\n', '\\n', 'Downloading https://ossci-\ndatasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz', '\\n', 'Downloading\nhttps://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to\n./data/MNIST/raw/t10k-images-idx3-ubyte.gz', '\\n', '\\r  0%|          |\n0.00/1.65M [00:00<?, ?B/s]', '\\r  2%|1         | 32.8k/1.65M [00:00<00:11,\n136kB/s]', '\\r  6%|5         | 98.3k/1.65M [00:00<00:07, 216kB/s]', '\\r 10%|9\n| 164k/1.65M [00:00<00:06, 242kB/s] ', '\\r 22%|##1       | 360k/1.65M\n[00:00<00:02, 470kB/s]', '\\r 46%|####5     | 754k/1.65M [00:01<00:01, 888kB/s]',\n'\\r 91%|#########1| 1.51M/1.65M [00:01<00:00, 1.65MB/s]', '',\n'\\r100%|##########| 1.65M/1.65M [00:01<00:00, 1.13MB/s]', '\\n', 'Extracting\n./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw', '\\n', '\\n',\n'Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', '\\n',\n'Failed to download (trying next):\\nHTTP Error 404: Not Found', '\\n', '\\n',\n'Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-\nidx1-ubyte.gz', '\\n', 'Downloading https://ossci-\ndatasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to\n./data/MNIST/raw/t10k-labels-idx1-ubyte.gz', '\\n', '\\r  0%|          |\n0.00/4.54k [00:00<?, ?B/s]', '', '\\r100%|##########| 4.54k/4.54k [00:00<00:00,\n1.82MB/s]', '\\n', 'Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to\n./data/MNIST/raw', '\\n', '\\n', 'BS 32 MLP E1: tr_loss 0.2303, val_loss 0.1094,\norig_acc 0.9659, aug_acc 0.8794', '\\n', 'BS 32 MLP E2: tr_loss 0.1011, val_loss\n0.0961, orig_acc 0.9712, aug_acc 0.8951', '\\n', 'BS 32 MLP E3: tr_loss 0.0715,\nval_loss 0.0853, orig_acc 0.9751, aug_acc 0.8980', '\\n', 'BS 32 MLP E4: tr_loss\n0.0566, val_loss 0.0896, orig_acc 0.9743, aug_acc 0.9037', '\\n', 'BS 32 MLP E5:\ntr_loss 0.0447, val_loss 0.0875, orig_acc 0.9735, aug_acc 0.8989', '\\n', 'BS 32\nCNN E1: tr_loss 0.1605, val_loss 0.0719, orig_acc 0.9762, aug_acc 0.9013', '\\n',\n'BS 32 CNN E2: tr_loss 0.0598, val_loss 0.0576, orig_acc 0.9812, aug_acc\n0.9202', '\\n', 'BS 32 CNN E3: tr_loss 0.0422, val_loss 0.0604, orig_acc 0.9802,\naug_acc 0.9079', '\\n', 'BS 32 CNN E4: tr_loss 0.0293, val_loss 0.0454, orig_acc\n0.9855, aug_acc 0.9157', '\\n', 'BS 32 CNN E5: tr_loss 0.0222, val_loss 0.0461,\norig_acc 0.9843, aug_acc 0.9193', '\\n', 'BS 64 MLP E1: tr_loss 0.2607, val_loss\n0.1392, orig_acc 0.9568, aug_acc 0.8641', '\\n', 'BS 64 MLP E2: tr_loss 0.1162,\nval_loss 0.1149, orig_acc 0.9647, aug_acc 0.8728', '\\n', 'BS 64 MLP E3: tr_loss\n0.0790, val_loss 0.0898, orig_acc 0.9717, aug_acc 0.8879', '\\n', 'BS 64 MLP E4:\ntr_loss 0.0598, val_loss 0.0812, orig_acc 0.9733, aug_acc 0.8896', '\\n', 'BS 64\nMLP E5: tr_loss 0.0468, val_loss 0.0783, orig_acc 0.9760, aug_acc 0.8983', '\\n',\n'BS 64 CNN E1: tr_loss 0.2000, val_loss 0.0977, orig_acc 0.9690, aug_acc\n0.8850', '\\n', 'BS 64 CNN E2: tr_loss 0.0729, val_loss 0.0646, orig_acc 0.9796,\naug_acc 0.9112', '\\n', 'BS 64 CNN E3: tr_loss 0.0503, val_loss 0.0508, orig_acc\n0.9817, aug_acc 0.9083', '\\n', 'BS 64 CNN E4: tr_loss 0.0385, val_loss 0.0503,\norig_acc 0.9833, aug_acc 0.9148', '\\n', 'BS 64 CNN E5: tr_loss 0.0295, val_loss\n0.0474, orig_acc 0.9842, aug_acc 0.9147', '\\n', 'BS 128 MLP E1: tr_loss 0.3029,\nval_loss 0.1674, orig_acc 0.9517, aug_acc 0.8497', '\\n', 'BS 128 MLP E2: tr_loss\n0.1286, val_loss 0.1072, orig_acc 0.9685, aug_acc 0.8781', '\\n', 'BS 128 MLP E3:\ntr_loss 0.0889, val_loss 0.0938, orig_acc 0.9720, aug_acc 0.8976', '\\n', 'BS 128\nMLP E4: tr_loss 0.0680, val_loss 0.0860, orig_acc 0.9738, aug_acc 0.8868', '\\n',\n'BS 128 MLP E5: tr_loss 0.0530, val_loss 0.0831, orig_acc 0.9756, aug_acc\n0.9041', '\\n', 'BS 128 CNN E1: tr_loss 0.2546, val_loss 0.1149, orig_acc 0.9655,\naug_acc 0.8745', '\\n', 'BS 128 CNN E2: tr_loss 0.0873, val_loss 0.0701, orig_acc\n0.9795, aug_acc 0.9093', '\\n', 'BS 128 CNN E3: tr_loss 0.0611, val_loss 0.0595,\norig_acc 0.9805, aug_acc 0.9107', '\\n', 'BS 128 CNN E4: tr_loss 0.0469, val_loss\n0.0512, orig_acc 0.9838, aug_acc 0.9168', '\\n', 'BS 128 CNN E5: tr_loss 0.0380,\nval_loss 0.0537, orig_acc 0.9817, aug_acc 0.9003', '\\n', 'BS 256 MLP E1: tr_loss\n0.3724, val_loss 0.1956, orig_acc 0.9429, aug_acc 0.8373', '\\n', 'BS 256 MLP E2:\ntr_loss 0.1640, val_loss 0.1340, orig_acc 0.9624, aug_acc 0.8638', '\\n', 'BS 256\nMLP E3: tr_loss 0.1146, val_loss 0.1151, orig_acc 0.9674, aug_acc 0.8699', '\\n',\n'BS 256 MLP E4: tr_loss 0.0883, val_loss 0.0935, orig_acc 0.9736, aug_acc\n0.8749', '\\n', 'BS 256 MLP E5: tr_loss 0.0731, val_loss 0.0939, orig_acc 0.9705,\naug_acc 0.8866', '\\n', 'BS 256 CNN E1: tr_loss 0.3049, val_loss 0.1401, orig_acc\n0.9594, aug_acc 0.8588', '\\n', 'BS 256 CNN E2: tr_loss 0.1120, val_loss 0.0923,\norig_acc 0.9713, aug_acc 0.8907', '\\n', 'BS 256 CNN E3: tr_loss 0.0756, val_loss\n0.0687, orig_acc 0.9781, aug_acc 0.8902', '\\n', 'BS 256 CNN E4: tr_loss 0.0563,\nval_loss 0.0660, orig_acc 0.9780, aug_acc 0.9072', '\\n', 'BS 256 CNN E5: tr_loss\n0.0467, val_loss 0.0552, orig_acc 0.9818, aug_acc 0.9148', '\\n', 'Execution\ntime: 9 minutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'WD 0 Epoch 1: train_loss=0.2270, val_loss=0.1086,\norig_acc=0.9664, aug_acc=0.8838, CGR=0.9227', '\\n', 'WD 0 Epoch 2:\ntrain_loss=0.0919, val_loss=0.0797, orig_acc=0.9751, aug_acc=0.8945,\nCGR=-0.3846', '\\n', 'WD 0 Epoch 3: train_loss=0.0651, val_loss=0.0675,\norig_acc=0.9788, aug_acc=0.9033, CGR=-0.0488', '\\n', 'WD 0 Epoch 4:\ntrain_loss=0.0475, val_loss=0.0658, orig_acc=0.9790, aug_acc=0.8979,\nCGR=-0.5765', '\\n', 'WD 0 Epoch 5: train_loss=0.0378, val_loss=0.0705,\norig_acc=0.9790, aug_acc=0.9089, CGR=1.0308', '\\n', 'WD 1e-05 Epoch 1:\ntrain_loss=0.2303, val_loss=0.1056, orig_acc=0.9671, aug_acc=0.8810,\nCGR=0.0599', '\\n', 'WD 1e-05 Epoch 2: train_loss=0.0923, val_loss=0.0886,\norig_acc=0.9728, aug_acc=0.8907, CGR=0.2603', '\\n', 'WD 1e-05 Epoch 3:\ntrain_loss=0.0629, val_loss=0.0760, orig_acc=0.9755, aug_acc=0.8982,\nCGR=0.6870', '\\n', 'WD 1e-05 Epoch 4: train_loss=0.0484, val_loss=0.0740,\norig_acc=0.9778, aug_acc=0.8862, CGR=0.8862', '\\n', 'WD 1e-05 Epoch 5:\ntrain_loss=0.0385, val_loss=0.0749, orig_acc=0.9773, aug_acc=0.8966,\nCGR=0.2121', '\\n', 'WD 0.0001 Epoch 1: train_loss=0.2373, val_loss=0.1082,\norig_acc=0.9670, aug_acc=0.8808, CGR=1.5841', '\\n', 'WD 0.0001 Epoch 2:\ntrain_loss=0.0946, val_loss=0.0796, orig_acc=0.9748, aug_acc=0.9001,\nCGR=0.9741', '\\n', 'WD 0.0001 Epoch 3: train_loss=0.0663, val_loss=0.0742,\norig_acc=0.9766, aug_acc=0.8928, CGR=-0.4219', '\\n', 'WD 0.0001 Epoch 4:\ntrain_loss=0.0515, val_loss=0.0646, orig_acc=0.9790, aug_acc=0.9045,\nCGR=0.2817', '\\n', 'WD 0.0001 Epoch 5: train_loss=0.0422, val_loss=0.0635,\norig_acc=0.9798, aug_acc=0.9009, CGR=1.0575', '\\n', 'WD 0.001 Epoch 1:\ntrain_loss=0.2480, val_loss=0.1253, orig_acc=0.9635, aug_acc=0.8700,\nCGR=0.2321', '\\n', 'WD 0.001 Epoch 2: train_loss=0.1080, val_loss=0.0905,\norig_acc=0.9722, aug_acc=0.8912, CGR=0.1408', '\\n', 'WD 0.001 Epoch 3:\ntrain_loss=0.0824, val_loss=0.0828, orig_acc=0.9746, aug_acc=0.8888,\nCGR=0.7607', '\\n', 'WD 0.001 Epoch 4: train_loss=0.0712, val_loss=0.0844,\norig_acc=0.9742, aug_acc=0.8911, CGR=-0.4545', '\\n', 'WD 0.001 Epoch 5:\ntrain_loss=0.0646, val_loss=0.0703, orig_acc=0.9779, aug_acc=0.9081,\nCGR=1.1154', '\\n', 'Execution time: 8 minutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Dropout 0.00: CGR = 1.2626', '\\n', 'Dropout 0.10:\nCGR = 0.6598', '\\n', 'Dropout 0.20: CGR = 0.1346', '\\n', 'Dropout 0.30: CGR =\n-0.7849', '\\n', 'Dropout 0.40: CGR = 0.2500', '\\n', 'Dropout 0.50: CGR =\n0.5481', '\\n', 'Execution time: 12 minutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '[\u03b5=0.0] Epoch 1/5 - tr_loss:0.2149,\nval_loss:0.0854, orig_acc:0.9733, aug_acc:0.8911', '\\n', '[\u03b5=0.0] Epoch 2/5 -\ntr_loss:0.0764, val_loss:0.0847, orig_acc:0.9721, aug_acc:0.8947', '\\n',\n'[\u03b5=0.0] Epoch 3/5 - tr_loss:0.0542, val_loss:0.0678, orig_acc:0.9766,\naug_acc:0.8991', '\\n', '[\u03b5=0.0] Epoch 4/5 - tr_loss:0.0404, val_loss:0.0511,\norig_acc:0.9828, aug_acc:0.9190', '\\n', '[\u03b5=0.0] Epoch 5/5 - tr_loss:0.0318,\nval_loss:0.0483, orig_acc:0.9843, aug_acc:0.9143', '\\n', '[\u03b5=0.05] Epoch 1/5 -\ntr_loss:0.5027, val_loss:0.4060, orig_acc:0.9776, aug_acc:0.8975', '\\n',\n'[\u03b5=0.05] Epoch 2/5 - tr_loss:0.3889, val_loss:0.3855, orig_acc:0.9840,\naug_acc:0.9083', '\\n', '[\u03b5=0.05] Epoch 3/5 - tr_loss:0.3699, val_loss:0.3717,\norig_acc:0.9866, aug_acc:0.9276', '\\n', '[\u03b5=0.05] Epoch 4/5 - tr_loss:0.3595,\nval_loss:0.3712, orig_acc:0.9865, aug_acc:0.9110', '\\n', '[\u03b5=0.05] Epoch 5/5 -\ntr_loss:0.3524, val_loss:0.3643, orig_acc:0.9880, aug_acc:0.9274', '\\n',\n'[\u03b5=0.1] Epoch 1/5 - tr_loss:0.7435, val_loss:0.6457, orig_acc:0.9777,\naug_acc:0.9024', '\\n', '[\u03b5=0.1] Epoch 2/5 - tr_loss:0.6346, val_loss:0.6296,\norig_acc:0.9821, aug_acc:0.9235', '\\n', '[\u03b5=0.1] Epoch 3/5 - tr_loss:0.6172,\nval_loss:0.6183, orig_acc:0.9836, aug_acc:0.9170', '\\n', '[\u03b5=0.1] Epoch 4/5 -\ntr_loss:0.6068, val_loss:0.6092, orig_acc:0.9865, aug_acc:0.9211', '\\n',\n'[\u03b5=0.1] Epoch 5/5 - tr_loss:0.5997, val_loss:0.6083, orig_acc:0.9871,\naug_acc:0.9249', '\\n', '[\u03b5=0.2] Epoch 1/5 - tr_loss:1.0905, val_loss:1.0201,\norig_acc:0.9805, aug_acc:0.8982', '\\n', '[\u03b5=0.2] Epoch 2/5 - tr_loss:1.0121,\nval_loss:1.0054, orig_acc:0.9834, aug_acc:0.9056', '\\n', '[\u03b5=0.2] Epoch 3/5 -\ntr_loss:0.9979, val_loss:0.9984, orig_acc:0.9866, aug_acc:0.9165', '\\n',\n'[\u03b5=0.2] Epoch 4/5 - tr_loss:0.9903, val_loss:0.9981, orig_acc:0.9863,\naug_acc:0.9030', '\\n', '[\u03b5=0.2] Epoch 5/5 - tr_loss:0.9853, val_loss:0.9969,\norig_acc:0.9867, aug_acc:0.8989', '\\n', 'Execution time: 5 minutes seconds (time\nlimit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 39, in <module>\\n    kmnist_ds = load_dataset(\"kmnist\",\nsplit=\"test\")\\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1652, in dataset_module_factory\\n    raise e1\nfrom None\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1578, in dataset_module_factory\\n    raise\nDatasetNotFoundError(f\"Dataset \\'{path}\\' doesn\\'t exist on the Hub or cannot be\naccessed.\") from e\\ndatasets.exceptions.DatasetNotFoundError: Dataset \\'kmnist\\'\ndoesn\\'t exist on the Hub or cannot be accessed.\\n', 'Execution time: 10 seconds\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\rMap:   0%|          | 0/10000 [00:00<?, ?\nexamples/s]', '\\rMap:   2%|2         | 242/10000 [00:00<00:04, 2387.49\nexamples/s]', '\\rMap:   5%|5         | 539/10000 [00:00<00:03, 2723.03\nexamples/s]', '\\rMap:   8%|8         | 839/10000 [00:00<00:03, 2845.03\nexamples/s]', '\\rMap:  12%|#1        | 1151/10000 [00:00<00:05, 1496.93\nexamples/s]', '\\rMap:  15%|#4        | 1455/10000 [00:00<00:04, 1837.86\nexamples/s]', '\\rMap:  18%|#7        | 1759/10000 [00:00<00:03, 2125.42\nexamples/s]', '\\rMap:  21%|##1       | 2148/10000 [00:01<00:05, 1562.11\nexamples/s]', '\\rMap:  25%|##4       | 2452/10000 [00:01<00:04, 1828.67\nexamples/s]', '\\rMap:  28%|##7       | 2756/10000 [00:01<00:03, 2076.88\nexamples/s]', '\\rMap:  31%|###1      | 3149/10000 [00:01<00:04, 1589.42\nexamples/s]', '\\rMap:  35%|###4      | 3454/10000 [00:01<00:03, 1837.75\nexamples/s]', '\\rMap:  38%|###7      | 3758/10000 [00:01<00:03, 2072.16\nexamples/s]', '\\rMap:  42%|####1     | 4150/10000 [00:02<00:04, 1239.91\nexamples/s]', '\\rMap:  45%|####4     | 4453/10000 [00:02<00:03, 1481.57\nexamples/s]', '\\rMap:  48%|####7     | 4756/10000 [00:02<00:03, 1732.05\nexamples/s]', '\\rMap:  51%|#####1    | 5147/10000 [00:03<00:03, 1451.70\nexamples/s]', '\\rMap:  54%|#####4    | 5449/10000 [00:03<00:02, 1692.49\nexamples/s]', '\\rMap:  58%|#####7    | 5751/10000 [00:03<00:02, 1930.14\nexamples/s]', '\\rMap:  62%|######1   | 6151/10000 [00:03<00:02, 1551.42\nexamples/s]', '\\rMap:  65%|######4   | 6454/10000 [00:03<00:01, 1789.80\nexamples/s]', '\\rMap:  67%|######7   | 6747/10000 [00:03<00:01, 2001.21\nexamples/s]', '\\rMap:  72%|#######1  | 7150/10000 [00:04<00:01, 1588.97\nexamples/s]', '\\rMap:  75%|#######4  | 7454/10000 [00:04<00:01, 1826.95\nexamples/s]', '\\rMap:  78%|#######7  | 7759/10000 [00:04<00:01, 2057.67\nexamples/s]', '\\rMap:  82%|########1 | 8151/10000 [00:04<00:01, 1609.57\nexamples/s]', '\\rMap:  85%|########4 | 8455/10000 [00:04<00:00, 1846.77\nexamples/s]', '\\rMap:  87%|########7 | 8745/10000 [00:04<00:00, 2047.81\nexamples/s]', '\\rMap:  92%|#########1| 9151/10000 [00:05<00:00, 1599.64\nexamples/s]', '\\rMap:  95%|#########4| 9455/10000 [00:05<00:00, 1837.36\nexamples/s]', '\\rMap:  98%|#########7| 9760/10000 [00:05<00:00, 2067.59\nexamples/s]', '', '\\rMap: 100%|##########| 10000/10000 [00:05<00:00, 1712.16\nexamples/s]', '\\n', 'Traceback (most recent call last):\\n  File \"runfile.py\",\nline 51, in <module>\\n    kmnist_ds = load_dataset(\"kmnist\",\nsplit=\"test\").map(hf_transform)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1652, in dataset_module_factory\\n    raise e1\nfrom None\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1578, in dataset_module_factory\\n    raise\nDatasetNotFoundError(f\"Dataset \\'{path}\\' doesn\\'t exist on the Hub or cannot be\naccessed.\") from e\\ndatasets.exceptions.DatasetNotFoundError: Dataset \\'kmnist\\'\ndoesn\\'t exist on the Hub or cannot be accessed.\\n', 'Execution time: 16 seconds\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 46, in <module>\\n    kuz_ds = load_dataset(\"kuzushiji_mnist\",\nsplit=\"test\")\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1652, in dataset_module_factory\\n    raise e1\nfrom None\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1578, in dataset_module_factory\\n    raise\nDatasetNotFoundError(f\"Dataset \\'{path}\\' doesn\\'t exist on the Hub or cannot be\naccessed.\") from e\\ndatasets.exceptions.DatasetNotFoundError: Dataset\n\\'kuzushiji_mnist\\' doesn\\'t exist on the Hub or cannot be accessed.\\n',\n'Execution time: 10 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\rMap:   0%|          | 0/10000 [00:00<?, ?\nexamples/s]', '\\rMap:   2%|2         | 243/10000 [00:00<00:04, 2398.30\nexamples/s]', '\\rMap:   5%|5         | 526/10000 [00:00<00:03, 2647.01\nexamples/s]', '\\rMap:   8%|8         | 809/10000 [00:00<00:03, 2727.06\nexamples/s]', '\\rMap:  11%|#1        | 1141/10000 [00:00<00:06, 1467.08\nexamples/s]', '\\rMap:  14%|#4        | 1417/10000 [00:00<00:04, 1747.35\nexamples/s]', '\\rMap:  17%|#7        | 1700/10000 [00:00<00:04, 2001.62\nexamples/s]', '\\rMap:  20%|#9        | 1982/10000 [00:00<00:03, 2205.26\nexamples/s]', '\\rMap:  23%|##2       | 2282/10000 [00:01<00:05, 1509.92\nexamples/s]', '\\rMap:  25%|##5       | 2526/10000 [00:01<00:05, 1271.67\nexamples/s]', '\\rMap:  28%|##8       | 2810/10000 [00:01<00:04, 1538.48\nexamples/s]', '\\rMap:  31%|###1      | 3139/10000 [00:02<00:05, 1279.32\nexamples/s]', '\\rMap:  34%|###4      | 3423/10000 [00:02<00:04, 1527.88\nexamples/s]', '\\rMap:  37%|###7      | 3708/10000 [00:02<00:03, 1772.62\nexamples/s]', '\\rMap:  40%|###9      | 3992/10000 [00:02<00:03, 1994.70\nexamples/s]', '\\rMap:  43%|####2     | 4283/10000 [00:02<00:03, 1464.54\nexamples/s]', '\\rMap:  46%|####5     | 4556/10000 [00:02<00:03, 1690.52\nexamples/s]', '\\rMap:  48%|####8     | 4837/10000 [00:02<00:02, 1917.96\nexamples/s]', '\\rMap:  51%|#####1    | 5142/10000 [00:03<00:03, 1432.60\nexamples/s]', '\\rMap:  54%|#####4    | 5427/10000 [00:03<00:02, 1679.83\nexamples/s]', '\\rMap:  57%|#####7    | 5712/10000 [00:03<00:02, 1912.96\nexamples/s]', '\\rMap:  60%|#####9    | 5997/10000 [00:03<00:01, 2119.04\nexamples/s]', '\\rMap:  63%|######2   | 6282/10000 [00:03<00:02, 1499.07\nexamples/s]', '\\rMap:  66%|######5   | 6567/10000 [00:03<00:01, 1745.56\nexamples/s]', '\\rMap:  69%|######8   | 6851/10000 [00:03<00:01, 1971.29\nexamples/s]', '\\rMap:  71%|#######1  | 7141/10000 [00:04<00:01, 1456.58\nexamples/s]', '\\rMap:  74%|#######4  | 7426/10000 [00:04<00:01, 1704.05\nexamples/s]', '\\rMap:  77%|#######7  | 7710/10000 [00:04<00:01, 1927.78\nexamples/s]', '\\rMap:  80%|#######9  | 7988/10000 [00:04<00:00, 2116.53\nexamples/s]', '\\rMap:  83%|########2 | 8284/10000 [00:04<00:01, 1501.44\nexamples/s]', '\\rMap:  86%|########5 | 8569/10000 [00:05<00:00, 1746.50\nexamples/s]', '\\rMap:  89%|########8 | 8853/10000 [00:05<00:00, 1971.68\nexamples/s]', '\\rMap:  91%|#########1| 9141/10000 [00:05<00:00, 1456.59\nexamples/s]', '\\rMap:  94%|#########4| 9418/10000 [00:05<00:00, 1690.64\nexamples/s]', '\\rMap:  97%|#########7| 9702/10000 [00:05<00:00, 1923.67\nexamples/s]', '\\rMap: 100%|#########9| 9988/10000 [00:05<00:00, 2132.35\nexamples/s]', '', '\\rMap: 100%|##########| 10000/10000 [00:06<00:00, 1658.31\nexamples/s]', '\\n', 'Traceback (most recent call last):\\n  File \"runfile.py\",\nline 63, in <module>\\n    kuzushiji_ds = load_dataset(\"kuzushiji_mnist\",\nsplit=\"test\").map(map_hf)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1652, in dataset_module_factory\\n    raise e1\nfrom None\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1578, in dataset_module_factory\\n    raise\nDatasetNotFoundError(f\"Dataset \\'{path}\\' doesn\\'t exist on the Hub or cannot be\naccessed.\") from e\\ndatasets.exceptions.DatasetNotFoundError: Dataset\n\\'kuzushiji_mnist\\' doesn\\'t exist on the Hub or cannot be accessed.\\n',\n'Execution time: 16 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 39, in <module>\\n    hf_kmnist = load_dataset(\"kmnist\",\n\"balanced\")\\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1652, in dataset_module_factory\\n    raise e1\nfrom None\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1578, in dataset_module_factory\\n    raise\nDatasetNotFoundError(f\"Dataset \\'{path}\\' doesn\\'t exist on the Hub or cannot be\naccessed.\") from e\\ndatasets.exceptions.DatasetNotFoundError: Dataset \\'kmnist\\'\ndoesn\\'t exist on the Hub or cannot be accessed.\\n', 'Execution time: 10 seconds\nseconds (time limit is an hour).']", "['beta1=0.8 Epoch 1/5 CGR=0.7574', '\\n', 'beta1=0.8 Epoch 2/5 CGR=-0.9635',\n'\\n', 'beta1=0.8 Epoch 3/5 CGR=1.0702', '\\n', 'beta1=0.8 Epoch 4/5 CGR=-0.6200',\n'\\n', 'beta1=0.8 Epoch 5/5 CGR=1.5941', '\\n', 'beta1=0.85 Epoch 1/5\nCGR=-0.2105', '\\n', 'beta1=0.85 Epoch 2/5 CGR=-0.9551', '\\n', 'beta1=0.85 Epoch\n3/5 CGR=0.2308', '\\n', 'beta1=0.85 Epoch 4/5 CGR=1.0752', '\\n', 'beta1=0.85\nEpoch 5/5 CGR=-0.0104', '\\n', 'beta1=0.95 Epoch 1/5 CGR=1.5221', '\\n',\n'beta1=0.95 Epoch 2/5 CGR=-0.0654', '\\n', 'beta1=0.95 Epoch 3/5 CGR=0.3333',\n'\\n', 'beta1=0.95 Epoch 4/5 CGR=0.6337', '\\n', 'beta1=0.95 Epoch 5/5\nCGR=-0.5189', '\\n', 'Execution time: 7 minutes seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', '\\n=== Running with n_epochs = 5 ===', '\\n', 'MLP\nEpoch 1: val_loss=0.1322, orig_acc=0.9594, aug_acc=0.8612', '\\n', 'CNN Epoch 1:\nval_loss=0.0755, orig_acc=0.9764, aug_acc=0.8966', '\\n', 'Epoch 1: CGR=1.0824',\n'\\n', 'MLP Epoch 2: val_loss=0.1064, orig_acc=0.9674, aug_acc=0.8860', '\\n',\n'CNN Epoch 2: val_loss=0.0616, orig_acc=0.9802, aug_acc=0.9069', '\\n', 'Epoch 2:\nCGR=0.6328', '\\n', 'MLP Epoch 3: val_loss=0.0829, orig_acc=0.9735,\naug_acc=0.8911', '\\n', 'CNN Epoch 3: val_loss=0.0559, orig_acc=0.9817,\naug_acc=0.9175', '\\n', 'Epoch 3: CGR=2.2195', '\\n', 'MLP Epoch 4:\nval_loss=0.0777, orig_acc=0.9761, aug_acc=0.8970', '\\n', 'CNN Epoch 4:\nval_loss=0.0513, orig_acc=0.9834, aug_acc=0.9137', '\\n', 'Epoch 4: CGR=1.2877',\n'\\n', 'MLP Epoch 5: val_loss=0.0834, orig_acc=0.9748, aug_acc=0.8958', '\\n',\n'CNN Epoch 5: val_loss=0.0479, orig_acc=0.9857, aug_acc=0.9213', '\\n', 'Epoch 5:\nCGR=1.3394', '\\n', '\\n=== Running with n_epochs = 10 ===', '\\n', 'MLP Epoch 1:\nval_loss=0.1431, orig_acc=0.9553, aug_acc=0.8605', '\\n', 'CNN Epoch 1:\nval_loss=0.0778, orig_acc=0.9749, aug_acc=0.9056', '\\n', 'Epoch 1: CGR=1.3010',\n'\\n', 'MLP Epoch 2: val_loss=0.0985, orig_acc=0.9698, aug_acc=0.8854', '\\n',\n'CNN Epoch 2: val_loss=0.0586, orig_acc=0.9807, aug_acc=0.9120', '\\n', 'Epoch 2:\nCGR=1.4404', '\\n', 'MLP Epoch 3: val_loss=0.0829, orig_acc=0.9749,\naug_acc=0.8993', '\\n', 'CNN Epoch 3: val_loss=0.0514, orig_acc=0.9835,\naug_acc=0.9158', '\\n', 'Epoch 3: CGR=0.9186', '\\n', 'MLP Epoch 4:\nval_loss=0.0806, orig_acc=0.9747, aug_acc=0.8942', '\\n', 'CNN Epoch 4:\nval_loss=0.0534, orig_acc=0.9828, aug_acc=0.9193', '\\n', 'Epoch 4: CGR=2.0988',\n'\\n', 'MLP Epoch 5: val_loss=0.0814, orig_acc=0.9758, aug_acc=0.8958', '\\n',\n'CNN Epoch 5: val_loss=0.0613, orig_acc=0.9805, aug_acc=0.8864', '\\n', 'Epoch 5:\nCGR=1.0000', '\\n', 'MLP Epoch 6: val_loss=0.0745, orig_acc=0.9771,\naug_acc=0.9047', '\\n', 'CNN Epoch 6: val_loss=0.0560, orig_acc=0.9823,\naug_acc=0.9098', '\\n', 'Epoch 6: CGR=-0.0192', '\\n', 'MLP Epoch 7:\nval_loss=0.0793, orig_acc=0.9756, aug_acc=0.9030', '\\n', 'CNN Epoch 7:\nval_loss=0.0487, orig_acc=0.9846, aug_acc=0.9132', '\\n', 'Epoch 7: CGR=0.1333',\n'\\n', 'MLP Epoch 8: val_loss=0.0787, orig_acc=0.9774, aug_acc=0.9077', '\\n',\n'CNN Epoch 8: val_loss=0.0504, orig_acc=0.9847, aug_acc=0.9192', '\\n', 'Epoch 8:\nCGR=0.5753', '\\n', 'MLP Epoch 9: val_loss=0.0833, orig_acc=0.9783,\naug_acc=0.9037', '\\n', 'CNN Epoch 9: val_loss=0.0505, orig_acc=0.9849,\naug_acc=0.9134', '\\n', 'Epoch 9: CGR=0.4697', '\\n', 'MLP Epoch 10:\nval_loss=0.0884, orig_acc=0.9773, aug_acc=0.8985', '\\n', 'CNN Epoch 10:\nval_loss=0.0550, orig_acc=0.9864, aug_acc=0.9263', '\\n', 'Epoch 10: CGR=2.0549',\n'\\n', '\\n=== Running with n_epochs = 15 ===', '\\n', 'MLP Epoch 1:\nval_loss=0.1417, orig_acc=0.9558, aug_acc=0.8717', '\\n', 'CNN Epoch 1:\nval_loss=0.1014, orig_acc=0.9666, aug_acc=0.8878', '\\n', 'Epoch 1: CGR=0.4907',\n'\\n', 'MLP Epoch 2: val_loss=0.0997, orig_acc=0.9703, aug_acc=0.8882', '\\n',\n'CNN Epoch 2: val_loss=0.0557, orig_acc=0.9820, aug_acc=0.9035', '\\n', 'Epoch 2:\nCGR=0.3077', '\\n', 'MLP Epoch 3: val_loss=0.0941, orig_acc=0.9709,\naug_acc=0.8864', '\\n', 'CNN Epoch 3: val_loss=0.0570, orig_acc=0.9807,\naug_acc=0.9095', '\\n', 'Epoch 3: CGR=1.3571', '\\n', 'MLP Epoch 4:\nval_loss=0.0810, orig_acc=0.9758, aug_acc=0.8988', '\\n', 'CNN Epoch 4:\nval_loss=0.0588, orig_acc=0.9819, aug_acc=0.9097', '\\n', 'Epoch 4: CGR=0.7869',\n'\\n', 'MLP Epoch 5: val_loss=0.0787, orig_acc=0.9741, aug_acc=0.8988', '\\n',\n'CNN Epoch 5: val_loss=0.0471, orig_acc=0.9851, aug_acc=0.9086', '\\n', 'Epoch 5:\nCGR=-0.1091', '\\n', 'MLP Epoch 6: val_loss=0.0905, orig_acc=0.9726,\naug_acc=0.8988', '\\n', 'CNN Epoch 6: val_loss=0.0516, orig_acc=0.9838,\naug_acc=0.9140', '\\n', 'Epoch 6: CGR=0.3571', '\\n', 'MLP Epoch 7:\nval_loss=0.0730, orig_acc=0.9783, aug_acc=0.8967', '\\n', 'CNN Epoch 7:\nval_loss=0.0536, orig_acc=0.9842, aug_acc=0.9021', '\\n', 'Epoch 7: CGR=-0.0847',\n'\\n', 'MLP Epoch 8: val_loss=0.0800, orig_acc=0.9775, aug_acc=0.8944', '\\n',\n'CNN Epoch 8: val_loss=0.0552, orig_acc=0.9834, aug_acc=0.9084', '\\n', 'Epoch 8:\nCGR=1.3729', '\\n', 'MLP Epoch 9: val_loss=0.0827, orig_acc=0.9776,\naug_acc=0.8990', '\\n', 'CNN Epoch 9: val_loss=0.0562, orig_acc=0.9853,\naug_acc=0.9114', '\\n', 'Epoch 9: CGR=0.6104', '\\n', 'MLP Epoch 10:\nval_loss=0.0913, orig_acc=0.9758, aug_acc=0.8932', '\\n', 'CNN Epoch 10:\nval_loss=0.0553, orig_acc=0.9850, aug_acc=0.9136', '\\n', 'Epoch 10: CGR=1.2174',\n'\\n', 'MLP Epoch 11: val_loss=0.0789, orig_acc=0.9798, aug_acc=0.8985', '\\n',\n'CNN Epoch 11: val_loss=0.0723, orig_acc=0.9820, aug_acc=0.9056', '\\n', 'Epoch\n11: CGR=2.2273', '\\n', 'MLP Epoch 12: val_loss=0.1132, orig_acc=0.9732,\naug_acc=0.8862', '\\n', 'CNN Epoch 12: val_loss=0.0546, orig_acc=0.9863,\naug_acc=0.9049', '\\n', 'Epoch 12: CGR=0.4275', '\\n', 'MLP Epoch 13:\nval_loss=0.0977, orig_acc=0.9768, aug_acc=0.8981', '\\n', 'CNN Epoch 13:\nval_loss=0.0650, orig_acc=0.9845, aug_acc=0.9101', '\\n', 'Epoch 13: CGR=0.5584',\n'\\n', 'MLP Epoch 14: val_loss=0.1004, orig_acc=0.9767, aug_acc=0.8995', '\\n',\n'CNN Epoch 14: val_loss=0.0612, orig_acc=0.9862, aug_acc=0.9182', '\\n', 'Epoch\n14: CGR=0.9684', '\\n', 'MLP Epoch 15: val_loss=0.0959, orig_acc=0.9784,\naug_acc=0.9047', '\\n', 'CNN Epoch 15: val_loss=0.0773, orig_acc=0.9831,\naug_acc=0.9085', '\\n', 'Epoch 15: CGR=-0.1915', '\\n', '\\n=== Running with\nn_epochs = 20 ===', '\\n', 'MLP Epoch 1: val_loss=0.1358, orig_acc=0.9609,\naug_acc=0.8643', '\\n', 'CNN Epoch 1: val_loss=0.0869, orig_acc=0.9718,\naug_acc=0.8771', '\\n', 'Epoch 1: CGR=0.1743', '\\n', 'MLP Epoch 2:\nval_loss=0.0990, orig_acc=0.9697, aug_acc=0.8877', '\\n', 'CNN Epoch 2:\nval_loss=0.0672, orig_acc=0.9790, aug_acc=0.9050', '\\n', 'Epoch 2: CGR=0.8602',\n'\\n', 'MLP Epoch 3: val_loss=0.0931, orig_acc=0.9719, aug_acc=0.8845', '\\n',\n'CNN Epoch 3: val_loss=0.0502, orig_acc=0.9842, aug_acc=0.9121', '\\n', 'Epoch 3:\nCGR=1.2439', '\\n', 'MLP Epoch 4: val_loss=0.0805, orig_acc=0.9778,\naug_acc=0.8919', '\\n', 'CNN Epoch 4: val_loss=0.0540, orig_acc=0.9830,\naug_acc=0.9104', '\\n', 'Epoch 4: CGR=2.5577', '\\n', 'MLP Epoch 5:\nval_loss=0.0796, orig_acc=0.9762, aug_acc=0.9063', '\\n', 'CNN Epoch 5:\nval_loss=0.0521, orig_acc=0.9835, aug_acc=0.9041', '\\n', 'Epoch 5: CGR=-0.6986',\n'\\n', 'MLP Epoch 6: val_loss=0.0885, orig_acc=0.9733, aug_acc=0.9004', '\\n',\n'CNN Epoch 6: val_loss=0.0534, orig_acc=0.9838, aug_acc=0.9020', '\\n', 'Epoch 6:\nCGR=-0.8476', '\\n', 'MLP Epoch 7: val_loss=0.0806, orig_acc=0.9776,\naug_acc=0.9080', '\\n', 'CNN Epoch 7: val_loss=0.0613, orig_acc=0.9822,\naug_acc=0.9081', '\\n', 'Epoch 7: CGR=-0.9783', '\\n', 'MLP Epoch 8:\nval_loss=0.0894, orig_acc=0.9756, aug_acc=0.8962', '\\n', 'CNN Epoch 8:\nval_loss=0.0653, orig_acc=0.9837, aug_acc=0.9088', '\\n', 'Epoch 8: CGR=0.5556',\n'\\n', 'MLP Epoch 9: val_loss=0.0921, orig_acc=0.9756, aug_acc=0.9047', '\\n',\n'CNN Epoch 9: val_loss=0.0601, orig_acc=0.9835, aug_acc=0.9127', '\\n', 'Epoch 9:\nCGR=0.0127', '\\n', 'MLP Epoch 10: val_loss=0.0924, orig_acc=0.9756,\naug_acc=0.9008', '\\n', 'CNN Epoch 10: val_loss=0.0565, orig_acc=0.9865,\naug_acc=0.9149', '\\n', 'Epoch 10: CGR=0.2936', '\\n', 'MLP Epoch 11:\nval_loss=0.1077, orig_acc=0.9735, aug_acc=0.8869', '\\n', 'CNN Epoch 11:\nval_loss=0.0687, orig_acc=0.9842, aug_acc=0.9132', '\\n', 'Epoch 11: CGR=1.4579',\n'\\n', 'MLP Epoch 12: val_loss=0.0910, orig_acc=0.9785, aug_acc=0.9037', '\\n',\n'CNN Epoch 12: val_loss=0.0716, orig_acc=0.9830, aug_acc=0.9164', '\\n', 'Epoch\n12: CGR=1.8222', '\\n', 'MLP Epoch 13: val_loss=0.1066, orig_acc=0.9772,\naug_acc=0.8994', '\\n', 'CNN Epoch 13: val_loss=0.0615, orig_acc=0.9852,\naug_acc=0.9171', '\\n', 'Epoch 13: CGR=1.2125', '\\n', 'MLP Epoch 14:\nval_loss=0.1036, orig_acc=0.9777, aug_acc=0.9053', '\\n', 'CNN Epoch 14:\nval_loss=0.0738, orig_acc=0.9843, aug_acc=0.9222', '\\n', 'Epoch 14: CGR=1.5606',\n'\\n', 'MLP Epoch 15: val_loss=0.1042, orig_acc=0.9782, aug_acc=0.9049', '\\n',\n'CNN Epoch 15: val_loss=0.0716, orig_acc=0.9854, aug_acc=0.9086', '\\n', 'Epoch\n15: CGR=-0.4861', '\\n', 'MLP Epoch 16: val_loss=0.1080, orig_acc=0.9754,\naug_acc=0.8973', '\\n', 'CNN Epoch 16: val_loss=0.0791, orig_acc=0.9833,\naug_acc=0.9157', '\\n', 'Epoch 16: CGR=1.3291', '\\n', 'MLP Epoch 17:\nval_loss=0.0969, orig_acc=0.9807, aug_acc=0.9011', '\\n', 'CNN Epoch 17:\nval_loss=0.0806, orig_acc=0.9854, aug_acc=0.9112', '\\n', 'Epoch 17: CGR=1.1489',\n'\\n', 'MLP Epoch 18: val_loss=0.1313, orig_acc=0.9729, aug_acc=0.8924', '\\n',\n'CNN Epoch 18: val_loss=0.0802, orig_acc=0.9838, aug_acc=0.8973', '\\n', 'Epoch\n18: CGR=-0.5505', '\\n', 'MLP Epoch 19: val_loss=0.1108, orig_acc=0.9769,\naug_acc=0.9036', '\\n', 'CNN Epoch 19: val_loss=0.0903, orig_acc=0.9818,\naug_acc=0.9158', '\\n', 'Epoch 19: CGR=1.4898', '\\n', 'MLP Epoch 20:\nval_loss=0.1144, orig_acc=0.9791, aug_acc=0.9039', '\\n', 'CNN Epoch 20:\nval_loss=0.0759, orig_acc=0.9859, aug_acc=0.9156', '\\n', 'Epoch 20: CGR=0.7206',\n'\\n', 'Execution time: 22 minutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Running with n_epochs = 5 ===', '\\n', 'MLP\nEpoch 1: val_loss=0.1322, orig_acc=0.9594, aug_acc=0.8612', '\\n', 'CNN Epoch 1:\nval_loss=0.0755, orig_acc=0.9764, aug_acc=0.8966', '\\n', 'Epoch 1: CGR=1.0824',\n'\\n', 'MLP Epoch 2: val_loss=0.1064, orig_acc=0.9674, aug_acc=0.8860', '\\n',\n'CNN Epoch 2: val_loss=0.0609, orig_acc=0.9806, aug_acc=0.9100', '\\n', 'Epoch 2:\nCGR=0.8182', '\\n', 'MLP Epoch 3: val_loss=0.0829, orig_acc=0.9735,\naug_acc=0.8911', '\\n', 'CNN Epoch 3: val_loss=0.0563, orig_acc=0.9818,\naug_acc=0.9177', '\\n', 'Epoch 3: CGR=2.2048', '\\n', 'MLP Epoch 4:\nval_loss=0.0777, orig_acc=0.9761, aug_acc=0.8970', '\\n', 'CNN Epoch 4:\nval_loss=0.0490, orig_acc=0.9839, aug_acc=0.9149', '\\n', 'Epoch 4: CGR=1.2949',\n'\\n', 'MLP Epoch 5: val_loss=0.0834, orig_acc=0.9748, aug_acc=0.8958', '\\n',\n'CNN Epoch 5: val_loss=0.0531, orig_acc=0.9841, aug_acc=0.9207', '\\n', 'Epoch 5:\nCGR=1.6774', '\\n', '\\n=== Running with n_epochs = 10 ===', '\\n', 'MLP Epoch 1:\nval_loss=0.1431, orig_acc=0.9553, aug_acc=0.8605', '\\n', 'CNN Epoch 1:\nval_loss=0.0778, orig_acc=0.9749, aug_acc=0.9056', '\\n', 'Epoch 1: CGR=1.3010',\n'\\n', 'MLP Epoch 2: val_loss=0.0985, orig_acc=0.9698, aug_acc=0.8854', '\\n',\n'CNN Epoch 2: val_loss=0.0586, orig_acc=0.9807, aug_acc=0.9120', '\\n', 'Epoch 2:\nCGR=1.4404', '\\n', 'MLP Epoch 3: val_loss=0.0829, orig_acc=0.9749,\naug_acc=0.8993', '\\n', 'CNN Epoch 3: val_loss=0.0515, orig_acc=0.9834,\naug_acc=0.9157', '\\n', 'Epoch 3: CGR=0.9294', '\\n', 'MLP Epoch 4:\nval_loss=0.0806, orig_acc=0.9747, aug_acc=0.8942', '\\n', 'CNN Epoch 4:\nval_loss=0.0511, orig_acc=0.9826, aug_acc=0.9194', '\\n', 'Epoch 4: CGR=2.1899',\n'\\n', 'MLP Epoch 5: val_loss=0.0814, orig_acc=0.9758, aug_acc=0.8958', '\\n',\n'CNN Epoch 5: val_loss=0.0589, orig_acc=0.9804, aug_acc=0.8889', '\\n', 'Epoch 5:\nCGR=0.5000', '\\n', 'MLP Epoch 6: val_loss=0.0745, orig_acc=0.9771,\naug_acc=0.9047', '\\n', 'CNN Epoch 6: val_loss=0.0545, orig_acc=0.9826,\naug_acc=0.9112', '\\n', 'Epoch 6: CGR=0.1818', '\\n', 'MLP Epoch 7:\nval_loss=0.0793, orig_acc=0.9756, aug_acc=0.9030', '\\n', 'CNN Epoch 7:\nval_loss=0.0513, orig_acc=0.9853, aug_acc=0.9135', '\\n', 'Epoch 7: CGR=0.0825',\n'\\n', 'MLP Epoch 8: val_loss=0.0787, orig_acc=0.9774, aug_acc=0.9077', '\\n',\n'CNN Epoch 8: val_loss=0.0499, orig_acc=0.9848, aug_acc=0.9187', '\\n', 'Epoch 8:\nCGR=0.4865', '\\n', 'MLP Epoch 9: val_loss=0.0833, orig_acc=0.9783,\naug_acc=0.9037', '\\n', 'CNN Epoch 9: val_loss=0.0502, orig_acc=0.9855,\naug_acc=0.9137', '\\n', 'Epoch 9: CGR=0.3889', '\\n', 'MLP Epoch 10:\nval_loss=0.0884, orig_acc=0.9773, aug_acc=0.8985', '\\n', 'CNN Epoch 10:\nval_loss=0.0549, orig_acc=0.9846, aug_acc=0.9086', '\\n', 'Epoch 10: CGR=0.3836',\n'\\n', '\\n=== Running with n_epochs = 15 ===', '\\n', 'MLP Epoch 1:\nval_loss=0.1417, orig_acc=0.9558, aug_acc=0.8717', '\\n', 'CNN Epoch 1:\nval_loss=0.1014, orig_acc=0.9666, aug_acc=0.8878', '\\n', 'Epoch 1: CGR=0.4907',\n'\\n', 'MLP Epoch 2: val_loss=0.0997, orig_acc=0.9703, aug_acc=0.8882', '\\n',\n'CNN Epoch 2: val_loss=0.0558, orig_acc=0.9823, aug_acc=0.9029', '\\n', 'Epoch 2:\nCGR=0.2250', '\\n', 'MLP Epoch 3: val_loss=0.0941, orig_acc=0.9709,\naug_acc=0.8864', '\\n', 'CNN Epoch 3: val_loss=0.0532, orig_acc=0.9819,\naug_acc=0.9128', '\\n', 'Epoch 3: CGR=1.4000', '\\n', 'MLP Epoch 4:\nval_loss=0.0810, orig_acc=0.9758, aug_acc=0.8988', '\\n', 'CNN Epoch 4:\nval_loss=0.0601, orig_acc=0.9811, aug_acc=0.9076', '\\n', 'Epoch 4: CGR=0.6604',\n'\\n', 'MLP Epoch 5: val_loss=0.0787, orig_acc=0.9741, aug_acc=0.8988', '\\n',\n'CNN Epoch 5: val_loss=0.0480, orig_acc=0.9852, aug_acc=0.9089', '\\n', 'Epoch 5:\nCGR=-0.0901', '\\n', 'MLP Epoch 6: val_loss=0.0905, orig_acc=0.9726,\naug_acc=0.8988', '\\n', 'CNN Epoch 6: val_loss=0.0528, orig_acc=0.9832,\naug_acc=0.9157', '\\n', 'Epoch 6: CGR=0.5943', '\\n', 'MLP Epoch 7:\nval_loss=0.0730, orig_acc=0.9783, aug_acc=0.8967', '\\n', 'CNN Epoch 7:\nval_loss=0.0548, orig_acc=0.9834, aug_acc=0.9027', '\\n', 'Epoch 7: CGR=0.1765',\n'\\n', 'MLP Epoch 8: val_loss=0.0800, orig_acc=0.9775, aug_acc=0.8944', '\\n',\n'CNN Epoch 8: val_loss=0.0546, orig_acc=0.9843, aug_acc=0.9108', '\\n', 'Epoch 8:\nCGR=1.4118', '\\n', 'MLP Epoch 9: val_loss=0.0827, orig_acc=0.9776,\naug_acc=0.8990', '\\n', 'CNN Epoch 9: val_loss=0.0624, orig_acc=0.9815,\naug_acc=0.8979', '\\n', 'Epoch 9: CGR=-0.7179', '\\n', 'MLP Epoch 10:\nval_loss=0.0913, orig_acc=0.9758, aug_acc=0.8932', '\\n', 'CNN Epoch 10:\nval_loss=0.0644, orig_acc=0.9829, aug_acc=0.9143', '\\n', 'Epoch 10: CGR=1.9718',\n'\\n', 'MLP Epoch 11: val_loss=0.0789, orig_acc=0.9798, aug_acc=0.8985', '\\n',\n'CNN Epoch 11: val_loss=0.0732, orig_acc=0.9830, aug_acc=0.9102', '\\n', 'Epoch\n11: CGR=2.6562', '\\n', 'MLP Epoch 12: val_loss=0.1132, orig_acc=0.9732,\naug_acc=0.8862', '\\n', 'CNN Epoch 12: val_loss=0.0690, orig_acc=0.9835,\naug_acc=0.9109', '\\n', 'Epoch 12: CGR=1.3981', '\\n', 'MLP Epoch 13:\nval_loss=0.0977, orig_acc=0.9768, aug_acc=0.8981', '\\n', 'CNN Epoch 13:\nval_loss=0.0572, orig_acc=0.9854, aug_acc=0.9088', '\\n', 'Epoch 13: CGR=0.2442',\n'\\n', 'MLP Epoch 14: val_loss=0.1004, orig_acc=0.9767, aug_acc=0.8995', '\\n',\n'CNN Epoch 14: val_loss=0.0564, orig_acc=0.9866, aug_acc=0.9076', '\\n', 'Epoch\n14: CGR=-0.1818', '\\n', 'MLP Epoch 15: val_loss=0.0959, orig_acc=0.9784,\naug_acc=0.9047', '\\n', 'CNN Epoch 15: val_loss=0.0644, orig_acc=0.9863,\naug_acc=0.9139', '\\n', 'Epoch 15: CGR=0.1646', '\\n', '\\n=== Running with\nn_epochs = 20 ===', '\\n', 'MLP Epoch 1: val_loss=0.1358, orig_acc=0.9609,\naug_acc=0.8643', '\\n', 'CNN Epoch 1: val_loss=0.0869, orig_acc=0.9718,\naug_acc=0.8771', '\\n', 'Epoch 1: CGR=0.1743', '\\n', 'MLP Epoch 2:\nval_loss=0.0990, orig_acc=0.9697, aug_acc=0.8877', '\\n', 'CNN Epoch 2:\nval_loss=0.0668, orig_acc=0.9783, aug_acc=0.9052', '\\n', 'Epoch 2: CGR=1.0349',\n'\\n', 'MLP Epoch 3: val_loss=0.0931, orig_acc=0.9719, aug_acc=0.8845', '\\n',\n'CNN Epoch 3: val_loss=0.0507, orig_acc=0.9844, aug_acc=0.9103', '\\n', 'Epoch 3:\nCGR=1.0640', '\\n', 'MLP Epoch 4: val_loss=0.0805, orig_acc=0.9778,\naug_acc=0.8919', '\\n', 'CNN Epoch 4: val_loss=0.0512, orig_acc=0.9839,\naug_acc=0.9108', '\\n', 'Epoch 4: CGR=2.0984', '\\n', 'MLP Epoch 5:\nval_loss=0.0796, orig_acc=0.9762, aug_acc=0.9063', '\\n', 'CNN Epoch 5:\nval_loss=0.0523, orig_acc=0.9836, aug_acc=0.9068', '\\n', 'Epoch 5: CGR=-0.9324',\n'\\n', 'MLP Epoch 6: val_loss=0.0885, orig_acc=0.9733, aug_acc=0.9004', '\\n',\n'CNN Epoch 6: val_loss=0.0559, orig_acc=0.9826, aug_acc=0.9047', '\\n', 'Epoch 6:\nCGR=-0.5376', '\\n', 'MLP Epoch 7: val_loss=0.0806, orig_acc=0.9776,\naug_acc=0.9080', '\\n', 'CNN Epoch 7: val_loss=0.0696, orig_acc=0.9792,\naug_acc=0.9085', '\\n', 'Epoch 7: CGR=-0.6875', '\\n', 'MLP Epoch 8:\nval_loss=0.0894, orig_acc=0.9756, aug_acc=0.8962', '\\n', 'CNN Epoch 8:\nval_loss=0.0603, orig_acc=0.9845, aug_acc=0.9079', '\\n', 'Epoch 8: CGR=0.3146',\n'\\n', 'MLP Epoch 9: val_loss=0.0921, orig_acc=0.9756, aug_acc=0.9047', '\\n',\n'CNN Epoch 9: val_loss=0.0582, orig_acc=0.9842, aug_acc=0.9108', '\\n', 'Epoch 9:\nCGR=-0.2907', '\\n', 'MLP Epoch 10: val_loss=0.0924, orig_acc=0.9756,\naug_acc=0.9008', '\\n', 'CNN Epoch 10: val_loss=0.0623, orig_acc=0.9837,\naug_acc=0.9057', '\\n', 'Epoch 10: CGR=-0.3951', '\\n', 'MLP Epoch 11:\nval_loss=0.1077, orig_acc=0.9735, aug_acc=0.8869', '\\n', 'CNN Epoch 11:\nval_loss=0.0643, orig_acc=0.9843, aug_acc=0.9100', '\\n', 'Epoch 11: CGR=1.1389',\n'\\n', 'MLP Epoch 12: val_loss=0.0910, orig_acc=0.9785, aug_acc=0.9037', '\\n',\n'CNN Epoch 12: val_loss=0.0669, orig_acc=0.9835, aug_acc=0.9092', '\\n', 'Epoch\n12: CGR=0.1000', '\\n', 'MLP Epoch 13: val_loss=0.1066, orig_acc=0.9772,\naug_acc=0.8994', '\\n', 'CNN Epoch 13: val_loss=0.0619, orig_acc=0.9850,\naug_acc=0.9148', '\\n', 'Epoch 13: CGR=0.9744', '\\n', 'MLP Epoch 14:\nval_loss=0.1036, orig_acc=0.9777, aug_acc=0.9053', '\\n', 'CNN Epoch 14:\nval_loss=0.0794, orig_acc=0.9827, aug_acc=0.9053', '\\n', 'Epoch 14:\nCGR=-1.0000', '\\n', 'MLP Epoch 15: val_loss=0.1042, orig_acc=0.9782,\naug_acc=0.9049', '\\n', 'CNN Epoch 15: val_loss=0.0822, orig_acc=0.9826,\naug_acc=0.8976', '\\n', 'Epoch 15: CGR=0.6591', '\\n', 'MLP Epoch 16:\nval_loss=0.1080, orig_acc=0.9754, aug_acc=0.8973', '\\n', 'CNN Epoch 16:\nval_loss=0.0728, orig_acc=0.9836, aug_acc=0.9091', '\\n', 'Epoch 16: CGR=0.4390',\n'\\n', 'MLP Epoch 17: val_loss=0.0969, orig_acc=0.9807, aug_acc=0.9011', '\\n',\n'CNN Epoch 17: val_loss=0.0728, orig_acc=0.9853, aug_acc=0.9170', '\\n', 'Epoch\n17: CGR=2.4565', '\\n', 'MLP Epoch 18: val_loss=0.1313, orig_acc=0.9729,\naug_acc=0.8924', '\\n', 'CNN Epoch 18: val_loss=0.0793, orig_acc=0.9838,\naug_acc=0.9092', '\\n', 'Epoch 18: CGR=0.5413', '\\n', 'MLP Epoch 19:\nval_loss=0.1108, orig_acc=0.9769, aug_acc=0.9036', '\\n', 'CNN Epoch 19:\nval_loss=0.0881, orig_acc=0.9837, aug_acc=0.9017', '\\n', 'Epoch 19:\nCGR=-0.7206', '\\n', 'MLP Epoch 20: val_loss=0.1144, orig_acc=0.9791,\naug_acc=0.9039', '\\n', 'CNN Epoch 20: val_loss=0.0812, orig_acc=0.9845,\naug_acc=0.9193', '\\n', 'Epoch 20: CGR=1.8518', '\\n', 'Execution time: 22 minutes\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Running with n_epochs = 5 ===', '\\n', 'MLP\nEpoch 1: val_loss=0.1322, orig_acc=0.9594, aug_acc=0.8612', '\\n', 'CNN Epoch 1:\nval_loss=0.0755, orig_acc=0.9764, aug_acc=0.8966', '\\n', 'Epoch 1: CGR=1.0824',\n'\\n', 'MLP Epoch 2: val_loss=0.1064, orig_acc=0.9674, aug_acc=0.8860', '\\n',\n'CNN Epoch 2: val_loss=0.0609, orig_acc=0.9806, aug_acc=0.9100', '\\n', 'Epoch 2:\nCGR=0.8182', '\\n', 'MLP Epoch 3: val_loss=0.0829, orig_acc=0.9735,\naug_acc=0.8911', '\\n', 'CNN Epoch 3: val_loss=0.0546, orig_acc=0.9830,\naug_acc=0.9157', '\\n', 'Epoch 3: CGR=1.5895', '\\n', 'MLP Epoch 4:\nval_loss=0.0777, orig_acc=0.9761, aug_acc=0.8970', '\\n', 'CNN Epoch 4:\nval_loss=0.0502, orig_acc=0.9839, aug_acc=0.9162', '\\n', 'Epoch 4: CGR=1.4615',\n'\\n', 'MLP Epoch 5: val_loss=0.0834, orig_acc=0.9748, aug_acc=0.8958', '\\n',\n'CNN Epoch 5: val_loss=0.0533, orig_acc=0.9836, aug_acc=0.9188', '\\n', 'Epoch 5:\nCGR=1.6136', '\\n', '\\n=== Running with n_epochs = 10 ===', '\\n', 'MLP Epoch 1:\nval_loss=0.1431, orig_acc=0.9553, aug_acc=0.8605', '\\n', 'CNN Epoch 1:\nval_loss=0.0778, orig_acc=0.9749, aug_acc=0.9056', '\\n', 'Epoch 1: CGR=1.3010',\n'\\n', 'MLP Epoch 2: val_loss=0.0985, orig_acc=0.9698, aug_acc=0.8854', '\\n',\n'CNN Epoch 2: val_loss=0.0581, orig_acc=0.9805, aug_acc=0.9131', '\\n', 'Epoch 2:\nCGR=1.5888', '\\n', 'MLP Epoch 3: val_loss=0.0829, orig_acc=0.9749,\naug_acc=0.8993', '\\n', 'CNN Epoch 3: val_loss=0.0512, orig_acc=0.9837,\naug_acc=0.9178', '\\n', 'Epoch 3: CGR=1.1023', '\\n', 'MLP Epoch 4:\nval_loss=0.0806, orig_acc=0.9747, aug_acc=0.8942', '\\n', 'CNN Epoch 4:\nval_loss=0.0530, orig_acc=0.9834, aug_acc=0.9188', '\\n', 'Epoch 4: CGR=1.8276',\n'\\n', 'MLP Epoch 5: val_loss=0.0814, orig_acc=0.9758, aug_acc=0.8958', '\\n',\n'CNN Epoch 5: val_loss=0.0595, orig_acc=0.9805, aug_acc=0.8889', '\\n', 'Epoch 5:\nCGR=0.4681', '\\n', 'MLP Epoch 6: val_loss=0.0745, orig_acc=0.9771,\naug_acc=0.9047', '\\n', 'CNN Epoch 6: val_loss=0.0537, orig_acc=0.9840,\naug_acc=0.9130', '\\n', 'Epoch 6: CGR=0.2029', '\\n', 'MLP Epoch 7:\nval_loss=0.0793, orig_acc=0.9756, aug_acc=0.9030', '\\n', 'CNN Epoch 7:\nval_loss=0.0490, orig_acc=0.9851, aug_acc=0.9150', '\\n', 'Epoch 7: CGR=0.2632',\n'\\n', 'MLP Epoch 8: val_loss=0.0787, orig_acc=0.9774, aug_acc=0.9077', '\\n',\n'CNN Epoch 8: val_loss=0.0483, orig_acc=0.9849, aug_acc=0.9132', '\\n', 'Epoch 8:\nCGR=-0.2667', '\\n', 'MLP Epoch 9: val_loss=0.0833, orig_acc=0.9783,\naug_acc=0.9037', '\\n', 'CNN Epoch 9: val_loss=0.0523, orig_acc=0.9847,\naug_acc=0.9128', '\\n', 'Epoch 9: CGR=0.4219', '\\n', 'MLP Epoch 10:\nval_loss=0.0884, orig_acc=0.9773, aug_acc=0.8985', '\\n', 'CNN Epoch 10:\nval_loss=0.0573, orig_acc=0.9856, aug_acc=0.9228', '\\n', 'Epoch 10: CGR=1.9277',\n'\\n', '\\n=== Running with n_epochs = 15 ===', '\\n', 'MLP Epoch 1:\nval_loss=0.1417, orig_acc=0.9558, aug_acc=0.8717', '\\n', 'CNN Epoch 1:\nval_loss=0.1000, orig_acc=0.9676, aug_acc=0.8891', '\\n', 'Epoch 1: CGR=0.4746',\n'\\n', 'MLP Epoch 2: val_loss=0.0997, orig_acc=0.9703, aug_acc=0.8882', '\\n',\n'CNN Epoch 2: val_loss=0.0554, orig_acc=0.9823, aug_acc=0.9040', '\\n', 'Epoch 2:\nCGR=0.3167', '\\n', 'MLP Epoch 3: val_loss=0.0941, orig_acc=0.9709,\naug_acc=0.8864', '\\n', 'CNN Epoch 3: val_loss=0.0581, orig_acc=0.9805,\naug_acc=0.9094', '\\n', 'Epoch 3: CGR=1.3958', '\\n', 'MLP Epoch 4:\nval_loss=0.0810, orig_acc=0.9758, aug_acc=0.8988', '\\n', 'CNN Epoch 4:\nval_loss=0.0580, orig_acc=0.9819, aug_acc=0.9049', '\\n', 'Epoch 4: CGR=0.0000',\n'\\n', 'MLP Epoch 5: val_loss=0.0787, orig_acc=0.9741, aug_acc=0.8988', '\\n',\n'CNN Epoch 5: val_loss=0.0455, orig_acc=0.9859, aug_acc=0.9086', '\\n', 'Epoch 5:\nCGR=-0.1695', '\\n', 'MLP Epoch 6: val_loss=0.0905, orig_acc=0.9726,\naug_acc=0.8988', '\\n', 'CNN Epoch 6: val_loss=0.0520, orig_acc=0.9836,\naug_acc=0.9071', '\\n', 'Epoch 6: CGR=-0.2455', '\\n', 'MLP Epoch 7:\nval_loss=0.0730, orig_acc=0.9783, aug_acc=0.8967', '\\n', 'CNN Epoch 7:\nval_loss=0.0499, orig_acc=0.9854, aug_acc=0.9114', '\\n', 'Epoch 7: CGR=1.0704',\n'\\n', 'MLP Epoch 8: val_loss=0.0800, orig_acc=0.9775, aug_acc=0.8944', '\\n',\n'CNN Epoch 8: val_loss=0.0516, orig_acc=0.9858, aug_acc=0.9184', '\\n', 'Epoch 8:\nCGR=1.8916', '\\n', 'MLP Epoch 9: val_loss=0.0827, orig_acc=0.9776,\naug_acc=0.8990', '\\n', 'CNN Epoch 9: val_loss=0.0653, orig_acc=0.9824,\naug_acc=0.9021', '\\n', 'Epoch 9: CGR=-0.3542', '\\n', 'MLP Epoch 10:\nval_loss=0.0913, orig_acc=0.9758, aug_acc=0.8932', '\\n', 'CNN Epoch 10:\nval_loss=0.0591, orig_acc=0.9852, aug_acc=0.9137', '\\n', 'Epoch 10: CGR=1.1808',\n'\\n', 'MLP Epoch 11: val_loss=0.0789, orig_acc=0.9798, aug_acc=0.8985', '\\n',\n'CNN Epoch 11: val_loss=0.0703, orig_acc=0.9832, aug_acc=0.9037', '\\n', 'Epoch\n11: CGR=0.5294', '\\n', 'MLP Epoch 12: val_loss=0.1132, orig_acc=0.9732,\naug_acc=0.8862', '\\n', 'CNN Epoch 12: val_loss=0.0614, orig_acc=0.9860,\naug_acc=0.9066', '\\n', 'Epoch 12: CGR=0.5937', '\\n', 'MLP Epoch 13:\nval_loss=0.0977, orig_acc=0.9768, aug_acc=0.8981', '\\n', 'CNN Epoch 13:\nval_loss=0.0594, orig_acc=0.9865, aug_acc=0.9103', '\\n', 'Epoch 13: CGR=0.2577',\n'\\n', 'MLP Epoch 14: val_loss=0.1004, orig_acc=0.9767, aug_acc=0.8995', '\\n',\n'CNN Epoch 14: val_loss=0.0768, orig_acc=0.9829, aug_acc=0.9032', '\\n', 'Epoch\n14: CGR=-0.4032', '\\n', 'MLP Epoch 15: val_loss=0.0959, orig_acc=0.9784,\naug_acc=0.9047', '\\n', 'CNN Epoch 15: val_loss=0.0663, orig_acc=0.9854,\naug_acc=0.9047', '\\n', 'Epoch 15: CGR=-1.0000', '\\n', '\\n=== Running with\nn_epochs = 20 ===', '\\n', 'MLP Epoch 1: val_loss=0.1358, orig_acc=0.9609,\naug_acc=0.8643', '\\n', 'CNN Epoch 1: val_loss=0.0869, orig_acc=0.9718,\naug_acc=0.8771', '\\n', 'Epoch 1: CGR=0.1743', '\\n', 'MLP Epoch 2:\nval_loss=0.0990, orig_acc=0.9697, aug_acc=0.8877', '\\n', 'CNN Epoch 2:\nval_loss=0.0660, orig_acc=0.9791, aug_acc=0.9055', '\\n', 'Epoch 2: CGR=0.8936',\n'\\n', 'MLP Epoch 3: val_loss=0.0931, orig_acc=0.9719, aug_acc=0.8845', '\\n',\n'CNN Epoch 3: val_loss=0.0524, orig_acc=0.9844, aug_acc=0.9128', '\\n', 'Epoch 3:\nCGR=1.2640', '\\n', 'MLP Epoch 4: val_loss=0.0805, orig_acc=0.9778,\naug_acc=0.8919', '\\n', 'CNN Epoch 4: val_loss=0.0535, orig_acc=0.9825,\naug_acc=0.9102', '\\n', 'Epoch 4: CGR=2.8936', '\\n', 'MLP Epoch 5:\nval_loss=0.0796, orig_acc=0.9762, aug_acc=0.9063', '\\n', 'CNN Epoch 5:\nval_loss=0.0552, orig_acc=0.9825, aug_acc=0.9069', '\\n', 'Epoch 5: CGR=-0.9048',\n'\\n', 'MLP Epoch 6: val_loss=0.0885, orig_acc=0.9733, aug_acc=0.9004', '\\n',\n'CNN Epoch 6: val_loss=0.0558, orig_acc=0.9828, aug_acc=0.8979', '\\n', 'Epoch 6:\nCGR=-0.7368', '\\n', 'MLP Epoch 7: val_loss=0.0806, orig_acc=0.9776,\naug_acc=0.9080', '\\n', 'CNN Epoch 7: val_loss=0.0620, orig_acc=0.9821,\naug_acc=0.9110', '\\n', 'Epoch 7: CGR=-0.3333', '\\n', 'MLP Epoch 8:\nval_loss=0.0894, orig_acc=0.9756, aug_acc=0.8962', '\\n', 'CNN Epoch 8:\nval_loss=0.0613, orig_acc=0.9842, aug_acc=0.9125', '\\n', 'Epoch 8: CGR=0.8953',\n'\\n', 'MLP Epoch 9: val_loss=0.0921, orig_acc=0.9756, aug_acc=0.9047', '\\n',\n'CNN Epoch 9: val_loss=0.0639, orig_acc=0.9818, aug_acc=0.9075', '\\n', 'Epoch 9:\nCGR=-0.5484', '\\n', 'MLP Epoch 10: val_loss=0.0924, orig_acc=0.9756,\naug_acc=0.9008', '\\n', 'CNN Epoch 10: val_loss=0.0615, orig_acc=0.9847,\naug_acc=0.9108', '\\n', 'Epoch 10: CGR=0.0989', '\\n', 'MLP Epoch 11:\nval_loss=0.1077, orig_acc=0.9735, aug_acc=0.8869', '\\n', 'CNN Epoch 11:\nval_loss=0.0620, orig_acc=0.9859, aug_acc=0.9114', '\\n', 'Epoch 11: CGR=0.9758',\n'\\n', 'MLP Epoch 12: val_loss=0.0910, orig_acc=0.9785, aug_acc=0.9037', '\\n',\n'CNN Epoch 12: val_loss=0.0841, orig_acc=0.9807, aug_acc=0.9019', '\\n', 'Epoch\n12: CGR=-0.1818', '\\n', 'MLP Epoch 13: val_loss=0.1066, orig_acc=0.9772,\naug_acc=0.8994', '\\n', 'CNN Epoch 13: val_loss=0.0706, orig_acc=0.9845,\naug_acc=0.9124', '\\n', 'Epoch 13: CGR=0.7808', '\\n', 'MLP Epoch 14:\nval_loss=0.1036, orig_acc=0.9777, aug_acc=0.9053', '\\n', 'CNN Epoch 14:\nval_loss=0.0715, orig_acc=0.9844, aug_acc=0.9206', '\\n', 'Epoch 14: CGR=1.2836',\n'\\n', 'MLP Epoch 15: val_loss=0.1042, orig_acc=0.9782, aug_acc=0.9049', '\\n',\n'CNN Epoch 15: val_loss=0.0759, orig_acc=0.9845, aug_acc=0.9073', '\\n', 'Epoch\n15: CGR=-0.6190', '\\n', 'MLP Epoch 16: val_loss=0.1080, orig_acc=0.9754,\naug_acc=0.8973', '\\n', 'CNN Epoch 16: val_loss=0.0769, orig_acc=0.9832,\naug_acc=0.9049', '\\n', 'Epoch 16: CGR=-0.0256', '\\n', 'MLP Epoch 17:\nval_loss=0.0969, orig_acc=0.9807, aug_acc=0.9011', '\\n', 'CNN Epoch 17:\nval_loss=0.0803, orig_acc=0.9831, aug_acc=0.9111', '\\n', 'Epoch 17: CGR=3.1666',\n'\\n', 'MLP Epoch 18: val_loss=0.1313, orig_acc=0.9729, aug_acc=0.8924', '\\n',\n'CNN Epoch 18: val_loss=0.0733, orig_acc=0.9858, aug_acc=0.9189', '\\n', 'Epoch\n18: CGR=1.0543', '\\n', 'MLP Epoch 19: val_loss=0.1108, orig_acc=0.9769,\naug_acc=0.9036', '\\n', 'CNN Epoch 19: val_loss=0.0867, orig_acc=0.9840,\naug_acc=0.9150', '\\n', 'Epoch 19: CGR=0.6056', '\\n', 'MLP Epoch 20:\nval_loss=0.1144, orig_acc=0.9791, aug_acc=0.9039', '\\n', 'CNN Epoch 20:\nval_loss=0.0864, orig_acc=0.9838, aug_acc=0.9080', '\\n', 'Epoch 20:\nCGR=-0.1277', '\\n', 'Execution time: 22 minutes seconds (time limit is an\nhour).']", ""], "analysis": ["", "", "The training script executed successfully without any runtime errors. The MNIST\ndataset was downloaded and loaded correctly, models (MLP and CNN) trained across\nthe specified learning rates, and validation metrics (loss, original accuracy,\naugmented accuracy) along with the CGR metric were computed and logged for each\nepoch. No execution failures or exceptions were observed.", "", "The implementation did not include the two additional HuggingFace test datasets\nspecified in the sub-stage goals. The code only evaluates on MNIST (original and\naugmented). To fix, import and preprocess two more suitable HuggingFace datasets\n(e.g. FashionMNIST, KMNIST, or EMNIST) and add them to the evaluation loops\nalongside the existing loaders.", "The experiment ran successfully and reported CGR values for each dropout rate,\nbut it did not include the two additional HuggingFace datasets required by the\nsub-stage goals. Instead, it only used MNIST (with and without augmentation). To\nfix this, load two new datasets from HuggingFace (e.g., EMNIST and KMNIST from\nthe `datasets` library), create DataLoaders for them, evaluate the model on\nthese datasets, and store their metrics alongside the existing ones.", "", "The script crashes at `load_dataset(\"kmnist\")` with a DatasetNotFoundError\nbecause the HuggingFace dataset name is incorrect. The official HF ID for\nKuzushiji-MNIST is `kuzushiji_mnist`, or you can alternatively load KMNIST via\n`torchvision.datasets.KMNIST`. To fix, replace the faulty call with:  ```python\n# Option A: correct HF name kmnist_ds = load_dataset(\"kuzushiji_mnist\",\nsplit=\"test\")  # Option B: Torchvision from torchvision.datasets import KMNIST\nkmnist_ds = KMNIST(root=\"./data\", train=False, download=True,\ntransform=test_transform) ```", "The script fails to load the second HuggingFace dataset using\n`load_dataset(\"kmnist\")`, causing a DatasetNotFoundError. The correct identifier\nfor Kuzushiji-MNIST on the Hub is `kuzushiji_mnist`. To fix, replace\n`load_dataset(\"kmnist\", split=\"test\")` with `load_dataset(\"kuzushiji_mnist\",\nsplit=\"test\")`, or choose another valid dataset name.", "The script fails to load the HuggingFace dataset \"kuzushiji_mnist\" because no\nsuch dataset ID exists on the Hub. To fix this, correct the dataset name to the\nactual HuggingFace ID (e.g., \"kuzushiji-mnist\") or replace it with a different\nvalid OCR-style dataset such as the USPS dataset (load_dataset(\"usps\",\nsplit=\"test\")) or the EMNIST \"digits\" subset.", "The script crashes with a DatasetNotFoundError when attempting to load\n\"kuzushiji_mnist\" \u2013 the correct HuggingFace dataset ID is \"kuzushiji-mnist\"\n(with a hyphen). Updating the call to load_dataset(\"kuzushiji-mnist\",\nsplit=\"test\") (and similarly verifying other dataset names) will fix the issue.", "The script crashes at load_dataset(\"kmnist\", \"balanced\") because the 'kmnist'\ndataset on HuggingFace has no 'balanced' configuration. To fix, remove the\nsecond argument and simply call load_dataset(\"kmnist\"), or specify the correct\nconfig name as per the HuggingFace KMNIST documentation.", "", "", "", "The script executed successfully without runtime errors, but it does not meet\nthe sub\u2010stage requirements. Specifically: 1. Only the number of epochs\n(n_epochs) was varied\u2014no other hyperparameters (e.g., learning rate, batch size)\nwere tuned. 2. The code did not introduce any additional Hugging Face datasets\nfor testing (only original and augmented MNIST were used). 3. The log header for\nn_epochs=15 appears to run through 20 epochs, indicating a labeling or\ntruncation inconsistency in the output.  Proposed fixes: - Extend the\nhyperparameter grid search to include learning rates and batch sizes alongside\nepochs. - Integrate two new Hugging Face datasets (e.g., \"Kuzushiji-MNIST\" and\n\"FashionMNIST\") by loading them via `datasets.load_dataset`, applying\nappropriate transforms, and adding them to test loaders. - Ensure each\nexperiment run prints the correct header matching its epoch count and that the\noutput isn\u2019t inadvertently truncated or mislabeled.", ""], "exc_type": [null, null, null, null, null, null, null, "DatasetNotFoundError", "DatasetNotFoundError", "DatasetNotFoundError", "DatasetNotFoundError", "DatasetNotFoundError", null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, {"args": ["Dataset 'kmnist' doesn't exist on the Hub or cannot be accessed."]}, {"args": ["Dataset 'kmnist' doesn't exist on the Hub or cannot be accessed."]}, {"args": ["Dataset 'kuzushiji_mnist' doesn't exist on the Hub or cannot be accessed."]}, {"args": ["Dataset 'kuzushiji_mnist' doesn't exist on the Hub or cannot be accessed."]}, {"args": ["Dataset 'kmnist' doesn't exist on the Hub or cannot be accessed."]}, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 39, "<module>", "kmnist_ds = load_dataset(\"kmnist\", split=\"test\")"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1652, "dataset_module_factory", "raise e1 from None"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1578, "dataset_module_factory", "raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e"]], [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 51, "<module>", "kmnist_ds = load_dataset(\"kmnist\", split=\"test\").map(hf_transform)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1652, "dataset_module_factory", "raise e1 from None"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1578, "dataset_module_factory", "raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e"]], [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 46, "<module>", "kuz_ds = load_dataset(\"kuzushiji_mnist\", split=\"test\")"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1652, "dataset_module_factory", "raise e1 from None"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1578, "dataset_module_factory", "raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e"]], [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 63, "<module>", "kuzushiji_ds = load_dataset(\"kuzushiji_mnist\", split=\"test\").map(map_hf)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1652, "dataset_module_factory", "raise e1 from None"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1578, "dataset_module_factory", "raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e"]], [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 39, "<module>", "hf_kmnist = load_dataset(\"kmnist\", \"balanced\")"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1652, "dataset_module_factory", "raise e1 from None"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1578, "dataset_module_factory", "raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e"]], null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "training dataset", "final_value": 0.0289, "best_value": 0.0289}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the original test dataset", "data": [{"dataset_name": "original test dataset", "final_value": 0.0592, "best_value": 0.0592}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the original test dataset", "data": [{"dataset_name": "original test dataset", "final_value": 0.9823, "best_value": 0.9823}]}, {"metric_name": "augmented test accuracy", "lower_is_better": false, "description": "Accuracy on the augmented test dataset", "data": [{"dataset_name": "augmented test dataset", "final_value": 0.9122, "best_value": 0.9122}]}]}, {"metric_names": [{"metric_name": "MLP training loss", "lower_is_better": true, "description": "Training loss for the MLP model", "data": [{"dataset_name": "Training", "final_value": 0.0081, "best_value": 0.0081}]}, {"metric_name": "MLP validation loss", "lower_is_better": true, "description": "Validation loss for the MLP model", "data": [{"dataset_name": "Validation", "final_value": 0.1257, "best_value": 0.0808}]}, {"metric_name": "MLP original test accuracy", "lower_is_better": false, "description": "Accuracy on original test set for the MLP model", "data": [{"dataset_name": "Original Test", "final_value": 0.976, "best_value": 0.9799}]}, {"metric_name": "MLP augmented test accuracy", "lower_is_better": false, "description": "Accuracy on augmented test set for the MLP model", "data": [{"dataset_name": "Augmented Test", "final_value": 0.8928, "best_value": 0.9023}]}, {"metric_name": "CNN training loss", "lower_is_better": true, "description": "Training loss for the CNN model", "data": [{"dataset_name": "Training", "final_value": 0.003, "best_value": 0.003}]}, {"metric_name": "CNN validation loss", "lower_is_better": true, "description": "Validation loss for the CNN model", "data": [{"dataset_name": "Validation", "final_value": 0.0772, "best_value": 0.0587}]}, {"metric_name": "CNN original test accuracy", "lower_is_better": false, "description": "Accuracy on original test set for the CNN model", "data": [{"dataset_name": "Original Test", "final_value": 0.9861, "best_value": 0.9861}]}, {"metric_name": "CNN augmented test accuracy", "lower_is_better": false, "description": "Accuracy on augmented test set for the CNN model", "data": [{"dataset_name": "Augmented Test", "final_value": 0.92, "best_value": 0.92}]}]}, {"metric_names": [{"metric_name": "Training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "MLP, lr=0.0001", "final_value": 0.1719, "best_value": 0.1719}, {"dataset_name": "CNN, lr=0.0001", "final_value": 0.0917, "best_value": 0.0917}, {"dataset_name": "MLP, lr=0.0005", "final_value": 0.0623, "best_value": 0.0623}, {"dataset_name": "CNN, lr=0.0005", "final_value": 0.0387, "best_value": 0.0387}, {"dataset_name": "MLP, lr=0.001", "final_value": 0.0463, "best_value": 0.0463}, {"dataset_name": "CNN, lr=0.001", "final_value": 0.0311, "best_value": 0.0311}, {"dataset_name": "MLP, lr=0.005", "final_value": 0.0982, "best_value": 0.0982}, {"dataset_name": "CNN, lr=0.005", "final_value": 0.0281, "best_value": 0.0281}, {"dataset_name": "MLP, lr=0.01", "final_value": 0.1628, "best_value": 0.1628}, {"dataset_name": "CNN, lr=0.01", "final_value": 0.045, "best_value": 0.045}]}, {"metric_name": "Validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "MLP, lr=0.0001", "final_value": 0.1662, "best_value": 0.1662}, {"dataset_name": "CNN, lr=0.0001", "final_value": 0.0822, "best_value": 0.0822}, {"dataset_name": "MLP, lr=0.0005", "final_value": 0.083, "best_value": 0.083}, {"dataset_name": "CNN, lr=0.0005", "final_value": 0.058, "best_value": 0.058}, {"dataset_name": "MLP, lr=0.001", "final_value": 0.0804, "best_value": 0.0804}, {"dataset_name": "CNN, lr=0.001", "final_value": 0.0421, "best_value": 0.0421}, {"dataset_name": "MLP, lr=0.005", "final_value": 0.1495, "best_value": 0.1495}, {"dataset_name": "CNN, lr=0.005", "final_value": 0.0756, "best_value": 0.0756}, {"dataset_name": "MLP, lr=0.01", "final_value": 0.2, "best_value": 0.2}, {"dataset_name": "CNN, lr=0.01", "final_value": 0.0949, "best_value": 0.0949}]}, {"metric_name": "Original test accuracy", "lower_is_better": false, "description": "Final original test accuracy", "data": [{"dataset_name": "MLP, lr=0.0001", "final_value": 0.9522, "best_value": 0.9522}, {"dataset_name": "CNN, lr=0.0001", "final_value": 0.9742, "best_value": 0.9742}, {"dataset_name": "MLP, lr=0.0005", "final_value": 0.9753, "best_value": 0.9753}, {"dataset_name": "CNN, lr=0.0005", "final_value": 0.9807, "best_value": 0.9807}, {"dataset_name": "MLP, lr=0.001", "final_value": 0.976, "best_value": 0.976}, {"dataset_name": "CNN, lr=0.001", "final_value": 0.9852, "best_value": 0.9852}, {"dataset_name": "MLP, lr=0.005", "final_value": 0.9661, "best_value": 0.9661}, {"dataset_name": "CNN, lr=0.005", "final_value": 0.9807, "best_value": 0.9807}, {"dataset_name": "MLP, lr=0.01", "final_value": 0.9529, "best_value": 0.9529}, {"dataset_name": "CNN, lr=0.01", "final_value": 0.9747, "best_value": 0.9747}]}, {"metric_name": "Augmented test accuracy", "lower_is_better": false, "description": "Final augmented test accuracy", "data": [{"dataset_name": "MLP, lr=0.0001", "final_value": 0.8464, "best_value": 0.8464}, {"dataset_name": "CNN, lr=0.0001", "final_value": 0.8995, "best_value": 0.8995}, {"dataset_name": "MLP, lr=0.0005", "final_value": 0.8881, "best_value": 0.8881}, {"dataset_name": "CNN, lr=0.0005", "final_value": 0.8948, "best_value": 0.8948}, {"dataset_name": "MLP, lr=0.001", "final_value": 0.8936, "best_value": 0.8936}, {"dataset_name": "CNN, lr=0.001", "final_value": 0.9115, "best_value": 0.9115}, {"dataset_name": "MLP, lr=0.005", "final_value": 0.8796, "best_value": 0.8796}, {"dataset_name": "CNN, lr=0.005", "final_value": 0.9026, "best_value": 0.9026}, {"dataset_name": "MLP, lr=0.01", "final_value": 0.853, "best_value": 0.853}, {"dataset_name": "CNN, lr=0.01", "final_value": 0.8903, "best_value": 0.8903}]}]}, {"metric_names": [{"metric_name": "Final training loss", "lower_is_better": true, "description": "Training loss at the end of training for each model and batch size", "data": [{"dataset_name": "MLP_bs32", "final_value": 0.0447, "best_value": 0.0447}, {"dataset_name": "CNN_bs32", "final_value": 0.0222, "best_value": 0.0222}, {"dataset_name": "MLP_bs64", "final_value": 0.0468, "best_value": 0.0468}, {"dataset_name": "CNN_bs64", "final_value": 0.0295, "best_value": 0.0295}, {"dataset_name": "MLP_bs128", "final_value": 0.053, "best_value": 0.053}, {"dataset_name": "CNN_bs128", "final_value": 0.038, "best_value": 0.038}, {"dataset_name": "MLP_bs256", "final_value": 0.0731, "best_value": 0.0731}, {"dataset_name": "CNN_bs256", "final_value": 0.0467, "best_value": 0.0467}]}, {"metric_name": "Final validation loss", "lower_is_better": true, "description": "Validation loss at the end of training for each model and batch size", "data": [{"dataset_name": "MLP_bs32", "final_value": 0.0875, "best_value": 0.0875}, {"dataset_name": "CNN_bs32", "final_value": 0.0461, "best_value": 0.0461}, {"dataset_name": "MLP_bs64", "final_value": 0.0783, "best_value": 0.0783}, {"dataset_name": "CNN_bs64", "final_value": 0.0474, "best_value": 0.0474}, {"dataset_name": "MLP_bs128", "final_value": 0.0831, "best_value": 0.0831}, {"dataset_name": "CNN_bs128", "final_value": 0.0537, "best_value": 0.0537}, {"dataset_name": "MLP_bs256", "final_value": 0.0939, "best_value": 0.0939}, {"dataset_name": "CNN_bs256", "final_value": 0.0552, "best_value": 0.0552}]}, {"metric_name": "Final original test accuracy", "lower_is_better": false, "description": "Accuracy on the original test dataset for each model and batch size", "data": [{"dataset_name": "MLP_bs32", "final_value": 0.9735, "best_value": 0.9735}, {"dataset_name": "CNN_bs32", "final_value": 0.9843, "best_value": 0.9843}, {"dataset_name": "MLP_bs64", "final_value": 0.976, "best_value": 0.976}, {"dataset_name": "CNN_bs64", "final_value": 0.9842, "best_value": 0.9842}, {"dataset_name": "MLP_bs128", "final_value": 0.9756, "best_value": 0.9756}, {"dataset_name": "CNN_bs128", "final_value": 0.9817, "best_value": 0.9817}, {"dataset_name": "MLP_bs256", "final_value": 0.9705, "best_value": 0.9705}, {"dataset_name": "CNN_bs256", "final_value": 0.9818, "best_value": 0.9818}]}, {"metric_name": "Final augmented test accuracy", "lower_is_better": false, "description": "Accuracy on the augmented test dataset for each model and batch size", "data": [{"dataset_name": "MLP_bs32", "final_value": 0.8989, "best_value": 0.8989}, {"dataset_name": "CNN_bs32", "final_value": 0.9193, "best_value": 0.9193}, {"dataset_name": "MLP_bs64", "final_value": 0.8983, "best_value": 0.8983}, {"dataset_name": "CNN_bs64", "final_value": 0.9147, "best_value": 0.9147}, {"dataset_name": "MLP_bs128", "final_value": 0.9041, "best_value": 0.9041}, {"dataset_name": "CNN_bs128", "final_value": 0.9003, "best_value": 0.9003}, {"dataset_name": "MLP_bs256", "final_value": 0.8866, "best_value": 0.8866}, {"dataset_name": "CNN_bs256", "final_value": 0.9148, "best_value": 0.9148}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "Training", "final_value": 0.0422, "best_value": 0.0422}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "Validation", "final_value": 0.0635, "best_value": 0.0635}]}, {"metric_name": "original test accuracy", "lower_is_better": false, "description": "Final accuracy on the original test dataset", "data": [{"dataset_name": "Original Test", "final_value": 0.9798, "best_value": 0.9798}]}, {"metric_name": "augmented test accuracy", "lower_is_better": false, "description": "Final accuracy on the augmented test dataset", "data": [{"dataset_name": "Augmented Test", "final_value": 0.9009, "best_value": 0.9009}]}]}, {"metric_names": [{"metric_name": "Training Loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "Training Dataset (MLP)", "final_value": 0.0719, "best_value": 0.0719}, {"dataset_name": "Training Dataset (CNN)", "final_value": 0.0551, "best_value": 0.0551}]}, {"metric_name": "Validation Loss", "lower_is_better": true, "description": "Loss on the original test dataset", "data": [{"dataset_name": "Original Test Dataset (MLP)", "final_value": 0.0951, "best_value": 0.0951}, {"dataset_name": "Original Test Dataset (CNN)", "final_value": 0.0625, "best_value": 0.0625}]}, {"metric_name": "Original Test Accuracy", "lower_is_better": false, "description": "Accuracy on the original test dataset", "data": [{"dataset_name": "Original Test Dataset (MLP)", "final_value": 0.9716, "best_value": 0.9716}, {"dataset_name": "Original Test Dataset (CNN)", "final_value": 0.982, "best_value": 0.982}]}, {"metric_name": "Augmented Test Accuracy", "lower_is_better": false, "description": "Accuracy on the augmented test dataset", "data": [{"dataset_name": "Augmented Test Dataset (MLP)", "final_value": 0.8859, "best_value": 0.8859}, {"dataset_name": "Augmented Test Dataset (CNN)", "final_value": 0.902, "best_value": 0.902}]}]}, {"metric_names": [{"metric_name": "training loss (epsilon=0.0)", "lower_is_better": true, "description": "Final training loss with label smoothing epsilon=0.0.", "data": [{"dataset_name": "Training Dataset", "final_value": 0.0318, "best_value": 0.0318}]}, {"metric_name": "validation loss (epsilon=0.0)", "lower_is_better": true, "description": "Final validation loss with label smoothing epsilon=0.0.", "data": [{"dataset_name": "Validation Dataset", "final_value": 0.0483, "best_value": 0.0483}]}, {"metric_name": "original test accuracy (epsilon=0.0)", "lower_is_better": false, "description": "Final original test accuracy with label smoothing epsilon=0.0.", "data": [{"dataset_name": "Original Test Dataset", "final_value": 0.9843, "best_value": 0.9843}]}, {"metric_name": "augmented test accuracy (epsilon=0.0)", "lower_is_better": false, "description": "Final augmented test accuracy with label smoothing epsilon=0.0.", "data": [{"dataset_name": "Augmented Test Dataset", "final_value": 0.9143, "best_value": 0.9143}]}, {"metric_name": "training loss (epsilon=0.05)", "lower_is_better": true, "description": "Final training loss with label smoothing epsilon=0.05.", "data": [{"dataset_name": "Training Dataset", "final_value": 0.3524, "best_value": 0.3524}]}, {"metric_name": "validation loss (epsilon=0.05)", "lower_is_better": true, "description": "Final validation loss with label smoothing epsilon=0.05.", "data": [{"dataset_name": "Validation Dataset", "final_value": 0.3643, "best_value": 0.3643}]}, {"metric_name": "original test accuracy (epsilon=0.05)", "lower_is_better": false, "description": "Final original test accuracy with label smoothing epsilon=0.05.", "data": [{"dataset_name": "Original Test Dataset", "final_value": 0.988, "best_value": 0.988}]}, {"metric_name": "augmented test accuracy (epsilon=0.05)", "lower_is_better": false, "description": "Final augmented test accuracy with label smoothing epsilon=0.05.", "data": [{"dataset_name": "Augmented Test Dataset", "final_value": 0.9274, "best_value": 0.9274}]}, {"metric_name": "training loss (epsilon=0.1)", "lower_is_better": true, "description": "Final training loss with label smoothing epsilon=0.1.", "data": [{"dataset_name": "Training Dataset", "final_value": 0.5997, "best_value": 0.5997}]}, {"metric_name": "validation loss (epsilon=0.1)", "lower_is_better": true, "description": "Final validation loss with label smoothing epsilon=0.1.", "data": [{"dataset_name": "Validation Dataset", "final_value": 0.6083, "best_value": 0.6083}]}, {"metric_name": "original test accuracy (epsilon=0.1)", "lower_is_better": false, "description": "Final original test accuracy with label smoothing epsilon=0.1.", "data": [{"dataset_name": "Original Test Dataset", "final_value": 0.9871, "best_value": 0.9871}]}, {"metric_name": "augmented test accuracy (epsilon=0.1)", "lower_is_better": false, "description": "Final augmented test accuracy with label smoothing epsilon=0.1.", "data": [{"dataset_name": "Augmented Test Dataset", "final_value": 0.9249, "best_value": 0.9249}]}, {"metric_name": "training loss (epsilon=0.2)", "lower_is_better": true, "description": "Final training loss with label smoothing epsilon=0.2.", "data": [{"dataset_name": "Training Dataset", "final_value": 0.9853, "best_value": 0.9853}]}, {"metric_name": "validation loss (epsilon=0.2)", "lower_is_better": true, "description": "Final validation loss with label smoothing epsilon=0.2.", "data": [{"dataset_name": "Validation Dataset", "final_value": 0.9969, "best_value": 0.9969}]}, {"metric_name": "original test accuracy (epsilon=0.2)", "lower_is_better": false, "description": "Final original test accuracy with label smoothing epsilon=0.2.", "data": [{"dataset_name": "Original Test Dataset", "final_value": 0.9867, "best_value": 0.9867}]}, {"metric_name": "augmented test accuracy (epsilon=0.2)", "lower_is_better": false, "description": "Final augmented test accuracy with label smoothing epsilon=0.2.", "data": [{"dataset_name": "Augmented Test Dataset", "final_value": 0.8989, "best_value": 0.8989}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Lowest final training loss across all runs", "data": [{"dataset_name": "Training dataset", "final_value": 0.0295, "best_value": 0.0295}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Lowest final validation loss on original test dataset across all runs", "data": [{"dataset_name": "Original test dataset", "final_value": 0.0433, "best_value": 0.0433}]}, {"metric_name": "original test accuracy", "lower_is_better": false, "description": "Highest final accuracy on original test dataset across all runs", "data": [{"dataset_name": "Original test dataset", "final_value": 0.9855, "best_value": 0.9855}]}, {"metric_name": "augmented test accuracy", "lower_is_better": false, "description": "Highest final accuracy on augmented test dataset across all runs", "data": [{"dataset_name": "Augmented test dataset", "final_value": 0.9205, "best_value": 0.9205}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Training loss per model", "data": [{"dataset_name": "MLP", "final_value": 0.0088, "best_value": 0.0088}, {"dataset_name": "CNN", "final_value": 0.0041, "best_value": 0.0041}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss per model", "data": [{"dataset_name": "MLP", "final_value": 0.1144, "best_value": 0.0834}, {"dataset_name": "CNN", "final_value": 0.0759, "best_value": 0.0479}]}, {"metric_name": "original test accuracy", "lower_is_better": false, "description": "Accuracy on the original test set per model", "data": [{"dataset_name": "MLP", "final_value": 0.9791, "best_value": 0.9791}, {"dataset_name": "CNN", "final_value": 0.9859, "best_value": 0.9864}]}, {"metric_name": "augmented test accuracy", "lower_is_better": false, "description": "Accuracy on the augmented test set per model", "data": [{"dataset_name": "MLP", "final_value": 0.9039, "best_value": 0.9047}, {"dataset_name": "CNN", "final_value": 0.9156, "best_value": 0.9263}]}]}, {"metric_names": [{"metric_name": "MLP training loss", "lower_is_better": true, "description": "Training loss for the MLP model", "data": [{"dataset_name": "Training", "final_value": 0.0088, "best_value": 0.0088}]}, {"metric_name": "MLP validation loss", "lower_is_better": true, "description": "Validation loss for the MLP model", "data": [{"dataset_name": "Validation", "final_value": 0.1144, "best_value": 0.0834}]}, {"metric_name": "MLP original test accuracy", "lower_is_better": false, "description": "Accuracy on the original test set for the MLP model", "data": [{"dataset_name": "Original Test", "final_value": 0.9791, "best_value": 0.9791}]}, {"metric_name": "MLP augmented test accuracy", "lower_is_better": false, "description": "Accuracy on the augmented test set for the MLP model", "data": [{"dataset_name": "Augmented Test", "final_value": 0.9039, "best_value": 0.9047}]}, {"metric_name": "CNN training loss", "lower_is_better": true, "description": "Training loss for the CNN model", "data": [{"dataset_name": "Training", "final_value": 0.0043, "best_value": 0.0043}]}, {"metric_name": "CNN validation loss", "lower_is_better": true, "description": "Validation loss for the CNN model", "data": [{"dataset_name": "Validation", "final_value": 0.0812, "best_value": 0.0531}]}, {"metric_name": "CNN original test accuracy", "lower_is_better": false, "description": "Accuracy on the original test set for the CNN model", "data": [{"dataset_name": "Original Test", "final_value": 0.9845, "best_value": 0.9863}]}, {"metric_name": "CNN augmented test accuracy", "lower_is_better": false, "description": "Accuracy on the augmented test set for the CNN model", "data": [{"dataset_name": "Augmented Test", "final_value": 0.9193, "best_value": 0.9207}]}]}, {"metric_names": [{"metric_name": "MLP training loss", "lower_is_better": true, "description": "Training loss for MLP across epochs", "data": [{"dataset_name": "Training", "final_value": 0.0088, "best_value": 0.0088}]}, {"metric_name": "MLP validation loss", "lower_is_better": true, "description": "Validation loss for MLP across epochs", "data": [{"dataset_name": "Validation", "final_value": 0.1144, "best_value": 0.0834}]}, {"metric_name": "MLP original test accuracy", "lower_is_better": false, "description": "Original test set accuracy for MLP across epochs", "data": [{"dataset_name": "Original Test", "final_value": 0.9791, "best_value": 0.9791}]}, {"metric_name": "MLP augmented test accuracy", "lower_is_better": false, "description": "Augmented test set accuracy for MLP across epochs", "data": [{"dataset_name": "Augmented Test", "final_value": 0.9039, "best_value": 0.9047}]}, {"metric_name": "CNN training loss", "lower_is_better": true, "description": "Training loss for CNN across epochs", "data": [{"dataset_name": "Training", "final_value": 0.0051, "best_value": 0.0051}]}, {"metric_name": "CNN validation loss", "lower_is_better": true, "description": "Validation loss for CNN across epochs", "data": [{"dataset_name": "Validation", "final_value": 0.0864, "best_value": 0.0533}]}, {"metric_name": "CNN original test accuracy", "lower_is_better": false, "description": "Original test set accuracy for CNN across epochs", "data": [{"dataset_name": "Original Test", "final_value": 0.9838, "best_value": 0.9856}]}, {"metric_name": "CNN augmented test accuracy", "lower_is_better": false, "description": "Augmented test set accuracy for CNN across epochs", "data": [{"dataset_name": "Augmented Test", "final_value": 0.908, "best_value": 0.9228}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_bf06f8102609494db1f0ef77f656a3d3_proc_3702064/mnist_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_bf06f8102609494db1f0ef77f656a3d3_proc_3702064/mnist_loss_curves.png", "../../logs/0-run/experiment_results/experiment_bf06f8102609494db1f0ef77f656a3d3_proc_3702064/mnist_cgr_curve.png"], ["../../logs/0-run/experiment_results/experiment_ba6e0e5c2f904eeb9af493981feb0491_proc_3707563/mnist_loss_n_epochs_15.png", "../../logs/0-run/experiment_results/experiment_ba6e0e5c2f904eeb9af493981feb0491_proc_3707563/mnist_loss_n_epochs_10.png", "../../logs/0-run/experiment_results/experiment_ba6e0e5c2f904eeb9af493981feb0491_proc_3707563/mnist_loss_n_epochs_5.png", "../../logs/0-run/experiment_results/experiment_ba6e0e5c2f904eeb9af493981feb0491_proc_3707563/mnist_loss_n_epochs_20.png", "../../logs/0-run/experiment_results/experiment_ba6e0e5c2f904eeb9af493981feb0491_proc_3707563/mnist_cgr.png"], ["../../logs/0-run/experiment_results/experiment_49d03344c2f344bcb0138833679e764d_proc_3707564/MNIST_MLP_loss_curves.png", "../../logs/0-run/experiment_results/experiment_49d03344c2f344bcb0138833679e764d_proc_3707564/MNIST_MLP_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_49d03344c2f344bcb0138833679e764d_proc_3707564/MNIST_CNN_loss_curves.png", "../../logs/0-run/experiment_results/experiment_49d03344c2f344bcb0138833679e764d_proc_3707564/MNIST_CGR_curves.png", "../../logs/0-run/experiment_results/experiment_49d03344c2f344bcb0138833679e764d_proc_3707564/MNIST_CNN_accuracy_curves.png"], ["../../logs/0-run/experiment_results/experiment_37cab4e5477b4df5a7725c1588c249df_proc_3707565/mnist_cnn_loss_curves.png", "../../logs/0-run/experiment_results/experiment_37cab4e5477b4df5a7725c1588c249df_proc_3707565/mnist_mlp_loss_curves.png"], [], [], ["../../logs/0-run/experiment_results/experiment_85159681bbe84bbc9bccdb7aed08ca02_proc_3707565/mnist_loss_curves_label_smoothing.png", "../../logs/0-run/experiment_results/experiment_85159681bbe84bbc9bccdb7aed08ca02_proc_3707565/mnist_accuracy_curves_label_smoothing.png"], [], [], [], [], [], ["../../logs/0-run/experiment_results/experiment_7e8b9755eedc4da898b0fa8d8ee0f3a9_proc_3707564/accuracy_curves_MNIST.png", "../../logs/0-run/experiment_results/experiment_7e8b9755eedc4da898b0fa8d8ee0f3a9_proc_3707564/loss_curves_MNIST.png", "../../logs/0-run/experiment_results/experiment_7e8b9755eedc4da898b0fa8d8ee0f3a9_proc_3707564/CGR_curves_MNIST.png"], ["../../logs/0-run/experiment_results/experiment_f41f33d4aedb4e4d9add6d07ea1eb8fb_proc_3707563/mnist_loss_n_epochs_15.png", "../../logs/0-run/experiment_results/experiment_f41f33d4aedb4e4d9add6d07ea1eb8fb_proc_3707563/mnist_loss_n_epochs_10.png", "../../logs/0-run/experiment_results/experiment_f41f33d4aedb4e4d9add6d07ea1eb8fb_proc_3707563/mnist_loss_n_epochs_5.png", "../../logs/0-run/experiment_results/experiment_f41f33d4aedb4e4d9add6d07ea1eb8fb_proc_3707563/mnist_loss_n_epochs_20.png", "../../logs/0-run/experiment_results/experiment_f41f33d4aedb4e4d9add6d07ea1eb8fb_proc_3707563/mnist_cgr.png"], ["../../logs/0-run/experiment_results/experiment_bd4200573eac4c5b88b8cc01b63cfa1d_proc_3707565/mnist_loss_n_epochs_15.png", "../../logs/0-run/experiment_results/experiment_bd4200573eac4c5b88b8cc01b63cfa1d_proc_3707565/mnist_loss_n_epochs_10.png", "../../logs/0-run/experiment_results/experiment_bd4200573eac4c5b88b8cc01b63cfa1d_proc_3707565/mnist_loss_n_epochs_5.png", "../../logs/0-run/experiment_results/experiment_bd4200573eac4c5b88b8cc01b63cfa1d_proc_3707565/mnist_loss_n_epochs_20.png", "../../logs/0-run/experiment_results/experiment_bd4200573eac4c5b88b8cc01b63cfa1d_proc_3707565/mnist_cgr.png"], [], ["../../logs/0-run/experiment_results/seed_aggregation_5f79ad9b7d624e6592c2f7230b5acc62/mnist_mean_cgr.png", "../../logs/0-run/experiment_results/seed_aggregation_5f79ad9b7d624e6592c2f7230b5acc62/mnist_mean_loss_n_epochs_20.png", "../../logs/0-run/experiment_results/seed_aggregation_5f79ad9b7d624e6592c2f7230b5acc62/mnist_mean_loss_n_epochs_10.png", "../../logs/0-run/experiment_results/seed_aggregation_5f79ad9b7d624e6592c2f7230b5acc62/mnist_mean_orig_acc.png", "../../logs/0-run/experiment_results/seed_aggregation_5f79ad9b7d624e6592c2f7230b5acc62/mnist_mean_loss_n_epochs_5.png", "../../logs/0-run/experiment_results/seed_aggregation_5f79ad9b7d624e6592c2f7230b5acc62/mnist_mean_aug_acc.png", "../../logs/0-run/experiment_results/seed_aggregation_5f79ad9b7d624e6592c2f7230b5acc62/mnist_mean_loss_n_epochs_15.png"]], "plot_paths": [["experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_bf06f8102609494db1f0ef77f656a3d3_proc_3702064/mnist_accuracy_curves.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_bf06f8102609494db1f0ef77f656a3d3_proc_3702064/mnist_loss_curves.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_bf06f8102609494db1f0ef77f656a3d3_proc_3702064/mnist_cgr_curve.png"], ["experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_ba6e0e5c2f904eeb9af493981feb0491_proc_3707563/mnist_loss_n_epochs_15.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_ba6e0e5c2f904eeb9af493981feb0491_proc_3707563/mnist_loss_n_epochs_10.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_ba6e0e5c2f904eeb9af493981feb0491_proc_3707563/mnist_loss_n_epochs_5.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_ba6e0e5c2f904eeb9af493981feb0491_proc_3707563/mnist_loss_n_epochs_20.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_ba6e0e5c2f904eeb9af493981feb0491_proc_3707563/mnist_cgr.png"], ["experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_49d03344c2f344bcb0138833679e764d_proc_3707564/MNIST_MLP_loss_curves.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_49d03344c2f344bcb0138833679e764d_proc_3707564/MNIST_MLP_accuracy_curves.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_49d03344c2f344bcb0138833679e764d_proc_3707564/MNIST_CNN_loss_curves.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_49d03344c2f344bcb0138833679e764d_proc_3707564/MNIST_CGR_curves.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_49d03344c2f344bcb0138833679e764d_proc_3707564/MNIST_CNN_accuracy_curves.png"], ["experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_37cab4e5477b4df5a7725c1588c249df_proc_3707565/mnist_cnn_loss_curves.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_37cab4e5477b4df5a7725c1588c249df_proc_3707565/mnist_mlp_loss_curves.png"], [], [], ["experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_85159681bbe84bbc9bccdb7aed08ca02_proc_3707565/mnist_loss_curves_label_smoothing.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_85159681bbe84bbc9bccdb7aed08ca02_proc_3707565/mnist_accuracy_curves_label_smoothing.png"], [], [], [], [], [], ["experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_7e8b9755eedc4da898b0fa8d8ee0f3a9_proc_3707564/accuracy_curves_MNIST.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_7e8b9755eedc4da898b0fa8d8ee0f3a9_proc_3707564/loss_curves_MNIST.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_7e8b9755eedc4da898b0fa8d8ee0f3a9_proc_3707564/CGR_curves_MNIST.png"], ["experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_f41f33d4aedb4e4d9add6d07ea1eb8fb_proc_3707563/mnist_loss_n_epochs_15.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_f41f33d4aedb4e4d9add6d07ea1eb8fb_proc_3707563/mnist_loss_n_epochs_10.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_f41f33d4aedb4e4d9add6d07ea1eb8fb_proc_3707563/mnist_loss_n_epochs_5.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_f41f33d4aedb4e4d9add6d07ea1eb8fb_proc_3707563/mnist_loss_n_epochs_20.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_f41f33d4aedb4e4d9add6d07ea1eb8fb_proc_3707563/mnist_cgr.png"], ["experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_bd4200573eac4c5b88b8cc01b63cfa1d_proc_3707565/mnist_loss_n_epochs_15.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_bd4200573eac4c5b88b8cc01b63cfa1d_proc_3707565/mnist_loss_n_epochs_10.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_bd4200573eac4c5b88b8cc01b63cfa1d_proc_3707565/mnist_loss_n_epochs_5.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_bd4200573eac4c5b88b8cc01b63cfa1d_proc_3707565/mnist_loss_n_epochs_20.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_bd4200573eac4c5b88b8cc01b63cfa1d_proc_3707565/mnist_cgr.png"], [], ["experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/seed_aggregation_5f79ad9b7d624e6592c2f7230b5acc62/mnist_mean_cgr.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/seed_aggregation_5f79ad9b7d624e6592c2f7230b5acc62/mnist_mean_loss_n_epochs_20.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/seed_aggregation_5f79ad9b7d624e6592c2f7230b5acc62/mnist_mean_loss_n_epochs_10.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/seed_aggregation_5f79ad9b7d624e6592c2f7230b5acc62/mnist_mean_orig_acc.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/seed_aggregation_5f79ad9b7d624e6592c2f7230b5acc62/mnist_mean_loss_n_epochs_5.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/seed_aggregation_5f79ad9b7d624e6592c2f7230b5acc62/mnist_mean_aug_acc.png", "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/seed_aggregation_5f79ad9b7d624e6592c2f7230b5acc62/mnist_mean_loss_n_epochs_15.png"]], "plot_analyses": [[{"analysis": "Original and augmented accuracy curves for MLP and CNN on MNIST reveal rapid saturation of performance on the static test set, with MLP original accuracy rising from ~95.7% to ~97.7% and CNN from ~97.5% to ~98.5% by epoch 5. Augmented accuracy lags behind: MLP moves from ~86.5% up to ~90.3%, and CNN from ~90.2% to ~91.2%, peaking around epoch 3 (MLP: ~89.9%, CNN: ~90.6%) then dipping at epoch 4 before a final uptick. This persistent gap indicates that even simple augmentations still leave substantial unexplored challenge in MNIST, and that CNNs close it slightly faster than MLPs but still far from original-level performance.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_bf06f8102609494db1f0ef77f656a3d3_proc_3702064/mnist_accuracy_curves.png"}, {"analysis": "Training and validation losses exhibit steady declines for both architectures. MLP train loss falls from ~0.26 to ~0.05 and validation loss from ~0.14 to ~0.08 over five epochs, showing consistent learning with minimal overfitting. CNN train loss plunges from ~0.20 to ~0.028 and validation loss from ~0.075 down to ~0.05 by epoch 4, then rises slightly to ~0.06 at epoch 5\u2014evidence of slight overfitting emerging late but overall strong generalization.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_bf06f8102609494db1f0ef77f656a3d3_proc_3702064/mnist_loss_curves.png"}, {"analysis": "CGR (Challenge Gap Recovery) fluctuates markedly across epochs: starting high (~0.96), dropping negative at epoch 2 (~\u20130.19), hovering near zero at epoch 3, plunging to ~\u20130.96 at epoch 4, and rebounding to ~0.53 at epoch 5. This volatility suggests that simple augmentations temporarily restore discriminative power only at certain training stages, while at others they may even exacerbate saturation. Fine-grained, epoch-aware rejuvenation strategies could stabilize CGR and deliver more consistent challenge restoration.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_bf06f8102609494db1f0ef77f656a3d3_proc_3702064/mnist_cgr_curve.png"}], [{"analysis": "Training an MLP for 15 epochs yields steadily decreasing train loss but validation loss plateaus around epoch 5\u201310 and then oscillates upward after epoch 11, indicating overfitting beyond roughly 10 epochs. A CNN trained for the same 15 epochs shows similar behavior: smooth train\u2010loss decline but a spike in validation loss around epoch 10, followed by gradual rise, suggesting that additional epochs chiefly refine memorization rather than generalization.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_ba6e0e5c2f904eeb9af493981feb0491_proc_3707563/mnist_loss_n_epochs_15.png"}, {"analysis": "With 10 epochs, the MLP\u2019s train loss decreases steadily and validation loss reaches a low around epoch 5\u20136 before rising slightly towards epoch 10; overfitting is present but less severe than at 15 epochs. The CNN under 10 epochs shows a clear trough in validation loss around epoch 3\u20135 with limited post\u2010trough drift, indicating that around 6\u20138 epochs may be close to its sweet spot before generalization degrades.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_ba6e0e5c2f904eeb9af493981feb0491_proc_3707563/mnist_loss_n_epochs_10.png"}, {"analysis": "After only 5 epochs, the MLP still shows a gap between train and validation losses, but the validation curve is relatively flat after epoch 3, suggesting mild underfitting. The CNN\u2019s validation loss similarly flattens by epoch 3\u20134 without upward drift, implying that it may not have fully captured the dataset complexity and would benefit from more epochs to improve validation performance.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_ba6e0e5c2f904eeb9af493981feb0491_proc_3707563/mnist_loss_n_epochs_5.png"}, {"analysis": "Running 20 epochs exacerbates overfitting: the MLP\u2019s validation loss begins increasing around epoch 6 and continues rising with volatility, while training loss approaches zero. The CNN behaves similarly, with validation loss creeping upward after epoch 5 and pronounced fluctuations thereafter. Extending beyond 10\u201312 epochs yields diminishing returns on true generalization.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_ba6e0e5c2f904eeb9af493981feb0491_proc_3707563/mnist_loss_n_epochs_20.png"}, {"analysis": "Computing the Challenge Gap Ratio (CGR) for each epoch shows that the 15\u2010epoch run achieves the highest CGR peak around epoch 5, marking maximal discrimination among models, whereas the 10\u2010epoch curve peaks earlier but declines more sharply. The 5\u2010epoch setting never attains high CGR, and the 20\u2010epoch curve indicates an initial moderate CGR that then decays. These trends suggest optimal generalization and model ranking separation around 5\u20138 epochs.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_ba6e0e5c2f904eeb9af493981feb0491_proc_3707563/mnist_cgr.png"}], [], [{"analysis": "Training loss curves for the CNN on MNIST reveal that smaller batch sizes consistently yield faster descent and lower final training loss. At epoch 1, batch size 32 starts around 0.16 and falls to approximately 0.02 by epoch 5, whereas batch size 256 begins near 0.31 and only reaches about 0.045 at the end. The sharpest drop occurs between epochs 1 and 2 across all batch sizes, suggesting that diminishing returns set in after epoch 2 in terms of raw training\u2010loss reduction.\n\nValidation losses mirror this trend: batch size 32 and 64 achieve the lowest minima (around 0.044\u20130.048) by epoch 4\u20135, while batch sizes 128 and 256 settle higher (around 0.051\u20130.056). A mild uptick in validation loss for batch size 32 at epoch 5 hints at slight overfitting if training continues beyond epoch 4. Overall, the CNN benefits from smaller batches for both convergence speed and generalization, although all batch sizes narrow the gap by epoch 5, indicating stable convergence across hyperparameter choices by the end of training.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_37cab4e5477b4df5a7725c1588c249df_proc_3707565/mnist_cnn_loss_curves.png"}, {"analysis": "Loss curves for the MLP on MNIST exhibit the same qualitative ordering by batch size but at generally higher loss levels and slower convergence compared to the CNN. Training loss for batch size 32 starts near 0.23 and falls to about 0.04 by epoch 5. In contrast, batch size 256 goes from roughly 0.38 down to 0.07\u2014about double the final loss of the smallest batch. The steepest descent again appears between epochs 1 and 2.\n\nValidation metrics confirm that the MLP underfits relative to the CNN, with final losses around 0.08\u20130.095 versus 0.044\u20130.056 for the CNN. Batch size 32 yields the best MLP validation loss (~0.087 by epoch 5), but a slight upward blip at epoch 4 suggests over-training past epoch 3. Batch sizes 64 and 128 also display similar minor fluctuations. These patterns point to a sweet spot of 3\u20134 epochs for the MLP, beyond which gains are marginal or negative in terms of validation performance.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_37cab4e5477b4df5a7725c1588c249df_proc_3707565/mnist_mlp_loss_curves.png"}], [], [], [{"analysis": "Label smoothing strongly influences convergence speed and final loss values. With \u03b5=0.0 the model attains the lowest training and validation losses by epoch 5, but it also converges fastest, risking overconfidence. Moderate smoothing (\u03b5=0.05) slows down training loss reduction\u2014ending at ~0.36\u2014but validation loss tracks training closely, suggesting better calibration. Higher smoothing rates (\u03b5=0.1, \u03b5=0.2) progressively raise both train and val curves, with \u03b5=0.2 still sitting near 1.0 at epoch 5. All settings close the train\u2013val gap by the end, so smoothing does not appear to deliver a clear regularization boost in loss, but it does control overconfident predictions and pushes up the loss floor.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_85159681bbe84bbc9bccdb7aed08ca02_proc_3707565/mnist_loss_curves_label_smoothing.png"}, {"analysis": "Original\u2010test accuracy remains high (98\u201399%) across all \u03b5 values, indicating the base model already saturates MNIST. Out\u2010of\u2010distribution or augmented\u2010data accuracy (\u201caug_acc\u201d) tells a different story. Without smoothing (\u03b5=0.0) aug_acc stalls around 0.91; moderate smoothing (\u03b5=0.05) peaks at ~0.93 (epoch 3), boosting robustness to perturbations. \u03b5=0.1 also improves aug_acc (up to 0.925) but plateaus, while heavy smoothing (\u03b5=0.2) drags aug_acc downward. Hence \u03b5\u22480.05 strikes the best balance between in\u2010distribution performance and generalization to augmented or corrupted inputs.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_85159681bbe84bbc9bccdb7aed08ca02_proc_3707565/mnist_accuracy_curves_label_smoothing.png"}], [], [], [], [], [], [{"analysis": "Original accuracy curves show that CNN consistently outperforms MLP across all \u03b2 values. Higher \u03b2 tends to boost final accuracy: at epoch 5, CNN \u03b2=0.95 reaches ~98.6%, beating \u03b2=0.85 (~98.3%) and \u03b2=0.8 (~98.5%). MLP curves rise more gradually, topping at ~97.5% for \u03b2=0.95. Performance saturates around epoch 3\u20134, with diminishing gains thereafter, and slight plateau or drop at epoch 5 for some settings (e.g., CNN \u03b2=0.85).", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_7e8b9755eedc4da898b0fa8d8ee0f3a9_proc_3707564/accuracy_curves_MNIST.png"}, {"analysis": "Augmented accuracy is lower across the board, indicating the augmented test set is more challenging. CNN still leads: CNN \u03b2=0.8 improves from ~0.895 at epoch 1 to ~0.912 at epoch 5, while CNN \u03b2=0.95 peaks at ~0.917 (epoch 3) then dips to ~0.902. MLP shows a narrower spread (~0.87\u20130.90) and less epoch\u2010to\u2010epoch gain. Augmentation increases variance in rankings by epoch, pointing to remaining capacity for hyperparameter optimization to stabilize results under data shifts.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_7e8b9755eedc4da898b0fa8d8ee0f3a9_proc_3707564/loss_curves_MNIST.png"}, {"analysis": "CGR (Challenge Gap Recovery) is highly volatile early on. For \u03b2=0.8, CGR swings from +0.8 (epoch 1) to \u20131.0 (epoch 2), then rebounds to +1.6 by epoch 5. \u03b2=0.85 shows a similar U\u2010shape: falling to \u20131.0 at epoch 2, rising to +1.1 at epoch 4, then flattening. \u03b2=0.95 starts strong (+1.5), dips slightly, peaks at +0.65 (epoch 4) before dropping. These fluctuations suggest that 5 epochs may be insufficient for stable rejuvenation effects and that \u03b2 around 0.95 gives the most consistent positive CGR early but still dips at later epochs.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_7e8b9755eedc4da898b0fa8d8ee0f3a9_proc_3707564/CGR_curves_MNIST.png"}], [{"analysis": "For n_epochs=15 (MLP vs CNN): MLP training loss steadily decreases to near zero by epoch 15, but validation loss bottoms around epoch 4\u20136 (~0.075\u20130.085) and then fluctuates upward to ~0.10, indicating overfitting after 5 epochs. CNN training loss similarly decreases rapidly, with validation loss minimizing around epoch 3\u20134 (~0.055) before climbing to ~0.08\u20130.10 by epoch 15. CNN generalizes better initially but also overfits if trained too long.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_f41f33d4aedb4e4d9add6d07ea1eb8fb_proc_3707563/mnist_loss_n_epochs_15.png"}, {"analysis": "For n_epochs=10: MLP validation loss reaches its minimum around epoch 4 (~0.08) then increases slightly to ~0.09 by epoch 10, while training loss continues falling. CNN validation loss is lowest around epoch 3\u20134 (~0.05), then modestly rises to ~0.055 by epoch 10. Both models show optimal generalization in the first 4\u20135 epochs and overfitting thereafter, with CNN consistently outperforming the MLP on validation loss.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_f41f33d4aedb4e4d9add6d07ea1eb8fb_proc_3707563/mnist_loss_n_epochs_10.png"}, {"analysis": "For n_epochs=5: MLP validation loss decreases until epoch 3\u20134 (~0.075\u20130.08) then edges up slightly at epoch 5 (~0.083), suggesting an early stopping point around 3\u20134 epochs. CNN validation loss falls monotonically across all 5 epochs, reaching ~0.048 at epoch 5, with no clear sign of overfitting in this short run. This confirms that very few epochs suffice for strong CNN performance on MNIST.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_f41f33d4aedb4e4d9add6d07ea1eb8fb_proc_3707563/mnist_loss_n_epochs_5.png"}, {"analysis": "For n_epochs=20: MLP validation loss again bottoms at ~0.08 around epochs 3\u20136 before trending upward with spikes up to ~0.13, a clear overfitting pattern. CNN shows its lowest validation loss (~0.05) at epochs 3\u20136, then a steady increase to ~0.085 by epoch 20. Extending training beyond ~6 epochs harms generalization for both models.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_f41f33d4aedb4e4d9add6d07ea1eb8fb_proc_3707563/mnist_loss_n_epochs_20.png"}, {"analysis": "CGR vs Epoch: All four curves peak between epochs 3\u20135, with the 20-epoch case achieving the highest maximum CGR (~2.55 at epoch 4), but then exhibiting negative dips around epochs 6\u20138 and high variance thereafter. Lower-epoch runs (5 and 10) deliver more stable CGR with peaks ~2.2 and ~2.0, respectively. This suggests that a small number of training epochs (~4\u20135) maximizes challenge gap recovery while avoiding instability.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_f41f33d4aedb4e4d9add6d07ea1eb8fb_proc_3707563/mnist_cgr.png"}], [{"analysis": "For n_epochs=15, the MLP training loss steadily declines from ~0.25 to ~0.015 by epoch 15, while validation loss drops quickly to ~0.08 by epoch 4 then oscillates between 0.08 and 0.11 thereafter, indicating mild overfitting beyond epoch 4\u20136. The CNN training loss similarly falls rapidly to ~0.01, with validation loss bottoming around ~0.045 at epoch 4 before gradually rising to ~0.065 by epoch 11, showing that both architectures start to overfit when trained past ~5 epochs, though the CNN achieves a lower validation floor.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_bd4200573eac4c5b88b8cc01b63cfa1d_proc_3707565/mnist_loss_n_epochs_15.png"}, {"analysis": "For n_epochs=10, the MLP training curve decreases from ~0.26 to ~0.02, with validation loss reaching a minimum of ~0.08 at epoch 4 before drifting upward to ~0.09 by epoch 10. The CNN again converges faster, training loss falling to ~0.015 and validation reaching ~0.05 by epoch 3, then stabilizing around 0.05\u20130.055. This run length reduces overfitting compared to 15 epochs, and suggests optimal stopping around epoch 4 for the CNN and epoch 5 for the MLP.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_bd4200573eac4c5b88b8cc01b63cfa1d_proc_3707565/mnist_loss_n_epochs_10.png"}, {"analysis": "For n_epochs=5, the MLP shows underfitting relative to longer schedules\u2014training loss from ~0.26 to ~0.045 and validation from ~0.135 to ~0.076, with no strong divergence. The CNN training loss drops from ~0.20 to ~0.03, and validation from ~0.075 to ~0.053, exhibiting a small generalization gap but still not reaching the lower floors seen in longer runs. This suggests 5 epochs may be too few for full convergence, especially for MLP.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_bd4200573eac4c5b88b8cc01b63cfa1d_proc_3707565/mnist_loss_n_epochs_5.png"}, {"analysis": "For n_epochs=20, the MLP training loss continues to shrink to near-zero, but validation loss bottoms around ~0.08 at epoch 4 then increases up to ~0.115 by epoch 20, showing pronounced overfitting. The CNN mirrors this trend: training loss reaches ~0.005, while validation loss plateaus near ~0.045 by epoch 3 before climbing to ~0.09 at epoch 18. This confirms that extending beyond ~10 epochs yields diminishing returns and greater overfitting.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_bd4200573eac4c5b88b8cc01b63cfa1d_proc_3707565/mnist_loss_n_epochs_20.png"}, {"analysis": "In the CGR (Challenge Gap Recovery) versus epoch plot, runs with 5 and 10 epochs exhibit moderate, stable CGR values around ~1.2\u20131.4 across their lifespans. The 15-epoch run peaks around ~2.6 at epoch 10 before fluctuating, while the 20-epoch run shows sporadic high peaks (e.g., ~2.45 at epoch 18) but overall noisy, unstable CGR. This indicates that a mid-range training duration (around 10 epochs) delivers reliable, consistent challenge-gap improvements, whereas overly long schedules introduce volatility.", "plot_path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_bd4200573eac4c5b88b8cc01b63cfa1d_proc_3707565/mnist_cgr.png"}], [], []], "vlm_feedback_summary": ["Experiment on MNIST shows static benchmarks saturate quickly, augmentations\npartially restore challenge but in a volatile way. Metrics reveal room to\nimprove targeted rejuvenation to maintain discriminative power across training.", "Validation loss analysis reveals overfitting emerges beyond roughly 8\u201310 epochs\nfor both MLP and CNN architectures, with the CNN stabilizing generalization\nslightly earlier. Underfitting occurs with too few epochs (\u22645). The CGR analysis\nconfirms the best discrimination among models at mid\u2010training (5\u20138 epochs). To\nrefine baseline tuning, focus future runs on epoch ranges 6\u201312, adjust learning\nrate decay or early stopping to balance under- and overfitting, and evaluate on\nadditional analogous digit benchmarks to test robustness.", "[]", "CNN outperforms MLP in both convergence speed and final generalization on MNIST.\nSmaller batch sizes (32, 64) consistently achieve lower training and validation\nlosses, but slight validation overfitting appears around epoch 4\u20135. MLP shows\nhigher losses and earlier overfitting, recommending fewer epochs. Consider\ntuning learning rates per batch size and adding weight decay or dropout to\ncounteract overfitting.", "[]", "[]", "Loss curves show that increasing label smoothing slows convergence and raises\nasymptotic loss without widening the train\u2013val gap. Accuracy curves reveal that\nmoderate smoothing (\u03b5\u22480.05) yields the strongest robustness gains on augmented\nevaluation sets, while heavy smoothing hurts both in\u2010 and out\u2010of\u2010distribution\nperformance.", "[]", "[]", "[]", "[]", "[]", "Baseline shows CNNs outperform MLPs and benefit from higher \u03b2. Augmented tests\nconfirm increased challenge with more variance. Loss curves (from separate view)\nindicate minimal overfitting but reveal plateauing after ~epoch 3. CGR\ninstability implies extending epochs, learning\u2010rate scheduling, or batch\u2010size\nadjustments is needed to achieve steady challenge recovery. Suggest next tuning\nwith a lower initial learning rate (e.g., \u00d70.1), extended training to 10 epochs,\nand experimenting with batch sizes (64\u2192256). Propose evaluating on two\nadditional HuggingFace image\u2010classification benchmarks: EMNIST (balanced split)\nand Kuzushiji\u2010MNIST to measure generalization under distributional shifts.", "Validation curves and CGR trends indicate that both the MLP and CNN models on\nMNIST generalize best within the first 4\u20136 epochs; training beyond that leads to\noverfitting and diminishing returns. CNN outperforms the MLP consistently.\nChallenge Gap Recovery is maximized at around 3\u20135 epochs, with longer runs\nintroducing noise and negative dips. Optimal hyperparameter strategy should\ninclude early stopping around 5 epochs, potential learning rate decay, and\nregularization to stabilize CGR.", "Longer epoch schedules reduce training loss but incur overfitting past ~10\nepochs; optimal performance and stable challenge-gap recovery occur around 5\u201310\nepochs, with CNN outperforming MLP in generalization. Mid-range training offers\nthe best trade-off.", "[]", "[]"], "exec_time": [188.66474843025208, 1350.9031834602356, 687.9656908512115, 560.3982903957367, 533.4731593132019, 729.8796484470367, 301.5841636657715, 10.381654024124146, 16.511117935180664, 10.402327060699463, 16.842084646224976, 10.981927156448364, 470.07356905937195, 1355.7535536289215, 1343.0728869438171, 1329.9110941886902, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"MNIST\"]"], ["['MNIST']"], [], ["[MNIST]"], [], [], ["MNIST"], [], [], [], [], [], ["[\"MNIST\"]"], ["['MNIST']"], ["['MNIST']"], [], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = {}\n\n# Extract arrays\nloss_train = data.get(\"original\", {}).get(\"losses\", {}).get(\"train\", [])\nloss_val = data.get(\"original\", {}).get(\"losses\", {}).get(\"val\", [])\norig_acc = data.get(\"original\", {}).get(\"metrics\", {}).get(\"orig_acc\", [])\naug_acc = data.get(\"original\", {}).get(\"metrics\", {}).get(\"aug_acc\", [])\ncgr_vals = data.get(\"CGR\", [])\n\n# Determine epochs and split per model (MLP then CNN)\nn_models = 2\nn_epochs = len(loss_train) // n_models if n_models else 0\nepochs = np.arange(1, n_epochs + 1)\ntrain_mlp = loss_train[0::2]\ntrain_cnn = loss_train[1::2]\nval_mlp = loss_val[0::2]\nval_cnn = loss_val[1::2]\norig_mlp = orig_acc[0::2]\norig_cnn = orig_acc[1::2]\naug_mlp = aug_acc[0::2]\naug_cnn = aug_acc[1::2]\n\n# Plot loss curves\ntry:\n    plt.figure()\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_mlp, label=\"Train Loss\")\n    plt.plot(epochs, val_mlp, label=\"Val Loss\")\n    plt.title(\"Left: MLP on MNIST\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_cnn, label=\"Train Loss\")\n    plt.plot(epochs, val_cnn, label=\"Val Loss\")\n    plt.title(\"Right: CNN on MNIST\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.suptitle(\"Training and Validation Losses (MNIST)\")\n    plt.savefig(os.path.join(working_dir, \"mnist_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Plot accuracy curves\ntry:\n    plt.figure()\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, orig_mlp, label=\"Orig Acc\")\n    plt.plot(epochs, aug_mlp, label=\"Aug Acc\")\n    plt.title(\"Left: MLP on MNIST\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, orig_cnn, label=\"Orig Acc\")\n    plt.plot(epochs, aug_cnn, label=\"Aug Acc\")\n    plt.title(\"Right: CNN on MNIST\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.suptitle(\"Original and Augmented Accuracy (MNIST)\")\n    plt.savefig(os.path.join(working_dir, \"mnist_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves plot: {e}\")\n    plt.close()\n\n# Plot CGR curve\ntry:\n    plt.figure()\n    plt.plot(np.arange(1, len(cgr_vals) + 1), cgr_vals, marker=\"o\")\n    plt.title(\"CGR across Epochs on MNIST\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CGR\")\n    plt.savefig(os.path.join(working_dir, \"mnist_cgr_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CGR plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Print final original and augmented accuracy for each model & n_epochs\nfor n in sorted(experiment_data.get(\"n_epochs\", {}), key=lambda x: int(x)):\n    run = experiment_data[\"n_epochs\"][n]\n    for model in run[\"models\"]:\n        orig_acc = run[\"models\"][model][\"metrics\"][\"orig_acc\"][-1]\n        aug_acc = run[\"models\"][model][\"metrics\"][\"aug_acc\"][-1]\n        print(f\"{model} (n_epochs={n}): orig_acc={orig_acc:.4f}, aug_acc={aug_acc:.4f}\")\n\n# Plot training vs val loss for each n_epochs (MLP & CNN)\nfor n in sorted(experiment_data.get(\"n_epochs\", {}), key=lambda x: int(x)):\n    run = experiment_data[\"n_epochs\"][n]\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n        # MLP losses\n        mlp = run[\"models\"][\"MLP\"][\"losses\"]\n        epochs_mlp = range(1, len(mlp[\"train\"]) + 1)\n        axes[0].plot(epochs_mlp, mlp[\"train\"], label=\"Train Loss\")\n        axes[0].plot(epochs_mlp, mlp[\"val\"], label=\"Val Loss\")\n        axes[0].set_title(\"Left: Training vs Val Loss (MLP)\")\n        axes[0].set_xlabel(\"Epoch\")\n        axes[0].set_ylabel(\"Loss\")\n        axes[0].legend()\n        # CNN losses\n        cnn = run[\"models\"][\"CNN\"][\"losses\"]\n        epochs_cnn = range(1, len(cnn[\"train\"]) + 1)\n        axes[1].plot(epochs_cnn, cnn[\"train\"], label=\"Train Loss\")\n        axes[1].plot(epochs_cnn, cnn[\"val\"], label=\"Val Loss\")\n        axes[1].set_title(\"Right: Training vs Val Loss (CNN)\")\n        axes[1].set_xlabel(\"Epoch\")\n        axes[1].set_ylabel(\"Loss\")\n        axes[1].legend()\n        fig.suptitle(f\"MNIST Loss Curves (n_epochs={n})\")\n        plt.tight_layout(rect=[0, 0, 1, 0.95])\n        fp = os.path.join(working_dir, f\"mnist_loss_n_epochs_{n}.png\")\n        plt.savefig(fp)\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating loss plot for n_epochs={n}: {e}\")\n        plt.close()\n\n# Plot CGR vs epoch for all n_epochs\ntry:\n    plt.figure()\n    for n in sorted(experiment_data.get(\"n_epochs\", {}), key=lambda x: int(x)):\n        cgr = experiment_data[\"n_epochs\"][n][\"cgr\"]\n        plt.plot(range(1, len(cgr) + 1), cgr, marker=\"o\", label=f\"n_epochs={n}\")\n    plt.title(\"CGR vs Epoch for MNIST\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CGR\")\n    plt.legend()\n    fp = os.path.join(working_dir, \"mnist_cgr.png\")\n    plt.savefig(fp)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CGR plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    lr_keys = sorted(experiment_data[\"learning_rate\"].keys(), key=float)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot loss curves for MLP and CNN\nfor model in [\"MLP\", \"CNN\"]:\n    try:\n        fig = plt.figure(figsize=(10, 4))\n        # Training loss\n        ax1 = fig.add_subplot(1, 2, 1)\n        for lr in lr_keys:\n            losses = experiment_data[\"learning_rate\"][lr][model][\"losses\"][\"train\"]\n            ax1.plot(np.arange(1, len(losses) + 1), losses, label=lr)\n        ax1.set_title(\"Left: Training Loss Curves\")\n        ax1.set_xlabel(\"Epoch\")\n        ax1.set_ylabel(\"Loss\")\n        ax1.legend(title=\"LR\")\n        # Validation loss\n        ax2 = fig.add_subplot(1, 2, 2)\n        for lr in lr_keys:\n            losses = experiment_data[\"learning_rate\"][lr][model][\"losses\"][\"val\"]\n            ax2.plot(np.arange(1, len(losses) + 1), losses, label=lr)\n        ax2.set_title(\"Right: Validation Loss Curves\")\n        ax2.set_xlabel(\"Epoch\")\n        ax2.set_ylabel(\"Loss\")\n        ax2.legend(title=\"LR\")\n        fig.suptitle(f\"{model} Loss Curves on MNIST\")\n        plt.tight_layout(rect=[0, 0, 1, 0.95])\n        plt.savefig(os.path.join(working_dir, f\"MNIST_{model}_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {model}: {e}\")\n        plt.close()\n\n# Plot accuracy curves for MLP and CNN\nfor model in [\"MLP\", \"CNN\"]:\n    try:\n        fig = plt.figure(figsize=(10, 4))\n        ax1 = fig.add_subplot(1, 2, 1)\n        for lr in lr_keys:\n            acc = experiment_data[\"learning_rate\"][lr][model][\"metrics\"][\"orig_acc\"]\n            ax1.plot(np.arange(1, len(acc) + 1), acc, label=lr)\n        ax1.set_title(\"Left: Original Test Accuracy\")\n        ax1.set_xlabel(\"Epoch\")\n        ax1.set_ylabel(\"Accuracy\")\n        ax1.legend(title=\"LR\")\n        ax2 = fig.add_subplot(1, 2, 2)\n        for lr in lr_keys:\n            acc = experiment_data[\"learning_rate\"][lr][model][\"metrics\"][\"aug_acc\"]\n            ax2.plot(np.arange(1, len(acc) + 1), acc, label=lr)\n        ax2.set_title(\"Right: Augmented Test Accuracy\")\n        ax2.set_xlabel(\"Epoch\")\n        ax2.set_ylabel(\"Accuracy\")\n        ax2.legend(title=\"LR\")\n        fig.suptitle(f\"{model} Accuracy Curves on MNIST\")\n        plt.tight_layout(rect=[0, 0, 1, 0.95])\n        plt.savefig(os.path.join(working_dir, f\"MNIST_{model}_accuracy_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot for {model}: {e}\")\n        plt.close()\n\n# Plot CGR curves\ntry:\n    plt.figure()\n    for lr in lr_keys:\n        cgr = experiment_data[\"learning_rate\"][lr][\"CGR\"]\n        plt.plot(np.arange(1, len(cgr) + 1), cgr, label=lr)\n    plt.title(\"CGR over Epochs on MNIST\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CGR\")\n    plt.legend(title=\"LR\")\n    plt.savefig(os.path.join(working_dir, \"MNIST_CGR_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CGR plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nmodels = [\"MLP\", \"CNN\"]\nbs_keys = sorted(experiment_data.get(\"batch_size_sweep\", {}).keys(), key=int)\n\nfor model_name in models:\n    try:\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        for bs in bs_keys:\n            mdata = experiment_data[\"batch_size_sweep\"][bs][model_name]\n            epochs = range(1, len(mdata[\"losses\"][\"train\"]) + 1)\n            axs[0].plot(epochs, mdata[\"losses\"][\"train\"], label=f\"BS {bs}\")\n            axs[1].plot(epochs, mdata[\"losses\"][\"val\"], label=f\"BS {bs}\")\n        axs[0].set_title(\"Left: Training Loss, Dataset: MNIST\")\n        axs[0].set_xlabel(\"Epoch\")\n        axs[0].set_ylabel(\"Loss\")\n        axs[0].legend()\n        axs[1].set_title(\"Right: Validation Loss, Dataset: MNIST\")\n        axs[1].set_xlabel(\"Epoch\")\n        axs[1].set_ylabel(\"Loss\")\n        axs[1].legend()\n        fig.suptitle(f\"MNIST Loss Curves - {model_name}\")\n        plt.savefig(\n            os.path.join(working_dir, f\"mnist_{model_name.lower()}_loss_curves.png\")\n        )\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating {model_name} loss plot: {e}\")\n        plt.close()\n\nprint(\"\\nFinal Test Accuracies (Original / Augmented):\")\nfor bs in bs_keys:\n    for model_name in models:\n        mdata = experiment_data[\"batch_size_sweep\"][bs][model_name]\n        oacc = mdata[\"metrics\"][\"orig_acc\"][-1]\n        aacc = mdata[\"metrics\"][\"aug_acc\"][-1]\n        print(f\"BS {bs} {model_name}: orig_acc={oacc:.4f}, aug_acc={aacc:.4f}\")\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Print final accuracies\nfor key, data in experiment_data.get(\"label_smoothing\", {}).items():\n    eps = key.split(\"_\")[1]\n    orig_acc = data[\"metrics\"][\"orig_acc\"][-1]\n    aug_acc = data[\"metrics\"][\"aug_acc\"][-1]\n    print(\n        f\"\u03b5={eps}: original accuracy = {orig_acc:.4f}, augmented accuracy = {aug_acc:.4f}\"\n    )\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for key, data in experiment_data.get(\"label_smoothing\", {}).items():\n        eps = key.split(\"_\")[1]\n        train_losses = data[\"losses\"][\"train\"]\n        val_losses = data[\"losses\"][\"val\"]\n        epochs = np.arange(1, len(train_losses) + 1)\n        plt.plot(epochs, train_losses, label=f\"\u03b5={eps} train\")\n        plt.plot(epochs, val_losses, linestyle=\"--\", label=f\"\u03b5={eps} val\")\n    plt.title(\"MNIST Loss Curves Across Label Smoothing Values\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"mnist_loss_curves_label_smoothing.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Plot accuracy curves\ntry:\n    plt.figure()\n    for key, data in experiment_data.get(\"label_smoothing\", {}).items():\n        eps = key.split(\"_\")[1]\n        orig_accs = data[\"metrics\"][\"orig_acc\"]\n        aug_accs = data[\"metrics\"][\"aug_acc\"]\n        epochs = np.arange(1, len(orig_accs) + 1)\n        plt.plot(epochs, orig_accs, label=f\"\u03b5={eps} orig_acc\")\n        plt.plot(epochs, aug_accs, linestyle=\"--\", label=f\"\u03b5={eps} aug_acc\")\n    plt.title(\"MNIST Accuracy Curves Across Label Smoothing Values\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"mnist_accuracy_curves_label_smoothing.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n", null, null, null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()[\"adam_beta1\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n# 1) Loss curves\ntry:\n    fig, axes = plt.subplots(2, 1, figsize=(6, 8))\n    betas = list(exp.keys())\n    epochs = range(1, len(exp[betas[0]][\"MLP\"][\"losses\"][\"train\"]) + 1)\n    colors = [\"C0\", \"C1\", \"C2\"]\n    for i, b in enumerate(betas):\n        # MLP\n        tr = exp[b][\"MLP\"][\"losses\"][\"train\"]\n        val = exp[b][\"MLP\"][\"losses\"][\"val\"]\n        axes[0].plot(\n            epochs,\n            tr,\n            color=colors[i],\n            linestyle=\"-\",\n            label=f\"MLP \u03b2={b.split('_')[1]} train\",\n        )\n        axes[0].plot(\n            epochs,\n            val,\n            color=colors[i],\n            linestyle=\"--\",\n            label=f\"MLP \u03b2={b.split('_')[1]} val\",\n        )\n        # CNN\n        tr = exp[b][\"CNN\"][\"losses\"][\"train\"]\n        val = exp[b][\"CNN\"][\"losses\"][\"val\"]\n        axes[1].plot(\n            epochs,\n            tr,\n            color=colors[i],\n            linestyle=\"-\",\n            label=f\"CNN \u03b2={b.split('_')[1]} train\",\n        )\n        axes[1].plot(\n            epochs,\n            val,\n            color=colors[i],\n            linestyle=\"--\",\n            label=f\"CNN \u03b2={b.split('_')[1]} val\",\n        )\n    fig.suptitle(\"Training vs Validation Loss - MNIST\")\n    axes[0].set_title(\"MLP Loss (solid=train, dashed=val)\")\n    axes[1].set_title(\"CNN Loss (solid=train, dashed=val)\")\n    for ax in axes:\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n        ax.legend(fontsize=8)\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(os.path.join(working_dir, \"loss_curves_MNIST.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# 2) Accuracy curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    for i, b in enumerate(betas):\n        oa_mlp = exp[b][\"MLP\"][\"metrics\"][\"orig_acc\"]\n        aa_mlp = exp[b][\"MLP\"][\"metrics\"][\"aug_acc\"]\n        oa_cnn = exp[b][\"CNN\"][\"metrics\"][\"orig_acc\"]\n        aa_cnn = exp[b][\"CNN\"][\"metrics\"][\"aug_acc\"]\n        axes[0].plot(\n            epochs,\n            oa_mlp,\n            color=colors[i],\n            linestyle=\"-\",\n            label=f\"MLP \u03b2={b.split('_')[1]}\",\n        )\n        axes[0].plot(\n            epochs,\n            oa_cnn,\n            color=colors[i],\n            linestyle=\"--\",\n            label=f\"CNN \u03b2={b.split('_')[1]}\",\n        )\n        axes[1].plot(\n            epochs,\n            aa_mlp,\n            color=colors[i],\n            linestyle=\"-\",\n            label=f\"MLP \u03b2={b.split('_')[1]}\",\n        )\n        axes[1].plot(\n            epochs,\n            aa_cnn,\n            color=colors[i],\n            linestyle=\"--\",\n            label=f\"CNN \u03b2={b.split('_')[1]}\",\n        )\n    fig.suptitle(\"Accuracy Curves - MNIST\")\n    axes[0].set_title(\"Left: Original Accuracy\")\n    axes[1].set_title(\"Right: Augmented Accuracy\")\n    for ax in axes:\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Accuracy\")\n        ax.legend(fontsize=8)\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(os.path.join(working_dir, \"accuracy_curves_MNIST.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves plot: {e}\")\n    plt.close()\n\n# 3) CGR curves\ntry:\n    plt.figure(figsize=(6, 4))\n    for i, b in enumerate(betas):\n        cgr = exp[b][\"CGR\"]\n        plt.plot(epochs, cgr, color=colors[i], label=f\"\u03b2={b.split('_')[1]}\")\n    plt.title(\"CGR over epochs - MNIST\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CGR\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, \"CGR_curves_MNIST.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CGR curves plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Print final original and augmented accuracy for each model & n_epochs\nfor n in sorted(experiment_data.get(\"n_epochs\", {}), key=lambda x: int(x)):\n    run = experiment_data[\"n_epochs\"][n]\n    for model in run[\"models\"]:\n        orig_acc = run[\"models\"][model][\"metrics\"][\"orig_acc\"][-1]\n        aug_acc = run[\"models\"][model][\"metrics\"][\"aug_acc\"][-1]\n        print(f\"{model} (n_epochs={n}): orig_acc={orig_acc:.4f}, aug_acc={aug_acc:.4f}\")\n\n# Plot training vs val loss for each n_epochs (MLP & CNN)\nfor n in sorted(experiment_data.get(\"n_epochs\", {}), key=lambda x: int(x)):\n    run = experiment_data[\"n_epochs\"][n]\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n        # MLP losses\n        mlp = run[\"models\"][\"MLP\"][\"losses\"]\n        epochs_mlp = range(1, len(mlp[\"train\"]) + 1)\n        axes[0].plot(epochs_mlp, mlp[\"train\"], label=\"Train Loss\")\n        axes[0].plot(epochs_mlp, mlp[\"val\"], label=\"Val Loss\")\n        axes[0].set_title(\"Left: Training vs Val Loss (MLP)\")\n        axes[0].set_xlabel(\"Epoch\")\n        axes[0].set_ylabel(\"Loss\")\n        axes[0].legend()\n        # CNN losses\n        cnn = run[\"models\"][\"CNN\"][\"losses\"]\n        epochs_cnn = range(1, len(cnn[\"train\"]) + 1)\n        axes[1].plot(epochs_cnn, cnn[\"train\"], label=\"Train Loss\")\n        axes[1].plot(epochs_cnn, cnn[\"val\"], label=\"Val Loss\")\n        axes[1].set_title(\"Right: Training vs Val Loss (CNN)\")\n        axes[1].set_xlabel(\"Epoch\")\n        axes[1].set_ylabel(\"Loss\")\n        axes[1].legend()\n        fig.suptitle(f\"MNIST Loss Curves (n_epochs={n})\")\n        plt.tight_layout(rect=[0, 0, 1, 0.95])\n        fp = os.path.join(working_dir, f\"mnist_loss_n_epochs_{n}.png\")\n        plt.savefig(fp)\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating loss plot for n_epochs={n}: {e}\")\n        plt.close()\n\n# Plot CGR vs epoch for all n_epochs\ntry:\n    plt.figure()\n    for n in sorted(experiment_data.get(\"n_epochs\", {}), key=lambda x: int(x)):\n        cgr = experiment_data[\"n_epochs\"][n][\"cgr\"]\n        plt.plot(range(1, len(cgr) + 1), cgr, marker=\"o\", label=f\"n_epochs={n}\")\n    plt.title(\"CGR vs Epoch for MNIST\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CGR\")\n    plt.legend()\n    fp = os.path.join(working_dir, \"mnist_cgr.png\")\n    plt.savefig(fp)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CGR plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Print final original and augmented accuracy for each model & n_epochs\nfor n in sorted(experiment_data.get(\"n_epochs\", {}), key=lambda x: int(x)):\n    run = experiment_data[\"n_epochs\"][n]\n    for model in run[\"models\"]:\n        orig_acc = run[\"models\"][model][\"metrics\"][\"orig_acc\"][-1]\n        aug_acc = run[\"models\"][model][\"metrics\"][\"aug_acc\"][-1]\n        print(f\"{model} (n_epochs={n}): orig_acc={orig_acc:.4f}, aug_acc={aug_acc:.4f}\")\n\n# Plot training vs val loss for each n_epochs (MLP & CNN)\nfor n in sorted(experiment_data.get(\"n_epochs\", {}), key=lambda x: int(x)):\n    run = experiment_data[\"n_epochs\"][n]\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n        # MLP losses\n        mlp = run[\"models\"][\"MLP\"][\"losses\"]\n        epochs_mlp = range(1, len(mlp[\"train\"]) + 1)\n        axes[0].plot(epochs_mlp, mlp[\"train\"], label=\"Train Loss\")\n        axes[0].plot(epochs_mlp, mlp[\"val\"], label=\"Val Loss\")\n        axes[0].set_title(\"Left: Training vs Val Loss (MLP)\")\n        axes[0].set_xlabel(\"Epoch\")\n        axes[0].set_ylabel(\"Loss\")\n        axes[0].legend()\n        # CNN losses\n        cnn = run[\"models\"][\"CNN\"][\"losses\"]\n        epochs_cnn = range(1, len(cnn[\"train\"]) + 1)\n        axes[1].plot(epochs_cnn, cnn[\"train\"], label=\"Train Loss\")\n        axes[1].plot(epochs_cnn, cnn[\"val\"], label=\"Val Loss\")\n        axes[1].set_title(\"Right: Training vs Val Loss (CNN)\")\n        axes[1].set_xlabel(\"Epoch\")\n        axes[1].set_ylabel(\"Loss\")\n        axes[1].legend()\n        fig.suptitle(f\"MNIST Loss Curves (n_epochs={n})\")\n        plt.tight_layout(rect=[0, 0, 1, 0.95])\n        fp = os.path.join(working_dir, f\"mnist_loss_n_epochs_{n}.png\")\n        plt.savefig(fp)\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating loss plot for n_epochs={n}: {e}\")\n        plt.close()\n\n# Plot CGR vs epoch for all n_epochs\ntry:\n    plt.figure()\n    for n in sorted(experiment_data.get(\"n_epochs\", {}), key=lambda x: int(x)):\n        cgr = experiment_data[\"n_epochs\"][n][\"cgr\"]\n        plt.plot(range(1, len(cgr) + 1), cgr, marker=\"o\", label=f\"n_epochs={n}\")\n    plt.title(\"CGR vs Epoch for MNIST\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CGR\")\n    plt.legend()\n    fp = os.path.join(working_dir, \"mnist_cgr.png\")\n    plt.savefig(fp)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CGR plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Print final original and augmented accuracy for each model & n_epochs\nfor n in sorted(experiment_data.get(\"n_epochs\", {}), key=lambda x: int(x)):\n    run = experiment_data[\"n_epochs\"][n]\n    for model in run[\"models\"]:\n        orig_acc = run[\"models\"][model][\"metrics\"][\"orig_acc\"][-1]\n        aug_acc = run[\"models\"][model][\"metrics\"][\"aug_acc\"][-1]\n        print(f\"{model} (n_epochs={n}): orig_acc={orig_acc:.4f}, aug_acc={aug_acc:.4f}\")\n\n# Plot training vs val loss for each n_epochs (MLP & CNN)\nfor n in sorted(experiment_data.get(\"n_epochs\", {}), key=lambda x: int(x)):\n    run = experiment_data[\"n_epochs\"][n]\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n        # MLP losses\n        mlp = run[\"models\"][\"MLP\"][\"losses\"]\n        epochs_mlp = range(1, len(mlp[\"train\"]) + 1)\n        axes[0].plot(epochs_mlp, mlp[\"train\"], label=\"Train Loss\")\n        axes[0].plot(epochs_mlp, mlp[\"val\"], label=\"Val Loss\")\n        axes[0].set_title(\"Left: Training vs Val Loss (MLP)\")\n        axes[0].set_xlabel(\"Epoch\")\n        axes[0].set_ylabel(\"Loss\")\n        axes[0].legend()\n        # CNN losses\n        cnn = run[\"models\"][\"CNN\"][\"losses\"]\n        epochs_cnn = range(1, len(cnn[\"train\"]) + 1)\n        axes[1].plot(epochs_cnn, cnn[\"train\"], label=\"Train Loss\")\n        axes[1].plot(epochs_cnn, cnn[\"val\"], label=\"Val Loss\")\n        axes[1].set_title(\"Right: Training vs Val Loss (CNN)\")\n        axes[1].set_xlabel(\"Epoch\")\n        axes[1].set_ylabel(\"Loss\")\n        axes[1].legend()\n        fig.suptitle(f\"MNIST Loss Curves (n_epochs={n})\")\n        plt.tight_layout(rect=[0, 0, 1, 0.95])\n        fp = os.path.join(working_dir, f\"mnist_loss_n_epochs_{n}.png\")\n        plt.savefig(fp)\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating loss plot for n_epochs={n}: {e}\")\n        plt.close()\n\n# Plot CGR vs epoch for all n_epochs\ntry:\n    plt.figure()\n    for n in sorted(experiment_data.get(\"n_epochs\", {}), key=lambda x: int(x)):\n        cgr = experiment_data[\"n_epochs\"][n][\"cgr\"]\n        plt.plot(range(1, len(cgr) + 1), cgr, marker=\"o\", label=f\"n_epochs={n}\")\n    plt.title(\"CGR vs Epoch for MNIST\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CGR\")\n    plt.legend()\n    fp = os.path.join(working_dir, \"mnist_cgr.png\")\n    plt.savefig(fp)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CGR plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Paths to all experiment_data.npy files\nexp_paths = [\n    \"experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_f41f33d4aedb4e4d9add6d07ea1eb8fb_proc_3707563/experiment_data.npy\",\n    \"experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_bd4200573eac4c5b88b8cc01b63cfa1d_proc_3707565/experiment_data.npy\",\n    \"None/experiment_data.npy\",\n]\n\n# Load all experiment data\nall_data = []\nfor path in exp_paths:\n    try:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), path)\n        data = np.load(full_path, allow_pickle=True).item()\n        all_data.append(data)\n    except Exception as e:\n        print(f\"Error loading {path}: {e}\")\n\n# Determine all n_epochs values and all model names\nn_epochs_list = sorted(\n    {n for d in all_data for n in d.get(\"n_epochs\", {})}, key=lambda x: int(x)\n)\nmodels = sorted(\n    {\n        m\n        for d in all_data\n        for n in d.get(\"n_epochs\", {})\n        for m in d[\"n_epochs\"][n][\"models\"]\n    }\n)\n\n# Plot mean final original accuracy with SE bars\ntry:\n    x = np.arange(len(n_epochs_list))\n    width = 0.8 / len(models)\n    plt.figure()\n    for idx, model in enumerate(models):\n        means, ses = [], []\n        for n in n_epochs_list:\n            vals = [\n                d[\"n_epochs\"][n][\"models\"][model][\"metrics\"][\"orig_acc\"][-1]\n                for d in all_data\n                if n in d.get(\"n_epochs\", {})\n            ]\n            means.append(np.mean(vals))\n            ses.append(np.std(vals) / np.sqrt(len(vals)))\n            print(\n                f\"{model} (n_epochs={n}): mean_orig_acc={means[-1]:.4f} +/- {ses[-1]:.4f}\"\n            )\n        plt.bar(x + idx * width, means, width, yerr=ses, label=f\"{model} orig_acc\")\n    plt.xlabel(\"n_epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Mean Final Original Accuracy with SE Bars (MNIST)\")\n    plt.xticks(x + width * (len(models) - 1) / 2, n_epochs_list)\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"mnist_mean_orig_acc.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating orig accuracy bar plot: {e}\")\n    plt.close()\n\n# Plot mean final augmented accuracy with SE bars\ntry:\n    x = np.arange(len(n_epochs_list))\n    width = 0.8 / len(models)\n    plt.figure()\n    for idx, model in enumerate(models):\n        means, ses = [], []\n        for n in n_epochs_list:\n            vals = [\n                d[\"n_epochs\"][n][\"models\"][model][\"metrics\"][\"aug_acc\"][-1]\n                for d in all_data\n                if n in d.get(\"n_epochs\", {})\n            ]\n            means.append(np.mean(vals))\n            ses.append(np.std(vals) / np.sqrt(len(vals)))\n            print(\n                f\"{model} (n_epochs={n}): mean_aug_acc={means[-1]:.4f} +/- {ses[-1]:.4f}\"\n            )\n        plt.bar(x + idx * width, means, width, yerr=ses, label=f\"{model} aug_acc\")\n    plt.xlabel(\"n_epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Mean Final Augmented Accuracy with SE Bars (MNIST)\")\n    plt.xticks(x + width * (len(models) - 1) / 2, n_epochs_list)\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"mnist_mean_aug_acc.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aug accuracy bar plot: {e}\")\n    plt.close()\n\n# Plot aggregated training vs val loss for each n_epochs (MLP & CNN)\nfor n in n_epochs_list:\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n        for idx, model in enumerate(models):\n            # collect losses across runs\n            tr_list = [\n                d[\"n_epochs\"][n][\"models\"][model][\"losses\"][\"train\"]\n                for d in all_data\n                if n in d.get(\"n_epochs\", {})\n            ]\n            val_list = [\n                d[\"n_epochs\"][n][\"models\"][model][\"losses\"][\"val\"]\n                for d in all_data\n                if n in d.get(\"n_epochs\", {})\n            ]\n            arr_tr = np.vstack(tr_list)\n            arr_val = np.vstack(val_list)\n            mean_tr = arr_tr.mean(axis=0)\n            se_tr = arr_tr.std(axis=0) / np.sqrt(arr_tr.shape[0])\n            mean_val = arr_val.mean(axis=0)\n            se_val = arr_val.std(axis=0) / np.sqrt(arr_val.shape[0])\n            epochs = np.arange(1, len(mean_tr) + 1)\n            ax = axes[idx]\n            ax.plot(epochs, mean_tr, label=\"Mean Train Loss\")\n            ax.fill_between(epochs, mean_tr - se_tr, mean_tr + se_tr, alpha=0.3)\n            ax.plot(epochs, mean_val, label=\"Mean Val Loss\")\n            ax.fill_between(epochs, mean_val - se_val, mean_val + se_val, alpha=0.3)\n            side = \"Left\" if idx == 0 else \"Right\"\n            ax.set_title(f\"{side}: Mean Train vs Val Loss ({model})\")\n            ax.set_xlabel(\"Epoch\")\n            ax.set_ylabel(\"Loss\")\n            ax.legend()\n        fig.suptitle(f\"MNIST Mean Loss Curves with SE (n_epochs={n})\")\n        plt.tight_layout(rect=[0, 0, 1, 0.95])\n        plt.savefig(os.path.join(working_dir, f\"mnist_mean_loss_n_epochs_{n}.png\"))\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating aggregated loss plot for n_epochs={n}: {e}\")\n        plt.close()\n\n# Plot aggregated CGR vs epoch for all n_epochs\ntry:\n    plt.figure()\n    for n in n_epochs_list:\n        cgr_list = [\n            d[\"n_epochs\"][n][\"cgr\"] for d in all_data if n in d.get(\"n_epochs\", {})\n        ]\n        arr = np.vstack(cgr_list)\n        mean_cgr = arr.mean(axis=0)\n        se_cgr = arr.std(axis=0) / np.sqrt(arr.shape[0])\n        epochs = np.arange(1, len(mean_cgr) + 1)\n        plt.plot(epochs, mean_cgr, marker=\"o\", label=f\"n_epochs={n}\")\n        plt.fill_between(epochs, mean_cgr - se_cgr, mean_cgr + se_cgr, alpha=0.3)\n    plt.title(\"Mean CGR vs Epoch for MNIST with SE\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CGR\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"mnist_mean_cgr.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated CGR plot: {e}\")\n    plt.close()\n"], "plot_plan": ["We\u2019ll load the saved experiment_data.npy dict, then extract losses, accuracies, and CGR for each epoch. Because train/val losses and orig/aug accs are recorded per model (MLP then CNN) each epoch, we split each list into two arrays using slicing. We then plot three figures in separate try-except blocks: (1) a 1\u00d72 subplot of training vs validation loss curves, left for MLP, right for CNN, titled and saved as \u201cmnist_loss_curves.png\u201d, (2) a 1\u00d72 subplot of original vs augmented accuracy curves for MLP and CNN, saved as \u201cmnist_accuracy_curves.png\u201d, and (3) the CGR across epochs saved as \u201cmnist_cgr_curve.png\u201d. Each figure includes clear titles/subtitles referencing the MNIST dataset. Figures are always closed after saving and any plotting errors are caught and printed.", "I will load the saved experiment data and print out the final original and augmented accuracy for each model across all n_epochs settings. For each n_epochs run, I will create a two\u2010panel loss curve figure showing training vs. validation loss for both the MLP and CNN, adding clear subplot titles, axis labels, and a descriptive suptitle. Each loss plot is wrapped in its own try\u2010except block, saved to working_dir with a filename indicating the dataset, model, and n_epochs, and then closed to free resources. Finally, I will compile a single CGR (Conditional Generalization Ratio) plot that overlays lines for all n_epochs values, complete with a legend, axis labels, and a title, also wrapped in a try\u2010except block. Throughout, I use only the data in experiment_data.npy, stick to basic matplotlib, and ensure no more than five figures are generated. All figures are saved under working_dir and any plotting errors are printed.", "First, we load the saved experiment_data and extract sorted learning\u2010rate keys. Then for each model (MLP and CNN), we generate a two\u2010panel plot of training vs. validation loss across epochs, using left/right subtitles and a suptitle naming the dataset. Next, we produce for each model another two\u2010panel figure showing original and augmented accuracy curves, again with clear left/right subtitles. We then plot the CGR metric over epochs for all learning rates in a single figure. Each of these five summary plots is wrapped in its own try\u2010except block, given descriptive titles and axis labels, saved to the working directory with informative filenames, and closed after saving to free resources.", "We will load the saved experiment data and extract loss curves and metrics for both the MLP and CNN over the batch\u2010size sweep. For each model we aggregate all four batch sizes on the same axes and plot two subplots\u2014left showing training loss curves, right validation loss curves\u2014with clear subtitles indicating \u201cTraining\u201d and \u201cValidation\u201d respectively and specifying the MNIST dataset. Each figure creation is wrapped in a try\u2010except block, saved to the working directory under a descriptive name, and closed after saving or on error. After plotting, we iterate through all batch sizes and models to print the final original and augmented test accuracies. The code uses only basic matplotlib, numpy, and os, and does not invent any data beyond what\u2019s in experiment_data.npy.", null, null, "I will load the saved experiment data and extract the label smoothing results for each \u03b5 value. For each setting, I will print the final test accuracies on the original and augmented MNIST data. Then I will create two separate plots in their own try-except blocks. The first figure will overlay train and validation loss curves across epochs for all \u03b5 values. The second figure will overlay original and augmented accuracy curves across epochs for all \u03b5 values. Both plots will use clear titles indicating the MNIST dataset and the type of curves, along with axis labels and legends. The figures will be saved to the working directory with descriptive filenames. After saving each plot, I will close the figure to free resources. Any errors during loading or plotting will be caught and printed.", null, null, null, null, null, "We\u2019ll load the saved experiment_data from the working directory and extract the three \u03b21 settings under \u201cadam_beta1.\u201d For each beta, we collect per\u2010epoch training/validation losses for both MLP and CNN, original and augmented accuracies, and the CGR values. We then create three composite figures: (1) Loss curves with two subplots (MLP and CNN) showing solid lines for training and dashed lines for validation across epochs and \u03b21 values. (2) Accuracy curves with left/right subplots for original and augmented test set accuracies, plotting both models for each \u03b21. (3) A CGR plot across epochs for each \u03b21. Each plot is wrapped in a try/except block, includes titles, clear subtitles (e.g. \u201cLeft: Original Accuracy, Right: Augmented Accuracy\u201d), legends, and is saved to the working directory with descriptive filenames. We close each figure after saving to free resources. All plots use vanilla matplotlib and numpy and never simulate missing data.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "n_epochs", "learning_rate", "batch_size", "weight_decay", "dropout_rate", "label_smoothing", null, null, null, null, null, "adam_beta1", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["We load the saved experiment dictionary from the working directory using numpy,\nthen pull out the final entries of each recorded metric: the training loss,\nvalidation loss on the original test split, and test accuracies on both the\noriginal and augmented splits.  Each dataset name is printed, followed by its\ncorresponding metric label and final value, formatted to four decimal places.\nThe script runs immediately at import, with no entry\u2010point checks and no\nplotting.", "The script below loads the saved numpy experiment data from the working\ndirectory, iterates over each run (different `n_epochs`) and each model (`MLP`\nand `CNN`), and extracts the final training loss, validation loss, original test\naccuracy, and augmented test accuracy. It prints the dataset name before each\nmetric and uses clear labels for metrics as specified. The code runs at global\nscope without any entry\u2010point guard.", "The following script locates and loads the saved numpy file, then iterates\nthrough each learning rate and model to extract the last epoch\u2019s losses and\naccuracies.  It prints the dataset name before each metric and labels each\nmetric clearly (e.g., \u201cFinal training loss\u201d, \u201cFinal original test accuracy\u201d).\nNo entry\u2010point guard is used so the code executes immediately.", "I will load the saved experiment data from the working directory, then iterate\nover each batch size and model to extract the final training loss, validation\nloss, original test accuracy, and augmented test accuracy. For each combination\nI will print the dataset name followed by the metric name and its final value,\nformatted to four decimal places. The script runs immediately at global scope\nwithout any main guard.", "We load the saved experiment data from the working directory into a Python dict,\nthen iterate over each weight\u2010decay configuration. For each configuration, we\nextract the training and validation loss histories as well as the original and\naugmented test accuracies. We select the last epoch value for each metric and\nprint the dataset name followed by a clear metric label. All code runs\nimmediately at the global scope without an entry\u2010point guard.", "I will load the saved `experiment_data.npy` file from the working directory and\nthen loop over each model (\"MLP\" and \"CNN\") to retrieve the final (last) metrics\nfor each split. For the training split I\u2019ll print the final training loss; for\nthe original test split I\u2019ll print the final validation loss and the final\noriginal test accuracy; and for the augmented test split I\u2019ll print the final\naugmented test accuracy. Each block will clearly label the dataset and metric\nname, and the script runs immediately at global scope without any `if __name__\n== \"__main__\"` guard.", "The solution loads the saved experiment data from the \u201cworking\u201d directory, then\niterates over each label\u2010smoothing setting. For each epsilon, it extracts the\nfinal training loss, validation loss, original test accuracy, and augmented test\naccuracy from the stored lists. It prints the dataset name followed by the\nprecisely labelled metric value, all at global scope so the script executes\nimmediately. No plotting or `if __name__ == \"__main__\"` is used.", "", "The script loads the saved `experiment_data.npy` from the `working` directory\nand iterates over each weight decay configuration. For each of the four test\ndatasets (Original MNIST, Rotated MNIST, Fashion-MNIST, and Kuzushiji-MNIST), it\nextracts the final epoch\u2019s test accuracy and BDP (standard deviation across\nmodels) and prints them with explicit metric labels. The code runs immediately\nat the global scope and does not create any plots.", "I will load the saved `experiment_data.npy` from the working directory, extract\nthe metrics for each weight\u2010decay setting, and then for each of the four\nevaluation datasets (Original MNIST, Augmented MNIST, Fashion MNIST, and\nKuzushiji MNIST) I will print the final epoch\u2019s test accuracy and the between-\nmodel standard deviation (BDP) with clear, descriptive labels. All code is at\nglobal scope and runs immediately when the script is executed.", "", "The script below loads the saved `experiment_data.npy`, then iterates through\neach dataset to compute and print the highest test accuracy achieved by both the\nMLP and CNN models. It also computes and prints the maximum Between-Dataset\nPerformance Disparity (BDP) for each model and the maximum Category\nGeneralization Ratio (CGR) across all dropout rates. All outputs clearly label\neach dataset and metric and run immediately at the global scope.", "The script loads the saved NumPy experiment data from the `working` directory\nand retrieves the nested dictionary using `allow_pickle`. It then iterates\nthrough each `beta1` configuration under the `\"adam_beta1\"` key and, for both\nthe `MLP` and `CNN` models, extracts the final epoch metrics. For each\ndataset\u2014training, original test, and augmented test\u2014it prints the dataset name\nfollowed by clearly labeled metrics: final training loss, final validation loss\non the original test set, and final accuracies. Metric values are formatted to\nfour decimal places for readability. The code runs immediately at the global\nscope without an `__main__` guard.", "The script below loads the saved numpy experiment data from the working\ndirectory, iterates over each run (different `n_epochs`) and each model (`MLP`\nand `CNN`), and extracts the final training loss, validation loss, original test\naccuracy, and augmented test accuracy. It prints the dataset name before each\nmetric and uses clear labels for metrics as specified. The code runs at global\nscope without any entry\u2010point guard.", "The script below loads the saved numpy experiment data from the working\ndirectory, iterates over each run (different `n_epochs`) and each model (`MLP`\nand `CNN`), and extracts the final training loss, validation loss, original test\naccuracy, and augmented test accuracy. It prints the dataset name before each\nmetric and uses clear labels for metrics as specified. The code runs at global\nscope without any entry\u2010point guard.", "The script below loads the saved numpy experiment data from the working\ndirectory, iterates over each run (different `n_epochs`) and each model (`MLP`\nand `CNN`), and extracts the final training loss, validation loss, original test\naccuracy, and augmented test accuracy. It prints the dataset name before each\nmetric and uses clear labels for metrics as specified. The code runs at global\nscope without any entry\u2010point guard.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# 0. Locate the saved experiment file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# 1. Load the data dictionary\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# 2. Extract the recorded metrics\ntrain_losses = experiment_data[\"original\"][\"losses\"][\"train\"]\nval_losses = experiment_data[\"original\"][\"losses\"][\"val\"]\norig_accuracies = experiment_data[\"original\"][\"metrics\"][\"orig_acc\"]\naug_accuracies = experiment_data[\"original\"][\"metrics\"][\"aug_acc\"]\n\n# 5. Get the final values\nfinal_train_loss = train_losses[-1]\nfinal_val_loss = val_losses[-1]\nfinal_orig_accuracy = orig_accuracies[-1]\nfinal_aug_accuracy = aug_accuracies[-1]\n\n# 3 & 4. Print each dataset name followed by its metrics with clear labels\nprint(\"Training dataset:\")\nprint(\"final training loss:\", f\"{final_train_loss:.4f}\")\n\nprint(\"Original test dataset:\")\nprint(\"final validation loss:\", f\"{final_val_loss:.4f}\")\nprint(\"final test accuracy:\", f\"{final_orig_accuracy:.4f}\")\n\nprint(\"Augmented test dataset:\")\nprint(\"final test accuracy:\", f\"{final_aug_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# Load saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each experiment configuration\nfor n_epochs, run_data in experiment_data[\"n_epochs\"].items():\n    print(f\"n_epochs: {n_epochs}\")\n    # Iterate through each model in the run\n    for model_name, model_data in run_data[\"models\"].items():\n        print(f\"Model: {model_name}\")\n        # Extract final epoch metrics\n        training_loss = model_data[\"losses\"][\"train\"][-1]\n        validation_loss = model_data[\"losses\"][\"val\"][-1]\n        original_test_accuracy = model_data[\"metrics\"][\"orig_acc\"][-1]\n        augmented_test_accuracy = model_data[\"metrics\"][\"aug_acc\"][-1]\n        # Print metrics by dataset\n        print(\"Dataset: Training\")\n        print(f\"training loss: {training_loss:.4f}\")\n        print(\"Dataset: Validation\")\n        print(f\"validation loss: {validation_loss:.4f}\")\n        print(\"Dataset: Original Test\")\n        print(f\"original test accuracy: {original_test_accuracy:.4f}\")\n        print(\"Dataset: Augmented Test\")\n        print(f\"augmented test accuracy: {augmented_test_accuracy:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# iterate over each learning rate and model to print final metrics\nfor lr_str, lr_data in experiment_data[\"learning_rate\"].items():\n    print(f\"Learning rate: {lr_str}\")\n    for model_name in [\"MLP\", \"CNN\"]:\n        model_metrics = lr_data[model_name]\n        final_train_loss = model_metrics[\"losses\"][\"train\"][-1]\n        final_val_loss = model_metrics[\"losses\"][\"val\"][-1]\n        final_orig_acc = model_metrics[\"metrics\"][\"orig_acc\"][-1]\n        final_aug_acc = model_metrics[\"metrics\"][\"aug_acc\"][-1]\n\n        print(f\"Model: {model_name}\")\n        print(\"Dataset: Training set\")\n        print(f\"Final training loss: {final_train_loss:.4f}\")\n        print(\"Dataset: Validation set\")\n        print(f\"Final validation loss: {final_val_loss:.4f}\")\n        print(\"Dataset: Original test set\")\n        print(f\"Final original test accuracy: {final_orig_acc:.4f}\")\n        print(\"Dataset: Augmented test set\")\n        print(f\"Final augmented test accuracy: {final_aug_acc:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_file, allow_pickle=True).item()\n\n# Iterate through batch sizes and models, printing final metrics per dataset\nfor bs_str, models in experiment_data.get(\"batch_size_sweep\", {}).items():\n    for model_name, mdata in models.items():\n        print(f\"Batch size: {bs_str}, Model: {model_name}\")\n        # Extract final values\n        final_train_loss = mdata[\"losses\"][\"train\"][-1]\n        final_val_loss = mdata[\"losses\"][\"val\"][-1]\n        final_orig_acc = mdata[\"metrics\"][\"orig_acc\"][-1]\n        final_aug_acc = mdata[\"metrics\"][\"aug_acc\"][-1]\n        # Print with clear labels\n        print(f\"Training Dataset - Final training loss: {final_train_loss:.4f}\")\n        print(f\"Validation Dataset - Final validation loss: {final_val_loss:.4f}\")\n        print(\n            f\"Original Test Dataset - Final original test accuracy: {final_orig_acc:.4f}\"\n        )\n        print(\n            f\"Augmented Test Dataset - Final augmented test accuracy: {final_aug_acc:.4f}\"\n        )\n        print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract and print final metrics for each weight decay\nfor wd_str in sorted(experiment_data[\"weight_decay\"], key=lambda s: float(s)):\n    ed = experiment_data[\"weight_decay\"][wd_str]\n    train_losses = ed[\"losses\"][\"train\"]\n    val_losses = ed[\"losses\"][\"val\"]\n    orig_accs = ed[\"metrics\"][\"orig_acc\"]\n    aug_accs = ed[\"metrics\"][\"aug_acc\"]\n\n    final_train_loss = train_losses[-1]\n    final_val_loss = val_losses[-1]\n    final_orig_acc = orig_accs[-1]\n    final_aug_acc = aug_accs[-1]\n\n    print(f\"Weight decay: {wd_str}\")\n    print(\"Dataset: Training\")\n    print(f\"  Train loss: {final_train_loss:.4f}\")\n    print(\"Dataset: Validation\")\n    print(f\"  Validation loss: {final_val_loss:.4f}\")\n    print(\"Dataset: Original Test\")\n    print(f\"  Original test accuracy: {final_orig_acc:.4f}\")\n    print(\"Dataset: Augmented Test\")\n    print(f\"  Augmented test accuracy: {final_aug_acc:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract dropout results\ndropout_results = experiment_data[\"dropout\"]\n\n# Iterate over models and print final metrics\nfor model_name in [\"MLP\", \"CNN\"]:\n    model_data = dropout_results[model_name]\n    # get index of the last dropout setting\n    idx = -1\n\n    # Training split\n    final_train_loss = model_data[\"losses\"][\"train\"][idx]\n    print(f\"Dataset: Training Dataset ({model_name})\")\n    print(f\"  Training Loss: {final_train_loss:.4f}\")\n\n    # Original test split (used as validation in original code)\n    final_val_loss = model_data[\"losses\"][\"val\"][idx]\n    final_orig_acc = model_data[\"metrics\"][\"orig_acc\"][idx]\n    print(f\"Dataset: Original Test Dataset ({model_name})\")\n    print(f\"  Validation Loss: {final_val_loss:.4f}\")\n    print(f\"  Original Test Accuracy: {final_orig_acc:.4f}\")\n\n    # Augmented test split\n    final_aug_acc = model_data[\"metrics\"][\"aug_acc\"][idx]\n    print(f\"Dataset: Augmented Test Dataset ({model_name})\")\n    print(f\"  Augmented Test Accuracy: {final_aug_acc:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each label_smoothing setting\nfor eps_key, results in experiment_data[\"label_smoothing\"].items():\n    eps_value = eps_key.split(\"_\", 1)[1]\n    print(f\"Results for label smoothing \u03b5 = {eps_value}\")\n\n    # Extract final metrics\n    final_train_loss = results[\"losses\"][\"train\"][-1]\n    final_val_loss = results[\"losses\"][\"val\"][-1]\n    final_orig_acc = results[\"metrics\"][\"orig_acc\"][-1]\n    final_aug_acc = results[\"metrics\"][\"aug_acc\"][-1]\n\n    # Print metrics with dataset names\n    print(\"Dataset: Training Dataset\")\n    print(f\"Final training loss: {final_train_loss:.4f}\")\n    print(\"Dataset: Validation Dataset\")\n    print(f\"Final validation loss: {final_val_loss:.4f}\")\n    print(\"Dataset: Original Test Dataset\")\n    print(f\"Final original test accuracy: {final_orig_acc:.4f}\")\n    print(\"Dataset: Augmented Test Dataset\")\n    print(f\"Final augmented test accuracy: {final_aug_acc:.4f}\")\n    print()\n", "", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Mapping of metric keys to dataset identifiers and human-readable names\ndataset_keys = [\n    (\"orig_acc\", \"orig\"),\n    (\"aug_acc\", \"aug\"),\n    (\"fashion_acc\", \"fashion\"),\n    (\"kmnist_acc\", \"kmnist\"),\n]\ndataset_names = {\n    \"orig\": \"Original MNIST\",\n    \"aug\": \"Rotated MNIST\",\n    \"fashion\": \"Fashion-MNIST\",\n    \"kmnist\": \"Kuzushiji-MNIST\",\n}\n\n# Iterate over each weight decay setting and print final metrics\nfor wd, results in experiment_data[\"weight_decay\"].items():\n    print(f\"Weight decay: {wd}\")\n    metrics = results[\"metrics\"]\n    bdp = results[\"BDP\"]\n    for acc_key, bdp_key in dataset_keys:\n        ds_name = dataset_names[bdp_key]\n        final_acc = metrics[acc_key][-1]\n        final_bdp = bdp[bdp_key][-1]\n        print(f\"Dataset: {ds_name}\")\n        print(f\"  Final test accuracy: {final_acc:.4f}\")\n        print(f\"  Final BDP (std deviation of accuracies): {final_bdp:.4f}\")\n    print()  # Blank line for readability between settings\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each weight decay setting\nfor wd_str, ed in experiment_data[\"weight_decay\"].items():\n    wd = float(wd_str)\n    print(f\"Weight Decay: {wd}\\n\")\n    metrics = ed[\"metrics\"]\n\n    # Define dataset labels and corresponding metric keys\n    dataset_info = [\n        (\"Original MNIST\", \"orig_acc\", \"orig_bdp\"),\n        (\"Augmented MNIST\", \"aug_acc\", \"aug_bdp\"),\n        (\"Fashion MNIST\", \"hug1_acc\", \"hug1_bdp\"),\n        (\"Kuzushiji MNIST\", \"hug2_acc\", \"hug2_bdp\"),\n    ]\n\n    # Print final accuracy and BDP for each dataset\n    for name, acc_key, bdp_key in dataset_info:\n        final_acc = metrics[acc_key][-1]\n        final_bdp = metrics[bdp_key][-1]\n        print(f\"{name}:\")\n        print(f\"  Final Test Accuracy: {final_acc:.4f}\")\n        print(f\"  Between\u2010model Std (BDP): {final_bdp:.4f}\\n\")\n", "", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_data = np.load(\n    os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n).item()\n\n# Map metric keys to human-readable dataset names\ndatasets = {\n    \"orig_acc\": \"Original MNIST\",\n    \"aug_acc\": \"Augmented MNIST\",\n    \"fashion_acc\": \"FashionMNIST\",\n    \"kmnist_acc\": \"KMNIST\",\n}\n\n# Print best test accuracy per dataset for each model\nfor ds_key, ds_name in datasets.items():\n    print(f\"{ds_name}:\")\n    for model in (\"MLP\", \"CNN\"):\n        metrics = experiment_data[\"dropout\"][model][\"metrics\"]\n        best_acc = max(metrics[ds_key])\n        print(f\"  {model} best {ds_name} test accuracy: {best_acc:.4f}\")\n    print()\n\n# Print best Between-Dataset Performance Disparity (BDP) per model\nfor model in (\"MLP\", \"CNN\"):\n    metrics = experiment_data[\"dropout\"][model][\"metrics\"]\n    best_bdp = max(metrics[\"BDP\"])\n    print(f\"Between-Dataset Performance Disparity (BDP) for {model}:\")\n    print(f\"  Best BDP: {best_bdp:.4f}\\n\")\n\n# Print best Category Generalization Ratio (CGR) across dropout rates\ncgr_values = experiment_data[\"dropout\"][\"CGR\"]\nbest_cgr = max(cgr_values)\nprint(\"Category Generalization Ratio (CGR):\")\nprint(f\"  Best CGR across dropout rates: {best_cgr:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over each beta1 setting under \"adam_beta1\"\nfor beta_key, beta_entry in experiment_data.get(\"adam_beta1\", {}).items():\n    beta_value = beta_key.split(\"_\", 1)[1]\n    print(f\"Hyperparameter setting: beta1 = {beta_value}\\n\")\n\n    # For each model, extract and print the final metrics\n    for model_name in [\"MLP\", \"CNN\"]:\n        model_data = beta_entry.get(model_name, {})\n        train_losses = model_data.get(\"losses\", {}).get(\"train\", [])\n        val_losses = model_data.get(\"losses\", {}).get(\"val\", [])\n        orig_accs = model_data.get(\"metrics\", {}).get(\"orig_acc\", [])\n        aug_accs = model_data.get(\"metrics\", {}).get(\"aug_acc\", [])\n\n        print(f\"Model: {model_name}\")\n\n        # Training dataset metrics\n        if train_losses:\n            print(\"Dataset: Training dataset\")\n            print(f\"  Final training loss: {train_losses[-1]:.4f}\")\n\n        # Original test dataset metrics\n        if val_losses or orig_accs:\n            print(\"Dataset: Original test dataset\")\n            if val_losses:\n                print(f\"  Final validation loss: {val_losses[-1]:.4f}\")\n            if orig_accs:\n                print(f\"  Final original test accuracy: {orig_accs[-1]:.4f}\")\n\n        # Augmented test dataset metrics\n        if aug_accs:\n            print(\"Dataset: Augmented test dataset\")\n            print(f\"  Final augmented test accuracy: {aug_accs[-1]:.4f}\")\n\n        print()  # Blank line for readability\n", "import os\nimport numpy as np\n\n# Load saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each experiment configuration\nfor n_epochs, run_data in experiment_data[\"n_epochs\"].items():\n    print(f\"n_epochs: {n_epochs}\")\n    # Iterate through each model in the run\n    for model_name, model_data in run_data[\"models\"].items():\n        print(f\"Model: {model_name}\")\n        # Extract final epoch metrics\n        training_loss = model_data[\"losses\"][\"train\"][-1]\n        validation_loss = model_data[\"losses\"][\"val\"][-1]\n        original_test_accuracy = model_data[\"metrics\"][\"orig_acc\"][-1]\n        augmented_test_accuracy = model_data[\"metrics\"][\"aug_acc\"][-1]\n        # Print metrics by dataset\n        print(\"Dataset: Training\")\n        print(f\"training loss: {training_loss:.4f}\")\n        print(\"Dataset: Validation\")\n        print(f\"validation loss: {validation_loss:.4f}\")\n        print(\"Dataset: Original Test\")\n        print(f\"original test accuracy: {original_test_accuracy:.4f}\")\n        print(\"Dataset: Augmented Test\")\n        print(f\"augmented test accuracy: {augmented_test_accuracy:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each experiment configuration\nfor n_epochs, run_data in experiment_data[\"n_epochs\"].items():\n    print(f\"n_epochs: {n_epochs}\")\n    # Iterate through each model in the run\n    for model_name, model_data in run_data[\"models\"].items():\n        print(f\"Model: {model_name}\")\n        # Extract final epoch metrics\n        training_loss = model_data[\"losses\"][\"train\"][-1]\n        validation_loss = model_data[\"losses\"][\"val\"][-1]\n        original_test_accuracy = model_data[\"metrics\"][\"orig_acc\"][-1]\n        augmented_test_accuracy = model_data[\"metrics\"][\"aug_acc\"][-1]\n        # Print metrics by dataset\n        print(\"Dataset: Training\")\n        print(f\"training loss: {training_loss:.4f}\")\n        print(\"Dataset: Validation\")\n        print(f\"validation loss: {validation_loss:.4f}\")\n        print(\"Dataset: Original Test\")\n        print(f\"original test accuracy: {original_test_accuracy:.4f}\")\n        print(\"Dataset: Augmented Test\")\n        print(f\"augmented test accuracy: {augmented_test_accuracy:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each experiment configuration\nfor n_epochs, run_data in experiment_data[\"n_epochs\"].items():\n    print(f\"n_epochs: {n_epochs}\")\n    # Iterate through each model in the run\n    for model_name, model_data in run_data[\"models\"].items():\n        print(f\"Model: {model_name}\")\n        # Extract final epoch metrics\n        training_loss = model_data[\"losses\"][\"train\"][-1]\n        validation_loss = model_data[\"losses\"][\"val\"][-1]\n        original_test_accuracy = model_data[\"metrics\"][\"orig_acc\"][-1]\n        augmented_test_accuracy = model_data[\"metrics\"][\"aug_acc\"][-1]\n        # Print metrics by dataset\n        print(\"Dataset: Training\")\n        print(f\"training loss: {training_loss:.4f}\")\n        print(\"Dataset: Validation\")\n        print(f\"validation loss: {validation_loss:.4f}\")\n        print(\"Dataset: Original Test\")\n        print(f\"original test accuracy: {original_test_accuracy:.4f}\")\n        print(\"Dataset: Augmented Test\")\n        print(f\"augmented test accuracy: {augmented_test_accuracy:.4f}\")\n        print()\n", ""], "parse_term_out": ["['Training dataset:', '\\n', 'final training loss:', ' ', '0.0289', '\\n',\n'Original test dataset:', '\\n', 'final validation loss:', ' ', '0.0592', '\\n',\n'final test accuracy:', ' ', '0.9823', '\\n', 'Augmented test dataset:', '\\n',\n'final test accuracy:', ' ', '0.9122', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['n_epochs: 5', '\\n', 'Model: MLP', '\\n', 'Dataset: Training', '\\n', 'training\nloss: 0.0472', '\\n', 'Dataset: Validation', '\\n', 'validation loss: 0.0808',\n'\\n', 'Dataset: Original Test', '\\n', 'original test accuracy: 0.9758', '\\n',\n'Dataset: Augmented Test', '\\n', 'augmented test accuracy: 0.9023', '\\n', '\\n',\n'Model: CNN', '\\n', 'Dataset: Training', '\\n', 'training loss: 0.0287', '\\n',\n'Dataset: Validation', '\\n', 'validation loss: 0.0587', '\\n', 'Dataset: Original\nTest', '\\n', 'original test accuracy: 0.9821', '\\n', 'Dataset: Augmented Test',\n'\\n', 'augmented test accuracy: 0.9167', '\\n', '\\n', 'n_epochs: 10', '\\n',\n'Model: MLP', '\\n', 'Dataset: Training', '\\n', 'training loss: 0.0176', '\\n',\n'Dataset: Validation', '\\n', 'validation loss: 0.0893', '\\n', 'Dataset: Original\nTest', '\\n', 'original test accuracy: 0.9779', '\\n', 'Dataset: Augmented Test',\n'\\n', 'augmented test accuracy: 0.8993', '\\n', '\\n', 'Model: CNN', '\\n',\n'Dataset: Training', '\\n', 'training loss: 0.0081', '\\n', 'Dataset: Validation',\n'\\n', 'validation loss: 0.0604', '\\n', 'Dataset: Original Test', '\\n', 'original\ntest accuracy: 0.9855', '\\n', 'Dataset: Augmented Test', '\\n', 'augmented test\naccuracy: 0.9047', '\\n', '\\n', 'n_epochs: 15', '\\n', 'Model: MLP', '\\n',\n'Dataset: Training', '\\n', 'training loss: 0.0107', '\\n', 'Dataset: Validation',\n'\\n', 'validation loss: 0.1061', '\\n', 'Dataset: Original Test', '\\n', 'original\ntest accuracy: 0.9799', '\\n', 'Dataset: Augmented Test', '\\n', 'augmented test\naccuracy: 0.9013', '\\n', '\\n', 'Model: CNN', '\\n', 'Dataset: Training', '\\n',\n'training loss: 0.0058', '\\n', 'Dataset: Validation', '\\n', 'validation loss:\n0.0608', '\\n', 'Dataset: Original Test', '\\n', 'original test accuracy: 0.9855',\n'\\n', 'Dataset: Augmented Test', '\\n', 'augmented test accuracy: 0.9114', '\\n',\n'\\n', 'n_epochs: 20', '\\n', 'Model: MLP', '\\n', 'Dataset: Training', '\\n',\n'training loss: 0.0081', '\\n', 'Dataset: Validation', '\\n', 'validation loss:\n0.1257', '\\n', 'Dataset: Original Test', '\\n', 'original test accuracy: 0.9760',\n'\\n', 'Dataset: Augmented Test', '\\n', 'augmented test accuracy: 0.8928', '\\n',\n'\\n', 'Model: CNN', '\\n', 'Dataset: Training', '\\n', 'training loss: 0.0030',\n'\\n', 'Dataset: Validation', '\\n', 'validation loss: 0.0772', '\\n', 'Dataset:\nOriginal Test', '\\n', 'original test accuracy: 0.9861', '\\n', 'Dataset:\nAugmented Test', '\\n', 'augmented test accuracy: 0.9200', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Learning rate: 0.0001', '\\n', 'Model: MLP', '\\n', 'Dataset: Training set',\n'\\n', 'Final training loss: 0.1719', '\\n', 'Dataset: Validation set', '\\n',\n'Final validation loss: 0.1662', '\\n', 'Dataset: Original test set', '\\n',\n'Final original test accuracy: 0.9522', '\\n', 'Dataset: Augmented test set',\n'\\n', 'Final augmented test accuracy: 0.8464\\n', '\\n', 'Model: CNN', '\\n',\n'Dataset: Training set', '\\n', 'Final training loss: 0.0917', '\\n', 'Dataset:\nValidation set', '\\n', 'Final validation loss: 0.0822', '\\n', 'Dataset: Original\ntest set', '\\n', 'Final original test accuracy: 0.9742', '\\n', 'Dataset:\nAugmented test set', '\\n', 'Final augmented test accuracy: 0.8995\\n', '\\n',\n'Learning rate: 0.0005', '\\n', 'Model: MLP', '\\n', 'Dataset: Training set',\n'\\n', 'Final training loss: 0.0623', '\\n', 'Dataset: Validation set', '\\n',\n'Final validation loss: 0.0830', '\\n', 'Dataset: Original test set', '\\n',\n'Final original test accuracy: 0.9753', '\\n', 'Dataset: Augmented test set',\n'\\n', 'Final augmented test accuracy: 0.8881\\n', '\\n', 'Model: CNN', '\\n',\n'Dataset: Training set', '\\n', 'Final training loss: 0.0387', '\\n', 'Dataset:\nValidation set', '\\n', 'Final validation loss: 0.0580', '\\n', 'Dataset: Original\ntest set', '\\n', 'Final original test accuracy: 0.9807', '\\n', 'Dataset:\nAugmented test set', '\\n', 'Final augmented test accuracy: 0.8948\\n', '\\n',\n'Learning rate: 0.001', '\\n', 'Model: MLP', '\\n', 'Dataset: Training set', '\\n',\n'Final training loss: 0.0463', '\\n', 'Dataset: Validation set', '\\n', 'Final\nvalidation loss: 0.0804', '\\n', 'Dataset: Original test set', '\\n', 'Final\noriginal test accuracy: 0.9760', '\\n', 'Dataset: Augmented test set', '\\n',\n'Final augmented test accuracy: 0.8936\\n', '\\n', 'Model: CNN', '\\n', 'Dataset:\nTraining set', '\\n', 'Final training loss: 0.0311', '\\n', 'Dataset: Validation\nset', '\\n', 'Final validation loss: 0.0421', '\\n', 'Dataset: Original test set',\n'\\n', 'Final original test accuracy: 0.9852', '\\n', 'Dataset: Augmented test\nset', '\\n', 'Final augmented test accuracy: 0.9115\\n', '\\n', 'Learning rate:\n0.005', '\\n', 'Model: MLP', '\\n', 'Dataset: Training set', '\\n', 'Final training\nloss: 0.0982', '\\n', 'Dataset: Validation set', '\\n', 'Final validation loss:\n0.1495', '\\n', 'Dataset: Original test set', '\\n', 'Final original test\naccuracy: 0.9661', '\\n', 'Dataset: Augmented test set', '\\n', 'Final augmented\ntest accuracy: 0.8796\\n', '\\n', 'Model: CNN', '\\n', 'Dataset: Training set',\n'\\n', 'Final training loss: 0.0281', '\\n', 'Dataset: Validation set', '\\n',\n'Final validation loss: 0.0756', '\\n', 'Dataset: Original test set', '\\n',\n'Final original test accuracy: 0.9807', '\\n', 'Dataset: Augmented test set',\n'\\n', 'Final augmented test accuracy: 0.9026\\n', '\\n', 'Learning rate: 0.01',\n'\\n', 'Model: MLP', '\\n', 'Dataset: Training set', '\\n', 'Final training loss:\n0.1628', '\\n', 'Dataset: Validation set', '\\n', 'Final validation loss: 0.2000',\n'\\n', 'Dataset: Original test set', '\\n', 'Final original test accuracy:\n0.9529', '\\n', 'Dataset: Augmented test set', '\\n', 'Final augmented test\naccuracy: 0.8530\\n', '\\n', 'Model: CNN', '\\n', 'Dataset: Training set', '\\n',\n'Final training loss: 0.0450', '\\n', 'Dataset: Validation set', '\\n', 'Final\nvalidation loss: 0.0949', '\\n', 'Dataset: Original test set', '\\n', 'Final\noriginal test accuracy: 0.9747', '\\n', 'Dataset: Augmented test set', '\\n',\n'Final augmented test accuracy: 0.8903\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Batch size: 32, Model: MLP', '\\n', 'Training Dataset - Final training loss:\n0.0447', '\\n', 'Validation Dataset - Final validation loss: 0.0875', '\\n',\n'Original Test Dataset - Final original test accuracy: 0.9735', '\\n', 'Augmented\nTest Dataset - Final augmented test accuracy: 0.8989', '\\n', '\\n', 'Batch size:\n32, Model: CNN', '\\n', 'Training Dataset - Final training loss: 0.0222', '\\n',\n'Validation Dataset - Final validation loss: 0.0461', '\\n', 'Original Test\nDataset - Final original test accuracy: 0.9843', '\\n', 'Augmented Test Dataset -\nFinal augmented test accuracy: 0.9193', '\\n', '\\n', 'Batch size: 64, Model:\nMLP', '\\n', 'Training Dataset - Final training loss: 0.0468', '\\n', 'Validation\nDataset - Final validation loss: 0.0783', '\\n', 'Original Test Dataset - Final\noriginal test accuracy: 0.9760', '\\n', 'Augmented Test Dataset - Final augmented\ntest accuracy: 0.8983', '\\n', '\\n', 'Batch size: 64, Model: CNN', '\\n',\n'Training Dataset - Final training loss: 0.0295', '\\n', 'Validation Dataset -\nFinal validation loss: 0.0474', '\\n', 'Original Test Dataset - Final original\ntest accuracy: 0.9842', '\\n', 'Augmented Test Dataset - Final augmented test\naccuracy: 0.9147', '\\n', '\\n', 'Batch size: 128, Model: MLP', '\\n', 'Training\nDataset - Final training loss: 0.0530', '\\n', 'Validation Dataset - Final\nvalidation loss: 0.0831', '\\n', 'Original Test Dataset - Final original test\naccuracy: 0.9756', '\\n', 'Augmented Test Dataset - Final augmented test\naccuracy: 0.9041', '\\n', '\\n', 'Batch size: 128, Model: CNN', '\\n', 'Training\nDataset - Final training loss: 0.0380', '\\n', 'Validation Dataset - Final\nvalidation loss: 0.0537', '\\n', 'Original Test Dataset - Final original test\naccuracy: 0.9817', '\\n', 'Augmented Test Dataset - Final augmented test\naccuracy: 0.9003', '\\n', '\\n', 'Batch size: 256, Model: MLP', '\\n', 'Training\nDataset - Final training loss: 0.0731', '\\n', 'Validation Dataset - Final\nvalidation loss: 0.0939', '\\n', 'Original Test Dataset - Final original test\naccuracy: 0.9705', '\\n', 'Augmented Test Dataset - Final augmented test\naccuracy: 0.8866', '\\n', '\\n', 'Batch size: 256, Model: CNN', '\\n', 'Training\nDataset - Final training loss: 0.0467', '\\n', 'Validation Dataset - Final\nvalidation loss: 0.0552', '\\n', 'Original Test Dataset - Final original test\naccuracy: 0.9818', '\\n', 'Augmented Test Dataset - Final augmented test\naccuracy: 0.9148', '\\n', '\\n', 'Execution time: a moment seconds (time limit is\nan hour).']", "['Weight decay: 0', '\\n', 'Dataset: Training', '\\n', '  Train loss: 0.0378',\n'\\n', 'Dataset: Validation', '\\n', '  Validation loss: 0.0705', '\\n', 'Dataset:\nOriginal Test', '\\n', '  Original test accuracy: 0.9790', '\\n', 'Dataset:\nAugmented Test', '\\n', '  Augmented test accuracy: 0.9089\\n', '\\n', 'Weight\ndecay: 1e-05', '\\n', 'Dataset: Training', '\\n', '  Train loss: 0.0385', '\\n',\n'Dataset: Validation', '\\n', '  Validation loss: 0.0749', '\\n', 'Dataset:\nOriginal Test', '\\n', '  Original test accuracy: 0.9773', '\\n', 'Dataset:\nAugmented Test', '\\n', '  Augmented test accuracy: 0.8966\\n', '\\n', 'Weight\ndecay: 0.0001', '\\n', 'Dataset: Training', '\\n', '  Train loss: 0.0422', '\\n',\n'Dataset: Validation', '\\n', '  Validation loss: 0.0635', '\\n', 'Dataset:\nOriginal Test', '\\n', '  Original test accuracy: 0.9798', '\\n', 'Dataset:\nAugmented Test', '\\n', '  Augmented test accuracy: 0.9009\\n', '\\n', 'Weight\ndecay: 0.001', '\\n', 'Dataset: Training', '\\n', '  Train loss: 0.0646', '\\n',\n'Dataset: Validation', '\\n', '  Validation loss: 0.0703', '\\n', 'Dataset:\nOriginal Test', '\\n', '  Original test accuracy: 0.9779', '\\n', 'Dataset:\nAugmented Test', '\\n', '  Augmented test accuracy: 0.9081\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: Training Dataset (MLP)', '\\n', '  Training Loss: 0.0719', '\\n',\n'Dataset: Original Test Dataset (MLP)', '\\n', '  Validation Loss: 0.0951', '\\n',\n'  Original Test Accuracy: 0.9716', '\\n', 'Dataset: Augmented Test Dataset\n(MLP)', '\\n', '  Augmented Test Accuracy: 0.8859\\n', '\\n', 'Dataset: Training\nDataset (CNN)', '\\n', '  Training Loss: 0.0551', '\\n', 'Dataset: Original Test\nDataset (CNN)', '\\n', '  Validation Loss: 0.0625', '\\n', '  Original Test\nAccuracy: 0.9820', '\\n', 'Dataset: Augmented Test Dataset (CNN)', '\\n', '\nAugmented Test Accuracy: 0.9020\\n', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Results for label smoothing \u03b5 = 0.0', '\\n', 'Dataset: Training Dataset', '\\n',\n'Final training loss: 0.0318', '\\n', 'Dataset: Validation Dataset', '\\n', 'Final\nvalidation loss: 0.0483', '\\n', 'Dataset: Original Test Dataset', '\\n', 'Final\noriginal test accuracy: 0.9843', '\\n', 'Dataset: Augmented Test Dataset', '\\n',\n'Final augmented test accuracy: 0.9143', '\\n', '\\n', 'Results for label\nsmoothing \u03b5 = 0.05', '\\n', 'Dataset: Training Dataset', '\\n', 'Final training\nloss: 0.3524', '\\n', 'Dataset: Validation Dataset', '\\n', 'Final validation\nloss: 0.3643', '\\n', 'Dataset: Original Test Dataset', '\\n', 'Final original\ntest accuracy: 0.9880', '\\n', 'Dataset: Augmented Test Dataset', '\\n', 'Final\naugmented test accuracy: 0.9274', '\\n', '\\n', 'Results for label smoothing \u03b5 =\n0.1', '\\n', 'Dataset: Training Dataset', '\\n', 'Final training loss: 0.5997',\n'\\n', 'Dataset: Validation Dataset', '\\n', 'Final validation loss: 0.6083',\n'\\n', 'Dataset: Original Test Dataset', '\\n', 'Final original test accuracy:\n0.9871', '\\n', 'Dataset: Augmented Test Dataset', '\\n', 'Final augmented test\naccuracy: 0.9249', '\\n', '\\n', 'Results for label smoothing \u03b5 = 0.2', '\\n',\n'Dataset: Training Dataset', '\\n', 'Final training loss: 0.9853', '\\n',\n'Dataset: Validation Dataset', '\\n', 'Final validation loss: 0.9969', '\\n',\n'Dataset: Original Test Dataset', '\\n', 'Final original test accuracy: 0.9867',\n'\\n', 'Dataset: Augmented Test Dataset', '\\n', 'Final augmented test accuracy:\n0.8989', '\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "", "['Weight decay: 0', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 27, in <module>\\n    bdp = results[\"BDP\"]\\n\n~~~~~~~^^^^^^^\\nKeyError: \\'BDP\\'\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 10, in\n<module>\\n    for wd_str, ed in experiment_data[\"weight_decay\"].items():\\n\n~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\\nKeyError: \\'weight_decay\\'\\n', 'Execution time:\na moment seconds (time limit is an hour).']", "", "['Original MNIST:', '\\n', '  MLP best Original MNIST test accuracy: 0.9767',\n'\\n', '  CNN best Original MNIST test accuracy: 0.9856', '\\n', '\\n', 'Augmented\nMNIST:', '\\n', '  MLP best Augmented MNIST test accuracy: 0.9003', '\\n', '  CNN\nbest Augmented MNIST test accuracy: 0.9198', '\\n', '\\n', 'FashionMNIST:', '\\n',\n'Traceback (most recent call last):\\n  File \"runfile.py\", line 23, in <module>\\n\nbest_acc = max(metrics[ds_key])\\n                   ~~~~~~~^^^^^^^^\\nKeyError:\n\\'fashion_acc\\'\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Hyperparameter setting: beta1 = 0.8\\n', '\\n', 'Model: MLP', '\\n', 'Dataset:\nTraining dataset', '\\n', '  Final training loss: 0.0460', '\\n', 'Dataset:\nOriginal test dataset', '\\n', '  Final validation loss: 0.0892', '\\n', '  Final\noriginal test accuracy: 0.9744', '\\n', 'Dataset: Augmented test dataset', '\\n',\n'  Final augmented test accuracy: 0.8943', '\\n', '\\n', 'Model: CNN', '\\n',\n'Dataset: Training dataset', '\\n', '  Final training loss: 0.0295', '\\n',\n'Dataset: Original test dataset', '\\n', '  Final validation loss: 0.0474', '\\n',\n'  Final original test accuracy: 0.9845', '\\n', 'Dataset: Augmented test\ndataset', '\\n', '  Final augmented test accuracy: 0.9205', '\\n', '\\n',\n'Hyperparameter setting: beta1 = 0.85\\n', '\\n', 'Model: MLP', '\\n', 'Dataset:\nTraining dataset', '\\n', '  Final training loss: 0.0462', '\\n', 'Dataset:\nOriginal test dataset', '\\n', '  Final validation loss: 0.0889', '\\n', '  Final\noriginal test accuracy: 0.9735', '\\n', 'Dataset: Augmented test dataset', '\\n',\n'  Final augmented test accuracy: 0.8921', '\\n', '\\n', 'Model: CNN', '\\n',\n'Dataset: Training dataset', '\\n', '  Final training loss: 0.0296', '\\n',\n'Dataset: Original test dataset', '\\n', '  Final validation loss: 0.0544', '\\n',\n'  Final original test accuracy: 0.9831', '\\n', 'Dataset: Augmented test\ndataset', '\\n', '  Final augmented test accuracy: 0.9016', '\\n', '\\n',\n'Hyperparameter setting: beta1 = 0.95\\n', '\\n', 'Model: MLP', '\\n', 'Dataset:\nTraining dataset', '\\n', '  Final training loss: 0.0482', '\\n', 'Dataset:\nOriginal test dataset', '\\n', '  Final validation loss: 0.0796', '\\n', '  Final\noriginal test accuracy: 0.9749', '\\n', 'Dataset: Augmented test dataset', '\\n',\n'  Final augmented test accuracy: 0.8962', '\\n', '\\n', 'Model: CNN', '\\n',\n'Dataset: Training dataset', '\\n', '  Final training loss: 0.0299', '\\n',\n'Dataset: Original test dataset', '\\n', '  Final validation loss: 0.0433', '\\n',\n'  Final original test accuracy: 0.9855', '\\n', 'Dataset: Augmented test\ndataset', '\\n', '  Final augmented test accuracy: 0.9013', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['n_epochs: 5', '\\n', 'Model: MLP', '\\n', 'Dataset: Training', '\\n', 'training\nloss: 0.0465', '\\n', 'Dataset: Validation', '\\n', 'validation loss: 0.0834',\n'\\n', 'Dataset: Original Test', '\\n', 'original test accuracy: 0.9748', '\\n',\n'Dataset: Augmented Test', '\\n', 'augmented test accuracy: 0.8958', '\\n', '\\n',\n'Model: CNN', '\\n', 'Dataset: Training', '\\n', 'training loss: 0.0296', '\\n',\n'Dataset: Validation', '\\n', 'validation loss: 0.0479', '\\n', 'Dataset: Original\nTest', '\\n', 'original test accuracy: 0.9857', '\\n', 'Dataset: Augmented Test',\n'\\n', 'augmented test accuracy: 0.9213', '\\n', '\\n', 'n_epochs: 10', '\\n',\n'Model: MLP', '\\n', 'Dataset: Training', '\\n', 'training loss: 0.0196', '\\n',\n'Dataset: Validation', '\\n', 'validation loss: 0.0884', '\\n', 'Dataset: Original\nTest', '\\n', 'original test accuracy: 0.9773', '\\n', 'Dataset: Augmented Test',\n'\\n', 'augmented test accuracy: 0.8985', '\\n', '\\n', 'Model: CNN', '\\n',\n'Dataset: Training', '\\n', 'training loss: 0.0108', '\\n', 'Dataset: Validation',\n'\\n', 'validation loss: 0.0550', '\\n', 'Dataset: Original Test', '\\n', 'original\ntest accuracy: 0.9864', '\\n', 'Dataset: Augmented Test', '\\n', 'augmented test\naccuracy: 0.9263', '\\n', '\\n', 'n_epochs: 15', '\\n', 'Model: MLP', '\\n',\n'Dataset: Training', '\\n', 'training loss: 0.0140', '\\n', 'Dataset: Validation',\n'\\n', 'validation loss: 0.0959', '\\n', 'Dataset: Original Test', '\\n', 'original\ntest accuracy: 0.9784', '\\n', 'Dataset: Augmented Test', '\\n', 'augmented test\naccuracy: 0.9047', '\\n', '\\n', 'Model: CNN', '\\n', 'Dataset: Training', '\\n',\n'training loss: 0.0059', '\\n', 'Dataset: Validation', '\\n', 'validation loss:\n0.0773', '\\n', 'Dataset: Original Test', '\\n', 'original test accuracy: 0.9831',\n'\\n', 'Dataset: Augmented Test', '\\n', 'augmented test accuracy: 0.9085', '\\n',\n'\\n', 'n_epochs: 20', '\\n', 'Model: MLP', '\\n', 'Dataset: Training', '\\n',\n'training loss: 0.0088', '\\n', 'Dataset: Validation', '\\n', 'validation loss:\n0.1144', '\\n', 'Dataset: Original Test', '\\n', 'original test accuracy: 0.9791',\n'\\n', 'Dataset: Augmented Test', '\\n', 'augmented test accuracy: 0.9039', '\\n',\n'\\n', 'Model: CNN', '\\n', 'Dataset: Training', '\\n', 'training loss: 0.0041',\n'\\n', 'Dataset: Validation', '\\n', 'validation loss: 0.0759', '\\n', 'Dataset:\nOriginal Test', '\\n', 'original test accuracy: 0.9859', '\\n', 'Dataset:\nAugmented Test', '\\n', 'augmented test accuracy: 0.9156', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['n_epochs: 5', '\\n', 'Model: MLP', '\\n', 'Dataset: Training', '\\n', 'training\nloss: 0.0465', '\\n', 'Dataset: Validation', '\\n', 'validation loss: 0.0834',\n'\\n', 'Dataset: Original Test', '\\n', 'original test accuracy: 0.9748', '\\n',\n'Dataset: Augmented Test', '\\n', 'augmented test accuracy: 0.8958', '\\n', '\\n',\n'Model: CNN', '\\n', 'Dataset: Training', '\\n', 'training loss: 0.0295', '\\n',\n'Dataset: Validation', '\\n', 'validation loss: 0.0531', '\\n', 'Dataset: Original\nTest', '\\n', 'original test accuracy: 0.9841', '\\n', 'Dataset: Augmented Test',\n'\\n', 'augmented test accuracy: 0.9207', '\\n', '\\n', 'n_epochs: 10', '\\n',\n'Model: MLP', '\\n', 'Dataset: Training', '\\n', 'training loss: 0.0196', '\\n',\n'Dataset: Validation', '\\n', 'validation loss: 0.0884', '\\n', 'Dataset: Original\nTest', '\\n', 'original test accuracy: 0.9773', '\\n', 'Dataset: Augmented Test',\n'\\n', 'augmented test accuracy: 0.8985', '\\n', '\\n', 'Model: CNN', '\\n',\n'Dataset: Training', '\\n', 'training loss: 0.0104', '\\n', 'Dataset: Validation',\n'\\n', 'validation loss: 0.0549', '\\n', 'Dataset: Original Test', '\\n', 'original\ntest accuracy: 0.9846', '\\n', 'Dataset: Augmented Test', '\\n', 'augmented test\naccuracy: 0.9086', '\\n', '\\n', 'n_epochs: 15', '\\n', 'Model: MLP', '\\n',\n'Dataset: Training', '\\n', 'training loss: 0.0140', '\\n', 'Dataset: Validation',\n'\\n', 'validation loss: 0.0959', '\\n', 'Dataset: Original Test', '\\n', 'original\ntest accuracy: 0.9784', '\\n', 'Dataset: Augmented Test', '\\n', 'augmented test\naccuracy: 0.9047', '\\n', '\\n', 'Model: CNN', '\\n', 'Dataset: Training', '\\n',\n'training loss: 0.0071', '\\n', 'Dataset: Validation', '\\n', 'validation loss:\n0.0644', '\\n', 'Dataset: Original Test', '\\n', 'original test accuracy: 0.9863',\n'\\n', 'Dataset: Augmented Test', '\\n', 'augmented test accuracy: 0.9139', '\\n',\n'\\n', 'n_epochs: 20', '\\n', 'Model: MLP', '\\n', 'Dataset: Training', '\\n',\n'training loss: 0.0088', '\\n', 'Dataset: Validation', '\\n', 'validation loss:\n0.1144', '\\n', 'Dataset: Original Test', '\\n', 'original test accuracy: 0.9791',\n'\\n', 'Dataset: Augmented Test', '\\n', 'augmented test accuracy: 0.9039', '\\n',\n'\\n', 'Model: CNN', '\\n', 'Dataset: Training', '\\n', 'training loss: 0.0043',\n'\\n', 'Dataset: Validation', '\\n', 'validation loss: 0.0812', '\\n', 'Dataset:\nOriginal Test', '\\n', 'original test accuracy: 0.9845', '\\n', 'Dataset:\nAugmented Test', '\\n', 'augmented test accuracy: 0.9193', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['n_epochs: 5', '\\n', 'Model: MLP', '\\n', 'Dataset: Training', '\\n', 'training\nloss: 0.0465', '\\n', 'Dataset: Validation', '\\n', 'validation loss: 0.0834',\n'\\n', 'Dataset: Original Test', '\\n', 'original test accuracy: 0.9748', '\\n',\n'Dataset: Augmented Test', '\\n', 'augmented test accuracy: 0.8958', '\\n', '\\n',\n'Model: CNN', '\\n', 'Dataset: Training', '\\n', 'training loss: 0.0293', '\\n',\n'Dataset: Validation', '\\n', 'validation loss: 0.0533', '\\n', 'Dataset: Original\nTest', '\\n', 'original test accuracy: 0.9836', '\\n', 'Dataset: Augmented Test',\n'\\n', 'augmented test accuracy: 0.9188', '\\n', '\\n', 'n_epochs: 10', '\\n',\n'Model: MLP', '\\n', 'Dataset: Training', '\\n', 'training loss: 0.0196', '\\n',\n'Dataset: Validation', '\\n', 'validation loss: 0.0884', '\\n', 'Dataset: Original\nTest', '\\n', 'original test accuracy: 0.9773', '\\n', 'Dataset: Augmented Test',\n'\\n', 'augmented test accuracy: 0.8985', '\\n', '\\n', 'Model: CNN', '\\n',\n'Dataset: Training', '\\n', 'training loss: 0.0096', '\\n', 'Dataset: Validation',\n'\\n', 'validation loss: 0.0573', '\\n', 'Dataset: Original Test', '\\n', 'original\ntest accuracy: 0.9856', '\\n', 'Dataset: Augmented Test', '\\n', 'augmented test\naccuracy: 0.9228', '\\n', '\\n', 'n_epochs: 15', '\\n', 'Model: MLP', '\\n',\n'Dataset: Training', '\\n', 'training loss: 0.0140', '\\n', 'Dataset: Validation',\n'\\n', 'validation loss: 0.0959', '\\n', 'Dataset: Original Test', '\\n', 'original\ntest accuracy: 0.9784', '\\n', 'Dataset: Augmented Test', '\\n', 'augmented test\naccuracy: 0.9047', '\\n', '\\n', 'Model: CNN', '\\n', 'Dataset: Training', '\\n',\n'training loss: 0.0064', '\\n', 'Dataset: Validation', '\\n', 'validation loss:\n0.0663', '\\n', 'Dataset: Original Test', '\\n', 'original test accuracy: 0.9854',\n'\\n', 'Dataset: Augmented Test', '\\n', 'augmented test accuracy: 0.9047', '\\n',\n'\\n', 'n_epochs: 20', '\\n', 'Model: MLP', '\\n', 'Dataset: Training', '\\n',\n'training loss: 0.0088', '\\n', 'Dataset: Validation', '\\n', 'validation loss:\n0.1144', '\\n', 'Dataset: Original Test', '\\n', 'original test accuracy: 0.9791',\n'\\n', 'Dataset: Augmented Test', '\\n', 'augmented test accuracy: 0.9039', '\\n',\n'\\n', 'Model: CNN', '\\n', 'Dataset: Training', '\\n', 'training loss: 0.0051',\n'\\n', 'Dataset: Validation', '\\n', 'validation loss: 0.0864', '\\n', 'Dataset:\nOriginal Test', '\\n', 'original test accuracy: 0.9838', '\\n', 'Dataset:\nAugmented Test', '\\n', 'augmented test accuracy: 0.9080', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, "KeyError", "KeyError", null, "KeyError", null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, {"args": ["BDP"]}, {"args": ["weight_decay"]}, null, {"args": ["fashion_acc"]}, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 27, "<module>", "bdp = results[\"BDP\"]"]], [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 10, "<module>", "for wd_str, ed in experiment_data[\"weight_decay\"].items():"]], null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 23, "<module>", "best_acc = max(metrics[ds_key])"]], null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]}
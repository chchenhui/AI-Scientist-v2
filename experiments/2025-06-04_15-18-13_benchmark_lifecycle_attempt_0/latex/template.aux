\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{recht2019doic}
\citation{wang2019superglueas}
\citation{gupta2024twinadaptcl}
\citation{recht2019doic}
\citation{wang2018glueam}
\citation{wang2019superglueas}
\citation{goodfellow2014explainingah,heusel2017ganstb}
\citation{lakshminarayanan2016simpleas}
\citation{gretton2012akt}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{1}{section.2}\protected@file@percent }
\citation{karras2019analyzingai}
\citation{radford2019languagema}
\citation{lakshminarayanan2016simpleas}
\citation{heusel2017ganstb}
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Setup}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Metrics.}{2}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{2}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}MNIST Discrimination vs.\ Training Length}{2}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Text Benchmark Discrimination}{2}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Preliminary Synthetic Rejuvenation}{2}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{2}{section.6}\protected@file@percent }
\bibdata{references}
\bibcite{goodfellow2014explainingah}{{1}{2014}{{Goodfellow et~al.}}{{Goodfellow, Shlens, and Szegedy}}}
\bibcite{gretton2012akt}{{2}{2012}{{Gretton et~al.}}{{Gretton, Borgwardt, Rasch, Schölkopf, and Smola}}}
\bibcite{gupta2024twinadaptcl}{{3}{2024}{{Gupta et~al.}}{{Gupta, Tian, Wang, and Nahrstedt}}}
\bibcite{heusel2017ganstb}{{4}{2017}{{Heusel et~al.}}{{Heusel, Ramsauer, Unterthiner, Nessler, and Hochreiter}}}
\bibcite{karras2019analyzingai}{{5}{2019}{{Karras et~al.}}{{Karras, Laine, Aittala, Hellsten, Lehtinen, and Aila}}}
\bibcite{lakshminarayanan2016simpleas}{{6}{2016}{{Lakshminarayanan et~al.}}{{Lakshminarayanan, Pritzel, and Blundell}}}
\bibcite{radford2019languagema}{{7}{2019}{{Radford et~al.}}{{Radford, Wu, Child, Luan, Amodei, and Sutskever}}}
\bibcite{recht2019doic}{{8}{2019}{{Recht et~al.}}{{Recht, Roelofs, Schmidt, and Shankar}}}
\bibcite{wang2018glueam}{{9}{2018}{{Wang et~al.}}{{Wang, Singh, Michael, Hill, Levy, and Bowman}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces MNIST rotation study. (a) Training (solid) and validation (dashed) loss for the MLP (left) and CNN (right). Overfitting occurs at epoch\nobreakspace  {}5 for the MLP and epoch\nobreakspace  {}3 for the CNN. (b) Challenge Gap Ratio exhibits budget-specific peaks around epochs 5–8, then flattens or declines as budgets increase.}}{3}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Loss curves for MLP (left) and CNN (right)}}}{3}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {CGR vs.\ epoch for different budgets}}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:mnist_loss_cgr}{{1}{3}{MNIST rotation study. (a) Training (solid) and validation (dashed) loss for the MLP (left) and CNN (right). Overfitting occurs at epoch~5 for the MLP and epoch~3 for the CNN. (b) Challenge Gap Ratio exhibits budget-specific peaks around epochs 5–8, then flattens or declines as budgets increase}{figure.1}{}}
\newlabel{fig:mnist_loss_cgr@cref}{{[figure][1][]1}{[1][2][]3}}
\bibcite{wang2019superglueas}{{10}{2019}{{Wang et~al.}}{{Wang, Pruksachatkun, Nangia, Singh, Michael, Hill, Levy, and Bowman}}}
\bibstyle{iclr2025}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Text classification study. (a) Accuracy of BERT, RoBERTa, and DistilBERT on AG News, SST2, and Yelp. (b) Discrimination Score (std.\ dev.\ of accuracies) at epoch\nobreakspace  {}5: SST2 remains most discriminative.}}{4}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Final validation accuracies}}}{4}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Discrimination Score at final epoch}}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:text_accuracy}{{2}{4}{Text classification study. (a) Accuracy of BERT, RoBERTa, and DistilBERT on AG News, SST2, and Yelp. (b) Discrimination Score (std.\ dev.\ of accuracies) at epoch~5: SST2 remains most discriminative}{figure.2}{}}
\newlabel{fig:text_accuracy@cref}{{[figure][2][]2}{[1][2][]4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Hyperparameter ablation: left shows scheduler variants (constant, step, cosine), right shows weight decay settings (0.0–0.1).}}{4}{figure.3}\protected@file@percent }
\newlabel{fig:hyperparameter_ablation}{{3}{4}{Hyperparameter ablation: left shows scheduler variants (constant, step, cosine), right shows weight decay settings (0.0–0.1)}{figure.3}{}}
\newlabel{fig:hyperparameter_ablation@cref}{{[figure][3][2147483647]3}{[1][4][]4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Adversarial ($\epsilon $-perturbations) and mixup ($\alpha $) training ablations on MNIST rotation.}}{5}{figure.4}\protected@file@percent }
\newlabel{fig:adv_mixup_ablation}{{4}{5}{Adversarial ($\epsilon $-perturbations) and mixup ($\alpha $) training ablations on MNIST rotation}{figure.4}{}}
\newlabel{fig:adv_mixup_ablation@cref}{{[figure][4][2147483647]4}{[1][4][]5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces General augmentation schemes (left) and pooling mechanism ($\alpha $-blending) ablations (right).}}{5}{figure.5}\protected@file@percent }
\newlabel{fig:aug_pool_ablation}{{5}{5}{General augmentation schemes (left) and pooling mechanism ($\alpha $-blending) ablations (right)}{figure.5}{}}
\newlabel{fig:aug_pool_ablation@cref}{{[figure][5][2147483647]5}{[1][4][]5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Activation function ablation on MNIST: training (solid) vs.\ validation (dashed) loss and original vs.\ augmented accuracy.}}{5}{figure.6}\protected@file@percent }
\newlabel{fig:activation_ablation}{{6}{5}{Activation function ablation on MNIST: training (solid) vs.\ validation (dashed) loss and original vs.\ augmented accuracy}{figure.6}{}}
\newlabel{fig:activation_ablation@cref}{{[figure][6][2147483647]6}{[1][4][]5}}
\gdef \@abspage@last{5}

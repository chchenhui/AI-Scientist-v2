\documentclass{article} % For LaTeX2e
\usepackage{iclr2025,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{amsmath,amssymb,mathtools,amsthm}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage[capitalize,noabbrev]{cleveref}

\graphicspath{{../figures/}}

\begin{filecontents}{references.bib}
@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT Press}
}
@inproceedings{ribeiro2016whysi,
 author = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
 title = {“Why Should I Trust You?”: Explaining the Predictions of Any Classifier},
 booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 year = {2016}
}
@inproceedings{lundberg2017aua,
 author = {Scott M. Lundberg and Su-In Lee},
 title = {A Unified Approach to Interpreting Model Predictions},
 booktitle = {Neural Information Processing Systems},
 pages = {4765-4774},
 year = {2017}
}
@article{amershi2014powertt,
 author = {Saleema Amershi and M. Cakmak and W. B. Knox and Todd Kulesza},
 title = {Power to the People: The Role of Humans in Interactive Machine Learning},
 journal = {AI Mag.},
 volume = {35},
 pages = {105-120},
 year = {2014}
}
@inproceedings{kulesza2015principlesoe,
 author = {Todd Kulesza and M. Burnett and Weng-Keen Wong and S. Stumpf},
 title = {Principles of Explanatory Debugging to Personalize Interactive Machine Learning},
 booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces},
 year = {2015}
}
@inproceedings{poursabzi-sangdeh2018manipulatingam,
 author = {Forough Poursabzi-Sangdeh and D. Goldstein and Jake M. Hofman and Jennifer Wortman Vaughan and Hanna M. Wallach},
 title = {Manipulating and Measuring Model Interpretability},
 booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
 year = {2018}
}
@article{kingma2014adamam,
 author = {Diederik P. Kingma and Jimmy Ba},
 title = {Adam: A Method for Stochastic Optimization},
 journal = {CoRR},
 volume = {abs/1412.6980},
 year = {2014}
}
@article{tversky1974judgmentuu,
 author = {A. Tversky and Daniel Kahneman},
 title = {Judgment under Uncertainty: Heuristics and Biases},
 journal = {Science},
 volume = {185},
 pages = {1124-1131},
 year = {1974}
}
@article{doshi-velez2017towardsar,
 author = {F. Doshi-Velez and Been Kim},
 title = {Towards A Rigorous Science of Interpretable Machine Learning},
 journal = {arXiv:1702.08608},
 year = {2017}
}
@article{miller2017explanationia,
 author = {Tim Miller},
 title = {Explanation in Artificial Intelligence: Insights from the Social Sciences},
 journal = {ArXiv},
 volume = {abs/1706.07269},
 year = {2017}
}
@article{sweller1988cognitiveld,
 author = {J. Sweller},
 title = {Cognitive Load During Problem Solving: Effects on Learning},
 journal = {Cogn. Sci.},
 volume = {12},
 pages = {257-285},
 year = {1988}
}
\end{filecontents}

\title{Co-Adaptive Explanation Interfaces:\\Aligning AI and Human Reasoning through Dual-Channel Feedback}

\author{Anonymous}

\begin{document}
\maketitle

\begin{abstract}
We introduce Co-Adaptive Explanation Interfaces, an interactive XAI framework that models individual users' cognitive biases and delivers dual-channel explanations: (1) content justifications for model decisions and (2) bias-awareness signals when user inferences deviate from model reasoning. User corrections update both the AI's decision model and its bias estimator, closing the loop of mutual adaptation. In a synthetic 2D classification simulation with static, single-channel dynamic, and dual-channel interfaces, all variants saturate at ≈99\% alignment, masking any benefit of co-adaptation. We analyze ablations—feature removal, label noise, confidence thresholds—to reveal that trivial tasks and oversaturated metrics hinder differentiation of explanation methods. We discuss pitfalls in evaluation design and suggest directions for realistic, human-grounded co-adaptive studies.
\end{abstract}

\section{Introduction}
Static post hoc explainers such as LIME \citep{ribeiro2016whysi} and SHAP \citep{lundberg2017aua} justify complex model decisions but assume a one-way flow of information, ignoring how users' mental models and biases evolve. Interactive machine teaching \citep{amershi2014powertt,kulesza2015principlesoe} lets users correct models but overlooks the user side of feedback. We propose Co-Adaptive Explanation Interfaces that simultaneously learn a user's bias profile \citep{tversky1974judgmentuu} and adapt explanations through two channels: content justification and bias-awareness warnings. Users' corrections update both the classifier and the bias estimator, enabling bidirectional alignment.

Our contributions are: (1) a dual-channel interface combining feature attributions with bias signals; (2) a simulation comparing static, single-channel, and co-adaptive interfaces on a toy 2D task; and (3) negative/inconclusive results demonstrating that trivial tasks saturate all metrics, masking benefits of co-adaptation.

\section{Related Work}
Local explainers like LIME \citep{ribeiro2016whysi} and SHAP \citep{lundberg2017aua} provide static feature attributions. Personalized explanations \citep{poursabzi-sangdeh2018manipulatingam} adjust to user expertise but lack real-time bias modeling. Human-in-the-loop frameworks \citep{amershi2014powertt} and explanatory debugging \citep{kulesza2015principlesoe} enable interactive correction but ignore modeling user inferential errors. Calls for rigorous, human-grounded evaluation \citep{doshi-velez2017towardsar,miller2017explanationia} stress dynamic studies capturing mutual adaptation—a gap our work addresses.

\section{Method}
We evaluate three interfaces for a binary classifier on a synthetic 2D dataset:
\begin{itemize}
  \item \textbf{Static}: LIME-style content attributions only.
  \item \textbf{Single-channel dynamic}: explanations adapt to corrections but omit bias modeling.
  \item \textbf{Dual-channel co-adaptive}: adds bias-awareness warnings from an auxiliary bias estimator; feedback updates both networks.
\end{itemize}
The bias estimator learns online to predict systematic deviations between user actions and AI outputs, using cross-entropy and Adam \citep{kingma2014adamam}. We report trust calibration error, labeling accuracy, KL divergence of estimated vs.~true bias, and questionnaire alignment scores.

\section{Experimental Setup}
We simulate $N=2000$ samples in $\mathbb R^2$ with a logistic decision boundary. Splits are 60/15/25\% train/val/test. A small MLP (2–16–2) learns the boundary. User models are neural networks that either mimic the AI (static) or apply corrections per interface logic. We sweep batch sizes $\{16,32,64\}$, run 20 epochs, and ablate teacher features, label inputs (soft vs.~hard), and pseudo-labeling confidence thresholds $\{0.6,0.8,0.9\}$.

\section{Results}
\subsection{Baseline Convergence}
\Cref{fig:baseline} shows training (solid) and validation (dashed) accuracy (top) and loss (bottom) over epochs for batch sizes 16, 32, and 64. All curves reach ≈99\% accuracy and ≈0 loss by epoch 5; validation closely tracks training with minor ±0.5\% fluctuations at batch size 64, indicating negligible room for dynamic explanations.

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{baseline_metrics.png}
  \caption{Training (solid) and validation (dashed) accuracy (top) and loss (bottom) over 20 epochs for batch sizes 16, 32, and 64. All accuracies stabilize at ≈99\% and losses fall to ≈0 by epoch 5; minor fluctuations (±0.5\%) appear for batch size 64.}
  \label{fig:baseline}
\end{figure}

\subsection{Ablation Studies}
Figure~\ref{fig:ablations} reports two ablations: (a) removal of teacher probability features and (b) soft vs.~hard label inputs. In both cases, training and validation accuracy/loss converge to ≈99\%/≈0, showing these factors do not differentiate interface performance.

\begin{figure}[t]
  \centering
  \subfigure[]{\includegraphics[width=0.48\textwidth]{teacher_feature_ablation.png}}
  \hfill
  \subfigure[]{\includegraphics[width=0.48\textwidth]{label_input_ablation.png}}
  \caption{Ablation of (a) teacher probability features and (b) label input type: training (solid) and validation (dashed) accuracy (top) and loss (bottom) over 20 epochs. Both ablations converge to ≈99\% accuracy and ≈0 loss, indicating minimal impact.}
  \label{fig:ablations}
\end{figure}

\subsection{Confidence Threshold Ablation}
Figure~\ref{fig:confidence} shows pseudo-labeling at thresholds 0.6, 0.8, and 0.9: left, training/validation accuracy; middle, losses; right, test accuracy. All thresholds yield ≈100\% training accuracy, validation plateaus at 93–96\%, and test accuracy ≥98\%.

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{confidence_ablation_summary.png}
  \caption{Confidence-filtered pseudo-labeling for thresholds 0.6, 0.8, 0.9: (left) training (solid) and validation (dashed) accuracy; (middle) corresponding loss; (right) final test accuracy. All thresholds saturate, showing trivial task difficulty.}
  \label{fig:confidence}
\end{figure}

\subsection{Co-Adaptive Interface Evaluation}
We implemented the dual-channel interface but observed no measurable improvement on any alignment metric compared to static or single-channel variants; all interfaces saturate by epoch 5 (KL divergence →0, trust error →0). Detailed class imbalance and activation-function ablations in the Appendix (\Cref{fig:imbalance,fig:activation}) similarly show negligible effects.

\section{Conclusion}
Our negative results highlight a pitfall: synthetic tasks that saturate simple baselines cannot reveal benefits of co-adaptive explanations. We argue for richer, noisy benchmarks and human-subject studies incorporating cognitive-load measures \citep{sweller1988cognitiveld} and diverse bias profiles to assess whether bias-awareness signals truly improve long-term trust and mental-model alignment.

\bibliography{references}
\bibliographystyle{iclr2025}

\appendix
\section*{Supplementary Material}
\subsection{Class Imbalance Ablation}
\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{class_imbalance_ablation.png}
  \caption{Class imbalance ablation: test accuracy across nine batch-size/user-count settings under ratios 50:50, 70:30, 90:10. All configurations yield ≈98–99\%, showing negligible impact of class skew.}
  \label{fig:imbalance}
\end{figure}

\subsection{Activation Function Ablation}
\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{activation_ablation_appendix.png}
  \caption{Activation ablation: ReLU, Tanh, LeakyReLU, and Linear on the synthetic task. All activations converge to ≈100\% accuracy and near-zero loss by epoch 5; test accuracy differs by ≤1\%.}
  \label{fig:activation}
\end{figure}

\end{document}
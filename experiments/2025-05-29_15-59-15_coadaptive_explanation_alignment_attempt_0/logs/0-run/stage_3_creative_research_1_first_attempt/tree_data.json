{"edges": [[0, 16], [0, 14], [0, 2], [0, 13], [0, 5], [0, 15], [0, 3], [0, 1], [0, 6], [1, 8], [2, 11], [3, 4], [4, 7], [5, 9], [8, 12], [9, 10]], "layout": [[0.5, 0.0], [0.0, 0.33333333333333337], [0.125, 0.33333333333333337], [0.25, 0.33333333333333337], [0.25, 0.6666666666666667], [0.375, 0.33333333333333337], [0.5, 0.33333333333333337], [0.25, 1.0], [0.0, 0.6666666666666667], [0.375, 0.6666666666666667], [0.375, 1.0], [0.125, 0.6666666666666667], [0.0, 1.0], [0.625, 0.33333333333333337], [0.75, 0.33333333333333337], [0.875, 0.33333333333333337], [1.0, 0.33333333333333337]], "plan": ["Hyperparam tuning name: batch_size. Below is a solution that wraps the AI\u2010 and\nuser\u2010model training inside nested loops over a set of batch sizes. For each (AI,\nuser) batch\u2010size pair it reinitializes data loaders, models, and optimizers,\ntrains the AI model for 15 epochs, generates predictions, then trains the user\nmodel for 20 epochs. It collects per\u2010epoch train/val losses and accuracies plus\nfinal test predictions/labels under a key encoding the batch sizes. Finally it\nsaves the resulting `experiment_data` dict to `experiment_data.npy`.", "To evaluate co\u2010adaptive explanation interfaces across diverse real\u2010world tasks,\nwe load three tabular classification datasets from HuggingFace: the iris\ndataset, and the UCI wine\u2010quality datasets for red and white wine. For each\ndataset we split into train/validation/test sets, normalize features, and train\na simple logistic AIModel to predict ground\u2010truth labels. We extract AI feature\nimportance directly from the model\u2019s learned weights to serve as the true\nreasoning distribution. Next, we train a lightweight UserModel to mimic the AI\u2019s\npredictions based on both the original features and the AI\u2019s probability\noutputs, enabling the bias\u2010awareness channel. After each training epoch, we\ncompute the Human\u2013AI Feature Alignment Score as one minus the normalized KL\ndivergence between the AI and UserModel feature\u2010importance distributions. We\ntrack and print validation loss, accuracy, and alignment score at each epoch,\nand save all metrics for each dataset in numpy format. All code leverages GPU\nacceleration when available and strictly follows device placement and data\nsaving protocols.", "We extend the evaluation to three real\u2010world tabular tasks from HuggingFace\n(Iris, Wine Quality white, and Wine Quality red). For each dataset, we split\ninto train/val/test, normalize features, and train a lightweight AI MLP, logging\ntrain/validation losses and accuracies per epoch and printing validation loss.\nAfter AI training, we extract the model\u2019s true feature\u2010importance distribution\nfrom the first linear layer. We then simulate a \u201cuser\u201d MLP that ingests both\nnormalized features and AI\u2010predicted probabilities; during its training we\ncompute the Human\u2013AI Feature Alignment Score at each epoch as 1 \u2013 (normalized KL\ndivergence between AI and user importance distributions). We log and print user\nvalidation losses each epoch, collect per\u2010epoch metrics (losses, accuracies,\nalignment), and finally evaluate user predictions on held\u2010out test splits. All\nresults\u2014including per\u2010epoch metrics, feature distributions, predictions, and\nground truth\u2014are saved as numpy arrays in ./working/experiment_data.npy for\ndownstream analysis.", "We evaluate our co\u2010adaptive dual\u2010channel interface on three HuggingFace tabular\ndatasets (Iris, Wine Quality, Adult). For each, we load and split into\ntrain/val/test, select numeric features and labels, and train a small MLP\n(AIModel) while logging per\u2010epoch train/val losses and validation accuracy. We\nthen extract AI content signals (predicted probabilities) and a static AI\nfeature\u2010importance distribution. We simulate two user conditions: a static\nbaseline (UserModel trained on raw features only) and our dual\u2010channel interface\n(UserModel trained on features augmented with AI content and bias signals). At\neach user epoch, we compute labeling accuracy, trust calibration error (mean\n|user confidence \u2013 AI confidence|), and the Human\u2010AI Feature Alignment Score (1\n\u2013 normalized KL divergence between user and AI feature\u2010importance). We print\nvalidation losses each epoch and save all metrics in\n\u201cworking/experiment_data.npy\u201d for later analysis.", "I updated the dataset identifiers to match the HuggingFace hub names (using\n\u201cuci_iris\u201d instead of \u201ciris\u201d and \u201cwine-quality\u201d instead of \u201cwine_quality\u201d) and\nadded a check to wrap any single\u2010split Dataset into a DatasetDict with a \u201ctrain\u201d\nkey. This guarantees that `raw[\"train\"]` always exists and prevents the\nDatasetNotFoundError, while preserving the original split logic and experiment\nflow.", "We load three HuggingFace text\u2010classification datasets (AG News, Yelp Polarity,\nDBpedia14), subsample each to train/val/test splits and vectorize the text via a\nbag\u2010of\u2010words CountVectorizer into a 1000\u2010dim feature space. A simple\nfeed\u2010forward network is trained on GPU to predict labels and output class\nprobabilities (content channel), which are then concatenated with the original\nfeatures to train a lightweight user model for one epoch (bias\u2010awareness\nchannel). At each AI training epoch we print the validation loss, compute a\nHuman\u2013AI Feature Alignment Score by comparing the normalized absolute weights of\nthe AI and user models via a KL divergence, and save train/val losses and\nalignment scores into `experiment_data`. After the co\u2010adaptive loop, we run\nfinal evaluation on held\u2010out test data to collect predictions and ground truth\nlabels. Finally, all metrics, losses, predictions, and alignment scores are\nsaved in the `working` directory as a NumPy file for downstream analysis.", "We propose to evaluate co-adaptive explanation interfaces by simulating user\nbiases and tracking alignment on three real-world HuggingFace text\nclassification datasets: AG News, IMDB, and Yelp Reviews. For each dataset, we\nextract a bag-of-words representation limited to the top 100 tokens, train a\nsimple linear classification model on GPU, and record per-epoch train and\nvalidation losses and accuracies. We simulate an initial user bias distribution\nthat overweights a fixed subset of features and then implement three explanation\ninterfaces: static (no bias update), single-channel (slow bias correction), and\ndual-channel (fast bias correction). At each epoch we derive the AI\u2019s feature\nimportance distribution from the mean absolute learned weights and update the\nuser\u2019s bias estimate via an alpha-weighted blending. We compute the Human-AI\nFeature Alignment Score as one minus the KL divergence (normalized by log\nvocabulary size) between the user\u2019s estimated and AI\u2019s true feature\ndistributions. All metrics and alignment scores are stored per epoch and per\nmethod in a structured dictionary and saved as a NumPy array. This setup\nprovides a lightweight yet effective testbed for co-adaptive explanation\ndynamics across diverse datasets. The code handles GPU/CPU transparently,\nnormalizes features, and follows rigorous logging and saving protocols.", "We replace the nonexistent \u201cuci_iris\u201d dataset ID with the correct \u201ciris\u201d\nidentifier so that `load_dataset` can find and load it. No other changes are\nneeded: the rest of the data\u2010splitting, model training, signal generation, and\nevaluation loops remain intact. This simple swap fixes the dataset loading\nerror.", "We switch to loading the iris data via the CSV builder pointing at a public raw\nURL and explicitly map its string labels to integers, while wine-quality is\nloaded by its HF configs. We uniformly treat the last column as the label,\nnormalize features based on the training split, and ensure unique keys (\u201ciris\u201d,\n\u201cwine-quality-red\u201d, \u201cwine-quality-white\u201d) so results don\u2019t collide. The rest of\nthe training, alignment computation, and metrics saving remains the same.", "We replace the sklearn CountVectorizer with a simple custom vocabulary builder\nand tokenizer to avoid external dependencies, splitting on whitespace and\nselecting the top 1,000 most frequent tokens. These tokens are then used to\nbuild bag\u2010of\u2010words feature arrays for train, val, and test splits. All model\ninputs are normalized per document to ensure stable training, and we maintain\nthe original loops to compute and print train/val losses and human\u2013AI alignment\nscores. Finally, we save the complete `experiment_data` dictionary at\n`working_dir` as a NumPy file for later analysis.", "We restrict the user\u2010model importance vector to the original feature dimensions\nby slicing off the appended probability channels before computing KL divergence.\nSpecifically, we compute `usr_imp = mean(abs(user_model.fc1.weight[:, :d_in]),\ndim=0)`, so both `usr_imp` and `ai_imp` are length `d_in` and can be compared.\nThe rest of the training, evaluation, and metric logging remains unchanged,\nrecording epoch\u2010tagged training loss, validation loss, and the co\u2010adaptive\nalignment score.", "I\u2019ve removed the sklearn dependency by implementing manual feature normalization\nusing NumPy: computing per-feature mean and standard deviation on the training\nset and applying these parameters to the validation and test sets. This ensures\nproper input standardization without external libraries. The rest of the\npipeline\u2014including GPU device handling, model training, dual AI/user training\nloops, alignment metric computation, and data saving\u2014remains intact and\nexecutable as-is.", "I fixed the gradient issue by detaching the prediction tensors or wrapping them\nin a `torch.no_grad()` context before calling `.numpy()`. This ensures that\ntensors don\u2019t require gradients when converting to NumPy. The rest of the\nworkflow remains unchanged, and the complete pipeline is provided below.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic dataset generation\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Model definitions\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameter grid\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\n\n# Container for all results\nexperiment_data = {\"batch_size\": {}}\n\nfor ai_bs in ai_batch_sizes:\n    # AI data loaders\n    ai_tr_loader = DataLoader(\n        SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True\n    )\n    ai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n\n    # Initialize AI model\n    ai_model = AIModel(D, 16, 2).to(device)\n    criterion_ai = nn.CrossEntropyLoss()\n    optimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n\n    # Train AI model\n    for _ in range(15):\n        ai_model.train()\n        for batch in ai_tr_loader:\n            x = batch[\"x\"].to(device)\n            yb = batch[\"y\"].to(device)\n            out = ai_model(x)\n            loss = criterion_ai(out, yb)\n            optimizer_ai.zero_grad()\n            loss.backward()\n            optimizer_ai.step()\n\n    # Generate AI probabilities\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n    p_train = probs_all[: len(X_train)]\n    p_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\n    p_test = probs_all[-len(X_test) :]\n    f_train = p_train.argmax(axis=1)\n    f_val = p_val.argmax(axis=1)\n    f_test = p_test.argmax(axis=1)\n\n    # Prepare user features\n    X_usr_train = np.hstack([X_train, p_train])\n    X_usr_val = np.hstack([X_val, p_val])\n    X_usr_test = np.hstack([X_test, p_test])\n\n    for usr_bs in usr_batch_sizes:\n        # User data loaders\n        usr_tr_loader = DataLoader(\n            UserDS(X_usr_train, f_train), batch_size=usr_bs, shuffle=True\n        )\n        usr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=usr_bs)\n        usr_test_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=usr_bs)\n\n        # Initialize User model\n        user_model = UserModel(D + 2, 8, 2).to(device)\n        criterion_usr = nn.CrossEntropyLoss()\n        optimizer_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n\n        train_accs, val_accs = [], []\n        train_losses, val_losses = [], []\n\n        # Train User model\n        for _ in range(20):\n            user_model.train()\n            t_loss, corr, tot = 0.0, 0, 0\n            for batch in usr_tr_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                loss = criterion_usr(out, lbl)\n                optimizer_usr.zero_grad()\n                loss.backward()\n                optimizer_usr.step()\n                t_loss += loss.item() * feat.size(0)\n                preds = out.argmax(dim=1)\n                corr += (preds == lbl).sum().item()\n                tot += lbl.size(0)\n            train_losses.append(t_loss / tot)\n            train_accs.append(corr / tot)\n\n            user_model.eval()\n            v_loss, v_corr, v_tot = 0.0, 0, 0\n            with torch.no_grad():\n                for batch in usr_val_loader:\n                    feat = batch[\"feat\"].to(device)\n                    lbl = batch[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = criterion_usr(out, lbl)\n                    v_loss += loss.item() * feat.size(0)\n                    preds = out.argmax(dim=1)\n                    v_corr += (preds == lbl).sum().item()\n                    v_tot += lbl.size(0)\n            val_losses.append(v_loss / v_tot)\n            val_accs.append(v_corr / v_tot)\n\n        # Test evaluation\n        test_preds, test_gt = [], []\n        user_model.eval()\n        with torch.no_grad():\n            for batch in usr_test_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                p = out.argmax(dim=1).cpu().numpy()\n                test_preds.extend(p.tolist())\n                test_gt.extend(lbl.cpu().numpy().tolist())\n\n        key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n        experiment_data[\"batch_size\"][key] = {\n            \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n            \"losses\": {\"train\": np.array(train_losses), \"val\": np.array(val_losses)},\n            \"predictions\": np.array(test_preds),\n            \"ground_truth\": np.array(test_gt),\n        }\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass AIModel(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, out_dim)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, out_dim)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\ndatasets = [(\"iris\", None), (\"wine-quality\", \"red\"), (\"wine-quality\", \"white\")]\nexperiment_data = {}\n\nfor name, cfg in datasets:\n    # Load dataset\n    if cfg:\n        ds = load_dataset(name, cfg)[\"train\"]\n    else:\n        ds = load_dataset(name)[\"train\"]\n    cols = ds.column_names\n    label_col = \"label\" if \"label\" in cols else \"quality\"\n    feat_cols = [c for c in cols if c != label_col]\n    # Extract features and labels\n    data = ds\n    X = np.array([[ex[c] for c in feat_cols] for ex in data])\n    y = np.array([ex[label_col] for ex in data])\n    if label_col == \"quality\":\n        y = (y >= 6).astype(int)\n    # Split\n    N = len(y)\n    idx = np.random.default_rng(0).permutation(N)\n    n_train = int(0.8 * N)\n    n_val = int(0.1 * N)\n    train_idx = idx[:n_train]\n    val_idx = idx[n_train : n_train + n_val]\n    test_idx = idx[n_train + n_val :]\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_val, y_val = X[val_idx], y[val_idx]\n    X_test, y_test = X[test_idx], y[test_idx]\n    # Normalize\n    mean = X_train.mean(0)\n    std = X_train.std(0) + 1e-6\n    X_train = (X_train - mean) / std\n    X_val = (X_val - mean) / std\n    X_test = (X_test - mean) / std\n    num_feat = X_train.shape[1]\n    num_cls = len(np.unique(y_train))\n    # Train AI model\n    ai_model = AIModel(num_feat, num_cls).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(ai_model.parameters(), lr=1e-2)\n    tr_loader = DataLoader(SimpleDS(X_train, y_train), batch_size=32, shuffle=True)\n    val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=32)\n    for epoch in range(10):\n        ai_model.train()\n        for batch in tr_loader:\n            x = batch[\"x\"].to(device)\n            yb = batch[\"y\"].to(device)\n            out = ai_model(x)\n            loss = criterion(out, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        ai_model.eval()\n        v_loss = v_corr = v_tot = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                x = batch[\"x\"].to(device)\n                yb = batch[\"y\"].to(device)\n                out = ai_model(x)\n                loss = criterion(out, yb)\n                v_loss += loss.item() * x.size(0)\n                preds = out.argmax(dim=1)\n                v_corr += (preds == yb).sum().item()\n                v_tot += yb.size(0)\n        val_loss = v_loss / v_tot\n        print(f\"Dataset {name} AI Epoch {epoch+1}: validation_loss = {val_loss:.4f}\")\n    # AI feature distribution\n    with torch.no_grad():\n        w = ai_model.linear.weight.data.abs().cpu().numpy()\n        f_imp = w.sum(axis=0)\n        p_ai = f_imp / f_imp.sum()\n    # AI probabilities for user\n    ai_model.eval()\n    X_train_t = torch.from_numpy(X_train).float().to(device)\n    X_val_t = torch.from_numpy(X_val).float().to(device)\n    p_train = torch.softmax(ai_model(X_train_t), dim=1).cpu().numpy()\n    p_val = torch.softmax(ai_model(X_val_t), dim=1).cpu().numpy()\n    f_train = p_train.argmax(axis=1)\n    f_val = p_val.argmax(axis=1)\n    X_usr_train = np.hstack([X_train, p_train])\n    X_usr_val = np.hstack([X_val, p_val])\n    # Train User model\n    usr_input = num_feat + num_cls\n    user_model = UserModel(usr_input, num_cls).to(device)\n    optimizer_u = optim.Adam(user_model.parameters(), lr=1e-2)\n    tr_u_loader = DataLoader(UserDS(X_usr_train, f_train), batch_size=32, shuffle=True)\n    val_u_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=32)\n    experiment_data[name] = {\n        \"metrics\": {\n            \"train_loss\": [],\n            \"train_acc\": [],\n            \"val_loss\": [],\n            \"val_acc\": [],\n            \"alignment\": [],\n        }\n    }\n    for epoch in range(15):\n        user_model.train()\n        t_loss = t_corr = t_tot = 0\n        for batch in tr_u_loader:\n            xb = batch[\"x\"].to(device)\n            yb = batch[\"y\"].to(device)\n            out = user_model(xb)\n            loss = criterion(out, yb)\n            optimizer_u.zero_grad()\n            loss.backward()\n            optimizer_u.step()\n            t_loss += loss.item() * xb.size(0)\n            preds = out.argmax(dim=1)\n            t_corr += (preds == yb).sum().item()\n            t_tot += yb.size(0)\n        train_loss = t_loss / t_tot\n        train_acc = t_corr / t_tot\n        user_model.eval()\n        v_loss = v_corr = v_tot = 0\n        with torch.no_grad():\n            for batch in val_u_loader:\n                xb = batch[\"x\"].to(device)\n                yb = batch[\"y\"].to(device)\n                out = user_model(xb)\n                loss = criterion(out, yb)\n                v_loss += loss.item() * xb.size(0)\n                preds = out.argmax(dim=1)\n                v_corr += (preds == yb).sum().item()\n                v_tot += yb.size(0)\n        val_loss = v_loss / v_tot\n        val_acc = v_corr / v_tot\n        # Alignment\n        w_u = user_model.linear.weight.data.abs().cpu().numpy()[:, :num_feat]\n        f_imp_u = w_u.sum(axis=0)\n        p_u = f_imp_u / f_imp_u.sum()\n        eps = 1e-10\n        kl = (p_ai * np.log((p_ai + eps) / (p_u + eps))).sum()\n        norm_kl = kl / np.log(num_feat)\n        align_score = max(0.0, 1 - norm_kl)\n        experiment_data[name][\"metrics\"][\"train_loss\"].append(train_loss)\n        experiment_data[name][\"metrics\"][\"train_acc\"].append(train_acc)\n        experiment_data[name][\"metrics\"][\"val_loss\"].append(val_loss)\n        experiment_data[name][\"metrics\"][\"val_acc\"].append(val_acc)\n        experiment_data[name][\"metrics\"][\"alignment\"].append(align_score)\n        print(f\"Dataset {name} User Epoch {epoch+1}: validation_loss = {val_loss:.4f}\")\n# Save all metrics\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nfrom datasets import load_dataset\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nimport torch.nn.functional as F\nimport pandas as pd\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Dataset\nclass TabularDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\n# MLP\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Tasks\nhf_tasks = {\n    \"iris\": load_dataset(\"iris\"),\n    \"wine_white\": load_dataset(\"wine-quality\", \"white\"),\n    \"wine_red\": load_dataset(\"wine-quality\", \"red\"),\n}\n\nexperiment_data = {}\n\nfor name, raw in hf_tasks.items():\n    ds = raw[\"train\"] if isinstance(raw, dict) and \"train\" in raw else raw\n    split1 = ds.train_test_split(test_size=0.3, seed=42)\n    train_val = split1[\"train\"].train_test_split(test_size=0.1, seed=42)\n    train_ds, val_ds, test_ds = train_val[\"train\"], train_val[\"test\"], split1[\"test\"]\n\n    label_col = train_ds.column_names[-1]\n    feat_cols = train_ds.column_names[:-1]\n\n    df_train, df_val, df_test = (\n        train_ds.to_pandas(),\n        val_ds.to_pandas(),\n        test_ds.to_pandas(),\n    )\n\n    X_train, X_val, X_test = (\n        df_train[feat_cols].values,\n        df_val[feat_cols].values,\n        df_test[feat_cols].values,\n    )\n    y_train, y_val, y_test = (\n        df_train[label_col].values,\n        df_val[label_col].values,\n        df_test[label_col].values,\n    )\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(X_test)\n\n    inp_dim = X_train.shape[1]\n    num_classes = len(np.unique(y_train))\n\n    AI = MLP(inp_dim, 16, num_classes).to(device)\n    opt_ai = optim.Adam(AI.parameters(), lr=1e-2)\n    crit = nn.CrossEntropyLoss()\n    ai_tr_losses, ai_val_losses, ai_tr_accs, ai_val_accs = [], [], [], []\n\n    for epoch in range(10):\n        AI.train()\n        tr_loss, tr_corr, tr_tot = 0, 0, 0\n        for batch in DataLoader(TabularDataset(X_train, y_train), 32, True):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = AI(batch[\"x\"])\n            loss = crit(out, batch[\"y\"])\n            opt_ai.zero_grad()\n            loss.backward()\n            opt_ai.step()\n            tr_loss += loss.item() * batch[\"y\"].size(0)\n            preds = out.argmax(1)\n            tr_corr += (preds == batch[\"y\"]).sum().item()\n            tr_tot += batch[\"y\"].size(0)\n        ai_tr_losses.append(tr_loss / tr_tot)\n        ai_tr_accs.append(tr_corr / tr_tot)\n\n        AI.eval()\n        v_loss, v_corr, v_tot = 0, 0, 0\n        with torch.no_grad():\n            for batch in DataLoader(TabularDataset(X_val, y_val), 64):\n                batch = {k: v.to(device) for k, v in batch.items()}\n                out = AI(batch[\"x\"])\n                loss = crit(out, batch[\"y\"])\n                v_loss += loss.item() * batch[\"y\"].size(0)\n                preds = out.argmax(1)\n                v_corr += (preds == batch[\"y\"]).sum().item()\n                v_tot += batch[\"y\"].size(0)\n        val_loss = v_loss / v_tot\n        ai_val_losses.append(val_loss)\n        ai_val_accs.append(v_corr / v_tot)\n        print(f\"[{name}][AI] Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n    wi = AI.net[0].weight.detach().abs().sum(0)\n    true_dist = wi / wi.sum()\n    true_dist_np = true_dist.cpu().numpy()\n\n    AI.eval()\n    with torch.no_grad():\n        X_all = np.vstack([X_train, X_val, X_test])\n        X_t = torch.from_numpy(X_all).float().to(device)\n        logits = AI(X_t)\n        probs = F.softmax(logits, 1).cpu().numpy()\n        preds_all = logits.argmax(1).cpu().numpy()\n\n    p_train, p_val, p_test = (\n        probs[: len(X_train)],\n        probs[len(X_train) : len(X_train) + len(X_val)],\n        probs[-len(X_test) :],\n    )\n    f_train, f_val, f_test = (\n        preds_all[: len(X_train)],\n        preds_all[len(X_train) : len(X_train) + len(X_val)],\n        preds_all[-len(X_test) :],\n    )\n\n    X_usr_train = np.hstack([X_train, p_train])\n    X_usr_val = np.hstack([X_val, p_val])\n    X_usr_test = np.hstack([X_test, p_test])\n\n    user = MLP(inp_dim + num_classes, 16, num_classes).to(device)\n    opt_usr = optim.Adam(user.parameters(), lr=1e-2)\n    usr_tr_losses, usr_val_losses, usr_tr_accs, usr_val_accs, align_scores = (\n        [],\n        [],\n        [],\n        [],\n        [],\n    )\n\n    for epoch in range(10):\n        user.train()\n        t_loss, t_corr, t_tot = 0, 0, 0\n        for batch in DataLoader(TabularDataset(X_usr_train, f_train), 32, True):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = user(batch[\"x\"])\n            loss = crit(out, batch[\"y\"])\n            opt_usr.zero_grad()\n            loss.backward()\n            opt_usr.step()\n            t_loss += loss.item() * batch[\"y\"].size(0)\n            p = out.argmax(1)\n            t_corr += (p == batch[\"y\"]).sum().item()\n            t_tot += batch[\"y\"].size(0)\n        usr_tr_losses.append(t_loss / t_tot)\n        usr_tr_accs.append(t_corr / t_tot)\n\n        user.eval()\n        v_loss, v_corr, v_tot = 0, 0, 0\n        with torch.no_grad():\n            for batch in DataLoader(TabularDataset(X_usr_val, f_val), 64):\n                batch = {k: v.to(device) for k, v in batch.items()}\n                out = user(batch[\"x\"])\n                loss = crit(out, batch[\"y\"])\n                v_loss += loss.item() * batch[\"y\"].size(0)\n                p = out.argmax(1)\n                v_corr += (p == batch[\"y\"]).sum().item()\n                v_tot += batch[\"y\"].size(0)\n        val_loss = v_loss / v_tot\n        usr_val_losses.append(val_loss)\n        usr_val_accs.append(v_corr / v_tot)\n\n        w_u = user.net[0].weight.detach().abs()[:, :inp_dim].sum(0)\n        u_dist = w_u / w_u.sum()\n        kl = (true_dist * torch.log(true_dist / u_dist)).sum()\n        norm_kl = kl / np.log(inp_dim)\n        align_scores.append((1 - norm_kl).item())\n\n        print(f\"[{name}][User] Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n    preds_test, gt_test = [], []\n    user.eval()\n    with torch.no_grad():\n        for batch in DataLoader(TabularDataset(X_usr_test, f_test), 64):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = user(batch[\"x\"])\n            p = out.argmax(1).cpu().numpy()\n            preds_test.extend(p.tolist())\n            gt_test.extend(batch[\"y\"].cpu().numpy().tolist())\n\n    experiment_data[name] = {\n        \"ai\": {\n            \"losses\": {\"train\": np.array(ai_tr_losses), \"val\": np.array(ai_val_losses)},\n            \"metrics\": {\n                \"train_acc\": np.array(ai_tr_accs),\n                \"val_acc\": np.array(ai_val_accs),\n            },\n            \"feature_importance\": true_dist_np,\n        },\n        \"user\": {\n            \"losses\": {\n                \"train\": np.array(usr_tr_losses),\n                \"val\": np.array(usr_val_losses),\n            },\n            \"metrics\": {\n                \"train_acc\": np.array(usr_tr_accs),\n                \"val_acc\": np.array(usr_val_accs),\n            },\n            \"alignment_score\": np.array(align_scores),\n        },\n        \"predictions\": np.array(preds_test),\n        \"ground_truth\": np.array(gt_test),\n    }\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom datasets import load_dataset, Value, ClassLabel\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\nclass TabularDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"features\": self.X[idx], \"labels\": self.y[idx]}\n\n\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\ndataset_configs = [(\"iris\", None), (\"wine_quality\", \"white\"), (\"adult\", None)]\nexperiment_data = {}\n\nfor ds_name, cfg in dataset_configs:\n    raw = load_dataset(ds_name, cfg) if cfg is not None else load_dataset(ds_name)\n    # split train/val/test\n    if \"train\" in raw:\n        full = raw[\"train\"]\n        split1 = full.train_test_split(test_size=0.2, seed=42)\n        train_ds = split1[\"train\"]\n        tmp = split1[\"test\"].train_test_split(test_size=0.5, seed=42)\n        val_ds, test_ds = tmp[\"train\"], tmp[\"test\"]\n    else:\n        train_ds = raw[\"train\"]\n        tmp = train_ds.train_test_split(test_size=0.2, seed=42)\n        train_ds, val_ds = tmp[\"train\"], tmp[\"test\"]\n        test_ds = raw.get(\"test\", val_ds)\n    # feature/label selection\n    num_features = [\n        k\n        for k, v in train_ds.features.items()\n        if isinstance(v, Value) and v.dtype in (\"float32\", \"int64\", \"float64\")\n    ]\n    label_col = [k for k, v in train_ds.features.items() if isinstance(v, ClassLabel)][\n        0\n    ]\n\n    # extract numpy arrays\n    def to_np(ds, cols, label):\n        X = np.stack([np.array(ds[c], dtype=np.float32) for c in cols], 1)\n        y = np.array(ds[label], dtype=np.int64)\n        return X, y\n\n    X_train, y_train = to_np(train_ds, num_features, label_col)\n    X_val, y_val = to_np(val_ds, num_features, label_col)\n    X_test, y_test = to_np(test_ds, num_features, label_col)\n\n    # subsample for speed\n    def sub(X, y, n):\n        if len(y) > n:\n            idx = np.random.RandomState(0).permutation(len(y))[:n]\n            return X[idx], y[idx]\n        return X, y\n\n    X_train, y_train = sub(X_train, y_train, 2000)\n    X_val, y_val = sub(X_val, y_val, 500)\n    X_test, y_test = sub(X_test, y_test, 500)\n    # normalize\n    mu, sd = X_train.mean(0), X_train.std(0) + 1e-6\n    X_train = (X_train - mu) / sd\n    X_val = (X_val - mu) / sd\n    X_test = (X_test - mu) / sd\n    # DataLoaders for AIModel\n    bs_ai = 32\n    tr_ai = DataLoader(TabularDS(X_train, y_train), batch_size=bs_ai, shuffle=True)\n    vl_ai = DataLoader(TabularDS(X_val, y_val), batch_size=bs_ai, shuffle=False)\n    te_ai = DataLoader(TabularDS(X_test, y_test), batch_size=bs_ai, shuffle=False)\n    # init AIModel\n    inp_dim, out_dim = X_train.shape[1], int(y_train.max() + 1)\n    ai_model = AIModel(inp_dim, 16, out_dim).to(device)\n    crit = nn.CrossEntropyLoss()\n    opt = optim.Adam(ai_model.parameters(), lr=1e-2)\n    experiment_data[ds_name] = {\n        \"ai\": {\"train_losses\": [], \"val_losses\": [], \"val_accs\": []}\n    }\n    # train AIModel\n    for epoch in range(1, 11):\n        ai_model.train()\n        t_loss = t_corr = t_tot = 0\n        for b in tr_ai:\n            d = {k: v.to(device) for k, v in b.items()}\n            out = ai_model(d[\"features\"])\n            loss = crit(out, d[\"labels\"])\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            t_loss += loss.item() * d[\"labels\"].size(0)\n            t_corr += (out.argmax(1) == d[\"labels\"]).sum().item()\n            t_tot += d[\"labels\"].size(0)\n        train_loss = t_loss / t_tot\n        ai_model.eval()\n        v_loss = v_corr = v_tot = 0\n        with torch.no_grad():\n            for b in vl_ai:\n                d = {k: v.to(device) for k, v in b.items()}\n                out = ai_model(d[\"features\"])\n                loss = crit(out, d[\"labels\"])\n                v_loss += loss.item() * d[\"labels\"].size(0)\n                v_corr += (out.argmax(1) == d[\"labels\"]).sum().item()\n                v_tot += d[\"labels\"].size(0)\n        val_loss = v_loss / v_tot\n        val_acc = v_corr / v_tot\n        print(f\"{ds_name} AI Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n        experiment_data[ds_name][\"ai\"][\"train_losses\"].append(train_loss)\n        experiment_data[ds_name][\"ai\"][\"val_losses\"].append(val_loss)\n        experiment_data[ds_name][\"ai\"][\"val_accs\"].append(val_acc)\n    # generate AI content signals\n    ai_model.eval()\n\n    def get_signals(dl):\n        ps = []\n        preds = []\n        with torch.no_grad():\n            for b in dl:\n                x = b[\"features\"].to(device)\n                out = ai_model(x)\n                p = F.softmax(out, 1).cpu().numpy()\n                ps.append(p)\n                preds.append(p.argmax(1))\n        return np.vstack(ps), np.concatenate(preds)\n\n    ai_p_train, _ = get_signals(tr_ai)\n    ai_p_val, _ = get_signals(vl_ai)\n    ai_p_test, _ = get_signals(te_ai)\n    ai_conf_val = ai_p_val.max(1)\n    # AI feature importance\n    w0 = ai_model.net[0].weight.data.cpu().numpy()  # [hid, inp]\n    ai_feat_imp = np.linalg.norm(w0, axis=0)\n    ai_feat_imp /= ai_feat_imp.sum()\n    # prepare user inputs\n    Xu_stat_train, Xu_stat_val = X_train, X_val\n    Xu_dual_train = np.concatenate(\n        [X_train, ai_p_train, np.tile(ai_feat_imp, (len(X_train), 1))], 1\n    )\n    Xu_dual_val = np.concatenate(\n        [X_val, ai_p_val, np.tile(ai_feat_imp, (len(X_val), 1))], 1\n    )\n    # record user metrics\n    experiment_data[ds_name][\"user\"] = {\n        \"static\": {\"label_accs\": [], \"trust_errors\": [], \"align_scores\": []},\n        \"dual\": {\"label_accs\": [], \"trust_errors\": [], \"align_scores\": []},\n    }\n    for mode in (\"static\", \"dual\"):\n        Xtr = Xu_stat_train if mode == \"static\" else Xu_dual_train\n        Xvl = Xu_stat_val if mode == \"static\" else Xu_dual_val\n        y_tr = ai_p_train.argmax(1)\n        y_vl = ai_p_val.argmax(1)\n        tr_dl = DataLoader(TabularDS(Xtr, y_tr), batch_size=32, shuffle=True)\n        vl_dl = DataLoader(TabularDS(Xvl, y_vl), batch_size=32, shuffle=False)\n        inp_u = Xtr.shape[1]\n        user = UserModel(inp_u, 8, out_dim).to(device)\n        opt_u = optim.Adam(user.parameters(), lr=1e-2)\n        for epoch in range(1, 11):\n            user.train()\n            for b in tr_dl:\n                d = {k: v.to(device) for k, v in b.items()}\n                out = user(d[\"features\"])\n                loss = crit(out, d[\"labels\"])\n                opt_u.zero_grad()\n                loss.backward()\n                opt_u.step()\n            # eval\n            user.eval()\n            # labeling acc\n            c = 0\n            t = 0\n            with torch.no_grad():\n                for b in vl_dl:\n                    d = {k: v.to(device) for k, v in b.items()}\n                    o = user(d[\"features\"])\n                    c += (o.argmax(1) == d[\"labels\"]).sum().item()\n                    t += d[\"labels\"].size(0)\n            lab_acc = c / t\n            # trust error\n            with torch.no_grad():\n                up = (\n                    F.softmax(user(torch.from_numpy(Xvl).float().to(device)), 1)\n                    .cpu()\n                    .numpy()\n                )\n            usr_conf = up.max(1)\n            tr_err = np.mean(np.abs(usr_conf - ai_conf_val))\n            # alignment\n            w_u = user.net[0].weight.data.cpu().numpy()\n            w_u_feat = w_u[:, : len(num_features)] if mode == \"dual\" else w_u\n            usr_imp = np.linalg.norm(w_u_feat, axis=0)\n            usr_imp /= usr_imp.sum()\n            kld = np.sum(usr_imp * np.log((usr_imp) / (ai_feat_imp + 1e-8) + 1e-8))\n            align = 1 - kld / np.log(len(ai_feat_imp))\n            # record\n            exp_u = experiment_data[ds_name][\"user\"][mode]\n            exp_u[\"label_accs\"].append(lab_acc)\n            exp_u[\"trust_errors\"].append(tr_err)\n            exp_u[\"align_scores\"].append(align)\n    del ai_model, user\n\n# save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom datasets import load_dataset, DatasetDict, Dataset, Value, ClassLabel\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\nclass TabularDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"features\": self.X[idx], \"labels\": self.y[idx]}\n\n\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\ndataset_configs = [(\"uci_iris\", None), (\"wine-quality\", \"white\"), (\"adult\", None)]\nexperiment_data = {}\n\nfor ds_name, cfg in dataset_configs:\n    # load and ensure a dict with 'train'\n    raw = load_dataset(ds_name, cfg) if cfg is not None else load_dataset(ds_name)\n    if isinstance(raw, Dataset):\n        raw = DatasetDict({\"train\": raw})\n    # split train/val/test\n    full = raw[\"train\"]\n    split1 = full.train_test_split(test_size=0.2, seed=42)\n    train_ds = split1[\"train\"]\n    tmp = split1[\"test\"].train_test_split(test_size=0.5, seed=42)\n    val_ds, test_ds = tmp[\"train\"], tmp[\"test\"]\n    # feature/label selection\n    num_features = [\n        k\n        for k, v in train_ds.features.items()\n        if isinstance(v, Value) and v.dtype in (\"float32\", \"int64\", \"float64\")\n    ]\n    label_col = [k for k, v in train_ds.features.items() if isinstance(v, ClassLabel)][\n        0\n    ]\n\n    # extract numpy arrays\n    def to_np(ds, cols, label):\n        X = np.stack([np.array(ds[c], dtype=np.float32) for c in cols], 1)\n        y = np.array(ds[label], dtype=np.int64)\n        return X, y\n\n    X_train, y_train = to_np(train_ds, num_features, label_col)\n    X_val, y_val = to_np(val_ds, num_features, label_col)\n    X_test, y_test = to_np(test_ds, num_features, label_col)\n\n    # subsample for speed\n    def sub(X, y, n):\n        if len(y) > n:\n            idx = np.random.RandomState(0).permutation(len(y))[:n]\n            return X[idx], y[idx]\n        return X, y\n\n    X_train, y_train = sub(X_train, y_train, 2000)\n    X_val, y_val = sub(X_val, y_val, 500)\n    X_test, y_test = sub(X_test, y_test, 500)\n    # normalize\n    mu, sd = X_train.mean(0), X_train.std(0) + 1e-6\n    X_train = (X_train - mu) / sd\n    X_val = (X_val - mu) / sd\n    X_test = (X_test - mu) / sd\n    # DataLoaders for AIModel\n    bs_ai = 32\n    tr_ai = DataLoader(TabularDS(X_train, y_train), batch_size=bs_ai, shuffle=True)\n    vl_ai = DataLoader(TabularDS(X_val, y_val), batch_size=bs_ai, shuffle=False)\n    te_ai = DataLoader(TabularDS(X_test, y_test), batch_size=bs_ai, shuffle=False)\n    # init AIModel\n    inp_dim, out_dim = X_train.shape[1], int(y_train.max() + 1)\n    ai_model = AIModel(inp_dim, 16, out_dim).to(device)\n    crit = nn.CrossEntropyLoss()\n    opt = optim.Adam(ai_model.parameters(), lr=1e-2)\n    experiment_data[ds_name] = {\n        \"ai\": {\"train_losses\": [], \"val_losses\": [], \"val_accs\": []}\n    }\n    # train AIModel\n    for epoch in range(1, 11):\n        ai_model.train()\n        t_loss = t_corr = t_tot = 0\n        for b in tr_ai:\n            batch = {k: v.to(device) for k, v in b.items()}\n            out = ai_model(batch[\"features\"])\n            loss = crit(out, batch[\"labels\"])\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            t_loss += loss.item() * batch[\"labels\"].size(0)\n            t_corr += (out.argmax(1) == batch[\"labels\"]).sum().item()\n            t_tot += batch[\"labels\"].size(0)\n        train_loss = t_loss / t_tot\n        ai_model.eval()\n        v_loss = v_corr = v_tot = 0\n        with torch.no_grad():\n            for b in vl_ai:\n                batch = {k: v.to(device) for k, v in b.items()}\n                out = ai_model(batch[\"features\"])\n                loss = crit(out, batch[\"labels\"])\n                v_loss += loss.item() * batch[\"labels\"].size(0)\n                v_corr += (out.argmax(1) == batch[\"labels\"]).sum().item()\n                v_tot += batch[\"labels\"].size(0)\n        val_loss = v_loss / v_tot\n        val_acc = v_corr / v_tot\n        print(f\"{ds_name} AI Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n        experiment_data[ds_name][\"ai\"][\"train_losses\"].append(train_loss)\n        experiment_data[ds_name][\"ai\"][\"val_losses\"].append(val_loss)\n        experiment_data[ds_name][\"ai\"][\"val_accs\"].append(val_acc)\n    # generate AI content signals\n    ai_model.eval()\n\n    def get_signals(dl):\n        ps = []\n        preds = []\n        with torch.no_grad():\n            for b in dl:\n                x = b[\"features\"].to(device)\n                out = ai_model(x)\n                p = F.softmax(out, 1).cpu().numpy()\n                ps.append(p)\n                preds.append(p.argmax(1))\n        return np.vstack(ps), np.concatenate(preds)\n\n    ai_p_train, _ = get_signals(tr_ai)\n    ai_p_val, _ = get_signals(vl_ai)\n    ai_p_test, _ = get_signals(te_ai)\n    ai_conf_val = ai_p_val.max(1)\n    # AI feature importance\n    w0 = ai_model.net[0].weight.data.cpu().numpy()\n    ai_feat_imp = np.linalg.norm(w0, axis=0)\n    ai_feat_imp /= ai_feat_imp.sum()\n    # prepare user inputs\n    Xu_stat_train, Xu_stat_val = X_train, X_val\n    Xu_dual_train = np.concatenate(\n        [X_train, ai_p_train, np.tile(ai_feat_imp, (len(X_train), 1))], 1\n    )\n    Xu_dual_val = np.concatenate(\n        [X_val, ai_p_val, np.tile(ai_feat_imp, (len(X_val), 1))], 1\n    )\n    # record user metrics\n    experiment_data[ds_name][\"user\"] = {\n        \"static\": {\"label_accs\": [], \"trust_errors\": [], \"align_scores\": []},\n        \"dual\": {\"label_accs\": [], \"trust_errors\": [], \"align_scores\": []},\n    }\n    for mode in (\"static\", \"dual\"):\n        Xtr = Xu_stat_train if mode == \"static\" else Xu_dual_train\n        Xvl = Xu_stat_val if mode == \"static\" else Xu_dual_val\n        y_tr = ai_p_train.argmax(1)\n        y_vl = ai_p_val.argmax(1)\n        tr_dl = DataLoader(TabularDS(Xtr, y_tr), batch_size=32, shuffle=True)\n        vl_dl = DataLoader(TabularDS(Xvl, y_vl), batch_size=32, shuffle=False)\n        inp_u = Xtr.shape[1]\n        user = UserModel(inp_u, 8, out_dim).to(device)\n        opt_u = optim.Adam(user.parameters(), lr=1e-2)\n        for epoch in range(1, 11):\n            user.train()\n            for b in tr_dl:\n                batch = {k: v.to(device) for k, v in b.items()}\n                out = user(batch[\"features\"])\n                loss = crit(out, batch[\"labels\"])\n                opt_u.zero_grad()\n                loss.backward()\n                opt_u.step()\n            user.eval()\n            # labeling acc\n            c = t = 0\n            with torch.no_grad():\n                for b in vl_dl:\n                    batch = {k: v.to(device) for k, v in b.items()}\n                    o = user(batch[\"features\"])\n                    c += (o.argmax(1) == batch[\"labels\"]).sum().item()\n                    t += batch[\"labels\"].size(0)\n            lab_acc = c / t\n            # trust error\n            with torch.no_grad():\n                up = (\n                    F.softmax(user(torch.from_numpy(Xvl).float().to(device)), 1)\n                    .cpu()\n                    .numpy()\n                )\n            usr_conf = up.max(1)\n            tr_err = np.mean(np.abs(usr_conf - ai_conf_val))\n            # alignment\n            w_u = user.net[0].weight.data.cpu().numpy()\n            w_u_feat = w_u[:, : len(num_features)] if mode == \"dual\" else w_u\n            usr_imp = np.linalg.norm(w_u_feat, axis=0)\n            usr_imp /= usr_imp.sum()\n            kld = np.sum(usr_imp * np.log((usr_imp) / (ai_feat_imp + 1e-8) + 1e-8))\n            align = 1 - kld / np.log(len(ai_feat_imp))\n            # record\n            exp_u = experiment_data[ds_name][\"user\"][mode]\n            exp_u[\"label_accs\"].append(lab_acc)\n            exp_u[\"trust_errors\"].append(tr_err)\n            exp_u[\"align_scores\"].append(align)\n    del ai_model, user\n\n# save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n\nclass TextDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\nclass TextClassifier(nn.Module):\n    def __init__(self, d_in, d_hid, d_out):\n        super().__init__()\n        self.fc1 = nn.Linear(d_in, d_hid)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(d_hid, d_out)\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n\n\nclass UserModel(nn.Module):\n    def __init__(self, d_in, d_hid, d_out):\n        super().__init__()\n        self.fc1 = nn.Linear(d_in, d_hid)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(d_hid, d_out)\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n\n\nexperiment_data = {}\ndatasets_list = [\"ag_news\", \"yelp_review_polarity\", \"dbpedia_14\"]\n\nfor ds_name in datasets_list:\n    experiment_data[ds_name] = {\n        \"metrics\": {\"train\": [], \"val\": [], \"alignment_score\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    ds = load_dataset(ds_name)\n    split = ds[\"train\"].train_test_split(test_size=0.2, seed=0)\n    train_ds = split[\"train\"].shuffle(seed=0).select(range(2000))\n    val_ds = split[\"test\"].shuffle(seed=0).select(range(500))\n    test_ds = ds[\"test\"].shuffle(seed=0).select(range(500))\n\n    train_text, train_labels = train_ds[\"text\"], train_ds[\"label\"]\n    val_text, val_labels = val_ds[\"text\"], val_ds[\"label\"]\n    test_text, test_labels = test_ds[\"text\"], test_ds[\"label\"]\n\n    vectorizer = CountVectorizer(max_features=1000)\n    X_train = vectorizer.fit_transform(train_text).toarray()\n    X_val = vectorizer.transform(val_text).toarray()\n    X_test = vectorizer.transform(test_text).toarray()\n    y_train = np.array(train_labels)\n    y_val = np.array(val_labels)\n    y_test = np.array(test_labels)\n\n    d_in = X_train.shape[1]\n    d_hid = 64\n    n_cls = len(set(train_labels))\n    ai_model = TextClassifier(d_in, d_hid, n_cls).to(device)\n    optimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n    train_loader = DataLoader(\n        TextDataset(X_train, y_train), batch_size=32, shuffle=True\n    )\n    val_loader = DataLoader(TextDataset(X_val, y_val), batch_size=32)\n\n    for epoch in range(5):\n        ai_model.train()\n        run_loss = 0.0\n        for batch in train_loader:\n            x = batch[\"x\"].to(device)\n            yb = batch[\"y\"].to(device)\n            out = ai_model(x)\n            loss = criterion(out, yb)\n            optimizer_ai.zero_grad()\n            loss.backward()\n            optimizer_ai.step()\n            run_loss += loss.item() * x.size(0)\n        train_loss = run_loss / len(train_loader.dataset)\n\n        ai_model.eval()\n        v_loss = 0.0\n        with torch.no_grad():\n            for batch in val_loader:\n                x = batch[\"x\"].to(device)\n                yb = batch[\"y\"].to(device)\n                l = criterion(ai_model(x), yb)\n                v_loss += l.item() * x.size(0)\n        val_loss = v_loss / len(val_loader.dataset)\n        print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n        with torch.no_grad():\n            Xtr_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n            ptr = torch.softmax(ai_model(Xtr_t), dim=1).cpu().numpy()\n            Xva_t = torch.tensor(X_val, dtype=torch.float32).to(device)\n            pva = torch.softmax(ai_model(Xva_t), dim=1).cpu().numpy()\n\n        X_usr_train = np.hstack([X_train, ptr])\n        y_usr_train = ptr.argmax(axis=1)\n        if epoch == 0:\n            user_model = UserModel(d_in + n_cls, 32, n_cls).to(device)\n            optimizer_usr = optim.Adam(user_model.parameters(), lr=1e-3)\n        user_model.train()\n        usr_loader = DataLoader(\n            TextDataset(X_usr_train, y_usr_train), batch_size=32, shuffle=True\n        )\n        for batch in usr_loader:\n            feat = batch[\"x\"].to(device)\n            lbl = batch[\"y\"].to(device)\n            out_u = user_model(feat)\n            lu = criterion(out_u, lbl)\n            optimizer_usr.zero_grad()\n            lu.backward()\n            optimizer_usr.step()\n\n        ai_imp = torch.mean(torch.abs(ai_model.fc1.weight), dim=0)\n        usr_imp = torch.mean(torch.abs(user_model.fc1.weight), dim=0)\n        dist_ai = ai_imp / ai_imp.sum()\n        dist_usr = usr_imp / usr_imp.sum()\n        kl = torch.sum(dist_usr * torch.log(dist_usr / (dist_ai + 1e-10))).item()\n        align_score = 1 - kl / np.log(len(dist_ai))\n\n        experiment_data[ds_name][\"metrics\"][\"train\"].append(train_loss)\n        experiment_data[ds_name][\"metrics\"][\"val\"].append(val_loss)\n        experiment_data[ds_name][\"metrics\"][\"alignment_score\"].append(align_score)\n\n    ai_model.eval()\n    preds, gts = [], []\n    test_loader = DataLoader(TextDataset(X_test, y_test), batch_size=32)\n    with torch.no_grad():\n        for batch in test_loader:\n            x = batch[\"x\"].to(device)\n            out = ai_model(x).argmax(dim=1).cpu().numpy()\n            preds.extend(out.tolist())\n            gts.extend(batch[\"y\"].tolist())\n    experiment_data[ds_name][\"predictions\"] = np.array(preds)\n    experiment_data[ds_name][\"ground_truth\"] = np.array(gts)\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom collections import Counter\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nnp.random.seed(0)\ntorch.manual_seed(0)\n\ndatasets_list = [\"ag_news\", \"imdb\", \"yelp_review_full\"]\nmax_train, max_val, max_test = 2000, 500, 500\nvocab_size = 100\nbatch_size = 32\nnum_epochs = 5\nalpha_slow, alpha_fast = 0.9, 0.5\neps = 1e-8\n\n\nclass TabularDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass LinearModel(nn.Module):\n    def __init__(self, inp_dim, out_dim):\n        super().__init__()\n        self.fc = nn.Linear(inp_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nexperiment_data = {}\n\nfor dataset_name in datasets_list:\n    # load and subsample\n    ds = load_dataset(dataset_name)\n    train_ds = ds[\"train\"].shuffle(seed=0)\n    test_ds = ds[\"test\"]\n    train_texts = train_ds[\"text\"][:max_train]\n    train_labels = train_ds[\"label\"][:max_train]\n    val_texts = train_ds[\"text\"][max_train : max_train + max_val]\n    val_labels = train_ds[\"label\"][max_train : max_train + max_val]\n    test_texts = test_ds[\"text\"][:max_test]\n    test_labels = test_ds[\"label\"][:max_test]\n    # build vocab\n    ctr = Counter()\n    for txt in train_texts:\n        for w in txt.split():\n            ctr[w.lower()] += 1\n    most = [w for w, _ in ctr.most_common(vocab_size)]\n    vocab = {w: i for i, w in enumerate(most)}\n\n    def vectorize(txts):\n        X = np.zeros((len(txts), vocab_size), dtype=np.float32)\n        for i, txt in enumerate(txts):\n            for w in txt.split():\n                j = vocab.get(w.lower())\n                if j is not None:\n                    X[i, j] += 1\n        return X\n\n    X_train = vectorize(train_texts)\n    X_val = vectorize(val_texts)\n    X_test = vectorize(test_texts)\n    # normalize\n    m, s = X_train.mean(0), X_train.std(0) + 1e-6\n    X_train = (X_train - m) / s\n    X_val = (X_val - m) / s\n    X_test = (X_test - m) / s\n\n    num_classes = len(set(train_labels))\n    # initialize experiment entry\n    experiment_data[dataset_name] = {}\n    # simulate initial user bias\n    bias = np.zeros(vocab_size, np.float32)\n    bias[:10] = 1\n    p_user0 = bias / bias.sum()\n\n    for method, alpha in [\n        (\"static\", 1.0),\n        (\"single\", alpha_slow),\n        (\"dual\", alpha_fast),\n    ]:\n        # model setup\n        model = LinearModel(vocab_size, num_classes).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=1e-2)\n        criterion = nn.CrossEntropyLoss()\n        # dataloaders\n        tr_loader = DataLoader(\n            TabularDS(X_train, np.array(train_labels)),\n            batch_size=batch_size,\n            shuffle=True,\n        )\n        val_loader = DataLoader(\n            TabularDS(X_val, np.array(val_labels)), batch_size=batch_size\n        )\n        # tracking\n        experiment_data[dataset_name][method] = {\n            \"train_loss\": [],\n            \"val_loss\": [],\n            \"train_acc\": [],\n            \"val_acc\": [],\n            \"alignment\": [],\n        }\n        p_user = p_user0.copy()\n        # epochs\n        for epoch in range(num_epochs):\n            model.train()\n            tl, tc, tt = 0, 0, 0\n            for batch in tr_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                out = model(batch[\"x\"])\n                loss = criterion(out, batch[\"y\"])\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                tl += loss.item() * batch[\"x\"].size(0)\n                preds = out.argmax(dim=1)\n                tc += (preds == batch[\"y\"]).sum().item()\n                tt += batch[\"y\"].size(0)\n            train_loss = tl / tt\n            train_acc = tc / tt\n\n            model.eval()\n            vl, vc, vt = 0, 0, 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    out = model(batch[\"x\"])\n                    loss = criterion(out, batch[\"y\"])\n                    vl += loss.item() * batch[\"x\"].size(0)\n                    preds = out.argmax(dim=1)\n                    vc += (preds == batch[\"y\"]).sum().item()\n                    vt += batch[\"y\"].size(0)\n            val_loss = vl / vt\n            val_acc = vc / vt\n\n            print(\n                f\"Dataset: {dataset_name} Method: {method} Epoch {epoch}: validation_loss = {val_loss:.4f}\"\n            )\n\n            # compute feature importance & update user estimate\n            w = model.fc.weight.detach().cpu().numpy()\n            ai_imp = np.abs(w).mean(axis=0)\n            p_ai = ai_imp / (ai_imp.sum() + eps)\n            p_user = alpha * p_user + (1 - alpha) * p_ai\n            kl = np.sum(p_user * (np.log(p_user + eps) - np.log(p_ai + eps)))\n            norm_kl = kl / np.log(vocab_size)\n            align = 1.0 - norm_kl\n\n            # log metrics\n            experiment_data[dataset_name][method][\"train_loss\"].append(train_loss)\n            experiment_data[dataset_name][method][\"val_loss\"].append(val_loss)\n            experiment_data[dataset_name][method][\"train_acc\"].append(train_acc)\n            experiment_data[dataset_name][method][\"val_acc\"].append(val_acc)\n            experiment_data[dataset_name][method][\"alignment\"].append(align)\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom datasets import load_dataset, DatasetDict, Dataset, Value, ClassLabel\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\nclass TabularDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"features\": self.X[idx], \"labels\": self.y[idx]}\n\n\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim),\n            nn.ReLU(),\n            nn.Linear(hid_dim, out_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim),\n            nn.ReLU(),\n            nn.Linear(hid_dim, out_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\ndataset_configs = [(\"iris\", None), (\"wine-quality\", \"white\"), (\"adult\", None)]\nexperiment_data = {}\n\nfor ds_name, cfg in dataset_configs:\n    raw = load_dataset(ds_name, cfg) if cfg is not None else load_dataset(ds_name)\n    if isinstance(raw, Dataset):\n        raw = DatasetDict({\"train\": raw})\n    full = raw[\"train\"]\n    split1 = full.train_test_split(test_size=0.2, seed=42)\n    train_ds = split1[\"train\"]\n    tmp = split1[\"test\"].train_test_split(test_size=0.5, seed=42)\n    val_ds, test_ds = tmp[\"train\"], tmp[\"test\"]\n\n    num_features = [\n        k\n        for k, v in train_ds.features.items()\n        if isinstance(v, Value) and v.dtype in (\"float32\", \"int64\", \"float64\")\n    ]\n    label_col = [k for k, v in train_ds.features.items() if isinstance(v, ClassLabel)][\n        0\n    ]\n\n    def to_np(ds, cols, label):\n        X = np.stack([np.array(ds[c], dtype=np.float32) for c in cols], 1)\n        y = np.array(ds[label], dtype=np.int64)\n        return X, y\n\n    X_train, y_train = to_np(train_ds, num_features, label_col)\n    X_val, y_val = to_np(val_ds, num_features, label_col)\n    X_test, y_test = to_np(test_ds, num_features, label_col)\n\n    def sub(X, y, n):\n        if len(y) > n:\n            idx = np.random.RandomState(0).permutation(len(y))[:n]\n            return X[idx], y[idx]\n        return X, y\n\n    X_train, y_train = sub(X_train, y_train, 2000)\n    X_val, y_val = sub(X_val, y_val, 500)\n    X_test, y_test = sub(X_test, y_test, 500)\n\n    mu, sd = X_train.mean(0), X_train.std(0) + 1e-6\n    X_train = (X_train - mu) / sd\n    X_val = (X_val - mu) / sd\n    X_test = (X_test - mu) / sd\n\n    bs_ai = 32\n    tr_ai = DataLoader(TabularDS(X_train, y_train), batch_size=bs_ai, shuffle=True)\n    vl_ai = DataLoader(TabularDS(X_val, y_val), batch_size=bs_ai, shuffle=False)\n    te_ai = DataLoader(TabularDS(X_test, y_test), batch_size=bs_ai, shuffle=False)\n\n    inp_dim, out_dim = X_train.shape[1], int(y_train.max() + 1)\n    ai_model = AIModel(inp_dim, 16, out_dim).to(device)\n    crit = nn.CrossEntropyLoss()\n    opt = optim.Adam(ai_model.parameters(), lr=1e-2)\n\n    experiment_data[ds_name] = {\n        \"ai\": {\"train_losses\": [], \"val_losses\": [], \"val_accs\": []}\n    }\n\n    for epoch in range(1, 11):\n        ai_model.train()\n        t_loss = t_corr = t_tot = 0\n        for b in tr_ai:\n            batch = {k: v.to(device) for k, v in b.items()}\n            out = ai_model(batch[\"features\"])\n            loss = crit(out, batch[\"labels\"])\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            t_loss += loss.item() * batch[\"labels\"].size(0)\n            t_corr += (out.argmax(1) == batch[\"labels\"]).sum().item()\n            t_tot += batch[\"labels\"].size(0)\n        train_loss = t_loss / t_tot\n\n        ai_model.eval()\n        v_loss = v_corr = v_tot = 0\n        with torch.no_grad():\n            for b in vl_ai:\n                batch = {k: v.to(device) for k, v in b.items()}\n                out = ai_model(batch[\"features\"])\n                loss = crit(out, batch[\"labels\"])\n                v_loss += loss.item() * batch[\"labels\"].size(0)\n                v_corr += (out.argmax(1) == batch[\"labels\"]).sum().item()\n                v_tot += batch[\"labels\"].size(0)\n        val_loss = v_loss / v_tot\n        val_acc = v_corr / v_tot\n        print(f\"{ds_name} AI Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n        experiment_data[ds_name][\"ai\"][\"train_losses\"].append(train_loss)\n        experiment_data[ds_name][\"ai\"][\"val_losses\"].append(val_loss)\n        experiment_data[ds_name][\"ai\"][\"val_accs\"].append(val_acc)\n\n    ai_model.eval()\n\n    def get_signals(dl):\n        ps, preds = [], []\n        with torch.no_grad():\n            for b in dl:\n                x = b[\"features\"].to(device)\n                out = ai_model(x)\n                p = F.softmax(out, 1).cpu().numpy()\n                ps.append(p)\n                preds.append(p.argmax(1))\n        return np.vstack(ps), np.concatenate(preds)\n\n    ai_p_train, _ = get_signals(tr_ai)\n    ai_p_val, _ = get_signals(vl_ai)\n    ai_p_test, _ = get_signals(te_ai)\n    ai_conf_val = ai_p_val.max(1)\n\n    w0 = ai_model.net[0].weight.data.cpu().numpy()\n    ai_feat_imp = np.linalg.norm(w0, axis=0)\n    ai_feat_imp /= ai_feat_imp.sum()\n\n    Xu_stat_train, Xu_stat_val = X_train, X_val\n    Xu_dual_train = np.concatenate(\n        [X_train, ai_p_train, np.tile(ai_feat_imp, (len(X_train), 1))], 1\n    )\n    Xu_dual_val = np.concatenate(\n        [X_val, ai_p_val, np.tile(ai_feat_imp, (len(X_val), 1))], 1\n    )\n\n    experiment_data[ds_name][\"user\"] = {\n        \"static\": {\"label_accs\": [], \"trust_errors\": [], \"align_scores\": []},\n        \"dual\": {\"label_accs\": [], \"trust_errors\": [], \"align_scores\": []},\n    }\n\n    for mode in (\"static\", \"dual\"):\n        Xtr = Xu_stat_train if mode == \"static\" else Xu_dual_train\n        Xvl = Xu_stat_val if mode == \"static\" else Xu_dual_val\n        y_tr = ai_p_train.argmax(1)\n        y_vl = ai_p_val.argmax(1)\n        tr_dl = DataLoader(TabularDS(Xtr, y_tr), batch_size=32, shuffle=True)\n        vl_dl = DataLoader(TabularDS(Xvl, y_vl), batch_size=32, shuffle=False)\n\n        inp_u = Xtr.shape[1]\n        user = UserModel(inp_u, 8, out_dim).to(device)\n        opt_u = optim.Adam(user.parameters(), lr=1e-2)\n\n        for epoch in range(1, 11):\n            user.train()\n            for b in tr_dl:\n                batch = {k: v.to(device) for k, v in b.items()}\n                out = user(batch[\"features\"])\n                loss = crit(out, batch[\"labels\"])\n                opt_u.zero_grad()\n                loss.backward()\n                opt_u.step()\n\n            user.eval()\n            c = t = 0\n            with torch.no_grad():\n                for b in vl_dl:\n                    batch = {k: v.to(device) for k, v in b.items()}\n                    o = user(batch[\"features\"])\n                    c += (o.argmax(1) == batch[\"labels\"]).sum().item()\n                    t += batch[\"labels\"].size(0)\n            lab_acc = c / t\n\n            with torch.no_grad():\n                up = (\n                    F.softmax(user(torch.from_numpy(Xvl).float().to(device)), 1)\n                    .cpu()\n                    .numpy()\n                )\n            usr_conf = up.max(1)\n            tr_err = np.mean(np.abs(usr_conf - ai_conf_val))\n\n            w_u = user.net[0].weight.data.cpu().numpy()\n            w_u_feat = w_u[:, : len(num_features)] if mode == \"dual\" else w_u\n            usr_imp = np.linalg.norm(w_u_feat, axis=0)\n            usr_imp /= usr_imp.sum()\n            kld = np.sum(usr_imp * np.log((usr_imp) / (ai_feat_imp + 1e-8) + 1e-8))\n            align = 1 - kld / np.log(len(ai_feat_imp))\n\n            exp_u = experiment_data[ds_name][\"user\"][mode]\n            exp_u[\"label_accs\"].append(lab_acc)\n            exp_u[\"trust_errors\"].append(tr_err)\n            exp_u[\"align_scores\"].append(align)\n\n    del ai_model, user\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\nclass AIModel(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, out_dim)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, out_dim)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\n# Dataset specifications\niris_url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\ndataset_configs = [(\"iris\", None), (\"wine-quality\", \"red\"), (\"wine-quality\", \"white\")]\n\nexperiment_data = {}\nfor name, cfg in dataset_configs:\n    ds_key = name if cfg is None else f\"{name}-{cfg}\"\n    # Load dataset\n    if name == \"iris\":\n        ds = load_dataset(\"csv\", data_files={\"train\": iris_url})[\"train\"]\n        label_col = \"species\"\n    else:\n        ds = load_dataset(\"wine-quality\", cfg)[\"train\"]\n        label_col = \"quality\"\n    cols = ds.column_names\n    feat_cols = [c for c in cols if c != label_col]\n    # Extract features/labels\n    X = np.array([[ex[c] for c in feat_cols] for ex in ds])\n    y = np.array([ex[label_col] for ex in ds])\n    # Map string labels for iris\n    if name == \"iris\":\n        uni = sorted(set(y))\n        m = {lab: i for i, lab in enumerate(uni)}\n        y = np.array([m[v] for v in y])\n    else:\n        y = (y >= 6).astype(int)\n    # Split\n    N = len(y)\n    rng = np.random.default_rng(0)\n    idx = rng.permutation(N)\n    n_train = int(0.8 * N)\n    n_val = int(0.1 * N)\n    train_idx = idx[:n_train]\n    val_idx = idx[n_train : n_train + n_val]\n    test_idx = idx[n_train + n_val :]\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_val, y_val = X[val_idx], y[val_idx]\n    X_test, y_test = X[test_idx], y[test_idx]\n    # Normalize\n    mean = X_train.mean(axis=0)\n    std = X_train.std(axis=0) + 1e-6\n    X_train = (X_train - mean) / std\n    X_val = (X_val - mean) / std\n    X_test = (X_test - mean) / std\n\n    num_feat = X_train.shape[1]\n    num_cls = len(np.unique(y_train))\n\n    # Train AI model\n    ai_model = AIModel(num_feat, num_cls).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(ai_model.parameters(), lr=1e-2)\n    tr_loader = DataLoader(SimpleDS(X_train, y_train), batch_size=32, shuffle=True)\n    val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=32)\n    for epoch in range(10):\n        ai_model.train()\n        for batch in tr_loader:\n            x, yb = batch[\"x\"].to(device), batch[\"y\"].to(device)\n            out = ai_model(x)\n            loss = criterion(out, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        ai_model.eval()\n        v_loss = v_corr = v_tot = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                x, yb = batch[\"x\"].to(device), batch[\"y\"].to(device)\n                out = ai_model(x)\n                l = criterion(out, yb)\n                v_loss += l.item() * x.size(0)\n                preds = out.argmax(1)\n                v_corr += (preds == yb).sum().item()\n                v_tot += yb.size(0)\n        val_loss = v_loss / v_tot\n        print(f\"Dataset {ds_key} AI Epoch {epoch+1}: validation_loss = {val_loss:.4f}\")\n\n    # AI feature importance & user inputs\n    with torch.no_grad():\n        w = ai_model.linear.weight.abs().cpu().numpy()\n        f_imp = w.sum(axis=0)\n        p_ai = f_imp / f_imp.sum()\n    ai_model.eval()\n    X_tr_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n    X_vl_t = torch.tensor(X_val, dtype=torch.float32).to(device)\n    p_train = torch.softmax(ai_model(X_tr_t), dim=1).cpu().numpy()\n    p_val = torch.softmax(ai_model(X_vl_t), dim=1).cpu().numpy()\n    f_train = p_train.argmax(axis=1)\n    f_val = p_val.argmax(axis=1)\n    X_usr_train = np.hstack([X_train, p_train])\n    X_usr_val = np.hstack([X_val, p_val])\n\n    # Train User model\n    usr_input = num_feat + num_cls\n    user_model = UserModel(usr_input, num_cls).to(device)\n    optimizer_u = optim.Adam(user_model.parameters(), lr=1e-2)\n    tr_u_loader = DataLoader(UserDS(X_usr_train, f_train), batch_size=32, shuffle=True)\n    val_u_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=32)\n    experiment_data[ds_key] = {\n        \"metrics\": {\n            \"train_loss\": [],\n            \"train_acc\": [],\n            \"val_loss\": [],\n            \"val_acc\": [],\n            \"alignment\": [],\n        }\n    }\n    for epoch in range(15):\n        user_model.train()\n        t_loss = t_corr = t_tot = 0\n        for batch in tr_u_loader:\n            x, yb = batch[\"x\"].to(device), batch[\"y\"].to(device)\n            out = user_model(x)\n            l = criterion(out, yb)\n            optimizer_u.zero_grad()\n            l.backward()\n            optimizer_u.step()\n            t_loss += l.item() * x.size(0)\n            preds = out.argmax(1)\n            t_corr += (preds == yb).sum().item()\n            t_tot += yb.size(0)\n        train_loss = t_loss / t_tot\n        train_acc = t_corr / t_tot\n\n        user_model.eval()\n        v_loss = v_corr = v_tot = 0\n        with torch.no_grad():\n            for batch in val_u_loader:\n                x, yb = batch[\"x\"].to(device), batch[\"y\"].to(device)\n                out = user_model(x)\n                l = criterion(out, yb)\n                v_loss += l.item() * x.size(0)\n                preds = out.argmax(1)\n                v_corr += (preds == yb).sum().item()\n                v_tot += yb.size(0)\n        val_loss = v_loss / v_tot\n        val_acc = v_corr / v_tot\n\n        # Alignment\n        w_u = user_model.linear.weight.abs().cpu().numpy()[:, :num_feat]\n        f_imp_u = w_u.sum(axis=0)\n        p_u = f_imp_u / f_imp_u.sum()\n        eps = 1e-10\n        kl = (p_ai * np.log((p_ai + eps) / (p_u + eps))).sum()\n        norm_kl = kl / np.log(num_feat)\n        align_sc = max(0.0, 1 - norm_kl)\n\n        md = experiment_data[ds_key][\"metrics\"]\n        md[\"train_loss\"].append(train_loss)\n        md[\"train_acc\"].append(train_acc)\n        md[\"val_loss\"].append(val_loss)\n        md[\"val_acc\"].append(val_acc)\n        md[\"alignment\"].append(align_sc)\n        print(\n            f\"Dataset {ds_key} User Epoch {epoch+1}: validation_loss = {val_loss:.4f}, alignment = {align_sc:.4f}\"\n        )\n\n# Save all metrics\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nfrom collections import Counter\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\n\n# setup working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# reproducibility\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n\n# simple dataset wrapper\nclass TextDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\n# classifier and user model\nclass TextClassifier(nn.Module):\n    def __init__(self, d_in, d_hid, d_out):\n        super().__init__()\n        self.fc1 = nn.Linear(d_in, d_hid)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(d_hid, d_out)\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n\n\nclass UserModel(nn.Module):\n    def __init__(self, d_in, d_hid, d_out):\n        super().__init__()\n        self.fc1 = nn.Linear(d_in, d_hid)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(d_hid, d_out)\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n\n\n# build vocab and vectorize\ndef build_vocab(texts, max_features=1000):\n    cnt = Counter()\n    for t in texts:\n        cnt.update(t.lower().split())\n    most = cnt.most_common(max_features)\n    return {w: i for i, (w, _) in enumerate(most)}\n\n\ndef vectorize_texts(texts, vocab):\n    X = np.zeros((len(texts), len(vocab)), dtype=np.float32)\n    for i, t in enumerate(texts):\n        for tok in t.lower().split():\n            j = vocab.get(tok)\n            if j is not None:\n                X[i, j] += 1.0\n    return X\n\n\n# main experiment\nexperiment_data = {}\ndatasets_list = [\"ag_news\", \"yelp_review_polarity\", \"dbpedia_14\"]\nfor ds_name in datasets_list:\n    experiment_data[ds_name] = {\n        \"metrics\": {\"train\": [], \"val\": [], \"alignment_score\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    ds = load_dataset(ds_name)\n    col = \"text\" if \"text\" in ds[\"train\"].column_names else \"content\"\n    split = ds[\"train\"].train_test_split(test_size=0.2, seed=0)\n    train_ds = split[\"train\"].shuffle(seed=0).select(range(2000))\n    val_ds = split[\"test\"].shuffle(seed=0).select(range(500))\n    test_ds = ds[\"test\"].shuffle(seed=0).select(range(500))\n\n    train_text, train_labels = train_ds[col], train_ds[\"label\"]\n    val_text, val_labels = val_ds[col], val_ds[\"label\"]\n    test_text, test_labels = test_ds[col], test_ds[\"label\"]\n\n    vocab = build_vocab(train_text, max_features=1000)\n    X_train = vectorize_texts(train_text, vocab)\n    X_val = vectorize_texts(val_text, vocab)\n    X_test = vectorize_texts(test_text, vocab)\n    y_train = np.array(train_labels)\n    y_val = np.array(val_labels)\n    y_test = np.array(test_labels)\n\n    d_in = X_train.shape[1]\n    d_hid = 64\n    n_cls = len(set(y_train.tolist()))\n\n    ai_model = TextClassifier(d_in, d_hid, n_cls).to(device)\n    optimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n\n    train_loader = DataLoader(\n        TextDataset(X_train, y_train), batch_size=32, shuffle=True\n    )\n    val_loader = DataLoader(TextDataset(X_val, y_val), batch_size=32)\n\n    for epoch in range(5):\n        # train AI model\n        ai_model.train()\n        run_loss = 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            x, yb = batch[\"x\"], batch[\"y\"]\n            x = x / (x.sum(dim=1, keepdim=True) + 1e-10)\n            out = ai_model(x)\n            loss = criterion(out, yb)\n            optimizer_ai.zero_grad()\n            loss.backward()\n            optimizer_ai.step()\n            run_loss += loss.item() * x.size(0)\n        train_loss = run_loss / len(train_loader.dataset)\n\n        # validate AI model\n        ai_model.eval()\n        v_loss = 0.0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                x, yb = batch[\"x\"], batch[\"y\"]\n                x = x / (x.sum(dim=1, keepdim=True) + 1e-10)\n                l = criterion(ai_model(x), yb)\n                v_loss += l.item() * x.size(0)\n        val_loss = v_loss / len(val_loader.dataset)\n        print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n        # prepare user model data\n        with torch.no_grad():\n            ptr = (\n                torch.softmax(\n                    ai_model(\n                        torch.tensor(X_train, dtype=torch.float32).to(device)\n                        / (\n                            torch.tensor(X_train, dtype=torch.float32)\n                            .sum(1, keepdim=True)\n                            .to(device)\n                            + 1e-10\n                        )\n                    ),\n                    dim=1,\n                )\n                .cpu()\n                .numpy()\n            )\n        X_usr_train = np.hstack([X_train, ptr])\n        y_usr_train = ptr.argmax(axis=1)\n\n        # train user model\n        if epoch == 0:\n            user_model = UserModel(d_in + n_cls, 32, n_cls).to(device)\n            optimizer_usr = optim.Adam(user_model.parameters(), lr=1e-3)\n        user_model.train()\n        usr_loader = DataLoader(\n            TextDataset(X_usr_train, y_usr_train), batch_size=32, shuffle=True\n        )\n        for batch in usr_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            feat, lbl = batch[\"x\"], batch[\"y\"]\n            out_u = user_model(feat)\n            lu = criterion(out_u, lbl)\n            optimizer_usr.zero_grad()\n            lu.backward()\n            optimizer_usr.step()\n\n        # compute alignment score\n        ai_imp = torch.mean(torch.abs(ai_model.fc1.weight), dim=0)\n        usr_imp = torch.mean(torch.abs(user_model.fc1.weight), dim=0)\n        dist_ai = ai_imp / (ai_imp.sum() + 1e-10)\n        dist_usr = usr_imp / (usr_imp.sum() + 1e-10)\n        kl = torch.sum(dist_usr * torch.log(dist_usr / (dist_ai + 1e-10))).item()\n        align_score = 1 - kl / np.log(len(dist_ai))\n\n        experiment_data[ds_name][\"metrics\"][\"train\"].append(train_loss)\n        experiment_data[ds_name][\"metrics\"][\"val\"].append(val_loss)\n        experiment_data[ds_name][\"metrics\"][\"alignment_score\"].append(align_score)\n\n    # test AI model\n    ai_model.eval()\n    preds, gts = [], []\n    test_loader = DataLoader(TextDataset(X_test, y_test), batch_size=32)\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            x = batch[\"x\"]\n            x = x / (x.sum(dim=1, keepdim=True) + 1e-10)\n            out = ai_model(x).argmax(dim=1).cpu().numpy()\n            preds.extend(out.tolist())\n            gts.extend(batch[\"y\"].cpu().numpy().tolist())\n    experiment_data[ds_name][\"predictions\"] = np.array(preds)\n    experiment_data[ds_name][\"ground_truth\"] = np.array(gts)\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import Counter\nfrom datasets import load_dataset\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# reproducibility\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n\nclass TextDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\nclass TextClassifier(nn.Module):\n    def __init__(self, d_in, d_hid, d_out):\n        super().__init__()\n        self.fc1 = nn.Linear(d_in, d_hid)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(d_hid, d_out)\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n\n\nclass UserModel(nn.Module):\n    def __init__(self, d_in, d_hid, d_out):\n        super().__init__()\n        self.fc1 = nn.Linear(d_in, d_hid)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(d_hid, d_out)\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n\n\ndef build_vocab(texts, max_features=1000):\n    cnt = Counter()\n    for t in texts:\n        cnt.update(t.lower().split())\n    most = cnt.most_common(max_features)\n    return {w: i for i, (w, _) in enumerate(most)}\n\n\ndef vectorize_texts(texts, vocab):\n    X = np.zeros((len(texts), len(vocab)), dtype=np.float32)\n    for i, t in enumerate(texts):\n        for tok in t.lower().split():\n            j = vocab.get(tok)\n            if j is not None:\n                X[i, j] += 1.0\n    return X\n\n\nexperiment_data = {}\ndatasets_list = [\"ag_news\", \"yelp_review_polarity\", \"dbpedia_14\"]\nfor ds_name in datasets_list:\n    experiment_data[ds_name] = {\n        \"metrics\": {\"train\": [], \"val\": [], \"alignment_score\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    ds = load_dataset(ds_name)\n    col = \"text\" if \"text\" in ds[\"train\"].column_names else \"content\"\n    split = ds[\"train\"].train_test_split(test_size=0.2, seed=0)\n    train_ds = split[\"train\"].shuffle(seed=0).select(range(2000))\n    val_ds = split[\"test\"].shuffle(seed=0).select(range(500))\n    test_ds = ds[\"test\"].shuffle(seed=0).select(range(500))\n\n    train_text, train_labels = train_ds[col], train_ds[\"label\"]\n    val_text, val_labels = val_ds[col], val_ds[\"label\"]\n    test_text, test_labels = test_ds[col], test_ds[\"label\"]\n\n    vocab = build_vocab(train_text, max_features=1000)\n    X_train = vectorize_texts(train_text, vocab)\n    X_val = vectorize_texts(val_text, vocab)\n    X_test = vectorize_texts(test_text, vocab)\n    y_train = np.array(train_labels)\n    y_val = np.array(val_labels)\n    y_test = np.array(test_labels)\n\n    d_in, d_hid = X_train.shape[1], 64\n    n_cls = len(set(y_train.tolist()))\n\n    ai_model = TextClassifier(d_in, d_hid, n_cls).to(device)\n    optimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n\n    user_model = UserModel(d_in + n_cls, 32, n_cls).to(device)\n    optimizer_usr = optim.Adam(user_model.parameters(), lr=1e-3)\n\n    train_loader = DataLoader(\n        TextDataset(X_train, y_train), batch_size=32, shuffle=True\n    )\n    val_loader = DataLoader(TextDataset(X_val, y_val), batch_size=32)\n\n    for epoch in range(5):\n        # train AI model\n        ai_model.train()\n        run_loss = 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            x, yb = batch[\"x\"], batch[\"y\"]\n            x = x / (x.sum(dim=1, keepdim=True) + 1e-10)\n            out = ai_model(x)\n            loss = criterion(out, yb)\n            optimizer_ai.zero_grad()\n            loss.backward()\n            optimizer_ai.step()\n            run_loss += loss.item() * x.size(0)\n        train_loss = run_loss / len(train_loader.dataset)\n\n        # validate AI model\n        ai_model.eval()\n        v_loss = 0.0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                x, yb = batch[\"x\"], batch[\"y\"]\n                x = x / (x.sum(dim=1, keepdim=True) + 1e-10)\n                v_loss += criterion(ai_model(x), yb).item() * x.size(0)\n        val_loss = v_loss / len(val_loader.dataset)\n        print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n        # prepare user-model inputs\n        X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n        norm = X_train_t.sum(dim=1, keepdim=True) + 1e-10\n        with torch.no_grad():\n            ptr = torch.softmax(ai_model(X_train_t / norm), dim=1).cpu().numpy()\n        X_usr_train = np.hstack([X_train, ptr])\n        y_usr_train = ptr.argmax(axis=1)\n\n        # train user model\n        user_model.train()\n        usr_loader = DataLoader(\n            TextDataset(X_usr_train, y_usr_train), batch_size=32, shuffle=True\n        )\n        for batch in usr_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            feat, lbl = batch[\"x\"], batch[\"y\"]\n            out_u = user_model(feat)\n            lu = criterion(out_u, lbl)\n            optimizer_usr.zero_grad()\n            lu.backward()\n            optimizer_usr.step()\n\n        # compute co-adaptive alignment (slice to d_in)\n        with torch.no_grad():\n            ai_imp = torch.mean(torch.abs(ai_model.fc1.weight), dim=0)\n            usr_imp = torch.mean(torch.abs(user_model.fc1.weight[:, :d_in]), dim=0)\n            dist_ai = ai_imp / (ai_imp.sum() + 1e-10)\n            dist_usr = usr_imp / (usr_imp.sum() + 1e-10)\n            kl = torch.sum(dist_usr * torch.log(dist_usr / (dist_ai + 1e-10))).item()\n            align_score = 1 - kl / np.log(len(dist_ai))\n\n        experiment_data[ds_name][\"metrics\"][\"train\"].append((epoch, train_loss))\n        experiment_data[ds_name][\"metrics\"][\"val\"].append((epoch, val_loss))\n        experiment_data[ds_name][\"metrics\"][\"alignment_score\"].append(\n            (epoch, align_score)\n        )\n\n    # test AI model\n    ai_model.eval()\n    preds, gts = [], []\n    test_loader = DataLoader(TextDataset(X_test, y_test), batch_size=32)\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            x = batch[\"x\"]\n            x = x / (x.sum(dim=1, keepdim=True) + 1e-10)\n            out = ai_model(x).argmax(dim=1).cpu().numpy()\n            preds.extend(out.tolist())\n            gts.extend(batch[\"y\"].cpu().numpy().tolist())\n    experiment_data[ds_name][\"predictions\"] = np.array(preds)\n    experiment_data[ds_name][\"ground_truth\"] = np.array(gts)\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nfrom datasets import load_dataset\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Dataset wrapper\nclass TabularDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\n# Simple MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Load three HuggingFace tabular tasks\nhf_tasks = {\n    \"iris\": load_dataset(\"iris\"),\n    \"wine_white\": load_dataset(\"wine-quality\", \"white\"),\n    \"wine_red\": load_dataset(\"wine-quality\", \"red\"),\n}\n\nexperiment_data = {}\n\nfor name, raw in hf_tasks.items():\n    # create train/val/test splits\n    ds = raw[\"train\"] if isinstance(raw, dict) and \"train\" in raw else raw\n    split1 = ds.train_test_split(test_size=0.3, seed=42)\n    train_val = split1[\"train\"].train_test_split(test_size=0.1, seed=42)\n    train_ds, val_ds, test_ds = train_val[\"train\"], train_val[\"test\"], split1[\"test\"]\n\n    # identify features and label\n    label_col = train_ds.column_names[-1]\n    feat_cols = train_ds.column_names[:-1]\n\n    # convert to pandas and numpy\n    df_train, df_val, df_test = (\n        train_ds.to_pandas(),\n        val_ds.to_pandas(),\n        test_ds.to_pandas(),\n    )\n    X_train, X_val, X_test = (\n        df_train[feat_cols].values,\n        df_val[feat_cols].values,\n        df_test[feat_cols].values,\n    )\n    y_train, y_val, y_test = (\n        df_train[label_col].values,\n        df_val[label_col].values,\n        df_test[label_col].values,\n    )\n\n    # manual standard scaling\n    feat_mean = X_train.mean(axis=0)\n    feat_std = X_train.std(axis=0) + 1e-6\n    X_train = (X_train - feat_mean) / feat_std\n    X_val = (X_val - feat_mean) / feat_std\n    X_test = (X_test - feat_mean) / feat_std\n\n    inp_dim = X_train.shape[1]\n    num_classes = len(np.unique(y_train))\n\n    # AI model training\n    AI = MLP(inp_dim, 16, num_classes).to(device)\n    opt_ai = optim.Adam(AI.parameters(), lr=1e-2)\n    crit = nn.CrossEntropyLoss()\n    ai_tr_losses, ai_val_losses, ai_tr_accs, ai_val_accs = [], [], [], []\n\n    for epoch in range(10):\n        AI.train()\n        tr_loss = tr_corr = tr_tot = 0\n        for batch in DataLoader(\n            TabularDataset(X_train, y_train), batch_size=32, shuffle=True\n        ):\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            out = AI(batch[\"x\"])\n            loss = crit(out, batch[\"y\"])\n            opt_ai.zero_grad()\n            loss.backward()\n            opt_ai.step()\n            tr_loss += loss.item() * batch[\"y\"].size(0)\n            preds = out.argmax(1)\n            tr_corr += (preds == batch[\"y\"]).sum().item()\n            tr_tot += batch[\"y\"].size(0)\n        ai_tr_losses.append(tr_loss / tr_tot)\n        ai_tr_accs.append(tr_corr / tr_tot)\n\n        AI.eval()\n        v_loss = v_corr = v_tot = 0\n        with torch.no_grad():\n            for batch in DataLoader(TabularDataset(X_val, y_val), batch_size=64):\n                batch = {\n                    k: v.to(device)\n                    for k, v in batch.items()\n                    if isinstance(v, torch.Tensor)\n                }\n                out = AI(batch[\"x\"])\n                loss = crit(out, batch[\"y\"])\n                v_loss += loss.item() * batch[\"y\"].size(0)\n                preds = out.argmax(1)\n                v_corr += (preds == batch[\"y\"]).sum().item()\n                v_tot += batch[\"y\"].size(0)\n        val_loss = v_loss / v_tot\n        ai_val_losses.append(val_loss)\n        ai_val_accs.append(v_corr / v_tot)\n        print(f\"[{name}][AI] Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n    # compute true feature importance distribution\n    wi = AI.net[0].weight.detach().abs().sum(0)\n    true_dist = wi / wi.sum()\n    true_dist_np = true_dist.cpu().numpy()\n\n    # gather AI outputs for user-training input\n    AI.eval()\n    with torch.no_grad():\n        X_all = np.vstack([X_train, X_val, X_test])\n        X_t = torch.from_numpy(X_all).float().to(device)\n        logits = AI(X_t)\n        probs = F.softmax(logits, 1).cpu().numpy()\n        preds_all = logits.argmax(1).cpu().numpy()\n\n    p_train = probs[: len(X_train)]\n    p_val = probs[len(X_train) : len(X_train) + len(X_val)]\n    p_test = probs[-len(X_test) :]\n    f_train = preds_all[: len(X_train)]\n    f_val = preds_all[len(X_train) : len(X_train) + len(X_val)]\n    f_test = preds_all[-len(X_test) :]\n\n    X_usr_train = np.hstack([X_train, p_train])\n    X_usr_val = np.hstack([X_val, p_val])\n    X_usr_test = np.hstack([X_test, p_test])\n\n    # User-model training (to simulate human bias alignment)\n    user = MLP(inp_dim + num_classes, 16, num_classes).to(device)\n    opt_usr = optim.Adam(user.parameters(), lr=1e-2)\n    usr_tr_losses, usr_val_losses, usr_tr_accs, usr_val_accs, align_scores = (\n        [],\n        [],\n        [],\n        [],\n        [],\n    )\n\n    for epoch in range(10):\n        user.train()\n        t_loss = t_corr = t_tot = 0\n        for batch in DataLoader(\n            TabularDataset(X_usr_train, f_train), batch_size=32, shuffle=True\n        ):\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            out = user(batch[\"x\"])\n            loss = crit(out, batch[\"y\"])\n            opt_usr.zero_grad()\n            loss.backward()\n            opt_usr.step()\n            t_loss += loss.item() * batch[\"y\"].size(0)\n            p = out.argmax(1)\n            t_corr += (p == batch[\"y\"]).sum().item()\n            t_tot += batch[\"y\"].size(0)\n        usr_tr_losses.append(t_loss / t_tot)\n        usr_tr_accs.append(t_corr / t_tot)\n\n        user.eval()\n        v_loss = v_corr = v_tot = 0\n        with torch.no_grad():\n            for batch in DataLoader(TabularDataset(X_usr_val, f_val), batch_size=64):\n                batch = {\n                    k: v.to(device)\n                    for k, v in batch.items()\n                    if isinstance(v, torch.Tensor)\n                }\n                out = user(batch[\"x\"])\n                loss = crit(out, batch[\"y\"])\n                v_loss += loss.item() * batch[\"y\"].size(0)\n                p = out.argmax(1)\n                v_corr += (p == batch[\"y\"]).sum().item()\n                v_tot += batch[\"y\"].size(0)\n        val_loss = v_loss / v_tot\n        usr_val_losses.append(val_loss)\n        usr_val_accs.append(v_corr / v_tot)\n\n        # feature-alignment score (1 - normalized KL)\n        w_u = user.net[0].weight.detach().abs()[:, :inp_dim].sum(0)\n        u_dist = w_u / w_u.sum()\n        kl = (true_dist * torch.log(true_dist / u_dist)).sum()\n        norm_kl = kl / np.log(inp_dim)\n        align_scores.append((1 - norm_kl).item())\n        print(f\"[{name}][User] Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n    # test user-model\n    preds_test, gt_test = [], []\n    user.eval()\n    with torch.no_grad():\n        for batch in DataLoader(TabularDataset(X_usr_test, f_test), batch_size=64):\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            out = user(batch[\"x\"])\n            p = out.argmax(1).cpu().numpy()\n            preds_test.extend(p.tolist())\n            gt_test.extend(batch[\"y\"].cpu().numpy().tolist())\n\n    # collect experiment data\n    experiment_data[name] = {\n        \"ai\": {\n            \"losses\": {\"train\": np.array(ai_tr_losses), \"val\": np.array(ai_val_losses)},\n            \"metrics\": {\n                \"train_acc\": np.array(ai_tr_accs),\n                \"val_acc\": np.array(ai_val_accs),\n            },\n            \"feature_importance\": true_dist_np,\n        },\n        \"user\": {\n            \"losses\": {\n                \"train\": np.array(usr_tr_losses),\n                \"val\": np.array(usr_val_losses),\n            },\n            \"metrics\": {\n                \"train_acc\": np.array(usr_tr_accs),\n                \"val_acc\": np.array(usr_val_accs),\n            },\n            \"alignment_score\": np.array(align_scores),\n        },\n        \"predictions\": np.array(preds_test),\n        \"ground_truth\": np.array(gt_test),\n    }\n\n# save all metrics and data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n\n\nclass AIModel(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, out_dim)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, out_dim)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\n# Dataset specs\niris_url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\ndataset_configs = [(\"iris\", None), (\"wine-quality\", \"red\"), (\"wine-quality\", \"white\")]\n\nexperiment_data = {}\nfor name, cfg in dataset_configs:\n    ds_key = name if cfg is None else f\"{name}-{cfg}\"\n    if name == \"iris\":\n        ds = load_dataset(\"csv\", data_files={\"train\": iris_url})[\"train\"]\n        label_col = \"species\"\n    else:\n        ds = load_dataset(\"wine-quality\", cfg)[\"train\"]\n        label_col = \"quality\"\n    feat_cols = [c for c in ds.column_names if c != label_col]\n    X = np.array([[ex[c] for c in feat_cols] for ex in ds])\n    y = np.array([ex[label_col] for ex in ds])\n    if name == \"iris\":\n        m = {lab: i for i, lab in enumerate(sorted(set(y)))}\n        y = np.array([m[v] for v in y])\n    else:\n        y = (y >= 6).astype(int)\n    N = len(y)\n    rng = np.random.default_rng(0)\n    idx = rng.permutation(N)\n    n_train = int(0.8 * N)\n    n_val = int(0.1 * N)\n    train_idx = idx[:n_train]\n    val_idx = idx[n_train : n_train + n_val]\n    test_idx = idx[n_train + n_val :]\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_val, y_val = X[val_idx], y[val_idx]\n    X_test, y_test = X[test_idx], y[test_idx]\n    mean, std = X_train.mean(0), X_train.std(0) + 1e-6\n    X_train = (X_train - mean) / std\n    X_val = (X_val - mean) / std\n    X_test = (X_test - mean) / std\n\n    num_feat = X_train.shape[1]\n    num_cls = len(np.unique(y_train))\n\n    # Train AI model\n    ai_model = AIModel(num_feat, num_cls).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(ai_model.parameters(), lr=1e-2)\n    tr_loader = DataLoader(SimpleDS(X_train, y_train), batch_size=32, shuffle=True)\n    val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=32)\n    for epoch in range(10):\n        ai_model.train()\n        for batch in tr_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = ai_model(batch[\"x\"])\n            loss = criterion(out, batch[\"y\"])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        ai_model.eval()\n        v_loss = v_corr = v_tot = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                out = ai_model(batch[\"x\"])\n                l = criterion(out, batch[\"y\"])\n                v_loss += l.item() * batch[\"x\"].size(0)\n                v_corr += (out.argmax(1) == batch[\"y\"]).sum().item()\n                v_tot += batch[\"x\"].size(0)\n        print(\n            f\"Dataset {ds_key} AI Epoch {epoch+1}: validation_loss = {v_loss/v_tot:.4f}\"\n        )\n\n    # Feature importance & user inputs\n    with torch.no_grad():\n        w = ai_model.linear.weight.abs().cpu().numpy()\n        p_ai = w.sum(0)\n        p_ai = p_ai / p_ai.sum()\n    ai_model.eval()\n    X_tr_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n    X_vl_t = torch.tensor(X_val, dtype=torch.float32).to(device)\n    with torch.no_grad():\n        p_train = torch.softmax(ai_model(X_tr_t), dim=1).cpu().numpy()\n        p_val = torch.softmax(ai_model(X_vl_t), dim=1).cpu().numpy()\n    f_train = p_train.argmax(1)\n    f_val = p_val.argmax(1)\n    X_usr_train = np.hstack([X_train, p_train])\n    X_usr_val = np.hstack([X_val, p_val])\n\n    # Train User model\n    usr_input = num_feat + num_cls\n    user_model = UserModel(usr_input, num_cls).to(device)\n    optimizer_u = optim.Adam(user_model.parameters(), lr=1e-2)\n    tr_u_loader = DataLoader(UserDS(X_usr_train, f_train), batch_size=32, shuffle=True)\n    val_u_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=32)\n    experiment_data[ds_key] = {\n        \"metrics\": {\n            \"train_loss\": [],\n            \"train_acc\": [],\n            \"val_loss\": [],\n            \"val_acc\": [],\n            \"alignment\": [],\n        }\n    }\n    for epoch in range(15):\n        user_model.train()\n        t_loss = t_corr = t_tot = 0\n        for batch in tr_u_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = user_model(batch[\"x\"])\n            l = criterion(out, batch[\"y\"])\n            optimizer_u.zero_grad()\n            l.backward()\n            optimizer_u.step()\n            t_loss += l.item() * batch[\"x\"].size(0)\n            t_corr += (out.argmax(1) == batch[\"y\"]).sum().item()\n            t_tot += batch[\"x\"].size(0)\n        train_loss = t_loss / t_tot\n        train_acc = t_corr / t_tot\n\n        user_model.eval()\n        v_loss = v_corr = v_tot = 0\n        with torch.no_grad():\n            for batch in val_u_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                out = user_model(batch[\"x\"])\n                l = criterion(out, batch[\"y\"])\n                v_loss += l.item() * batch[\"x\"].size(0)\n                v_corr += (out.argmax(1) == batch[\"y\"]).sum().item()\n                v_tot += batch[\"x\"].size(0)\n        val_loss = v_loss / v_tot\n        val_acc = v_corr / v_tot\n\n        # Alignment\n        w_u = user_model.linear.weight.abs().cpu().numpy()[:, :num_feat]\n        p_u = w_u.sum(0)\n        p_u = p_u / p_u.sum()\n        eps = 1e-10\n        kl = (p_ai * np.log((p_ai + eps) / (p_u + eps))).sum()\n        norm_kl = kl / np.log(num_feat)\n        align_sc = max(0.0, 1 - norm_kl)\n\n        md = experiment_data[ds_key][\"metrics\"]\n        md[\"train_loss\"].append(train_loss)\n        md[\"train_acc\"].append(train_acc)\n        md[\"val_loss\"].append(val_loss)\n        md[\"val_acc\"].append(val_acc)\n        md[\"alignment\"].append(align_sc)\n        print(\n            f\"Dataset {ds_key} User Epoch {epoch+1}: validation_loss = {val_loss:.4f}, alignment = {align_sc:.4f}\"\n        )\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic dataset generation\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Model definitions\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameter grid\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\n\n# Container for all results\nexperiment_data = {\"batch_size\": {}}\n\nfor ai_bs in ai_batch_sizes:\n    # AI data loaders\n    ai_tr_loader = DataLoader(\n        SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True\n    )\n    ai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n\n    # Initialize AI model\n    ai_model = AIModel(D, 16, 2).to(device)\n    criterion_ai = nn.CrossEntropyLoss()\n    optimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n\n    # Train AI model\n    for _ in range(15):\n        ai_model.train()\n        for batch in ai_tr_loader:\n            x = batch[\"x\"].to(device)\n            yb = batch[\"y\"].to(device)\n            out = ai_model(x)\n            loss = criterion_ai(out, yb)\n            optimizer_ai.zero_grad()\n            loss.backward()\n            optimizer_ai.step()\n\n    # Generate AI probabilities\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n    p_train = probs_all[: len(X_train)]\n    p_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\n    p_test = probs_all[-len(X_test) :]\n    f_train = p_train.argmax(axis=1)\n    f_val = p_val.argmax(axis=1)\n    f_test = p_test.argmax(axis=1)\n\n    # Prepare user features\n    X_usr_train = np.hstack([X_train, p_train])\n    X_usr_val = np.hstack([X_val, p_val])\n    X_usr_test = np.hstack([X_test, p_test])\n\n    for usr_bs in usr_batch_sizes:\n        # User data loaders\n        usr_tr_loader = DataLoader(\n            UserDS(X_usr_train, f_train), batch_size=usr_bs, shuffle=True\n        )\n        usr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=usr_bs)\n        usr_test_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=usr_bs)\n\n        # Initialize User model\n        user_model = UserModel(D + 2, 8, 2).to(device)\n        criterion_usr = nn.CrossEntropyLoss()\n        optimizer_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n\n        train_accs, val_accs = [], []\n        train_losses, val_losses = [], []\n\n        # Train User model\n        for _ in range(20):\n            user_model.train()\n            t_loss, corr, tot = 0.0, 0, 0\n            for batch in usr_tr_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                loss = criterion_usr(out, lbl)\n                optimizer_usr.zero_grad()\n                loss.backward()\n                optimizer_usr.step()\n                t_loss += loss.item() * feat.size(0)\n                preds = out.argmax(dim=1)\n                corr += (preds == lbl).sum().item()\n                tot += lbl.size(0)\n            train_losses.append(t_loss / tot)\n            train_accs.append(corr / tot)\n\n            user_model.eval()\n            v_loss, v_corr, v_tot = 0.0, 0, 0\n            with torch.no_grad():\n                for batch in usr_val_loader:\n                    feat = batch[\"feat\"].to(device)\n                    lbl = batch[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = criterion_usr(out, lbl)\n                    v_loss += loss.item() * feat.size(0)\n                    preds = out.argmax(dim=1)\n                    v_corr += (preds == lbl).sum().item()\n                    v_tot += lbl.size(0)\n            val_losses.append(v_loss / v_tot)\n            val_accs.append(v_corr / v_tot)\n\n        # Test evaluation\n        test_preds, test_gt = [], []\n        user_model.eval()\n        with torch.no_grad():\n            for batch in usr_test_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                p = out.argmax(dim=1).cpu().numpy()\n                test_preds.extend(p.tolist())\n                test_gt.extend(lbl.cpu().numpy().tolist())\n\n        key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n        experiment_data[\"batch_size\"][key] = {\n            \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n            \"losses\": {\"train\": np.array(train_losses), \"val\": np.array(val_losses)},\n            \"predictions\": np.array(test_preds),\n            \"ground_truth\": np.array(test_gt),\n        }\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic dataset generation\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Model definitions\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameter grid\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\n\n# Container for all results\nexperiment_data = {\"batch_size\": {}}\n\nfor ai_bs in ai_batch_sizes:\n    # AI data loaders\n    ai_tr_loader = DataLoader(\n        SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True\n    )\n    ai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n\n    # Initialize AI model\n    ai_model = AIModel(D, 16, 2).to(device)\n    criterion_ai = nn.CrossEntropyLoss()\n    optimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n\n    # Train AI model\n    for _ in range(15):\n        ai_model.train()\n        for batch in ai_tr_loader:\n            x = batch[\"x\"].to(device)\n            yb = batch[\"y\"].to(device)\n            out = ai_model(x)\n            loss = criterion_ai(out, yb)\n            optimizer_ai.zero_grad()\n            loss.backward()\n            optimizer_ai.step()\n\n    # Generate AI probabilities\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n    p_train = probs_all[: len(X_train)]\n    p_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\n    p_test = probs_all[-len(X_test) :]\n    f_train = p_train.argmax(axis=1)\n    f_val = p_val.argmax(axis=1)\n    f_test = p_test.argmax(axis=1)\n\n    # Prepare user features\n    X_usr_train = np.hstack([X_train, p_train])\n    X_usr_val = np.hstack([X_val, p_val])\n    X_usr_test = np.hstack([X_test, p_test])\n\n    for usr_bs in usr_batch_sizes:\n        # User data loaders\n        usr_tr_loader = DataLoader(\n            UserDS(X_usr_train, f_train), batch_size=usr_bs, shuffle=True\n        )\n        usr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=usr_bs)\n        usr_test_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=usr_bs)\n\n        # Initialize User model\n        user_model = UserModel(D + 2, 8, 2).to(device)\n        criterion_usr = nn.CrossEntropyLoss()\n        optimizer_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n\n        train_accs, val_accs = [], []\n        train_losses, val_losses = [], []\n\n        # Train User model\n        for _ in range(20):\n            user_model.train()\n            t_loss, corr, tot = 0.0, 0, 0\n            for batch in usr_tr_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                loss = criterion_usr(out, lbl)\n                optimizer_usr.zero_grad()\n                loss.backward()\n                optimizer_usr.step()\n                t_loss += loss.item() * feat.size(0)\n                preds = out.argmax(dim=1)\n                corr += (preds == lbl).sum().item()\n                tot += lbl.size(0)\n            train_losses.append(t_loss / tot)\n            train_accs.append(corr / tot)\n\n            user_model.eval()\n            v_loss, v_corr, v_tot = 0.0, 0, 0\n            with torch.no_grad():\n                for batch in usr_val_loader:\n                    feat = batch[\"feat\"].to(device)\n                    lbl = batch[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = criterion_usr(out, lbl)\n                    v_loss += loss.item() * feat.size(0)\n                    preds = out.argmax(dim=1)\n                    v_corr += (preds == lbl).sum().item()\n                    v_tot += lbl.size(0)\n            val_losses.append(v_loss / v_tot)\n            val_accs.append(v_corr / v_tot)\n\n        # Test evaluation\n        test_preds, test_gt = [], []\n        user_model.eval()\n        with torch.no_grad():\n            for batch in usr_test_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                p = out.argmax(dim=1).cpu().numpy()\n                test_preds.extend(p.tolist())\n                test_gt.extend(lbl.cpu().numpy().tolist())\n\n        key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n        experiment_data[\"batch_size\"][key] = {\n            \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n            \"losses\": {\"train\": np.array(train_losses), \"val\": np.array(val_losses)},\n            \"predictions\": np.array(test_preds),\n            \"ground_truth\": np.array(test_gt),\n        }\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic dataset generation\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Model definitions\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameter grid\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\n\n# Container for all results\nexperiment_data = {\"batch_size\": {}}\n\nfor ai_bs in ai_batch_sizes:\n    # AI data loaders\n    ai_tr_loader = DataLoader(\n        SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True\n    )\n    ai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n\n    # Initialize AI model\n    ai_model = AIModel(D, 16, 2).to(device)\n    criterion_ai = nn.CrossEntropyLoss()\n    optimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n\n    # Train AI model\n    for _ in range(15):\n        ai_model.train()\n        for batch in ai_tr_loader:\n            x = batch[\"x\"].to(device)\n            yb = batch[\"y\"].to(device)\n            out = ai_model(x)\n            loss = criterion_ai(out, yb)\n            optimizer_ai.zero_grad()\n            loss.backward()\n            optimizer_ai.step()\n\n    # Generate AI probabilities\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n    p_train = probs_all[: len(X_train)]\n    p_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\n    p_test = probs_all[-len(X_test) :]\n    f_train = p_train.argmax(axis=1)\n    f_val = p_val.argmax(axis=1)\n    f_test = p_test.argmax(axis=1)\n\n    # Prepare user features\n    X_usr_train = np.hstack([X_train, p_train])\n    X_usr_val = np.hstack([X_val, p_val])\n    X_usr_test = np.hstack([X_test, p_test])\n\n    for usr_bs in usr_batch_sizes:\n        # User data loaders\n        usr_tr_loader = DataLoader(\n            UserDS(X_usr_train, f_train), batch_size=usr_bs, shuffle=True\n        )\n        usr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=usr_bs)\n        usr_test_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=usr_bs)\n\n        # Initialize User model\n        user_model = UserModel(D + 2, 8, 2).to(device)\n        criterion_usr = nn.CrossEntropyLoss()\n        optimizer_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n\n        train_accs, val_accs = [], []\n        train_losses, val_losses = [], []\n\n        # Train User model\n        for _ in range(20):\n            user_model.train()\n            t_loss, corr, tot = 0.0, 0, 0\n            for batch in usr_tr_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                loss = criterion_usr(out, lbl)\n                optimizer_usr.zero_grad()\n                loss.backward()\n                optimizer_usr.step()\n                t_loss += loss.item() * feat.size(0)\n                preds = out.argmax(dim=1)\n                corr += (preds == lbl).sum().item()\n                tot += lbl.size(0)\n            train_losses.append(t_loss / tot)\n            train_accs.append(corr / tot)\n\n            user_model.eval()\n            v_loss, v_corr, v_tot = 0.0, 0, 0\n            with torch.no_grad():\n                for batch in usr_val_loader:\n                    feat = batch[\"feat\"].to(device)\n                    lbl = batch[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = criterion_usr(out, lbl)\n                    v_loss += loss.item() * feat.size(0)\n                    preds = out.argmax(dim=1)\n                    v_corr += (preds == lbl).sum().item()\n                    v_tot += lbl.size(0)\n            val_losses.append(v_loss / v_tot)\n            val_accs.append(v_corr / v_tot)\n\n        # Test evaluation\n        test_preds, test_gt = [], []\n        user_model.eval()\n        with torch.no_grad():\n            for batch in usr_test_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                p = out.argmax(dim=1).cpu().numpy()\n                test_preds.extend(p.tolist())\n                test_gt.extend(lbl.cpu().numpy().tolist())\n\n        key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n        experiment_data[\"batch_size\"][key] = {\n            \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n            \"losses\": {\"train\": np.array(train_losses), \"val\": np.array(val_losses)},\n            \"predictions\": np.array(test_preds),\n            \"ground_truth\": np.array(test_gt),\n        }\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Execution time: 15 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 67, in <module>\\n    ds = load_dataset(name)[\"train\"]\\n\n^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1652, in dataset_module_factory\\n    raise e1\nfrom None\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1578, in dataset_module_factory\\n    raise\nDatasetNotFoundError(f\"Dataset \\'{path}\\' doesn\\'t exist on the Hub or cannot be\naccessed.\") from e\\ndatasets.exceptions.DatasetNotFoundError: Dataset \\'iris\\'\ndoesn\\'t exist on the Hub or cannot be accessed.\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 12, in\n<module>\\n    from sklearn.preprocessing import\nStandardScaler\\nModuleNotFoundError: No module named \\'sklearn\\'\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 55, in <module>\\n    raw = load_dataset(ds_name, cfg) if cfg\nis not None else load_dataset(ds_name)\\n\n^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1652, in dataset_module_factory\\n    raise e1\nfrom None\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1578, in dataset_module_factory\\n    raise\nDatasetNotFoundError(f\"Dataset \\'{path}\\' doesn\\'t exist on the Hub or cannot be\naccessed.\") from e\\ndatasets.exceptions.DatasetNotFoundError: Dataset \\'iris\\'\ndoesn\\'t exist on the Hub or cannot be accessed.\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 57, in <module>\\n    raw = load_dataset(ds_name, cfg) if cfg\nis not None else load_dataset(ds_name)\\n\n^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1652, in dataset_module_factory\\n    raise e1\nfrom None\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1578, in dataset_module_factory\\n    raise\nDatasetNotFoundError(f\"Dataset \\'{path}\\' doesn\\'t exist on the Hub or cannot be\naccessed.\") from e\\ndatasets.exceptions.DatasetNotFoundError: Dataset\n\\'uci_iris\\' doesn\\'t exist on the Hub or cannot be accessed.\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 12, in\n<module>\\n    from sklearn.feature_extraction.text import\nCountVectorizer\\nModuleNotFoundError: No module named \\'sklearn\\'\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Dataset: ag_news Method: static Epoch 0:\nvalidation_loss = 1.0334', '\\n', 'Dataset: ag_news Method: static Epoch 1:\nvalidation_loss = 1.0506', '\\n', 'Dataset: ag_news Method: static Epoch 2:\nvalidation_loss = 1.0396', '\\n', 'Dataset: ag_news Method: static Epoch 3:\nvalidation_loss = 1.0632', '\\n', 'Dataset: ag_news Method: static Epoch 4:\nvalidation_loss = 1.0602', '\\n', 'Dataset: ag_news Method: single Epoch 0:\nvalidation_loss = 1.0348', '\\n', 'Dataset: ag_news Method: single Epoch 1:\nvalidation_loss = 1.0259', '\\n', 'Dataset: ag_news Method: single Epoch 2:\nvalidation_loss = 1.0628', '\\n', 'Dataset: ag_news Method: single Epoch 3:\nvalidation_loss = 1.0635', '\\n', 'Dataset: ag_news Method: single Epoch 4:\nvalidation_loss = 1.0637', '\\n', 'Dataset: ag_news Method: dual Epoch 0:\nvalidation_loss = 1.0436', '\\n', 'Dataset: ag_news Method: dual Epoch 1:\nvalidation_loss = 1.0390', '\\n', 'Dataset: ag_news Method: dual Epoch 2:\nvalidation_loss = 1.0495', '\\n', 'Dataset: ag_news Method: dual Epoch 3:\nvalidation_loss = 1.0535', '\\n', 'Dataset: ag_news Method: dual Epoch 4:\nvalidation_loss = 1.0615', '\\n', 'Dataset: imdb Method: static Epoch 0:\nvalidation_loss = 0.6081', '\\n', 'Dataset: imdb Method: static Epoch 1:\nvalidation_loss = 0.6111', '\\n', 'Dataset: imdb Method: static Epoch 2:\nvalidation_loss = 0.6046', '\\n', 'Dataset: imdb Method: static Epoch 3:\nvalidation_loss = 0.6031', '\\n', 'Dataset: imdb Method: static Epoch 4:\nvalidation_loss = 0.5976', '\\n', 'Dataset: imdb Method: single Epoch 0:\nvalidation_loss = 0.5959', '\\n', 'Dataset: imdb Method: single Epoch 1:\nvalidation_loss = 0.6092', '\\n', 'Dataset: imdb Method: single Epoch 2:\nvalidation_loss = 0.6455', '\\n', 'Dataset: imdb Method: single Epoch 3:\nvalidation_loss = 0.6159', '\\n', 'Dataset: imdb Method: single Epoch 4:\nvalidation_loss = 0.6183', '\\n', 'Dataset: imdb Method: dual Epoch 0:\nvalidation_loss = 0.6127', '\\n', 'Dataset: imdb Method: dual Epoch 1:\nvalidation_loss = 0.6100', '\\n', 'Dataset: imdb Method: dual Epoch 2:\nvalidation_loss = 0.6299', '\\n', 'Dataset: imdb Method: dual Epoch 3:\nvalidation_loss = 0.6070', '\\n', 'Dataset: imdb Method: dual Epoch 4:\nvalidation_loss = 0.6227', '\\n', 'Dataset: yelp_review_full Method: static Epoch\n0: validation_loss = 1.4878', '\\n', 'Dataset: yelp_review_full Method: static\nEpoch 1: validation_loss = 1.4959', '\\n', 'Dataset: yelp_review_full Method:\nstatic Epoch 2: validation_loss = 1.5128', '\\n', 'Dataset: yelp_review_full\nMethod: static Epoch 3: validation_loss = 1.5494', '\\n', 'Dataset:\nyelp_review_full Method: static Epoch 4: validation_loss = 1.5641', '\\n',\n'Dataset: yelp_review_full Method: single Epoch 0: validation_loss = 1.5164',\n'\\n', 'Dataset: yelp_review_full Method: single Epoch 1: validation_loss =\n1.4899', '\\n', 'Dataset: yelp_review_full Method: single Epoch 2:\nvalidation_loss = 1.5313', '\\n', 'Dataset: yelp_review_full Method: single Epoch\n3: validation_loss = 1.5115', '\\n', 'Dataset: yelp_review_full Method: single\nEpoch 4: validation_loss = 1.5386', '\\n', 'Dataset: yelp_review_full Method:\ndual Epoch 0: validation_loss = 1.5290', '\\n', 'Dataset: yelp_review_full\nMethod: dual Epoch 1: validation_loss = 1.5038', '\\n', 'Dataset:\nyelp_review_full Method: dual Epoch 2: validation_loss = 1.5569', '\\n',\n'Dataset: yelp_review_full Method: dual Epoch 3: validation_loss = 1.5926',\n'\\n', 'Dataset: yelp_review_full Method: dual Epoch 4: validation_loss =\n1.5579', '\\n', 'Execution time: 49 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 60, in <module>\\n    raw = load_dataset(ds_name, cfg) if cfg\nis not None else load_dataset(ds_name)\\n\n^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1652, in dataset_module_factory\\n    raise e1\nfrom None\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1578, in dataset_module_factory\\n    raise\nDatasetNotFoundError(f\"Dataset \\'{path}\\' doesn\\'t exist on the Hub or cannot be\naccessed.\") from e\\ndatasets.exceptions.DatasetNotFoundError: Dataset \\'iris\\'\ndoesn\\'t exist on the Hub or cannot be accessed.\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n\\n\\n', '\\rDownloading data:   0%|          |\n0.00/3.86k [00:00<?, ?B/s]', '\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 3.86k/3.86k [00:00<00:00, 4.32MB/s]', '\\n', '\\rGenerating train\nsplit: 0 examples [00:00, ? examples/s]', '', '\\rGenerating train split: 150\nexamples [00:00, 12137.94 examples/s]', '\\n', 'Dataset iris AI Epoch 1:\nvalidation_loss = 0.8167', '\\n', 'Dataset iris AI Epoch 2: validation_loss =\n0.7397', '\\n', 'Dataset iris AI Epoch 3: validation_loss = 0.6753', '\\n',\n'Dataset iris AI Epoch 4: validation_loss = 0.6249', '\\n', 'Dataset iris AI\nEpoch 5: validation_loss = 0.5864', '\\n', 'Dataset iris AI Epoch 6:\nvalidation_loss = 0.5563', '\\n', 'Dataset iris AI Epoch 7: validation_loss =\n0.5334', '\\n', 'Dataset iris AI Epoch 8: validation_loss = 0.5153', '\\n',\n'Dataset iris AI Epoch 9: validation_loss = 0.5010', '\\n', 'Dataset iris AI\nEpoch 10: validation_loss = 0.4888', '\\n', 'Traceback (most recent call last):\\n\nFile \"runfile.py\", line 144, in <module>\\n    p_train =\ntorch.softmax(ai_model(X_tr_t), dim=1).cpu().numpy()\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: Can\\'t call\nnumpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\\n',\n'Execution time: 3 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 1.3716', '\\n',\n'Traceback (most recent call last):\\n  File \"runfile.py\", line 192, in\n<module>\\n    kl = torch.sum(dist_usr * torch.log(dist_usr / (dist_ai +\n1e-10))).item()\\n\n~~~~~~~~~^~~~~~~~~~~~~~~~~~~\\nRuntimeError: The size of tensor a (1004) must\nmatch the size of tensor b (1000) at non-singleton dimension 0\\n', 'Execution\ntime: 10 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 1.3719', '\\n', 'Epoch\n1: validation_loss = 1.3280', '\\n', 'Epoch 2: validation_loss = 1.2454', '\\n',\n'Epoch 3: validation_loss = 1.1273', '\\n', 'Epoch 4: validation_loss = 1.0030',\n'\\n', 'Traceback (most recent call last):\\n  File \"runfile.py\", line 83, in\n<module>\\n    ds = load_dataset(ds_name)\\n         ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1652, in dataset_module_factory\\n    raise e1\nfrom None\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1578, in dataset_module_factory\\n    raise\nDatasetNotFoundError(f\"Dataset \\'{path}\\' doesn\\'t exist on the Hub or cannot be\naccessed.\") from e\\ndatasets.exceptions.DatasetNotFoundError: Dataset\n\\'yelp_review_polarity\\' doesn\\'t exist on the Hub or cannot be accessed.\\n',\n'Execution time: 12 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 46, in <module>\\n    \"iris\": load_dataset(\"iris\"),\\n\n^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1652, in dataset_module_factory\\n    raise e1\nfrom None\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1578, in dataset_module_factory\\n    raise\nDatasetNotFoundError(f\"Dataset \\'{path}\\' doesn\\'t exist on the Hub or cannot be\naccessed.\") from e\\ndatasets.exceptions.DatasetNotFoundError: Dataset \\'iris\\'\ndoesn\\'t exist on the Hub or cannot be accessed.\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Dataset iris AI Epoch 1: validation_loss =\n0.8167', '\\n', 'Dataset iris AI Epoch 2: validation_loss = 0.7397', '\\n',\n'Dataset iris AI Epoch 3: validation_loss = 0.6753', '\\n', 'Dataset iris AI\nEpoch 4: validation_loss = 0.6249', '\\n', 'Dataset iris AI Epoch 5:\nvalidation_loss = 0.5864', '\\n', 'Dataset iris AI Epoch 6: validation_loss =\n0.5563', '\\n', 'Dataset iris AI Epoch 7: validation_loss = 0.5334', '\\n',\n'Dataset iris AI Epoch 8: validation_loss = 0.5153', '\\n', 'Dataset iris AI\nEpoch 9: validation_loss = 0.5010', '\\n', 'Dataset iris AI Epoch 10:\nvalidation_loss = 0.4888', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 189, in <module>\\n    w_u =\nuser_model.linear.weight.abs().cpu().numpy()[:, :num_feat]\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: Can\\'t call numpy()\non Tensor that requires grad. Use tensor.detach().numpy() instead.\\n',\n'Execution time: 2 seconds seconds (time limit is an hour).']", "['Execution time: 15 seconds seconds (time limit is an hour).']", "['Execution time: 15 seconds seconds (time limit is an hour).']", "['Execution time: 15 seconds seconds (time limit is an hour).']", ""], "analysis": ["", "The script fails immediately when calling load_dataset('iris') because there is\nno HuggingFace dataset with the id 'iris'. As a result, the entire experiment\npipeline is never executed. To fix this, replace the first dataset entry\n('iris', None) with a valid HF dataset identifier (for example 'uci_iris' or\nload via the OpenML builder: load_dataset('openml', 'iris')). Additionally,\nupdate the label\u2010column detection logic to handle the new dataset\u2019s column name\n(e.g. 'target' instead of 'label').", "The script failed with ModuleNotFoundError: No module named 'sklearn'. The\nStandardScaler import from scikit-learn is missing. To fix, add scikit-learn as\na dependency (e.g., pip install scikit-learn) or replace the scaler with an\nequivalent implementation (e.g., torch-based normalization).", "The script crashes at the first dataset load because the HuggingFace dataset\nidentifier 'iris' doesn\u2019t exist on the Hub. Additionally, 'wine_quality' is not\nthe correct name for the wine-quality dataset (it should be 'wine-quality'). As\na result, load_dataset fails and the entire experiment aborts. To fix this,\nupdate dataset_configs to use valid HF identifiers or builders\u2014for example, load\nIris via scikit-learn (Dataset.from_pandas) or via HF\u2019s openml builder\n(dataset_id=61), use 'wine-quality' for the wine dataset, and ensure three valid\nHF datasets (e.g. openml Iris, wine-quality, and adult) are loaded.", "The script fails at loading the 'uci_iris' dataset because the name is incorrect\non the HuggingFace Hub. Renaming it to the correct dataset ID (e.g., 'iris' or\n'uciml/iris') fixes the error.", "ModuleNotFoundError: No module named 'sklearn'. The script fails at the import\nof CountVectorizer because scikit-learn is not installed in the environment. To\nfix, install scikit-learn (e.g., `pip install scikit-learn`) or replace it with\nan alternative vectorization library (e.g., HuggingFace tokenizers or\ntorchtext).", "The script executed without runtime errors, but the static, single-channel, and\ndual-channel methods all follow identical training procedures \u2014 alpha only\nupdates the simulated user bias (p_user) and never affects the model or loss. As\na result, validation losses for all three methods are virtually the same,\ndefeating the intended experimental comparison. To fix this, the user-feedback\nestimate (p_user) must be integrated into the training loop (e.g., by\nreweighting losses or feature gradients based on p_user or by incorporating user\ncorrections into the model update) so that single and dual conditions truly\ndiffer from the static baseline.", "The script fails to load the \u2018iris\u2019 dataset from the HuggingFace Hub, raising a\nDatasetNotFoundError. The dataset identifier is incorrect or unavailable. To fix\nthis, use a valid dataset name (e.g., 'uci_iris') or load Iris via scikit-learn\n(from sklearn.datasets import load_iris), or otherwise ensure the specified\ndataset is accessible on the Hub.", "The script crashes at the computation of p_train (and similarly p_val) because\nit calls .numpy() on a tensor that still requires gradients. The RuntimeError\nsuggests using .detach(). Proposed fix: wrap the AI inference in a no_grad()\nblock or detach the tensor before converting to numpy, e.g.: p_train =\ntorch.softmax(ai_model(X_tr_t), dim=1).detach().cpu().numpy().", "The alignment score computation assumes ai_imp and usr_imp have the same\ndimensionality. However, ai_model.fc1.weight has shape [d_hid, d_in] (1000\nfeatures) while user_model.fc1.weight has shape [d_hid, d_in + n_cls] (1004\nfeatures because class probabilities are appended). This causes a size mismatch\nwhen computing KL divergence. A fix is to compute user feature importances only\non the original d_in text features by slicing user_model.fc1.weight to [:,\n:d_in] before taking the mean, e.g.: `usr_imp =\ntorch.mean(torch.abs(user_model.fc1.weight[:, :d_in]), dim=0)`.", "The script crashes when loading the second dataset because\nload_dataset('yelp_review_polarity') raises a DatasetNotFoundError. This stops\nthe experiment loop early. To fix, use the correct dataset identifier (e.g.,\n'yelp_polarity' or proper config), ensure the dataset is available, or wrap\nload_dataset in a try/except to skip or retry missing datasets.", "The script crashes when attempting to load the Iris dataset via\nload_dataset('iris'), because the HuggingFace Hub does not host a dataset under\nthat name. As a result, the experiment never runs. To fix this, replace the\nfaulty call with a valid dataset loader\u2014e.g., use the correct HuggingFace\nidentifier (such as 'uci_iris' if available) or load the Iris data via scikit-\nlearn (sklearn.datasets.load_iris()), then wrap it into a Dataset for\nconsistency.", "The script crashes during the user-model weight extraction because it tries to\ncall .numpy() on a tensor that still requires gradients. The fix is to detach\nthe tensor from the computation graph before converting to numpy, e.g., w_u =\nuser_model.linear.weight.abs().detach().cpu().numpy()[:, :num_feat].", "", "", "", ""], "exc_type": [null, "DatasetNotFoundError", "ModuleNotFoundError", "DatasetNotFoundError", "DatasetNotFoundError", "ModuleNotFoundError", null, "DatasetNotFoundError", "RuntimeError", "RuntimeError", "DatasetNotFoundError", "DatasetNotFoundError", "RuntimeError", null, null, null, null], "exc_info": [null, {"args": ["Dataset 'iris' doesn't exist on the Hub or cannot be accessed."]}, {"args": ["No module named 'sklearn'"], "name": "sklearn", "msg": "No module named 'sklearn'"}, {"args": ["Dataset 'iris' doesn't exist on the Hub or cannot be accessed."]}, {"args": ["Dataset 'uci_iris' doesn't exist on the Hub or cannot be accessed."]}, {"args": ["No module named 'sklearn'"], "name": "sklearn", "msg": "No module named 'sklearn'"}, null, {"args": ["Dataset 'iris' doesn't exist on the Hub or cannot be accessed."]}, {"args": ["Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."]}, {"args": ["The size of tensor a (1004) must match the size of tensor b (1000) at non-singleton dimension 0"]}, {"args": ["Dataset 'yelp_review_polarity' doesn't exist on the Hub or cannot be accessed."]}, {"args": ["Dataset 'iris' doesn't exist on the Hub or cannot be accessed."]}, {"args": ["Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."]}, null, null, null, null], "exc_stack": [null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 67, "<module>", "ds = load_dataset(name)[\"train\"]"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1652, "dataset_module_factory", "raise e1 from None"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1578, "dataset_module_factory", "raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e"]], [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 12, "<module>", "from sklearn.preprocessing import StandardScaler"]], [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 55, "<module>", "raw = load_dataset(ds_name, cfg) if cfg is not None else load_dataset(ds_name)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1652, "dataset_module_factory", "raise e1 from None"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1578, "dataset_module_factory", "raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e"]], [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 57, "<module>", "raw = load_dataset(ds_name, cfg) if cfg is not None else load_dataset(ds_name)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1652, "dataset_module_factory", "raise e1 from None"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1578, "dataset_module_factory", "raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e"]], [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 12, "<module>", "from sklearn.feature_extraction.text import CountVectorizer"]], null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 60, "<module>", "raw = load_dataset(ds_name, cfg) if cfg is not None else load_dataset(ds_name)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1652, "dataset_module_factory", "raise e1 from None"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1578, "dataset_module_factory", "raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e"]], [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 144, "<module>", "p_train = torch.softmax(ai_model(X_tr_t), dim=1).cpu().numpy()"]], [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 192, "<module>", "kl = torch.sum(dist_usr * torch.log(dist_usr / (dist_ai + 1e-10))).item()"]], [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 83, "<module>", "ds = load_dataset(ds_name)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1652, "dataset_module_factory", "raise e1 from None"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1578, "dataset_module_factory", "raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e"]], [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 46, "<module>", "\"iris\": load_dataset(\"iris\"),"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1652, "dataset_module_factory", "raise e1 from None"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1578, "dataset_module_factory", "raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e"]], [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 189, "<module>", "w_u = user_model.linear.weight.abs().cpu().numpy()[:, :num_feat]"]], null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.995, "best_value": 0.995}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0152, "best_value": 0.0152}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0162, "best_value": 0.0162}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0195, "best_value": 0.0195}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.014, "best_value": 0.014}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0142, "best_value": 0.0142}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0119, "best_value": 0.0119}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0138, "best_value": 0.0138}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0188, "best_value": 0.0188}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9867, "best_value": 0.9867}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.99, "best_value": 0.99}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0095, "best_value": 0.0095}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0103, "best_value": 0.0103}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0157, "best_value": 0.0157}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.0083, "best_value": 0.0083}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0212, "best_value": 0.0212}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0179, "best_value": 0.0179}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0345, "best_value": 0.0345}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0182, "best_value": 0.0182}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test accuracy", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.992, "best_value": 0.992}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.998, "best_value": 0.998}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Cross-entropy loss on the training set", "data": [{"dataset_name": "ag_news (static)", "final_value": 0.933, "best_value": 0.933}, {"dataset_name": "ag_news (single)", "final_value": 0.9314, "best_value": 0.9314}, {"dataset_name": "ag_news (dual)", "final_value": 0.9342, "best_value": 0.9342}, {"dataset_name": "imdb (static)", "final_value": 0.5707, "best_value": 0.5707}, {"dataset_name": "imdb (single)", "final_value": 0.5734, "best_value": 0.5734}, {"dataset_name": "imdb (dual)", "final_value": 0.5603, "best_value": 0.5603}, {"dataset_name": "yelp_review_full (static)", "final_value": 1.298, "best_value": 1.298}, {"dataset_name": "yelp_review_full (single)", "final_value": 1.288, "best_value": 1.288}, {"dataset_name": "yelp_review_full (dual)", "final_value": 1.283, "best_value": 1.283}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Cross-entropy loss on the validation set", "data": [{"dataset_name": "ag_news (static)", "final_value": 1.0602, "best_value": 1.0602}, {"dataset_name": "ag_news (single)", "final_value": 1.0637, "best_value": 1.0637}, {"dataset_name": "ag_news (dual)", "final_value": 1.0615, "best_value": 1.0615}, {"dataset_name": "imdb (static)", "final_value": 0.5976, "best_value": 0.5976}, {"dataset_name": "imdb (single)", "final_value": 0.6183, "best_value": 0.6183}, {"dataset_name": "imdb (dual)", "final_value": 0.6227, "best_value": 0.6227}, {"dataset_name": "yelp_review_full (static)", "final_value": 1.5641, "best_value": 1.5641}, {"dataset_name": "yelp_review_full (single)", "final_value": 1.5386, "best_value": 1.5386}, {"dataset_name": "yelp_review_full (dual)", "final_value": 1.5579, "best_value": 1.5579}]}, {"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training set", "data": [{"dataset_name": "ag_news (static)", "final_value": 0.603, "best_value": 0.603}, {"dataset_name": "ag_news (single)", "final_value": 0.617, "best_value": 0.617}, {"dataset_name": "ag_news (dual)", "final_value": 0.6095, "best_value": 0.6095}, {"dataset_name": "imdb (static)", "final_value": 0.725, "best_value": 0.725}, {"dataset_name": "imdb (single)", "final_value": 0.7225, "best_value": 0.7225}, {"dataset_name": "imdb (dual)", "final_value": 0.7155, "best_value": 0.7155}, {"dataset_name": "yelp_review_full (static)", "final_value": 0.447, "best_value": 0.447}, {"dataset_name": "yelp_review_full (single)", "final_value": 0.4515, "best_value": 0.4515}, {"dataset_name": "yelp_review_full (dual)", "final_value": 0.4505, "best_value": 0.4505}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "ag_news (static)", "final_value": 0.568, "best_value": 0.568}, {"dataset_name": "ag_news (single)", "final_value": 0.57, "best_value": 0.57}, {"dataset_name": "ag_news (dual)", "final_value": 0.56, "best_value": 0.56}, {"dataset_name": "imdb (static)", "final_value": 0.698, "best_value": 0.698}, {"dataset_name": "imdb (single)", "final_value": 0.694, "best_value": 0.694}, {"dataset_name": "imdb (dual)", "final_value": 0.678, "best_value": 0.678}, {"dataset_name": "yelp_review_full (static)", "final_value": 0.354, "best_value": 0.354}, {"dataset_name": "yelp_review_full (single)", "final_value": 0.364, "best_value": 0.364}, {"dataset_name": "yelp_review_full (dual)", "final_value": 0.36, "best_value": 0.36}]}, {"metric_name": "alignment", "lower_is_better": false, "description": "Alignment metric (higher is better)", "data": [{"dataset_name": "ag_news (static)", "final_value": 0.492, "best_value": 0.492}, {"dataset_name": "ag_news (single)", "final_value": 0.8124, "best_value": 0.8124}, {"dataset_name": "ag_news (dual)", "final_value": 0.9986, "best_value": 0.9986}, {"dataset_name": "imdb (static)", "final_value": 0.5363, "best_value": 0.5363}, {"dataset_name": "imdb (single)", "final_value": 0.832, "best_value": 0.832}, {"dataset_name": "imdb (dual)", "final_value": 0.9972, "best_value": 0.9972}, {"dataset_name": "yelp_review_full (static)", "final_value": 0.5302, "best_value": 0.5302}, {"dataset_name": "yelp_review_full (single)", "final_value": 0.8397, "best_value": 0.8397}, {"dataset_name": "yelp_review_full (dual)", "final_value": 0.999, "best_value": 0.999}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on training set", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.995, "best_value": 0.995}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on training set", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0152, "best_value": 0.0152}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0162, "best_value": 0.0162}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0195, "best_value": 0.0195}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.014, "best_value": 0.014}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0142, "best_value": 0.0142}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0119, "best_value": 0.0119}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0138, "best_value": 0.0138}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0188, "best_value": 0.0188}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on validation set", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9867, "best_value": 0.9867}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.99, "best_value": 0.99}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on validation set", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0095, "best_value": 0.0095}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0103, "best_value": 0.0103}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0157, "best_value": 0.0157}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.0083, "best_value": 0.0083}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0212, "best_value": 0.0212}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0179, "best_value": 0.0179}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0345, "best_value": 0.0345}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0182, "best_value": 0.0182}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on test set", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.992, "best_value": 0.992}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.998, "best_value": 0.998}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Classification accuracy on the training dataset", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.995, "best_value": 0.995}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0152, "best_value": 0.0152}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0162, "best_value": 0.0162}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0195, "best_value": 0.0195}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.014, "best_value": 0.014}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0142, "best_value": 0.0142}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0119, "best_value": 0.0119}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0138, "best_value": 0.0138}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0188, "best_value": 0.0188}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Classification accuracy on the validation dataset", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9867, "best_value": 0.9867}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.99, "best_value": 0.99}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0095, "best_value": 0.0095}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0103, "best_value": 0.0103}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0157, "best_value": 0.0157}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.0083, "best_value": 0.0083}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0212, "best_value": 0.0212}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0179, "best_value": 0.0179}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0345, "best_value": 0.0345}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0182, "best_value": 0.0182}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Classification accuracy on the test dataset", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.992, "best_value": 0.992}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.998, "best_value": 0.998}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.995, "best_value": 0.995}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0152, "best_value": 0.0152}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0162, "best_value": 0.0162}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0195, "best_value": 0.0195}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.014, "best_value": 0.014}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0142, "best_value": 0.0142}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0119, "best_value": 0.0119}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0138, "best_value": 0.0138}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0188, "best_value": 0.0188}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9867, "best_value": 0.9867}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.99, "best_value": 0.99}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0095, "best_value": 0.0095}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0103, "best_value": 0.0103}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0157, "best_value": 0.0157}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.0083, "best_value": 0.0083}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0212, "best_value": 0.0212}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0179, "best_value": 0.0179}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0345, "best_value": 0.0345}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0182, "best_value": 0.0182}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test accuracy", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.992, "best_value": 0.992}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.998, "best_value": 0.998}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_32_user_bs_32_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_16_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_16_train_val_curves.png"], [], [], [], [], [], [], [], [], [], [], [], [], ["../../logs/0-run/experiment_results/experiment_12bdbf98f1184a0393723ce3fe12841d_proc_2569236/ai_bs_64_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_12bdbf98f1184a0393723ce3fe12841d_proc_2569236/ai_bs_32_user_bs_32_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_12bdbf98f1184a0393723ce3fe12841d_proc_2569236/ai_bs_16_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_12bdbf98f1184a0393723ce3fe12841d_proc_2569236/ai_bs_16_user_bs_16_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_12bdbf98f1184a0393723ce3fe12841d_proc_2569236/ai_bs_64_user_bs_16_train_val_curves.png"], ["../../logs/0-run/experiment_results/experiment_7ad4f03fb5c54511b750217b607b4f1b_proc_2569238/ai_bs_64_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_7ad4f03fb5c54511b750217b607b4f1b_proc_2569238/ai_bs_32_user_bs_32_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_7ad4f03fb5c54511b750217b607b4f1b_proc_2569238/ai_bs_16_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_7ad4f03fb5c54511b750217b607b4f1b_proc_2569238/ai_bs_16_user_bs_16_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_7ad4f03fb5c54511b750217b607b4f1b_proc_2569238/ai_bs_64_user_bs_16_train_val_curves.png"], ["../../logs/0-run/experiment_results/experiment_bc59bb1520b04b6e831de7abd7f7da42_proc_2569237/ai_bs_64_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_bc59bb1520b04b6e831de7abd7f7da42_proc_2569237/ai_bs_32_user_bs_32_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_bc59bb1520b04b6e831de7abd7f7da42_proc_2569237/ai_bs_16_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_bc59bb1520b04b6e831de7abd7f7da42_proc_2569237/ai_bs_16_user_bs_16_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_bc59bb1520b04b6e831de7abd7f7da42_proc_2569237/ai_bs_64_user_bs_16_train_val_curves.png"], ["../../logs/0-run/experiment_results/seed_aggregation_5f061d8c4e614f9d9c067f292cdea492/synthetic_binary_bs_ai_bs_64_user_bs_16_aggregated_mean_sem.png", "../../logs/0-run/experiment_results/seed_aggregation_5f061d8c4e614f9d9c067f292cdea492/synthetic_binary_bs_ai_bs_64_user_bs_64_aggregated_mean_sem.png", "../../logs/0-run/experiment_results/seed_aggregation_5f061d8c4e614f9d9c067f292cdea492/synthetic_binary_bs_ai_bs_16_user_bs_64_aggregated_mean_sem.png", "../../logs/0-run/experiment_results/seed_aggregation_5f061d8c4e614f9d9c067f292cdea492/synthetic_binary_bs_ai_bs_16_user_bs_16_aggregated_mean_sem.png", "../../logs/0-run/experiment_results/seed_aggregation_5f061d8c4e614f9d9c067f292cdea492/synthetic_binary_bs_ai_bs_32_user_bs_32_aggregated_mean_sem.png"]], "plot_paths": [["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_32_user_bs_32_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_16_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_16_train_val_curves.png"], [], [], [], [], [], [], [], [], [], [], [], [], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_12bdbf98f1184a0393723ce3fe12841d_proc_2569236/ai_bs_64_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_12bdbf98f1184a0393723ce3fe12841d_proc_2569236/ai_bs_32_user_bs_32_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_12bdbf98f1184a0393723ce3fe12841d_proc_2569236/ai_bs_16_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_12bdbf98f1184a0393723ce3fe12841d_proc_2569236/ai_bs_16_user_bs_16_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_12bdbf98f1184a0393723ce3fe12841d_proc_2569236/ai_bs_64_user_bs_16_train_val_curves.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7ad4f03fb5c54511b750217b607b4f1b_proc_2569238/ai_bs_64_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7ad4f03fb5c54511b750217b607b4f1b_proc_2569238/ai_bs_32_user_bs_32_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7ad4f03fb5c54511b750217b607b4f1b_proc_2569238/ai_bs_16_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7ad4f03fb5c54511b750217b607b4f1b_proc_2569238/ai_bs_16_user_bs_16_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7ad4f03fb5c54511b750217b607b4f1b_proc_2569238/ai_bs_64_user_bs_16_train_val_curves.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_bc59bb1520b04b6e831de7abd7f7da42_proc_2569237/ai_bs_64_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_bc59bb1520b04b6e831de7abd7f7da42_proc_2569237/ai_bs_32_user_bs_32_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_bc59bb1520b04b6e831de7abd7f7da42_proc_2569237/ai_bs_16_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_bc59bb1520b04b6e831de7abd7f7da42_proc_2569237/ai_bs_16_user_bs_16_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_bc59bb1520b04b6e831de7abd7f7da42_proc_2569237/ai_bs_64_user_bs_16_train_val_curves.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_5f061d8c4e614f9d9c067f292cdea492/synthetic_binary_bs_ai_bs_64_user_bs_16_aggregated_mean_sem.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_5f061d8c4e614f9d9c067f292cdea492/synthetic_binary_bs_ai_bs_64_user_bs_64_aggregated_mean_sem.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_5f061d8c4e614f9d9c067f292cdea492/synthetic_binary_bs_ai_bs_16_user_bs_64_aggregated_mean_sem.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_5f061d8c4e614f9d9c067f292cdea492/synthetic_binary_bs_ai_bs_16_user_bs_16_aggregated_mean_sem.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_5f061d8c4e614f9d9c067f292cdea492/synthetic_binary_bs_ai_bs_32_user_bs_32_aggregated_mean_sem.png"]], "plot_analyses": [[{"analysis": "Synthetic binary dataset - ai_bs_64_user_bs_64: Training accuracy ramps up from about 91.5% at epoch 1 to above 99% by epoch 3 and then plateaus with minor oscillations around 99.3\u201399.8%. Validation accuracy closely tracks training, peaking near 99.5% by epoch 10. Training and validation losses both begin high (~0.40 and ~0.22) and decay rapidly to near zero by epoch 10. There is no sign of overfitting; both curves remain well\u2010aligned throughout. Given this rapid convergence, you could reduce the total number of epochs or introduce early stopping around epoch 8 without sacrificing performance.", "valid_plots_received": true, "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_64_train_val_curves.png"}, {"analysis": "Synthetic binary dataset - ai_bs_32_user_bs_32: Accuracy jumps from around 60% at epoch 1 to 98% by epoch 3 and almost 100% by epoch 5. Validation accuracy follows closely, with only small dips around epoch 8 and epoch 16 but never falling below 95%. Loss curves start higher (~0.53 train, ~0.35 val) but drop sharply to below 0.02 by epoch 8. Convergence is slightly faster than with batch size 64, and there is again no evidence of overfitting. Consider reducing epochs to 6\u20138 or experimenting with a slightly lower learning rate to smooth out minor validation fluctuations.", "valid_plots_received": true, "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_32_user_bs_32_train_val_curves.png"}, {"analysis": "Synthetic binary dataset - ai_bs_16_user_bs_64: Training accuracy moves from 65% at epoch 1 to 97% by epoch 4 and stabilizes above 98.5% thereafter. Validation accuracy closely follows, peaking near 99.8% around epoch 10. Loss starts around 0.63 (train) and 0.48 (val), and steadily declines to ~0.02 by epoch 10. The initial slow ramp suggests noisier gradients with the smaller AI batch but longer stability despite the larger user batch. This combination achieves high final performance but could benefit from a warmup schedule or a slightly higher initial learning rate to speed up the early epochs.", "valid_plots_received": true, "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_64_train_val_curves.png"}, {"analysis": "Synthetic binary dataset - ai_bs_16_user_bs_16: Accuracy surges to ~99% by epoch 2 and hovers between 99.5% and 100% thereafter. Validation accuracy consistently tracks at or above training. Loss falls from ~0.42 (train) and ~0.10 (val) at epoch 1 to near zero by epoch 5, with only minor noise later. Small batch sizes produce very fast convergence but also slightly noisier loss curves in later epochs. You could experiment with gradient\u2010noise reduction techniques (e.g., gradient clipping or learning rate decay) to further stabilize training.", "valid_plots_received": true, "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_16_train_val_curves.png"}, {"analysis": "Synthetic binary dataset - ai_bs_64_user_bs_16: Training accuracy climbs from ~90% at epoch 1 to above 98% by epoch 3, then plateaus around 99.2\u201399.8%. Validation accuracy mirrors training, though it dips to ~98.3% around epoch 10 before rebounding. Loss starts around 0.31 (train) and 0.06 (val), dropping to under 0.02 by epoch 8. This mixed\u2010batch configuration converges almost as rapidly as the all\u2010small\u2010batch setup but with slightly smoother late\u2010epoch loss. You might reduce total epochs to 8\u201310 and tune the learning\u2010rate schedule for marginal speed gains.", "valid_plots_received": true, "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_16_train_val_curves.png"}], [], [], [], [], [], [], [], [], [], [], [], [], [{"analysis": "Batch sizes of 64 for both AI and user modules show very rapid convergence: training accuracy rises from about 0.92 to over 0.99 by epoch 3, validation stays around 0.99 throughout, and both train/validation losses drop below 0.05 by epoch 4 and below 0.01 by epoch 7. Curves are smooth with minimal jitter, indicating highly stable optimization at this scale.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_12bdbf98f1184a0393723ce3fe12841d_proc_2569236/ai_bs_64_user_bs_64_train_val_curves.png"}, {"analysis": "With both AI and user batch sizes at 32, accuracy climbs from 0.60/0.90 (train/val) at epoch 1 to around 0.99 by epoch 6. Losses for train/validation fall sharply to below 0.05 by epoch 4 and reach near-zero by epoch 10. Slight fluctuations appear in validation accuracy after epoch 8, but overall stability remains strong.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_12bdbf98f1184a0393723ce3fe12841d_proc_2569236/ai_bs_32_user_bs_32_train_val_curves.png"}, {"analysis": "AI batch size 16 and user batch size 64 yields slightly noisier early dynamics: training accuracy jumps from 0.66 to 0.96 by epoch 3 and to 0.99 by epoch 7, validation follows similarly from 0.91 to 0.995. Train/validation losses drop under 0.10 by epoch 2 and below 0.02 by epoch 5. Later epochs plateau around 0.01 with minor oscillations.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_12bdbf98f1184a0393723ce3fe12841d_proc_2569236/ai_bs_16_user_bs_64_train_val_curves.png"}, {"analysis": "Both AI and user batch sizes at 16 exhibit the most noise: accuracy reaches above 0.99 by epoch 3 but oscillates \u00b10.01 through epoch 20. Loss curves decline rapidly\u2014train/validation losses are under 0.05 by epoch 3 and around 0.01 by epoch 6\u2014but small jitters persist across later epochs, suggesting trade-off between convergence speed and noise with smaller batches.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_12bdbf98f1184a0393723ce3fe12841d_proc_2569236/ai_bs_16_user_bs_16_train_val_curves.png"}, {"analysis": "With AI batch size 64 and user batch size 16, accuracy grows from 0.90/0.99 at epoch 1 to ~0.995 by epoch 5. Loss drops below 0.05 by epoch 3 and hovers around 0.005 thereafter. Validation loss shows slight spikes late in training (notably epoch 20), indicating some instability when mixing large AI batches with small user batches.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_12bdbf98f1184a0393723ce3fe12841d_proc_2569236/ai_bs_64_user_bs_16_train_val_curves.png"}], [{"analysis": "In the ai_bs_64_user_bs_64 run, training accuracy jumps from about 91% at epoch 1 to over 99% by epoch 4. Validation accuracy closely follows, rising from ~99% at epoch 2 and stabilizing around 99.0\u201399.5% thereafter. Both training and validation loss plummet in the first five epochs (train from ~0.40 to ~0.03, validation from ~0.22 to ~0.02) then slowly taper off to ~0.01 by epoch 20. The tight alignment suggests no overfitting and very rapid convergence when both AI and user batch sizes are large.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7ad4f03fb5c54511b750217b607b4f1b_proc_2569238/ai_bs_64_user_bs_64_train_val_curves.png"}, {"analysis": "In the ai_bs_32_user_bs_32 condition, initial training accuracy is lower (~60% at epoch 1) but quickly climbs to ~98% by epoch 4 and nearly 100% by epoch 6. Validation accuracy follows a similar trajectory, catching up by epoch 3 and remaining in the high 99% range. Loss curves drop steeply (train from ~0.53 to ~0.04, validation from ~0.36 to ~0.03 within five epochs) and then flatten near ~0.02. Reducing both batch sizes slows very early convergence but yields the same asymptotic performance without overfitting.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7ad4f03fb5c54511b750217b607b4f1b_proc_2569238/ai_bs_32_user_bs_32_train_val_curves.png"}, {"analysis": "With ai_bs_16_user_bs_64, training accuracy starts around 65% but reaches ~98% by epoch 3 and oscillates narrowly around 99% after epoch 5. Validation accuracy rises from ~91% to ~99% in the first four epochs, then remains stable. Loss declines from ~0.63 to ~0.05 (train) and from ~0.47 to ~0.04 (validation) within five epochs, then smooths toward ~0.02. A smaller AI batch size slightly delays convergence but the large user batch maintains stable validation behavior.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7ad4f03fb5c54511b750217b607b4f1b_proc_2569238/ai_bs_16_user_bs_64_train_val_curves.png"}, {"analysis": "Under ai_bs_16_user_bs_16, training accuracy begins at ~77% and hits ~99% by epoch 3. Validation accuracy is similarly high early (about 99% by epoch 3), with both curves tracking each other closely. Loss curves fall from ~0.42 (train) and ~0.10 (validation) at epoch 1 to below ~0.03 by epoch 5, eventually reaching ~0.01. Reducing both batch sizes decelerates the very first update but does not affect final accuracy or introduce overfitting.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7ad4f03fb5c54511b750217b607b4f1b_proc_2569238/ai_bs_16_user_bs_16_train_val_curves.png"}, {"analysis": "In the ai_bs_64_user_bs_16 scenario, training accuracy moves from ~90% at epoch 1 to ~99% by epoch 4 and remains around 99\u201399.5%. Validation accuracy fluctuates slightly but sits within 98.5\u201399.5%. Loss drops rapidly from ~0.31 (train) and ~0.06 (validation) to under 0.05 by epoch 3, then stabilizes around 0.01. Here, a large AI batch size ensures rapid initial learning, while a smaller user batch size introduces minor variability in validation accuracy but no degradation in overall performance.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7ad4f03fb5c54511b750217b607b4f1b_proc_2569238/ai_bs_64_user_bs_16_train_val_curves.png"}], [{"analysis": "Under ai_bs_64_user_bs_64, accuracy quickly jumps from around 0.92 at epoch 1 to about 0.99 by epoch 3, then plateaus near 0.995\u20130.998. Training and validation losses both fall steeply\u2014from \u22480.40/0.22 to near 0.02 by epoch 10\u2014and remain low, with the curves almost overlapping. This indicates stable convergence with minimal gap and no obvious overfitting.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_bc59bb1520b04b6e831de7abd7f7da42_proc_2569237/ai_bs_64_user_bs_64_train_val_curves.png"}, {"analysis": "Under ai_bs_32_user_bs_32, accuracy begins unusually low (~0.60) at epoch 1 but rises to \u223c0.98 by epoch 2 and settles around 0.995\u20130.999 in subsequent epochs. Loss decreases from \u22480.53/0.35 to around 0.02 by epoch 5, after which fluctuations are small. Training and validation curves remain closely matched, though validation accuracy shows minor variability in later epochs.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_bc59bb1520b04b6e831de7abd7f7da42_proc_2569237/ai_bs_32_user_bs_32_train_val_curves.png"}, {"analysis": "Under ai_bs_16_user_bs_64, accuracy starts at ~0.66, climbs to ~0.97 by epoch 2, and achieves \u22480.99 by epoch 3, remaining around 0.995 afterwards. Loss declines from \u22480.62/0.47 to \u223c0.02 by epoch 8. A brief discrepancy at epoch 2\u2014validation loss higher than training\u2014quickly resolves. Beyond that, both curves align tightly, demonstrating solid convergence.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_bc59bb1520b04b6e831de7abd7f7da42_proc_2569237/ai_bs_16_user_bs_64_train_val_curves.png"}, {"analysis": "Under ai_bs_16_user_bs_16, validation accuracy is already very high (~0.995) at epoch 1, while training accuracy catches up by epoch 2. Both converge near 0.996\u20130.998 by epoch 5. Loss rapidly decays from \u22480.42/0.10 to ~0.01 by epoch 5, with only slight noise in validation loss at later stages. This configuration exhibits the fastest convergence and extremely low variance.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_bc59bb1520b04b6e831de7abd7f7da42_proc_2569237/ai_bs_16_user_bs_16_train_val_curves.png"}, {"analysis": "Under ai_bs_64_user_bs_16, training accuracy rises from ~0.90 at epoch 1 to \u223c0.99 by epoch 3, matching validation accuracy (\u22480.99) and plateauing thereafter. Loss drops from \u22480.31/0.06 to about 0.02 by epoch 5 and stays low; a minor spike in validation loss appears at epoch 20. Overall, convergence is smooth and generalization gap negligible.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_bc59bb1520b04b6e831de7abd7f7da42_proc_2569237/ai_bs_64_user_bs_16_train_val_curves.png"}], []], "vlm_feedback_summary": ["All five batch\u2010size configurations achieve near\u2010perfect performance on the\nsynthetic binary dataset with no overfitting detected. Smaller AI batch sizes\n(16, 32) reach high accuracy faster, while larger validation batches provide\nslightly smoother loss curves. Training beyond epoch 8 yields diminishing\nreturns; consider early stopping or epoch reduction and fine\u2010tuned learning rate\nschedules to optimize training time without architecture changes.", "[]", "[]", "[]", "[]", "[]", "[]", "[]", "[]", "[]", "[]", "[]", "[]", "All batch-size configurations reach near-perfect accuracy on the synthetic\nbinary task and very low loss, with larger batch sizes delivering faster,\nsmoother convergence. Smaller batches introduce jitters and slower saturation.\nGiven the triviality of the synthetic task (all models saturate above 99% by\nepoch 10), it may be advisable to adopt more challenging datasets or incorporate\nricher metrics (e.g., trust calibration error and KL divergence of bias\nestimates) to better reveal performance differences among co-adaptive\nexplanation interfaces.", "All five batch-size configurations achieve very high training and validation\naccuracy (~99%+) with closely aligned loss curves, indicating strong\ngeneralization and minimal overfitting. Larger AI batch sizes speed initial\nconvergence, while varying the user batch size introduces only minor\noscillations in validation metrics. Overall robustness suggests the model is\nstable across batch-size settings.", "All five batch-size pairings produce rapid convergence: accuracy reaches roughly\n99 % within a few epochs, and loss drops to near zero with training and\nvalidation curves tracking each other closely. Smaller ai batch sizes show\nslightly slower initial progress but ultimately achieve comparable performance.\nThere is minimal overfitting across configurations, indicating the model and\nlearning rate are well suited to this synthetic binary task. To further stress-\ntest and differentiate interface designs, it would be valuable to introduce more\nchallenging datasets (e.g., imbalanced or noisy labels) or vary hyperparameters\nsuch as learning rate and regularization.", "[]"], "exec_time": [15.119279384613037, 0.8244144916534424, 0.441953182220459, 0.8270614147186279, 0.8059751987457275, 0.24397897720336914, 49.29011154174805, 0.6331286430358887, 3.026001453399658, 10.289295434951782, 12.342624187469482, 0.8209857940673828, 2.983574151992798, 15.855947971343994, 15.86950969696045, 15.07435154914856, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["ai_bs_64_user_bs_64", "ai_bs_32_user_bs_32", "ai_bs_16_user_bs_64", "ai_bs_16_user_bs_16", "ai_bs_64_user_bs_16"], [], [], [], [], [], [], [], [], [], [], [], [], ["[\"Synthetic binary dataset - 64_64\"", "\"Synthetic binary dataset - 32_32\"", "\"Synthetic binary dataset - 16_64\"", "\"Synthetic binary dataset - 16_16\"", "\"Synthetic binary dataset - 64_16\"]"], ["ai_bs_64_user_bs_64", "ai_bs_32_user_bs_32", "ai_bs_16_user_bs_64", "ai_bs_16_user_bs_16", "ai_bs_64_user_bs_16"], ["[ai_bs_64_user_bs_64", "ai_bs_32_user_bs_32", "ai_bs_16_user_bs_64", "ai_bs_16_user_bs_16", "ai_bs_64_user_bs_16]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Select five evenly spaced configurations\nkeys = list(exp[\"batch_size\"].keys())\nif len(keys) > 5:\n    idxs = [int(i * (len(keys) - 1) / 4) for i in range(5)]\nelse:\n    idxs = list(range(len(keys)))\nselected = [keys[i] for i in idxs]\n\n# Plot train/val accuracy and loss for each selected config\nfor key in selected:\n    try:\n        data = exp[\"batch_size\"][key]\n        epochs = range(1, len(data[\"metrics\"][\"train\"]) + 1)\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        # Accuracy subplot\n        axs[0].plot(epochs, data[\"metrics\"][\"train\"], label=\"Train\")\n        axs[0].plot(epochs, data[\"metrics\"][\"val\"], label=\"Validation\")\n        axs[0].set_title(\"Accuracy\")\n        axs[0].set_xlabel(\"Epoch\")\n        axs[0].legend()\n        # Loss subplot\n        axs[1].plot(epochs, data[\"losses\"][\"train\"], label=\"Train\")\n        axs[1].plot(epochs, data[\"losses\"][\"val\"], label=\"Validation\")\n        axs[1].set_title(\"Loss\")\n        axs[1].set_xlabel(\"Epoch\")\n        axs[1].legend()\n        # Composite title\n        fig.suptitle(f\"Synthetic binary dataset - {key}\")\n        # Save and close\n        fname = os.path.join(working_dir, f\"{key}_train_val_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {key}: {e}\")\n        plt.close()\n", null, null, null, null, null, null, null, null, null, null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Select five evenly spaced configurations\nkeys = list(exp[\"batch_size\"].keys())\nif len(keys) > 5:\n    idxs = [int(i * (len(keys) - 1) / 4) for i in range(5)]\nelse:\n    idxs = list(range(len(keys)))\nselected = [keys[i] for i in idxs]\n\n# Plot train/val accuracy and loss for each selected config\nfor key in selected:\n    try:\n        data = exp[\"batch_size\"][key]\n        epochs = range(1, len(data[\"metrics\"][\"train\"]) + 1)\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        # Accuracy subplot\n        axs[0].plot(epochs, data[\"metrics\"][\"train\"], label=\"Train\")\n        axs[0].plot(epochs, data[\"metrics\"][\"val\"], label=\"Validation\")\n        axs[0].set_title(\"Accuracy\")\n        axs[0].set_xlabel(\"Epoch\")\n        axs[0].legend()\n        # Loss subplot\n        axs[1].plot(epochs, data[\"losses\"][\"train\"], label=\"Train\")\n        axs[1].plot(epochs, data[\"losses\"][\"val\"], label=\"Validation\")\n        axs[1].set_title(\"Loss\")\n        axs[1].set_xlabel(\"Epoch\")\n        axs[1].legend()\n        # Composite title\n        fig.suptitle(f\"Synthetic binary dataset - {key}\")\n        # Save and close\n        fname = os.path.join(working_dir, f\"{key}_train_val_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {key}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Select five evenly spaced configurations\nkeys = list(exp[\"batch_size\"].keys())\nif len(keys) > 5:\n    idxs = [int(i * (len(keys) - 1) / 4) for i in range(5)]\nelse:\n    idxs = list(range(len(keys)))\nselected = [keys[i] for i in idxs]\n\n# Plot train/val accuracy and loss for each selected config\nfor key in selected:\n    try:\n        data = exp[\"batch_size\"][key]\n        epochs = range(1, len(data[\"metrics\"][\"train\"]) + 1)\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        # Accuracy subplot\n        axs[0].plot(epochs, data[\"metrics\"][\"train\"], label=\"Train\")\n        axs[0].plot(epochs, data[\"metrics\"][\"val\"], label=\"Validation\")\n        axs[0].set_title(\"Accuracy\")\n        axs[0].set_xlabel(\"Epoch\")\n        axs[0].legend()\n        # Loss subplot\n        axs[1].plot(epochs, data[\"losses\"][\"train\"], label=\"Train\")\n        axs[1].plot(epochs, data[\"losses\"][\"val\"], label=\"Validation\")\n        axs[1].set_title(\"Loss\")\n        axs[1].set_xlabel(\"Epoch\")\n        axs[1].legend()\n        # Composite title\n        fig.suptitle(f\"Synthetic binary dataset - {key}\")\n        # Save and close\n        fname = os.path.join(working_dir, f\"{key}_train_val_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {key}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Select five evenly spaced configurations\nkeys = list(exp[\"batch_size\"].keys())\nif len(keys) > 5:\n    idxs = [int(i * (len(keys) - 1) / 4) for i in range(5)]\nelse:\n    idxs = list(range(len(keys)))\nselected = [keys[i] for i in idxs]\n\n# Plot train/val accuracy and loss for each selected config\nfor key in selected:\n    try:\n        data = exp[\"batch_size\"][key]\n        epochs = range(1, len(data[\"metrics\"][\"train\"]) + 1)\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        # Accuracy subplot\n        axs[0].plot(epochs, data[\"metrics\"][\"train\"], label=\"Train\")\n        axs[0].plot(epochs, data[\"metrics\"][\"val\"], label=\"Validation\")\n        axs[0].set_title(\"Accuracy\")\n        axs[0].set_xlabel(\"Epoch\")\n        axs[0].legend()\n        # Loss subplot\n        axs[1].plot(epochs, data[\"losses\"][\"train\"], label=\"Train\")\n        axs[1].plot(epochs, data[\"losses\"][\"val\"], label=\"Validation\")\n        axs[1].set_title(\"Loss\")\n        axs[1].set_xlabel(\"Epoch\")\n        axs[1].legend()\n        # Composite title\n        fig.suptitle(f\"Synthetic binary dataset - {key}\")\n        # Save and close\n        fname = os.path.join(working_dir, f\"{key}_train_val_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {key}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Paths to experiment_data files relative to AI_SCIENTIST_ROOT\nexperiment_data_path_list = [\n    \"experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_12bdbf98f1184a0393723ce3fe12841d_proc_2569236/experiment_data.npy\",\n    \"experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7ad4f03fb5c54511b750217b607b4f1b_proc_2569238/experiment_data.npy\",\n    \"experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_bc59bb1520b04b6e831de7abd7f7da42_proc_2569237/experiment_data.npy\",\n]\n\n# Load all experiment data\ntry:\n    all_experiment_data = []\n    for exp_path in experiment_data_path_list:\n        loaded = np.load(\n            os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\"), exp_path), allow_pickle=True\n        ).item()\n        all_experiment_data.append(loaded)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Proceed only if data was loaded\nif all_experiment_data:\n    # Extract batch_size keys and select up to 5 evenly spaced\n    keys = list(all_experiment_data[0][\"batch_size\"].keys())\n    if len(keys) > 5:\n        idxs = [int(i * (len(keys) - 1) / 4) for i in range(5)]\n    else:\n        idxs = list(range(len(keys)))\n    selected_keys = [keys[i] for i in idxs]\n\n    # Plot aggregated mean and SEM for each key\n    for key in selected_keys:\n        try:\n            # Gather metrics and losses across runs\n            train_accs = np.array(\n                [\n                    exp[\"batch_size\"][key][\"metrics\"][\"train\"]\n                    for exp in all_experiment_data\n                ]\n            )\n            val_accs = np.array(\n                [\n                    exp[\"batch_size\"][key][\"metrics\"][\"val\"]\n                    for exp in all_experiment_data\n                ]\n            )\n            train_losses = np.array(\n                [\n                    exp[\"batch_size\"][key][\"losses\"][\"train\"]\n                    for exp in all_experiment_data\n                ]\n            )\n            val_losses = np.array(\n                [exp[\"batch_size\"][key][\"losses\"][\"val\"] for exp in all_experiment_data]\n            )\n\n            # Compute mean and standard error\n            mean_train = train_accs.mean(axis=0)\n            sem_train = train_accs.std(axis=0) / np.sqrt(train_accs.shape[0])\n            mean_val = val_accs.mean(axis=0)\n            sem_val = val_accs.std(axis=0) / np.sqrt(val_accs.shape[0])\n            mean_l_train = train_losses.mean(axis=0)\n            sem_l_train = train_losses.std(axis=0) / np.sqrt(train_losses.shape[0])\n            mean_l_val = val_losses.mean(axis=0)\n            sem_l_val = val_losses.std(axis=0) / np.sqrt(val_losses.shape[0])\n\n            epochs = np.arange(1, mean_train.shape[0] + 1)\n            fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n\n            # Accuracy subplot\n            axs[0].plot(epochs, mean_train, label=\"Train Mean\", color=\"blue\")\n            axs[0].fill_between(\n                epochs,\n                mean_train - sem_train,\n                mean_train + sem_train,\n                color=\"blue\",\n                alpha=0.3,\n                label=\"Train SEM\",\n            )\n            axs[0].plot(epochs, mean_val, label=\"Val Mean\", color=\"green\")\n            axs[0].fill_between(\n                epochs,\n                mean_val - sem_val,\n                mean_val + sem_val,\n                color=\"green\",\n                alpha=0.3,\n                label=\"Val SEM\",\n            )\n            axs[0].set_title(\"Accuracy\")\n            axs[0].set_xlabel(\"Epoch\")\n            axs[0].legend()\n\n            # Loss subplot\n            axs[1].plot(epochs, mean_l_train, label=\"Train Mean\", color=\"red\")\n            axs[1].fill_between(\n                epochs,\n                mean_l_train - sem_l_train,\n                mean_l_train + sem_l_train,\n                color=\"red\",\n                alpha=0.3,\n                label=\"Train SEM\",\n            )\n            axs[1].plot(epochs, mean_l_val, label=\"Val Mean\", color=\"orange\")\n            axs[1].fill_between(\n                epochs,\n                mean_l_val - sem_l_val,\n                mean_l_val + sem_l_val,\n                color=\"orange\",\n                alpha=0.3,\n                label=\"Val SEM\",\n            )\n            axs[1].set_title(\"Loss\")\n            axs[1].set_xlabel(\"Epoch\")\n            axs[1].legend()\n\n            fig.suptitle(\n                f\"Synthetic binary dataset - batch size {key} (Left: Accuracy, Right: Loss)\"\n            )\n            fname = os.path.join(\n                working_dir, f\"synthetic_binary_bs_{key}_aggregated_mean_sem.png\"\n            )\n            plt.savefig(fname)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating aggregated plot for {key}: {e}\")\n            plt.close()\n"], "plot_plan": ["We will load the saved experiment data and extract the batch\u2010size keys. To avoid overcrowding, we select exactly five configurations evenly spaced across all combinations. For each selected run, we create a single figure with two side\u2010by\u2010side subplots: one for training vs. validation accuracy, and one for training vs. validation loss over epochs. Each subplot is clearly titled (\u201cAccuracy\u201d and \u201cLoss\u201d) and we add a composite suptitle indicating the synthetic binary dataset and the specific batch\u2010size setting. All figures are saved into the working directory under descriptive filenames. Each plotting routine is wrapped in a try\u2010except to ensure errors are caught and figures closed.", null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": ["batch_size", null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["I will load the saved experiment data from the working directory, then loop over\neach hyperparameter setting under the \"batch_size\" field. For each dataset I\nextract the final training and validation accuracy and loss from the recorded\narrays, compute the test accuracy from the stored predictions and ground truths,\nand print the dataset name followed by clearly labeled metric values. The script\nruns immediately at global scope and does not use any special entry point.", "", "", "", "", "", "The following script constructs the working directory path, loads the\n`experiment_data.npy` file, and then iterates through each dataset and method to\nextract and print the final epoch\u2019s train loss, validation loss, train accuracy,\nvalidation accuracy, and alignment. Each metric is labeled with a clear name and\nformatted to four decimal places. The dataset name is printed before its\ncorresponding methods and metrics. The code executes immediately at the global\nscope with no special entry point.", "", "", "I will load the results dictionary from the saved NumPy file in the \u201cworking\u201d\ndirectory, then iterate over each dataset key to pull out its recorded training\nloss, validation loss, and alignment score lists. For each dataset I\u2019ll take the\nlast element of each list as the final value and then print the dataset name\nfollowed by clear labels and the corresponding values. The script runs at global\nscope and executes immediately on invocation.", "", "The script below loads the saved experiment data from the `working` directory\nand iterates over each dataset entry. For each dataset, it retrieves the final\ntraining and validation losses and accuracies for both the AI and simulated user\nmodels, as well as the final user alignment score. It also computes the user-\nmodel test accuracy from the stored predictions and ground truth labels. All\nmetrics are printed with clear, descriptive labels immediately upon execution.", "", "I will load the saved experiment data from the working directory, then loop over\neach hyperparameter setting under the \"batch_size\" field. For each dataset I\nextract the final training and validation accuracy and loss from the recorded\narrays, compute the test accuracy from the stored predictions and ground truths,\nand print the dataset name followed by clearly labeled metric values. The script\nruns immediately at global scope and does not use any special entry point.", "I will load the saved experiment data from the working directory, then loop over\neach hyperparameter setting under the \"batch_size\" field. For each dataset I\nextract the final training and validation accuracy and loss from the recorded\narrays, compute the test accuracy from the stored predictions and ground truths,\nand print the dataset name followed by clearly labeled metric values. The script\nruns immediately at global scope and does not use any special entry point.", "I will load the saved experiment data from the working directory, then loop over\neach hyperparameter setting under the \"batch_size\" field. For each dataset I\nextract the final training and validation accuracy and loss from the recorded\narrays, compute the test accuracy from the stored predictions and ground truths,\nand print the dataset name followed by clearly labeled metric values. The script\nruns immediately at global scope and does not use any special entry point.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset setting and print final metrics\nfor dataset_name, results in experiment_data.get(\"batch_size\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    # Final train accuracy and loss\n    train_acc = results[\"metrics\"][\"train\"][-1]\n    train_loss = results[\"losses\"][\"train\"][-1]\n    print(f\"train accuracy: {train_acc:.4f}\")\n    print(f\"train loss: {train_loss:.4f}\")\n    # Final validation accuracy and loss\n    val_acc = results[\"metrics\"][\"val\"][-1]\n    val_loss = results[\"losses\"][\"val\"][-1]\n    print(f\"validation accuracy: {val_acc:.4f}\")\n    print(f\"validation loss: {val_loss:.4f}\")\n    # Compute and print test accuracy\n    preds = results[\"predictions\"]\n    gt = results[\"ground_truth\"]\n    test_accuracy = np.mean(preds == gt)\n    print(f\"test accuracy: {test_accuracy:.4f}\\n\")\n", "", "", "", "", "", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through datasets and methods, printing final metric values\nfor dataset_name, methods in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n    for method_name, metrics in methods.items():\n        train_loss = metrics[\"train_loss\"][-1]\n        validation_loss = metrics[\"val_loss\"][-1]\n        train_accuracy = metrics[\"train_acc\"][-1]\n        validation_accuracy = metrics[\"val_acc\"][-1]\n        alignment_score = metrics[\"alignment\"][-1]\n        print(f\"  Method: {method_name}\")\n        print(f\"    train loss: {train_loss:.4f}\")\n        print(f\"    validation loss: {validation_loss:.4f}\")\n        print(f\"    train accuracy: {train_accuracy:.4f}\")\n        print(f\"    validation accuracy: {validation_accuracy:.4f}\")\n        print(f\"    alignment: {alignment_score:.4f}\")\n", "", "", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over datasets and print final metrics\nfor ds_name, ds_results in experiment_data.items():\n    metrics = ds_results[\"metrics\"]\n    final_train_loss = metrics[\"train\"][-1]\n    final_val_loss = metrics[\"val\"][-1]\n    final_alignment_score = metrics[\"alignment_score\"][-1]\n\n    print(ds_name)\n    print(f\"final training loss: {final_train_loss:.4f}\")\n    print(f\"final validation loss: {final_val_loss:.4f}\")\n    print(f\"final alignment score: {final_alignment_score:.4f}\")\n", "", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print the final metrics\nfor dataset_name, results in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # AI model metrics\n    ai_losses = results[\"ai\"][\"losses\"]\n    ai_metrics = results[\"ai\"][\"metrics\"]\n    print(f\"AI final train loss: {ai_losses['train'][-1]:.4f}\")\n    print(f\"AI final validation loss: {ai_losses['val'][-1]:.4f}\")\n    print(f\"AI final train accuracy: {ai_metrics['train_acc'][-1]:.4f}\")\n    print(f\"AI final validation accuracy: {ai_metrics['val_acc'][-1]:.4f}\")\n\n    # User-model metrics\n    user_losses = results[\"user\"][\"losses\"]\n    user_metrics = results[\"user\"][\"metrics\"]\n    print(f\"User final train loss: {user_losses['train'][-1]:.4f}\")\n    print(f\"User final validation loss: {user_losses['val'][-1]:.4f}\")\n    print(f\"User final train accuracy: {user_metrics['train_acc'][-1]:.4f}\")\n    print(f\"User final validation accuracy: {user_metrics['val_acc'][-1]:.4f}\")\n\n    # User alignment score\n    align_score = results[\"user\"][\"alignment_score\"][-1]\n    print(f\"User final alignment score: {align_score:.4f}\")\n\n    # Compute and print user-model test accuracy\n    preds = results[\"predictions\"]\n    ground_truth = results[\"ground_truth\"]\n    test_accuracy = np.mean(preds == ground_truth)\n    print(f\"User test accuracy: {test_accuracy:.4f}\\n\")\n", "", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset setting and print final metrics\nfor dataset_name, results in experiment_data.get(\"batch_size\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    # Final train accuracy and loss\n    train_acc = results[\"metrics\"][\"train\"][-1]\n    train_loss = results[\"losses\"][\"train\"][-1]\n    print(f\"train accuracy: {train_acc:.4f}\")\n    print(f\"train loss: {train_loss:.4f}\")\n    # Final validation accuracy and loss\n    val_acc = results[\"metrics\"][\"val\"][-1]\n    val_loss = results[\"losses\"][\"val\"][-1]\n    print(f\"validation accuracy: {val_acc:.4f}\")\n    print(f\"validation loss: {val_loss:.4f}\")\n    # Compute and print test accuracy\n    preds = results[\"predictions\"]\n    gt = results[\"ground_truth\"]\n    test_accuracy = np.mean(preds == gt)\n    print(f\"test accuracy: {test_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset setting and print final metrics\nfor dataset_name, results in experiment_data.get(\"batch_size\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    # Final train accuracy and loss\n    train_acc = results[\"metrics\"][\"train\"][-1]\n    train_loss = results[\"losses\"][\"train\"][-1]\n    print(f\"train accuracy: {train_acc:.4f}\")\n    print(f\"train loss: {train_loss:.4f}\")\n    # Final validation accuracy and loss\n    val_acc = results[\"metrics\"][\"val\"][-1]\n    val_loss = results[\"losses\"][\"val\"][-1]\n    print(f\"validation accuracy: {val_acc:.4f}\")\n    print(f\"validation loss: {val_loss:.4f}\")\n    # Compute and print test accuracy\n    preds = results[\"predictions\"]\n    gt = results[\"ground_truth\"]\n    test_accuracy = np.mean(preds == gt)\n    print(f\"test accuracy: {test_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset setting and print final metrics\nfor dataset_name, results in experiment_data.get(\"batch_size\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    # Final train accuracy and loss\n    train_acc = results[\"metrics\"][\"train\"][-1]\n    train_loss = results[\"losses\"][\"train\"][-1]\n    print(f\"train accuracy: {train_acc:.4f}\")\n    print(f\"train loss: {train_loss:.4f}\")\n    # Final validation accuracy and loss\n    val_acc = results[\"metrics\"][\"val\"][-1]\n    val_loss = results[\"losses\"][\"val\"][-1]\n    print(f\"validation accuracy: {val_acc:.4f}\")\n    print(f\"validation loss: {val_loss:.4f}\")\n    # Compute and print test accuracy\n    preds = results[\"predictions\"]\n    gt = results[\"ground_truth\"]\n    test_accuracy = np.mean(preds == gt)\n    print(f\"test accuracy: {test_accuracy:.4f}\\n\")\n", ""], "parse_term_out": ["['Dataset: ai_bs_16_user_bs_16', '\\n', 'train accuracy: 0.9933', '\\n', 'train\nloss: 0.0152', '\\n', 'validation accuracy: 0.9933', '\\n', 'validation loss:\n0.0095', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Dataset: ai_bs_16_user_bs_32',\n'\\n', 'train accuracy: 0.9942', '\\n', 'train loss: 0.0162', '\\n', 'validation\naccuracy: 1.0000', '\\n', 'validation loss: 0.0103', '\\n', 'test accuracy:\n0.9960\\n', '\\n', 'Dataset: ai_bs_16_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0195', '\\n', 'validation accuracy: 0.9933', '\\n',\n'validation loss: 0.0157', '\\n', 'test accuracy: 0.9960\\n', '\\n', 'Dataset:\nai_bs_32_user_bs_16', '\\n', 'train accuracy: 0.9958', '\\n', 'train loss:\n0.0140', '\\n', 'validation accuracy: 0.9967', '\\n', 'validation loss: 0.0083',\n'\\n', 'test accuracy: 0.9920\\n', '\\n', 'Dataset: ai_bs_32_user_bs_32', '\\n',\n'train accuracy: 0.9958', '\\n', 'train loss: 0.0142', '\\n', 'validation\naccuracy: 0.9967', '\\n', 'validation loss: 0.0212', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_32_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0186', '\\n', 'validation accuracy: 0.9967', '\\n',\n'validation loss: 0.0179', '\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset:\nai_bs_64_user_bs_16', '\\n', 'train accuracy: 0.9975', '\\n', 'train loss:\n0.0119', '\\n', 'validation accuracy: 0.9867', '\\n', 'validation loss: 0.0345',\n'\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_32', '\\n',\n'train accuracy: 0.9975', '\\n', 'train loss: 0.0138', '\\n', 'validation\naccuracy: 0.9900', '\\n', 'validation loss: 0.0182', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_64', '\\n', 'train accuracy: 0.9950',\n'\\n', 'train loss: 0.0188', '\\n', 'validation accuracy: 0.9900', '\\n',\n'validation loss: 0.0186', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "", "", "", "", "", "['Dataset: ag_news', '\\n', '  Method: static', '\\n', '    train loss: 0.9330',\n'\\n', '    validation loss: 1.0602', '\\n', '    train accuracy: 0.6030', '\\n', '\nvalidation accuracy: 0.5680', '\\n', '    alignment: 0.4920', '\\n', '  Method:\nsingle', '\\n', '    train loss: 0.9314', '\\n', '    validation loss: 1.0637',\n'\\n', '    train accuracy: 0.6170', '\\n', '    validation accuracy: 0.5700',\n'\\n', '    alignment: 0.8124', '\\n', '  Method: dual', '\\n', '    train loss:\n0.9342', '\\n', '    validation loss: 1.0615', '\\n', '    train accuracy:\n0.6095', '\\n', '    validation accuracy: 0.5600', '\\n', '    alignment: 0.9986',\n'\\n', 'Dataset: imdb', '\\n', '  Method: static', '\\n', '    train loss: 0.5707',\n'\\n', '    validation loss: 0.5976', '\\n', '    train accuracy: 0.7250', '\\n', '\nvalidation accuracy: 0.6980', '\\n', '    alignment: 0.5363', '\\n', '  Method:\nsingle', '\\n', '    train loss: 0.5734', '\\n', '    validation loss: 0.6183',\n'\\n', '    train accuracy: 0.7225', '\\n', '    validation accuracy: 0.6940',\n'\\n', '    alignment: 0.8320', '\\n', '  Method: dual', '\\n', '    train loss:\n0.5603', '\\n', '    validation loss: 0.6227', '\\n', '    train accuracy:\n0.7155', '\\n', '    validation accuracy: 0.6780', '\\n', '    alignment: 0.9972',\n'\\n', 'Dataset: yelp_review_full', '\\n', '  Method: static', '\\n', '    train\nloss: 1.2980', '\\n', '    validation loss: 1.5641', '\\n', '    train accuracy:\n0.4470', '\\n', '    validation accuracy: 0.3540', '\\n', '    alignment: 0.5302',\n'\\n', '  Method: single', '\\n', '    train loss: 1.2880', '\\n', '    validation\nloss: 1.5386', '\\n', '    train accuracy: 0.4515', '\\n', '    validation\naccuracy: 0.3640', '\\n', '    alignment: 0.8397', '\\n', '  Method: dual', '\\n',\n'    train loss: 1.2830', '\\n', '    validation loss: 1.5579', '\\n', '    train\naccuracy: 0.4505', '\\n', '    validation accuracy: 0.3600', '\\n', '\nalignment: 0.9990', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "", "", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 11, in\n<module>\\n    metrics = ds_results[\"metrics\"]\\n\n~~~~~~~~~~^^^^^^^^^^^\\nKeyError: \\'metrics\\'\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "", "['Dataset: ag_news', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 14, in <module>\\n    ai_losses = results[\"ai\"][\"losses\"]\\n\n~~~~~~~^^^^^^\\nKeyError: \\'ai\\'\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "", "['Dataset: ai_bs_16_user_bs_16', '\\n', 'train accuracy: 0.9933', '\\n', 'train\nloss: 0.0152', '\\n', 'validation accuracy: 0.9933', '\\n', 'validation loss:\n0.0095', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Dataset: ai_bs_16_user_bs_32',\n'\\n', 'train accuracy: 0.9942', '\\n', 'train loss: 0.0162', '\\n', 'validation\naccuracy: 1.0000', '\\n', 'validation loss: 0.0103', '\\n', 'test accuracy:\n0.9960\\n', '\\n', 'Dataset: ai_bs_16_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0195', '\\n', 'validation accuracy: 0.9933', '\\n',\n'validation loss: 0.0157', '\\n', 'test accuracy: 0.9960\\n', '\\n', 'Dataset:\nai_bs_32_user_bs_16', '\\n', 'train accuracy: 0.9958', '\\n', 'train loss:\n0.0140', '\\n', 'validation accuracy: 0.9967', '\\n', 'validation loss: 0.0083',\n'\\n', 'test accuracy: 0.9920\\n', '\\n', 'Dataset: ai_bs_32_user_bs_32', '\\n',\n'train accuracy: 0.9958', '\\n', 'train loss: 0.0142', '\\n', 'validation\naccuracy: 0.9967', '\\n', 'validation loss: 0.0212', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_32_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0186', '\\n', 'validation accuracy: 0.9967', '\\n',\n'validation loss: 0.0179', '\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset:\nai_bs_64_user_bs_16', '\\n', 'train accuracy: 0.9975', '\\n', 'train loss:\n0.0119', '\\n', 'validation accuracy: 0.9867', '\\n', 'validation loss: 0.0345',\n'\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_32', '\\n',\n'train accuracy: 0.9975', '\\n', 'train loss: 0.0138', '\\n', 'validation\naccuracy: 0.9900', '\\n', 'validation loss: 0.0182', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_64', '\\n', 'train accuracy: 0.9950',\n'\\n', 'train loss: 0.0188', '\\n', 'validation accuracy: 0.9900', '\\n',\n'validation loss: 0.0186', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: ai_bs_16_user_bs_16', '\\n', 'train accuracy: 0.9933', '\\n', 'train\nloss: 0.0152', '\\n', 'validation accuracy: 0.9933', '\\n', 'validation loss:\n0.0095', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Dataset: ai_bs_16_user_bs_32',\n'\\n', 'train accuracy: 0.9942', '\\n', 'train loss: 0.0162', '\\n', 'validation\naccuracy: 1.0000', '\\n', 'validation loss: 0.0103', '\\n', 'test accuracy:\n0.9960\\n', '\\n', 'Dataset: ai_bs_16_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0195', '\\n', 'validation accuracy: 0.9933', '\\n',\n'validation loss: 0.0157', '\\n', 'test accuracy: 0.9960\\n', '\\n', 'Dataset:\nai_bs_32_user_bs_16', '\\n', 'train accuracy: 0.9958', '\\n', 'train loss:\n0.0140', '\\n', 'validation accuracy: 0.9967', '\\n', 'validation loss: 0.0083',\n'\\n', 'test accuracy: 0.9920\\n', '\\n', 'Dataset: ai_bs_32_user_bs_32', '\\n',\n'train accuracy: 0.9958', '\\n', 'train loss: 0.0142', '\\n', 'validation\naccuracy: 0.9967', '\\n', 'validation loss: 0.0212', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_32_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0186', '\\n', 'validation accuracy: 0.9967', '\\n',\n'validation loss: 0.0179', '\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset:\nai_bs_64_user_bs_16', '\\n', 'train accuracy: 0.9975', '\\n', 'train loss:\n0.0119', '\\n', 'validation accuracy: 0.9867', '\\n', 'validation loss: 0.0345',\n'\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_32', '\\n',\n'train accuracy: 0.9975', '\\n', 'train loss: 0.0138', '\\n', 'validation\naccuracy: 0.9900', '\\n', 'validation loss: 0.0182', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_64', '\\n', 'train accuracy: 0.9950',\n'\\n', 'train loss: 0.0188', '\\n', 'validation accuracy: 0.9900', '\\n',\n'validation loss: 0.0186', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: ai_bs_16_user_bs_16', '\\n', 'train accuracy: 0.9933', '\\n', 'train\nloss: 0.0152', '\\n', 'validation accuracy: 0.9933', '\\n', 'validation loss:\n0.0095', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Dataset: ai_bs_16_user_bs_32',\n'\\n', 'train accuracy: 0.9942', '\\n', 'train loss: 0.0162', '\\n', 'validation\naccuracy: 1.0000', '\\n', 'validation loss: 0.0103', '\\n', 'test accuracy:\n0.9960\\n', '\\n', 'Dataset: ai_bs_16_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0195', '\\n', 'validation accuracy: 0.9933', '\\n',\n'validation loss: 0.0157', '\\n', 'test accuracy: 0.9960\\n', '\\n', 'Dataset:\nai_bs_32_user_bs_16', '\\n', 'train accuracy: 0.9958', '\\n', 'train loss:\n0.0140', '\\n', 'validation accuracy: 0.9967', '\\n', 'validation loss: 0.0083',\n'\\n', 'test accuracy: 0.9920\\n', '\\n', 'Dataset: ai_bs_32_user_bs_32', '\\n',\n'train accuracy: 0.9958', '\\n', 'train loss: 0.0142', '\\n', 'validation\naccuracy: 0.9967', '\\n', 'validation loss: 0.0212', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_32_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0186', '\\n', 'validation accuracy: 0.9967', '\\n',\n'validation loss: 0.0179', '\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset:\nai_bs_64_user_bs_16', '\\n', 'train accuracy: 0.9975', '\\n', 'train loss:\n0.0119', '\\n', 'validation accuracy: 0.9867', '\\n', 'validation loss: 0.0345',\n'\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_32', '\\n',\n'train accuracy: 0.9975', '\\n', 'train loss: 0.0138', '\\n', 'validation\naccuracy: 0.9900', '\\n', 'validation loss: 0.0182', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_64', '\\n', 'train accuracy: 0.9950',\n'\\n', 'train loss: 0.0188', '\\n', 'validation accuracy: 0.9900', '\\n',\n'validation loss: 0.0186', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, "KeyError", null, "KeyError", null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, {"args": ["metrics"]}, null, {"args": ["ai"]}, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 11, "<module>", "metrics = ds_results[\"metrics\"]"]], null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 14, "<module>", "ai_losses = results[\"ai\"][\"losses\"]"]], null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3"]}
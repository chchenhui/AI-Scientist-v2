{"edges": [[2, 6], [2, 5], [2, 4], [2, 3]], "layout": [[0.0, 0.0], [0.2857142857142857, 0.0], [0.5714285714285714, 0.0], [0.14285714285714285, 1.0], [0.42857142857142855, 1.0], [0.7142857142857143, 1.0], [1.0, 1.0]], "plan": ["We generate a small 2D synthetic binary classification dataset and split it into\ntraining and validation sets. A simple two\u2010layer MLP is defined in PyTorch and\nmoved to GPU if available. We simulate a human user\u2019s mental model as a fixed,\nbiased linear predictor on the same features and compute the mental model\nalignment accuracy at each epoch by comparing its predicted labels against the\nmodel\u2019s decisions. The classifier is trained for a few epochs with tracking of\ntraining/validation losses and alignment accuracies printed at each epoch. All\nmetrics, losses, the model\u2019s validation predictions, and ground truth labels are\nstored in an `experiment_data` dictionary. Finally, we save `experiment_data` as\na NumPy file in the working directory for downstream analysis.", "We generate a simple 2D binary classification problem, split and normalize the\ndata, then train a small MLP on the task using PyTorch. After each epoch, we\nfreeze the AI model, collect its predictions on the training set, and fit a\nlogistic regression as a proxy for a user's mental model to predict the AI\u2019s\nlabels. We then measure how often this \u201cuser\u201d model matches the AI on the\nheld\u2010out test set\u2014our Mental Model Alignment Accuracy. Throughout training we\nrecord both the AI\u2019s losses and accuracies as well as the alignment metric per\nepoch, print the validation loss, and at the end save all data arrays into a\nworking directory for further analysis.", "We implement a static baseline by generating a synthetic 2D binary\nclassification dataset and training a small MLP as our AI model. We extract\ncontent\u2010justification explanations by taking the AI model\u2019s predicted class\nprobabilities for each input. To simulate a user\u2019s mental model, we train a\nsimple neural \u201cuser network\u201d that takes both the normalized original features\nand the static explanation probabilities as inputs, and is trained to predict\nthe AI model\u2019s decisions. We split data into train, validation, and test sets,\nnormalizing based on training statistics. During user\u2010model training we track\nand print validation loss and compute the Mental Model Alignment Accuracy\u2014the\nfraction of validation examples on which the user model agrees with the AI\nmodel\u2014at each epoch. After training, we evaluate on a held\u2010out test set to\nreport final alignment accuracy. All metrics, losses, predictions, and\nground\u2010truth AI decisions are stored in an `experiment_data` dictionary and\nsaved to `./working/experiment_data.npy`. This gives a concise, working\nstatic\u2010explainer baseline for future co\u2010adaptive comparisons.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Seed for reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic dataset\nN = 1000\nX = np.random.randn(N, 2)\nw_true = np.array([1.0, 1.0])\ny = (X.dot(w_true) + 0.5 * np.random.randn(N) > 0).astype(int)\nsplit = int(0.8 * N)\nX_train, y_train = X[:split], y[:split]\nX_val, y_val = X[split:], y[split:]\n\n# Convert to torch tensors\nX_train = torch.FloatTensor(X_train)\ny_train = torch.LongTensor(y_train)\nX_val = torch.FloatTensor(X_val)\ny_val = torch.LongTensor(y_val)\n\n# DataLoaders\ntrain_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\nval_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=64)\n\n\n# Model definition\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 16)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(16, 2)\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n\n\nmodel = MLP().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# Simulated human mental model bias\nw_human = torch.tensor([0.5, -0.5], device=device)\n\n# Initialize experiment data\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(1, num_epochs + 1):\n    # Training\n    model.train()\n    total_train_loss = 0.0\n    for x, yb in train_loader:\n        x, yb = x.to(device), yb.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, yb)\n        loss.backward()\n        optimizer.step()\n        total_train_loss += loss.item() * x.size(0)\n    train_loss = total_train_loss / len(train_loader.dataset)\n\n    # Compute training alignment accuracy\n    model.eval()\n    correct_align_train = 0\n    with torch.no_grad():\n        for x, _ in train_loader:\n            x = x.to(device)\n            logits = model(x)\n            model_pred = logits.argmax(dim=1)\n            human_pred = (x @ w_human > 0).long()\n            correct_align_train += (human_pred == model_pred).sum().item()\n    train_alignment = correct_align_train / len(train_loader.dataset)\n\n    # Validation\n    total_val_loss = 0.0\n    correct_align_val = 0\n    val_preds = []\n    val_truth = []\n    with torch.no_grad():\n        for x, yb in val_loader:\n            x, yb = x.to(device), yb.to(device)\n            logits = model(x)\n            loss = criterion(logits, yb)\n            total_val_loss += loss.item() * x.size(0)\n            model_pred = logits.argmax(dim=1)\n            human_pred = (x @ w_human > 0).long()\n            correct_align_val += (human_pred == model_pred).sum().item()\n            val_preds.extend(model_pred.cpu().numpy().tolist())\n            val_truth.extend(yb.cpu().numpy().tolist())\n    val_loss = total_val_loss / len(val_loader.dataset)\n    val_alignment = correct_align_val / len(val_loader.dataset)\n\n    # Logging\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(train_alignment)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(val_alignment)\n    experiment_data[\"synthetic\"][\"predictions\"].append(np.array(val_preds))\n    experiment_data[\"synthetic\"][\"ground_truth\"].append(np.array(val_truth))\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# 1) Create and split synthetic 2D data\nnp.random.seed(0)\nn = 1000\nX0 = np.random.randn(n // 2, 2) + [-2, -2]\nX1 = np.random.randn(n - n // 2, 2) + [2, 2]\nX = np.vstack([X0, X1])\ny = np.hstack([np.zeros(n // 2), np.ones(n - n // 2)])\nX_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_tmp, y_tmp, test_size=0.5, random_state=42\n)\n\n# 2) Normalize features\nmean, std = X_train.mean(0), X_train.std(0)\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n# 3) Build PyTorch dataloaders\ntrain_ds = TensorDataset(\n    torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long()\n)\nval_ds = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=32)\n\n\n# 4) Define a simple MLP\nclass SimpleMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(2, 16), nn.ReLU(), nn.Linear(16, 2))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nmodel = SimpleMLP().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nloss_fn = nn.CrossEntropyLoss()\n\n# 5) Prepare data structure for logging\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"alignment\": [],\n    }\n}\n\n# 6) Training + co-adaptive user simulation loop\nepochs = 10\nfor epoch in range(1, epochs + 1):\n    # Train AI model\n    model.train()\n    train_loss = 0\n    correct_train = 0\n    total_train = 0\n    for Xb, yb in train_loader:\n        Xb, yb = Xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        logits = model(Xb)\n        loss = loss_fn(logits, yb)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * Xb.size(0)\n        preds = logits.argmax(1)\n        correct_train += (preds == yb).sum().item()\n        total_train += yb.size(0)\n    train_loss /= total_train\n    train_acc = correct_train / total_train\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(train_acc)\n\n    # Validate AI model\n    model.eval()\n    val_loss = 0\n    correct_val = 0\n    total_val = 0\n    with torch.no_grad():\n        for Xb, yb in val_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            logits = model(Xb)\n            loss = loss_fn(logits, yb)\n            val_loss += loss.item() * Xb.size(0)\n            preds = logits.argmax(1)\n            correct_val += (preds == yb).sum().item()\n            total_val += yb.size(0)\n    val_loss /= total_val\n    val_acc = correct_val / total_val\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(val_acc)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n    # Simulate user mental model via logistic regression\n    with torch.no_grad():\n        Xt_train = torch.from_numpy(X_train).float().to(device)\n        ai_train_preds = model(Xt_train).argmax(1).cpu().numpy()\n        Xt_test = torch.from_numpy(X_test).float().to(device)\n        ai_test_preds = model(Xt_test).argmax(1).cpu().numpy()\n    user_model = LogisticRegression()\n    user_model.fit(X_train, ai_train_preds)\n    user_pred = user_model.predict(X_test)\n    align_acc = np.mean(user_pred == ai_test_preds)\n    experiment_data[\"synthetic\"][\"alignment\"].append(align_acc)\n    experiment_data[\"synthetic\"][\"predictions\"].append(user_pred)\n    experiment_data[\"synthetic\"][\"ground_truth\"].append(ai_test_preds)\n\n# 7) Save all collected data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Synthetic dataset generation\nnp.random.seed(0)\nN = 2000\nD = 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# AI model definition\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Train AI model\nai_model = AIModel(D, 16, 2).to(device)\ncriterion_ai = nn.CrossEntropyLoss()\noptimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n\n\n# DataLoaders for AI\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nai_train_loader = DataLoader(SimpleDS(X_train, y_train), batch_size=32, shuffle=True)\nai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=32)\n\nfor epoch in range(15):\n    ai_model.train()\n    for batch in ai_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = ai_model(batch[\"x\"])\n        loss = criterion_ai(logits, batch[\"y\"])\n        optimizer_ai.zero_grad()\n        loss.backward()\n        optimizer_ai.step()\n    # validation\n    ai_model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in ai_val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = ai_model(batch[\"x\"])\n            val_loss += criterion_ai(out, batch[\"y\"]).item() * batch[\"x\"].size(0)\n    val_loss /= len(ai_val_loader.dataset)\n    print(f\"Epoch {epoch}: AI val loss = {val_loss:.4f}\")\n\n# Generate explanations (predicted probabilities)\nai_model.eval()\nwith torch.no_grad():\n    X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n    logits_all = ai_model(X_all)\n    probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n# Split explanations\np_train = probs_all[: len(X_train)]\np_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\np_test = probs_all[-len(X_test) :]\n# Ground truth AI decisions\nf_train = np.argmax(p_train, axis=1)\nf_val = np.argmax(p_val, axis=1)\nf_test = np.argmax(p_test, axis=1)\n\n# Prepare user dataset (features = [X, p], labels = f)\nX_usr_train = np.hstack([X_train, p_train])\nX_usr_val = np.hstack([X_val, p_val])\nX_usr_test = np.hstack([X_test, p_test])\n\n\n# User Dataset & DataLoaders\nclass UserDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\nusr_train_loader = DataLoader(UserDS(X_usr_train, f_train), batch_size=32, shuffle=True)\nusr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=32)\nusr_test_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=32)\n\n\n# User model\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nuser_model = UserModel(D + 2, 8, 2).to(device)\ncriterion_usr = nn.CrossEntropyLoss()\noptimizer_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n\n# Experiment data\nexperiment_data = {\n    \"static_explainer\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# Train user model and track metrics\nfor epoch in range(20):\n    user_model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for batch in usr_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = user_model(batch[\"feat\"])\n        loss = criterion_usr(out, batch[\"label\"])\n        optimizer_usr.zero_grad()\n        loss.backward()\n        optimizer_usr.step()\n        total_loss += loss.item() * batch[\"feat\"].size(0)\n        preds = out.argmax(dim=1)\n        correct += (preds == batch[\"label\"]).sum().item()\n        total += len(batch[\"label\"])\n    train_loss = total_loss / total\n    train_acc = correct / total\n    experiment_data[\"static_explainer\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"static_explainer\"][\"metrics\"][\"train\"].append(train_acc)\n\n    # validation\n    user_model.eval()\n    val_loss, val_corr, val_tot = 0.0, 0, 0\n    with torch.no_grad():\n        for batch in usr_val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = user_model(batch[\"feat\"])\n            val_loss += criterion_usr(out, batch[\"label\"]).item() * batch[\"feat\"].size(\n                0\n            )\n            preds = out.argmax(dim=1)\n            val_corr += (preds == batch[\"label\"]).sum().item()\n            val_tot += len(batch[\"label\"])\n    val_loss /= val_tot\n    val_acc = val_corr / val_tot\n    experiment_data[\"static_explainer\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"static_explainer\"][\"metrics\"][\"val\"].append(val_acc)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n# Test evaluation\nuser_model.eval()\ntest_preds, test_gt = [], []\nwith torch.no_grad():\n    for batch in usr_test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = user_model(batch[\"feat\"])\n        p = out.argmax(dim=1).cpu().numpy().tolist()\n        test_preds.extend(p)\n        test_gt.extend(batch[\"label\"].cpu().numpy().tolist())\n\nexperiment_data[\"static_explainer\"][\"predictions\"] = test_preds\nexperiment_data[\"static_explainer\"][\"ground_truth\"] = test_gt\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Synthetic dataset generation\nnp.random.seed(0)\nN = 2000\nD = 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# AI model definition\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Train AI model\nai_model = AIModel(D, 16, 2).to(device)\ncriterion_ai = nn.CrossEntropyLoss()\noptimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n\n\n# DataLoaders for AI\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nai_train_loader = DataLoader(SimpleDS(X_train, y_train), batch_size=32, shuffle=True)\nai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=32)\n\nfor epoch in range(15):\n    ai_model.train()\n    for batch in ai_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = ai_model(batch[\"x\"])\n        loss = criterion_ai(logits, batch[\"y\"])\n        optimizer_ai.zero_grad()\n        loss.backward()\n        optimizer_ai.step()\n    # validation\n    ai_model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in ai_val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = ai_model(batch[\"x\"])\n            val_loss += criterion_ai(out, batch[\"y\"]).item() * batch[\"x\"].size(0)\n    val_loss /= len(ai_val_loader.dataset)\n    print(f\"Epoch {epoch}: AI val loss = {val_loss:.4f}\")\n\n# Generate explanations (predicted probabilities)\nai_model.eval()\nwith torch.no_grad():\n    X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n    logits_all = ai_model(X_all)\n    probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n# Split explanations\np_train = probs_all[: len(X_train)]\np_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\np_test = probs_all[-len(X_test) :]\n# Ground truth AI decisions\nf_train = np.argmax(p_train, axis=1)\nf_val = np.argmax(p_val, axis=1)\nf_test = np.argmax(p_test, axis=1)\n\n# Prepare user dataset (features = [X, p], labels = f)\nX_usr_train = np.hstack([X_train, p_train])\nX_usr_val = np.hstack([X_val, p_val])\nX_usr_test = np.hstack([X_test, p_test])\n\n\n# User Dataset & DataLoaders\nclass UserDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\nusr_train_loader = DataLoader(UserDS(X_usr_train, f_train), batch_size=32, shuffle=True)\nusr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=32)\nusr_test_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=32)\n\n\n# User model\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nuser_model = UserModel(D + 2, 8, 2).to(device)\ncriterion_usr = nn.CrossEntropyLoss()\noptimizer_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n\n# Experiment data\nexperiment_data = {\n    \"static_explainer\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# Train user model and track metrics\nfor epoch in range(20):\n    user_model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for batch in usr_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = user_model(batch[\"feat\"])\n        loss = criterion_usr(out, batch[\"label\"])\n        optimizer_usr.zero_grad()\n        loss.backward()\n        optimizer_usr.step()\n        total_loss += loss.item() * batch[\"feat\"].size(0)\n        preds = out.argmax(dim=1)\n        correct += (preds == batch[\"label\"]).sum().item()\n        total += len(batch[\"label\"])\n    train_loss = total_loss / total\n    train_acc = correct / total\n    experiment_data[\"static_explainer\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"static_explainer\"][\"metrics\"][\"train\"].append(train_acc)\n\n    # validation\n    user_model.eval()\n    val_loss, val_corr, val_tot = 0.0, 0, 0\n    with torch.no_grad():\n        for batch in usr_val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = user_model(batch[\"feat\"])\n            val_loss += criterion_usr(out, batch[\"label\"]).item() * batch[\"feat\"].size(\n                0\n            )\n            preds = out.argmax(dim=1)\n            val_corr += (preds == batch[\"label\"]).sum().item()\n            val_tot += len(batch[\"label\"])\n    val_loss /= val_tot\n    val_acc = val_corr / val_tot\n    experiment_data[\"static_explainer\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"static_explainer\"][\"metrics\"][\"val\"].append(val_acc)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n# Test evaluation\nuser_model.eval()\ntest_preds, test_gt = [], []\nwith torch.no_grad():\n    for batch in usr_test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = user_model(batch[\"feat\"])\n        p = out.argmax(dim=1).cpu().numpy().tolist()\n        test_preds.extend(p)\n        test_gt.extend(batch[\"label\"].cpu().numpy().tolist())\n\nexperiment_data[\"static_explainer\"][\"predictions\"] = test_preds\nexperiment_data[\"static_explainer\"][\"ground_truth\"] = test_gt\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Synthetic dataset generation\nnp.random.seed(0)\nN = 2000\nD = 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# AI model definition\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Train AI model\nai_model = AIModel(D, 16, 2).to(device)\ncriterion_ai = nn.CrossEntropyLoss()\noptimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n\n\n# DataLoaders for AI\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nai_train_loader = DataLoader(SimpleDS(X_train, y_train), batch_size=32, shuffle=True)\nai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=32)\n\nfor epoch in range(15):\n    ai_model.train()\n    for batch in ai_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = ai_model(batch[\"x\"])\n        loss = criterion_ai(logits, batch[\"y\"])\n        optimizer_ai.zero_grad()\n        loss.backward()\n        optimizer_ai.step()\n    # validation\n    ai_model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in ai_val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = ai_model(batch[\"x\"])\n            val_loss += criterion_ai(out, batch[\"y\"]).item() * batch[\"x\"].size(0)\n    val_loss /= len(ai_val_loader.dataset)\n    print(f\"Epoch {epoch}: AI val loss = {val_loss:.4f}\")\n\n# Generate explanations (predicted probabilities)\nai_model.eval()\nwith torch.no_grad():\n    X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n    logits_all = ai_model(X_all)\n    probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n# Split explanations\np_train = probs_all[: len(X_train)]\np_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\np_test = probs_all[-len(X_test) :]\n# Ground truth AI decisions\nf_train = np.argmax(p_train, axis=1)\nf_val = np.argmax(p_val, axis=1)\nf_test = np.argmax(p_test, axis=1)\n\n# Prepare user dataset (features = [X, p], labels = f)\nX_usr_train = np.hstack([X_train, p_train])\nX_usr_val = np.hstack([X_val, p_val])\nX_usr_test = np.hstack([X_test, p_test])\n\n\n# User Dataset & DataLoaders\nclass UserDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\nusr_train_loader = DataLoader(UserDS(X_usr_train, f_train), batch_size=32, shuffle=True)\nusr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=32)\nusr_test_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=32)\n\n\n# User model\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nuser_model = UserModel(D + 2, 8, 2).to(device)\ncriterion_usr = nn.CrossEntropyLoss()\noptimizer_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n\n# Experiment data\nexperiment_data = {\n    \"static_explainer\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# Train user model and track metrics\nfor epoch in range(20):\n    user_model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for batch in usr_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = user_model(batch[\"feat\"])\n        loss = criterion_usr(out, batch[\"label\"])\n        optimizer_usr.zero_grad()\n        loss.backward()\n        optimizer_usr.step()\n        total_loss += loss.item() * batch[\"feat\"].size(0)\n        preds = out.argmax(dim=1)\n        correct += (preds == batch[\"label\"]).sum().item()\n        total += len(batch[\"label\"])\n    train_loss = total_loss / total\n    train_acc = correct / total\n    experiment_data[\"static_explainer\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"static_explainer\"][\"metrics\"][\"train\"].append(train_acc)\n\n    # validation\n    user_model.eval()\n    val_loss, val_corr, val_tot = 0.0, 0, 0\n    with torch.no_grad():\n        for batch in usr_val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = user_model(batch[\"feat\"])\n            val_loss += criterion_usr(out, batch[\"label\"]).item() * batch[\"feat\"].size(\n                0\n            )\n            preds = out.argmax(dim=1)\n            val_corr += (preds == batch[\"label\"]).sum().item()\n            val_tot += len(batch[\"label\"])\n    val_loss /= val_tot\n    val_acc = val_corr / val_tot\n    experiment_data[\"static_explainer\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"static_explainer\"][\"metrics\"][\"val\"].append(val_acc)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n# Test evaluation\nuser_model.eval()\ntest_preds, test_gt = [], []\nwith torch.no_grad():\n    for batch in usr_test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = user_model(batch[\"feat\"])\n        p = out.argmax(dim=1).cpu().numpy().tolist()\n        test_preds.extend(p)\n        test_gt.extend(batch[\"label\"].cpu().numpy().tolist())\n\nexperiment_data[\"static_explainer\"][\"predictions\"] = test_preds\nexperiment_data[\"static_explainer\"][\"ground_truth\"] = test_gt\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Synthetic dataset generation\nnp.random.seed(0)\nN = 2000\nD = 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# AI model definition\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Train AI model\nai_model = AIModel(D, 16, 2).to(device)\ncriterion_ai = nn.CrossEntropyLoss()\noptimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n\n\n# DataLoaders for AI\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nai_train_loader = DataLoader(SimpleDS(X_train, y_train), batch_size=32, shuffle=True)\nai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=32)\n\nfor epoch in range(15):\n    ai_model.train()\n    for batch in ai_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = ai_model(batch[\"x\"])\n        loss = criterion_ai(logits, batch[\"y\"])\n        optimizer_ai.zero_grad()\n        loss.backward()\n        optimizer_ai.step()\n    # validation\n    ai_model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in ai_val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = ai_model(batch[\"x\"])\n            val_loss += criterion_ai(out, batch[\"y\"]).item() * batch[\"x\"].size(0)\n    val_loss /= len(ai_val_loader.dataset)\n    print(f\"Epoch {epoch}: AI val loss = {val_loss:.4f}\")\n\n# Generate explanations (predicted probabilities)\nai_model.eval()\nwith torch.no_grad():\n    X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n    logits_all = ai_model(X_all)\n    probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n# Split explanations\np_train = probs_all[: len(X_train)]\np_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\np_test = probs_all[-len(X_test) :]\n# Ground truth AI decisions\nf_train = np.argmax(p_train, axis=1)\nf_val = np.argmax(p_val, axis=1)\nf_test = np.argmax(p_test, axis=1)\n\n# Prepare user dataset (features = [X, p], labels = f)\nX_usr_train = np.hstack([X_train, p_train])\nX_usr_val = np.hstack([X_val, p_val])\nX_usr_test = np.hstack([X_test, p_test])\n\n\n# User Dataset & DataLoaders\nclass UserDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\nusr_train_loader = DataLoader(UserDS(X_usr_train, f_train), batch_size=32, shuffle=True)\nusr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=32)\nusr_test_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=32)\n\n\n# User model\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nuser_model = UserModel(D + 2, 8, 2).to(device)\ncriterion_usr = nn.CrossEntropyLoss()\noptimizer_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n\n# Experiment data\nexperiment_data = {\n    \"static_explainer\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# Train user model and track metrics\nfor epoch in range(20):\n    user_model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for batch in usr_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = user_model(batch[\"feat\"])\n        loss = criterion_usr(out, batch[\"label\"])\n        optimizer_usr.zero_grad()\n        loss.backward()\n        optimizer_usr.step()\n        total_loss += loss.item() * batch[\"feat\"].size(0)\n        preds = out.argmax(dim=1)\n        correct += (preds == batch[\"label\"]).sum().item()\n        total += len(batch[\"label\"])\n    train_loss = total_loss / total\n    train_acc = correct / total\n    experiment_data[\"static_explainer\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"static_explainer\"][\"metrics\"][\"train\"].append(train_acc)\n\n    # validation\n    user_model.eval()\n    val_loss, val_corr, val_tot = 0.0, 0, 0\n    with torch.no_grad():\n        for batch in usr_val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = user_model(batch[\"feat\"])\n            val_loss += criterion_usr(out, batch[\"label\"]).item() * batch[\"feat\"].size(\n                0\n            )\n            preds = out.argmax(dim=1)\n            val_corr += (preds == batch[\"label\"]).sum().item()\n            val_tot += len(batch[\"label\"])\n    val_loss /= val_tot\n    val_acc = val_corr / val_tot\n    experiment_data[\"static_explainer\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"static_explainer\"][\"metrics\"][\"val\"].append(val_acc)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n# Test evaluation\nuser_model.eval()\ntest_preds, test_gt = [], []\nwith torch.no_grad():\n    for batch in usr_test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = user_model(batch[\"feat\"])\n        p = out.argmax(dim=1).cpu().numpy().tolist()\n        test_preds.extend(p)\n        test_gt.extend(batch[\"label\"].cpu().numpy().tolist())\n\nexperiment_data[\"static_explainer\"][\"predictions\"] = test_preds\nexperiment_data[\"static_explainer\"][\"ground_truth\"] = test_gt\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 0.6428', '\\n', 'Epoch\n2: validation_loss = 0.6059', '\\n', 'Epoch 3: validation_loss = 0.5730', '\\n',\n'Epoch 4: validation_loss = 0.5424', '\\n', 'Epoch 5: validation_loss = 0.5152',\n'\\n', 'Epoch 6: validation_loss = 0.4902', '\\n', 'Epoch 7: validation_loss =\n0.4667', '\\n', 'Epoch 8: validation_loss = 0.4447', '\\n', 'Epoch 9:\nvalidation_loss = 0.4242', '\\n', 'Epoch 10: validation_loss = 0.4054', '\\n',\n'Execution time: a second seconds (time limit is an hour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 10, in\n<module>\\n    from sklearn.model_selection import\ntrain_test_split\\nModuleNotFoundError: No module named \\'sklearn\\'\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: AI val loss = 0.3713', '\\n', 'Epoch 1: AI\nval loss = 0.3778', '\\n', 'Epoch 2: AI val loss = 0.3716', '\\n', 'Epoch 3: AI\nval loss = 0.3739', '\\n', 'Epoch 4: AI val loss = 0.3740', '\\n', 'Epoch 5: AI\nval loss = 0.3715', '\\n', 'Epoch 6: AI val loss = 0.3738', '\\n', 'Epoch 7: AI\nval loss = 0.3714', '\\n', 'Epoch 8: AI val loss = 0.3680', '\\n', 'Epoch 9: AI\nval loss = 0.3694', '\\n', 'Epoch 10: AI val loss = 0.3687', '\\n', 'Epoch 11: AI\nval loss = 0.3689', '\\n', 'Epoch 12: AI val loss = 0.3705', '\\n', 'Epoch 13: AI\nval loss = 0.3700', '\\n', 'Epoch 14: AI val loss = 0.3719', '\\n', 'Epoch 0:\nvalidation_loss = 0.1379', '\\n', 'Epoch 1: validation_loss = 0.0539', '\\n',\n'Epoch 2: validation_loss = 0.0401', '\\n', 'Epoch 3: validation_loss = 0.0310',\n'\\n', 'Epoch 4: validation_loss = 0.0261', '\\n', 'Epoch 5: validation_loss =\n0.0245', '\\n', 'Epoch 6: validation_loss = 0.0214', '\\n', 'Epoch 7:\nvalidation_loss = 0.0216', '\\n', 'Epoch 8: validation_loss = 0.0175', '\\n',\n'Epoch 9: validation_loss = 0.0160', '\\n', 'Epoch 10: validation_loss = 0.0183',\n'\\n', 'Epoch 11: validation_loss = 0.0148', '\\n', 'Epoch 12: validation_loss =\n0.0155', '\\n', 'Epoch 13: validation_loss = 0.0130', '\\n', 'Epoch 14:\nvalidation_loss = 0.0133', '\\n', 'Epoch 15: validation_loss = 0.0157', '\\n',\n'Epoch 16: validation_loss = 0.0122', '\\n', 'Epoch 17: validation_loss =\n0.0123', '\\n', 'Epoch 18: validation_loss = 0.0124', '\\n', 'Epoch 19:\nvalidation_loss = 0.0123', '\\n', 'Execution time: 3 seconds seconds (time limit\nis an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: AI val loss = 0.3662', '\\n', 'Epoch 1: AI\nval loss = 0.3742', '\\n', 'Epoch 2: AI val loss = 0.3729', '\\n', 'Epoch 3: AI\nval loss = 0.3739', '\\n', 'Epoch 4: AI val loss = 0.3710', '\\n', 'Epoch 5: AI\nval loss = 0.3742', '\\n', 'Epoch 6: AI val loss = 0.3787', '\\n', 'Epoch 7: AI\nval loss = 0.3756', '\\n', 'Epoch 8: AI val loss = 0.3714', '\\n', 'Epoch 9: AI\nval loss = 0.3750', '\\n', 'Epoch 10: AI val loss = 0.3695', '\\n', 'Epoch 11: AI\nval loss = 0.3748', '\\n', 'Epoch 12: AI val loss = 0.3780', '\\n', 'Epoch 13: AI\nval loss = 0.3749', '\\n', 'Epoch 14: AI val loss = 0.3752', '\\n', 'Epoch 0:\nvalidation_loss = 0.1184', '\\n', 'Epoch 1: validation_loss = 0.0471', '\\n',\n'Epoch 2: validation_loss = 0.0359', '\\n', 'Epoch 3: validation_loss = 0.0291',\n'\\n', 'Epoch 4: validation_loss = 0.0246', '\\n', 'Epoch 5: validation_loss =\n0.0214', '\\n', 'Epoch 6: validation_loss = 0.0203', '\\n', 'Epoch 7:\nvalidation_loss = 0.0192', '\\n', 'Epoch 8: validation_loss = 0.0177', '\\n',\n'Epoch 9: validation_loss = 0.0162', '\\n', 'Epoch 10: validation_loss = 0.0174',\n'\\n', 'Epoch 11: validation_loss = 0.0163', '\\n', 'Epoch 12: validation_loss =\n0.0208', '\\n', 'Epoch 13: validation_loss = 0.0191', '\\n', 'Epoch 14:\nvalidation_loss = 0.0158', '\\n', 'Epoch 15: validation_loss = 0.0167', '\\n',\n'Epoch 16: validation_loss = 0.0123', '\\n', 'Epoch 17: validation_loss =\n0.0138', '\\n', 'Epoch 18: validation_loss = 0.0189', '\\n', 'Epoch 19:\nvalidation_loss = 0.0170', '\\n', 'Execution time: 3 seconds seconds (time limit\nis an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: AI val loss = 0.3662', '\\n', 'Epoch 1: AI\nval loss = 0.3742', '\\n', 'Epoch 2: AI val loss = 0.3729', '\\n', 'Epoch 3: AI\nval loss = 0.3739', '\\n', 'Epoch 4: AI val loss = 0.3710', '\\n', 'Epoch 5: AI\nval loss = 0.3742', '\\n', 'Epoch 6: AI val loss = 0.3787', '\\n', 'Epoch 7: AI\nval loss = 0.3756', '\\n', 'Epoch 8: AI val loss = 0.3714', '\\n', 'Epoch 9: AI\nval loss = 0.3750', '\\n', 'Epoch 10: AI val loss = 0.3695', '\\n', 'Epoch 11: AI\nval loss = 0.3748', '\\n', 'Epoch 12: AI val loss = 0.3780', '\\n', 'Epoch 13: AI\nval loss = 0.3749', '\\n', 'Epoch 14: AI val loss = 0.3752', '\\n', 'Epoch 0:\nvalidation_loss = 0.1184', '\\n', 'Epoch 1: validation_loss = 0.0471', '\\n',\n'Epoch 2: validation_loss = 0.0359', '\\n', 'Epoch 3: validation_loss = 0.0291',\n'\\n', 'Epoch 4: validation_loss = 0.0246', '\\n', 'Epoch 5: validation_loss =\n0.0214', '\\n', 'Epoch 6: validation_loss = 0.0203', '\\n', 'Epoch 7:\nvalidation_loss = 0.0192', '\\n', 'Epoch 8: validation_loss = 0.0177', '\\n',\n'Epoch 9: validation_loss = 0.0162', '\\n', 'Epoch 10: validation_loss = 0.0174',\n'\\n', 'Epoch 11: validation_loss = 0.0163', '\\n', 'Epoch 12: validation_loss =\n0.0208', '\\n', 'Epoch 13: validation_loss = 0.0191', '\\n', 'Epoch 14:\nvalidation_loss = 0.0158', '\\n', 'Epoch 15: validation_loss = 0.0167', '\\n',\n'Epoch 16: validation_loss = 0.0123', '\\n', 'Epoch 17: validation_loss =\n0.0138', '\\n', 'Epoch 18: validation_loss = 0.0189', '\\n', 'Epoch 19:\nvalidation_loss = 0.0170', '\\n', 'Execution time: 3 seconds seconds (time limit\nis an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: AI val loss = 0.3659', '\\n', 'Epoch 1: AI\nval loss = 0.3749', '\\n', 'Epoch 2: AI val loss = 0.3696', '\\n', 'Epoch 3: AI\nval loss = 0.3727', '\\n', 'Epoch 4: AI val loss = 0.3705', '\\n', 'Epoch 5: AI\nval loss = 0.3692', '\\n', 'Epoch 6: AI val loss = 0.3675', '\\n', 'Epoch 7: AI\nval loss = 0.3763', '\\n', 'Epoch 8: AI val loss = 0.3722', '\\n', 'Epoch 9: AI\nval loss = 0.3714', '\\n', 'Epoch 10: AI val loss = 0.3712', '\\n', 'Epoch 11: AI\nval loss = 0.3705', '\\n', 'Epoch 12: AI val loss = 0.3718', '\\n', 'Epoch 13: AI\nval loss = 0.3731', '\\n', 'Epoch 14: AI val loss = 0.3696', '\\n', 'Epoch 0:\nvalidation_loss = 0.1682', '\\n', 'Epoch 1: validation_loss = 0.0611', '\\n',\n'Epoch 2: validation_loss = 0.0409', '\\n', 'Epoch 3: validation_loss = 0.0345',\n'\\n', 'Epoch 4: validation_loss = 0.0283', '\\n', 'Epoch 5: validation_loss =\n0.0257', '\\n', 'Epoch 6: validation_loss = 0.0226', '\\n', 'Epoch 7:\nvalidation_loss = 0.0209', '\\n', 'Epoch 8: validation_loss = 0.0185', '\\n',\n'Epoch 9: validation_loss = 0.0212', '\\n', 'Epoch 10: validation_loss = 0.0186',\n'\\n', 'Epoch 11: validation_loss = 0.0195', '\\n', 'Epoch 12: validation_loss =\n0.0144', '\\n', 'Epoch 13: validation_loss = 0.0153', '\\n', 'Epoch 14:\nvalidation_loss = 0.0127', '\\n', 'Epoch 15: validation_loss = 0.0206', '\\n',\n'Epoch 16: validation_loss = 0.0125', '\\n', 'Epoch 17: validation_loss =\n0.0131', '\\n', 'Epoch 18: validation_loss = 0.0108', '\\n', 'Epoch 19:\nvalidation_loss = 0.0112', '\\n', 'Execution time: 3 seconds seconds (time limit\nis an hour).']", ""], "analysis": ["", "The script fails immediately with ModuleNotFoundError: No module named\n'sklearn'. The environment lacks scikit-learn. To fix this, ensure scikit-learn\nis installed (e.g., add `pip install scikit-learn` or include scikit-learn in\ndependencies), or replace calls to train_test_split with a custom NumPy-based\nsplit.", "", "", "", "", ""], "exc_type": [null, "ModuleNotFoundError", null, null, null, null, null], "exc_info": [null, {"args": ["No module named 'sklearn'"], "name": "sklearn", "msg": "No module named 'sklearn'"}, null, null, null, null, null], "exc_stack": [null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 10, "<module>", "from sklearn.model_selection import train_test_split"]], null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.4106, "best_value": 0.4106}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.4054, "best_value": 0.4054}]}, {"metric_name": "train alignment accuracy", "lower_is_better": false, "description": "Alignment accuracy on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.495, "best_value": 0.495}]}, {"metric_name": "validation alignment accuracy", "lower_is_better": false, "description": "Alignment accuracy on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.47, "best_value": 0.47}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset", "data": [{"dataset_name": "Dataset 1", "final_value": 0.9958333333333333, "best_value": 0.9958333333333333}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "Dataset 1", "final_value": 0.012646730159176514, "best_value": 0.012646730159176514}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset", "data": [{"dataset_name": "Dataset 2", "final_value": 0.9966666666666667, "best_value": 0.9966666666666667}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "Dataset 2", "final_value": 0.012335669596542781, "best_value": 0.012335669596542781}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset", "data": [{"dataset_name": "Dataset 3", "final_value": 0.99, "best_value": 0.99}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training data", "data": [{"dataset_name": "development set", "final_value": 0.9941666666666666, "best_value": 0.9941666666666666}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training data", "data": [{"dataset_name": "development set", "final_value": 0.014887199808144942, "best_value": 0.014887199808144942}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation data", "data": [{"dataset_name": "hold-out set", "final_value": 0.99, "best_value": 0.99}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation data", "data": [{"dataset_name": "hold-out set", "final_value": 0.016995876728626628, "best_value": 0.016995876728626628}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test data", "data": [{"dataset_name": "unseen set", "final_value": 0.992, "best_value": 0.992}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset", "data": [{"dataset_name": "Training", "final_value": 0.9941666666666666, "best_value": 0.9941666666666666}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "Training", "final_value": 0.014887199808144942, "best_value": 0.014887199808144942}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset", "data": [{"dataset_name": "Validation", "final_value": 0.99, "best_value": 0.99}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "Validation", "final_value": 0.016995876728626628, "best_value": 0.016995876728626628}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset", "data": [{"dataset_name": "Test", "final_value": 0.992, "best_value": 0.992}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "Model accuracy", "data": [{"dataset_name": "Training dataset", "final_value": 0.9966666666666667, "best_value": 0.9966666666666667}, {"dataset_name": "Validation dataset", "final_value": 0.9966666666666667, "best_value": 0.9966666666666667}, {"dataset_name": "Test dataset", "final_value": 0.996, "best_value": 0.996}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Model loss", "data": [{"dataset_name": "Training dataset", "final_value": 0.011508071264446092, "best_value": 0.011508071264446092}, {"dataset_name": "Validation dataset", "final_value": 0.011210518143294526, "best_value": 0.011210518143294526}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, true, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_confusion_epoch10.png", "../../logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_confusion_epoch5.png", "../../logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_confusion_epoch3.png", "../../logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_confusion_epoch1.png", "../../logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_confusion_epoch7.png", "../../logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_alignment_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_accuracy.png", "../../logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_loss.png", "../../logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_test_distribution.png"], ["../../logs/0-run/experiment_results/experiment_d8ba5ee3c5a44f94b002565339e07c2a_proc_2561565/static_explainer_accuracy.png", "../../logs/0-run/experiment_results/experiment_d8ba5ee3c5a44f94b002565339e07c2a_proc_2561565/static_explainer_loss.png", "../../logs/0-run/experiment_results/experiment_d8ba5ee3c5a44f94b002565339e07c2a_proc_2561565/static_explainer_test_distribution.png"], ["../../logs/0-run/experiment_results/experiment_0c76e6b5896d4343a17fd7ba6713a6fb_proc_2561566/static_explainer_accuracy.png", "../../logs/0-run/experiment_results/experiment_0c76e6b5896d4343a17fd7ba6713a6fb_proc_2561566/static_explainer_loss.png", "../../logs/0-run/experiment_results/experiment_0c76e6b5896d4343a17fd7ba6713a6fb_proc_2561566/static_explainer_test_distribution.png"], ["../../logs/0-run/experiment_results/experiment_2ad53f5eba634a988a98ec655763bc43_proc_2561564/static_explainer_accuracy.png", "../../logs/0-run/experiment_results/experiment_2ad53f5eba634a988a98ec655763bc43_proc_2561564/static_explainer_loss.png", "../../logs/0-run/experiment_results/experiment_2ad53f5eba634a988a98ec655763bc43_proc_2561564/static_explainer_test_distribution.png"], ["../../logs/0-run/experiment_results/seed_aggregation_00aab80e93b34e91a547b27e1a08d404/static_explainer_accuracy_aggregated.png", "../../logs/0-run/experiment_results/seed_aggregation_00aab80e93b34e91a547b27e1a08d404/static_explainer_loss_aggregated.png", "../../logs/0-run/experiment_results/seed_aggregation_00aab80e93b34e91a547b27e1a08d404/static_explainer_test_distribution_aggregated.png"]], "plot_paths": [["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_confusion_epoch10.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_confusion_epoch5.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_loss_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_confusion_epoch3.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_confusion_epoch1.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_confusion_epoch7.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_alignment_curves.png"], [], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_accuracy.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_loss.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_test_distribution.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_d8ba5ee3c5a44f94b002565339e07c2a_proc_2561565/static_explainer_accuracy.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_d8ba5ee3c5a44f94b002565339e07c2a_proc_2561565/static_explainer_loss.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_d8ba5ee3c5a44f94b002565339e07c2a_proc_2561565/static_explainer_test_distribution.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_0c76e6b5896d4343a17fd7ba6713a6fb_proc_2561566/static_explainer_accuracy.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_0c76e6b5896d4343a17fd7ba6713a6fb_proc_2561566/static_explainer_loss.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_0c76e6b5896d4343a17fd7ba6713a6fb_proc_2561566/static_explainer_test_distribution.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_2ad53f5eba634a988a98ec655763bc43_proc_2561564/static_explainer_accuracy.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_2ad53f5eba634a988a98ec655763bc43_proc_2561564/static_explainer_loss.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_2ad53f5eba634a988a98ec655763bc43_proc_2561564/static_explainer_test_distribution.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_00aab80e93b34e91a547b27e1a08d404/static_explainer_accuracy_aggregated.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_00aab80e93b34e91a547b27e1a08d404/static_explainer_loss_aggregated.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_00aab80e93b34e91a547b27e1a08d404/static_explainer_test_distribution_aggregated.png"]], "plot_analyses": [[{"analysis": "Confusion Matrix at epoch 10 shows strong class\u2010balanced performance. True negatives (92) and true positives (86) dominate, with only 14 false positives and 8 false negatives. Overall, the model achieves about 89% accuracy and exhibits balanced recall (\u224892%) and precision (\u224886%), suggesting convergence to a robust decision boundary by epoch 10.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_confusion_epoch10.png"}, {"analysis": "Confusion Matrix at epoch 5 reveals more false positives (29) compared to false negatives (0). True positives are high (94), and true negatives (77) remain respectable. Accuracy (~85.5%) is lower than at epoch 10. Perfect recall on the positive class comes at the expense of classifying some negatives incorrectly.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_confusion_epoch5.png"}, {"analysis": "Loss Curves illustrate smooth decrease in both training and validation loss from ~0.66 and ~0.64 down to ~0.41 and ~0.40 over ten epochs. No divergence between curves appears, indicating stable learning without overfitting. Slightly higher training loss early on may reflect regularization or data noise, but overall optimization proceeds as intended.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_loss_curves.png"}, {"analysis": "Confusion Matrix at epoch 3 shows zero false negatives (perfect recall for positives) but a high false positive rate (44). True negatives are only 62 against 44 false positives. Accuracy (~78%) is moderate. Model at this stage overgeneralizes toward the positive class, which may reflect an undertrained decision threshold.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_confusion_epoch3.png"}, {"analysis": "Confusion Matrix at epoch 1 is heavily skewed toward positive predictions: 98 false positives versus only 8 true negatives, and 94 true positives with zero false negatives. Overall accuracy (~51%) is marginally above chance. Recall is 100% for class 1 but precision is very low for positive predictions, indicating the model initially defaults to predicting the dominant or easier class.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_confusion_epoch1.png"}, {"analysis": "Confusion Matrix at epoch 7 achieves very balanced performance again: 90 true negatives, 16 false positives, 91 true positives, and only 3 false negatives. Accuracy (~90.5%) slightly exceeds that at epoch 10. Recall and precision on both classes converge around 90\u201394%, showing steady refinement of the classification boundary.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_confusion_epoch7.png"}, {"analysis": "Alignment Metrics plot contrasts train versus validation alignment accuracy. Training alignment rises from ~46% at epoch 1 to a peak ~51.5% at epoch 3, then drifts down slightly to ~49.5% by epoch 10. Validation alignment peaks even earlier (~54% at epoch 2\u20134) before declining to ~47% at epoch 10. This pattern suggests that alignment learning saturates quickly and may overfit user bias modeling, as validation alignment degrades after mid\u2010training while losses continue improving.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_c9d8d40bde1c4828964544ae7a8abf58_proc_2561564/synthetic_alignment_curves.png"}], [], [{"analysis": "Training and validation accuracy rapidly climb from ~0.90 at epoch 1 to ~0.99 by epoch 3, then oscillate narrowly around 0.995\u20131.00 through epoch 20. The close alignment of the two curves and minimal gap suggest the static explainer generalizes well with almost no overfitting and converges within a few epochs.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_accuracy.png"}, {"analysis": "Training loss plummets from ~0.40 to ~0.05 by epoch 3, reaching ~0.01 by epoch 10, then stays near zero. Validation loss follows a similar trajectory, dropping from ~0.14 to ~0.02 by epoch 5 and remaining flat thereafter. The parallel decline of both curves further confirms stable learning and convergence without divergence.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_loss.png"}, {"analysis": "Ground truth class counts (0: 230, 1: 270) closely match predicted counts (0: 228, 1: 272). This near parity shows the static explainer maintains class balance on the test set, making only minor miscounts (\u00b12) and reflecting high-fidelity performance across both classes.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_test_distribution.png"}], [{"analysis": "Accuracy curves show that both training and validation accuracy jump to around 0.995 by epoch 3 and remain tightly clustered between 0.995 and 0.998 thereafter. The minimal gap and flat curve indicate extremely rapid convergence with no obvious overfitting and strong generalization on this simple task.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_d8ba5ee3c5a44f94b002565339e07c2a_proc_2561565/static_explainer_accuracy.png"}, {"analysis": "Loss curves reveal a steep decline in both training loss (from ~0.37 to ~0.02 by epoch 5) and validation loss (from ~0.11 to ~0.02) followed by a plateau. Slight fluctuations around epochs 12 and 18 on validation loss are negligible. Overall, the model minimizes loss quickly and remains stable.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_d8ba5ee3c5a44f94b002565339e07c2a_proc_2561565/static_explainer_loss.png"}, {"analysis": "Test set class distribution comparison shows ground truth counts of roughly 225 for class 0 and 275 for class 1, while predictions are nearly identical (slightly underpredicting class 0 and overpredicting class 1 by a couple of samples). This close match indicates balanced performance across classes.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_d8ba5ee3c5a44f94b002565339e07c2a_proc_2561565/static_explainer_test_distribution.png"}], [{"analysis": "Accuracy curves for static explainer show rapid convergence: training accuracy jumps from ~0.89 at epoch 1 to ~0.995 by epoch 2 and remains stable between 0.99 and 0.997 thereafter. Validation accuracy follows a similar trend, starting at ~0.985 and stabilizing around 0.99\u20130.995. The minimal gap between training and validation accuracy suggests low overfitting and strong generalization on held-out data.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_0c76e6b5896d4343a17fd7ba6713a6fb_proc_2561566/static_explainer_accuracy.png"}, {"analysis": "Loss curves exhibit a steep decline in training loss from ~0.375 at epoch 1 to ~0.035 by epoch 3, then gradually decrease to ~0.012 by epoch 10 and plateau thereafter. Validation loss drops from ~0.12 to ~0.045 by epoch 2, then steadily decreases to ~0.01. The convergence of both curves to low values indicates effective optimization and good model calibration without significant divergence.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_0c76e6b5896d4343a17fd7ba6713a6fb_proc_2561566/static_explainer_loss.png"}, {"analysis": "Test set class distribution comparison shows close alignment between ground truth and predictions: class 0 counts are ~225 ground truth vs ~223 predicted, and class 1 counts are ~275 ground truth vs ~277 predicted. There is a slight over-prediction for class 1 (by ~2 samples) and under-prediction for class 0 (by ~2 samples), reflecting balanced performance and minimal bias in class-wise outcomes.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_0c76e6b5896d4343a17fd7ba6713a6fb_proc_2561566/static_explainer_test_distribution.png"}], [{"analysis": "Accuracy curves reveal that the static explainer model achieves very high performance extremely quickly. Training accuracy jumps from around 0.84 at epoch 1 to about 0.995 by epoch 3, and validation accuracy follows closely, peaking at nearly 1.0. After this rapid early convergence, both training and validation accuracies remain stable above 0.98 with only minor oscillations, indicating strong fit and minimal generalization gap.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_2ad53f5eba634a988a98ec655763bc43_proc_2561564/static_explainer_accuracy.png"}, {"analysis": "Loss curves show a steep decline in both training and validation losses during the first few epochs, dropping from roughly 0.45 on the training side and 0.17 on validation at epoch 1 down to below 0.05 by epoch 5. Beyond epoch 5, losses plateau near zero, and the close alignment of training and validation loss throughout suggests that the model is not significantly overfitting and maintains consistent performance on unseen data.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_2ad53f5eba634a988a98ec655763bc43_proc_2561564/static_explainer_loss.png"}, {"analysis": "Test set class distribution comparison indicates that the static explainer\u2019s predictions are almost perfectly balanced against the ground truth. For class 0, ground truth count is 227 versus 225 predictions, and for class 1, 273 versus 275. This slight overprediction of class 1 is negligible and suggests the model is not suffering from substantial class bias.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_2ad53f5eba634a988a98ec655763bc43_proc_2561564/static_explainer_test_distribution.png"}], []], "vlm_feedback_summary": ["Confusion matrices indicate rapid elimination of false negatives by epoch 3 but\nat the cost of false positives, with balanced errors achieved around epochs\n7\u201310. Loss curves descend smoothly without overfitting. Alignment accuracy peaks\nearly and then declines on validation, suggesting potential overfitting in the\nco\u2010adaptive alignment channel and need for regularization or early stopping on\nalignment metrics.", "[]", "The static explainer attains near-perfect accuracy and minimal loss, converging\nrapidly and generalizing robustly. Test predictions closely mirror the true\nclass distribution, establishing a strong baseline for subsequent co-adaptive\nexplainability experiments.", "The static explainer baseline achieves near-perfect accuracy and minimal loss\nalmost immediately, with balanced class-level performance. While this\ndemonstrates correct implementation and capacity for this simple dataset, such\nceiling-level results suggest a trivial task or potential lack of challenge for\nevaluating explanation strategies. It is important to introduce more complexity\nor human-in-the-loop elements in the next stage to meaningfully assess co-\nadaptive explanation benefits.", "Static explainer baseline achieves extremely high accuracy (>99%) with\nnegligible overfitting and balanced class predictions on the test set,\ndemonstrating a robust initial implementation. Given this near-perfect\nperformance, subsequent experiments should focus on dynamic, co-adaptive\nexplanation methods to create meaningful distinctions and scientific insights\nbeyond this strong static baseline.", "The static explainer baseline trains quickly to nearly perfect accuracy with\nnegligible loss and balanced class predictions. These results provide a robust\nbenchmark for later comparisons against dynamic and co-adaptive explanation\ninterfaces.", "[]"], "exec_time": [1.6521174907684326, 0.004121303558349609, 3.225198745727539, 3.3792293071746826, 3.3010451793670654, 3.305612325668335, null], "exec_time_feedback": ["", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"synthetic\"]"], [], ["['static_explainer']"], ["[\"static_explainer\"]"], ["[\"static_explainer\"]"], ["[\"static_explainer\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Print final validation metrics\ntry:\n    val_losses = experiment_data[\"synthetic\"][\"losses\"][\"val\"]\n    val_align = experiment_data[\"synthetic\"][\"metrics\"][\"val\"]\n    print(f\"Final validation loss: {val_losses[-1]:.4f}\")\n    print(f\"Final validation alignment: {val_align[-1]:.4f}\")\nexcept Exception:\n    pass\n\n# Plot loss curves\ntry:\n    losses = experiment_data[\"synthetic\"][\"losses\"]\n    epochs = range(1, len(losses[\"train\"]) + 1)\n    plt.figure()\n    plt.suptitle(\"Synthetic Dataset\")\n    plt.title(\"Loss Curves\\nTrain vs Validation\")\n    plt.plot(epochs, losses[\"train\"], \"b-\", label=\"Train Loss\")\n    plt.plot(epochs, losses[\"val\"], \"r--\", label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\n# Plot alignment curves\ntry:\n    metrics = experiment_data[\"synthetic\"][\"metrics\"]\n    epochs = range(1, len(metrics[\"train\"]) + 1)\n    plt.figure()\n    plt.suptitle(\"Synthetic Dataset\")\n    plt.title(\"Alignment Metrics\\nTrain vs Validation\")\n    plt.plot(epochs, metrics[\"train\"], \"b-\", label=\"Train Alignment\")\n    plt.plot(epochs, metrics[\"val\"], \"r--\", label=\"Val Alignment\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Alignment Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_alignment_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n\n# Plot confusion matrices for selected epochs\nepochs_to_plot = [0, 2, 4, 6, 9]\npreds_list = experiment_data[\"synthetic\"][\"predictions\"]\ngt_list = experiment_data[\"synthetic\"][\"ground_truth\"]\nfor idx in epochs_to_plot:\n    try:\n        preds = preds_list[idx]\n        gt = gt_list[idx]\n        cm = np.zeros((2, 2), int)\n        for t, p in zip(gt, preds):\n            cm[t, p] += 1\n        plt.figure()\n        plt.suptitle(\"Synthetic Dataset\")\n        plt.title(\n            f\"Confusion Matrix Epoch {idx+1}\\nRows: Ground Truth, Cols: Predictions\"\n        )\n        plt.imshow(cm, cmap=\"Blues\", interpolation=\"nearest\")\n        for i in range(2):\n            for j in range(2):\n                plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"white\")\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"True Label\")\n        fname = f\"synthetic_confusion_epoch{idx+1}.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot_confusion_epoch{idx+1}: {e}\")\n        plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nstatic = experiment_data.get(\"static_explainer\", {})\nmetrics = static.get(\"metrics\", {})\nlosses = static.get(\"losses\", {})\npreds = static.get(\"predictions\", [])\ngt = static.get(\"ground_truth\", [])\n\n# Accuracy curves\ntry:\n    acc_train = metrics.get(\"train\", [])\n    acc_val = metrics.get(\"val\", [])\n    epochs = range(1, len(acc_train) + 1)\n    plt.figure()\n    plt.plot(epochs, acc_train, label=\"Train\")\n    plt.plot(epochs, acc_val, label=\"Val\")\n    plt.title(\"Static Explainer Accuracy Curves\\nTraining vs Validation Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"static_explainer_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# Loss curves\ntry:\n    loss_train = losses.get(\"train\", [])\n    loss_val = losses.get(\"val\", [])\n    epochs = range(1, len(loss_train) + 1)\n    plt.figure()\n    plt.plot(epochs, loss_train, label=\"Train\")\n    plt.plot(epochs, loss_val, label=\"Val\")\n    plt.title(\"Static Explainer Loss Curves\\nTraining vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"static_explainer_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Test set prediction distribution\ntry:\n    if preds and gt:\n        counts_pred = np.bincount(preds, minlength=2)\n        counts_gt = np.bincount(gt, minlength=2)\n        classes = np.arange(len(counts_gt))\n        width = 0.35\n        plt.figure()\n        plt.bar(classes - width / 2, counts_gt, width, label=\"Ground Truth\")\n        plt.bar(classes + width / 2, counts_pred, width, label=\"Predictions\")\n        plt.title(\n            \"Static Explainer Test Predictions vs Ground Truth\\nTest Set Class Distribution\"\n        )\n        plt.xlabel(\"Class\")\n        plt.ylabel(\"Count\")\n        plt.xticks(classes)\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"static_explainer_test_distribution.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating prediction distribution plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nstatic = experiment_data.get(\"static_explainer\", {})\nmetrics = static.get(\"metrics\", {})\nlosses = static.get(\"losses\", {})\npreds = static.get(\"predictions\", [])\ngt = static.get(\"ground_truth\", [])\n\n# Accuracy curves\ntry:\n    acc_train = metrics.get(\"train\", [])\n    acc_val = metrics.get(\"val\", [])\n    epochs = range(1, len(acc_train) + 1)\n    plt.figure()\n    plt.plot(epochs, acc_train, label=\"Train\")\n    plt.plot(epochs, acc_val, label=\"Val\")\n    plt.title(\"Static Explainer Accuracy Curves\\nTraining vs Validation Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"static_explainer_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# Loss curves\ntry:\n    loss_train = losses.get(\"train\", [])\n    loss_val = losses.get(\"val\", [])\n    epochs = range(1, len(loss_train) + 1)\n    plt.figure()\n    plt.plot(epochs, loss_train, label=\"Train\")\n    plt.plot(epochs, loss_val, label=\"Val\")\n    plt.title(\"Static Explainer Loss Curves\\nTraining vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"static_explainer_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Test set prediction distribution\ntry:\n    if preds and gt:\n        counts_pred = np.bincount(preds, minlength=2)\n        counts_gt = np.bincount(gt, minlength=2)\n        classes = np.arange(len(counts_gt))\n        width = 0.35\n        plt.figure()\n        plt.bar(classes - width / 2, counts_gt, width, label=\"Ground Truth\")\n        plt.bar(classes + width / 2, counts_pred, width, label=\"Predictions\")\n        plt.title(\n            \"Static Explainer Test Predictions vs Ground Truth\\nTest Set Class Distribution\"\n        )\n        plt.xlabel(\"Class\")\n        plt.ylabel(\"Count\")\n        plt.xticks(classes)\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"static_explainer_test_distribution.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating prediction distribution plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nstatic = experiment_data.get(\"static_explainer\", {})\nmetrics = static.get(\"metrics\", {})\nlosses = static.get(\"losses\", {})\npreds = static.get(\"predictions\", [])\ngt = static.get(\"ground_truth\", [])\n\n# Accuracy curves\ntry:\n    acc_train = metrics.get(\"train\", [])\n    acc_val = metrics.get(\"val\", [])\n    epochs = range(1, len(acc_train) + 1)\n    plt.figure()\n    plt.plot(epochs, acc_train, label=\"Train\")\n    plt.plot(epochs, acc_val, label=\"Val\")\n    plt.title(\"Static Explainer Accuracy Curves\\nTraining vs Validation Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"static_explainer_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# Loss curves\ntry:\n    loss_train = losses.get(\"train\", [])\n    loss_val = losses.get(\"val\", [])\n    epochs = range(1, len(loss_train) + 1)\n    plt.figure()\n    plt.plot(epochs, loss_train, label=\"Train\")\n    plt.plot(epochs, loss_val, label=\"Val\")\n    plt.title(\"Static Explainer Loss Curves\\nTraining vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"static_explainer_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Test set prediction distribution\ntry:\n    if preds and gt:\n        counts_pred = np.bincount(preds, minlength=2)\n        counts_gt = np.bincount(gt, minlength=2)\n        classes = np.arange(len(counts_gt))\n        width = 0.35\n        plt.figure()\n        plt.bar(classes - width / 2, counts_gt, width, label=\"Ground Truth\")\n        plt.bar(classes + width / 2, counts_pred, width, label=\"Predictions\")\n        plt.title(\n            \"Static Explainer Test Predictions vs Ground Truth\\nTest Set Class Distribution\"\n        )\n        plt.xlabel(\"Class\")\n        plt.ylabel(\"Count\")\n        plt.xticks(classes)\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"static_explainer_test_distribution.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating prediction distribution plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nstatic = experiment_data.get(\"static_explainer\", {})\nmetrics = static.get(\"metrics\", {})\nlosses = static.get(\"losses\", {})\npreds = static.get(\"predictions\", [])\ngt = static.get(\"ground_truth\", [])\n\n# Accuracy curves\ntry:\n    acc_train = metrics.get(\"train\", [])\n    acc_val = metrics.get(\"val\", [])\n    epochs = range(1, len(acc_train) + 1)\n    plt.figure()\n    plt.plot(epochs, acc_train, label=\"Train\")\n    plt.plot(epochs, acc_val, label=\"Val\")\n    plt.title(\"Static Explainer Accuracy Curves\\nTraining vs Validation Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"static_explainer_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# Loss curves\ntry:\n    loss_train = losses.get(\"train\", [])\n    loss_val = losses.get(\"val\", [])\n    epochs = range(1, len(loss_train) + 1)\n    plt.figure()\n    plt.plot(epochs, loss_train, label=\"Train\")\n    plt.plot(epochs, loss_val, label=\"Val\")\n    plt.title(\"Static Explainer Loss Curves\\nTraining vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"static_explainer_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Test set prediction distribution\ntry:\n    if preds and gt:\n        counts_pred = np.bincount(preds, minlength=2)\n        counts_gt = np.bincount(gt, minlength=2)\n        classes = np.arange(len(counts_gt))\n        width = 0.35\n        plt.figure()\n        plt.bar(classes - width / 2, counts_gt, width, label=\"Ground Truth\")\n        plt.bar(classes + width / 2, counts_pred, width, label=\"Predictions\")\n        plt.title(\n            \"Static Explainer Test Predictions vs Ground Truth\\nTest Set Class Distribution\"\n        )\n        plt.xlabel(\"Class\")\n        plt.ylabel(\"Count\")\n        plt.xticks(classes)\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"static_explainer_test_distribution.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating prediction distribution plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load all experiment data\ntry:\n    experiment_data_path_list = [\n        \"experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_d8ba5ee3c5a44f94b002565339e07c2a_proc_2561565/experiment_data.npy\",\n        \"experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_0c76e6b5896d4343a17fd7ba6713a6fb_proc_2561566/experiment_data.npy\",\n        \"experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_2ad53f5eba634a988a98ec655763bc43_proc_2561564/experiment_data.npy\",\n    ]\n    all_experiment_data = []\n    for exp_path in experiment_data_path_list:\n        data = np.load(\n            os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\"), exp_path), allow_pickle=True\n        ).item()\n        all_experiment_data.append(data)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_experiment_data = []\n\n# Aggregated accuracy curves\ntry:\n    train_list, val_list = [], []\n    for exp in all_experiment_data:\n        static = exp.get(\"static_explainer\", {})\n        metrics = static.get(\"metrics\", {})\n        train_list.append(metrics.get(\"train\", []))\n        val_list.append(metrics.get(\"val\", []))\n    # Align lengths\n    min_epochs = min(min(len(a) for a in train_list), min(len(a) for a in val_list))\n    train_arr = np.array([a[:min_epochs] for a in train_list])\n    val_arr = np.array([a[:min_epochs] for a in val_list])\n    epochs = np.arange(1, min_epochs + 1)\n    mean_train = train_arr.mean(axis=0)\n    se_train = train_arr.std(axis=0, ddof=1) / np.sqrt(len(train_arr))\n    mean_val = val_arr.mean(axis=0)\n    se_val = val_arr.std(axis=0, ddof=1) / np.sqrt(len(val_arr))\n    plt.figure()\n    plt.errorbar(epochs, mean_train, yerr=se_train, label=\"Train\")\n    plt.errorbar(epochs, mean_val, yerr=se_val, label=\"Val\")\n    plt.title(\n        \"Static Explainer Accuracy Curves (Aggregated)\\nTraining vs Validation Accuracy\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"static_explainer_accuracy_aggregated.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated accuracy plot: {e}\")\n    plt.close()\n\n# Aggregated loss curves\ntry:\n    loss_train_list, loss_val_list = [], []\n    for exp in all_experiment_data:\n        static = exp.get(\"static_explainer\", {})\n        losses = static.get(\"losses\", {})\n        loss_train_list.append(losses.get(\"train\", []))\n        loss_val_list.append(losses.get(\"val\", []))\n    min_epochs = min(\n        min(len(a) for a in loss_train_list), min(len(a) for a in loss_val_list)\n    )\n    lt = np.array([a[:min_epochs] for a in loss_train_list])\n    lv = np.array([a[:min_epochs] for a in loss_val_list])\n    epochs = np.arange(1, min_epochs + 1)\n    mean_lt = lt.mean(axis=0)\n    se_lt = lt.std(axis=0, ddof=1) / np.sqrt(len(lt))\n    mean_lv = lv.mean(axis=0)\n    se_lv = lv.std(axis=0, ddof=1) / np.sqrt(len(lv))\n    plt.figure()\n    plt.errorbar(epochs, mean_lt, yerr=se_lt, label=\"Train\")\n    plt.errorbar(epochs, mean_lv, yerr=se_lv, label=\"Val\")\n    plt.title(\"Static Explainer Loss Curves (Aggregated)\\nTraining vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"static_explainer_loss_aggregated.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated loss plot: {e}\")\n    plt.close()\n\n# Aggregated test distribution\ntry:\n    pred_counts, gt_counts = [], []\n    for exp in all_experiment_data:\n        static = exp.get(\"static_explainer\", {})\n        preds = static.get(\"predictions\", [])\n        gt = static.get(\"ground_truth\", [])\n        if len(preds) and len(gt):\n            n_cls = max(max(preds), max(gt)) + 1\n            pred_counts.append(np.bincount(preds, minlength=n_cls))\n            gt_counts.append(np.bincount(gt, minlength=n_cls))\n    if pred_counts:\n        pc = np.array(pred_counts)\n        gc = np.array(gt_counts)\n        classes = np.arange(pc.shape[1])\n        mean_pc = pc.mean(axis=0)\n        se_pc = pc.std(axis=0, ddof=1) / np.sqrt(len(pc))\n        mean_gc = gc.mean(axis=0)\n        se_gc = gc.std(axis=0, ddof=1) / np.sqrt(len(gc))\n        width = 0.35\n        plt.figure()\n        plt.bar(classes - width / 2, mean_gc, width, yerr=se_gc, label=\"Ground Truth\")\n        plt.bar(classes + width / 2, mean_pc, width, yerr=se_pc, label=\"Predictions\")\n        plt.title(\n            \"Static Explainer Test Predictions vs Ground Truth (Aggregated)\\nTest Set Class Distribution (GT left, Pred right)\"\n        )\n        plt.xlabel(\"Class\")\n        plt.ylabel(\"Count\")\n        plt.xticks(classes)\n        plt.legend()\n        plt.savefig(\n            os.path.join(\n                working_dir, \"static_explainer_test_distribution_aggregated.png\"\n            )\n        )\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated distribution plot: {e}\")\n    plt.close()\n"], "plot_plan": ["We will load the saved experiment_data from the working directory, extract training and validation losses and alignment metrics for the synthetic dataset, and print the final validation loss and alignment accuracy. Then we will plot the loss curves and alignment curves each in separate try-except blocks, including clear suptitles and titles indicating the dataset and plot type, saving as PNG in working_dir. Finally, we will compute confusion matrices for a subset of epochs ([1, 3, 5, 7, 10]) by comparing ground truth and model predictions, plot each matrix with annotated counts, and save each figure with a descriptive filename. All figures are closed after saving, and any errors during plotting are caught and printed.", null, "We load the saved experiment data and extract training and validation metrics and losses for the static explainer, along with the test set predictions and ground truth. We then generate three standard visualizations: (1) the accuracy curves over epochs for training vs validation, (2) the loss curves over epochs for training vs validation, and (3) a bar chart comparing the class distribution of test set ground truth vs model predictions. Each plot is created in its own try-except block to handle missing data gracefully, and figures are closed after saving. We save each figure under the working directory with descriptive filenames that include the dataset name and plot type. Clear titles and subtitles specify the dataset and the nature of each plot. This approach ensures robust and reproducible visualization of experiment results.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null], "is_seed_node": [false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, true], "parse_metrics_plan": ["The following script immediately loads the `experiment_data.npy` file from the\nworking directory, iterates over each dataset contained in the saved dictionary,\nextracts the final values for training loss, validation loss, and human-model\nalignment accuracies, and prints them with clear, descriptive labels. All code\nexecutes at the global scope without any conditional entry points.", "", "I will load the saved experiment data from the working directory, extract the\nrecorded training and validation accuracies and losses, select their final epoch\nvalues, and compute the overall test accuracy from the stored predictions and\nground truth. Then, I will print each dataset name followed by its specific\nmetrics with clear labels. This code runs immediately upon execution and does\nnot use any entry-point guard.", "I will load the saved experiment data from the working directory, extract the\nrecorded training and validation accuracies and losses, select their final epoch\nvalues, and compute the overall test accuracy from the stored predictions and\nground truth. Then, I will print each dataset name followed by its specific\nmetrics with clear labels. This code runs immediately upon execution and does\nnot use any entry-point guard.", "I will load the saved experiment data from the working directory, extract the\nrecorded training and validation accuracies and losses, select their final epoch\nvalues, and compute the overall test accuracy from the stored predictions and\nground truth. Then, I will print each dataset name followed by its specific\nmetrics with clear labels. This code runs immediately upon execution and does\nnot use any entry-point guard.", "I will load the saved experiment data from the working directory, extract the\nrecorded training and validation accuracies and losses, select their final epoch\nvalues, and compute the overall test accuracy from the stored predictions and\nground truth. Then, I will print each dataset name followed by its specific\nmetrics with clear labels. This code runs immediately upon execution and does\nnot use any entry-point guard.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, dataset_info in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n    # Extract final losses\n    train_loss = dataset_info[\"losses\"][\"train\"][-1]\n    validation_loss = dataset_info[\"losses\"][\"val\"][-1]\n    # Extract final alignment metrics\n    train_alignment_accuracy = dataset_info[\"metrics\"][\"train\"][-1]\n    validation_alignment_accuracy = dataset_info[\"metrics\"][\"val\"][-1]\n    # Print metrics with clear labels\n    print(f\"training loss: {train_loss:.4f}\")\n    print(f\"validation loss: {validation_loss:.4f}\")\n    print(f\"train alignment accuracy: {train_alignment_accuracy:.4f}\")\n    print(f\"validation alignment accuracy: {validation_alignment_accuracy:.4f}\\n\")\n", "", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata = np.load(\n    os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n).item()\nexp = data[\"static_explainer\"]\n\n# Extract training and validation metrics\ntrain_accuracies = exp[\"metrics\"][\"train\"]\nval_accuracies = exp[\"metrics\"][\"val\"]\ntrain_losses = exp[\"losses\"][\"train\"]\nval_losses = exp[\"losses\"][\"val\"]\n\n# Select final epoch values\nfinal_train_accuracy = train_accuracies[-1]\nfinal_train_loss = train_losses[-1]\nfinal_validation_accuracy = val_accuracies[-1]\nfinal_validation_loss = val_losses[-1]\n\n# Compute test accuracy\npreds = np.array(exp[\"predictions\"])\ngt = np.array(exp[\"ground_truth\"])\ntest_accuracy = (preds == gt).mean()\n\n# Print results\nprint(\"Training dataset:\")\nprint(\"Train accuracy:\", final_train_accuracy)\nprint(\"Training loss:\", final_train_loss)\nprint()\nprint(\"Validation dataset:\")\nprint(\"Validation accuracy:\", final_validation_accuracy)\nprint(\"Validation loss:\", final_validation_loss)\nprint()\nprint(\"Test dataset:\")\nprint(\"Test accuracy:\", test_accuracy)\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata = np.load(\n    os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n).item()\nexp = data[\"static_explainer\"]\n\n# Extract training and validation metrics\ntrain_accuracies = exp[\"metrics\"][\"train\"]\nval_accuracies = exp[\"metrics\"][\"val\"]\ntrain_losses = exp[\"losses\"][\"train\"]\nval_losses = exp[\"losses\"][\"val\"]\n\n# Select final epoch values\nfinal_train_accuracy = train_accuracies[-1]\nfinal_train_loss = train_losses[-1]\nfinal_validation_accuracy = val_accuracies[-1]\nfinal_validation_loss = val_losses[-1]\n\n# Compute test accuracy\npreds = np.array(exp[\"predictions\"])\ngt = np.array(exp[\"ground_truth\"])\ntest_accuracy = (preds == gt).mean()\n\n# Print results\nprint(\"Training dataset:\")\nprint(\"Train accuracy:\", final_train_accuracy)\nprint(\"Training loss:\", final_train_loss)\nprint()\nprint(\"Validation dataset:\")\nprint(\"Validation accuracy:\", final_validation_accuracy)\nprint(\"Validation loss:\", final_validation_loss)\nprint()\nprint(\"Test dataset:\")\nprint(\"Test accuracy:\", test_accuracy)\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata = np.load(\n    os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n).item()\nexp = data[\"static_explainer\"]\n\n# Extract training and validation metrics\ntrain_accuracies = exp[\"metrics\"][\"train\"]\nval_accuracies = exp[\"metrics\"][\"val\"]\ntrain_losses = exp[\"losses\"][\"train\"]\nval_losses = exp[\"losses\"][\"val\"]\n\n# Select final epoch values\nfinal_train_accuracy = train_accuracies[-1]\nfinal_train_loss = train_losses[-1]\nfinal_validation_accuracy = val_accuracies[-1]\nfinal_validation_loss = val_losses[-1]\n\n# Compute test accuracy\npreds = np.array(exp[\"predictions\"])\ngt = np.array(exp[\"ground_truth\"])\ntest_accuracy = (preds == gt).mean()\n\n# Print results\nprint(\"Training dataset:\")\nprint(\"Train accuracy:\", final_train_accuracy)\nprint(\"Training loss:\", final_train_loss)\nprint()\nprint(\"Validation dataset:\")\nprint(\"Validation accuracy:\", final_validation_accuracy)\nprint(\"Validation loss:\", final_validation_loss)\nprint()\nprint(\"Test dataset:\")\nprint(\"Test accuracy:\", test_accuracy)\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata = np.load(\n    os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n).item()\nexp = data[\"static_explainer\"]\n\n# Extract training and validation metrics\ntrain_accuracies = exp[\"metrics\"][\"train\"]\nval_accuracies = exp[\"metrics\"][\"val\"]\ntrain_losses = exp[\"losses\"][\"train\"]\nval_losses = exp[\"losses\"][\"val\"]\n\n# Select final epoch values\nfinal_train_accuracy = train_accuracies[-1]\nfinal_train_loss = train_losses[-1]\nfinal_validation_accuracy = val_accuracies[-1]\nfinal_validation_loss = val_losses[-1]\n\n# Compute test accuracy\npreds = np.array(exp[\"predictions\"])\ngt = np.array(exp[\"ground_truth\"])\ntest_accuracy = (preds == gt).mean()\n\n# Print results\nprint(\"Training dataset:\")\nprint(\"Train accuracy:\", final_train_accuracy)\nprint(\"Training loss:\", final_train_loss)\nprint()\nprint(\"Validation dataset:\")\nprint(\"Validation accuracy:\", final_validation_accuracy)\nprint(\"Validation loss:\", final_validation_loss)\nprint()\nprint(\"Test dataset:\")\nprint(\"Test accuracy:\", test_accuracy)\n", ""], "parse_term_out": ["['Dataset: synthetic', '\\n', 'training loss: 0.4106', '\\n', 'validation loss:\n0.4054', '\\n', 'train alignment accuracy: 0.4950', '\\n', 'validation alignment\naccuracy: 0.4700\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "", "['Training dataset:', '\\n', 'Train accuracy:', ' ', '0.9958333333333333', '\\n',\n'Training loss:', ' ', '0.012646730159176514', '\\n', '\\n', 'Validation\ndataset:', '\\n', 'Validation accuracy:', ' ', '0.9966666666666667', '\\n',\n'Validation loss:', ' ', '0.012335669596542781', '\\n', '\\n', 'Test dataset:',\n'\\n', 'Test accuracy:', ' ', '0.99', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Training dataset:', '\\n', 'Train accuracy:', ' ', '0.9941666666666666', '\\n',\n'Training loss:', ' ', '0.014887199808144942', '\\n', '\\n', 'Validation\ndataset:', '\\n', 'Validation accuracy:', ' ', '0.99', '\\n', 'Validation loss:',\n' ', '0.016995876728626628', '\\n', '\\n', 'Test dataset:', '\\n', 'Test\naccuracy:', ' ', '0.992', '\\n', 'Execution time: a moment seconds (time limit is\nan hour).']", "['Training dataset:', '\\n', 'Train accuracy:', ' ', '0.9941666666666666', '\\n',\n'Training loss:', ' ', '0.014887199808144942', '\\n', '\\n', 'Validation\ndataset:', '\\n', 'Validation accuracy:', ' ', '0.99', '\\n', 'Validation loss:',\n' ', '0.016995876728626628', '\\n', '\\n', 'Test dataset:', '\\n', 'Test\naccuracy:', ' ', '0.992', '\\n', 'Execution time: a moment seconds (time limit is\nan hour).']", "['Training dataset:', '\\n', 'Train accuracy:', ' ', '0.9966666666666667', '\\n',\n'Training loss:', ' ', '0.011508071264446092', '\\n', '\\n', 'Validation\ndataset:', '\\n', 'Validation accuracy:', ' ', '0.9966666666666667', '\\n',\n'Validation loss:', ' ', '0.011210518143294526', '\\n', '\\n', 'Test dataset:',\n'\\n', 'Test accuracy:', ' ', '0.996', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null], "completed_stages": ["Stage_1"]}
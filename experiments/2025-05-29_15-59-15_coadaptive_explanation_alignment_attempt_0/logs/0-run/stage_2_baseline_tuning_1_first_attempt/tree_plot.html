<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 2], [0, 3], [0, 1], [2, 6], [2, 7], [2, 4], [2, 5]], "layout": [[0.5, 0.0], [0.16666666666666666, 0.5], [0.5, 0.5], [0.8333333333333334, 0.5], [0.0, 1.0], [0.3333333333333333, 1.0], [0.6666666666666666, 1.0], [1.0, 1.0]], "plan": ["We implement a static baseline by generating a synthetic 2D binary\nclassification dataset and training a small MLP as our AI model. We extract\ncontent\u2010justification explanations by taking the AI model\u2019s predicted class\nprobabilities for each input. To simulate a user\u2019s mental model, we train a\nsimple neural \u201cuser network\u201d that takes both the normalized original features\nand the static explanation probabilities as inputs, and is trained to predict\nthe AI model\u2019s decisions. We split data into train, validation, and test sets,\nnormalizing based on training statistics. During user\u2010model training we track\nand print validation loss and compute the Mental Model Alignment Accuracy\u2014the\nfraction of validation examples on which the user model agrees with the AI\nmodel\u2014at each epoch. After training, we evaluate on a held\u2010out test set to\nreport final alignment accuracy. All metrics, losses, predictions, and\nground\u2010truth AI decisions are stored in an `experiment_data` dictionary and\nsaved to `./working/experiment_data.npy`. This gives a concise, working\nstatic\u2010explainer baseline for future co\u2010adaptive comparisons.", "Hyperparam tuning name: learning_rate. We will extend the baseline script to\nsweep several learning rates for the user model. For each candidate lr we\ninstantiate and train a fresh user model, tracking per\u2010epoch train/validation\nlosses and accuracies, then evaluate test predictions. All collected data is\nstored under a single `experiment_data` dict keyed by `'learning_rate_sweep' ->\n'user_model'` and saved to `experiment_data.npy`.", "Hyperparam tuning name: batch_size. Below is a solution that wraps the AI\u2010 and\nuser\u2010model training inside nested loops over a set of batch sizes. For each (AI,\nuser) batch\u2010size pair it reinitializes data loaders, models, and optimizers,\ntrains the AI model for 15 epochs, generates predictions, then trains the user\nmodel for 20 epochs. It collects per\u2010epoch train/val losses and accuracies plus\nfinal test predictions/labels under a key encoding the batch sizes. Finally it\nsaves the resulting `experiment_data` dict to `experiment_data.npy`.", "Hyperparam tuning name: weight_decay. I will generate the hyperparameter sweep\nover weight_decay values, retrain the user model for each, and record\ntraining/validation losses and accuracies per epoch as well as test\u2010time\npredictions. All results are stored in a single `experiment_data` dictionary\nkeyed by weight_decay, then saved to `experiment_data.npy`. The AI model and\ndata generation steps remain unchanged.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Synthetic dataset generation\nnp.random.seed(0)\nN = 2000\nD = 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# AI model definition\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Train AI model\nai_model = AIModel(D, 16, 2).to(device)\ncriterion_ai = nn.CrossEntropyLoss()\noptimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n\n\n# DataLoaders for AI\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nai_train_loader = DataLoader(SimpleDS(X_train, y_train), batch_size=32, shuffle=True)\nai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=32)\n\nfor epoch in range(15):\n    ai_model.train()\n    for batch in ai_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = ai_model(batch[\"x\"])\n        loss = criterion_ai(logits, batch[\"y\"])\n        optimizer_ai.zero_grad()\n        loss.backward()\n        optimizer_ai.step()\n    # validation\n    ai_model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in ai_val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = ai_model(batch[\"x\"])\n            val_loss += criterion_ai(out, batch[\"y\"]).item() * batch[\"x\"].size(0)\n    val_loss /= len(ai_val_loader.dataset)\n    print(f\"Epoch {epoch}: AI val loss = {val_loss:.4f}\")\n\n# Generate explanations (predicted probabilities)\nai_model.eval()\nwith torch.no_grad():\n    X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n    logits_all = ai_model(X_all)\n    probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n# Split explanations\np_train = probs_all[: len(X_train)]\np_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\np_test = probs_all[-len(X_test) :]\n# Ground truth AI decisions\nf_train = np.argmax(p_train, axis=1)\nf_val = np.argmax(p_val, axis=1)\nf_test = np.argmax(p_test, axis=1)\n\n# Prepare user dataset (features = [X, p], labels = f)\nX_usr_train = np.hstack([X_train, p_train])\nX_usr_val = np.hstack([X_val, p_val])\nX_usr_test = np.hstack([X_test, p_test])\n\n\n# User Dataset & DataLoaders\nclass UserDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\nusr_train_loader = DataLoader(UserDS(X_usr_train, f_train), batch_size=32, shuffle=True)\nusr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=32)\nusr_test_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=32)\n\n\n# User model\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nuser_model = UserModel(D + 2, 8, 2).to(device)\ncriterion_usr = nn.CrossEntropyLoss()\noptimizer_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n\n# Experiment data\nexperiment_data = {\n    \"static_explainer\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# Train user model and track metrics\nfor epoch in range(20):\n    user_model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for batch in usr_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = user_model(batch[\"feat\"])\n        loss = criterion_usr(out, batch[\"label\"])\n        optimizer_usr.zero_grad()\n        loss.backward()\n        optimizer_usr.step()\n        total_loss += loss.item() * batch[\"feat\"].size(0)\n        preds = out.argmax(dim=1)\n        correct += (preds == batch[\"label\"]).sum().item()\n        total += len(batch[\"label\"])\n    train_loss = total_loss / total\n    train_acc = correct / total\n    experiment_data[\"static_explainer\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"static_explainer\"][\"metrics\"][\"train\"].append(train_acc)\n\n    # validation\n    user_model.eval()\n    val_loss, val_corr, val_tot = 0.0, 0, 0\n    with torch.no_grad():\n        for batch in usr_val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = user_model(batch[\"feat\"])\n            val_loss += criterion_usr(out, batch[\"label\"]).item() * batch[\"feat\"].size(\n                0\n            )\n            preds = out.argmax(dim=1)\n            val_corr += (preds == batch[\"label\"]).sum().item()\n            val_tot += len(batch[\"label\"])\n    val_loss /= val_tot\n    val_acc = val_corr / val_tot\n    experiment_data[\"static_explainer\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"static_explainer\"][\"metrics\"][\"val\"].append(val_acc)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n# Test evaluation\nuser_model.eval()\ntest_preds, test_gt = [], []\nwith torch.no_grad():\n    for batch in usr_test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = user_model(batch[\"feat\"])\n        p = out.argmax(dim=1).cpu().numpy().tolist()\n        test_preds.extend(p)\n        test_gt.extend(batch[\"label\"].cpu().numpy().tolist())\n\nexperiment_data[\"static_explainer\"][\"predictions\"] = test_preds\nexperiment_data[\"static_explainer\"][\"ground_truth\"] = test_gt\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Synthetic dataset generation\nnp.random.seed(0)\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# AI model definition\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Train AI model\nai_model = AIModel(D, 16, 2).to(device)\ncriterion_ai = nn.CrossEntropyLoss()\noptimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n\n\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nai_train_loader = DataLoader(SimpleDS(X_train, y_train), batch_size=32, shuffle=True)\nai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=32)\n\nfor epoch in range(15):\n    ai_model.train()\n    for batch in ai_train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        logits = ai_model(batch[\"x\"])\n        loss = criterion_ai(logits, batch[\"y\"])\n        optimizer_ai.zero_grad()\n        loss.backward()\n        optimizer_ai.step()\n    ai_model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in ai_val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = ai_model(batch[\"x\"])\n            val_loss += criterion_ai(out, batch[\"y\"]).item() * batch[\"x\"].size(0)\n    val_loss /= len(ai_val_loader.dataset)\n    print(f\"AI Epoch {epoch}: val loss = {val_loss:.4f}\")\n\n# Generate explanations (predicted probabilities)\nai_model.eval()\nwith torch.no_grad():\n    X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n    logits_all = ai_model(X_all)\n    probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\np_train = probs_all[: len(X_train)]\np_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\np_test = probs_all[-len(X_test) :]\nf_train = np.argmax(p_train, axis=1)\nf_val = np.argmax(p_val, axis=1)\nf_test = np.argmax(p_test, axis=1)\n\n# Prepare user dataset features and labels\nX_usr_train = np.hstack([X_train, p_train])\nX_usr_val = np.hstack([X_val, p_val])\nX_usr_test = np.hstack([X_test, p_test])\n\n\nclass UserDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\nusr_train_loader = DataLoader(UserDS(X_usr_train, f_train), batch_size=32, shuffle=True)\nusr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=32)\nusr_test_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=32)\n\n# Hyperparameter sweep: learning rates for user model\nlearning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2]\nexperiment_data = {\n    \"learning_rate_sweep\": {\n        \"user_model\": {\n            \"lrs\": learning_rates,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor lr in learning_rates:\n    user_model = nn.Sequential(nn.Linear(D + 2, 8), nn.ReLU(), nn.Linear(8, 2)).to(\n        device\n    )\n    optimizer_usr = optim.Adam(user_model.parameters(), lr=lr)\n    criterion_usr = nn.CrossEntropyLoss()\n    train_accs, val_accs = [], []\n    train_losses, val_losses = [], []\n    # train for 20 epochs\n    for epoch in range(20):\n        user_model.train()\n        tot_loss, corr, tot = 0.0, 0, 0\n        for batch in usr_train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = user_model(batch[\"feat\"])\n            loss = criterion_usr(out, batch[\"label\"])\n            optimizer_usr.zero_grad()\n            loss.backward()\n            optimizer_usr.step()\n            tot_loss += loss.item() * batch[\"feat\"].size(0)\n            preds = out.argmax(dim=1)\n            corr += (preds == batch[\"label\"]).sum().item()\n            tot += len(batch[\"label\"])\n        train_losses.append(tot_loss / tot)\n        train_accs.append(corr / tot)\n        user_model.eval()\n        v_loss, v_corr, v_tot = 0.0, 0, 0\n        with torch.no_grad():\n            for batch in usr_val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                out = user_model(batch[\"feat\"])\n                v_loss += criterion_usr(out, batch[\"label\"]).item() * batch[\n                    \"feat\"\n                ].size(0)\n                preds = out.argmax(dim=1)\n                v_corr += (preds == batch[\"label\"]).sum().item()\n                v_tot += len(batch[\"label\"])\n        val_losses.append(v_loss / v_tot)\n        val_accs.append(v_corr / v_tot)\n    # record per-lr data\n    em = experiment_data[\"learning_rate_sweep\"][\"user_model\"]\n    em[\"metrics\"][\"train\"].append(train_accs)\n    em[\"metrics\"][\"val\"].append(val_accs)\n    em[\"losses\"][\"train\"].append(train_losses)\n    em[\"losses\"][\"val\"].append(val_losses)\n    # test evaluation\n    test_preds, test_gt = [], []\n    user_model.eval()\n    with torch.no_grad():\n        for batch in usr_test_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = user_model(batch[\"feat\"])\n            p = out.argmax(dim=1).cpu().numpy().tolist()\n            test_preds.extend(p)\n            test_gt.extend(batch[\"label\"].cpu().numpy().tolist())\n    em[\"predictions\"].append(test_preds)\n    em[\"ground_truth\"].append(test_gt)\n    print(f\"Completed LR={lr:.1e}, final val acc={val_accs[-1]:.4f}\")\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic dataset generation\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Model definitions\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameter grid\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\n\n# Container for all results\nexperiment_data = {\"batch_size\": {}}\n\nfor ai_bs in ai_batch_sizes:\n    # AI data loaders\n    ai_tr_loader = DataLoader(\n        SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True\n    )\n    ai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n\n    # Initialize AI model\n    ai_model = AIModel(D, 16, 2).to(device)\n    criterion_ai = nn.CrossEntropyLoss()\n    optimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n\n    # Train AI model\n    for _ in range(15):\n        ai_model.train()\n        for batch in ai_tr_loader:\n            x = batch[\"x\"].to(device)\n            yb = batch[\"y\"].to(device)\n            out = ai_model(x)\n            loss = criterion_ai(out, yb)\n            optimizer_ai.zero_grad()\n            loss.backward()\n            optimizer_ai.step()\n\n    # Generate AI probabilities\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n    p_train = probs_all[: len(X_train)]\n    p_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\n    p_test = probs_all[-len(X_test) :]\n    f_train = p_train.argmax(axis=1)\n    f_val = p_val.argmax(axis=1)\n    f_test = p_test.argmax(axis=1)\n\n    # Prepare user features\n    X_usr_train = np.hstack([X_train, p_train])\n    X_usr_val = np.hstack([X_val, p_val])\n    X_usr_test = np.hstack([X_test, p_test])\n\n    for usr_bs in usr_batch_sizes:\n        # User data loaders\n        usr_tr_loader = DataLoader(\n            UserDS(X_usr_train, f_train), batch_size=usr_bs, shuffle=True\n        )\n        usr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=usr_bs)\n        usr_test_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=usr_bs)\n\n        # Initialize User model\n        user_model = UserModel(D + 2, 8, 2).to(device)\n        criterion_usr = nn.CrossEntropyLoss()\n        optimizer_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n\n        train_accs, val_accs = [], []\n        train_losses, val_losses = [], []\n\n        # Train User model\n        for _ in range(20):\n            user_model.train()\n            t_loss, corr, tot = 0.0, 0, 0\n            for batch in usr_tr_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                loss = criterion_usr(out, lbl)\n                optimizer_usr.zero_grad()\n                loss.backward()\n                optimizer_usr.step()\n                t_loss += loss.item() * feat.size(0)\n                preds = out.argmax(dim=1)\n                corr += (preds == lbl).sum().item()\n                tot += lbl.size(0)\n            train_losses.append(t_loss / tot)\n            train_accs.append(corr / tot)\n\n            user_model.eval()\n            v_loss, v_corr, v_tot = 0.0, 0, 0\n            with torch.no_grad():\n                for batch in usr_val_loader:\n                    feat = batch[\"feat\"].to(device)\n                    lbl = batch[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = criterion_usr(out, lbl)\n                    v_loss += loss.item() * feat.size(0)\n                    preds = out.argmax(dim=1)\n                    v_corr += (preds == lbl).sum().item()\n                    v_tot += lbl.size(0)\n            val_losses.append(v_loss / v_tot)\n            val_accs.append(v_corr / v_tot)\n\n        # Test evaluation\n        test_preds, test_gt = [], []\n        user_model.eval()\n        with torch.no_grad():\n            for batch in usr_test_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                p = out.argmax(dim=1).cpu().numpy()\n                test_preds.extend(p.tolist())\n                test_gt.extend(lbl.cpu().numpy().tolist())\n\n        key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n        experiment_data[\"batch_size\"][key] = {\n            \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n            \"losses\": {\"train\": np.array(train_losses), \"val\": np.array(val_losses)},\n            \"predictions\": np.array(test_preds),\n            \"ground_truth\": np.array(test_gt),\n        }\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Synthetic dataset generation\nnp.random.seed(0)\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# AI model definition & training\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nai_model = AIModel(D, 16, 2).to(device)\ncriterion_ai = nn.CrossEntropyLoss()\noptimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n\n\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nai_train_loader = DataLoader(SimpleDS(X_train, y_train), batch_size=32, shuffle=True)\nai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=32)\nfor epoch in range(15):\n    ai_model.train()\n    for batch in ai_train_loader:\n        xb, yb = batch[\"x\"].to(device), batch[\"y\"].to(device)\n        out = ai_model(xb)\n        loss = criterion_ai(out, yb)\n        optimizer_ai.zero_grad()\n        loss.backward()\n        optimizer_ai.step()\n    ai_model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in ai_val_loader:\n            xb, yb = batch[\"x\"].to(device), batch[\"y\"].to(device)\n            val_loss += criterion_ai(ai_model(xb), yb).item() * xb.size(0)\n    val_loss /= len(ai_val_loader.dataset)\n    print(f\"Epoch {epoch}: AI val loss = {val_loss:.4f}\")\n\n# Generate AI explanations\nai_model.eval()\nwith torch.no_grad():\n    X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n    logits_all = ai_model(X_all)\n    probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\np_train = probs_all[: len(X_train)]\np_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\np_test = probs_all[-len(X_test) :]\nf_train = np.argmax(p_train, axis=1)\nf_val = np.argmax(p_val, axis=1)\nf_test = np.argmax(p_test, axis=1)\n\n# Prepare user dataset\nX_usr_train = np.hstack([X_train, p_train])\nX_usr_val = np.hstack([X_val, p_val])\nX_usr_test = np.hstack([X_test, p_test])\n\n\nclass UserDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\nusr_train_loader = DataLoader(UserDS(X_usr_train, f_train), batch_size=32, shuffle=True)\nusr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=32)\nusr_test_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=32)\n\n\n# User model definition\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameter sweep on weight_decay\nweight_decays = np.logspace(-5, -2, 4)\nexperiment_data = {\n    \"weight_decay\": {\n        \"static_explainer\": {\n            \"weight_decays\": weight_decays.tolist(),\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor wd in weight_decays:\n    usr_model = UserModel(D + 2, 8, 2).to(device)\n    optimizer_usr = optim.Adam(usr_model.parameters(), lr=1e-2, weight_decay=wd)\n    train_accs, val_accs = [], []\n    train_losses, val_losses = [], []\n    for epoch in range(20):\n        usr_model.train()\n        t_loss, t_corr, t_tot = 0.0, 0, 0\n        for batch in usr_train_loader:\n            xb, yb = batch[\"feat\"].to(device), batch[\"label\"].to(device)\n            out = usr_model(xb)\n            loss = nn.CrossEntropyLoss()(out, yb)\n            optimizer_usr.zero_grad()\n            loss.backward()\n            optimizer_usr.step()\n            t_loss += loss.item() * xb.size(0)\n            preds = out.argmax(1)\n            t_corr += (preds == yb).sum().item()\n            t_tot += len(yb)\n        train_losses.append(t_loss / t_tot)\n        train_accs.append(t_corr / t_tot)\n        usr_model.eval()\n        v_loss, v_corr, v_tot = 0.0, 0, 0\n        with torch.no_grad():\n            for batch in usr_val_loader:\n                xb, yb = batch[\"feat\"].to(device), batch[\"label\"].to(device)\n                out = usr_model(xb)\n                loss = nn.CrossEntropyLoss()(out, yb)\n                v_loss += loss.item() * xb.size(0)\n                preds = out.argmax(1)\n                v_corr += (preds == yb).sum().item()\n                v_tot += len(yb)\n        val_losses.append(v_loss / v_tot)\n        val_accs.append(v_corr / v_tot)\n    # store per\u2010WD results\n    sd = experiment_data[\"weight_decay\"][\"static_explainer\"]\n    sd[\"metrics\"][\"train\"].append(train_accs)\n    sd[\"metrics\"][\"val\"].append(val_accs)\n    sd[\"losses\"][\"train\"].append(train_losses)\n    sd[\"losses\"][\"val\"].append(val_losses)\n    # test predictions\n    usr_model.eval()\n    test_preds = []\n    with torch.no_grad():\n        for batch in usr_test_loader:\n            xb = batch[\"feat\"].to(device)\n            test_preds.extend(usr_model(xb).argmax(1).cpu().numpy().tolist())\n    sd[\"predictions\"].append(test_preds)\n    if not sd[\"ground_truth\"]:\n        sd[\"ground_truth\"] = f_test.tolist()\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic dataset generation\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Model definitions\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameter grid\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\n\n# Container for all results\nexperiment_data = {\"batch_size\": {}}\n\nfor ai_bs in ai_batch_sizes:\n    # AI data loaders\n    ai_tr_loader = DataLoader(\n        SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True\n    )\n    ai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n\n    # Initialize AI model\n    ai_model = AIModel(D, 16, 2).to(device)\n    criterion_ai = nn.CrossEntropyLoss()\n    optimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n\n    # Train AI model\n    for _ in range(15):\n        ai_model.train()\n        for batch in ai_tr_loader:\n            x = batch[\"x\"].to(device)\n            yb = batch[\"y\"].to(device)\n            out = ai_model(x)\n            loss = criterion_ai(out, yb)\n            optimizer_ai.zero_grad()\n            loss.backward()\n            optimizer_ai.step()\n\n    # Generate AI probabilities\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n    p_train = probs_all[: len(X_train)]\n    p_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\n    p_test = probs_all[-len(X_test) :]\n    f_train = p_train.argmax(axis=1)\n    f_val = p_val.argmax(axis=1)\n    f_test = p_test.argmax(axis=1)\n\n    # Prepare user features\n    X_usr_train = np.hstack([X_train, p_train])\n    X_usr_val = np.hstack([X_val, p_val])\n    X_usr_test = np.hstack([X_test, p_test])\n\n    for usr_bs in usr_batch_sizes:\n        # User data loaders\n        usr_tr_loader = DataLoader(\n            UserDS(X_usr_train, f_train), batch_size=usr_bs, shuffle=True\n        )\n        usr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=usr_bs)\n        usr_test_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=usr_bs)\n\n        # Initialize User model\n        user_model = UserModel(D + 2, 8, 2).to(device)\n        criterion_usr = nn.CrossEntropyLoss()\n        optimizer_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n\n        train_accs, val_accs = [], []\n        train_losses, val_losses = [], []\n\n        # Train User model\n        for _ in range(20):\n            user_model.train()\n            t_loss, corr, tot = 0.0, 0, 0\n            for batch in usr_tr_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                loss = criterion_usr(out, lbl)\n                optimizer_usr.zero_grad()\n                loss.backward()\n                optimizer_usr.step()\n                t_loss += loss.item() * feat.size(0)\n                preds = out.argmax(dim=1)\n                corr += (preds == lbl).sum().item()\n                tot += lbl.size(0)\n            train_losses.append(t_loss / tot)\n            train_accs.append(corr / tot)\n\n            user_model.eval()\n            v_loss, v_corr, v_tot = 0.0, 0, 0\n            with torch.no_grad():\n                for batch in usr_val_loader:\n                    feat = batch[\"feat\"].to(device)\n                    lbl = batch[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = criterion_usr(out, lbl)\n                    v_loss += loss.item() * feat.size(0)\n                    preds = out.argmax(dim=1)\n                    v_corr += (preds == lbl).sum().item()\n                    v_tot += lbl.size(0)\n            val_losses.append(v_loss / v_tot)\n            val_accs.append(v_corr / v_tot)\n\n        # Test evaluation\n        test_preds, test_gt = [], []\n        user_model.eval()\n        with torch.no_grad():\n            for batch in usr_test_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                p = out.argmax(dim=1).cpu().numpy()\n                test_preds.extend(p.tolist())\n                test_gt.extend(lbl.cpu().numpy().tolist())\n\n        key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n        experiment_data[\"batch_size\"][key] = {\n            \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n            \"losses\": {\"train\": np.array(train_losses), \"val\": np.array(val_losses)},\n            \"predictions\": np.array(test_preds),\n            \"ground_truth\": np.array(test_gt),\n        }\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic dataset generation\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Model definitions\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameter grid\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\n\n# Container for all results\nexperiment_data = {\"batch_size\": {}}\n\nfor ai_bs in ai_batch_sizes:\n    # AI data loaders\n    ai_tr_loader = DataLoader(\n        SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True\n    )\n    ai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n\n    # Initialize AI model\n    ai_model = AIModel(D, 16, 2).to(device)\n    criterion_ai = nn.CrossEntropyLoss()\n    optimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n\n    # Train AI model\n    for _ in range(15):\n        ai_model.train()\n        for batch in ai_tr_loader:\n            x = batch[\"x\"].to(device)\n            yb = batch[\"y\"].to(device)\n            out = ai_model(x)\n            loss = criterion_ai(out, yb)\n            optimizer_ai.zero_grad()\n            loss.backward()\n            optimizer_ai.step()\n\n    # Generate AI probabilities\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n    p_train = probs_all[: len(X_train)]\n    p_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\n    p_test = probs_all[-len(X_test) :]\n    f_train = p_train.argmax(axis=1)\n    f_val = p_val.argmax(axis=1)\n    f_test = p_test.argmax(axis=1)\n\n    # Prepare user features\n    X_usr_train = np.hstack([X_train, p_train])\n    X_usr_val = np.hstack([X_val, p_val])\n    X_usr_test = np.hstack([X_test, p_test])\n\n    for usr_bs in usr_batch_sizes:\n        # User data loaders\n        usr_tr_loader = DataLoader(\n            UserDS(X_usr_train, f_train), batch_size=usr_bs, shuffle=True\n        )\n        usr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=usr_bs)\n        usr_test_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=usr_bs)\n\n        # Initialize User model\n        user_model = UserModel(D + 2, 8, 2).to(device)\n        criterion_usr = nn.CrossEntropyLoss()\n        optimizer_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n\n        train_accs, val_accs = [], []\n        train_losses, val_losses = [], []\n\n        # Train User model\n        for _ in range(20):\n            user_model.train()\n            t_loss, corr, tot = 0.0, 0, 0\n            for batch in usr_tr_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                loss = criterion_usr(out, lbl)\n                optimizer_usr.zero_grad()\n                loss.backward()\n                optimizer_usr.step()\n                t_loss += loss.item() * feat.size(0)\n                preds = out.argmax(dim=1)\n                corr += (preds == lbl).sum().item()\n                tot += lbl.size(0)\n            train_losses.append(t_loss / tot)\n            train_accs.append(corr / tot)\n\n            user_model.eval()\n            v_loss, v_corr, v_tot = 0.0, 0, 0\n            with torch.no_grad():\n                for batch in usr_val_loader:\n                    feat = batch[\"feat\"].to(device)\n                    lbl = batch[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = criterion_usr(out, lbl)\n                    v_loss += loss.item() * feat.size(0)\n                    preds = out.argmax(dim=1)\n                    v_corr += (preds == lbl).sum().item()\n                    v_tot += lbl.size(0)\n            val_losses.append(v_loss / v_tot)\n            val_accs.append(v_corr / v_tot)\n\n        # Test evaluation\n        test_preds, test_gt = [], []\n        user_model.eval()\n        with torch.no_grad():\n            for batch in usr_test_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                p = out.argmax(dim=1).cpu().numpy()\n                test_preds.extend(p.tolist())\n                test_gt.extend(lbl.cpu().numpy().tolist())\n\n        key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n        experiment_data[\"batch_size\"][key] = {\n            \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n            \"losses\": {\"train\": np.array(train_losses), \"val\": np.array(val_losses)},\n            \"predictions\": np.array(test_preds),\n            \"ground_truth\": np.array(test_gt),\n        }\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic dataset generation\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Model definitions\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameter grid\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\n\n# Container for all results\nexperiment_data = {\"batch_size\": {}}\n\nfor ai_bs in ai_batch_sizes:\n    # AI data loaders\n    ai_tr_loader = DataLoader(\n        SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True\n    )\n    ai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n\n    # Initialize AI model\n    ai_model = AIModel(D, 16, 2).to(device)\n    criterion_ai = nn.CrossEntropyLoss()\n    optimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n\n    # Train AI model\n    for _ in range(15):\n        ai_model.train()\n        for batch in ai_tr_loader:\n            x = batch[\"x\"].to(device)\n            yb = batch[\"y\"].to(device)\n            out = ai_model(x)\n            loss = criterion_ai(out, yb)\n            optimizer_ai.zero_grad()\n            loss.backward()\n            optimizer_ai.step()\n\n    # Generate AI probabilities\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n    p_train = probs_all[: len(X_train)]\n    p_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\n    p_test = probs_all[-len(X_test) :]\n    f_train = p_train.argmax(axis=1)\n    f_val = p_val.argmax(axis=1)\n    f_test = p_test.argmax(axis=1)\n\n    # Prepare user features\n    X_usr_train = np.hstack([X_train, p_train])\n    X_usr_val = np.hstack([X_val, p_val])\n    X_usr_test = np.hstack([X_test, p_test])\n\n    for usr_bs in usr_batch_sizes:\n        # User data loaders\n        usr_tr_loader = DataLoader(\n            UserDS(X_usr_train, f_train), batch_size=usr_bs, shuffle=True\n        )\n        usr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=usr_bs)\n        usr_test_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=usr_bs)\n\n        # Initialize User model\n        user_model = UserModel(D + 2, 8, 2).to(device)\n        criterion_usr = nn.CrossEntropyLoss()\n        optimizer_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n\n        train_accs, val_accs = [], []\n        train_losses, val_losses = [], []\n\n        # Train User model\n        for _ in range(20):\n            user_model.train()\n            t_loss, corr, tot = 0.0, 0, 0\n            for batch in usr_tr_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                loss = criterion_usr(out, lbl)\n                optimizer_usr.zero_grad()\n                loss.backward()\n                optimizer_usr.step()\n                t_loss += loss.item() * feat.size(0)\n                preds = out.argmax(dim=1)\n                corr += (preds == lbl).sum().item()\n                tot += lbl.size(0)\n            train_losses.append(t_loss / tot)\n            train_accs.append(corr / tot)\n\n            user_model.eval()\n            v_loss, v_corr, v_tot = 0.0, 0, 0\n            with torch.no_grad():\n                for batch in usr_val_loader:\n                    feat = batch[\"feat\"].to(device)\n                    lbl = batch[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = criterion_usr(out, lbl)\n                    v_loss += loss.item() * feat.size(0)\n                    preds = out.argmax(dim=1)\n                    v_corr += (preds == lbl).sum().item()\n                    v_tot += lbl.size(0)\n            val_losses.append(v_loss / v_tot)\n            val_accs.append(v_corr / v_tot)\n\n        # Test evaluation\n        test_preds, test_gt = [], []\n        user_model.eval()\n        with torch.no_grad():\n            for batch in usr_test_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                p = out.argmax(dim=1).cpu().numpy()\n                test_preds.extend(p.tolist())\n                test_gt.extend(lbl.cpu().numpy().tolist())\n\n        key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n        experiment_data[\"batch_size\"][key] = {\n            \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n            \"losses\": {\"train\": np.array(train_losses), \"val\": np.array(val_losses)},\n            \"predictions\": np.array(test_preds),\n            \"ground_truth\": np.array(test_gt),\n        }\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Epoch 0: AI val loss = 0.3713', '\\n', 'Epoch 1: AI\nval loss = 0.3778', '\\n', 'Epoch 2: AI val loss = 0.3716', '\\n', 'Epoch 3: AI\nval loss = 0.3739', '\\n', 'Epoch 4: AI val loss = 0.3740', '\\n', 'Epoch 5: AI\nval loss = 0.3715', '\\n', 'Epoch 6: AI val loss = 0.3738', '\\n', 'Epoch 7: AI\nval loss = 0.3714', '\\n', 'Epoch 8: AI val loss = 0.3680', '\\n', 'Epoch 9: AI\nval loss = 0.3694', '\\n', 'Epoch 10: AI val loss = 0.3687', '\\n', 'Epoch 11: AI\nval loss = 0.3689', '\\n', 'Epoch 12: AI val loss = 0.3705', '\\n', 'Epoch 13: AI\nval loss = 0.3700', '\\n', 'Epoch 14: AI val loss = 0.3719', '\\n', 'Epoch 0:\nvalidation_loss = 0.1379', '\\n', 'Epoch 1: validation_loss = 0.0539', '\\n',\n'Epoch 2: validation_loss = 0.0401', '\\n', 'Epoch 3: validation_loss = 0.0310',\n'\\n', 'Epoch 4: validation_loss = 0.0261', '\\n', 'Epoch 5: validation_loss =\n0.0245', '\\n', 'Epoch 6: validation_loss = 0.0214', '\\n', 'Epoch 7:\nvalidation_loss = 0.0216', '\\n', 'Epoch 8: validation_loss = 0.0175', '\\n',\n'Epoch 9: validation_loss = 0.0160', '\\n', 'Epoch 10: validation_loss = 0.0183',\n'\\n', 'Epoch 11: validation_loss = 0.0148', '\\n', 'Epoch 12: validation_loss =\n0.0155', '\\n', 'Epoch 13: validation_loss = 0.0130', '\\n', 'Epoch 14:\nvalidation_loss = 0.0133', '\\n', 'Epoch 15: validation_loss = 0.0157', '\\n',\n'Epoch 16: validation_loss = 0.0122', '\\n', 'Epoch 17: validation_loss =\n0.0123', '\\n', 'Epoch 18: validation_loss = 0.0124', '\\n', 'Epoch 19:\nvalidation_loss = 0.0123', '\\n', 'Execution time: 3 seconds seconds (time limit\nis an hour).']", "['Using device: cuda', '\\n', 'AI Epoch 0: val loss = 0.3713', '\\n', 'AI Epoch 1:\nval loss = 0.3778', '\\n', 'AI Epoch 2: val loss = 0.3716', '\\n', 'AI Epoch 3:\nval loss = 0.3739', '\\n', 'AI Epoch 4: val loss = 0.3740', '\\n', 'AI Epoch 5:\nval loss = 0.3715', '\\n', 'AI Epoch 6: val loss = 0.3738', '\\n', 'AI Epoch 7:\nval loss = 0.3714', '\\n', 'AI Epoch 8: val loss = 0.3680', '\\n', 'AI Epoch 9:\nval loss = 0.3694', '\\n', 'AI Epoch 10: val loss = 0.3687', '\\n', 'AI Epoch 11:\nval loss = 0.3689', '\\n', 'AI Epoch 12: val loss = 0.3705', '\\n', 'AI Epoch 13:\nval loss = 0.3700', '\\n', 'AI Epoch 14: val loss = 0.3719', '\\n', 'Completed\nLR=1.0e-04, final val acc=0.9000', '\\n', 'Completed LR=5.0e-04, final val\nacc=0.9933', '\\n', 'Completed LR=1.0e-03, final val acc=0.9967', '\\n',\n'Completed LR=5.0e-03, final val acc=0.9933', '\\n', 'Completed LR=1.0e-02, final\nval acc=0.9967', '\\n', 'Completed LR=5.0e-02, final val acc=0.9933', '\\n',\n'Execution time: 8 seconds seconds (time limit is an hour).']", "['Execution time: 15 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: AI val loss = 0.3713', '\\n', 'Epoch 1: AI\nval loss = 0.3778', '\\n', 'Epoch 2: AI val loss = 0.3716', '\\n', 'Epoch 3: AI\nval loss = 0.3739', '\\n', 'Epoch 4: AI val loss = 0.3740', '\\n', 'Epoch 5: AI\nval loss = 0.3715', '\\n', 'Epoch 6: AI val loss = 0.3738', '\\n', 'Epoch 7: AI\nval loss = 0.3714', '\\n', 'Epoch 8: AI val loss = 0.3680', '\\n', 'Epoch 9: AI\nval loss = 0.3694', '\\n', 'Epoch 10: AI val loss = 0.3687', '\\n', 'Epoch 11: AI\nval loss = 0.3689', '\\n', 'Epoch 12: AI val loss = 0.3705', '\\n', 'Epoch 13: AI\nval loss = 0.3700', '\\n', 'Epoch 14: AI val loss = 0.3719', '\\n', 'Execution\ntime: 6 seconds seconds (time limit is an hour).']", "['Execution time: 15 seconds seconds (time limit is an hour).']", "['Execution time: 15 seconds seconds (time limit is an hour).']", "['Execution time: 15 seconds seconds (time limit is an hour).']", ""], "analysis": ["", "", "", "", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset", "data": [{"dataset_name": "Dataset 1", "final_value": 0.9958333333333333, "best_value": 0.9958333333333333}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "Dataset 1", "final_value": 0.012646730159176514, "best_value": 0.012646730159176514}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset", "data": [{"dataset_name": "Dataset 2", "final_value": 0.9966666666666667, "best_value": 0.9966666666666667}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "Dataset 2", "final_value": 0.012335669596542781, "best_value": 0.012335669596542781}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset", "data": [{"dataset_name": "Dataset 3", "final_value": 0.99, "best_value": 0.99}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "Model accuracy", "data": [{"dataset_name": "Fitting set", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "Tuning set", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "Holdout set", "final_value": 0.98, "best_value": 0.98}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Model loss", "data": [{"dataset_name": "Fitting set", "final_value": 0.0116, "best_value": 0.0116}, {"dataset_name": "Tuning set", "final_value": 0.0126, "best_value": 0.0126}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.995, "best_value": 0.995}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0152, "best_value": 0.0152}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0162, "best_value": 0.0162}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0195, "best_value": 0.0195}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.014, "best_value": 0.014}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0142, "best_value": 0.0142}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0119, "best_value": 0.0119}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0138, "best_value": 0.0138}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0188, "best_value": 0.0188}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9867, "best_value": 0.9867}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.99, "best_value": 0.99}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0095, "best_value": 0.0095}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0103, "best_value": 0.0103}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0157, "best_value": 0.0157}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.0083, "best_value": 0.0083}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0212, "best_value": 0.0212}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0179, "best_value": 0.0179}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0345, "best_value": 0.0345}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0182, "best_value": 0.0182}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test accuracy", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.992, "best_value": 0.992}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.998, "best_value": 0.998}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Proportion of correctly classified examples on the training split.", "data": [{"dataset_name": "dataset", "final_value": 0.9967, "best_value": 0.9967}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Cross-entropy loss on the training split.", "data": [{"dataset_name": "dataset", "final_value": 0.0123, "best_value": 0.0123}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Proportion of correctly classified examples on the validation split.", "data": [{"dataset_name": "dataset", "final_value": 0.9967, "best_value": 0.9967}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Cross-entropy loss on the validation split.", "data": [{"dataset_name": "dataset", "final_value": 0.012, "best_value": 0.012}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Proportion of correctly classified examples on the test split.", "data": [{"dataset_name": "dataset", "final_value": 0.994, "best_value": 0.994}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.995, "best_value": 0.995}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0152, "best_value": 0.0152}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0162, "best_value": 0.0162}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0195, "best_value": 0.0195}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.014, "best_value": 0.014}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0142, "best_value": 0.0142}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0119, "best_value": 0.0119}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0138, "best_value": 0.0138}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0188, "best_value": 0.0188}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9867, "best_value": 0.9867}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.99, "best_value": 0.99}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0095, "best_value": 0.0095}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0103, "best_value": 0.0103}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0157, "best_value": 0.0157}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.0083, "best_value": 0.0083}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0212, "best_value": 0.0212}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0179, "best_value": 0.0179}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0345, "best_value": 0.0345}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0182, "best_value": 0.0182}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test accuracy", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.992, "best_value": 0.992}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.998, "best_value": 0.998}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy for each dataset", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.995, "best_value": 0.995}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss for each dataset", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0152, "best_value": 0.0152}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0162, "best_value": 0.0162}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0195, "best_value": 0.0195}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.014, "best_value": 0.014}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0142, "best_value": 0.0142}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0119, "best_value": 0.0119}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0138, "best_value": 0.0138}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0188, "best_value": 0.0188}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy for each dataset", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9867, "best_value": 0.9867}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.99, "best_value": 0.99}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss for each dataset", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0095, "best_value": 0.0095}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0103, "best_value": 0.0103}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0157, "best_value": 0.0157}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.0083, "best_value": 0.0083}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0212, "best_value": 0.0212}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0179, "best_value": 0.0179}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0345, "best_value": 0.0345}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0182, "best_value": 0.0182}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test accuracy for each dataset", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.992, "best_value": 0.992}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.998, "best_value": 0.998}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training set", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.995, "best_value": 0.995}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0152, "best_value": 0.0152}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0162, "best_value": 0.0162}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0195, "best_value": 0.0195}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.014, "best_value": 0.014}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0142, "best_value": 0.0142}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0119, "best_value": 0.0119}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0138, "best_value": 0.0138}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0188, "best_value": 0.0188}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9867, "best_value": 0.9867}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.99, "best_value": 0.99}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0095, "best_value": 0.0095}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0103, "best_value": 0.0103}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0157, "best_value": 0.0157}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.0083, "best_value": 0.0083}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0212, "best_value": 0.0212}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0179, "best_value": 0.0179}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0345, "best_value": 0.0345}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0182, "best_value": 0.0182}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test set", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.992, "best_value": 0.992}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.998, "best_value": 0.998}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, true, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_accuracy.png", "../../logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_loss.png", "../../logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_test_distribution.png"], ["../../logs/0-run/experiment_results/experiment_99ec7bfa3f744471b49767d42b310922_proc_2565127/learning_rate_sweep_user_model_train_accuracy.png", "../../logs/0-run/experiment_results/experiment_99ec7bfa3f744471b49767d42b310922_proc_2565127/learning_rate_sweep_user_model_test_accuracy.png", "../../logs/0-run/experiment_results/experiment_99ec7bfa3f744471b49767d42b310922_proc_2565127/learning_rate_sweep_user_model_val_accuracy.png", "../../logs/0-run/experiment_results/experiment_99ec7bfa3f744471b49767d42b310922_proc_2565127/learning_rate_sweep_user_model_val_loss.png", "../../logs/0-run/experiment_results/experiment_99ec7bfa3f744471b49767d42b310922_proc_2565127/learning_rate_sweep_user_model_train_loss.png"], ["../../logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_32_user_bs_32_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_16_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_16_train_val_curves.png"], ["../../logs/0-run/experiment_results/experiment_edb57f5261a34434b163f5d94a1f8d96_proc_2565129/static_explainer_weight_decay_test_accuracy.png", "../../logs/0-run/experiment_results/experiment_edb57f5261a34434b163f5d94a1f8d96_proc_2565129/static_explainer_weight_decay_loss.png", "../../logs/0-run/experiment_results/experiment_edb57f5261a34434b163f5d94a1f8d96_proc_2565129/static_explainer_weight_decay_accuracy.png"], ["../../logs/0-run/experiment_results/experiment_7825e809747547f8a44853d045bf88a6_proc_2565128/ai_bs_64_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_7825e809747547f8a44853d045bf88a6_proc_2565128/ai_bs_32_user_bs_32_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_7825e809747547f8a44853d045bf88a6_proc_2565128/ai_bs_16_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_7825e809747547f8a44853d045bf88a6_proc_2565128/ai_bs_16_user_bs_16_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_7825e809747547f8a44853d045bf88a6_proc_2565128/ai_bs_64_user_bs_16_train_val_curves.png"], ["../../logs/0-run/experiment_results/experiment_14836264e96244ab803c267b614bfeb9_proc_2565127/ai_bs_64_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_14836264e96244ab803c267b614bfeb9_proc_2565127/ai_bs_32_user_bs_32_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_14836264e96244ab803c267b614bfeb9_proc_2565127/ai_bs_16_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_14836264e96244ab803c267b614bfeb9_proc_2565127/ai_bs_16_user_bs_16_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_14836264e96244ab803c267b614bfeb9_proc_2565127/ai_bs_64_user_bs_16_train_val_curves.png"], ["../../logs/0-run/experiment_results/experiment_3e554f7f44604e0894bd0e6d91ed7346_proc_2565129/ai_bs_64_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_3e554f7f44604e0894bd0e6d91ed7346_proc_2565129/ai_bs_32_user_bs_32_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_3e554f7f44604e0894bd0e6d91ed7346_proc_2565129/ai_bs_16_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_3e554f7f44604e0894bd0e6d91ed7346_proc_2565129/ai_bs_16_user_bs_16_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_3e554f7f44604e0894bd0e6d91ed7346_proc_2565129/ai_bs_64_user_bs_16_train_val_curves.png"], ["../../logs/0-run/experiment_results/seed_aggregation_c255d3ad366b41249cbb5ed9c2987413/synthetic_binary_dataset_agg_batch_size_ai_bs_64_user_bs_64_metrics.png", "../../logs/0-run/experiment_results/seed_aggregation_c255d3ad366b41249cbb5ed9c2987413/synthetic_binary_dataset_agg_batch_size_ai_bs_32_user_bs_32_metrics.png", "../../logs/0-run/experiment_results/seed_aggregation_c255d3ad366b41249cbb5ed9c2987413/synthetic_binary_dataset_agg_batch_size_ai_bs_16_user_bs_64_metrics.png", "../../logs/0-run/experiment_results/seed_aggregation_c255d3ad366b41249cbb5ed9c2987413/synthetic_binary_dataset_agg_batch_size_ai_bs_64_user_bs_16_metrics.png", "../../logs/0-run/experiment_results/seed_aggregation_c255d3ad366b41249cbb5ed9c2987413/synthetic_binary_dataset_agg_batch_size_ai_bs_16_user_bs_16_metrics.png"]], "plot_paths": [["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_accuracy.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_loss.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_test_distribution.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_99ec7bfa3f744471b49767d42b310922_proc_2565127/learning_rate_sweep_user_model_train_accuracy.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_99ec7bfa3f744471b49767d42b310922_proc_2565127/learning_rate_sweep_user_model_test_accuracy.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_99ec7bfa3f744471b49767d42b310922_proc_2565127/learning_rate_sweep_user_model_val_accuracy.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_99ec7bfa3f744471b49767d42b310922_proc_2565127/learning_rate_sweep_user_model_val_loss.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_99ec7bfa3f744471b49767d42b310922_proc_2565127/learning_rate_sweep_user_model_train_loss.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_32_user_bs_32_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_16_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_16_train_val_curves.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_edb57f5261a34434b163f5d94a1f8d96_proc_2565129/static_explainer_weight_decay_test_accuracy.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_edb57f5261a34434b163f5d94a1f8d96_proc_2565129/static_explainer_weight_decay_loss.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_edb57f5261a34434b163f5d94a1f8d96_proc_2565129/static_explainer_weight_decay_accuracy.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7825e809747547f8a44853d045bf88a6_proc_2565128/ai_bs_64_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7825e809747547f8a44853d045bf88a6_proc_2565128/ai_bs_32_user_bs_32_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7825e809747547f8a44853d045bf88a6_proc_2565128/ai_bs_16_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7825e809747547f8a44853d045bf88a6_proc_2565128/ai_bs_16_user_bs_16_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7825e809747547f8a44853d045bf88a6_proc_2565128/ai_bs_64_user_bs_16_train_val_curves.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_14836264e96244ab803c267b614bfeb9_proc_2565127/ai_bs_64_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_14836264e96244ab803c267b614bfeb9_proc_2565127/ai_bs_32_user_bs_32_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_14836264e96244ab803c267b614bfeb9_proc_2565127/ai_bs_16_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_14836264e96244ab803c267b614bfeb9_proc_2565127/ai_bs_16_user_bs_16_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_14836264e96244ab803c267b614bfeb9_proc_2565127/ai_bs_64_user_bs_16_train_val_curves.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_3e554f7f44604e0894bd0e6d91ed7346_proc_2565129/ai_bs_64_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_3e554f7f44604e0894bd0e6d91ed7346_proc_2565129/ai_bs_32_user_bs_32_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_3e554f7f44604e0894bd0e6d91ed7346_proc_2565129/ai_bs_16_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_3e554f7f44604e0894bd0e6d91ed7346_proc_2565129/ai_bs_16_user_bs_16_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_3e554f7f44604e0894bd0e6d91ed7346_proc_2565129/ai_bs_64_user_bs_16_train_val_curves.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_c255d3ad366b41249cbb5ed9c2987413/synthetic_binary_dataset_agg_batch_size_ai_bs_64_user_bs_64_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_c255d3ad366b41249cbb5ed9c2987413/synthetic_binary_dataset_agg_batch_size_ai_bs_32_user_bs_32_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_c255d3ad366b41249cbb5ed9c2987413/synthetic_binary_dataset_agg_batch_size_ai_bs_16_user_bs_64_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_c255d3ad366b41249cbb5ed9c2987413/synthetic_binary_dataset_agg_batch_size_ai_bs_64_user_bs_16_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_c255d3ad366b41249cbb5ed9c2987413/synthetic_binary_dataset_agg_batch_size_ai_bs_16_user_bs_16_metrics.png"]], "plot_analyses": [[{"analysis": "Training and validation accuracy rapidly climb from ~0.90 at epoch 1 to ~0.99 by epoch 3, then oscillate narrowly around 0.995\u20131.00 through epoch 20. The close alignment of the two curves and minimal gap suggest the static explainer generalizes well with almost no overfitting and converges within a few epochs.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_accuracy.png"}, {"analysis": "Training loss plummets from ~0.40 to ~0.05 by epoch 3, reaching ~0.01 by epoch 10, then stays near zero. Validation loss follows a similar trajectory, dropping from ~0.14 to ~0.02 by epoch 5 and remaining flat thereafter. The parallel decline of both curves further confirms stable learning and convergence without divergence.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_loss.png"}, {"analysis": "Ground truth class counts (0: 230, 1: 270) closely match predicted counts (0: 228, 1: 272). This near parity shows the static explainer maintains class balance on the test set, making only minor miscounts (\u00b12) and reflecting high-fidelity performance across both classes.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_b20a7f712889421f93ec66e9a453b526_proc_2561566/static_explainer_test_distribution.png"}], [{"analysis": "Training accuracy curves show that learning rates 0.005, 0.01, and 0.05 deliver near-perfect training accuracy within the first two epochs. A rate of 0.001 achieves convergence by around eight epochs, while 0.0005 requires about 15\u201320 epochs to approach 0.98. The smallest rate, 0.0001, is too slow, plateauing below 0.90 even after 20 epochs.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_99ec7bfa3f744471b49767d42b310922_proc_2565127/learning_rate_sweep_user_model_train_accuracy.png"}, {"analysis": "Test accuracy bars indicate strongest generalization (\u22480.99) for learning rates from 0.0005 up to 0.01, with a slight drop at 0.05 (\u22480.98) and substantially lower performance at 0.0001 (\u22480.88).", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_99ec7bfa3f744471b49767d42b310922_proc_2565127/learning_rate_sweep_user_model_test_accuracy.png"}, {"analysis": "Validation accuracy trends mirror the training curves: 0.005\u20130.05 reach \u22480.99 by epochs 1\u20132; 0.001 climbs to that level by around epoch 8; 0.0005 gradually rises to \u22480.99 near epoch 20; and 0.0001 slowly increases to \u22480.90 after 20 epochs.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_99ec7bfa3f744471b49767d42b310922_proc_2565127/learning_rate_sweep_user_model_val_accuracy.png"}, {"analysis": "Validation loss drops fastest for the highest rates: 0.05 and 0.01 fall below 0.05 within two epochs and stabilize near zero, 0.005 drops under 0.02 by epoch four, while 0.001 and 0.0005 show a more gradual decrease and 0.0001 remains above 0.55 after 20 epochs.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_99ec7bfa3f744471b49767d42b310922_proc_2565127/learning_rate_sweep_user_model_val_loss.png"}, {"analysis": "Training loss exhibits similar behavior: 0.05, 0.01, and 0.005 collapse to near-zero within the first three epochs; 0.001 requires about ten epochs to reach very low loss; and 0.0005 and 0.0001 decline slowly, with 0.0001 still above 0.50 at epoch 20.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_99ec7bfa3f744471b49767d42b310922_proc_2565127/learning_rate_sweep_user_model_train_loss.png"}], [{"analysis": "Synthetic binary dataset - ai_bs_64_user_bs_64: Training accuracy ramps up from about 91.5% at epoch 1 to above 99% by epoch 3 and then plateaus with minor oscillations around 99.3\u201399.8%. Validation accuracy closely tracks training, peaking near 99.5% by epoch 10. Training and validation losses both begin high (~0.40 and ~0.22) and decay rapidly to near zero by epoch 10. There is no sign of overfitting; both curves remain well\u2010aligned throughout. Given this rapid convergence, you could reduce the total number of epochs or introduce early stopping around epoch 8 without sacrificing performance.", "valid_plots_received": true, "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_64_train_val_curves.png"}, {"analysis": "Synthetic binary dataset - ai_bs_32_user_bs_32: Accuracy jumps from around 60% at epoch 1 to 98% by epoch 3 and almost 100% by epoch 5. Validation accuracy follows closely, with only small dips around epoch 8 and epoch 16 but never falling below 95%. Loss curves start higher (~0.53 train, ~0.35 val) but drop sharply to below 0.02 by epoch 8. Convergence is slightly faster than with batch size 64, and there is again no evidence of overfitting. Consider reducing epochs to 6\u20138 or experimenting with a slightly lower learning rate to smooth out minor validation fluctuations.", "valid_plots_received": true, "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_32_user_bs_32_train_val_curves.png"}, {"analysis": "Synthetic binary dataset - ai_bs_16_user_bs_64: Training accuracy moves from 65% at epoch 1 to 97% by epoch 4 and stabilizes above 98.5% thereafter. Validation accuracy closely follows, peaking near 99.8% around epoch 10. Loss starts around 0.63 (train) and 0.48 (val), and steadily declines to ~0.02 by epoch 10. The initial slow ramp suggests noisier gradients with the smaller AI batch but longer stability despite the larger user batch. This combination achieves high final performance but could benefit from a warmup schedule or a slightly higher initial learning rate to speed up the early epochs.", "valid_plots_received": true, "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_64_train_val_curves.png"}, {"analysis": "Synthetic binary dataset - ai_bs_16_user_bs_16: Accuracy surges to ~99% by epoch 2 and hovers between 99.5% and 100% thereafter. Validation accuracy consistently tracks at or above training. Loss falls from ~0.42 (train) and ~0.10 (val) at epoch 1 to near zero by epoch 5, with only minor noise later. Small batch sizes produce very fast convergence but also slightly noisier loss curves in later epochs. You could experiment with gradient\u2010noise reduction techniques (e.g., gradient clipping or learning rate decay) to further stabilize training.", "valid_plots_received": true, "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_16_train_val_curves.png"}, {"analysis": "Synthetic binary dataset - ai_bs_64_user_bs_16: Training accuracy climbs from ~90% at epoch 1 to above 98% by epoch 3, then plateaus around 99.2\u201399.8%. Validation accuracy mirrors training, though it dips to ~98.3% around epoch 10 before rebounding. Loss starts around 0.31 (train) and 0.06 (val), dropping to under 0.02 by epoch 8. This mixed\u2010batch configuration converges almost as rapidly as the all\u2010small\u2010batch setup but with slightly smoother late\u2010epoch loss. You might reduce total epochs to 8\u201310 and tune the learning\u2010rate schedule for marginal speed gains.", "valid_plots_received": true, "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_16_train_val_curves.png"}], [{"analysis": "A bar chart depicts test accuracy measured at four weight decay values: 1e-5, 1e-4, 1e-3, and 1e-2. Performance is high across all settings, peaking at 1e-3 (\u22480.998) and showing a slight drop at 1e-2 (\u22480.994), indicating that moderate regularization yields the best generalization and that overly strong weight decay can marginally hurt performance.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_edb57f5261a34434b163f5d94a1f8d96_proc_2565129/static_explainer_weight_decay_test_accuracy.png"}, {"analysis": "A set of overlaid loss curves for training and validation over 20 epochs under the four weight decay sweeps. Training and validation losses drop rapidly in the first few epochs for decay values up to 1e-3 and then plateau near zero, demonstrating fast convergence. The largest decay (1e-2) displays a slower decrease, higher ultimate loss, and more fluctuation in validation, suggesting slight overregularization and reduced stability at high decay.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_edb57f5261a34434b163f5d94a1f8d96_proc_2565129/static_explainer_weight_decay_loss.png"}, {"analysis": "Overlaid accuracy trajectories for training and validation across epochs and decay values. All configurations achieve near-perfect accuracy within a few epochs. Weight decays up to 1e-3 converge smoothly to 99.8\u2013100% accuracy, while 1e-2 shows minor instability in validation accuracy with slight dips around 0.995, confirming the pattern of overregularization at the highest decay.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_edb57f5261a34434b163f5d94a1f8d96_proc_2565129/static_explainer_weight_decay_accuracy.png"}], [{"analysis": "Training accuracy rapidly increases, achieving ~0.99 by epoch 5, and validation accuracy closely follows. Training loss decays from ~0.40 to ~0.01, with validation loss from ~0.22 to ~0.01. Minimal gap indicates no overfitting and stable convergence.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7825e809747547f8a44853d045bf88a6_proc_2565128/ai_bs_64_user_bs_64_train_val_curves.png"}, {"analysis": "Initial training accuracy starts lower (~0.60) but climbs above 0.98 by epoch 4, reaching ~0.99 thereafter; validation accuracy mirrors this trend, starting at ~0.90. Training loss decays from ~0.53 to ~0.015, validation loss from ~0.35 to ~0.015. Convergence is stable with low variance.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7825e809747547f8a44853d045bf88a6_proc_2565128/ai_bs_32_user_bs_32_train_val_curves.png"}, {"analysis": "Training accuracy begins at ~0.66, rises to ~0.98 by epoch 4, and plateaus near ~0.995; validation accuracy follows a similar trajectory. Training loss decreases from ~0.63 to ~0.02, validation loss from ~0.47 to ~0.02. Slightly slower early convergence due to smaller AI batch size, but eventual performance matches others.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7825e809747547f8a44853d045bf88a6_proc_2565128/ai_bs_16_user_bs_64_train_val_curves.png"}, {"analysis": "Training accuracy starts at ~0.76, reaches ~0.995 by epoch 7, and validation accuracy remains around ~0.99\u20131.0. Training loss falls from ~0.415 to ~0.01, validation loss from ~0.10 to ~0.01. Dynamics are balanced with no noticeable overfitting.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7825e809747547f8a44853d045bf88a6_proc_2565128/ai_bs_16_user_bs_16_train_val_curves.png"}, {"analysis": "Training accuracy begins at ~0.90 and rises to ~0.995, while validation accuracy stays around ~0.99. Training loss decays from ~0.31 to ~0.01, and validation loss from ~0.05 to ~0.03, with slight fluctuations in validation loss after epoch 10, potentially due to the smaller user batch size. Overall convergence remains strong.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7825e809747547f8a44853d045bf88a6_proc_2565128/ai_bs_64_user_bs_16_train_val_curves.png"}], [{"analysis": "synthetic binary dataset - ai_bs_64_user_bs_64 exhibits rapid convergence, with training accuracy climbing from ~0.92 to ~0.99 by epoch 3 and validation accuracy following closely from ~0.99 onward. Loss curves drop sharply in the first five epochs and level off near zero, indicating stable training without overfitting. The smooth descent and tight train/val alignment suggest that a batch size of 64 for both AI and user channels yields reliable gradient estimates and effective bias-correction feedback.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_14836264e96244ab803c267b614bfeb9_proc_2565127/ai_bs_64_user_bs_64_train_val_curves.png"}, {"analysis": "synthetic binary dataset - ai_bs_32_user_bs_32 shows a slightly slower start, with training accuracy rising from ~0.60 to ~0.97 by epoch 3, and validation accuracy stabilizing near 0.98\u20130.99 after epoch 4. Initial loss is higher (~0.53) but falls steeply to below 0.02 by epoch 10. This mid\u2010size batch configuration still reaches high performance with minimal generalization gap, though it requires a few more epochs to match the larger-batch run.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_14836264e96244ab803c267b614bfeb9_proc_2565127/ai_bs_32_user_bs_32_train_val_curves.png"}, {"analysis": "synthetic binary dataset - ai_bs_16_user_bs_64 yields a modest training start (~0.66) but catches up to ~0.98 by epoch 4, with validation accuracy mirroring at ~0.97\u20130.99 thereafter. Loss declines quickly, albeit with a slightly longer tail than in larger-AI-batch cases. The combination of small AI batch and large user batch offers fine-grained model updates alongside stable user-bias estimates, trading off a bit of initial convergence speed for smooth long-term accuracy.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_14836264e96244ab803c267b614bfeb9_proc_2565127/ai_bs_16_user_bs_64_train_val_curves.png"}, {"analysis": "synthetic binary dataset - ai_bs_16_user_bs_16 reaches nearly 1.00 on both train and validation by epoch 4, beginning from ~0.77 accuracy at epoch 1. Loss drops precipitously to ~0.01 by epoch 5, but slight fluctuations in validation loss indicate more noise due to smaller batches on both sides. This dual-small batch configuration can achieve top accuracy fastest in the medium term, but at the cost of more variance in each update step.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_14836264e96244ab803c267b614bfeb9_proc_2565127/ai_bs_16_user_bs_16_train_val_curves.png"}, {"analysis": "synthetic binary dataset - ai_bs_64_user_bs_16 delivers strong initial training (~0.90) and validation (~0.99) accuracy by epoch 2, with both settling around ~0.98\u20130.99. Early loss is moderate (~0.31 train, ~0.05 val) and decreases to near zero by epoch 10. While the AI side benefits from large batch stability, the small user batch introduces occasional swings in validation loss. Overall this mix converges quickly but shows some late\u2010epoch jitter in user\u2010feedback adaptation.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_14836264e96244ab803c267b614bfeb9_proc_2565127/ai_bs_64_user_bs_16_train_val_curves.png"}], [{"analysis": "Synthetic binary dataset \u2013 ai_bs_64_user_bs_64 accuracy starts at ~0.915 for training and ~0.990 for validation in epoch 1, then jumps above 0.99 by epoch 2 and remains flat around 0.995 thereafter. Loss drops steeply from ~0.40 (train) and ~0.22 (validation) to below 0.02 by epoch 10. Very fast convergence, minimal generalization gap (\u22640.5%). No signs of over- or under-fitting with this large batch configuration.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_3e554f7f44604e0894bd0e6d91ed7346_proc_2565129/ai_bs_64_user_bs_64_train_val_curves.png"}, {"analysis": "Synthetic binary dataset \u2013 ai_bs_32_user_bs_32 training accuracy begins lower, ~0.60 in epoch 1, but reaches ~0.97 by epoch 2 and plateaus at ~0.995 from epoch 4 onward. Validation accuracy follows closely, rising from ~0.90 to ~0.995. Loss trends mirror the accuracy; initial train/val loss ~0.53/0.35 drop to ~0.02 by epoch 8. Slightly slower warm-up than 64+64 but still very rapid convergence and tight train/val alignment.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_3e554f7f44604e0894bd0e6d91ed7346_proc_2565129/ai_bs_32_user_bs_32_train_val_curves.png"}, {"analysis": "Synthetic binary dataset \u2013 ai_bs_16_user_bs_64 shows a similar pattern: training starts around 0.66 and validation around 0.91 in epoch 1, then both exceed 0.97 by epoch 2 and settle near 0.995. Loss falls from ~0.63 (train) and ~0.46 (val) to ~0.02 by epoch 10. Convergence speed and final performance on par with other settings, though initial learning appears a bit slower due to smaller AI batch size.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_3e554f7f44604e0894bd0e6d91ed7346_proc_2565129/ai_bs_16_user_bs_64_train_val_curves.png"}, {"analysis": "Synthetic binary dataset \u2013 ai_bs_16_user_bs_16 achieves ~0.77 train and ~0.995 validation accuracy in epoch 1, hitting ~0.995/0.995 by epoch 3. Loss declines from ~0.41/0.10 to ~0.01 by epoch 8. Validation accuracy is anomalously high from the first epoch, suggesting very stable performance but worth checking data shuffling or evaluation frequency. Overall, still fast convergence with negligible gap.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_3e554f7f44604e0894bd0e6d91ed7346_proc_2565129/ai_bs_16_user_bs_16_train_val_curves.png"}, {"analysis": "Synthetic binary dataset \u2013 ai_bs_64_user_bs_16 starts training around 0.90/0.99 (train/val) in epoch 1, then both climb above 0.98 by epoch 2 and plateau near 0.995. Loss drops from ~0.30/0.06 to below 0.02 by epoch 8. Equally strong performance with large AI batch and smaller user batch. Consistent, balanced curves, indicating robustness across batch-size settings.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_3e554f7f44604e0894bd0e6d91ed7346_proc_2565129/ai_bs_64_user_bs_16_train_val_curves.png"}], []], "vlm_feedback_summary": ["The static explainer attains near-perfect accuracy and minimal loss, converging\nrapidly and generalizing robustly. Test predictions closely mirror the true\nclass distribution, establishing a strong baseline for subsequent co-adaptive\nexplainability experiments.", "The sweep identifies an optimal learning rate range between 0.005 and 0.01,\nwhere both fast convergence and high test performance are achieved. Extremely\nhigh rates (0.05) converge quickly but show a slight generalization drop, while\nvery low rates (\u22640.0001) are too slow. Recommend selecting lr\u22480.005\u20130.01 with\nearly stopping around epoch 5\u20138. For further robustness testing, introduce two\nHuggingFace classification benchmarks: the e-SNLI dataset (e-snli) for\nexplanation-grounded NLI and the AG News dataset (ag_news) for multi-class news\nclassification.", "All five batch\u2010size configurations achieve near\u2010perfect performance on the\nsynthetic binary dataset with no overfitting detected. Smaller AI batch sizes\n(16, 32) reach high accuracy faster, while larger validation batches provide\nslightly smoother loss curves. Training beyond epoch 8 yields diminishing\nreturns; consider early stopping or epoch reduction and fine\u2010tuned learning rate\nschedules to optimize training time without architecture changes.", "Static explainer results are robust to weight decay changes, with an optimal\nsetting around 1e-3. Very high decay (1e-2) overregularizes the model, slowing\nconvergence and destabilizing validation metrics. Recommended to adopt WD=1e-3\nfor baseline. Introduce two new HuggingFace classification benchmarks such as\nag_news and dbpedia_14 to further test generalization across domains.", "All five batch-size configurations yield near-perfect performance on the\nsynthetic binary dataset, characterized by rapid loss decay and high accuracy.\nSmaller AI batch sizes slow initial convergence but catch up by epoch 5, while\nlarger user batch sizes produce smoother validation curves. The\nai_bs_32_user_bs_32 pairing offers an excellent balance between convergence\nspeed and stability.", "All five batch-size configurations converge to >98% accuracy and near-zero loss\nwithin 10 epochs, with differences in convergence speed and stability. Larger AI\nbatch sizes speed up early accuracy gains while larger user batches smooth out\nvalidation fluctuations. Mid\u2010range batch sizes (e.g., ai_bs=32, user_bs=32)\noffer a balanced tradeoff. To further evaluate generality, test the co-adaptive\ninterface on real-world binary tasks such as the HuggingFace \u201cimdb\u201d sentiment\ndataset and \u201cyelp_polarity\u201d reviews dataset.", "All five hyperparameter configurations converge extremely quickly to ~99.5%\naccuracy with very low loss and minimal train-validation gap. Larger AI batch\nsizes (64) offer the fastest initial ramp-up, while smaller AI batches (16, 32)\nstill reach identical final performance within a few epochs. Given the plateau\nby epoch 5\u20138, epochs could be reduced to save compute. Next, introduce\nreal\u2010world binary classification benchmarks to test generalization beyond\nsynthetic data, for example the GLUE SST-2 sentiment dataset and the UCI Bank\nMarketing dataset from HuggingFace, to validate the co\u2010adaptive explainer under\nmore challenging, noisy conditions.", "[]"], "exec_time": [3.225198745727539, 8.648716926574707, 15.119279384613037, 6.642299175262451, 15.372913360595703, 15.305532693862915, 15.141626119613647, null], "exec_time_feedback": ["", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['static_explainer']"], [""], ["ai_bs_64_user_bs_64", "ai_bs_32_user_bs_32", "ai_bs_16_user_bs_64", "ai_bs_16_user_bs_16", "ai_bs_64_user_bs_16"], [""], ["[\"Synthetic binary dataset\"]"], ["['synthetic binary dataset']"], ["[\"Synthetic binary dataset\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nstatic = experiment_data.get(\"static_explainer\", {})\nmetrics = static.get(\"metrics\", {})\nlosses = static.get(\"losses\", {})\npreds = static.get(\"predictions\", [])\ngt = static.get(\"ground_truth\", [])\n\n# Accuracy curves\ntry:\n    acc_train = metrics.get(\"train\", [])\n    acc_val = metrics.get(\"val\", [])\n    epochs = range(1, len(acc_train) + 1)\n    plt.figure()\n    plt.plot(epochs, acc_train, label=\"Train\")\n    plt.plot(epochs, acc_val, label=\"Val\")\n    plt.title(\"Static Explainer Accuracy Curves\\nTraining vs Validation Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"static_explainer_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# Loss curves\ntry:\n    loss_train = losses.get(\"train\", [])\n    loss_val = losses.get(\"val\", [])\n    epochs = range(1, len(loss_train) + 1)\n    plt.figure()\n    plt.plot(epochs, loss_train, label=\"Train\")\n    plt.plot(epochs, loss_val, label=\"Val\")\n    plt.title(\"Static Explainer Loss Curves\\nTraining vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"static_explainer_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Test set prediction distribution\ntry:\n    if preds and gt:\n        counts_pred = np.bincount(preds, minlength=2)\n        counts_gt = np.bincount(gt, minlength=2)\n        classes = np.arange(len(counts_gt))\n        width = 0.35\n        plt.figure()\n        plt.bar(classes - width / 2, counts_gt, width, label=\"Ground Truth\")\n        plt.bar(classes + width / 2, counts_pred, width, label=\"Predictions\")\n        plt.title(\n            \"Static Explainer Test Predictions vs Ground Truth\\nTest Set Class Distribution\"\n        )\n        plt.xlabel(\"Class\")\n        plt.ylabel(\"Count\")\n        plt.xticks(classes)\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"static_explainer_test_distribution.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating prediction distribution plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n# Extract sweep info\nsweep = data[\"learning_rate_sweep\"][\"user_model\"]\nlrs = sweep[\"lrs\"]\ntrain_accs = sweep[\"metrics\"][\"train\"]\nval_accs = sweep[\"metrics\"][\"val\"]\ntrain_losses = sweep[\"losses\"][\"train\"]\nval_losses = sweep[\"losses\"][\"val\"]\npreds = sweep[\"predictions\"]\ngts = sweep[\"ground_truth\"]\n\n# Plot train accuracy curves\ntry:\n    plt.figure()\n    for lr, acc in zip(lrs, train_accs):\n        plt.plot(acc, label=f\"lr={lr}\")\n    plt.title(\"Training Accuracy (Learning Rate Sweep, User Model)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Train Accuracy\")\n    plt.legend()\n    plt.savefig(\n        os.path.join(working_dir, \"learning_rate_sweep_user_model_train_accuracy.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating train accuracy plot: {e}\")\n    plt.close()\n\n# Plot validation accuracy curves\ntry:\n    plt.figure()\n    for lr, acc in zip(lrs, val_accs):\n        plt.plot(acc, label=f\"lr={lr}\")\n    plt.title(\"Validation Accuracy (Learning Rate Sweep, User Model)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Val Accuracy\")\n    plt.legend()\n    plt.savefig(\n        os.path.join(working_dir, \"learning_rate_sweep_user_model_val_accuracy.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating val accuracy plot: {e}\")\n    plt.close()\n\n# Plot train loss curves\ntry:\n    plt.figure()\n    for lr, loss in zip(lrs, train_losses):\n        plt.plot(loss, label=f\"lr={lr}\")\n    plt.title(\"Training Loss (Learning Rate Sweep, User Model)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Train Loss\")\n    plt.legend()\n    plt.savefig(\n        os.path.join(working_dir, \"learning_rate_sweep_user_model_train_loss.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating train loss plot: {e}\")\n    plt.close()\n\n# Plot validation loss curves\ntry:\n    plt.figure()\n    for lr, loss in zip(lrs, val_losses):\n        plt.plot(loss, label=f\"lr={lr}\")\n    plt.title(\"Validation Loss (Learning Rate Sweep, User Model)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Val Loss\")\n    plt.legend()\n    plt.savefig(\n        os.path.join(working_dir, \"learning_rate_sweep_user_model_val_loss.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating val loss plot: {e}\")\n    plt.close()\n\n# Plot test accuracy vs learning rate\ntry:\n    test_acc = [np.mean(np.array(p) == np.array(gt)) for p, gt in zip(preds, gts)]\n    plt.figure()\n    plt.bar([str(lr) for lr in lrs], test_acc)\n    plt.title(\"Test Accuracy vs Learning Rate (User Model Sweep)\")\n    plt.xlabel(\"Learning Rate\")\n    plt.ylabel(\"Test Accuracy\")\n    plt.savefig(\n        os.path.join(working_dir, \"learning_rate_sweep_user_model_test_accuracy.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Select five evenly spaced configurations\nkeys = list(exp[\"batch_size\"].keys())\nif len(keys) > 5:\n    idxs = [int(i * (len(keys) - 1) / 4) for i in range(5)]\nelse:\n    idxs = list(range(len(keys)))\nselected = [keys[i] for i in idxs]\n\n# Plot train/val accuracy and loss for each selected config\nfor key in selected:\n    try:\n        data = exp[\"batch_size\"][key]\n        epochs = range(1, len(data[\"metrics\"][\"train\"]) + 1)\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        # Accuracy subplot\n        axs[0].plot(epochs, data[\"metrics\"][\"train\"], label=\"Train\")\n        axs[0].plot(epochs, data[\"metrics\"][\"val\"], label=\"Validation\")\n        axs[0].set_title(\"Accuracy\")\n        axs[0].set_xlabel(\"Epoch\")\n        axs[0].legend()\n        # Loss subplot\n        axs[1].plot(epochs, data[\"losses\"][\"train\"], label=\"Train\")\n        axs[1].plot(epochs, data[\"losses\"][\"val\"], label=\"Validation\")\n        axs[1].set_title(\"Loss\")\n        axs[1].set_xlabel(\"Epoch\")\n        axs[1].legend()\n        # Composite title\n        fig.suptitle(f\"Synthetic binary dataset - {key}\")\n        # Save and close\n        fname = os.path.join(working_dir, f\"{key}_train_val_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {key}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# Load experiment data\ntry:\n    exp = np.load(data_path, allow_pickle=True).item()\n    sd = exp[\"weight_decay\"][\"static_explainer\"]\n    wds = np.array(sd[\"weight_decays\"])\n    train_accs = sd[\"metrics\"][\"train\"]\n    val_accs = sd[\"metrics\"][\"val\"]\n    train_losses = sd[\"losses\"][\"train\"]\n    val_losses = sd[\"losses\"][\"val\"]\n    preds = sd[\"predictions\"]\n    gt = np.array(sd[\"ground_truth\"])\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    sd = None\n\n# Plot accuracy curves\ntry:\n    if sd:\n        plt.figure()\n        epochs = range(1, len(train_accs[0]) + 1)\n        for i, wd in enumerate(wds):\n            plt.plot(epochs, train_accs[i], \"--\", label=f\"Train wd={wd:.1e}\")\n            plt.plot(epochs, val_accs[i], \"-\", label=f\"Val wd={wd:.1e}\")\n        plt.title(\"Accuracy Curves (Static Explainer, Weight Decay Sweep)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.savefig(\n            os.path.join(working_dir, \"static_explainer_weight_decay_accuracy.png\")\n        )\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\n# Plot loss curves\ntry:\n    if sd:\n        plt.figure()\n        for i, wd in enumerate(wds):\n            plt.plot(epochs, train_losses[i], \"--\", label=f\"Train wd={wd:.1e}\")\n            plt.plot(epochs, val_losses[i], \"-\", label=f\"Val wd={wd:.1e}\")\n        plt.title(\"Loss Curves (Static Explainer, Weight Decay Sweep)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"static_explainer_weight_decay_loss.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n\n# Plot test accuracy vs weight decay\ntry:\n    if sd:\n        test_acc = [np.mean(np.array(p) == gt) for p in preds]\n        plt.figure()\n        plt.bar([f\"{wd:.1e}\" for wd in wds], test_acc)\n        plt.title(\"Test Accuracy vs Weight Decay (Static Explainer)\")\n        plt.xlabel(\"Weight Decay\")\n        plt.ylabel(\"Test Accuracy\")\n        plt.savefig(\n            os.path.join(working_dir, \"static_explainer_weight_decay_test_accuracy.png\")\n        )\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot3: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Select five evenly spaced configurations\nkeys = list(exp[\"batch_size\"].keys())\nif len(keys) > 5:\n    idxs = [int(i * (len(keys) - 1) / 4) for i in range(5)]\nelse:\n    idxs = list(range(len(keys)))\nselected = [keys[i] for i in idxs]\n\n# Plot train/val accuracy and loss for each selected config\nfor key in selected:\n    try:\n        data = exp[\"batch_size\"][key]\n        epochs = range(1, len(data[\"metrics\"][\"train\"]) + 1)\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        # Accuracy subplot\n        axs[0].plot(epochs, data[\"metrics\"][\"train\"], label=\"Train\")\n        axs[0].plot(epochs, data[\"metrics\"][\"val\"], label=\"Validation\")\n        axs[0].set_title(\"Accuracy\")\n        axs[0].set_xlabel(\"Epoch\")\n        axs[0].legend()\n        # Loss subplot\n        axs[1].plot(epochs, data[\"losses\"][\"train\"], label=\"Train\")\n        axs[1].plot(epochs, data[\"losses\"][\"val\"], label=\"Validation\")\n        axs[1].set_title(\"Loss\")\n        axs[1].set_xlabel(\"Epoch\")\n        axs[1].legend()\n        # Composite title\n        fig.suptitle(f\"Synthetic binary dataset - {key}\")\n        # Save and close\n        fname = os.path.join(working_dir, f\"{key}_train_val_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {key}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Select five evenly spaced configurations\nkeys = list(exp[\"batch_size\"].keys())\nif len(keys) > 5:\n    idxs = [int(i * (len(keys) - 1) / 4) for i in range(5)]\nelse:\n    idxs = list(range(len(keys)))\nselected = [keys[i] for i in idxs]\n\n# Plot train/val accuracy and loss for each selected config\nfor key in selected:\n    try:\n        data = exp[\"batch_size\"][key]\n        epochs = range(1, len(data[\"metrics\"][\"train\"]) + 1)\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        # Accuracy subplot\n        axs[0].plot(epochs, data[\"metrics\"][\"train\"], label=\"Train\")\n        axs[0].plot(epochs, data[\"metrics\"][\"val\"], label=\"Validation\")\n        axs[0].set_title(\"Accuracy\")\n        axs[0].set_xlabel(\"Epoch\")\n        axs[0].legend()\n        # Loss subplot\n        axs[1].plot(epochs, data[\"losses\"][\"train\"], label=\"Train\")\n        axs[1].plot(epochs, data[\"losses\"][\"val\"], label=\"Validation\")\n        axs[1].set_title(\"Loss\")\n        axs[1].set_xlabel(\"Epoch\")\n        axs[1].legend()\n        # Composite title\n        fig.suptitle(f\"Synthetic binary dataset - {key}\")\n        # Save and close\n        fname = os.path.join(working_dir, f\"{key}_train_val_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {key}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Select five evenly spaced configurations\nkeys = list(exp[\"batch_size\"].keys())\nif len(keys) > 5:\n    idxs = [int(i * (len(keys) - 1) / 4) for i in range(5)]\nelse:\n    idxs = list(range(len(keys)))\nselected = [keys[i] for i in idxs]\n\n# Plot train/val accuracy and loss for each selected config\nfor key in selected:\n    try:\n        data = exp[\"batch_size\"][key]\n        epochs = range(1, len(data[\"metrics\"][\"train\"]) + 1)\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        # Accuracy subplot\n        axs[0].plot(epochs, data[\"metrics\"][\"train\"], label=\"Train\")\n        axs[0].plot(epochs, data[\"metrics\"][\"val\"], label=\"Validation\")\n        axs[0].set_title(\"Accuracy\")\n        axs[0].set_xlabel(\"Epoch\")\n        axs[0].legend()\n        # Loss subplot\n        axs[1].plot(epochs, data[\"losses\"][\"train\"], label=\"Train\")\n        axs[1].plot(epochs, data[\"losses\"][\"val\"], label=\"Validation\")\n        axs[1].set_title(\"Loss\")\n        axs[1].set_xlabel(\"Epoch\")\n        axs[1].legend()\n        # Composite title\n        fig.suptitle(f\"Synthetic binary dataset - {key}\")\n        # Save and close\n        fname = os.path.join(working_dir, f\"{key}_train_val_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {key}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load all experiment data\ntry:\n    root_dir = os.getenv(\"AI_SCIENTIST_ROOT\", os.getcwd())\n    experiment_data_path_list = [\n        \"experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_7825e809747547f8a44853d045bf88a6_proc_2565128/experiment_data.npy\",\n        \"experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_14836264e96244ab803c267b614bfeb9_proc_2565127/experiment_data.npy\",\n        \"experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_3e554f7f44604e0894bd0e6d91ed7346_proc_2565129/experiment_data.npy\",\n    ]\n    all_data = []\n    for p in experiment_data_path_list:\n        full_path = os.path.join(root_dir, p)\n        exp = np.load(full_path, allow_pickle=True).item()\n        all_data.append(exp)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Determine common batch_size keys and select up to 5\ncommon_keys = set(all_data[0][\"batch_size\"].keys())\nfor d in all_data[1:]:\n    common_keys &= set(d[\"batch_size\"].keys())\ncommon_keys = sorted(common_keys)\nif len(common_keys) > 5:\n    idxs = [int(i * (len(common_keys) - 1) / 4) for i in range(5)]\n    selected_keys = [common_keys[i] for i in idxs]\nelse:\n    selected_keys = common_keys\n\n# Plot aggregated curves per batch size\nfor key in selected_keys:\n    try:\n        acc_trains, acc_vals = [], []\n        loss_trains, loss_vals = [], []\n        for d in all_data:\n            cfg = d[\"batch_size\"][key]\n            acc_trains.append(np.array(cfg[\"metrics\"][\"train\"]))\n            acc_vals.append(np.array(cfg[\"metrics\"][\"val\"]))\n            loss_trains.append(np.array(cfg[\"losses\"][\"train\"]))\n            loss_vals.append(np.array(cfg[\"losses\"][\"val\"]))\n        min_ep = min(len(x) for x in acc_trains)\n        acc_trains = np.vstack([x[:min_ep] for x in acc_trains])\n        acc_vals = np.vstack([x[:min_ep] for x in acc_vals])\n        loss_trains = np.vstack([x[:min_ep] for x in loss_trains])\n        loss_vals = np.vstack([x[:min_ep] for x in loss_vals])\n        mean_at = acc_trains.mean(axis=0)\n        sem_at = acc_trains.std(axis=0, ddof=1) / np.sqrt(acc_trains.shape[0])\n        mean_av = acc_vals.mean(axis=0)\n        sem_av = acc_vals.std(axis=0, ddof=1) / np.sqrt(acc_vals.shape[0])\n        mean_lt = loss_trains.mean(axis=0)\n        sem_lt = loss_trains.std(axis=0, ddof=1) / np.sqrt(loss_trains.shape[0])\n        mean_lv = loss_vals.mean(axis=0)\n        sem_lv = loss_vals.std(axis=0, ddof=1) / np.sqrt(loss_vals.shape[0])\n        epochs = np.arange(1, min_ep + 1)\n\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        axs[0].errorbar(epochs, mean_at, yerr=sem_at, label=\"Train\", capsize=3)\n        axs[0].errorbar(epochs, mean_av, yerr=sem_av, label=\"Validation\", capsize=3)\n        axs[0].set_title(\"Accuracy\")\n        axs[0].set_xlabel(\"Epoch\")\n        axs[0].legend()\n        axs[1].errorbar(epochs, mean_lt, yerr=sem_lt, label=\"Train\", capsize=3)\n        axs[1].errorbar(epochs, mean_lv, yerr=sem_lv, label=\"Validation\", capsize=3)\n        axs[1].set_title(\"Loss\")\n        axs[1].set_xlabel(\"Epoch\")\n        axs[1].legend()\n        fig.suptitle(f\"Synthetic binary dataset - batch_size {key} (Aggregated)\")\n        fname = os.path.join(\n            working_dir, f\"synthetic_binary_dataset_agg_batch_size_{key}_metrics.png\"\n        )\n        plt.savefig(fname)\n        plt.close()\n        print(f\"batch_size {key}: Final Val Acc = {mean_av[-1]:.4f} \u00b1 {sem_av[-1]:.4f}\")\n    except Exception as e:\n        print(f\"Error creating aggregated plot for batch_size {key}: {e}\")\n        plt.close()\n"], "plot_plan": ["We load the saved experiment data and extract training and validation metrics and losses for the static explainer, along with the test set predictions and ground truth. We then generate three standard visualizations: (1) the accuracy curves over epochs for training vs validation, (2) the loss curves over epochs for training vs validation, and (3) a bar chart comparing the class distribution of test set ground truth vs model predictions. Each plot is created in its own try-except block to handle missing data gracefully, and figures are closed after saving. We save each figure under the working directory with descriptive filenames that include the dataset name and plot type. Clear titles and subtitles specify the dataset and the nature of each plot. This approach ensures robust and reproducible visualization of experiment results.", "We load the saved experiment results and pull out the user\u2010model sweep data including learning rates, per\u2010epoch train/val accuracies and losses, plus final test predictions and labels. We then generate four line plots overlaying curves for each learning rate: training accuracy, validation accuracy, training loss, and validation loss across epochs. Next, we compute final test accuracy for each learning rate and display it in a bar chart. Every plot is wrapped in its own try\u2010except block to catch errors, saved to the working directory with clear descriptive filenames, and closed after saving. Titles clearly specify \u201cLearning Rate Sweep, User Model,\u2019 and axes are labeled for readability. We make no synthetic data and only plot what\u2019s in experiment_data.npy using basic matplotlib.", "We will load the saved experiment data and extract the batch\u2010size keys. To avoid overcrowding, we select exactly five configurations evenly spaced across all combinations. For each selected run, we create a single figure with two side\u2010by\u2010side subplots: one for training vs. validation accuracy, and one for training vs. validation loss over epochs. Each subplot is clearly titled (\u201cAccuracy\u201d and \u201cLoss\u201d) and we add a composite suptitle indicating the synthetic binary dataset and the specific batch\u2010size setting. All figures are saved into the working directory under descriptive filenames. Each plotting routine is wrapped in a try\u2010except to ensure errors are caught and figures closed.", "We load the saved experiment_data and extract the static_explainer results for the weight_decay sweep. We then generate three figures, each wrapped in its own try\u2010except: (1) training and validation accuracy curves across epochs for each weight_decay, (2) training and validation loss curves, and (3) bar chart of final test accuracy vs weight_decay. All figures are titled with clear descriptions including the dataset and explainer type, saved to disk under descriptive filenames in the working directory, and closed after saving.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null], "hyperparam_name": [null, "learning_rate", "batch_size", "weight_decay", null, null, null, null], "is_seed_node": [false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, true], "parse_metrics_plan": ["I will load the saved experiment data from the working directory, extract the\nrecorded training and validation accuracies and losses, select their final epoch\nvalues, and compute the overall test accuracy from the stored predictions and\nground truth. Then, I will print each dataset name followed by its specific\nmetrics with clear labels. This code runs immediately upon execution and does\nnot use any entry-point guard.", "The script loads the experiment results dictionary from the working directory\nand extracts the user model hyperparameter sweep data. For each learning rate,\nit grabs the final epoch\u2019s training and validation accuracy and loss, computes\nthe test accuracy from saved predictions, and then prints each under clear\ndataset labels. Metric names like \u201ctrain accuracy\u201d, \u201cvalidation loss\u201d, and \u201ctest\naccuracy\u201d are always spelled out. All code sits at the global level and runs\nimmediately with no plotting or special entry-point guard.", "I will load the saved experiment data from the working directory, then loop over\neach hyperparameter setting under the \"batch_size\" field. For each dataset I\nextract the final training and validation accuracy and loss from the recorded\narrays, compute the test accuracy from the stored predictions and ground truths,\nand print the dataset name followed by clearly labeled metric values. The script\nruns immediately at global scope and does not use any special entry point.", "The script loads the saved experiment_data.npy file from the working directory,\ndrills into the nested \u201cstatic_explainer\u201d section to pull out train/validation\naccuracies and losses for each weight decay, and computes test\u2010set accuracy by\ncomparing saved predictions against ground truth. It then loops over each\nweight_decay value and prints the dataset name followed by each final metric\nwith clear labels (\u201ctraining accuracy,\u201d \u201cvalidation loss,\u201d etc.). All code\nexecutes at the global scope with no special entry point guard.", "I will load the saved experiment data from the working directory, then loop over\neach hyperparameter setting under the \"batch_size\" field. For each dataset I\nextract the final training and validation accuracy and loss from the recorded\narrays, compute the test accuracy from the stored predictions and ground truths,\nand print the dataset name followed by clearly labeled metric values. The script\nruns immediately at global scope and does not use any special entry point.", "I will load the saved experiment data from the working directory, then loop over\neach hyperparameter setting under the \"batch_size\" field. For each dataset I\nextract the final training and validation accuracy and loss from the recorded\narrays, compute the test accuracy from the stored predictions and ground truths,\nand print the dataset name followed by clearly labeled metric values. The script\nruns immediately at global scope and does not use any special entry point.", "I will load the saved experiment data from the working directory, then loop over\neach hyperparameter setting under the \"batch_size\" field. For each dataset I\nextract the final training and validation accuracy and loss from the recorded\narrays, compute the test accuracy from the stored predictions and ground truths,\nand print the dataset name followed by clearly labeled metric values. The script\nruns immediately at global scope and does not use any special entry point.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata = np.load(\n    os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n).item()\nexp = data[\"static_explainer\"]\n\n# Extract training and validation metrics\ntrain_accuracies = exp[\"metrics\"][\"train\"]\nval_accuracies = exp[\"metrics\"][\"val\"]\ntrain_losses = exp[\"losses\"][\"train\"]\nval_losses = exp[\"losses\"][\"val\"]\n\n# Select final epoch values\nfinal_train_accuracy = train_accuracies[-1]\nfinal_train_loss = train_losses[-1]\nfinal_validation_accuracy = val_accuracies[-1]\nfinal_validation_loss = val_losses[-1]\n\n# Compute test accuracy\npreds = np.array(exp[\"predictions\"])\ngt = np.array(exp[\"ground_truth\"])\ntest_accuracy = (preds == gt).mean()\n\n# Print results\nprint(\"Training dataset:\")\nprint(\"Train accuracy:\", final_train_accuracy)\nprint(\"Training loss:\", final_train_loss)\nprint()\nprint(\"Validation dataset:\")\nprint(\"Validation accuracy:\", final_validation_accuracy)\nprint(\"Validation loss:\", final_validation_loss)\nprint()\nprint(\"Test dataset:\")\nprint(\"Test accuracy:\", test_accuracy)\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# Extract user model sweep results\nusr_data = experiment_data[\"learning_rate_sweep\"][\"user_model\"]\nlrs = usr_data[\"lrs\"]\ntrain_accs = usr_data[\"metrics\"][\"train\"]\nval_accs = usr_data[\"metrics\"][\"val\"]\ntrain_losses = usr_data[\"losses\"][\"train\"]\nval_losses = usr_data[\"losses\"][\"val\"]\ntest_preds = usr_data[\"predictions\"]\ntest_gt = usr_data[\"ground_truth\"]\n\n# Iterate over each learning rate and print metrics\nfor lr, tr_acc, vl_acc, tr_loss, vl_loss, preds, gt in zip(\n    lrs, train_accs, val_accs, train_losses, val_losses, test_preds, test_gt\n):\n    print(f\"Learning Rate: {lr}\")\n    print(\"Dataset: Training\")\n    print(f\"train accuracy: {tr_acc[-1]:.4f}\")\n    print(f\"train loss: {tr_loss[-1]:.4f}\")\n    print(\"Dataset: Validation\")\n    print(f\"validation accuracy: {vl_acc[-1]:.4f}\")\n    print(f\"validation loss: {vl_loss[-1]:.4f}\")\n    # Compute test accuracy\n    test_acc = np.mean(np.array(preds) == np.array(gt))\n    print(\"Dataset: Test\")\n    print(f\"test accuracy: {test_acc:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset setting and print final metrics\nfor dataset_name, results in experiment_data.get(\"batch_size\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    # Final train accuracy and loss\n    train_acc = results[\"metrics\"][\"train\"][-1]\n    train_loss = results[\"losses\"][\"train\"][-1]\n    print(f\"train accuracy: {train_acc:.4f}\")\n    print(f\"train loss: {train_loss:.4f}\")\n    # Final validation accuracy and loss\n    val_acc = results[\"metrics\"][\"val\"][-1]\n    val_loss = results[\"losses\"][\"val\"][-1]\n    print(f\"validation accuracy: {val_acc:.4f}\")\n    print(f\"validation loss: {val_loss:.4f}\")\n    # Compute and print test accuracy\n    preds = results[\"predictions\"]\n    gt = results[\"ground_truth\"]\n    test_accuracy = np.mean(preds == gt)\n    print(f\"test accuracy: {test_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Navigate into the static explainer results\nstatic = experiment_data[\"weight_decay\"][\"static_explainer\"]\nweight_decays = static[\"weight_decays\"]\ntrain_accs_list = static[\"metrics\"][\"train\"]\nval_accs_list = static[\"metrics\"][\"val\"]\ntrain_losses_list = static[\"losses\"][\"train\"]\nval_losses_list = static[\"losses\"][\"val\"]\ntest_preds_list = static[\"predictions\"]\nground_truth = np.array(static[\"ground_truth\"])\n\n# Print final epoch metrics for each weight decay\nfor wd, train_accs, val_accs, train_losses, val_losses, preds in zip(\n    weight_decays,\n    train_accs_list,\n    val_accs_list,\n    train_losses_list,\n    val_losses_list,\n    test_preds_list,\n):\n    # Final epoch values\n    final_train_acc = train_accs[-1]\n    final_val_acc = val_accs[-1]\n    final_train_loss = train_losses[-1]\n    final_val_loss = val_losses[-1]\n    test_preds = np.array(preds)\n    test_acc = (test_preds == ground_truth).mean()\n\n    # Output\n    print(f\"Weight decay: {wd:.5f}\")\n    print(\"Dataset: train\")\n    print(f\"training accuracy: {final_train_acc:.4f}\")\n    print(f\"training loss: {final_train_loss:.4f}\")\n    print(\"Dataset: validation\")\n    print(f\"validation accuracy: {final_val_acc:.4f}\")\n    print(f\"validation loss: {final_val_loss:.4f}\")\n    print(\"Dataset: test\")\n    print(f\"test accuracy: {test_acc:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset setting and print final metrics\nfor dataset_name, results in experiment_data.get(\"batch_size\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    # Final train accuracy and loss\n    train_acc = results[\"metrics\"][\"train\"][-1]\n    train_loss = results[\"losses\"][\"train\"][-1]\n    print(f\"train accuracy: {train_acc:.4f}\")\n    print(f\"train loss: {train_loss:.4f}\")\n    # Final validation accuracy and loss\n    val_acc = results[\"metrics\"][\"val\"][-1]\n    val_loss = results[\"losses\"][\"val\"][-1]\n    print(f\"validation accuracy: {val_acc:.4f}\")\n    print(f\"validation loss: {val_loss:.4f}\")\n    # Compute and print test accuracy\n    preds = results[\"predictions\"]\n    gt = results[\"ground_truth\"]\n    test_accuracy = np.mean(preds == gt)\n    print(f\"test accuracy: {test_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset setting and print final metrics\nfor dataset_name, results in experiment_data.get(\"batch_size\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    # Final train accuracy and loss\n    train_acc = results[\"metrics\"][\"train\"][-1]\n    train_loss = results[\"losses\"][\"train\"][-1]\n    print(f\"train accuracy: {train_acc:.4f}\")\n    print(f\"train loss: {train_loss:.4f}\")\n    # Final validation accuracy and loss\n    val_acc = results[\"metrics\"][\"val\"][-1]\n    val_loss = results[\"losses\"][\"val\"][-1]\n    print(f\"validation accuracy: {val_acc:.4f}\")\n    print(f\"validation loss: {val_loss:.4f}\")\n    # Compute and print test accuracy\n    preds = results[\"predictions\"]\n    gt = results[\"ground_truth\"]\n    test_accuracy = np.mean(preds == gt)\n    print(f\"test accuracy: {test_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset setting and print final metrics\nfor dataset_name, results in experiment_data.get(\"batch_size\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    # Final train accuracy and loss\n    train_acc = results[\"metrics\"][\"train\"][-1]\n    train_loss = results[\"losses\"][\"train\"][-1]\n    print(f\"train accuracy: {train_acc:.4f}\")\n    print(f\"train loss: {train_loss:.4f}\")\n    # Final validation accuracy and loss\n    val_acc = results[\"metrics\"][\"val\"][-1]\n    val_loss = results[\"losses\"][\"val\"][-1]\n    print(f\"validation accuracy: {val_acc:.4f}\")\n    print(f\"validation loss: {val_loss:.4f}\")\n    # Compute and print test accuracy\n    preds = results[\"predictions\"]\n    gt = results[\"ground_truth\"]\n    test_accuracy = np.mean(preds == gt)\n    print(f\"test accuracy: {test_accuracy:.4f}\\n\")\n", ""], "parse_term_out": ["['Training dataset:', '\\n', 'Train accuracy:', ' ', '0.9958333333333333', '\\n',\n'Training loss:', ' ', '0.012646730159176514', '\\n', '\\n', 'Validation\ndataset:', '\\n', 'Validation accuracy:', ' ', '0.9966666666666667', '\\n',\n'Validation loss:', ' ', '0.012335669596542781', '\\n', '\\n', 'Test dataset:',\n'\\n', 'Test accuracy:', ' ', '0.99', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Learning Rate: 0.0001', '\\n', 'Dataset: Training', '\\n', 'train accuracy:\n0.8925', '\\n', 'train loss: 0.5565', '\\n', 'Dataset: Validation', '\\n',\n'validation accuracy: 0.9000', '\\n', 'validation loss: 0.5494', '\\n', 'Dataset:\nTest', '\\n', 'test accuracy: 0.8840', '\\n', '\\n', 'Learning Rate: 0.0005', '\\n',\n'Dataset: Training', '\\n', 'train accuracy: 0.9875', '\\n', 'train loss: 0.2289',\n'\\n', 'Dataset: Validation', '\\n', 'validation accuracy: 0.9933', '\\n',\n'validation loss: 0.2096', '\\n', 'Dataset: Test', '\\n', 'test accuracy: 0.9900',\n'\\n', '\\n', 'Learning Rate: 0.001', '\\n', 'Dataset: Training', '\\n', 'train\naccuracy: 0.9975', '\\n', 'train loss: 0.0718', '\\n', 'Dataset: Validation',\n'\\n', 'validation accuracy: 0.9967', '\\n', 'validation loss: 0.0664', '\\n',\n'Dataset: Test', '\\n', 'test accuracy: 0.9960', '\\n', '\\n', 'Learning Rate:\n0.005', '\\n', 'Dataset: Training', '\\n', 'train accuracy: 0.9958', '\\n', 'train\nloss: 0.0190', '\\n', 'Dataset: Validation', '\\n', 'validation accuracy: 0.9933',\n'\\n', 'validation loss: 0.0163', '\\n', 'Dataset: Test', '\\n', 'test accuracy:\n0.9920', '\\n', '\\n', 'Learning Rate: 0.01', '\\n', 'Dataset: Training', '\\n',\n'train accuracy: 0.9967', '\\n', 'train loss: 0.0162', '\\n', 'Dataset:\nValidation', '\\n', 'validation accuracy: 0.9967', '\\n', 'validation loss:\n0.0117', '\\n', 'Dataset: Test', '\\n', 'test accuracy: 0.9960', '\\n', '\\n',\n'Learning Rate: 0.05', '\\n', 'Dataset: Training', '\\n', 'train accuracy:\n0.9958', '\\n', 'train loss: 0.0116', '\\n', 'Dataset: Validation', '\\n',\n'validation accuracy: 0.9933', '\\n', 'validation loss: 0.0126', '\\n', 'Dataset:\nTest', '\\n', 'test accuracy: 0.9800', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: ai_bs_16_user_bs_16', '\\n', 'train accuracy: 0.9933', '\\n', 'train\nloss: 0.0152', '\\n', 'validation accuracy: 0.9933', '\\n', 'validation loss:\n0.0095', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Dataset: ai_bs_16_user_bs_32',\n'\\n', 'train accuracy: 0.9942', '\\n', 'train loss: 0.0162', '\\n', 'validation\naccuracy: 1.0000', '\\n', 'validation loss: 0.0103', '\\n', 'test accuracy:\n0.9960\\n', '\\n', 'Dataset: ai_bs_16_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0195', '\\n', 'validation accuracy: 0.9933', '\\n',\n'validation loss: 0.0157', '\\n', 'test accuracy: 0.9960\\n', '\\n', 'Dataset:\nai_bs_32_user_bs_16', '\\n', 'train accuracy: 0.9958', '\\n', 'train loss:\n0.0140', '\\n', 'validation accuracy: 0.9967', '\\n', 'validation loss: 0.0083',\n'\\n', 'test accuracy: 0.9920\\n', '\\n', 'Dataset: ai_bs_32_user_bs_32', '\\n',\n'train accuracy: 0.9958', '\\n', 'train loss: 0.0142', '\\n', 'validation\naccuracy: 0.9967', '\\n', 'validation loss: 0.0212', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_32_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0186', '\\n', 'validation accuracy: 0.9967', '\\n',\n'validation loss: 0.0179', '\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset:\nai_bs_64_user_bs_16', '\\n', 'train accuracy: 0.9975', '\\n', 'train loss:\n0.0119', '\\n', 'validation accuracy: 0.9867', '\\n', 'validation loss: 0.0345',\n'\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_32', '\\n',\n'train accuracy: 0.9975', '\\n', 'train loss: 0.0138', '\\n', 'validation\naccuracy: 0.9900', '\\n', 'validation loss: 0.0182', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_64', '\\n', 'train accuracy: 0.9950',\n'\\n', 'train loss: 0.0188', '\\n', 'validation accuracy: 0.9900', '\\n',\n'validation loss: 0.0186', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Weight decay: 0.00001', '\\n', 'Dataset: train', '\\n', 'training accuracy:\n0.9958', '\\n', 'training loss: 0.0127', '\\n', 'Dataset: validation', '\\n',\n'validation accuracy: 0.9967', '\\n', 'validation loss: 0.0124', '\\n', 'Dataset:\ntest', '\\n', 'test accuracy: 0.9900', '\\n', '\\n', 'Weight decay: 0.00010', '\\n',\n'Dataset: train', '\\n', 'training accuracy: 0.9967', '\\n', 'training loss:\n0.0123', '\\n', 'Dataset: validation', '\\n', 'validation accuracy: 0.9967', '\\n',\n'validation loss: 0.0120', '\\n', 'Dataset: test', '\\n', 'test accuracy: 0.9940',\n'\\n', '\\n', 'Weight decay: 0.00100', '\\n', 'Dataset: train', '\\n', 'training\naccuracy: 0.9958', '\\n', 'training loss: 0.0193', '\\n', 'Dataset: validation',\n'\\n', 'validation accuracy: 0.9967', '\\n', 'validation loss: 0.0161', '\\n',\n'Dataset: test', '\\n', 'test accuracy: 0.9940', '\\n', '\\n', 'Weight decay:\n0.01000', '\\n', 'Dataset: train', '\\n', 'training accuracy: 0.9900', '\\n',\n'training loss: 0.0550', '\\n', 'Dataset: validation', '\\n', 'validation\naccuracy: 0.9967', '\\n', 'validation loss: 0.0457', '\\n', 'Dataset: test', '\\n',\n'test accuracy: 0.9920', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "['Dataset: ai_bs_16_user_bs_16', '\\n', 'train accuracy: 0.9933', '\\n', 'train\nloss: 0.0152', '\\n', 'validation accuracy: 0.9933', '\\n', 'validation loss:\n0.0095', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Dataset: ai_bs_16_user_bs_32',\n'\\n', 'train accuracy: 0.9942', '\\n', 'train loss: 0.0162', '\\n', 'validation\naccuracy: 1.0000', '\\n', 'validation loss: 0.0103', '\\n', 'test accuracy:\n0.9960\\n', '\\n', 'Dataset: ai_bs_16_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0195', '\\n', 'validation accuracy: 0.9933', '\\n',\n'validation loss: 0.0157', '\\n', 'test accuracy: 0.9960\\n', '\\n', 'Dataset:\nai_bs_32_user_bs_16', '\\n', 'train accuracy: 0.9958', '\\n', 'train loss:\n0.0140', '\\n', 'validation accuracy: 0.9967', '\\n', 'validation loss: 0.0083',\n'\\n', 'test accuracy: 0.9920\\n', '\\n', 'Dataset: ai_bs_32_user_bs_32', '\\n',\n'train accuracy: 0.9958', '\\n', 'train loss: 0.0142', '\\n', 'validation\naccuracy: 0.9967', '\\n', 'validation loss: 0.0212', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_32_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0186', '\\n', 'validation accuracy: 0.9967', '\\n',\n'validation loss: 0.0179', '\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset:\nai_bs_64_user_bs_16', '\\n', 'train accuracy: 0.9975', '\\n', 'train loss:\n0.0119', '\\n', 'validation accuracy: 0.9867', '\\n', 'validation loss: 0.0345',\n'\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_32', '\\n',\n'train accuracy: 0.9975', '\\n', 'train loss: 0.0138', '\\n', 'validation\naccuracy: 0.9900', '\\n', 'validation loss: 0.0182', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_64', '\\n', 'train accuracy: 0.9950',\n'\\n', 'train loss: 0.0188', '\\n', 'validation accuracy: 0.9900', '\\n',\n'validation loss: 0.0186', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: ai_bs_16_user_bs_16', '\\n', 'train accuracy: 0.9933', '\\n', 'train\nloss: 0.0152', '\\n', 'validation accuracy: 0.9933', '\\n', 'validation loss:\n0.0095', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Dataset: ai_bs_16_user_bs_32',\n'\\n', 'train accuracy: 0.9942', '\\n', 'train loss: 0.0162', '\\n', 'validation\naccuracy: 1.0000', '\\n', 'validation loss: 0.0103', '\\n', 'test accuracy:\n0.9960\\n', '\\n', 'Dataset: ai_bs_16_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0195', '\\n', 'validation accuracy: 0.9933', '\\n',\n'validation loss: 0.0157', '\\n', 'test accuracy: 0.9960\\n', '\\n', 'Dataset:\nai_bs_32_user_bs_16', '\\n', 'train accuracy: 0.9958', '\\n', 'train loss:\n0.0140', '\\n', 'validation accuracy: 0.9967', '\\n', 'validation loss: 0.0083',\n'\\n', 'test accuracy: 0.9920\\n', '\\n', 'Dataset: ai_bs_32_user_bs_32', '\\n',\n'train accuracy: 0.9958', '\\n', 'train loss: 0.0142', '\\n', 'validation\naccuracy: 0.9967', '\\n', 'validation loss: 0.0212', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_32_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0186', '\\n', 'validation accuracy: 0.9967', '\\n',\n'validation loss: 0.0179', '\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset:\nai_bs_64_user_bs_16', '\\n', 'train accuracy: 0.9975', '\\n', 'train loss:\n0.0119', '\\n', 'validation accuracy: 0.9867', '\\n', 'validation loss: 0.0345',\n'\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_32', '\\n',\n'train accuracy: 0.9975', '\\n', 'train loss: 0.0138', '\\n', 'validation\naccuracy: 0.9900', '\\n', 'validation loss: 0.0182', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_64', '\\n', 'train accuracy: 0.9950',\n'\\n', 'train loss: 0.0188', '\\n', 'validation accuracy: 0.9900', '\\n',\n'validation loss: 0.0186', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: ai_bs_16_user_bs_16', '\\n', 'train accuracy: 0.9933', '\\n', 'train\nloss: 0.0152', '\\n', 'validation accuracy: 0.9933', '\\n', 'validation loss:\n0.0095', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Dataset: ai_bs_16_user_bs_32',\n'\\n', 'train accuracy: 0.9942', '\\n', 'train loss: 0.0162', '\\n', 'validation\naccuracy: 1.0000', '\\n', 'validation loss: 0.0103', '\\n', 'test accuracy:\n0.9960\\n', '\\n', 'Dataset: ai_bs_16_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0195', '\\n', 'validation accuracy: 0.9933', '\\n',\n'validation loss: 0.0157', '\\n', 'test accuracy: 0.9960\\n', '\\n', 'Dataset:\nai_bs_32_user_bs_16', '\\n', 'train accuracy: 0.9958', '\\n', 'train loss:\n0.0140', '\\n', 'validation accuracy: 0.9967', '\\n', 'validation loss: 0.0083',\n'\\n', 'test accuracy: 0.9920\\n', '\\n', 'Dataset: ai_bs_32_user_bs_32', '\\n',\n'train accuracy: 0.9958', '\\n', 'train loss: 0.0142', '\\n', 'validation\naccuracy: 0.9967', '\\n', 'validation loss: 0.0212', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_32_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0186', '\\n', 'validation accuracy: 0.9967', '\\n',\n'validation loss: 0.0179', '\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset:\nai_bs_64_user_bs_16', '\\n', 'train accuracy: 0.9975', '\\n', 'train loss:\n0.0119', '\\n', 'validation accuracy: 0.9867', '\\n', 'validation loss: 0.0345',\n'\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_32', '\\n',\n'train accuracy: 0.9975', '\\n', 'train loss: 0.0138', '\\n', 'validation\naccuracy: 0.9900', '\\n', 'validation loss: 0.0182', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_64', '\\n', 'train accuracy: 0.9950',\n'\\n', 'train loss: 0.0188', '\\n', 'validation accuracy: 0.9900', '\\n',\n'validation loss: 0.0186', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>

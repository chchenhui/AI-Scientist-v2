{"edges": [[0, 13], [0, 17], [0, 1], [0, 15], [0, 4], [0, 3], [0, 5], [0, 10], [0, 2], [0, 11], [0, 12], [0, 9], [0, 14], [1, 8], [3, 6], [6, 7], [14, 21], [14, 20], [14, 19], [14, 18], [15, 16]], "layout": [[0.46153846153846156, 0.0], [0.0, 0.33333333333333337], [0.07407407407407407, 0.33333333333333337], [0.14814814814814814, 0.33333333333333337], [0.2222222222222222, 0.33333333333333337], [0.2962962962962963, 0.33333333333333337], [0.14814814814814814, 0.6666666666666667], [0.14814814814814814, 1.0], [0.0, 0.6666666666666667], [0.37037037037037035, 0.33333333333333337], [0.4444444444444444, 0.33333333333333337], [0.5185185185185185, 0.33333333333333337], [0.5925925925925926, 0.33333333333333337], [0.6666666666666666, 0.33333333333333337], [0.7407407407407407, 0.33333333333333337], [0.9259259259259259, 0.33333333333333337], [0.9259259259259259, 0.6666666666666667], [1.0, 0.33333333333333337], [0.6296296296296297, 0.6666666666666667], [0.7037037037037037, 0.6666666666666667], [0.7777777777777778, 0.6666666666666667], [0.8518518518518519, 0.6666666666666667]], "plan": ["Hyperparam tuning name: batch_size. Below is a solution that wraps the AI\u2010 and\nuser\u2010model training inside nested loops over a set of batch sizes. For each (AI,\nuser) batch\u2010size pair it reinitializes data loaders, models, and optimizers,\ntrains the AI model for 15 epochs, generates predictions, then trains the user\nmodel for 20 epochs. It collects per\u2010epoch train/val losses and accuracies plus\nfinal test predictions/labels under a key encoding the batch sizes. Finally it\nsaves the resulting `experiment_data` dict to `experiment_data.npy`.", "Ablation name: Synthetic Dataset Diversity Ablation. Here\u2019s a single\u2010file script\nthat defines three synthetic datasets with varying dimensionality, noise levels,\nand weight scales, then runs the AIModel\u2192UserModel pipeline on each. It\npreserves the original hyperparameter grid, collects train/validation accuracies\nand losses as well as test predictions, and organizes results under the\n\u201csynthetic_diversity\u201d ablation. Finally, it saves all collected data to\n`experiment_data.npy` for downstream analysis.", "Ablation name: Hard Label Input Ablation. We extend the baseline by adding two\nablation conditions: feeding the user model either the full softmax\nprobabilities or only the one-hot top\u20101 class. After training the AI for each\nbatch size, we generate both soft and hard label features for the train/val/test\nsplits. For each user batch size and ablation, we train a UserModel, record\nper\u2010epoch train/val accuracy and loss, and collect test predictions against\nground truth. All results are stored in an experiment_data dict under 'soft' and\n'hard' keys, then saved to 'experiment_data.npy'.", "Ablation name: Soft\u2010Label Distillation Loss Ablation. We extend the baseline by\nadding a `SoftLabelDS` that returns both hard labels and the AI\u2019s full soft\nprobabilities as targets, then train two user\u2010models per (ai_bs, usr_bs) pair:\none with CE on hard argmax labels and one with KL\u2010distillation on soft targets.\nWe record the same metrics (train/val accuracies, losses, test preds/GT) under\n`experiment_data['CE_hard_labels']` and\n`experiment_data['soft_label_distillation']`, keyed by\n`ai_bs_{...}_user_bs_{...}`, and finally save everything in\n`experiment_data.npy`.", "Ablation name: Temperature Scaling Ablation. We first train the AI model on the\nsynthetic data once, then loop over T\u2208{0.5,1.0,2.0,5.0}, scaling the AI logits\nby 1/T before softmax to get calibrated probabilities. For each T, we augment\nthe original features with these probabilities, train a user model for 20\nepochs, and record train/val losses and accuracies as well as test predictions\nand ground truth. All results are collected under\n`experiment_data['temperature_scaling']['T_<value>']` and saved to\n`experiment_data.npy`.", "Ablation name: Teacher Feature Removal Ablation. We extend the baseline by\nrunning two user\u2010model regimes: one with both raw inputs and teacher\nprobabilities (\u201cwith_teacher_probs\u201d) and one using only raw inputs\n(\u201craw_features_only\u201d). We keep the same AI and user batch\u2010size sweeps, logging\ntrain/val accuracies and losses plus test predictions and ground truths for each\nrun. All results are aggregated under the key \u201cteacher_feature_removal\u201d in a\nnested dictionary and saved to `experiment_data.npy`. The code below is\nself\u2010contained and directly executable.", "The baseline user\u2010model was wrongly given the AI\u2019s soft\u2010label features, so we\nrestore it to use only the original X inputs and adjust its input size to D. We\nalso compute and log the Mental Model Alignment Rate (the slope of 1\u2013normalized\nJensen\u2013Shannon divergence) at each epoch, print out validation losses, and add\ntwo new ablations (bias\u2010awareness only and full dual\u2010channel) alongside the\nexisting hard\u2010label and soft\u2010distillation conditions. All models and data are\nmoved to GPU/CPU via `.to(device)` and we save every metric, loss, and\npredictions into `experiment_data` at the end.", "We add a guard around the slope computation so that we only call `np.polyfit`\nonce at least two alignment points exist, and default the alignment rate to zero\notherwise. This prevents a degenerate design matrix and SVD errors when fitting\na line to a single point. We also wrap the call in a `try/except` block for\nadditional safety, falling back to a zero rate on any failure. All other\nexperiment logic remains unchanged.", "I\u2019ve extended the user dataset to include the AI\u2019s probability outputs and at\neach epoch compute the Jensen\u2013Shannon divergence between the AI distributions\nand the user model\u2019s softmaxed outputs on validation data, yielding an alignment\nscore that is printed alongside the validation loss. I then fit a simple linear\nslope to the sequence of alignment scores to obtain the Mental Model Alignment\nRate. The training loops now correctly move all tensors to the chosen device,\nand we save every tracked metric (including losses, accuracies,\nalignment_scores, alignment_rate, predictions, and ground truth) into\n`experiment_data.npy` at the end.", "Ablation name: Confidence\u2010Filtered Pseudo\u2010Labeling. We train one AIModel on the\nsynthetic data and compute teacher softmax outputs. For each confidence\nthreshold in [0.6, 0.8, 0.9], we filter the pseudo\u2010labeled training examples by\ntheir max softmax score, train a UserModel on that filtered set, and record\ntrain/val accuracy and losses as well as test predictions versus teacher labels.\nAll results are organized under\nexperiment_data['confidence_filter']['threshold_{t}'] and saved in\n\"experiment_data.npy\".", "Ablation name: Class Imbalance Ablation. We first generate the full synthetic\ndataset once and then subsample it to enforce the desired class ratios (50:50,\n70:30, 90:10). For each ratio, we shuffle and split into train/val/test,\nnormalize the features, train the AIModel to get pseudo\u2010labels and\nprobabilities, then train the UserModel across the batch\u2010size grid and record\nmetrics. All results are stored in a nested dictionary under\n\"class_imbalance\"\u2192ratio name\u2192batch\u2010size key, then saved to\n\"experiment_data.npy\".", "Ablation name: Network Depth Ablation. I propose extending the baseline script\nby introducing a model\u2010builder function that constructs linear (0\u2010layer),\none\u2010layer, or two\u2010layer networks based on a depth parameter. We fix all other\nhyperparameters (batch sizes, hidden dims, learning rates) and loop over\nai_depth and usr_depth in [0,1,2], training the AI teacher, generating\npseudo\u2010labels, and then training the user/student model. For each combination,\nwe record train/val accuracy and loss curves plus test predictions and ground\ntruth into a nested dict under \u201cnetwork_depth_ablation\u201d\u2192\u201csynthetic\u201d and save it\nwith np.save('experiment_data.npy'). This isolates depth effects while keeping\nthe distillation pipeline unchanged.", "Ablation name: Activation Function Ablation. We implement a self\u2010contained\nscript that defines four activation variants (ReLU, Tanh, LeakyReLU, and Linear)\nfor both the teacher (AIModel) and student (UserModel). For each activation, we\ntrain the AIModel on the synthetic classification task, record its train/val\nlosses and accuracies, generate soft labels, then train the UserModel on the\naugmented features and record its train/val metrics as well as final test\npredictions. All results are aggregated in a nested `experiment_data` dict keyed\nby activation variant and saved as `experiment_data.npy`. The script requires no\nexternal inputs and can be run as-is.", "Ablation name: Original Feature Removal Ablation. We train the teacher on raw\ninputs, then for each batch\u2010size combination we extract its softmax outputs and\nonly feed those probabilities (no original features) to the student. We record\ntraining/validation accuracies and losses over epochs and final test predictions\nversus true labels in a nested dict under the ablation\n\u201coriginal_feature_removal\u201d. Finally we save everything as a numpy file.", "Ablation name: Teacher Ensemble Size Ablation. We will loop over teacher\nensemble sizes [1,3,5], and for each we train that many AIModel teachers with\ndifferent random seeds, average their predicted probabilities to form soft\npseudo\u2010labels and augmented features. Then we train a UserModel on these\npseudo\u2010labels for each combination of AI and user batch sizes, tracking\ntraining/validation losses and accuracies over epochs, and final test\npredictions and ground truth. All results are stored in an experiment_data\ndictionary under \u2018teacher_ensemble_size\u2019, keyed by each ensemble size and\nbatch\u2010size combination, and finally saved as a numpy file.", "Ablation name: Teacher Probability Noise Injection Ablation. I will train the\nAIModel once per AI batch size, generate its probability outputs, then for each\nnoise level inject Gaussian noise into those probabilities, renormalize,\nrecompute the teacher labels, and train the UserModel over various user batch\nsizes.  All training/validation accuracies and losses plus test predictions and\nnoisy teacher labels are collected into a nested dict\nexperiment_data['teacher_prob_noise'][sigma]['batch_size'][key].  Finally the\nentire structure is saved via np.save to \"experiment_data.npy\".", "We add a check after clipping to ensure that no probability vector has a sum of\nzero: we compute the row sums, set any zeros to one, and then normalize by these\nadjusted sums to prevent divide-by-zero and eliminate NaNs. This simple fix\nensures that even if all values are clipped to zero, the normalization step will\nnot produce invalid values.", "Ablation name: Confidence-Weighted Pseudo-Label Loss. Here\u2019s a sketch of the\nsolution. We loop over AI batch sizes, train the teacher, get pseudo\u2010labels and\nconfidences, then for each of three ablation types (uniform, thresholded,\nconfidence\u2010weighted) and each user batch size, we train a student model using\nthe appropriate per\u2010sample weighting. We record train/val accuracy and\n(weighted) loss curves plus test predictions, and finally save everything in a\nnested dict under keys \u201cuniform\u201d, \u201cthresholded\u201d, and \u201cconfidence_weighted\u201d.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic dataset generation\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Model definitions\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameter grid\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\n\n# Container for all results\nexperiment_data = {\"batch_size\": {}}\n\nfor ai_bs in ai_batch_sizes:\n    # AI data loaders\n    ai_tr_loader = DataLoader(\n        SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True\n    )\n    ai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n\n    # Initialize AI model\n    ai_model = AIModel(D, 16, 2).to(device)\n    criterion_ai = nn.CrossEntropyLoss()\n    optimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n\n    # Train AI model\n    for _ in range(15):\n        ai_model.train()\n        for batch in ai_tr_loader:\n            x = batch[\"x\"].to(device)\n            yb = batch[\"y\"].to(device)\n            out = ai_model(x)\n            loss = criterion_ai(out, yb)\n            optimizer_ai.zero_grad()\n            loss.backward()\n            optimizer_ai.step()\n\n    # Generate AI probabilities\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n    p_train = probs_all[: len(X_train)]\n    p_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\n    p_test = probs_all[-len(X_test) :]\n    f_train = p_train.argmax(axis=1)\n    f_val = p_val.argmax(axis=1)\n    f_test = p_test.argmax(axis=1)\n\n    # Prepare user features\n    X_usr_train = np.hstack([X_train, p_train])\n    X_usr_val = np.hstack([X_val, p_val])\n    X_usr_test = np.hstack([X_test, p_test])\n\n    for usr_bs in usr_batch_sizes:\n        # User data loaders\n        usr_tr_loader = DataLoader(\n            UserDS(X_usr_train, f_train), batch_size=usr_bs, shuffle=True\n        )\n        usr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=usr_bs)\n        usr_test_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=usr_bs)\n\n        # Initialize User model\n        user_model = UserModel(D + 2, 8, 2).to(device)\n        criterion_usr = nn.CrossEntropyLoss()\n        optimizer_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n\n        train_accs, val_accs = [], []\n        train_losses, val_losses = [], []\n\n        # Train User model\n        for _ in range(20):\n            user_model.train()\n            t_loss, corr, tot = 0.0, 0, 0\n            for batch in usr_tr_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                loss = criterion_usr(out, lbl)\n                optimizer_usr.zero_grad()\n                loss.backward()\n                optimizer_usr.step()\n                t_loss += loss.item() * feat.size(0)\n                preds = out.argmax(dim=1)\n                corr += (preds == lbl).sum().item()\n                tot += lbl.size(0)\n            train_losses.append(t_loss / tot)\n            train_accs.append(corr / tot)\n\n            user_model.eval()\n            v_loss, v_corr, v_tot = 0.0, 0, 0\n            with torch.no_grad():\n                for batch in usr_val_loader:\n                    feat = batch[\"feat\"].to(device)\n                    lbl = batch[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = criterion_usr(out, lbl)\n                    v_loss += loss.item() * feat.size(0)\n                    preds = out.argmax(dim=1)\n                    v_corr += (preds == lbl).sum().item()\n                    v_tot += lbl.size(0)\n            val_losses.append(v_loss / v_tot)\n            val_accs.append(v_corr / v_tot)\n\n        # Test evaluation\n        test_preds, test_gt = [], []\n        user_model.eval()\n        with torch.no_grad():\n            for batch in usr_test_loader:\n                feat = batch[\"feat\"].to(device)\n                lbl = batch[\"label\"].to(device)\n                out = user_model(feat)\n                p = out.argmax(dim=1).cpu().numpy()\n                test_preds.extend(p.tolist())\n                test_gt.extend(lbl.cpu().numpy().tolist())\n\n        key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n        experiment_data[\"batch_size\"][key] = {\n            \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n            \"losses\": {\"train\": np.array(train_losses), \"val\": np.array(val_losses)},\n            \"predictions\": np.array(test_preds),\n            \"ground_truth\": np.array(test_gt),\n        }\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n# Dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Model definitions\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameter grid\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\n\n# Ablation: synthetic dataset diversity\ndataset_configs = [\n    {\"name\": \"D2_noise0.1_scale1.0\", \"D\": 2, \"noise\": 0.1, \"scale\": 1.0},\n    {\"name\": \"D5_noise0.5_scale2.0\", \"D\": 5, \"noise\": 0.5, \"scale\": 2.0},\n    {\"name\": \"D10_noise1.0_scale0.5\", \"D\": 10, \"noise\": 1.0, \"scale\": 0.5},\n]\n\nexperiment_data = {\"synthetic_diversity\": {}}\n\nfor cfg in dataset_configs:\n    name, D, noise, scale = cfg[\"name\"], cfg[\"D\"], cfg[\"noise\"], cfg[\"scale\"]\n    # Generate synthetic data\n    N = 2000\n    X = np.random.randn(N, D)\n    w_true = scale * np.random.randn(D)\n    b_true = 0.5\n    logits = X.dot(w_true) + b_true + noise * np.random.randn(N)\n    probs = 1 / (1 + np.exp(-logits))\n    y = (np.random.rand(N) < probs).astype(int)\n    # Split\n    idx = np.random.permutation(N)\n    train_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_val, y_val = X[val_idx], y[val_idx]\n    X_test, y_test = X[test_idx], y[test_idx]\n    # Normalize\n    mean, std = X_train.mean(0), X_train.std(0) + 1e-6\n    X_train = (X_train - mean) / std\n    X_val = (X_val - mean) / std\n    X_test = (X_test - mean) / std\n\n    # Container for this dataset\n    experiment_data[\"synthetic_diversity\"][name] = {\"batch_size\": {}}\n    for ai_bs in ai_batch_sizes:\n        # Train AI model\n        ai_tr = DataLoader(SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True)\n        ai_val = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n        ai_model = AIModel(D, 16, 2).to(device)\n        opt_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n        crit_ai = nn.CrossEntropyLoss()\n        for _ in range(15):\n            ai_model.train()\n            for b in ai_tr:\n                xb, yb = b[\"x\"].to(device), b[\"y\"].to(device)\n                out = ai_model(xb)\n                loss = crit_ai(out, yb)\n                opt_ai.zero_grad()\n                loss.backward()\n                opt_ai.step()\n        # Generate AI predictions\n        ai_model.eval()\n        with torch.no_grad():\n            X_all = (\n                torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n            )\n            logits_all = ai_model(X_all)\n            probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n        p_tr = probs_all[: len(X_train)]\n        p_vl = probs_all[len(X_train) : len(X_train) + len(X_val)]\n        p_ts = probs_all[-len(X_test) :]\n        f_tr, f_vl, f_ts = p_tr.argmax(1), p_vl.argmax(1), p_ts.argmax(1)\n        # Prepare user features\n        X_utr = np.hstack([X_train, p_tr])\n        X_uvl = np.hstack([X_val, p_vl])\n        X_uts = np.hstack([X_test, p_ts])\n\n        for usr_bs in usr_batch_sizes:\n            usr_tr = DataLoader(UserDS(X_utr, f_tr), batch_size=usr_bs, shuffle=True)\n            usr_vl = DataLoader(UserDS(X_uvl, f_vl), batch_size=usr_bs)\n            usr_ts = DataLoader(UserDS(X_uts, f_ts), batch_size=usr_bs)\n            user_model = UserModel(D + 2, 8, 2).to(device)\n            opt_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n            crit_usr = nn.CrossEntropyLoss()\n            train_accs, val_accs = [], []\n            train_losses, val_losses = [], []\n            # Train User model\n            for _ in range(20):\n                user_model.train()\n                tloss, corr, tot = 0.0, 0, 0\n                for b in usr_tr:\n                    feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = crit_usr(out, lbl)\n                    opt_usr.zero_grad()\n                    loss.backward()\n                    opt_usr.step()\n                    tloss += loss.item() * feat.size(0)\n                    preds = out.argmax(1)\n                    corr += (preds == lbl).sum().item()\n                    tot += lbl.size(0)\n                train_losses.append(tloss / tot)\n                train_accs.append(corr / tot)\n                # Validation\n                user_model.eval()\n                vloss, vcorr, vtot = 0.0, 0, 0\n                with torch.no_grad():\n                    for b in usr_vl:\n                        feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                        out = user_model(feat)\n                        loss = crit_usr(out, lbl)\n                        vloss += loss.item() * feat.size(0)\n                        preds = out.argmax(1)\n                        vcorr += (preds == lbl).sum().item()\n                        vtot += lbl.size(0)\n                val_losses.append(vloss / vtot)\n                val_accs.append(vcorr / vtot)\n\n            # Test\n            test_preds, test_gt = [], []\n            user_model.eval()\n            with torch.no_grad():\n                for b in usr_ts:\n                    feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                    out = user_model(feat)\n                    test_preds.extend(out.argmax(1).cpu().numpy().tolist())\n                    test_gt.extend(lbl.cpu().numpy().tolist())\n\n            key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n            experiment_data[\"synthetic_diversity\"][name][\"batch_size\"][key] = {\n                \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n                \"losses\": {\n                    \"train\": np.array(train_losses),\n                    \"val\": np.array(val_losses),\n                },\n                \"predictions\": np.array(test_preds),\n                \"ground_truth\": np.array(test_gt),\n            }\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic dataset generation\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Model definitions\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameters\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\n\n# Container for experiment data\nexperiment_data = {\"soft\": {}, \"hard\": {}}\n\nfor ai_bs in ai_batch_sizes:\n    # AI data loaders\n    ai_tr_loader = DataLoader(\n        SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True\n    )\n    ai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n    # Initialize AI model\n    ai_model = AIModel(D, 16, 2).to(device)\n    criterion_ai = nn.CrossEntropyLoss()\n    optimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n    # Train AI model\n    for _ in range(15):\n        ai_model.train()\n        for batch in ai_tr_loader:\n            x = batch[\"x\"].to(device)\n            yb = batch[\"y\"].to(device)\n            out = ai_model(x)\n            loss = criterion_ai(out, yb)\n            optimizer_ai.zero_grad()\n            loss.backward()\n            optimizer_ai.step()\n    # Generate AI probabilities\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n    p_train = probs_all[: len(X_train)]\n    p_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\n    p_test = probs_all[-len(X_test) :]\n\n    # Generate hard one-hot encodings\n    def to_onehot(p):\n        oh = np.zeros_like(p)\n        idx_max = p.argmax(axis=1)\n        oh[np.arange(len(idx_max)), idx_max] = 1\n        return oh\n\n    p_train_hard = to_onehot(p_train)\n    p_val_hard = to_onehot(p_val)\n    p_test_hard = to_onehot(p_test)\n    # Prepare user features for both ablations\n    X_usr = {\n        \"soft\": (\n            np.hstack([X_train, p_train]),\n            np.hstack([X_val, p_val]),\n            np.hstack([X_test, p_test]),\n        ),\n        \"hard\": (\n            np.hstack([X_train, p_train_hard]),\n            np.hstack([X_val, p_val_hard]),\n            np.hstack([X_test, p_test_hard]),\n        ),\n    }\n    for usr_bs in usr_batch_sizes:\n        for ablation in [\"soft\", \"hard\"]:\n            X_tr_ab, X_val_ab, X_test_ab = X_usr[ablation]\n            # User data loaders with ground truth labels\n            usr_tr_loader = DataLoader(\n                UserDS(X_tr_ab, y_train), batch_size=usr_bs, shuffle=True\n            )\n            usr_val_loader = DataLoader(UserDS(X_val_ab, y_val), batch_size=usr_bs)\n            usr_test_loader = DataLoader(UserDS(X_test_ab, y_test), batch_size=usr_bs)\n            # Init User model\n            user_model = UserModel(D + 2, 8, 2).to(device)\n            criterion_usr = nn.CrossEntropyLoss()\n            optimizer_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n            train_accs, val_accs = [], []\n            train_losses, val_losses = [], []\n            # Train User model\n            for _ in range(20):\n                user_model.train()\n                t_loss = 0.0\n                t_corr = 0\n                t_tot = 0\n                for batch in usr_tr_loader:\n                    feat = batch[\"feat\"].to(device)\n                    lbl = batch[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = criterion_usr(out, lbl)\n                    optimizer_usr.zero_grad()\n                    loss.backward()\n                    optimizer_usr.step()\n                    t_loss += loss.item() * feat.size(0)\n                    preds = out.argmax(dim=1)\n                    t_corr += (preds == lbl).sum().item()\n                    t_tot += lbl.size(0)\n                train_losses.append(t_loss / t_tot)\n                train_accs.append(t_corr / t_tot)\n                # Validation\n                user_model.eval()\n                v_loss = 0.0\n                v_corr = 0\n                v_tot = 0\n                with torch.no_grad():\n                    for batch in usr_val_loader:\n                        feat = batch[\"feat\"].to(device)\n                        lbl = batch[\"label\"].to(device)\n                        out = user_model(feat)\n                        loss = criterion_usr(out, lbl)\n                        v_loss += loss.item() * feat.size(0)\n                        preds = out.argmax(dim=1)\n                        v_corr += (preds == lbl).sum().item()\n                        v_tot += lbl.size(0)\n                val_losses.append(v_loss / v_tot)\n                val_accs.append(v_corr / v_tot)\n            # Test evaluation\n            test_preds, test_gt = [], []\n            user_model.eval()\n            with torch.no_grad():\n                for batch in usr_test_loader:\n                    feat = batch[\"feat\"].to(device)\n                    lbl = batch[\"label\"].to(device)\n                    out = user_model(feat)\n                    p = out.argmax(dim=1).cpu().numpy()\n                    test_preds.extend(p.tolist())\n                    test_gt.extend(lbl.cpu().numpy().tolist())\n            key = f\"ai_bs_{ai_bs}_usr_bs_{usr_bs}\"\n            experiment_data[ablation][key] = {\n                \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n                \"losses\": {\n                    \"train\": np.array(train_losses),\n                    \"val\": np.array(val_losses),\n                },\n                \"predictions\": np.array(test_preds),\n                \"ground_truth\": np.array(test_gt),\n            }\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, numpy as np, torch\nimport torch.nn as nn, torch.optim as optim, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device & seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true, b_true = np.array([2.0, -3.0]), 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Split\nidx = np.random.permutation(N)\ntr, va, te = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[tr], y[tr]\nX_val, y_val = X[va], y[va]\nX_test, y_test = X[te], y[te]\n\n# Normalize\nm, s = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - m) / s\nX_val = (X_val - m) / s\nX_test = (X_test - m) / s\n\n\n# Datasets\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X, self.y = torch.from_numpy(X).float(), torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\nclass SoftLabelDS(Dataset):\n    def __init__(self, feat, soft, hard):\n        self.X = torch.from_numpy(feat).float()\n        self.soft = torch.from_numpy(soft).float()\n        self.hard = torch.from_numpy(hard).long()\n\n    def __len__(self):\n        return len(self.hard)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"soft\": self.soft[i], \"hard\": self.hard[i]}\n\n\n# Models\nclass AIModel(nn.Module):\n    def __init__(self, i, h, o):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(i, h), nn.ReLU(), nn.Linear(h, o))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, i, h, o):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(i, h), nn.ReLU(), nn.Linear(h, o))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparams\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\nexperiment_data = {\"CE_hard_labels\": {}, \"soft_label_distillation\": {}}\n\nfor ai_bs in ai_batch_sizes:\n    # AI train\n    ai_tr = DataLoader(SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True)\n    ai_val = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n    ai_model = AIModel(D, 16, 2).to(device)\n    opt_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n    ce = nn.CrossEntropyLoss()\n    for _ in range(15):\n        ai_model.train()\n        for b in ai_tr:\n            x, yb = b[\"x\"].to(device), b[\"y\"].to(device)\n            l = ce(ai_model(x), yb)\n            opt_ai.zero_grad()\n            l.backward()\n            opt_ai.step()\n    # teacher probs\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n    p_tr, p_va, p_te = (\n        probs_all[: len(X_train)],\n        probs_all[len(X_train) : len(X_train) + len(X_val)],\n        probs_all[-len(X_test) :],\n    )\n    f_tr, f_va, f_te = p_tr.argmax(1), p_va.argmax(1), p_te.argmax(1)\n    X_usr_train = np.hstack([X_train, p_tr])\n    X_usr_val = np.hstack([X_val, p_va])\n    X_usr_test = np.hstack([X_test, p_te])\n    # KL loss\n    kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n\n    for usr_bs in usr_batch_sizes:\n        key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n        # Baseline CE on hard\n        tr_loader = DataLoader(\n            UserDS(X_usr_train, f_tr), batch_size=usr_bs, shuffle=True\n        )\n        va_loader = DataLoader(UserDS(X_usr_val, f_va), batch_size=usr_bs)\n        te_loader = DataLoader(UserDS(X_usr_test, f_te), batch_size=usr_bs)\n        um = UserModel(D + 2, 8, 2).to(device)\n        opt_u = optim.Adam(um.parameters(), lr=1e-2)\n        ce2 = nn.CrossEntropyLoss()\n        tr_acc, va_acc, tr_ls, va_ls = [], [], [], []\n        for _ in range(20):\n            um.train()\n            t_l, c, n = 0, 0, 0\n            for b in tr_loader:\n                x, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                out = um(x)\n                loss = ce2(out, lbl)\n                opt_u.zero_grad()\n                loss.backward()\n                opt_u.step()\n                t_l += loss.item() * x.size(0)\n                preds = out.argmax(1)\n                c += (preds == lbl).sum().item()\n                n += lbl.size(0)\n            tr_ls.append(t_l / n)\n            tr_acc.append(c / n)\n            um.eval()\n            v_l, c2, n2 = 0, 0, 0\n            with torch.no_grad():\n                for b in va_loader:\n                    x, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                    out = um(x)\n                    loss = ce2(out, lbl)\n                    v_l += loss.item() * x.size(0)\n                    preds = out.argmax(1)\n                    c2 += (preds == lbl).sum().item()\n                    n2 += lbl.size(0)\n            va_ls.append(v_l / n2)\n            va_acc.append(c2 / n2)\n        # test\n        tp, tg = [], []\n        um.eval()\n        with torch.no_grad():\n            for b in te_loader:\n                x, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                preds = um(x).argmax(1).cpu().numpy()\n                tp.extend(preds.tolist())\n                tg.extend(lbl.cpu().numpy().tolist())\n        experiment_data[\"CE_hard_labels\"][key] = {\n            \"metrics\": {\"train\": np.array(tr_acc), \"val\": np.array(va_acc)},\n            \"losses\": {\"train\": np.array(tr_ls), \"val\": np.array(va_ls)},\n            \"predictions\": np.array(tp),\n            \"ground_truth\": np.array(tg),\n        }\n\n        # Soft\u2010label distillation\n        tr_loader_s = DataLoader(\n            SoftLabelDS(X_usr_train, p_tr, f_tr), batch_size=usr_bs, shuffle=True\n        )\n        va_loader_s = DataLoader(SoftLabelDS(X_usr_val, p_va, f_va), batch_size=usr_bs)\n        te_loader_s = DataLoader(SoftLabelDS(X_usr_test, p_te, f_te), batch_size=usr_bs)\n        um2 = UserModel(D + 2, 8, 2).to(device)\n        opt_u2 = optim.Adam(um2.parameters(), lr=1e-2)\n        tr_acc_s, va_acc_s, tr_ls_s, va_ls_s = [], [], [], []\n        for _ in range(20):\n            um2.train()\n            t_l, c, n = 0, 0, 0\n            for b in tr_loader_s:\n                x, soft, hard = (\n                    b[\"feat\"].to(device),\n                    b[\"soft\"].to(device),\n                    b[\"hard\"].to(device),\n                )\n                out = um2(x)\n                logp = F.log_softmax(out, dim=1)\n                loss = kl_loss(logp, soft)\n                opt_u2.zero_grad()\n                loss.backward()\n                opt_u2.step()\n                t_l += loss.item() * x.size(0)\n                preds = out.argmax(1)\n                c += (preds == hard).sum().item()\n                n += hard.size(0)\n            tr_ls_s.append(t_l / n)\n            tr_acc_s.append(c / n)\n            um2.eval()\n            v_l, c2, n2 = 0, 0, 0\n            with torch.no_grad():\n                for b in va_loader_s:\n                    x, soft, hard = (\n                        b[\"feat\"].to(device),\n                        b[\"soft\"].to(device),\n                        b[\"hard\"].to(device),\n                    )\n                    out = um2(x)\n                    logp = F.log_softmax(out, dim=1)\n                    loss = kl_loss(logp, soft)\n                    v_l += loss.item() * x.size(0)\n                    preds = out.argmax(1)\n                    c2 += (preds == hard).sum().item()\n                    n2 += hard.size(0)\n            va_ls_s.append(v_l / n2)\n            va_acc_s.append(c2 / n2)\n        # test distill\n        tp2, tg2 = [], []\n        um2.eval()\n        with torch.no_grad():\n            for b in te_loader_s:\n                x, _, hard = b[\"feat\"].to(device), b[\"soft\"], b[\"hard\"].to(device)\n                preds = um2(x).argmax(1).cpu().numpy()\n                tp2.extend(preds.tolist())\n                tg2.extend(hard.cpu().numpy().tolist())\n        experiment_data[\"soft_label_distillation\"][key] = {\n            \"metrics\": {\"train\": np.array(tr_acc_s), \"val\": np.array(va_acc_s)},\n            \"losses\": {\"train\": np.array(tr_ls_s), \"val\": np.array(va_ls_s)},\n            \"predictions\": np.array(tp2),\n            \"ground_truth\": np.array(tg2),\n        }\n\n# Save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# set up working dir and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# generate synthetic data\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# model definitions\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# train AI model once\nai_bs = 32\nai_loader = DataLoader(SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True)\nai_model = AIModel(D, 16, 2).to(device)\nopt_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\ncrit_ai = nn.CrossEntropyLoss()\nfor _ in range(15):\n    ai_model.train()\n    for b in ai_loader:\n        x, lbl = b[\"x\"].to(device), b[\"y\"].to(device)\n        out = ai_model(x)\n        loss = crit_ai(out, lbl)\n        opt_ai.zero_grad()\n        loss.backward()\n        opt_ai.step()\n\n# get raw logits\nai_model.eval()\nwith torch.no_grad():\n    X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n    logits_all = ai_model(X_all)\n\n# ablation: temperature scaling\ntemperatures = [0.5, 1.0, 2.0, 5.0]\nusr_bs = 32\nexperiment_data = {\"temperature_scaling\": {}}\n\nfor T in temperatures:\n    # scale logits and compute probs\n    scaled = logits_all / T\n    probs_all = torch.softmax(scaled, dim=1).cpu().numpy()\n    p_train = probs_all[: len(X_train)]\n    p_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\n    p_test = probs_all[-len(X_test) :]\n\n    # pseudo-labels from argmax\n    f_train, f_val, f_test = p_train.argmax(1), p_val.argmax(1), p_test.argmax(1)\n\n    # construct user features\n    X_utrain = np.hstack([X_train, p_train])\n    X_uval = np.hstack([X_val, p_val])\n    X_utest = np.hstack([X_test, p_test])\n\n    # data loaders for user\n    tr_loader = DataLoader(UserDS(X_utrain, f_train), batch_size=usr_bs, shuffle=True)\n    va_loader = DataLoader(UserDS(X_uval, f_val), batch_size=usr_bs)\n    te_loader = DataLoader(UserDS(X_utest, f_test), batch_size=usr_bs)\n\n    # train user model\n    user_model = UserModel(D + 2, 8, 2).to(device)\n    opt_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n    crit_usr = nn.CrossEntropyLoss()\n\n    train_accs, val_accs = [], []\n    train_losses, val_losses = [], []\n\n    for _ in range(20):\n        user_model.train()\n        tloss = 0\n        tcor = 0\n        ttot = 0\n        for b in tr_loader:\n            x, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n            out = user_model(x)\n            loss = crit_usr(out, lbl)\n            opt_usr.zero_grad()\n            loss.backward()\n            opt_usr.step()\n            tloss += loss.item() * x.size(0)\n            tcor += (out.argmax(1) == lbl).sum().item()\n            ttot += lbl.size(0)\n        train_losses.append(tloss / ttot)\n        train_accs.append(tcor / ttot)\n\n        user_model.eval()\n        vloss = 0\n        vcor = 0\n        vtot = 0\n        with torch.no_grad():\n            for b in va_loader:\n                x, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                out = user_model(x)\n                loss = crit_usr(out, lbl)\n                vloss += loss.item() * x.size(0)\n                vcor += (out.argmax(1) == lbl).sum().item()\n                vtot += lbl.size(0)\n        val_losses.append(vloss / vtot)\n        val_accs.append(vcor / vtot)\n\n    # test evaluation\n    test_preds, test_gt = [], []\n    user_model.eval()\n    with torch.no_grad():\n        for b in te_loader:\n            x, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n            preds = user_model(x).argmax(1).cpu().numpy()\n            test_preds.extend(preds.tolist())\n            test_gt.extend(lbl.cpu().numpy().tolist())\n\n    # record results\n    key = f\"T_{T}\"\n    experiment_data[\"temperature_scaling\"][key] = {\n        \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n        \"losses\": {\"train\": np.array(train_losses), \"val\": np.array(val_losses)},\n        \"predictions\": np.array(test_preds),\n        \"ground_truth\": np.array(test_gt),\n    }\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device & reproducibility\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic dataset\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Models\nclass AIModel(nn.Module):\n    def __init__(self, inp, hid, out):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(inp, hid), nn.ReLU(), nn.Linear(hid, out))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp, hid, out):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(inp, hid), nn.ReLU(), nn.Linear(hid, out))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameters\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\n\n# Container\nexperiment_data = {\n    \"teacher_feature_removal\": {\"with_teacher_probs\": {}, \"raw_features_only\": {}}\n}\n\nfor ai_bs in ai_batch_sizes:\n    # AI data loaders\n    ai_tr = DataLoader(SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True)\n    ai_val = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n\n    # Train AI\n    ai_model = AIModel(D, 16, 2).to(device)\n    crit_ai = nn.CrossEntropyLoss()\n    opt_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n    for _ in range(15):\n        ai_model.train()\n        for b in ai_tr:\n            x, yb = b[\"x\"].to(device), b[\"y\"].to(device)\n            out = ai_model(x)\n            loss = crit_ai(out, yb)\n            opt_ai.zero_grad()\n            loss.backward()\n            opt_ai.step()\n\n    # AI outputs\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, 1).cpu().numpy()\n    p_train = probs_all[: len(X_train)]\n    p_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\n    p_test = probs_all[-len(X_test) :]\n    f_train = p_train.argmax(1)\n    f_val = p_val.argmax(1)\n    f_test = p_test.argmax(1)\n\n    # Prepare features for both regimes\n    feats = {\n        \"with_teacher_probs\": (\n            np.hstack([X_train, p_train]),\n            np.hstack([X_val, p_val]),\n            np.hstack([X_test, p_test]),\n        ),\n        \"raw_features_only\": (X_train, X_val, X_test),\n    }\n\n    for scenario, (X_ut, X_uv, X_ute) in feats.items():\n        for usr_bs in usr_batch_sizes:\n            usr_tr = DataLoader(UserDS(X_ut, f_train), batch_size=usr_bs, shuffle=True)\n            usr_val = DataLoader(UserDS(X_uv, f_val), batch_size=usr_bs)\n            usr_te = DataLoader(UserDS(X_ute, f_test), batch_size=usr_bs)\n\n            # User model\n            inp_dim = X_ut.shape[1]\n            user_model = UserModel(inp_dim, 8, 2).to(device)\n            crit_usr = nn.CrossEntropyLoss()\n            opt_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n\n            train_accs, val_accs = [], []\n            train_losses, val_losses = [], []\n\n            # Train user\n            for _ in range(20):\n                user_model.train()\n                t_loss = 0\n                corr = 0\n                tot = 0\n                for b in usr_tr:\n                    feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = crit_usr(out, lbl)\n                    opt_usr.zero_grad()\n                    loss.backward()\n                    opt_usr.step()\n                    t_loss += loss.item() * feat.size(0)\n                    preds = out.argmax(1)\n                    corr += (preds == lbl).sum().item()\n                    tot += lbl.size(0)\n                train_losses.append(t_loss / tot)\n                train_accs.append(corr / tot)\n\n                # Val\n                user_model.eval()\n                v_loss = 0\n                v_corr = 0\n                v_tot = 0\n                with torch.no_grad():\n                    for b in usr_val:\n                        feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                        out = user_model(feat)\n                        loss = crit_usr(out, lbl)\n                        v_loss += loss.item() * feat.size(0)\n                        preds = out.argmax(1)\n                        v_corr += (preds == lbl).sum().item()\n                        v_tot += lbl.size(0)\n                val_losses.append(v_loss / v_tot)\n                val_accs.append(v_corr / v_tot)\n\n            # Test\n            test_preds, test_gt = [], []\n            user_model.eval()\n            with torch.no_grad():\n                for b in usr_te:\n                    feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                    out = user_model(feat)\n                    p = out.argmax(1).cpu().numpy()\n                    test_preds.extend(p.tolist())\n                    test_gt.extend(lbl.cpu().numpy().tolist())\n\n            key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n            experiment_data[\"teacher_feature_removal\"][scenario][key] = {\n                \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n                \"losses\": {\n                    \"train\": np.array(train_losses),\n                    \"val\": np.array(val_losses),\n                },\n                \"predictions\": np.array(test_preds),\n                \"ground_truth\": np.array(test_gt),\n            }\n\n# Save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# synthetic data\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true, b_true = np.array([2.0, -3.0]), 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# split\nidx = np.random.permutation(N)\ntr_idx, va_idx, te_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[tr_idx], y[tr_idx]\nX_val, y_val = X[va_idx], y[va_idx]\nX_test, y_test = X[te_idx], y[te_idx]\n\n# normalize\nm, s = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - m) / s\nX_val = (X_val - m) / s\nX_test = (X_test - m) / s\n\n\n# datasets\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.x = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.x[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, X, y):\n        self.feat = torch.from_numpy(X).float()\n        self.label = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.label)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.feat[i], \"label\": self.label[i]}\n\n\nclass SoftLabelDS(Dataset):\n    def __init__(self, X, soft, hard):\n        self.feat = torch.from_numpy(X).float()\n        self.soft = torch.from_numpy(soft).float()\n        self.hard = torch.from_numpy(hard).long()\n\n    def __len__(self):\n        return len(self.hard)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.feat[i], \"soft\": self.soft[i], \"hard\": self.hard[i]}\n\n\n# models\nclass AIModel(nn.Module):\n    def __init__(self, i, h, o):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(i, h), nn.ReLU(), nn.Linear(h, o))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, i, h, o):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(i, h), nn.ReLU(), nn.Linear(h, o))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Jensen-Shannon divergence\ndef js_div(p, q):\n    m = 0.5 * (p + q)\n    # Kullback-Leibler parts\n    kl1 = np.sum(p * (np.log(p + 1e-12) - np.log(m + 1e-12)), axis=1)\n    kl2 = np.sum(q * (np.log(q + 1e-12) - np.log(m + 1e-12)), axis=1)\n    return 0.5 * (kl1 + kl2)\n\n\n# experiment storage\nexperiment_data = {\n    \"CE_hard_labels\": {},\n    \"soft_label_distillation\": {},\n    \"bias_awareness\": {},\n    \"dual_channel\": {},\n}\n\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\nfor ai_bs in ai_batch_sizes:\n    # train AI\n    ai_tr = DataLoader(SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True)\n    ai_val = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n    ai_model = AIModel(D, 16, 2).to(device)\n    opt_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n    ce = nn.CrossEntropyLoss()\n    for ep in range(15):\n        ai_model.train()\n        for batch in ai_tr:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = ai_model(batch[\"x\"])\n            loss = ce(out, batch[\"y\"])\n            opt_ai.zero_grad()\n            loss.backward()\n            opt_ai.step()\n\n    # get teacher probs\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n    p_tr = probs_all[: len(X_train)]\n    p_va = probs_all[len(X_train) : len(X_train) + len(X_val)]\n    p_te = probs_all[-len(X_test) :]\n    f_tr, f_va, f_te = p_tr.argmax(1), p_va.argmax(1), p_te.argmax(1)\n\n    # bias-awareness signal = 1 - normalized entropy\n    def bias_signal(probs):\n        ent = -np.sum(probs * np.log(probs + 1e-12), axis=1)\n        return 1 - ent / np.log(probs.shape[1])\n\n    b_tr, b_va, b_te = bias_signal(p_tr), bias_signal(p_va), bias_signal(p_te)\n\n    # build feature matrices\n    X_base = (X_train, X_val, X_test)\n    X_cont = (\n        np.hstack([X_train, p_tr]),\n        np.hstack([X_val, p_va]),\n        np.hstack([X_test, p_te]),\n    )\n    X_bias = (\n        np.hstack([X_train, b_tr[:, None]]),\n        np.hstack([X_val, b_va[:, None]]),\n        np.hstack([X_test, b_te[:, None]]),\n    )\n    X_dual = (\n        np.hstack([X_train, p_tr, b_tr[:, None]]),\n        np.hstack([X_val, p_va, b_va[:, None]]),\n        np.hstack([X_test, p_te, b_te[:, None]]),\n    )\n\n    for usr_bs in usr_batch_sizes:\n        key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n        scenarios = {\n            \"CE_hard_labels\": {\n                \"feat\": X_base,\n                \"hard\": (f_tr, f_va, f_te),\n                \"use_soft\": False,\n                \"input_dim\": D,\n            },\n            \"soft_label_distillation\": {\n                \"feat\": X_cont,\n                \"soft\": (p_tr, p_va, p_te),\n                \"hard\": (f_tr, f_va, f_te),\n                \"use_soft\": True,\n                \"input_dim\": D + 2,\n            },\n            \"bias_awareness\": {\n                \"feat\": X_bias,\n                \"hard\": (f_tr, f_va, f_te),\n                \"use_soft\": False,\n                \"input_dim\": D + 1,\n            },\n            \"dual_channel\": {\n                \"feat\": X_dual,\n                \"soft\": (p_tr, p_va, p_te),\n                \"hard\": (f_tr, f_va, f_te),\n                \"use_soft\": True,\n                \"input_dim\": D + 3,\n            },\n        }\n\n        for name, sc in scenarios.items():\n            # dataset setup\n            if sc[\"use_soft\"]:\n                tr_ds = SoftLabelDS(sc[\"feat\"][0], sc[\"soft\"][0], sc[\"hard\"][0])\n                va_ds = SoftLabelDS(sc[\"feat\"][1], sc[\"soft\"][1], sc[\"hard\"][1])\n                te_ds = SoftLabelDS(sc[\"feat\"][2], sc[\"soft\"][2], sc[\"hard\"][2])\n            else:\n                tr_ds = UserDS(sc[\"feat\"][0], sc[\"hard\"][0])\n                va_ds = UserDS(sc[\"feat\"][1], sc[\"hard\"][1])\n                te_ds = UserDS(sc[\"feat\"][2], sc[\"hard\"][2])\n\n            tr_ld = DataLoader(tr_ds, batch_size=usr_bs, shuffle=True)\n            va_ld = DataLoader(va_ds, batch_size=usr_bs, shuffle=False)\n\n            # model, optimizer, loss\n            um = UserModel(sc[\"input_dim\"], 8, 2).to(device)\n            opt_u = optim.Adam(um.parameters(), lr=1e-2)\n            if sc[\"use_soft\"]:\n                loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n            else:\n                loss_fn = nn.CrossEntropyLoss()\n\n            # storage\n            tr_acc, va_acc = [], []\n            tr_ls, va_ls = [], []\n            align_rates = []\n            align_hist = []\n\n            # train epochs\n            for epoch in range(1, 21):\n                um.train()\n                t_loss = corr = n = 0\n                for batch in tr_ld:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    out = um(batch[\"feat\"])\n                    if sc[\"use_soft\"]:\n                        logp = F.log_softmax(out, dim=1)\n                        loss = loss_fn(logp, batch[\"soft\"])\n                        true = batch[\"hard\"]\n                    else:\n                        loss = loss_fn(out, batch[\"label\"])\n                        true = batch[\"label\"]\n                    opt_u.zero_grad()\n                    loss.backward()\n                    opt_u.step()\n                    t_loss += loss.item() * batch[\"feat\"].size(0)\n                    preds = out.argmax(1)\n                    corr += (preds == true).sum().item()\n                    n += batch[\"feat\"].size(0)\n                tr_ls.append(t_loss / n)\n                tr_acc.append(corr / n)\n\n                # validation (direct full\u2010batch for metrics & alignment)\n                um.eval()\n                with torch.no_grad():\n                    x_va = torch.from_numpy(sc[\"feat\"][1]).float().to(device)\n                    out_va = um(x_va)\n                    if sc[\"use_soft\"]:\n                        logp_va = F.log_softmax(out_va, dim=1)\n                        v_loss = loss_fn(\n                            logp_va, torch.from_numpy(sc[\"soft\"][1]).float().to(device)\n                        ).item()\n                        true_va = torch.from_numpy(sc[\"hard\"][1]).long().to(device)\n                    else:\n                        v_loss = loss_fn(\n                            out_va, torch.from_numpy(sc[\"hard\"][1]).long().to(device)\n                        ).item()\n                        true_va = torch.from_numpy(sc[\"hard\"][1]).long().to(device)\n                    preds_va = out_va.argmax(1)\n                    va_ls.append(v_loss)\n                    va_acc.append((preds_va == true_va).float().mean().item())\n\n                    # alignment\n                    user_p = F.softmax(out_va, dim=1).cpu().numpy()\n                    model_p = p_va\n                    jsd = js_div(user_p, model_p) / np.log(2)\n                    align = 1 - jsd\n                    mean_align = align.mean()\n                    align_hist.append(mean_align)\n                    rate = np.polyfit(np.arange(len(align_hist)), align_hist, 1)[0]\n                    align_rates.append(rate)\n\n                print(f\"{name} {key} Epoch {epoch}: validation_loss = {v_loss:.4f}\")\n\n            # test predictions\n            um.eval()\n            x_te = torch.from_numpy(sc[\"feat\"][2]).float().to(device)\n            with torch.no_grad():\n                preds_te = um(x_te).argmax(1).cpu().numpy()\n            experiment_data[name][key] = {\n                \"metrics\": {\n                    \"train\": np.array(tr_acc),\n                    \"val\": np.array(va_acc),\n                    \"alignment_rate\": np.array(align_rates),\n                },\n                \"losses\": {\"train\": np.array(tr_ls), \"val\": np.array(va_ls)},\n                \"predictions\": np.array(preds_te),\n                \"ground_truth\": np.array(sc[\"hard\"][2]),\n            }\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# synthetic data\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true, b_true = np.array([2.0, -3.0]), 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# split\nidx = np.random.permutation(N)\ntr_idx, va_idx, te_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[tr_idx], y[tr_idx]\nX_val, y_val = X[va_idx], y[va_idx]\nX_test, y_test = X[te_idx], y[te_idx]\n\n# normalize\nm, s = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - m) / s\nX_val = (X_val - m) / s\nX_test = (X_test - m) / s\n\n\n# datasets\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.x = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.x[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, X, y):\n        self.feat = torch.from_numpy(X).float()\n        self.label = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.label)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.feat[i], \"label\": self.label[i]}\n\n\nclass SoftLabelDS(Dataset):\n    def __init__(self, X, soft, hard):\n        self.feat = torch.from_numpy(X).float()\n        self.soft = torch.from_numpy(soft).float()\n        self.hard = torch.from_numpy(hard).long()\n\n    def __len__(self):\n        return len(self.hard)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.feat[i], \"soft\": self.soft[i], \"hard\": self.hard[i]}\n\n\n# models\nclass AIModel(nn.Module):\n    def __init__(self, i, h, o):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(i, h), nn.ReLU(), nn.Linear(h, o))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, i, h, o):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(i, h), nn.ReLU(), nn.Linear(h, o))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Jensen-Shannon divergence\ndef js_div(p, q):\n    m = 0.5 * (p + q)\n    kl1 = np.sum(p * (np.log(p + 1e-12) - np.log(m + 1e-12)), axis=1)\n    kl2 = np.sum(q * (np.log(q + 1e-12) - np.log(m + 1e-12)), axis=1)\n    return 0.5 * (kl1 + kl2)\n\n\n# experiment storage\nexperiment_data = {\n    \"CE_hard_labels\": {},\n    \"soft_label_distillation\": {},\n    \"bias_awareness\": {},\n    \"dual_channel\": {},\n}\n\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\n\nfor ai_bs in ai_batch_sizes:\n    # train AI\n    ai_tr = DataLoader(SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True)\n    ai_val = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n    ai_model = AIModel(D, 16, 2).to(device)\n    opt_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n    ce = nn.CrossEntropyLoss()\n    for ep in range(15):\n        ai_model.train()\n        for batch in ai_tr:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = ai_model(batch[\"x\"])\n            loss = ce(out, batch[\"y\"])\n            opt_ai.zero_grad()\n            loss.backward()\n            opt_ai.step()\n    # teacher probs\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n    p_tr = probs_all[: len(X_train)]\n    p_va = probs_all[len(X_train) : len(X_train) + len(X_val)]\n    p_te = probs_all[-len(X_test) :]\n    f_tr, f_va, f_te = p_tr.argmax(1), p_va.argmax(1), p_te.argmax(1)\n\n    # bias-awareness signal\n    def bias_signal(probs):\n        ent = -np.sum(probs * np.log(probs + 1e-12), axis=1)\n        return 1 - ent / np.log(probs.shape[1])\n\n    b_tr, b_va, b_te = bias_signal(p_tr), bias_signal(p_va), bias_signal(p_te)\n\n    # feature matrices\n    X_base = (X_train, X_val, X_test)\n    X_cont = (\n        np.hstack([X_train, p_tr]),\n        np.hstack([X_val, p_va]),\n        np.hstack([X_test, p_te]),\n    )\n    X_bias = (\n        np.hstack([X_train, b_tr[:, None]]),\n        np.hstack([X_val, b_va[:, None]]),\n        np.hstack([X_test, b_te[:, None]]),\n    )\n    X_dual = (\n        np.hstack([X_train, p_tr, b_tr[:, None]]),\n        np.hstack([X_val, p_va, b_va[:, None]]),\n        np.hstack([X_test, p_te, b_te[:, None]]),\n    )\n\n    for usr_bs in usr_batch_sizes:\n        key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n        scenarios = {\n            \"CE_hard_labels\": {\n                \"feat\": X_base,\n                \"hard\": (f_tr, f_va, f_te),\n                \"use_soft\": False,\n                \"input_dim\": D,\n            },\n            \"soft_label_distillation\": {\n                \"feat\": X_cont,\n                \"soft\": (p_tr, p_va, p_te),\n                \"hard\": (f_tr, f_va, f_te),\n                \"use_soft\": True,\n                \"input_dim\": D + 2,\n            },\n            \"bias_awareness\": {\n                \"feat\": X_bias,\n                \"hard\": (f_tr, f_va, f_te),\n                \"use_soft\": False,\n                \"input_dim\": D + 1,\n            },\n            \"dual_channel\": {\n                \"feat\": X_dual,\n                \"soft\": (p_tr, p_va, p_te),\n                \"hard\": (f_tr, f_va, f_te),\n                \"use_soft\": True,\n                \"input_dim\": D + 3,\n            },\n        }\n\n        for name, sc in scenarios.items():\n            # dataset setup\n            if sc[\"use_soft\"]:\n                tr_ds = SoftLabelDS(sc[\"feat\"][0], sc[\"soft\"][0], sc[\"hard\"][0])\n                va_ds = SoftLabelDS(sc[\"feat\"][1], sc[\"soft\"][1], sc[\"hard\"][1])\n                te_ds = SoftLabelDS(sc[\"feat\"][2], sc[\"soft\"][2], sc[\"hard\"][2])\n            else:\n                tr_ds = UserDS(sc[\"feat\"][0], sc[\"hard\"][0])\n                va_ds = UserDS(sc[\"feat\"][1], sc[\"hard\"][1])\n                te_ds = UserDS(sc[\"feat\"][2], sc[\"hard\"][2])\n\n            tr_ld = DataLoader(tr_ds, batch_size=usr_bs, shuffle=True)\n            va_ld = DataLoader(va_ds, batch_size=usr_bs, shuffle=False)\n\n            # model setup\n            um = UserModel(sc[\"input_dim\"], 8, 2).to(device)\n            opt_u = optim.Adam(um.parameters(), lr=1e-2)\n            if sc[\"use_soft\"]:\n                loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n            else:\n                loss_fn = nn.CrossEntropyLoss()\n\n            # tracking\n            tr_acc, va_acc = [], []\n            tr_ls, va_ls = [], []\n            align_rates, align_hist = [], []\n\n            # train epochs\n            for epoch in range(1, 21):\n                um.train()\n                t_loss = corr = n = 0\n                for batch in tr_ld:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    out = um(batch[\"feat\"])\n                    if sc[\"use_soft\"]:\n                        logp = F.log_softmax(out, dim=1)\n                        loss = loss_fn(logp, batch[\"soft\"])\n                        true = batch[\"hard\"]\n                    else:\n                        loss = loss_fn(out, batch[\"label\"])\n                        true = batch[\"label\"]\n                    opt_u.zero_grad()\n                    loss.backward()\n                    opt_u.step()\n                    t_loss += loss.item() * batch[\"feat\"].size(0)\n                    preds = out.argmax(1)\n                    corr += (preds == true).sum().item()\n                    n += batch[\"feat\"].size(0)\n                tr_ls.append(t_loss / n)\n                tr_acc.append(corr / n)\n\n                um.eval()\n                with torch.no_grad():\n                    x_va = torch.from_numpy(sc[\"feat\"][1]).float().to(device)\n                    out_va = um(x_va)\n                    if sc[\"use_soft\"]:\n                        logp_va = F.log_softmax(out_va, dim=1)\n                        v_loss = loss_fn(\n                            logp_va, torch.from_numpy(sc[\"soft\"][1]).float().to(device)\n                        ).item()\n                        true_va = torch.from_numpy(sc[\"hard\"][1]).long().to(device)\n                    else:\n                        v_loss = loss_fn(\n                            out_va, torch.from_numpy(sc[\"hard\"][1]).long().to(device)\n                        ).item()\n                        true_va = torch.from_numpy(sc[\"hard\"][1]).long().to(device)\n                    preds_va = out_va.argmax(1)\n                    va_ls.append(v_loss)\n                    va_acc.append((preds_va == true_va).float().mean().item())\n\n                    # alignment\n                    user_p = F.softmax(out_va, dim=1).cpu().numpy()\n                    model_p = p_va\n                    jsd = js_div(user_p, model_p) / np.log(2)\n                    mean_align = (1 - jsd).mean()\n                    align_hist.append(mean_align)\n                    # safe slope computation\n                    if len(align_hist) > 1:\n                        try:\n                            rate = np.polyfit(\n                                np.arange(len(align_hist)), align_hist, 1\n                            )[0]\n                        except np.linalg.LinAlgError:\n                            rate = 0.0\n                    else:\n                        rate = 0.0\n                    align_rates.append(rate)\n\n                print(f\"{name} {key} Epoch {epoch}: validation_loss = {v_loss:.4f}\")\n\n            # test\n            um.eval()\n            x_te = torch.from_numpy(sc[\"feat\"][2]).float().to(device)\n            with torch.no_grad():\n                preds_te = um(x_te).argmax(1).cpu().numpy()\n            experiment_data[name][key] = {\n                \"metrics\": {\n                    \"train\": np.array(tr_acc),\n                    \"val\": np.array(va_acc),\n                    \"alignment_rate\": np.array(align_rates),\n                },\n                \"losses\": {\"train\": np.array(tr_ls), \"val\": np.array(va_ls)},\n                \"predictions\": preds_te,\n                \"ground_truth\": np.array(sc[\"hard\"][2]),\n            }\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n# Dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label, p_ai):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n        self.p_ai = torch.from_numpy(p_ai).float()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i], \"p_ai\": self.p_ai[i]}\n\n\n# Model definitions\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim),\n            nn.ReLU(),\n            nn.Linear(hid_dim, out_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim),\n            nn.ReLU(),\n            nn.Linear(hid_dim, out_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameters\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\ndataset_configs = [\n    {\"name\": \"D2_noise0.1_scale1.0\", \"D\": 2, \"noise\": 0.1, \"scale\": 1.0},\n    {\"name\": \"D5_noise0.5_scale2.0\", \"D\": 5, \"noise\": 0.5, \"scale\": 2.0},\n    {\"name\": \"D10_noise1.0_scale0.5\", \"D\": 10, \"noise\": 1.0, \"scale\": 0.5},\n]\n\n# Container for experiment data\nexperiment_data = {\"synthetic_diversity\": {}}\n\nfor cfg in dataset_configs:\n    name, D, noise, scale = cfg[\"name\"], cfg[\"D\"], cfg[\"noise\"], cfg[\"scale\"]\n    # Generate synthetic data\n    N = 2000\n    X = np.random.randn(N, D)\n    w_true = scale * np.random.randn(D)\n    b_true = 0.5\n    logits = X.dot(w_true) + b_true + noise * np.random.randn(N)\n    probs = 1 / (1 + np.exp(-logits))\n    y = (np.random.rand(N) < probs).astype(int)\n    # Split\n    idx = np.random.permutation(N)\n    tr_idx, vl_idx, ts_idx = idx[:1200], idx[1200:1500], idx[1500:]\n    X_tr, y_tr = X[tr_idx], y[tr_idx]\n    X_vl, y_vl = X[vl_idx], y[vl_idx]\n    X_ts, y_ts = X[ts_idx], y[ts_idx]\n    # Normalize\n    mean, std = X_tr.mean(0), X_tr.std(0) + 1e-6\n    X_tr = (X_tr - mean) / std\n    X_vl = (X_vl - mean) / std\n    X_ts = (X_ts - mean) / std\n\n    experiment_data[\"synthetic_diversity\"][name] = {\"batch_size\": {}}\n\n    # Train AI model for each batch size\n    for ai_bs in ai_batch_sizes:\n        # AI DataLoaders\n        ai_tr_loader = DataLoader(SimpleDS(X_tr, y_tr), batch_size=ai_bs, shuffle=True)\n        ai_vl_loader = DataLoader(SimpleDS(X_vl, y_vl), batch_size=ai_bs)\n        # Build and train AI\n        ai_model = AIModel(D, 16, 2).to(device)\n        opt_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n        crit_ai = nn.CrossEntropyLoss()\n        for epoch in range(15):\n            ai_model.train()\n            for batch in ai_tr_loader:\n                xb = batch[\"x\"].to(device)\n                yb = batch[\"y\"].to(device)\n                out = ai_model(xb)\n                loss = crit_ai(out, yb)\n                opt_ai.zero_grad()\n                loss.backward()\n                opt_ai.step()\n        # Gather AI predictions\n        ai_model.eval()\n        with torch.no_grad():\n            X_all = torch.from_numpy(np.vstack([X_tr, X_vl, X_ts])).float().to(device)\n            logits_all = ai_model(X_all)\n            probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n        p_tr = probs_all[: len(X_tr)]\n        p_vl = probs_all[len(X_tr) : len(X_tr) + len(X_vl)]\n        p_ts = probs_all[-len(X_ts) :]\n        f_tr = p_tr.argmax(1)\n        f_vl = p_vl.argmax(1)\n        f_ts = p_ts.argmax(1)\n        # Prepare user features\n        X_utr = np.hstack([X_tr, p_tr])\n        X_uvl = np.hstack([X_vl, p_vl])\n        X_uts = np.hstack([X_ts, p_ts])\n\n        for usr_bs in usr_batch_sizes:\n            # DataLoaders for user model\n            usr_tr_loader = DataLoader(\n                UserDS(X_utr, f_tr, p_tr), batch_size=usr_bs, shuffle=True\n            )\n            usr_vl_loader = DataLoader(UserDS(X_uvl, f_vl, p_vl), batch_size=usr_bs)\n            usr_ts_loader = DataLoader(UserDS(X_uts, f_ts, p_ts), batch_size=usr_bs)\n\n            user_model = UserModel(D + 2, 8, 2).to(device)\n            opt_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n            crit_usr = nn.CrossEntropyLoss()\n\n            train_accs, val_accs = [], []\n            train_losses, val_losses = [], []\n            alignment_scores = []\n\n            # Train user model with alignment metric\n            epochs = 20\n            for ep in range(epochs):\n                # Training\n                user_model.train()\n                t_loss, t_corr, t_tot = 0.0, 0, 0\n                for batch in usr_tr_loader:\n                    batch = {\n                        k: v.to(device)\n                        for k, v in batch.items()\n                        if isinstance(v, torch.Tensor)\n                    }\n                    out = user_model(batch[\"feat\"])\n                    loss = crit_usr(out, batch[\"label\"])\n                    opt_usr.zero_grad()\n                    loss.backward()\n                    opt_usr.step()\n                    t_loss += loss.item() * batch[\"feat\"].size(0)\n                    preds = out.argmax(1)\n                    t_corr += (preds == batch[\"label\"]).sum().item()\n                    t_tot += batch[\"feat\"].size(0)\n                train_losses.append(t_loss / t_tot)\n                train_accs.append(t_corr / t_tot)\n\n                # Validation + Alignment\n                user_model.eval()\n                v_loss, v_corr, v_tot = 0.0, 0, 0\n                p_list, q_list = [], []\n                with torch.no_grad():\n                    for batch in usr_vl_loader:\n                        batch = {\n                            k: v.to(device)\n                            for k, v in batch.items()\n                            if isinstance(v, torch.Tensor)\n                        }\n                        out = user_model(batch[\"feat\"])\n                        loss = crit_usr(out, batch[\"label\"])\n                        preds = out.argmax(1)\n                        v_loss += loss.item() * batch[\"feat\"].size(0)\n                        v_corr += (preds == batch[\"label\"]).sum().item()\n                        v_tot += batch[\"feat\"].size(0)\n                        q_prob = torch.softmax(out, dim=1).cpu().numpy()\n                        p_list.append(batch[\"p_ai\"].cpu().numpy())\n                        q_list.append(q_prob)\n                val_losses.append(v_loss / v_tot)\n                val_accs.append(v_corr / v_tot)\n\n                # Compute Jensen-Shannon divergence and alignment score\n                p_arr = np.vstack(p_list)\n                q_arr = np.vstack(q_list)\n                m = 0.5 * (p_arr + q_arr)\n                kl1 = np.sum(p_arr * np.log2((p_arr + 1e-12) / (m + 1e-12)), axis=1)\n                kl2 = np.sum(q_arr * np.log2((q_arr + 1e-12) / (m + 1e-12)), axis=1)\n                jsd = 0.5 * (kl1 + kl2)\n                align_score = 1.0 - jsd  # normalized in [0,1]\n                epoch_align = np.mean(align_score)\n                alignment_scores.append(epoch_align)\n\n                print(\n                    f\"Epoch {ep+1}: validation_loss = {val_losses[-1]:.4f}, alignment_score = {epoch_align:.4f}\"\n                )\n\n            # Compute alignment rate (slope)\n            x = np.arange(1, epochs + 1)\n            slope = np.polyfit(x, alignment_scores, 1)[0]\n\n            # Test predictions\n            test_preds, test_gt = [], []\n            user_model.eval()\n            with torch.no_grad():\n                for batch in usr_ts_loader:\n                    batch = {\n                        k: v.to(device)\n                        for k, v in batch.items()\n                        if isinstance(v, torch.Tensor)\n                    }\n                    out = user_model(batch[\"feat\"])\n                    test_preds.extend(out.argmax(1).cpu().numpy().tolist())\n                    test_gt.extend(batch[\"label\"].cpu().numpy().tolist())\n\n            key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n            experiment_data[\"synthetic_diversity\"][name][\"batch_size\"][key] = {\n                \"metrics\": {\n                    \"train_accs\": np.array(train_accs),\n                    \"val_accs\": np.array(val_accs),\n                    \"alignment_scores\": np.array(alignment_scores),\n                },\n                \"losses\": {\n                    \"train\": np.array(train_losses),\n                    \"val\": np.array(val_losses),\n                },\n                \"alignment_rate\": slope,\n                \"predictions\": np.array(test_preds),\n                \"ground_truth\": np.array(test_gt),\n            }\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Datasets\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Models\nclass AIModel(nn.Module):\n    def __init__(self, inp, hid, out):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(inp, hid), nn.ReLU(), nn.Linear(hid, out))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp, hid, out):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(inp, hid), nn.ReLU(), nn.Linear(hid, out))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Train teacher\nai_bs = 32\nai_tr_loader = DataLoader(SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True)\nai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs, shuffle=False)\nai_model = AIModel(D, 16, 2).to(device)\ncrit_ai = nn.CrossEntropyLoss()\nopt_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\nfor _ in range(15):\n    ai_model.train()\n    for batch in ai_tr_loader:\n        x, yb = batch[\"x\"].to(device), batch[\"y\"].to(device)\n        out = ai_model(x)\n        loss = crit_ai(out, yb)\n        opt_ai.zero_grad()\n        loss.backward()\n        opt_ai.step()\n\n# Get teacher probs & pseudo-labels\nai_model.eval()\nwith torch.no_grad():\n    X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n    logits_all = ai_model(X_all)\n    probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\np_train = probs_all[: len(X_train)]\np_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\np_test = probs_all[-len(X_test) :]\nf_train = p_train.argmax(1)\nf_val = p_val.argmax(1)\nf_test = p_test.argmax(1)\n\n# Build user features\nX_usr_train = np.hstack([X_train, p_train])\nX_usr_val = np.hstack([X_val, p_val])\nX_usr_test = np.hstack([X_test, p_test])\n\n# Ablation: confidence thresholds\nthresholds = [0.6, 0.8, 0.9]\nexperiment_data = {\"confidence_filter\": {}}\nusr_bs = 32\n\nfor thr in thresholds:\n    # filter train\n    keep = np.where(np.max(p_train, axis=1) >= thr)[0]\n    X_tr_f = X_usr_train[keep]\n    y_tr_f = f_train[keep]\n    # loaders\n    usr_tr = DataLoader(UserDS(X_tr_f, y_tr_f), batch_size=usr_bs, shuffle=True)\n    usr_val = DataLoader(UserDS(X_usr_val, f_val), batch_size=usr_bs, shuffle=False)\n    usr_test = DataLoader(UserDS(X_usr_test, f_test), batch_size=usr_bs, shuffle=False)\n    # user model\n    user = UserModel(D + 2, 8, 2).to(device)\n    crit_u = nn.CrossEntropyLoss()\n    opt_u = optim.Adam(user.parameters(), lr=1e-2)\n    train_accs, val_accs = [], []\n    train_losses, val_losses = [], []\n    # train\n    for _ in range(20):\n        user.train()\n        t_loss, corr, tot = 0.0, 0, 0\n        for b in usr_tr:\n            x = b[\"feat\"].to(device)\n            yb = b[\"label\"].to(device)\n            out = user(x)\n            loss = crit_u(out, yb)\n            opt_u.zero_grad()\n            loss.backward()\n            opt_u.step()\n            t_loss += loss.item() * len(yb)\n            pred = out.argmax(1)\n            corr += (pred == yb).sum().item()\n            tot += len(yb)\n        train_losses.append(t_loss / tot)\n        train_accs.append(corr / tot)\n        # val\n        user.eval()\n        v_loss, v_corr, v_tot = 0.0, 0, 0\n        with torch.no_grad():\n            for b in usr_val:\n                x = b[\"feat\"].to(device)\n                yb = b[\"label\"].to(device)\n                out = user(x)\n                loss = crit_u(out, yb)\n                v_loss += loss.item() * len(yb)\n                pred = out.argmax(1)\n                v_corr += (pred == yb).sum().item()\n                v_tot += len(yb)\n        val_losses.append(v_loss / v_tot)\n        val_accs.append(v_corr / v_tot)\n    # test\n    user.eval()\n    test_preds, test_gt = [], []\n    with torch.no_grad():\n        for b in usr_test:\n            x = b[\"feat\"].to(device)\n            yb = b[\"label\"].to(device)\n            out = user(x)\n            test_preds.extend(out.argmax(1).cpu().numpy().tolist())\n            test_gt.extend(yb.cpu().numpy().tolist())\n    # save\n    key = f\"threshold_{thr}\"\n    experiment_data[\"confidence_filter\"][key] = {\n        \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n        \"losses\": {\"train\": np.array(train_losses), \"val\": np.array(val_losses)},\n        \"predictions\": np.array(test_preds),\n        \"ground_truth\": np.array(test_gt),\n    }\n\n# dump\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device & seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# full synthetic data\nN, D = 2000, 2\nX_full = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X_full.dot(w_true) + b_true\nprobs_full = 1 / (1 + np.exp(-logits))\ny_full = (np.random.rand(N) < probs_full).astype(int)\n\n\n# dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# model defs\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# hyperparameters\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\n\n# prepare experiment container\nexperiment_data = {\"class_imbalance\": {}}\nratios = [0.5, 0.7, 0.9]\nnames = [\"50_50\", \"70_30\", \"90_10\"]\n\nfor ratio, name in zip(ratios, names):\n    # compute counts for class0 (majority) and class1\n    n0 = int(N * ratio)\n    n1 = N - n0\n    idx0 = np.where(y_full == 0)[0]\n    idx1 = np.where(y_full == 1)[0]\n    # sample with/without replacement\n    sel0 = np.random.choice(idx0, n0, replace=(len(idx0) < n0))\n    sel1 = np.random.choice(idx1, n1, replace=(len(idx1) < n1))\n    idxs = np.concatenate([sel0, sel1])\n    np.random.shuffle(idxs)\n    X, y = X_full[idxs], y_full[idxs]\n    # split\n    tr_i, val_i, te_i = np.arange(0, 1200), np.arange(1200, 1500), np.arange(1500, N)\n    X_train, y_train = X[tr_i], y[tr_i]\n    X_val, y_val = X[val_i], y[val_i]\n    X_test, y_test = X[te_i], y[te_i]\n    # normalize\n    mean, std = X_train.mean(0), X_train.std(0) + 1e-6\n    X_train = (X_train - mean) / std\n    X_val = (X_val - mean) / std\n    X_test = (X_test - mean) / std\n\n    experiment_data[\"class_imbalance\"][name] = {}\n\n    for ai_bs in ai_batch_sizes:\n        # AI loaders\n        ai_tr = DataLoader(SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True)\n        ai_val = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n        # AI model\n        ai_model = AIModel(D, 16, 2).to(device)\n        crit_ai = nn.CrossEntropyLoss()\n        opt_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n        # train AI\n        for _ in range(15):\n            ai_model.train()\n            for b in ai_tr:\n                x = b[\"x\"].to(device)\n                yb = b[\"y\"].to(device)\n                out = ai_model(x)\n                loss = crit_ai(out, yb)\n                opt_ai.zero_grad()\n                loss.backward()\n                opt_ai.step()\n        # get probs & preds\n        ai_model.eval()\n        with torch.no_grad():\n            X_all = (\n                torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n            )\n            logits_all = ai_model(X_all)\n            probs_all = torch.softmax(logits_all, 1).cpu().numpy()\n        p_tr = probs_all[: len(X_train)]\n        p_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\n        p_te = probs_all[-len(X_test) :]\n        f_tr, f_val, f_te = p_tr.argmax(1), p_val.argmax(1), p_te.argmax(1)\n        # user features\n        X_usr_tr = np.hstack([X_train, p_tr])\n        X_usr_val = np.hstack([X_val, p_val])\n        X_usr_te = np.hstack([X_test, p_te])\n\n        for usr_bs in usr_batch_sizes:\n            usr_tr = DataLoader(UserDS(X_usr_tr, f_tr), batch_size=usr_bs, shuffle=True)\n            usr_val = DataLoader(UserDS(X_usr_val, f_val), batch_size=usr_bs)\n            usr_te = DataLoader(UserDS(X_usr_te, f_te), batch_size=usr_bs)\n            user_model = UserModel(D + 2, 8, 2).to(device)\n            crit_u = nn.CrossEntropyLoss()\n            opt_u = optim.Adam(user_model.parameters(), lr=1e-2)\n            train_accs, val_accs = [], []\n            train_losses, val_losses = [], []\n\n            # train user\n            for _ in range(20):\n                user_model.train()\n                t_loss, corr, tot = 0, 0, 0\n                for b in usr_tr:\n                    feat = b[\"feat\"].to(device)\n                    lbl = b[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = crit_u(out, lbl)\n                    opt_u.zero_grad()\n                    loss.backward()\n                    opt_u.step()\n                    t_loss += loss.item() * feat.size(0)\n                    preds = out.argmax(1)\n                    corr += (preds == lbl).sum().item()\n                    tot += lbl.size(0)\n                train_losses.append(t_loss / tot)\n                train_accs.append(corr / tot)\n                user_model.eval()\n                v_loss, v_corr, v_tot = 0, 0, 0\n                with torch.no_grad():\n                    for b in usr_val:\n                        feat = b[\"feat\"].to(device)\n                        lbl = b[\"label\"].to(device)\n                        out = user_model(feat)\n                        loss = crit_u(out, lbl)\n                        v_loss += loss.item() * feat.size(0)\n                        p = out.argmax(1)\n                        v_corr += (p == lbl).sum().item()\n                        v_tot += lbl.size(0)\n                val_losses.append(v_loss / v_tot)\n                val_accs.append(v_corr / v_tot)\n\n            # test user\n            test_preds, test_gt = [], []\n            user_model.eval()\n            with torch.no_grad():\n                for b in usr_te:\n                    feat = b[\"feat\"].to(device)\n                    lbl = b[\"label\"].to(device)\n                    out = user_model(feat)\n                    test_preds.extend(out.argmax(1).cpu().numpy().tolist())\n                    test_gt.extend(lbl.cpu().numpy().tolist())\n\n            key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n            experiment_data[\"class_imbalance\"][name][key] = {\n                \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n                \"losses\": {\n                    \"train\": np.array(train_losses),\n                    \"val\": np.array(val_losses),\n                },\n                \"predictions\": np.array(test_preds),\n                \"ground_truth\": np.array(test_gt),\n            }\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic dataset generation\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Model builder supporting variable depth\ndef build_model(inp_dim, hid_dim, out_dim, depth):\n    if depth == 0:\n        return nn.Linear(inp_dim, out_dim)\n    layers = []\n    if depth >= 1:\n        layers += [nn.Linear(inp_dim, hid_dim), nn.ReLU()]\n    if depth == 2:\n        layers += [nn.Linear(hid_dim, hid_dim), nn.ReLU()]\n    layers += [nn.Linear(hid_dim, out_dim)]\n    return nn.Sequential(*layers)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Fixed hyperparameters\nai_bs, usr_bs = 32, 32\nai_hid, usr_hid = 16, 8\ndepths = [0, 1, 2]\n\n# Prepare experiment data container\nexperiment_data = {\"network_depth_ablation\": {\"synthetic\": {}}}\n\n# Pre-build AI data loaders\nai_tr_loader = DataLoader(SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True)\n\nfor ai_depth in depths:\n    # Train AI model\n    ai_model = build_model(D, ai_hid, 2, ai_depth).to(device)\n    optim_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n    crit_ai = nn.CrossEntropyLoss()\n    for _ in range(15):\n        ai_model.train()\n        for b in ai_tr_loader:\n            x, yb = b[\"x\"].to(device), b[\"y\"].to(device)\n            out = ai_model(x)\n            loss = crit_ai(out, yb)\n            optim_ai.zero_grad()\n            loss.backward()\n            optim_ai.step()\n    # Generate pseudo-labels\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n    p_train = probs_all[: len(X_train)]\n    p_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\n    p_test = probs_all[-len(X_test) :]\n    f_train, f_val, f_test = p_train.argmax(1), p_val.argmax(1), p_test.argmax(1)\n    # Prepare User datasets/loaders\n    X_utrain = np.hstack([X_train, p_train])\n    X_uval = np.hstack([X_val, p_val])\n    X_utest = np.hstack([X_test, p_test])\n    usr_tr_loader = DataLoader(\n        UserDS(X_utrain, f_train), batch_size=usr_bs, shuffle=True\n    )\n    usr_val_loader = DataLoader(UserDS(X_uval, f_val), batch_size=usr_bs)\n    usr_test_loader = DataLoader(UserDS(X_utest, f_test), batch_size=usr_bs)\n    # Loop over user depths\n    for usr_depth in depths:\n        user_model = build_model(D + 2, usr_hid, 2, usr_depth).to(device)\n        optim_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n        crit_usr = nn.CrossEntropyLoss()\n        train_accs, val_accs = [], []\n        train_losses, val_losses = [], []\n        for _ in range(20):\n            # train\n            user_model.train()\n            tloss, correct, total = 0.0, 0, 0\n            for b in usr_tr_loader:\n                feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                out = user_model(feat)\n                loss = crit_usr(out, lbl)\n                optim_usr.zero_grad()\n                loss.backward()\n                optim_usr.step()\n                tloss += loss.item() * feat.size(0)\n                preds = out.argmax(1)\n                correct += (preds == lbl).sum().item()\n                total += lbl.size(0)\n            train_losses.append(tloss / total)\n            train_accs.append(correct / total)\n            # val\n            user_model.eval()\n            vloss, vcorr, vtot = 0.0, 0, 0\n            with torch.no_grad():\n                for b in usr_val_loader:\n                    feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = crit_usr(out, lbl)\n                    vloss += loss.item() * feat.size(0)\n                    preds = out.argmax(1)\n                    vcorr += (preds == lbl).sum().item()\n                    vtot += lbl.size(0)\n            val_losses.append(vloss / vtot)\n            val_accs.append(vcorr / vtot)\n        # test\n        test_preds, test_gt = [], []\n        user_model.eval()\n        with torch.no_grad():\n            for b in usr_test_loader:\n                feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                out = user_model(feat)\n                pred = out.argmax(1).cpu().numpy().tolist()\n                test_preds.extend(pred)\n                test_gt.extend(lbl.cpu().numpy().tolist())\n        key = f\"ai_depth_{ai_depth}_usr_depth_{usr_depth}\"\n        experiment_data[\"network_depth_ablation\"][\"synthetic\"][key] = {\n            \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n            \"losses\": {\"train\": np.array(train_losses), \"val\": np.array(val_losses)},\n            \"predictions\": np.array(test_preds),\n            \"ground_truth\": np.array(test_gt),\n        }\n\n# Save data\nnp.save(\"experiment_data.npy\", experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic dataset generation\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Activation factory\ndef get_activation(name):\n    if name == \"relu\":\n        return nn.ReLU()\n    if name == \"tanh\":\n        return nn.Tanh()\n    if name == \"leaky_relu\":\n        return nn.LeakyReLU()\n    if name == \"linear\":\n        return nn.Identity()\n    raise ValueError(f\"Unknown activation {name}\")\n\n\n# Model definitions parametrized by activation\nclass AIModel(nn.Module):\n    def __init__(self, inp, hid, out, act):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp, hid),\n            get_activation(act),\n            nn.Linear(hid, out),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp, hid, out, act):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp, hid),\n            get_activation(act),\n            nn.Linear(hid, out),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Ablation over activation functions\nactivation_variants = [\"relu\", \"tanh\", \"leaky_relu\", \"linear\"]\nexperiment_data = {}\n\n# Fixed hyperparameters\nai_bs, usr_bs = 32, 32\nai_epochs, usr_epochs = 15, 20\nlr = 1e-2\n\nfor act in activation_variants:\n    data_dict = {\n        \"ai_losses\": {\"train\": [], \"val\": []},\n        \"ai_metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # AI data loaders\n    ai_tr_loader = DataLoader(\n        SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True\n    )\n    ai_val_loader = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n    # Initialize AI model\n    ai_model = AIModel(D, 16, 2, act).to(device)\n    criterion_ai = nn.CrossEntropyLoss()\n    opt_ai = optim.Adam(ai_model.parameters(), lr=lr)\n    # Train AI\n    for _ in range(ai_epochs):\n        ai_model.train()\n        t_loss, t_corr, t_tot = 0.0, 0, 0\n        for b in ai_tr_loader:\n            x, lbl = b[\"x\"].to(device), b[\"y\"].to(device)\n            out = ai_model(x)\n            loss = criterion_ai(out, lbl)\n            opt_ai.zero_grad()\n            loss.backward()\n            opt_ai.step()\n            t_loss += loss.item() * x.size(0)\n            preds = out.argmax(1)\n            t_corr += (preds == lbl).sum().item()\n            t_tot += x.size(0)\n        data_dict[\"ai_losses\"][\"train\"].append(t_loss / t_tot)\n        data_dict[\"ai_metrics\"][\"train\"].append(t_corr / t_tot)\n        ai_model.eval()\n        v_loss, v_corr, v_tot = 0.0, 0, 0\n        with torch.no_grad():\n            for b in ai_val_loader:\n                x, lbl = b[\"x\"].to(device), b[\"y\"].to(device)\n                out = ai_model(x)\n                loss = criterion_ai(out, lbl)\n                v_loss += loss.item() * x.size(0)\n                preds = out.argmax(1)\n                v_corr += (preds == lbl).sum().item()\n                v_tot += x.size(0)\n        data_dict[\"ai_losses\"][\"val\"].append(v_loss / v_tot)\n        data_dict[\"ai_metrics\"][\"val\"].append(v_corr / v_tot)\n    # Generate soft labels\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n    p_train = probs_all[: len(X_train)]\n    p_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\n    p_test = probs_all[-len(X_test) :]\n    f_train = p_train.argmax(1)\n    f_val = p_val.argmax(1)\n    f_test = p_test.argmax(1)\n    # User data loaders\n    X_usr_train = np.hstack([X_train, p_train])\n    X_usr_val = np.hstack([X_val, p_val])\n    X_usr_test = np.hstack([X_test, p_test])\n    usr_tr_loader = DataLoader(\n        UserDS(X_usr_train, f_train), batch_size=usr_bs, shuffle=True\n    )\n    usr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=usr_bs)\n    usr_test_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=usr_bs)\n    # Initialize User model\n    user_model = UserModel(D + 2, 8, 2, act).to(device)\n    criterion_usr = nn.CrossEntropyLoss()\n    opt_usr = optim.Adam(user_model.parameters(), lr=lr)\n    # Train User\n    for _ in range(usr_epochs):\n        user_model.train()\n        t_loss, t_corr, t_tot = 0.0, 0, 0\n        for b in usr_tr_loader:\n            feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n            out = user_model(feat)\n            loss = criterion_usr(out, lbl)\n            opt_usr.zero_grad()\n            loss.backward()\n            opt_usr.step()\n            t_loss += loss.item() * feat.size(0)\n            preds = out.argmax(1)\n            t_corr += (preds == lbl).sum().item()\n            t_tot += feat.size(0)\n        data_dict[\"losses\"][\"train\"].append(t_loss / t_tot)\n        data_dict[\"metrics\"][\"train\"].append(t_corr / t_tot)\n        user_model.eval()\n        v_loss, v_corr, v_tot = 0.0, 0, 0\n        with torch.no_grad():\n            for b in usr_val_loader:\n                feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                out = user_model(feat)\n                loss = criterion_usr(out, lbl)\n                v_loss += loss.item() * feat.size(0)\n                preds = out.argmax(1)\n                v_corr += (preds == lbl).sum().item()\n                v_tot += feat.size(0)\n        data_dict[\"losses\"][\"val\"].append(v_loss / v_tot)\n        data_dict[\"metrics\"][\"val\"].append(v_corr / v_tot)\n    # Test evaluation\n    test_preds, test_gt = [], []\n    user_model.eval()\n    with torch.no_grad():\n        for b in usr_test_loader:\n            feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n            out = user_model(feat)\n            test_preds.extend(out.argmax(1).cpu().numpy().tolist())\n            test_gt.extend(lbl.cpu().numpy().tolist())\n    data_dict[\"predictions\"] = np.array(test_preds)\n    data_dict[\"ground_truth\"] = np.array(test_gt)\n    # Convert lists to numpy arrays\n    for k in [\"ai_losses\", \"ai_metrics\", \"losses\", \"metrics\"]:\n        for phase in [\"train\", \"val\"]:\n            data_dict[k][phase] = np.array(data_dict[k][phase])\n    experiment_data[act] = {\"synthetic\": data_dict}\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import numpy as np, torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Reproducibility and device\nnp.random.seed(0)\ntorch.manual_seed(0)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Synthetic dataset\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true, b_true = np.array([2.0, -3.0]), 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Split and normalize\nidx = np.random.permutation(N)\ntr, va, te = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[tr], y[tr]\nX_val, y_val = X[va], y[va]\nX_test, y_test = X[te], y[te]\nmu, sd = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mu) / sd\nX_val = (X_val - mu) / sd\nX_test = (X_test - mu) / sd\n\n\n# Datasets\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.x, self.y = torch.from_numpy(X).float(), torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.x[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.f, self.y = torch.from_numpy(feat).float(), torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.f[i], \"label\": self.y[i]}\n\n\n# Models\nclass AIModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(D, 16), nn.ReLU(), nn.Linear(16, 2))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(inp, 8), nn.ReLU(), nn.Linear(8, 2))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Ablation study: original-feature removal\nai_bs_list = [16, 32, 64]\nusr_bs_list = [16, 32, 64]\nexperiment_data = {\"original_feature_removal\": {\"synthetic\": {}}}\n\nfor ai_bs in ai_bs_list:\n    ai_tr = DataLoader(SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True)\n    ai_val = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n    model_ai = AIModel().to(device)\n    opt_ai = optim.Adam(model_ai.parameters(), lr=1e-2)\n    crit = nn.CrossEntropyLoss()\n    for _ in range(15):\n        model_ai.train()\n        for b in ai_tr:\n            x, yb = b[\"x\"].to(device), b[\"y\"].to(device)\n            loss = crit(model_ai(x), yb)\n            opt_ai.zero_grad()\n            loss.backward()\n            opt_ai.step()\n    # get softmax outputs\n    model_ai.eval()\n    with torch.no_grad():\n        Xall = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        ps = torch.softmax(model_ai(Xall), 1).cpu().numpy()\n    p_tr, p_va, p_te = (\n        ps[: len(X_train)],\n        ps[len(X_train) : len(X_train) + len(X_val)],\n        ps[-len(X_test) :],\n    )\n    f_tr, f_va, f_te = p_tr.argmax(1), p_va.argmax(1), p_te.argmax(1)\n\n    # only probabilities as features\n    for usr_bs in usr_bs_list:\n        usr_tr = DataLoader(UserDS(p_tr, f_tr), batch_size=usr_bs, shuffle=True)\n        usr_val = DataLoader(UserDS(p_va, f_va), batch_size=usr_bs)\n        usr_test = DataLoader(UserDS(p_te, f_te), batch_size=usr_bs)\n        user = UserModel(inp=2).to(device)\n        opt_u = optim.Adam(user.parameters(), lr=1e-2)\n        acc_tr, acc_va, ls_tr, ls_va = [], [], [], []\n        for _ in range(20):\n            user.train()\n            tot, corr, L = 0, 0, 0.0\n            for b in usr_tr:\n                f, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                out = user(f)\n                loss = crit(out, lbl)\n                opt_u.zero_grad()\n                loss.backward()\n                opt_u.step()\n                L += loss.item() * f.size(0)\n                preds = out.argmax(1)\n                corr += (preds == lbl).sum().item()\n                tot += lbl.size(0)\n            ls_tr.append(L / tot)\n            acc_tr.append(corr / tot)\n            user.eval()\n            with torch.no_grad():\n                vt, vc, vL = 0, 0, 0.0\n                for b in usr_val:\n                    f, l = b[\"feat\"].to(device), b[\"label\"].to(device)\n                    o = user(f)\n                    loss = crit(o, l)\n                    vL += loss.item() * f.size(0)\n                    p = o.argmax(1)\n                    vc += (p == l).sum().item()\n                    vt += l.size(0)\n                ls_va.append(vL / vt)\n                acc_va.append(vc / vt)\n        # test\n        preds, gts = [], []\n        user.eval()\n        with torch.no_grad():\n            for b in usr_test:\n                f, l = b[\"feat\"].to(device), b[\"label\"].to(device)\n                p = user(f).argmax(1).cpu().numpy()\n                preds.extend(p.tolist())\n                gts.extend(l.cpu().numpy().tolist())\n        key = f\"ai_bs_{ai_bs}_usr_bs_{usr_bs}\"\n        experiment_data[\"original_feature_removal\"][\"synthetic\"][key] = {\n            \"metrics\": {\"train\": np.array(acc_tr), \"val\": np.array(acc_va)},\n            \"losses\": {\"train\": np.array(ls_tr), \"val\": np.array(ls_va)},\n            \"predictions\": np.array(preds),\n            \"ground_truth\": np.array(gts),\n        }\n\nnp.save(\"experiment_data.npy\", experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Synthetic dataset generation\nnp.random.seed(0)\ntorch.manual_seed(0)\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n# Pre-stack for teacher inference\nX_all_tensor = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\nn_tr, n_val, n_test = len(X_train), len(X_val), len(X_test)\n\n\n# Datasets\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Models\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameters\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\nteacher_sizes = [1, 3, 5]\n\n# Container for all results\nexperiment_data = {\"teacher_ensemble_size\": {}}\n\nfor size in teacher_sizes:\n    experiment_data[\"teacher_ensemble_size\"][str(size)] = {}\n    for ai_bs in ai_batch_sizes:\n        # Collect teacher probabilities\n        p_tr_list, p_val_list, p_te_list = [], [], []\n        for t in range(size):\n            seed = t\n            np.random.seed(seed)\n            torch.manual_seed(seed)\n            # AI data loader\n            ai_tr_loader = DataLoader(\n                SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True\n            )\n            # Train one teacher\n            ai_model = AIModel(D, 16, 2).to(device)\n            crit_ai = nn.CrossEntropyLoss()\n            opt_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n            for _ in range(15):\n                ai_model.train()\n                for b in ai_tr_loader:\n                    x, yb = b[\"x\"].to(device), b[\"y\"].to(device)\n                    out = ai_model(x)\n                    loss = crit_ai(out, yb)\n                    opt_ai.zero_grad()\n                    loss.backward()\n                    opt_ai.step()\n            # Inference\n            ai_model.eval()\n            with torch.no_grad():\n                logits_all = ai_model(X_all_tensor)\n                probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n            p_tr_list.append(probs_all[:n_tr])\n            p_val_list.append(probs_all[n_tr : n_tr + n_val])\n            p_te_list.append(probs_all[-n_test:])\n        # Average ensembled probabilities\n        p_train = np.mean(p_tr_list, axis=0)\n        p_val = np.mean(p_val_list, axis=0)\n        p_test = np.mean(p_te_list, axis=0)\n        # Pseudo\u2010labels\n        f_train = p_train.argmax(axis=1)\n        f_val = p_val.argmax(axis=1)\n        f_test = p_test.argmax(axis=1)\n        # User features\n        X_usr_train = np.hstack([X_train, p_train])\n        X_usr_val = np.hstack([X_val, p_val])\n        X_usr_test = np.hstack([X_test, p_test])\n\n        for usr_bs in usr_batch_sizes:\n            # Data loaders\n            usr_tr_loader = DataLoader(\n                UserDS(X_usr_train, f_train), batch_size=usr_bs, shuffle=True\n            )\n            usr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=usr_bs)\n            usr_te_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=usr_bs)\n            # Init user model\n            user_model = UserModel(D + 2, 8, 2).to(device)\n            crit_usr = nn.CrossEntropyLoss()\n            opt_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n            train_accs, val_accs = [], []\n            train_losses, val_losses = [], []\n            # Train user\n            for _ in range(20):\n                user_model.train()\n                t_loss, corr, tot = 0.0, 0, 0\n                for b in usr_tr_loader:\n                    feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = crit_usr(out, lbl)\n                    opt_usr.zero_grad()\n                    loss.backward()\n                    opt_usr.step()\n                    t_loss += loss.item() * feat.size(0)\n                    preds = out.argmax(1)\n                    corr += (preds == lbl).sum().item()\n                    tot += lbl.size(0)\n                train_losses.append(t_loss / tot)\n                train_accs.append(corr / tot)\n                # Validation\n                user_model.eval()\n                v_loss, v_corr, v_tot = 0.0, 0, 0\n                with torch.no_grad():\n                    for b in usr_val_loader:\n                        feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                        out = user_model(feat)\n                        loss = crit_usr(out, lbl)\n                        v_loss += loss.item() * feat.size(0)\n                        preds = out.argmax(1)\n                        v_corr += (preds == lbl).sum().item()\n                        v_tot += lbl.size(0)\n                val_losses.append(v_loss / v_tot)\n                val_accs.append(v_corr / v_tot)\n            # Test evaluation\n            test_preds, test_gt = [], []\n            user_model.eval()\n            with torch.no_grad():\n                for b in usr_te_loader:\n                    feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                    out = user_model(feat)\n                    test_preds.extend(out.argmax(1).cpu().numpy().tolist())\n                    test_gt.extend(lbl.cpu().numpy().tolist())\n            # Record\n            ds_name = f\"ensemble_{size}_ai_bs_{ai_bs}_usr_bs_{usr_bs}\"\n            experiment_data[\"teacher_ensemble_size\"][str(size)][ds_name] = {\n                \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n                \"losses\": {\n                    \"train\": np.array(train_losses),\n                    \"val\": np.array(val_losses),\n                },\n                \"predictions\": np.array(test_preds),\n                \"ground_truth\": np.array(test_gt),\n            }\n\n# Save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config and reproducibility\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic dataset generation\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split and normalization\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Model definitions\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameters\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\nnoise_levels = [0.0, 0.1, 0.2, 0.5]\n\n# Container for ablation results\nexperiment_data = {\"teacher_prob_noise\": {}}\n\nfor ai_bs in ai_batch_sizes:\n    # Train AIModel\n    ai_tr_loader = DataLoader(\n        SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True\n    )\n    ai_model = AIModel(D, 16, 2).to(device)\n    criterion_ai = nn.CrossEntropyLoss()\n    optimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n    for _ in range(15):\n        ai_model.train()\n        for batch in ai_tr_loader:\n            x, yb = batch[\"x\"].to(device), batch[\"y\"].to(device)\n            out = ai_model(x)\n            loss = criterion_ai(out, yb)\n            optimizer_ai.zero_grad()\n            loss.backward()\n            optimizer_ai.step()\n\n    # Generate teacher probabilities\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n    p_train = probs_all[: len(X_train)]\n    p_val = probs_all[len(X_train) : len(X_train) + len(X_val)]\n    p_test = probs_all[-len(X_test) :]\n\n    for sigma in noise_levels:\n        skey = f\"sigma_{sigma}\"\n        if skey not in experiment_data[\"teacher_prob_noise\"]:\n            experiment_data[\"teacher_prob_noise\"][skey] = {\"batch_size\": {}}\n\n        # Inject Gaussian noise and renormalize\n        noise_t = np.random.normal(0, sigma, p_train.shape)\n        noise_v = np.random.normal(0, sigma, p_val.shape)\n        noise_te = np.random.normal(0, sigma, p_test.shape)\n        p_train_n = np.clip(p_train + noise_t, 0, 1)\n        p_val_n = np.clip(p_val + noise_v, 0, 1)\n        p_test_n = np.clip(p_test + noise_te, 0, 1)\n        p_train_n /= p_train_n.sum(axis=1, keepdims=True)\n        p_val_n /= p_val_n.sum(axis=1, keepdims=True)\n        p_test_n /= p_test_n.sum(axis=1, keepdims=True)\n\n        # New teacher labels from noisy probs\n        f_train_n = p_train_n.argmax(axis=1)\n        f_val_n = p_val_n.argmax(axis=1)\n        f_test_n = p_test_n.argmax(axis=1)\n\n        # Prepare UserModel inputs\n        X_usr_train = np.hstack([X_train, p_train_n])\n        X_usr_val = np.hstack([X_val, p_val_n])\n        X_usr_test = np.hstack([X_test, p_test_n])\n\n        for usr_bs in usr_batch_sizes:\n            usr_tr_loader = DataLoader(\n                UserDS(X_usr_train, f_train_n), batch_size=usr_bs, shuffle=True\n            )\n            usr_val_loader = DataLoader(UserDS(X_usr_val, f_val_n), batch_size=usr_bs)\n            usr_test_loader = DataLoader(\n                UserDS(X_usr_test, f_test_n), batch_size=usr_bs\n            )\n\n            user_model = UserModel(D + 2, 8, 2).to(device)\n            criterion_usr = nn.CrossEntropyLoss()\n            optimizer_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n\n            train_accs, val_accs = [], []\n            train_losses, val_losses = [], []\n\n            # Train UserModel\n            for _ in range(20):\n                user_model.train()\n                t_loss, corr, tot = 0.0, 0, 0\n                for batch in usr_tr_loader:\n                    feat, lbl = batch[\"feat\"].to(device), batch[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = criterion_usr(out, lbl)\n                    optimizer_usr.zero_grad()\n                    loss.backward()\n                    optimizer_usr.step()\n                    t_loss += loss.item() * feat.size(0)\n                    preds = out.argmax(dim=1)\n                    corr += (preds == lbl).sum().item()\n                    tot += lbl.size(0)\n                train_losses.append(t_loss / tot)\n                train_accs.append(corr / tot)\n\n                user_model.eval()\n                v_loss, v_corr, v_tot = 0.0, 0, 0\n                with torch.no_grad():\n                    for batch in usr_val_loader:\n                        feat, lbl = batch[\"feat\"].to(device), batch[\"label\"].to(device)\n                        out = user_model(feat)\n                        loss = criterion_usr(out, lbl)\n                        v_loss += loss.item() * feat.size(0)\n                        preds = out.argmax(dim=1)\n                        v_corr += (preds == lbl).sum().item()\n                        v_tot += lbl.size(0)\n                val_losses.append(v_loss / v_tot)\n                val_accs.append(v_corr / v_tot)\n\n            # Test evaluation\n            test_preds, test_gt = [], []\n            user_model.eval()\n            with torch.no_grad():\n                for batch in usr_test_loader:\n                    feat, lbl = batch[\"feat\"].to(device), batch[\"label\"].to(device)\n                    out = user_model(feat)\n                    p = out.argmax(dim=1).cpu().numpy()\n                    test_preds.extend(p.tolist())\n                    test_gt.extend(lbl.cpu().numpy().tolist())\n\n            key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n            experiment_data[\"teacher_prob_noise\"][skey][\"batch_size\"][key] = {\n                \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n                \"losses\": {\n                    \"train\": np.array(train_losses),\n                    \"val\": np.array(val_losses),\n                },\n                \"predictions\": np.array(test_preds),\n                \"ground_truth\": np.array(test_gt),\n            }\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Device config and reproducibility\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic dataset generation\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split and normalization\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Dataset classes\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Model definitions\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameters\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\nnoise_levels = [0.0, 0.1, 0.2, 0.5]\nexperiment_data = {\"teacher_prob_noise\": {}}\n\nfor ai_bs in ai_batch_sizes:\n    # Train AIModel\n    ai_tr_loader = DataLoader(\n        SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True\n    )\n    ai_model = AIModel(D, 16, 2).to(device)\n    criterion_ai = nn.CrossEntropyLoss()\n    optimizer_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n    for epoch in range(15):\n        ai_model.train()\n        for batch in ai_tr_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = ai_model(batch[\"x\"])\n            loss = criterion_ai(out, batch[\"y\"])\n            optimizer_ai.zero_grad()\n            loss.backward()\n            optimizer_ai.step()\n\n    # Generate teacher probabilities\n    ai_model.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        logits_all = ai_model(X_all)\n        probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n    p_train, p_val, p_test = (\n        probs_all[: len(X_train)],\n        probs_all[len(X_train) : len(X_train) + len(X_val)],\n        probs_all[-len(X_test) :],\n    )\n\n    for sigma in noise_levels:\n        skey = f\"sigma_{sigma}\"\n        if skey not in experiment_data[\"teacher_prob_noise\"]:\n            experiment_data[\"teacher_prob_noise\"][skey] = {\"batch_size\": {}}\n\n        # Inject Gaussian noise and renormalize safely\n        noise_t = np.random.normal(0, sigma, p_train.shape)\n        noise_v = np.random.normal(0, sigma, p_val.shape)\n        noise_te = np.random.normal(0, sigma, p_test.shape)\n\n        p_train_n = np.clip(p_train + noise_t, 0, 1)\n        sums_t = p_train_n.sum(axis=1, keepdims=True)\n        sums_t[sums_t == 0] = 1\n        p_train_n /= sums_t\n\n        p_val_n = np.clip(p_val + noise_v, 0, 1)\n        sums_v = p_val_n.sum(axis=1, keepdims=True)\n        sums_v[sums_v == 0] = 1\n        p_val_n /= sums_v\n\n        p_test_n = np.clip(p_test + noise_te, 0, 1)\n        sums_te = p_test_n.sum(axis=1, keepdims=True)\n        sums_te[sums_te == 0] = 1\n        p_test_n /= sums_te\n\n        f_train_n, f_val_n, f_test_n = (\n            p_train_n.argmax(axis=1),\n            p_val_n.argmax(axis=1),\n            p_test_n.argmax(axis=1),\n        )\n\n        X_usr_train = np.hstack([X_train, p_train_n])\n        X_usr_val = np.hstack([X_val, p_val_n])\n        X_usr_test = np.hstack([X_test, p_test_n])\n\n        for usr_bs in usr_batch_sizes:\n            usr_tr_loader = DataLoader(\n                UserDS(X_usr_train, f_train_n), batch_size=usr_bs, shuffle=True\n            )\n            usr_val_loader = DataLoader(UserDS(X_usr_val, f_val_n), batch_size=usr_bs)\n            usr_test_loader = DataLoader(\n                UserDS(X_usr_test, f_test_n), batch_size=usr_bs\n            )\n\n            user_model = UserModel(D + 2, 8, 2).to(device)\n            criterion_usr = nn.CrossEntropyLoss()\n            optimizer_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n\n            train_accs, val_accs = [], []\n            train_losses, val_losses = [], []\n\n            for epoch in range(20):\n                user_model.train()\n                t_loss, corr, tot = 0.0, 0, 0\n                for batch in usr_tr_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    out = user_model(batch[\"feat\"])\n                    loss = criterion_usr(out, batch[\"label\"])\n                    optimizer_usr.zero_grad()\n                    loss.backward()\n                    optimizer_usr.step()\n                    t_loss += loss.item() * batch[\"feat\"].size(0)\n                    preds = out.argmax(dim=1)\n                    corr += (preds == batch[\"label\"]).sum().item()\n                    tot += batch[\"label\"].size(0)\n                train_losses.append(t_loss / tot)\n                train_accs.append(corr / tot)\n\n                user_model.eval()\n                v_loss, v_corr, v_tot = 0.0, 0, 0\n                with torch.no_grad():\n                    for batch in usr_val_loader:\n                        batch = {k: v.to(device) for k, v in batch.items()}\n                        out = user_model(batch[\"feat\"])\n                        loss = criterion_usr(out, batch[\"label\"])\n                        v_loss += loss.item() * batch[\"feat\"].size(0)\n                        preds = out.argmax(dim=1)\n                        v_corr += (preds == batch[\"label\"]).sum().item()\n                        v_tot += batch[\"label\"].size(0)\n                val_losses.append(v_loss / v_tot)\n                val_accs.append(v_corr / v_tot)\n                print(f\"Epoch {epoch+1}: validation_loss = {val_losses[-1]:.4f}\")\n\n            # Test evaluation\n            test_preds, test_gt = [], []\n            user_model.eval()\n            with torch.no_grad():\n                for batch in usr_test_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    out = user_model(batch[\"feat\"])\n                    p = out.argmax(dim=1).cpu().numpy()\n                    test_preds.extend(p.tolist())\n                    test_gt.extend(batch[\"label\"].cpu().numpy().tolist())\n\n            key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n            experiment_data[\"teacher_prob_noise\"][skey][\"batch_size\"][key] = {\n                \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n                \"losses\": {\n                    \"train\": np.array(train_losses),\n                    \"val\": np.array(val_losses),\n                },\n                \"predictions\": np.array(test_preds),\n                \"ground_truth\": np.array(test_gt),\n            }\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n\n# Datasets\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, feat, label, conf):\n        self.X = torch.from_numpy(feat).float()\n        self.y = torch.from_numpy(label).long()\n        self.w = torch.from_numpy(conf).float()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i], \"weight\": self.w[i]}\n\n\n# Models\nclass AIModel(nn.Module):\n    def __init__(self, inp, hid, out):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(inp, hid), nn.ReLU(), nn.Linear(hid, out))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp, hid, out):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(inp, hid), nn.ReLU(), nn.Linear(hid, out))\n\n    def forward(self, x):\n        return self.net(x)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\nthreshold = 0.8\n\nexperiment_data = {\"uniform\": {}, \"thresholded\": {}, \"confidence_weighted\": {}}\n\nfor ai_bs in ai_batch_sizes:\n    # Train AI model\n    ai_tr = DataLoader(SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True)\n    ai_val = DataLoader(SimpleDS(X_val, y_val), batch_size=ai_bs)\n    ai = AIModel(D, 16, 2).to(device)\n    opt_ai = optim.Adam(ai.parameters(), lr=1e-2)\n    crit_ai = nn.CrossEntropyLoss()\n    for _ in range(15):\n        ai.train()\n        for b in ai_tr:\n            x, yb = b[\"x\"].to(device), b[\"y\"].to(device)\n            out = ai(x)\n            loss = crit_ai(out, yb)\n            opt_ai.zero_grad()\n            loss.backward()\n            opt_ai.step()\n    # Teacher preds & confidences\n    ai.eval()\n    with torch.no_grad():\n        X_all = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\n        lg = ai(X_all)\n        pp = F.softmax(lg, dim=1).cpu().numpy()\n    p_train = pp[: len(X_train)]\n    p_val = pp[len(X_train) : len(X_train) + len(X_val)]\n    p_test = pp[-len(X_test) :]\n    f_train = p_train.argmax(1)\n    f_val = p_val.argmax(1)\n    f_test = p_test.argmax(1)\n    c_train = p_train.max(1)\n    c_val = p_val.max(1)\n    c_test = p_test.max(1)\n    # Student features include teacher probs\n    X_usr_train = np.hstack([X_train, p_train])\n    X_usr_val = np.hstack([X_val, p_val])\n    X_usr_test = np.hstack([X_test, p_test])\n\n    for ablation in [\"uniform\", \"thresholded\", \"confidence_weighted\"]:\n        for usr_bs in usr_batch_sizes:\n            tr_ds = UserDS(X_usr_train, f_train, c_train)\n            val_ds = UserDS(X_usr_val, f_val, c_val)\n            te_ds = UserDS(X_usr_test, f_test, c_test)\n            tr_ld = DataLoader(tr_ds, batch_size=usr_bs, shuffle=True)\n            val_ld = DataLoader(val_ds, batch_size=usr_bs)\n            te_ld = DataLoader(te_ds, batch_size=usr_bs)\n            user = UserModel(D + 2, 8, 2).to(device)\n            opt_u = optim.Adam(user.parameters(), lr=1e-2)\n\n            train_accs, val_accs = [], []\n            train_losses, val_losses = [], []\n\n            for _ in range(20):\n                # train\n                user.train()\n                num_sum = 0.0\n                w_sum = 0.0\n                corr = tot = 0\n                for b in tr_ld:\n                    feat = b[\"feat\"].to(device)\n                    lbl = b[\"label\"].to(device)\n                    wconf = b[\"weight\"].to(device)\n                    # select weights\n                    if ablation == \"uniform\":\n                        w = torch.ones_like(wconf)\n                    elif ablation == \"thresholded\":\n                        w = (wconf >= threshold).float()\n                    else:\n                        w = wconf\n                    out = user(feat)\n                    losses = F.cross_entropy(out, lbl, reduction=\"none\")\n                    weighted = (losses * w).sum() / (w.sum() + 1e-6)\n                    opt_u.zero_grad()\n                    weighted.backward()\n                    opt_u.step()\n                    num_sum += (losses * w).sum().item()\n                    w_sum += w.sum().item()\n                    preds = out.argmax(1)\n                    corr += (preds == lbl).sum().item()\n                    tot += lbl.size(0)\n                train_losses.append(num_sum / (w_sum + 1e-6))\n                train_accs.append(corr / tot)\n\n                # val\n                user.eval()\n                num_sum = 0.0\n                w_sum = 0.0\n                v_corr = v_tot = 0\n                with torch.no_grad():\n                    for b in val_ld:\n                        feat = b[\"feat\"].to(device)\n                        lbl = b[\"label\"].to(device)\n                        wconf = b[\"weight\"].to(device)\n                        if ablation == \"uniform\":\n                            w = torch.ones_like(wconf)\n                        elif ablation == \"thresholded\":\n                            w = (wconf >= threshold).float()\n                        else:\n                            w = wconf\n                        out = user(feat)\n                        losses = F.cross_entropy(out, lbl, reduction=\"none\")\n                        num_sum += (losses * w).sum().item()\n                        w_sum += w.sum().item()\n                        preds = out.argmax(1)\n                        v_corr += (preds == lbl).sum().item()\n                        v_tot += lbl.size(0)\n                val_losses.append(num_sum / (w_sum + 1e-6))\n                val_accs.append(v_corr / v_tot)\n\n            # test\n            test_preds = []\n            test_gt = []\n            user.eval()\n            with torch.no_grad():\n                for b in te_ld:\n                    feat = b[\"feat\"].to(device)\n                    lbl = b[\"label\"].to(device)\n                    out = user(feat)\n                    p = out.argmax(1).cpu().numpy().tolist()\n                    test_preds.extend(p)\n                    test_gt.extend(lbl.cpu().numpy().tolist())\n\n            key = f\"ai_bs_{ai_bs}_user_bs_{usr_bs}\"\n            experiment_data[ablation][key] = {\n                \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n                \"losses\": {\n                    \"train\": np.array(train_losses),\n                    \"val\": np.array(val_losses),\n                },\n                \"predictions\": np.array(test_preds),\n                \"ground_truth\": np.array(test_gt),\n            }\n\n# Save\nnp.save(\"experiment_data.npy\", experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Synthetic dataset generation\nnp.random.seed(0)\ntorch.manual_seed(0)\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n# Pre-stack for teacher inference\nX_all_tensor = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\nn_tr, n_val, n_test = len(X_train), len(X_val), len(X_test)\n\n\n# Datasets\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Models\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameters\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\nteacher_sizes = [1, 3, 5]\n\n# Container for all results\nexperiment_data = {\"teacher_ensemble_size\": {}}\n\nfor size in teacher_sizes:\n    experiment_data[\"teacher_ensemble_size\"][str(size)] = {}\n    for ai_bs in ai_batch_sizes:\n        # Collect teacher probabilities\n        p_tr_list, p_val_list, p_te_list = [], [], []\n        for t in range(size):\n            seed = t\n            np.random.seed(seed)\n            torch.manual_seed(seed)\n            # AI data loader\n            ai_tr_loader = DataLoader(\n                SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True\n            )\n            # Train one teacher\n            ai_model = AIModel(D, 16, 2).to(device)\n            crit_ai = nn.CrossEntropyLoss()\n            opt_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n            for _ in range(15):\n                ai_model.train()\n                for b in ai_tr_loader:\n                    x, yb = b[\"x\"].to(device), b[\"y\"].to(device)\n                    out = ai_model(x)\n                    loss = crit_ai(out, yb)\n                    opt_ai.zero_grad()\n                    loss.backward()\n                    opt_ai.step()\n            # Inference\n            ai_model.eval()\n            with torch.no_grad():\n                logits_all = ai_model(X_all_tensor)\n                probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n            p_tr_list.append(probs_all[:n_tr])\n            p_val_list.append(probs_all[n_tr : n_tr + n_val])\n            p_te_list.append(probs_all[-n_test:])\n        # Average ensembled probabilities\n        p_train = np.mean(p_tr_list, axis=0)\n        p_val = np.mean(p_val_list, axis=0)\n        p_test = np.mean(p_te_list, axis=0)\n        # Pseudo\u2010labels\n        f_train = p_train.argmax(axis=1)\n        f_val = p_val.argmax(axis=1)\n        f_test = p_test.argmax(axis=1)\n        # User features\n        X_usr_train = np.hstack([X_train, p_train])\n        X_usr_val = np.hstack([X_val, p_val])\n        X_usr_test = np.hstack([X_test, p_test])\n\n        for usr_bs in usr_batch_sizes:\n            # Data loaders\n            usr_tr_loader = DataLoader(\n                UserDS(X_usr_train, f_train), batch_size=usr_bs, shuffle=True\n            )\n            usr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=usr_bs)\n            usr_te_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=usr_bs)\n            # Init user model\n            user_model = UserModel(D + 2, 8, 2).to(device)\n            crit_usr = nn.CrossEntropyLoss()\n            opt_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n            train_accs, val_accs = [], []\n            train_losses, val_losses = [], []\n            # Train user\n            for _ in range(20):\n                user_model.train()\n                t_loss, corr, tot = 0.0, 0, 0\n                for b in usr_tr_loader:\n                    feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = crit_usr(out, lbl)\n                    opt_usr.zero_grad()\n                    loss.backward()\n                    opt_usr.step()\n                    t_loss += loss.item() * feat.size(0)\n                    preds = out.argmax(1)\n                    corr += (preds == lbl).sum().item()\n                    tot += lbl.size(0)\n                train_losses.append(t_loss / tot)\n                train_accs.append(corr / tot)\n                # Validation\n                user_model.eval()\n                v_loss, v_corr, v_tot = 0.0, 0, 0\n                with torch.no_grad():\n                    for b in usr_val_loader:\n                        feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                        out = user_model(feat)\n                        loss = crit_usr(out, lbl)\n                        v_loss += loss.item() * feat.size(0)\n                        preds = out.argmax(1)\n                        v_corr += (preds == lbl).sum().item()\n                        v_tot += lbl.size(0)\n                val_losses.append(v_loss / v_tot)\n                val_accs.append(v_corr / v_tot)\n            # Test evaluation\n            test_preds, test_gt = [], []\n            user_model.eval()\n            with torch.no_grad():\n                for b in usr_te_loader:\n                    feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                    out = user_model(feat)\n                    test_preds.extend(out.argmax(1).cpu().numpy().tolist())\n                    test_gt.extend(lbl.cpu().numpy().tolist())\n            # Record\n            ds_name = f\"ensemble_{size}_ai_bs_{ai_bs}_usr_bs_{usr_bs}\"\n            experiment_data[\"teacher_ensemble_size\"][str(size)][ds_name] = {\n                \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n                \"losses\": {\n                    \"train\": np.array(train_losses),\n                    \"val\": np.array(val_losses),\n                },\n                \"predictions\": np.array(test_preds),\n                \"ground_truth\": np.array(test_gt),\n            }\n\n# Save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Synthetic dataset generation\nnp.random.seed(0)\ntorch.manual_seed(0)\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n# Pre-stack for teacher inference\nX_all_tensor = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\nn_tr, n_val, n_test = len(X_train), len(X_val), len(X_test)\n\n\n# Datasets\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Models\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameters\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\nteacher_sizes = [1, 3, 5]\n\n# Container for all results\nexperiment_data = {\"teacher_ensemble_size\": {}}\n\nfor size in teacher_sizes:\n    experiment_data[\"teacher_ensemble_size\"][str(size)] = {}\n    for ai_bs in ai_batch_sizes:\n        # Collect teacher probabilities\n        p_tr_list, p_val_list, p_te_list = [], [], []\n        for t in range(size):\n            seed = t\n            np.random.seed(seed)\n            torch.manual_seed(seed)\n            # AI data loader\n            ai_tr_loader = DataLoader(\n                SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True\n            )\n            # Train one teacher\n            ai_model = AIModel(D, 16, 2).to(device)\n            crit_ai = nn.CrossEntropyLoss()\n            opt_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n            for _ in range(15):\n                ai_model.train()\n                for b in ai_tr_loader:\n                    x, yb = b[\"x\"].to(device), b[\"y\"].to(device)\n                    out = ai_model(x)\n                    loss = crit_ai(out, yb)\n                    opt_ai.zero_grad()\n                    loss.backward()\n                    opt_ai.step()\n            # Inference\n            ai_model.eval()\n            with torch.no_grad():\n                logits_all = ai_model(X_all_tensor)\n                probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n            p_tr_list.append(probs_all[:n_tr])\n            p_val_list.append(probs_all[n_tr : n_tr + n_val])\n            p_te_list.append(probs_all[-n_test:])\n        # Average ensembled probabilities\n        p_train = np.mean(p_tr_list, axis=0)\n        p_val = np.mean(p_val_list, axis=0)\n        p_test = np.mean(p_te_list, axis=0)\n        # Pseudo\u2010labels\n        f_train = p_train.argmax(axis=1)\n        f_val = p_val.argmax(axis=1)\n        f_test = p_test.argmax(axis=1)\n        # User features\n        X_usr_train = np.hstack([X_train, p_train])\n        X_usr_val = np.hstack([X_val, p_val])\n        X_usr_test = np.hstack([X_test, p_test])\n\n        for usr_bs in usr_batch_sizes:\n            # Data loaders\n            usr_tr_loader = DataLoader(\n                UserDS(X_usr_train, f_train), batch_size=usr_bs, shuffle=True\n            )\n            usr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=usr_bs)\n            usr_te_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=usr_bs)\n            # Init user model\n            user_model = UserModel(D + 2, 8, 2).to(device)\n            crit_usr = nn.CrossEntropyLoss()\n            opt_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n            train_accs, val_accs = [], []\n            train_losses, val_losses = [], []\n            # Train user\n            for _ in range(20):\n                user_model.train()\n                t_loss, corr, tot = 0.0, 0, 0\n                for b in usr_tr_loader:\n                    feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = crit_usr(out, lbl)\n                    opt_usr.zero_grad()\n                    loss.backward()\n                    opt_usr.step()\n                    t_loss += loss.item() * feat.size(0)\n                    preds = out.argmax(1)\n                    corr += (preds == lbl).sum().item()\n                    tot += lbl.size(0)\n                train_losses.append(t_loss / tot)\n                train_accs.append(corr / tot)\n                # Validation\n                user_model.eval()\n                v_loss, v_corr, v_tot = 0.0, 0, 0\n                with torch.no_grad():\n                    for b in usr_val_loader:\n                        feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                        out = user_model(feat)\n                        loss = crit_usr(out, lbl)\n                        v_loss += loss.item() * feat.size(0)\n                        preds = out.argmax(1)\n                        v_corr += (preds == lbl).sum().item()\n                        v_tot += lbl.size(0)\n                val_losses.append(v_loss / v_tot)\n                val_accs.append(v_corr / v_tot)\n            # Test evaluation\n            test_preds, test_gt = [], []\n            user_model.eval()\n            with torch.no_grad():\n                for b in usr_te_loader:\n                    feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                    out = user_model(feat)\n                    test_preds.extend(out.argmax(1).cpu().numpy().tolist())\n                    test_gt.extend(lbl.cpu().numpy().tolist())\n            # Record\n            ds_name = f\"ensemble_{size}_ai_bs_{ai_bs}_usr_bs_{usr_bs}\"\n            experiment_data[\"teacher_ensemble_size\"][str(size)][ds_name] = {\n                \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n                \"losses\": {\n                    \"train\": np.array(train_losses),\n                    \"val\": np.array(val_losses),\n                },\n                \"predictions\": np.array(test_preds),\n                \"ground_truth\": np.array(test_gt),\n            }\n\n# Save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Synthetic dataset generation\nnp.random.seed(0)\ntorch.manual_seed(0)\nN, D = 2000, 2\nX = np.random.randn(N, D)\nw_true = np.array([2.0, -3.0])\nb_true = 0.5\nlogits = X.dot(w_true) + b_true\nprobs = 1 / (1 + np.exp(-logits))\ny = (np.random.rand(N) < probs).astype(int)\n\n# Train/val/test split\nidx = np.random.permutation(N)\ntrain_idx, val_idx, test_idx = idx[:1200], idx[1200:1500], idx[1500:]\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\n# Normalize features\nmean, std = X_train.mean(0), X_train.std(0) + 1e-6\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\n# Pre-stack for teacher inference\nX_all_tensor = torch.from_numpy(np.vstack([X_train, X_val, X_test])).float().to(device)\nn_tr, n_val, n_test = len(X_train), len(X_val), len(X_test)\n\n\n# Datasets\nclass SimpleDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"x\": self.X[i], \"y\": self.y[i]}\n\n\nclass UserDS(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X).float()\n        self.y = torch.from_numpy(y).long()\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"feat\": self.X[i], \"label\": self.y[i]}\n\n\n# Models\nclass AIModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass UserModel(nn.Module):\n    def __init__(self, inp_dim, hid_dim, out_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, out_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Hyperparameters\nai_batch_sizes = [16, 32, 64]\nusr_batch_sizes = [16, 32, 64]\nteacher_sizes = [1, 3, 5]\n\n# Container for all results\nexperiment_data = {\"teacher_ensemble_size\": {}}\n\nfor size in teacher_sizes:\n    experiment_data[\"teacher_ensemble_size\"][str(size)] = {}\n    for ai_bs in ai_batch_sizes:\n        # Collect teacher probabilities\n        p_tr_list, p_val_list, p_te_list = [], [], []\n        for t in range(size):\n            seed = t\n            np.random.seed(seed)\n            torch.manual_seed(seed)\n            # AI data loader\n            ai_tr_loader = DataLoader(\n                SimpleDS(X_train, y_train), batch_size=ai_bs, shuffle=True\n            )\n            # Train one teacher\n            ai_model = AIModel(D, 16, 2).to(device)\n            crit_ai = nn.CrossEntropyLoss()\n            opt_ai = optim.Adam(ai_model.parameters(), lr=1e-2)\n            for _ in range(15):\n                ai_model.train()\n                for b in ai_tr_loader:\n                    x, yb = b[\"x\"].to(device), b[\"y\"].to(device)\n                    out = ai_model(x)\n                    loss = crit_ai(out, yb)\n                    opt_ai.zero_grad()\n                    loss.backward()\n                    opt_ai.step()\n            # Inference\n            ai_model.eval()\n            with torch.no_grad():\n                logits_all = ai_model(X_all_tensor)\n                probs_all = torch.softmax(logits_all, dim=1).cpu().numpy()\n            p_tr_list.append(probs_all[:n_tr])\n            p_val_list.append(probs_all[n_tr : n_tr + n_val])\n            p_te_list.append(probs_all[-n_test:])\n        # Average ensembled probabilities\n        p_train = np.mean(p_tr_list, axis=0)\n        p_val = np.mean(p_val_list, axis=0)\n        p_test = np.mean(p_te_list, axis=0)\n        # Pseudo\u2010labels\n        f_train = p_train.argmax(axis=1)\n        f_val = p_val.argmax(axis=1)\n        f_test = p_test.argmax(axis=1)\n        # User features\n        X_usr_train = np.hstack([X_train, p_train])\n        X_usr_val = np.hstack([X_val, p_val])\n        X_usr_test = np.hstack([X_test, p_test])\n\n        for usr_bs in usr_batch_sizes:\n            # Data loaders\n            usr_tr_loader = DataLoader(\n                UserDS(X_usr_train, f_train), batch_size=usr_bs, shuffle=True\n            )\n            usr_val_loader = DataLoader(UserDS(X_usr_val, f_val), batch_size=usr_bs)\n            usr_te_loader = DataLoader(UserDS(X_usr_test, f_test), batch_size=usr_bs)\n            # Init user model\n            user_model = UserModel(D + 2, 8, 2).to(device)\n            crit_usr = nn.CrossEntropyLoss()\n            opt_usr = optim.Adam(user_model.parameters(), lr=1e-2)\n            train_accs, val_accs = [], []\n            train_losses, val_losses = [], []\n            # Train user\n            for _ in range(20):\n                user_model.train()\n                t_loss, corr, tot = 0.0, 0, 0\n                for b in usr_tr_loader:\n                    feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                    out = user_model(feat)\n                    loss = crit_usr(out, lbl)\n                    opt_usr.zero_grad()\n                    loss.backward()\n                    opt_usr.step()\n                    t_loss += loss.item() * feat.size(0)\n                    preds = out.argmax(1)\n                    corr += (preds == lbl).sum().item()\n                    tot += lbl.size(0)\n                train_losses.append(t_loss / tot)\n                train_accs.append(corr / tot)\n                # Validation\n                user_model.eval()\n                v_loss, v_corr, v_tot = 0.0, 0, 0\n                with torch.no_grad():\n                    for b in usr_val_loader:\n                        feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                        out = user_model(feat)\n                        loss = crit_usr(out, lbl)\n                        v_loss += loss.item() * feat.size(0)\n                        preds = out.argmax(1)\n                        v_corr += (preds == lbl).sum().item()\n                        v_tot += lbl.size(0)\n                val_losses.append(v_loss / v_tot)\n                val_accs.append(v_corr / v_tot)\n            # Test evaluation\n            test_preds, test_gt = [], []\n            user_model.eval()\n            with torch.no_grad():\n                for b in usr_te_loader:\n                    feat, lbl = b[\"feat\"].to(device), b[\"label\"].to(device)\n                    out = user_model(feat)\n                    test_preds.extend(out.argmax(1).cpu().numpy().tolist())\n                    test_gt.extend(lbl.cpu().numpy().tolist())\n            # Record\n            ds_name = f\"ensemble_{size}_ai_bs_{ai_bs}_usr_bs_{usr_bs}\"\n            experiment_data[\"teacher_ensemble_size\"][str(size)][ds_name] = {\n                \"metrics\": {\"train\": np.array(train_accs), \"val\": np.array(val_accs)},\n                \"losses\": {\n                    \"train\": np.array(train_losses),\n                    \"val\": np.array(val_losses),\n                },\n                \"predictions\": np.array(test_preds),\n                \"ground_truth\": np.array(test_gt),\n            }\n\n# Save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Execution time: 15 seconds seconds (time limit is an hour).']", "['Execution time: 41 seconds seconds (time limit is an hour).']", "['Execution time: 26 seconds seconds (time limit is an hour).']", "['Execution time: 28 seconds seconds (time limit is an hour).']", "['Execution time: 6 seconds seconds (time limit is an hour).']", "['Execution time: 26 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n',\n'/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/numpy/lib/_polynomial_impl.py:657: RuntimeWarning: invalid value\nencountered in divide\\n  lhs /= scale\\n', 'Traceback (most recent call last):\\n\nFile \"runfile.py\", line 280, in <module>\\n    rate =\nnp.polyfit(np.arange(len(align_hist)), align_hist, 1)[0]\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/numpy/lib/_polynomial_impl.py\", line 658, in polyfit\\n    c, resids,\nrank, s = lstsq(lhs, rhs, rcond)\\n\n^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/numpy/linalg/_linalg.py\", line 2508, in lstsq\\n    x, resids, rank, s =\ngufunc(a, b, rcond, signature=signature)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/numpy/linalg/_linalg.py\", line 116, in _raise_linalgerror_lstsq\\n\nraise LinAlgError(\"SVD did not converge in Linear Least\nSquares\")\\nnumpy.linalg.LinAlgError: SVD did not converge in Linear Least\nSquares\\n', 'Execution time: 2 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 1:\nvalidation_loss = 0.1436', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 2:\nvalidation_loss = 0.0677', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 3:\nvalidation_loss = 0.0495', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 4:\nvalidation_loss = 0.0384', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 5:\nvalidation_loss = 0.0335', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 6:\nvalidation_loss = 0.0291', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 7:\nvalidation_loss = 0.0264', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 8:\nvalidation_loss = 0.0243', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 9:\nvalidation_loss = 0.0232', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 10:\nvalidation_loss = 0.0207', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 11:\nvalidation_loss = 0.0209', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 12:\nvalidation_loss = 0.0210', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 13:\nvalidation_loss = 0.0169', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 14:\nvalidation_loss = 0.0175', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 15:\nvalidation_loss = 0.0192', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 16:\nvalidation_loss = 0.0163', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 17:\nvalidation_loss = 0.0138', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 18:\nvalidation_loss = 0.0165', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 19:\nvalidation_loss = 0.0128', '\\n', 'CE_hard_labels ai_bs_16_user_bs_16 Epoch 20:\nvalidation_loss = 0.0137', '\\n', 'soft_label_distillation ai_bs_16_user_bs_16\nEpoch 1: validation_loss = 0.0056', '\\n', 'soft_label_distillation\nai_bs_16_user_bs_16 Epoch 2: validation_loss = 0.0025', '\\n',\n'soft_label_distillation ai_bs_16_user_bs_16 Epoch 3: validation_loss = 0.0016',\n'\\n', 'soft_label_distillation ai_bs_16_user_bs_16 Epoch 4: validation_loss =\n0.0009', '\\n', 'soft_label_distillation ai_bs_16_user_bs_16 Epoch 5:\nvalidation_loss = 0.0008', '\\n', 'soft_label_distillation ai_bs_16_user_bs_16\nEpoch 6: validation_loss = 0.0007', '\\n', 'soft_label_distillation\nai_bs_16_user_bs_16 Epoch 7: validation_loss = 0.0007', '\\n',\n'soft_label_distillation ai_bs_16_user_bs_16 Epoch 8: validation_loss = 0.0008',\n'\\n', 'soft_label_distillation ai_bs_16_user_bs_16 Epoch 9: validation_loss =\n0.0008', '\\n', 'soft_label_distillation ai_bs_16_user_bs_16 Epoch 10:\nvalidation_loss = 0.0006', '\\n', 'soft_label_distillation ai_bs_16_user_bs_16\nEpoch 11: validation_loss = 0.0006', '\\n', 'soft_label_distillation\nai_bs_16_user_bs_16 Epoch 12: validation_loss = 0.0005', '\\n',\n'soft_label_distillation ai_bs_16_user_bs_16 Epoch 13: validation_loss =\n0.0005', '\\n', 'soft_label_distillation ai_bs_16_user_bs_16 Epoch 14:\nvalidation_loss = 0.0008', '\\n', 'soft_label_distillation ai_bs_16_user_bs_16\nEpoch 15: validation_loss = 0.0005', '\\n', 'soft_label_distillation\nai_bs_16_user_bs_16 Epoch 16: validation_loss = 0.0008', '\\n',\n'soft_label_distillation ai_bs_16_user_bs_16 Epoch 17: validation_loss =\n0.0005', '\\n', 'soft_label_distillation ai_bs_16_user_bs_16 Epoch 18:\nvalidation_loss = 0.0005', '\\n', 'soft_label_distillation ai_bs_16_user_bs_16\nEpoch 19: validation_loss = 0.0008', '\\n', 'soft_label_distillation\nai_bs_16_user_bs_16 Epoch 20: validation_loss = 0.0006', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 1: validation_loss = 0.1139', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 2: validation_loss = 0.0602', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 3: validation_loss = 0.0448', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 4: validation_loss = 0.0374', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 5: validation_loss = 0.0322', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 6: validation_loss = 0.0284', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 7: validation_loss = 0.0255', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 8: validation_loss = 0.0231', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 9: validation_loss = 0.0245', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 10: validation_loss = 0.0237', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 11: validation_loss = 0.0192', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 12: validation_loss = 0.0224', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 13: validation_loss = 0.0179', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 14: validation_loss = 0.0172', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 15: validation_loss = 0.0152', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 16: validation_loss = 0.0155', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 17: validation_loss = 0.0145', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 18: validation_loss = 0.0168', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 19: validation_loss = 0.0130', '\\n', 'bias_awareness\nai_bs_16_user_bs_16 Epoch 20: validation_loss = 0.0152', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 1: validation_loss = 0.0047', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 2: validation_loss = 0.0025', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 3: validation_loss = 0.0014', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 4: validation_loss = 0.0006', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 5: validation_loss = 0.0004', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 6: validation_loss = 0.0003', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 7: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 8: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 9: validation_loss = 0.0003', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 10: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 11: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 12: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 13: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 14: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 15: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 16: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 17: validation_loss = 0.0001', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 18: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 19: validation_loss = 0.0001', '\\n', 'dual_channel\nai_bs_16_user_bs_16 Epoch 20: validation_loss = 0.0001', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 1: validation_loss = 0.2361', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 2: validation_loss = 0.0917', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 3: validation_loss = 0.0600', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 4: validation_loss = 0.0476', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 5: validation_loss = 0.0415', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 6: validation_loss = 0.0380', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 7: validation_loss = 0.0335', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 8: validation_loss = 0.0305', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 9: validation_loss = 0.0294', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 10: validation_loss = 0.0266', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 11: validation_loss = 0.0258', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 12: validation_loss = 0.0239', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 13: validation_loss = 0.0226', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 14: validation_loss = 0.0218', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 15: validation_loss = 0.0206', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 16: validation_loss = 0.0208', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 17: validation_loss = 0.0203', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 18: validation_loss = 0.0266', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 19: validation_loss = 0.0183', '\\n', 'CE_hard_labels\nai_bs_16_user_bs_32 Epoch 20: validation_loss = 0.0169', '\\n',\n'soft_label_distillation ai_bs_16_user_bs_32 Epoch 1: validation_loss = 0.0085',\n'\\n', 'soft_label_distillation ai_bs_16_user_bs_32 Epoch 2: validation_loss =\n0.0033', '\\n', 'soft_label_distillation ai_bs_16_user_bs_32 Epoch 3:\nvalidation_loss = 0.0015', '\\n', 'soft_label_distillation ai_bs_16_user_bs_32\nEpoch 4: validation_loss = 0.0009', '\\n', 'soft_label_distillation\nai_bs_16_user_bs_32 Epoch 5: validation_loss = 0.0007', '\\n',\n'soft_label_distillation ai_bs_16_user_bs_32 Epoch 6: validation_loss = 0.0006',\n'\\n', 'soft_label_distillation ai_bs_16_user_bs_32 Epoch 7: validation_loss =\n0.0006', '\\n', 'soft_label_distillation ai_bs_16_user_bs_32 Epoch 8:\nvalidation_loss = 0.0006', '\\n', 'soft_label_distillation ai_bs_16_user_bs_32\nEpoch 9: validation_loss = 0.0005', '\\n', 'soft_label_distillation\nai_bs_16_user_bs_32 Epoch 10: validation_loss = 0.0005', '\\n',\n'soft_label_distillation ai_bs_16_user_bs_32 Epoch 11: validation_loss =\n0.0006', '\\n', 'soft_label_distillation ai_bs_16_user_bs_32 Epoch 12:\nvalidation_loss = 0.0005', '\\n', 'soft_label_distillation ai_bs_16_user_bs_32\nEpoch 13: validation_loss = 0.0005', '\\n', 'soft_label_distillation\nai_bs_16_user_bs_32 Epoch 14: validation_loss = 0.0005', '\\n',\n'soft_label_distillation ai_bs_16_user_bs_32 Epoch 15: validation_loss =\n0.0005', '\\n', 'soft_label_distillation ai_bs_16_user_bs_32 Epoch 16:\nvalidation_loss = 0.0005', '\\n', 'soft_label_distillation ai_bs_16_user_bs_32\nEpoch 17: validation_loss = 0.0006', '\\n', 'soft_label_distillation\nai_bs_16_user_bs_32 Epoch 18: validation_loss = 0.0005', '\\n',\n'soft_label_distillation ai_bs_16_user_bs_32 Epoch 19: validation_loss =\n0.0005', '\\n', 'soft_label_distillation ai_bs_16_user_bs_32 Epoch 20:\nvalidation_loss = 0.0005', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 1:\nvalidation_loss = 0.2098', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 2:\nvalidation_loss = 0.0985', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 3:\nvalidation_loss = 0.0720', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 4:\nvalidation_loss = 0.0605', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 5:\nvalidation_loss = 0.0509', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 6:\nvalidation_loss = 0.0447', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 7:\nvalidation_loss = 0.0406', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 8:\nvalidation_loss = 0.0394', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 9:\nvalidation_loss = 0.0334', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 10:\nvalidation_loss = 0.0313', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 11:\nvalidation_loss = 0.0284', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 12:\nvalidation_loss = 0.0296', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 13:\nvalidation_loss = 0.0267', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 14:\nvalidation_loss = 0.0240', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 15:\nvalidation_loss = 0.0261', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 16:\nvalidation_loss = 0.0214', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 17:\nvalidation_loss = 0.0204', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 18:\nvalidation_loss = 0.0197', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 19:\nvalidation_loss = 0.0191', '\\n', 'bias_awareness ai_bs_16_user_bs_32 Epoch 20:\nvalidation_loss = 0.0204', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 1:\nvalidation_loss = 0.0046', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 2:\nvalidation_loss = 0.0020', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 3:\nvalidation_loss = 0.0010', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 4:\nvalidation_loss = 0.0006', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 5:\nvalidation_loss = 0.0004', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 6:\nvalidation_loss = 0.0003', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 7:\nvalidation_loss = 0.0003', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 8:\nvalidation_loss = 0.0002', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 9:\nvalidation_loss = 0.0002', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 10:\nvalidation_loss = 0.0002', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 11:\nvalidation_loss = 0.0002', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 12:\nvalidation_loss = 0.0002', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 13:\nvalidation_loss = 0.0002', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 14:\nvalidation_loss = 0.0002', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 15:\nvalidation_loss = 0.0002', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 16:\nvalidation_loss = 0.0002', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 17:\nvalidation_loss = 0.0002', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 18:\nvalidation_loss = 0.0002', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 19:\nvalidation_loss = 0.0002', '\\n', 'dual_channel ai_bs_16_user_bs_32 Epoch 20:\nvalidation_loss = 0.0002', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 1:\nvalidation_loss = 0.5904', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 2:\nvalidation_loss = 0.4315', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 3:\nvalidation_loss = 0.2721', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 4:\nvalidation_loss = 0.1709', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 5:\nvalidation_loss = 0.1216', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 6:\nvalidation_loss = 0.0965', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 7:\nvalidation_loss = 0.0814', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 8:\nvalidation_loss = 0.0712', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 9:\nvalidation_loss = 0.0648', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 10:\nvalidation_loss = 0.0590', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 11:\nvalidation_loss = 0.0542', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 12:\nvalidation_loss = 0.0509', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 13:\nvalidation_loss = 0.0476', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 14:\nvalidation_loss = 0.0450', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 15:\nvalidation_loss = 0.0428', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 16:\nvalidation_loss = 0.0412', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 17:\nvalidation_loss = 0.0388', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 18:\nvalidation_loss = 0.0372', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 19:\nvalidation_loss = 0.0356', '\\n', 'CE_hard_labels ai_bs_16_user_bs_64 Epoch 20:\nvalidation_loss = 0.0344', '\\n', 'soft_label_distillation ai_bs_16_user_bs_64\nEpoch 1: validation_loss = 0.1596', '\\n', 'soft_label_distillation\nai_bs_16_user_bs_64 Epoch 2: validation_loss = 0.0067', '\\n',\n'soft_label_distillation ai_bs_16_user_bs_64 Epoch 3: validation_loss = 0.0070',\n'\\n', 'soft_label_distillation ai_bs_16_user_bs_64 Epoch 4: validation_loss =\n0.0032', '\\n', 'soft_label_distillation ai_bs_16_user_bs_64 Epoch 5:\nvalidation_loss = 0.0028', '\\n', 'soft_label_distillation ai_bs_16_user_bs_64\nEpoch 6: validation_loss = 0.0024', '\\n', 'soft_label_distillation\nai_bs_16_user_bs_64 Epoch 7: validation_loss = 0.0022', '\\n',\n'soft_label_distillation ai_bs_16_user_bs_64 Epoch 8: validation_loss = 0.0020',\n'\\n', 'soft_label_distillation ai_bs_16_user_bs_64 Epoch 9: validation_loss =\n0.0020', '\\n', 'soft_label_distillation ai_bs_16_user_bs_64 Epoch 10:\nvalidation_loss = 0.0016', '\\n', 'soft_label_distillation ai_bs_16_user_bs_64\nEpoch 11: validation_loss = 0.0015', '\\n', 'soft_label_distillation\nai_bs_16_user_bs_64 Epoch 12: validation_loss = 0.0014', '\\n',\n'soft_label_distillation ai_bs_16_user_bs_64 Epoch 13: validation_loss =\n0.0013', '\\n', 'soft_label_distillation ai_bs_16_user_bs_64 Epoch 14:\nvalidation_loss = 0.0012', '\\n', 'soft_label_distillation ai_bs_16_user_bs_64\nEpoch 15: validation_loss = 0.0010', '\\n', 'soft_label_distillation\nai_bs_16_user_bs_64 Epoch 16: validation_loss = 0.0010', '\\n',\n'soft_label_distillation ai_bs_16_user_bs_64 Epoch 17: validation_loss =\n0.0010', '\\n', 'soft_label_distillation ai_bs_16_user_bs_64 Epoch 18:\nvalidation_loss = 0.0009', '\\n', 'soft_label_distillation ai_bs_16_user_bs_64\nEpoch 19: validation_loss = 0.0008', '\\n', 'soft_label_distillation\nai_bs_16_user_bs_64 Epoch 20: validation_loss = 0.0008', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 1: validation_loss = 0.2715', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 2: validation_loss = 0.1243', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 3: validation_loss = 0.0815', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 4: validation_loss = 0.0617', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 5: validation_loss = 0.0523', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 6: validation_loss = 0.0460', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 7: validation_loss = 0.0425', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 8: validation_loss = 0.0391', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 9: validation_loss = 0.0365', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 10: validation_loss = 0.0337', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 11: validation_loss = 0.0314', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 12: validation_loss = 0.0306', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 13: validation_loss = 0.0284', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 14: validation_loss = 0.0276', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 15: validation_loss = 0.0262', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 16: validation_loss = 0.0255', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 17: validation_loss = 0.0239', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 18: validation_loss = 0.0239', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 19: validation_loss = 0.0222', '\\n', 'bias_awareness\nai_bs_16_user_bs_64 Epoch 20: validation_loss = 0.0217', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 1: validation_loss = 0.1698', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 2: validation_loss = 0.0096', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 3: validation_loss = 0.0062', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 4: validation_loss = 0.0031', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 5: validation_loss = 0.0027', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 6: validation_loss = 0.0021', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 7: validation_loss = 0.0017', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 8: validation_loss = 0.0013', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 9: validation_loss = 0.0010', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 10: validation_loss = 0.0008', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 11: validation_loss = 0.0006', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 12: validation_loss = 0.0005', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 13: validation_loss = 0.0004', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 14: validation_loss = 0.0003', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 15: validation_loss = 0.0003', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 16: validation_loss = 0.0003', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 17: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 18: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 19: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_16_user_bs_64 Epoch 20: validation_loss = 0.0002', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 1: validation_loss = 0.1538', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 2: validation_loss = 0.0634', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 3: validation_loss = 0.0477', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 4: validation_loss = 0.0388', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 5: validation_loss = 0.0377', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 6: validation_loss = 0.0272', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 7: validation_loss = 0.0267', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 8: validation_loss = 0.0244', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 9: validation_loss = 0.0414', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 10: validation_loss = 0.0246', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 11: validation_loss = 0.0226', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 12: validation_loss = 0.0245', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 13: validation_loss = 0.0198', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 14: validation_loss = 0.0228', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 15: validation_loss = 0.0170', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 16: validation_loss = 0.0197', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 17: validation_loss = 0.0223', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 18: validation_loss = 0.0267', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 19: validation_loss = 0.0155', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_16 Epoch 20: validation_loss = 0.0182', '\\n',\n'soft_label_distillation ai_bs_32_user_bs_16 Epoch 1: validation_loss = 0.0034',\n'\\n', 'soft_label_distillation ai_bs_32_user_bs_16 Epoch 2: validation_loss =\n0.0012', '\\n', 'soft_label_distillation ai_bs_32_user_bs_16 Epoch 3:\nvalidation_loss = 0.0007', '\\n', 'soft_label_distillation ai_bs_32_user_bs_16\nEpoch 4: validation_loss = 0.0005', '\\n', 'soft_label_distillation\nai_bs_32_user_bs_16 Epoch 5: validation_loss = 0.0005', '\\n',\n'soft_label_distillation ai_bs_32_user_bs_16 Epoch 6: validation_loss = 0.0005',\n'\\n', 'soft_label_distillation ai_bs_32_user_bs_16 Epoch 7: validation_loss =\n0.0004', '\\n', 'soft_label_distillation ai_bs_32_user_bs_16 Epoch 8:\nvalidation_loss = 0.0008', '\\n', 'soft_label_distillation ai_bs_32_user_bs_16\nEpoch 9: validation_loss = 0.0005', '\\n', 'soft_label_distillation\nai_bs_32_user_bs_16 Epoch 10: validation_loss = 0.0005', '\\n',\n'soft_label_distillation ai_bs_32_user_bs_16 Epoch 11: validation_loss =\n0.0004', '\\n', 'soft_label_distillation ai_bs_32_user_bs_16 Epoch 12:\nvalidation_loss = 0.0004', '\\n', 'soft_label_distillation ai_bs_32_user_bs_16\nEpoch 13: validation_loss = 0.0005', '\\n', 'soft_label_distillation\nai_bs_32_user_bs_16 Epoch 14: validation_loss = 0.0006', '\\n',\n'soft_label_distillation ai_bs_32_user_bs_16 Epoch 15: validation_loss =\n0.0012', '\\n', 'soft_label_distillation ai_bs_32_user_bs_16 Epoch 16:\nvalidation_loss = 0.0005', '\\n', 'soft_label_distillation ai_bs_32_user_bs_16\nEpoch 17: validation_loss = 0.0005', '\\n', 'soft_label_distillation\nai_bs_32_user_bs_16 Epoch 18: validation_loss = 0.0004', '\\n',\n'soft_label_distillation ai_bs_32_user_bs_16 Epoch 19: validation_loss =\n0.0011', '\\n', 'soft_label_distillation ai_bs_32_user_bs_16 Epoch 20:\nvalidation_loss = 0.0005', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 1:\nvalidation_loss = 0.1057', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 2:\nvalidation_loss = 0.0587', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 3:\nvalidation_loss = 0.0476', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 4:\nvalidation_loss = 0.0370', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 5:\nvalidation_loss = 0.0344', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 6:\nvalidation_loss = 0.0279', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 7:\nvalidation_loss = 0.0280', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 8:\nvalidation_loss = 0.0276', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 9:\nvalidation_loss = 0.0264', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 10:\nvalidation_loss = 0.0246', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 11:\nvalidation_loss = 0.0256', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 12:\nvalidation_loss = 0.0208', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 13:\nvalidation_loss = 0.0192', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 14:\nvalidation_loss = 0.0179', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 15:\nvalidation_loss = 0.0194', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 16:\nvalidation_loss = 0.0160', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 17:\nvalidation_loss = 0.0163', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 18:\nvalidation_loss = 0.0166', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 19:\nvalidation_loss = 0.0149', '\\n', 'bias_awareness ai_bs_32_user_bs_16 Epoch 20:\nvalidation_loss = 0.0144', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 1:\nvalidation_loss = 0.0034', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 2:\nvalidation_loss = 0.0013', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 3:\nvalidation_loss = 0.0007', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 4:\nvalidation_loss = 0.0004', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 5:\nvalidation_loss = 0.0003', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 6:\nvalidation_loss = 0.0002', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 7:\nvalidation_loss = 0.0001', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 8:\nvalidation_loss = 0.0001', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 9:\nvalidation_loss = 0.0001', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 10:\nvalidation_loss = 0.0001', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 11:\nvalidation_loss = 0.0001', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 12:\nvalidation_loss = 0.0001', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 13:\nvalidation_loss = 0.0001', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 14:\nvalidation_loss = 0.0001', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 15:\nvalidation_loss = 0.0001', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 16:\nvalidation_loss = 0.0001', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 17:\nvalidation_loss = 0.0001', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 18:\nvalidation_loss = 0.0001', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 19:\nvalidation_loss = 0.0001', '\\n', 'dual_channel ai_bs_32_user_bs_16 Epoch 20:\nvalidation_loss = 0.0001', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 1:\nvalidation_loss = 0.2273', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 2:\nvalidation_loss = 0.0946', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 3:\nvalidation_loss = 0.0655', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 4:\nvalidation_loss = 0.0551', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 5:\nvalidation_loss = 0.0467', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 6:\nvalidation_loss = 0.0395', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 7:\nvalidation_loss = 0.0426', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 8:\nvalidation_loss = 0.0390', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 9:\nvalidation_loss = 0.0310', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 10:\nvalidation_loss = 0.0305', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 11:\nvalidation_loss = 0.0275', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 12:\nvalidation_loss = 0.0296', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 13:\nvalidation_loss = 0.0229', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 14:\nvalidation_loss = 0.0260', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 15:\nvalidation_loss = 0.0247', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 16:\nvalidation_loss = 0.0260', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 17:\nvalidation_loss = 0.0256', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 18:\nvalidation_loss = 0.0226', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 19:\nvalidation_loss = 0.0222', '\\n', 'CE_hard_labels ai_bs_32_user_bs_32 Epoch 20:\nvalidation_loss = 0.0208', '\\n', 'soft_label_distillation ai_bs_32_user_bs_32\nEpoch 1: validation_loss = 0.0570', '\\n', 'soft_label_distillation\nai_bs_32_user_bs_32 Epoch 2: validation_loss = 0.0064', '\\n',\n'soft_label_distillation ai_bs_32_user_bs_32 Epoch 3: validation_loss = 0.0034',\n'\\n', 'soft_label_distillation ai_bs_32_user_bs_32 Epoch 4: validation_loss =\n0.0023', '\\n', 'soft_label_distillation ai_bs_32_user_bs_32 Epoch 5:\nvalidation_loss = 0.0016', '\\n', 'soft_label_distillation ai_bs_32_user_bs_32\nEpoch 6: validation_loss = 0.0011', '\\n', 'soft_label_distillation\nai_bs_32_user_bs_32 Epoch 7: validation_loss = 0.0008', '\\n',\n'soft_label_distillation ai_bs_32_user_bs_32 Epoch 8: validation_loss = 0.0008',\n'\\n', 'soft_label_distillation ai_bs_32_user_bs_32 Epoch 9: validation_loss =\n0.0007', '\\n', 'soft_label_distillation ai_bs_32_user_bs_32 Epoch 10:\nvalidation_loss = 0.0006', '\\n', 'soft_label_distillation ai_bs_32_user_bs_32\nEpoch 11: validation_loss = 0.0007', '\\n', 'soft_label_distillation\nai_bs_32_user_bs_32 Epoch 12: validation_loss = 0.0006', '\\n',\n'soft_label_distillation ai_bs_32_user_bs_32 Epoch 13: validation_loss =\n0.0006', '\\n', 'soft_label_distillation ai_bs_32_user_bs_32 Epoch 14:\nvalidation_loss = 0.0006', '\\n', 'soft_label_distillation ai_bs_32_user_bs_32\nEpoch 15: validation_loss = 0.0006', '\\n', 'soft_label_distillation\nai_bs_32_user_bs_32 Epoch 16: validation_loss = 0.0005', '\\n',\n'soft_label_distillation ai_bs_32_user_bs_32 Epoch 17: validation_loss =\n0.0005', '\\n', 'soft_label_distillation ai_bs_32_user_bs_32 Epoch 18:\nvalidation_loss = 0.0006', '\\n', 'soft_label_distillation ai_bs_32_user_bs_32\nEpoch 19: validation_loss = 0.0006', '\\n', 'soft_label_distillation\nai_bs_32_user_bs_32 Epoch 20: validation_loss = 0.0005', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 1: validation_loss = 0.2008', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 2: validation_loss = 0.0869', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 3: validation_loss = 0.0608', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 4: validation_loss = 0.0438', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 5: validation_loss = 0.0359', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 6: validation_loss = 0.0402', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 7: validation_loss = 0.0297', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 8: validation_loss = 0.0274', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 9: validation_loss = 0.0239', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 10: validation_loss = 0.0392', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 11: validation_loss = 0.0245', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 12: validation_loss = 0.0251', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 13: validation_loss = 0.0199', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 14: validation_loss = 0.0195', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 15: validation_loss = 0.0233', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 16: validation_loss = 0.0194', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 17: validation_loss = 0.0176', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 18: validation_loss = 0.0174', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 19: validation_loss = 0.0196', '\\n', 'bias_awareness\nai_bs_32_user_bs_32 Epoch 20: validation_loss = 0.0189', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 1: validation_loss = 0.0368', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 2: validation_loss = 0.0034', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 3: validation_loss = 0.0010', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 4: validation_loss = 0.0005', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 5: validation_loss = 0.0003', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 6: validation_loss = 0.0003', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 7: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 8: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 9: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 10: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 11: validation_loss = 0.0001', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 12: validation_loss = 0.0001', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 13: validation_loss = 0.0001', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 14: validation_loss = 0.0001', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 15: validation_loss = 0.0001', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 16: validation_loss = 0.0001', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 17: validation_loss = 0.0001', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 18: validation_loss = 0.0001', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 19: validation_loss = 0.0001', '\\n', 'dual_channel\nai_bs_32_user_bs_32 Epoch 20: validation_loss = 0.0001', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 1: validation_loss = 0.5697', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 2: validation_loss = 0.3637', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 3: validation_loss = 0.2040', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 4: validation_loss = 0.1299', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 5: validation_loss = 0.0935', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 6: validation_loss = 0.0749', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 7: validation_loss = 0.0632', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 8: validation_loss = 0.0557', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 9: validation_loss = 0.0514', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 10: validation_loss = 0.0467', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 11: validation_loss = 0.0451', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 12: validation_loss = 0.0403', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 13: validation_loss = 0.0386', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 14: validation_loss = 0.0380', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 15: validation_loss = 0.0346', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 16: validation_loss = 0.0327', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 17: validation_loss = 0.0334', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 18: validation_loss = 0.0307', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 19: validation_loss = 0.0303', '\\n', 'CE_hard_labels\nai_bs_32_user_bs_64 Epoch 20: validation_loss = 0.0281', '\\n',\n'soft_label_distillation ai_bs_32_user_bs_64 Epoch 1: validation_loss = 0.1917',\n'\\n', 'soft_label_distillation ai_bs_32_user_bs_64 Epoch 2: validation_loss =\n0.0169', '\\n', 'soft_label_distillation ai_bs_32_user_bs_64 Epoch 3:\nvalidation_loss = 0.0045', '\\n', 'soft_label_distillation ai_bs_32_user_bs_64\nEpoch 4: validation_loss = 0.0022', '\\n', 'soft_label_distillation\nai_bs_32_user_bs_64 Epoch 5: validation_loss = 0.0014', '\\n',\n'soft_label_distillation ai_bs_32_user_bs_64 Epoch 6: validation_loss = 0.0010',\n'\\n', 'soft_label_distillation ai_bs_32_user_bs_64 Epoch 7: validation_loss =\n0.0009', '\\n', 'soft_label_distillation ai_bs_32_user_bs_64 Epoch 8:\nvalidation_loss = 0.0008', '\\n', 'soft_label_distillation ai_bs_32_user_bs_64\nEpoch 9: validation_loss = 0.0007', '\\n', 'soft_label_distillation\nai_bs_32_user_bs_64 Epoch 10: validation_loss = 0.0007', '\\n',\n'soft_label_distillation ai_bs_32_user_bs_64 Epoch 11: validation_loss =\n0.0007', '\\n', 'soft_label_distillation ai_bs_32_user_bs_64 Epoch 12:\nvalidation_loss = 0.0007', '\\n', 'soft_label_distillation ai_bs_32_user_bs_64\nEpoch 13: validation_loss = 0.0006', '\\n', 'soft_label_distillation\nai_bs_32_user_bs_64 Epoch 14: validation_loss = 0.0006', '\\n',\n'soft_label_distillation ai_bs_32_user_bs_64 Epoch 15: validation_loss =\n0.0005', '\\n', 'soft_label_distillation ai_bs_32_user_bs_64 Epoch 16:\nvalidation_loss = 0.0006', '\\n', 'soft_label_distillation ai_bs_32_user_bs_64\nEpoch 17: validation_loss = 0.0005', '\\n', 'soft_label_distillation\nai_bs_32_user_bs_64 Epoch 18: validation_loss = 0.0005', '\\n',\n'soft_label_distillation ai_bs_32_user_bs_64 Epoch 19: validation_loss =\n0.0005', '\\n', 'soft_label_distillation ai_bs_32_user_bs_64 Epoch 20:\nvalidation_loss = 0.0004', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 1:\nvalidation_loss = 0.4450', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 2:\nvalidation_loss = 0.2142', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 3:\nvalidation_loss = 0.1306', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 4:\nvalidation_loss = 0.0978', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 5:\nvalidation_loss = 0.0813', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 6:\nvalidation_loss = 0.0684', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 7:\nvalidation_loss = 0.0599', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 8:\nvalidation_loss = 0.0543', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 9:\nvalidation_loss = 0.0508', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 10:\nvalidation_loss = 0.0457', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 11:\nvalidation_loss = 0.0429', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 12:\nvalidation_loss = 0.0411', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 13:\nvalidation_loss = 0.0368', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 14:\nvalidation_loss = 0.0365', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 15:\nvalidation_loss = 0.0333', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 16:\nvalidation_loss = 0.0343', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 17:\nvalidation_loss = 0.0305', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 18:\nvalidation_loss = 0.0303', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 19:\nvalidation_loss = 0.0291', '\\n', 'bias_awareness ai_bs_32_user_bs_64 Epoch 20:\nvalidation_loss = 0.0281', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 1:\nvalidation_loss = 0.1600', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 2:\nvalidation_loss = 0.0128', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 3:\nvalidation_loss = 0.0044', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 4:\nvalidation_loss = 0.0023', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 5:\nvalidation_loss = 0.0019', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 6:\nvalidation_loss = 0.0015', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 7:\nvalidation_loss = 0.0013', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 8:\nvalidation_loss = 0.0012', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 9:\nvalidation_loss = 0.0011', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 10:\nvalidation_loss = 0.0010', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 11:\nvalidation_loss = 0.0009', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 12:\nvalidation_loss = 0.0008', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 13:\nvalidation_loss = 0.0008', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 14:\nvalidation_loss = 0.0007', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 15:\nvalidation_loss = 0.0007', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 16:\nvalidation_loss = 0.0007', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 17:\nvalidation_loss = 0.0007', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 18:\nvalidation_loss = 0.0006', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 19:\nvalidation_loss = 0.0006', '\\n', 'dual_channel ai_bs_32_user_bs_64 Epoch 20:\nvalidation_loss = 0.0005', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 1:\nvalidation_loss = 0.1152', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 2:\nvalidation_loss = 0.0624', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 3:\nvalidation_loss = 0.0440', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 4:\nvalidation_loss = 0.0460', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 5:\nvalidation_loss = 0.0315', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 6:\nvalidation_loss = 0.0326', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 7:\nvalidation_loss = 0.0325', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 8:\nvalidation_loss = 0.0253', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 9:\nvalidation_loss = 0.0252', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 10:\nvalidation_loss = 0.0201', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 11:\nvalidation_loss = 0.0224', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 12:\nvalidation_loss = 0.0265', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 13:\nvalidation_loss = 0.0183', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 14:\nvalidation_loss = 0.0183', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 15:\nvalidation_loss = 0.0217', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 16:\nvalidation_loss = 0.0215', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 17:\nvalidation_loss = 0.0202', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 18:\nvalidation_loss = 0.0161', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 19:\nvalidation_loss = 0.0195', '\\n', 'CE_hard_labels ai_bs_64_user_bs_16 Epoch 20:\nvalidation_loss = 0.0234', '\\n', 'soft_label_distillation ai_bs_64_user_bs_16\nEpoch 1: validation_loss = 0.0040', '\\n', 'soft_label_distillation\nai_bs_64_user_bs_16 Epoch 2: validation_loss = 0.0017', '\\n',\n'soft_label_distillation ai_bs_64_user_bs_16 Epoch 3: validation_loss = 0.0010',\n'\\n', 'soft_label_distillation ai_bs_64_user_bs_16 Epoch 4: validation_loss =\n0.0007', '\\n', 'soft_label_distillation ai_bs_64_user_bs_16 Epoch 5:\nvalidation_loss = 0.0006', '\\n', 'soft_label_distillation ai_bs_64_user_bs_16\nEpoch 6: validation_loss = 0.0005', '\\n', 'soft_label_distillation\nai_bs_64_user_bs_16 Epoch 7: validation_loss = 0.0005', '\\n',\n'soft_label_distillation ai_bs_64_user_bs_16 Epoch 8: validation_loss = 0.0004',\n'\\n', 'soft_label_distillation ai_bs_64_user_bs_16 Epoch 9: validation_loss =\n0.0005', '\\n', 'soft_label_distillation ai_bs_64_user_bs_16 Epoch 10:\nvalidation_loss = 0.0004', '\\n', 'soft_label_distillation ai_bs_64_user_bs_16\nEpoch 11: validation_loss = 0.0005', '\\n', 'soft_label_distillation\nai_bs_64_user_bs_16 Epoch 12: validation_loss = 0.0004', '\\n',\n'soft_label_distillation ai_bs_64_user_bs_16 Epoch 13: validation_loss =\n0.0004', '\\n', 'soft_label_distillation ai_bs_64_user_bs_16 Epoch 14:\nvalidation_loss = 0.0004', '\\n', 'soft_label_distillation ai_bs_64_user_bs_16\nEpoch 15: validation_loss = 0.0005', '\\n', 'soft_label_distillation\nai_bs_64_user_bs_16 Epoch 16: validation_loss = 0.0003', '\\n',\n'soft_label_distillation ai_bs_64_user_bs_16 Epoch 17: validation_loss =\n0.0005', '\\n', 'soft_label_distillation ai_bs_64_user_bs_16 Epoch 18:\nvalidation_loss = 0.0004', '\\n', 'soft_label_distillation ai_bs_64_user_bs_16\nEpoch 19: validation_loss = 0.0003', '\\n', 'soft_label_distillation\nai_bs_64_user_bs_16 Epoch 20: validation_loss = 0.0004', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 1: validation_loss = 0.1373', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 2: validation_loss = 0.0649', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 3: validation_loss = 0.0498', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 4: validation_loss = 0.0396', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 5: validation_loss = 0.0337', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 6: validation_loss = 0.0309', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 7: validation_loss = 0.0317', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 8: validation_loss = 0.0272', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 9: validation_loss = 0.0241', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 10: validation_loss = 0.0311', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 11: validation_loss = 0.0281', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 12: validation_loss = 0.0274', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 13: validation_loss = 0.0242', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 14: validation_loss = 0.0223', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 15: validation_loss = 0.0315', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 16: validation_loss = 0.0302', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 17: validation_loss = 0.0235', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 18: validation_loss = 0.0265', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 19: validation_loss = 0.0224', '\\n', 'bias_awareness\nai_bs_64_user_bs_16 Epoch 20: validation_loss = 0.0274', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 1: validation_loss = 0.0040', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 2: validation_loss = 0.0016', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 3: validation_loss = 0.0011', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 4: validation_loss = 0.0010', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 5: validation_loss = 0.0008', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 6: validation_loss = 0.0006', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 7: validation_loss = 0.0006', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 8: validation_loss = 0.0005', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 9: validation_loss = 0.0004', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 10: validation_loss = 0.0003', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 11: validation_loss = 0.0004', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 12: validation_loss = 0.0003', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 13: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 14: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 15: validation_loss = 0.0002', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 16: validation_loss = 0.0001', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 17: validation_loss = 0.0001', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 18: validation_loss = 0.0001', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 19: validation_loss = 0.0001', '\\n', 'dual_channel\nai_bs_64_user_bs_16 Epoch 20: validation_loss = 0.0002', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 1: validation_loss = 0.2800', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 2: validation_loss = 0.1005', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 3: validation_loss = 0.0649', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 4: validation_loss = 0.0478', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 5: validation_loss = 0.0424', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 6: validation_loss = 0.0368', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 7: validation_loss = 0.0329', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 8: validation_loss = 0.0312', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 9: validation_loss = 0.0303', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 10: validation_loss = 0.0273', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 11: validation_loss = 0.0265', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 12: validation_loss = 0.0254', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 13: validation_loss = 0.0263', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 14: validation_loss = 0.0256', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 15: validation_loss = 0.0210', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 16: validation_loss = 0.0223', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 17: validation_loss = 0.0204', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 18: validation_loss = 0.0200', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 19: validation_loss = 0.0194', '\\n', 'CE_hard_labels\nai_bs_64_user_bs_32 Epoch 20: validation_loss = 0.0207', '\\n',\n'soft_label_distillation ai_bs_64_user_bs_32 Epoch 1: validation_loss = 0.0634',\n'\\n', 'soft_label_distillation ai_bs_64_user_bs_32 Epoch 2: validation_loss =\n0.0086', '\\n', 'soft_label_distillation ai_bs_64_user_bs_32 Epoch 3:\nvalidation_loss = 0.0042', '\\n', 'soft_label_distillation ai_bs_64_user_bs_32\nEpoch 4: validation_loss = 0.0023', '\\n', 'soft_label_distillation\nai_bs_64_user_bs_32 Epoch 5: validation_loss = 0.0015', '\\n',\n'soft_label_distillation ai_bs_64_user_bs_32 Epoch 6: validation_loss = 0.0011',\n'\\n', 'soft_label_distillation ai_bs_64_user_bs_32 Epoch 7: validation_loss =\n0.0009', '\\n', 'soft_label_distillation ai_bs_64_user_bs_32 Epoch 8:\nvalidation_loss = 0.0008', '\\n', 'soft_label_distillation ai_bs_64_user_bs_32\nEpoch 9: validation_loss = 0.0007', '\\n', 'soft_label_distillation\nai_bs_64_user_bs_32 Epoch 10: validation_loss = 0.0006', '\\n',\n'soft_label_distillation ai_bs_64_user_bs_32 Epoch 11: validation_loss =\n0.0006', '\\n', 'soft_label_distillation ai_bs_64_user_bs_32 Epoch 12:\nvalidation_loss = 0.0005', '\\n', 'soft_label_distillation ai_bs_64_user_bs_32\nEpoch 13: validation_loss = 0.0005', '\\n', 'soft_label_distillation\nai_bs_64_user_bs_32 Epoch 14: validation_loss = 0.0005', '\\n',\n'soft_label_distillation ai_bs_64_user_bs_32 Epoch 15: validation_loss =\n0.0007', '\\n', 'soft_label_distillation ai_bs_64_user_bs_32 Epoch 16:\nvalidation_loss = 0.0006', '\\n', 'soft_label_distillation ai_bs_64_user_bs_32\nEpoch 17: validation_loss = 0.0005', '\\n', 'soft_label_distillation\nai_bs_64_user_bs_32 Epoch 18: validation_loss = 0.0005', '\\n',\n'soft_label_distillation ai_bs_64_user_bs_32 Epoch 19: validation_loss =\n0.0006', '\\n', 'soft_label_distillation ai_bs_64_user_bs_32 Epoch 20:\nvalidation_loss = 0.0005', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 1:\nvalidation_loss = 0.2519', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 2:\nvalidation_loss = 0.1007', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 3:\nvalidation_loss = 0.0698', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 4:\nvalidation_loss = 0.0511', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 5:\nvalidation_loss = 0.0445', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 6:\nvalidation_loss = 0.0392', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 7:\nvalidation_loss = 0.0370', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 8:\nvalidation_loss = 0.0326', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 9:\nvalidation_loss = 0.0332', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 10:\nvalidation_loss = 0.0322', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 11:\nvalidation_loss = 0.0269', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 12:\nvalidation_loss = 0.0291', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 13:\nvalidation_loss = 0.0262', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 14:\nvalidation_loss = 0.0226', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 15:\nvalidation_loss = 0.0237', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 16:\nvalidation_loss = 0.0254', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 17:\nvalidation_loss = 0.0242', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 18:\nvalidation_loss = 0.0194', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 19:\nvalidation_loss = 0.0185', '\\n', 'bias_awareness ai_bs_64_user_bs_32 Epoch 20:\nvalidation_loss = 0.0187', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 1:\nvalidation_loss = 0.0055', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 2:\nvalidation_loss = 0.0027', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 3:\nvalidation_loss = 0.0015', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 4:\nvalidation_loss = 0.0010', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 5:\nvalidation_loss = 0.0008', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 6:\nvalidation_loss = 0.0007', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 7:\nvalidation_loss = 0.0006', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 8:\nvalidation_loss = 0.0006', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 9:\nvalidation_loss = 0.0005', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 10:\nvalidation_loss = 0.0005', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 11:\nvalidation_loss = 0.0005', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 12:\nvalidation_loss = 0.0005', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 13:\nvalidation_loss = 0.0004', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 14:\nvalidation_loss = 0.0004', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 15:\nvalidation_loss = 0.0004', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 16:\nvalidation_loss = 0.0004', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 17:\nvalidation_loss = 0.0004', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 18:\nvalidation_loss = 0.0004', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 19:\nvalidation_loss = 0.0003', '\\n', 'dual_channel ai_bs_64_user_bs_32 Epoch 20:\nvalidation_loss = 0.0003', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 1:\nvalidation_loss = 0.4122', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 2:\nvalidation_loss = 0.2104', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 3:\nvalidation_loss = 0.1179', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 4:\nvalidation_loss = 0.0749', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 5:\nvalidation_loss = 0.0567', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 6:\nvalidation_loss = 0.0471', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 7:\nvalidation_loss = 0.0416', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 8:\nvalidation_loss = 0.0381', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 9:\nvalidation_loss = 0.0352', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 10:\nvalidation_loss = 0.0332', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 11:\nvalidation_loss = 0.0310', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 12:\nvalidation_loss = 0.0298', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 13:\nvalidation_loss = 0.0285', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 14:\nvalidation_loss = 0.0280', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 15:\nvalidation_loss = 0.0273', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 16:\nvalidation_loss = 0.0252', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 17:\nvalidation_loss = 0.0257', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 18:\nvalidation_loss = 0.0242', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 19:\nvalidation_loss = 0.0233', '\\n', 'CE_hard_labels ai_bs_64_user_bs_64 Epoch 20:\nvalidation_loss = 0.0218', '\\n', 'soft_label_distillation ai_bs_64_user_bs_64\nEpoch 1: validation_loss = 0.1035', '\\n', 'soft_label_distillation\nai_bs_64_user_bs_64 Epoch 2: validation_loss = 0.0138', '\\n',\n'soft_label_distillation ai_bs_64_user_bs_64 Epoch 3: validation_loss = 0.0076',\n'\\n', 'soft_label_distillation ai_bs_64_user_bs_64 Epoch 4: validation_loss =\n0.0057', '\\n', 'soft_label_distillation ai_bs_64_user_bs_64 Epoch 5:\nvalidation_loss = 0.0045', '\\n', 'soft_label_distillation ai_bs_64_user_bs_64\nEpoch 6: validation_loss = 0.0038', '\\n', 'soft_label_distillation\nai_bs_64_user_bs_64 Epoch 7: validation_loss = 0.0032', '\\n',\n'soft_label_distillation ai_bs_64_user_bs_64 Epoch 8: validation_loss = 0.0027',\n'\\n', 'soft_label_distillation ai_bs_64_user_bs_64 Epoch 9: validation_loss =\n0.0023', '\\n', 'soft_label_distillation ai_bs_64_user_bs_64 Epoch 10:\nvalidation_loss = 0.0021', '\\n', 'soft_label_distillation ai_bs_64_user_bs_64\nEpoch 11: validation_loss = 0.0017', '\\n', 'soft_label_distillation\nai_bs_64_user_bs_64 Epoch 12: validation_loss = 0.0015', '\\n',\n'soft_label_distillation ai_bs_64_user_bs_64 Epoch 13: validation_loss =\n0.0013', '\\n', 'soft_label_distillation ai_bs_64_user_bs_64 Epoch 14:\nvalidation_loss = 0.0012', '\\n', 'soft_label_distillation ai_bs_64_user_bs_64\nEpoch 15: validation_loss = 0.0011', '\\n', 'soft_label_distillation\nai_bs_64_user_bs_64 Epoch 16: validation_loss = 0.0010', '\\n',\n'soft_label_distillation ai_bs_64_user_bs_64 Epoch 17: validation_loss =\n0.0010', '\\n', 'soft_label_distillation ai_bs_64_user_bs_64 Epoch 18:\nvalidation_loss = 0.0009', '\\n', 'soft_label_distillation ai_bs_64_user_bs_64\nEpoch 19: validation_loss = 0.0009', '\\n', 'soft_label_distillation\nai_bs_64_user_bs_64 Epoch 20: validation_loss = 0.0008', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 1: validation_loss = 0.3227', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 2: validation_loss = 0.1457', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 3: validation_loss = 0.0857', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 4: validation_loss = 0.0648', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 5: validation_loss = 0.0536', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 6: validation_loss = 0.0470', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 7: validation_loss = 0.0434', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 8: validation_loss = 0.0381', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 9: validation_loss = 0.0356', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 10: validation_loss = 0.0334', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 11: validation_loss = 0.0316', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 12: validation_loss = 0.0301', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 13: validation_loss = 0.0285', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 14: validation_loss = 0.0266', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 15: validation_loss = 0.0273', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 16: validation_loss = 0.0244', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 17: validation_loss = 0.0249', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 18: validation_loss = 0.0243', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 19: validation_loss = 0.0230', '\\n', 'bias_awareness\nai_bs_64_user_bs_64 Epoch 20: validation_loss = 0.0216', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 1: validation_loss = 0.1419', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 2: validation_loss = 0.0163', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 3: validation_loss = 0.0069', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 4: validation_loss = 0.0044', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 5: validation_loss = 0.0036', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 6: validation_loss = 0.0031', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 7: validation_loss = 0.0028', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 8: validation_loss = 0.0024', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 9: validation_loss = 0.0021', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 10: validation_loss = 0.0017', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 11: validation_loss = 0.0013', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 12: validation_loss = 0.0011', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 13: validation_loss = 0.0009', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 14: validation_loss = 0.0007', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 15: validation_loss = 0.0006', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 16: validation_loss = 0.0006', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 17: validation_loss = 0.0005', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 18: validation_loss = 0.0005', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 19: validation_loss = 0.0005', '\\n', 'dual_channel\nai_bs_64_user_bs_64 Epoch 20: validation_loss = 0.0005', '\\n', 'Execution time:\n49 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 0.0759, alignment_score\n= 0.9337', '\\n', 'Epoch 2: validation_loss = 0.0467, alignment_score = 0.9101',\n'\\n', 'Epoch 3: validation_loss = 0.0395, alignment_score = 0.8998', '\\n',\n'Epoch 4: validation_loss = 0.0396, alignment_score = 0.8945', '\\n', 'Epoch 5:\nvalidation_loss = 0.0373, alignment_score = 0.8934', '\\n', 'Epoch 6:\nvalidation_loss = 0.0245, alignment_score = 0.8898', '\\n', 'Epoch 7:\nvalidation_loss = 0.0235, alignment_score = 0.8882', '\\n', 'Epoch 8:\nvalidation_loss = 0.0220, alignment_score = 0.8868', '\\n', 'Epoch 9:\nvalidation_loss = 0.0180, alignment_score = 0.8844', '\\n', 'Epoch 10:\nvalidation_loss = 0.0360, alignment_score = 0.8842', '\\n', 'Epoch 11:\nvalidation_loss = 0.0194, alignment_score = 0.8834', '\\n', 'Epoch 12:\nvalidation_loss = 0.0239, alignment_score = 0.8839', '\\n', 'Epoch 13:\nvalidation_loss = 0.0165, alignment_score = 0.8815', '\\n', 'Epoch 14:\nvalidation_loss = 0.0171, alignment_score = 0.8812', '\\n', 'Epoch 15:\nvalidation_loss = 0.0182, alignment_score = 0.8807', '\\n', 'Epoch 16:\nvalidation_loss = 0.0225, alignment_score = 0.8809', '\\n', 'Epoch 17:\nvalidation_loss = 0.0140, alignment_score = 0.8796', '\\n', 'Epoch 18:\nvalidation_loss = 0.0174, alignment_score = 0.8801', '\\n', 'Epoch 19:\nvalidation_loss = 0.0124, alignment_score = 0.8777', '\\n', 'Epoch 20:\nvalidation_loss = 0.0244, alignment_score = 0.8787', '\\n', 'Epoch 1:\nvalidation_loss = 0.1646, alignment_score = 0.9819', '\\n', 'Epoch 2:\nvalidation_loss = 0.0811, alignment_score = 0.9361', '\\n', 'Epoch 3:\nvalidation_loss = 0.0589, alignment_score = 0.9191', '\\n', 'Epoch 4:\nvalidation_loss = 0.0479, alignment_score = 0.9103', '\\n', 'Epoch 5:\nvalidation_loss = 0.0420, alignment_score = 0.9051', '\\n', 'Epoch 6:\nvalidation_loss = 0.0391, alignment_score = 0.9013', '\\n', 'Epoch 7:\nvalidation_loss = 0.0337, alignment_score = 0.8971', '\\n', 'Epoch 8:\nvalidation_loss = 0.0333, alignment_score = 0.8955', '\\n', 'Epoch 9:\nvalidation_loss = 0.0306, alignment_score = 0.8931', '\\n', 'Epoch 10:\nvalidation_loss = 0.0292, alignment_score = 0.8920', '\\n', 'Epoch 11:\nvalidation_loss = 0.0248, alignment_score = 0.8894', '\\n', 'Epoch 12:\nvalidation_loss = 0.0239, alignment_score = 0.8885', '\\n', 'Epoch 13:\nvalidation_loss = 0.0231, alignment_score = 0.8873', '\\n', 'Epoch 14:\nvalidation_loss = 0.0231, alignment_score = 0.8863', '\\n', 'Epoch 15:\nvalidation_loss = 0.0199, alignment_score = 0.8844', '\\n', 'Epoch 16:\nvalidation_loss = 0.0233, alignment_score = 0.8850', '\\n', 'Epoch 17:\nvalidation_loss = 0.0182, alignment_score = 0.8838', '\\n', 'Epoch 18:\nvalidation_loss = 0.0236, alignment_score = 0.8845', '\\n', 'Epoch 19:\nvalidation_loss = 0.0175, alignment_score = 0.8826', '\\n', 'Epoch 20:\nvalidation_loss = 0.0177, alignment_score = 0.8826', '\\n', 'Epoch 1:\nvalidation_loss = 0.5264, alignment_score = 0.9590', '\\n', 'Epoch 2:\nvalidation_loss = 0.2818, alignment_score = 0.9943', '\\n', 'Epoch 3:\nvalidation_loss = 0.1513, alignment_score = 0.9770', '\\n', 'Epoch 4:\nvalidation_loss = 0.1036, alignment_score = 0.9524', '\\n', 'Epoch 5:\nvalidation_loss = 0.0790, alignment_score = 0.9372', '\\n', 'Epoch 6:\nvalidation_loss = 0.0678, alignment_score = 0.9274', '\\n', 'Epoch 7:\nvalidation_loss = 0.0586, alignment_score = 0.9203', '\\n', 'Epoch 8:\nvalidation_loss = 0.0524, alignment_score = 0.9151', '\\n', 'Epoch 9:\nvalidation_loss = 0.0478, alignment_score = 0.9110', '\\n', 'Epoch 10:\nvalidation_loss = 0.0436, alignment_score = 0.9075', '\\n', 'Epoch 11:\nvalidation_loss = 0.0410, alignment_score = 0.9046', '\\n', 'Epoch 12:\nvalidation_loss = 0.0389, alignment_score = 0.9024', '\\n', 'Epoch 13:\nvalidation_loss = 0.0349, alignment_score = 0.8999', '\\n', 'Epoch 14:\nvalidation_loss = 0.0355, alignment_score = 0.8989', '\\n', 'Epoch 15:\nvalidation_loss = 0.0321, alignment_score = 0.8969', '\\n', 'Epoch 16:\nvalidation_loss = 0.0317, alignment_score = 0.8956', '\\n', 'Epoch 17:\nvalidation_loss = 0.0292, alignment_score = 0.8940', '\\n', 'Epoch 18:\nvalidation_loss = 0.0300, alignment_score = 0.8936', '\\n', 'Epoch 19:\nvalidation_loss = 0.0289, alignment_score = 0.8927', '\\n', 'Epoch 20:\nvalidation_loss = 0.0270, alignment_score = 0.8914', '\\n', 'Epoch 1:\nvalidation_loss = 0.0875, alignment_score = 0.9479', '\\n', 'Epoch 2:\nvalidation_loss = 0.0512, alignment_score = 0.9217', '\\n', 'Epoch 3:\nvalidation_loss = 0.0390, alignment_score = 0.9109', '\\n', 'Epoch 4:\nvalidation_loss = 0.0322, alignment_score = 0.9045', '\\n', 'Epoch 5:\nvalidation_loss = 0.0280, alignment_score = 0.9003', '\\n', 'Epoch 6:\nvalidation_loss = 0.0247, alignment_score = 0.8970', '\\n', 'Epoch 7:\nvalidation_loss = 0.0234, alignment_score = 0.8939', '\\n', 'Epoch 8:\nvalidation_loss = 0.0207, alignment_score = 0.8928', '\\n', 'Epoch 9:\nvalidation_loss = 0.0199, alignment_score = 0.8916', '\\n', 'Epoch 10:\nvalidation_loss = 0.0184, alignment_score = 0.8901', '\\n', 'Epoch 11:\nvalidation_loss = 0.0180, alignment_score = 0.8894', '\\n', 'Epoch 12:\nvalidation_loss = 0.0214, alignment_score = 0.8873', '\\n', 'Epoch 13:\nvalidation_loss = 0.0158, alignment_score = 0.8872', '\\n', 'Epoch 14:\nvalidation_loss = 0.0159, alignment_score = 0.8870', '\\n', 'Epoch 15:\nvalidation_loss = 0.0364, alignment_score = 0.8875', '\\n', 'Epoch 16:\nvalidation_loss = 0.0143, alignment_score = 0.8854', '\\n', 'Epoch 17:\nvalidation_loss = 0.0146, alignment_score = 0.8849', '\\n', 'Epoch 18:\nvalidation_loss = 0.0145, alignment_score = 0.8847', '\\n', 'Epoch 19:\nvalidation_loss = 0.0132, alignment_score = 0.8845', '\\n', 'Epoch 20:\nvalidation_loss = 0.0150, alignment_score = 0.8836', '\\n', 'Epoch 1:\nvalidation_loss = 0.2137, alignment_score = 0.9954', '\\n', 'Epoch 2:\nvalidation_loss = 0.0893, alignment_score = 0.9455', '\\n', 'Epoch 3:\nvalidation_loss = 0.0613, alignment_score = 0.9266', '\\n', 'Epoch 4:\nvalidation_loss = 0.0464, alignment_score = 0.9167', '\\n', 'Epoch 5:\nvalidation_loss = 0.0412, alignment_score = 0.9107', '\\n', 'Epoch 6:\nvalidation_loss = 0.0334, alignment_score = 0.9058', '\\n', 'Epoch 7:\nvalidation_loss = 0.0298, alignment_score = 0.9020', '\\n', 'Epoch 8:\nvalidation_loss = 0.0281, alignment_score = 0.8991', '\\n', 'Epoch 9:\nvalidation_loss = 0.0245, alignment_score = 0.8971', '\\n', 'Epoch 10:\nvalidation_loss = 0.0251, alignment_score = 0.8962', '\\n', 'Epoch 11:\nvalidation_loss = 0.0267, alignment_score = 0.8952', '\\n', 'Epoch 12:\nvalidation_loss = 0.0212, alignment_score = 0.8927', '\\n', 'Epoch 13:\nvalidation_loss = 0.0207, alignment_score = 0.8925', '\\n', 'Epoch 14:\nvalidation_loss = 0.0192, alignment_score = 0.8914', '\\n', 'Epoch 15:\nvalidation_loss = 0.0181, alignment_score = 0.8903', '\\n', 'Epoch 16:\nvalidation_loss = 0.0173, alignment_score = 0.8892', '\\n', 'Epoch 17:\nvalidation_loss = 0.0179, alignment_score = 0.8889', '\\n', 'Epoch 18:\nvalidation_loss = 0.0163, alignment_score = 0.8880', '\\n', 'Epoch 19:\nvalidation_loss = 0.0178, alignment_score = 0.8870', '\\n', 'Epoch 20:\nvalidation_loss = 0.0154, alignment_score = 0.8870', '\\n', 'Epoch 1:\nvalidation_loss = 0.3148, alignment_score = 0.9926', '\\n', 'Epoch 2:\nvalidation_loss = 0.1223, alignment_score = 0.9705', '\\n', 'Epoch 3:\nvalidation_loss = 0.0762, alignment_score = 0.9415', '\\n', 'Epoch 4:\nvalidation_loss = 0.0603, alignment_score = 0.9283', '\\n', 'Epoch 5:\nvalidation_loss = 0.0504, alignment_score = 0.9200', '\\n', 'Epoch 6:\nvalidation_loss = 0.0449, alignment_score = 0.9155', '\\n', 'Epoch 7:\nvalidation_loss = 0.0390, alignment_score = 0.9109', '\\n', 'Epoch 8:\nvalidation_loss = 0.0361, alignment_score = 0.9083', '\\n', 'Epoch 9:\nvalidation_loss = 0.0329, alignment_score = 0.9049', '\\n', 'Epoch 10:\nvalidation_loss = 0.0342, alignment_score = 0.9040', '\\n', 'Epoch 11:\nvalidation_loss = 0.0307, alignment_score = 0.9019', '\\n', 'Epoch 12:\nvalidation_loss = 0.0279, alignment_score = 0.9000', '\\n', 'Epoch 13:\nvalidation_loss = 0.0257, alignment_score = 0.8982', '\\n', 'Epoch 14:\nvalidation_loss = 0.0246, alignment_score = 0.8970', '\\n', 'Epoch 15:\nvalidation_loss = 0.0251, alignment_score = 0.8964', '\\n', 'Epoch 16:\nvalidation_loss = 0.0231, alignment_score = 0.8941', '\\n', 'Epoch 17:\nvalidation_loss = 0.0215, alignment_score = 0.8938', '\\n', 'Epoch 18:\nvalidation_loss = 0.0224, alignment_score = 0.8936', '\\n', 'Epoch 19:\nvalidation_loss = 0.0202, alignment_score = 0.8923', '\\n', 'Epoch 20:\nvalidation_loss = 0.0200, alignment_score = 0.8918', '\\n', 'Epoch 1:\nvalidation_loss = 0.0959, alignment_score = 0.9510', '\\n', 'Epoch 2:\nvalidation_loss = 0.0479, alignment_score = 0.9140', '\\n', 'Epoch 3:\nvalidation_loss = 0.0365, alignment_score = 0.9003', '\\n', 'Epoch 4:\nvalidation_loss = 0.0286, alignment_score = 0.8932', '\\n', 'Epoch 5:\nvalidation_loss = 0.0257, alignment_score = 0.8899', '\\n', 'Epoch 6:\nvalidation_loss = 0.0248, alignment_score = 0.8858', '\\n', 'Epoch 7:\nvalidation_loss = 0.0183, alignment_score = 0.8836', '\\n', 'Epoch 8:\nvalidation_loss = 0.0183, alignment_score = 0.8823', '\\n', 'Epoch 9:\nvalidation_loss = 0.0161, alignment_score = 0.8801', '\\n', 'Epoch 10:\nvalidation_loss = 0.0150, alignment_score = 0.8794', '\\n', 'Epoch 11:\nvalidation_loss = 0.0149, alignment_score = 0.8785', '\\n', 'Epoch 12:\nvalidation_loss = 0.0140, alignment_score = 0.8779', '\\n', 'Epoch 13:\nvalidation_loss = 0.0155, alignment_score = 0.8775', '\\n', 'Epoch 14:\nvalidation_loss = 0.0126, alignment_score = 0.8761', '\\n', 'Epoch 15:\nvalidation_loss = 0.0122, alignment_score = 0.8759', '\\n', 'Epoch 16:\nvalidation_loss = 0.0126, alignment_score = 0.8758', '\\n', 'Epoch 17:\nvalidation_loss = 0.0149, alignment_score = 0.8743', '\\n', 'Epoch 18:\nvalidation_loss = 0.0221, alignment_score = 0.8762', '\\n', 'Epoch 19:\nvalidation_loss = 0.0115, alignment_score = 0.8739', '\\n', 'Epoch 20:\nvalidation_loss = 0.0107, alignment_score = 0.8742', '\\n', 'Epoch 1:\nvalidation_loss = 0.2701, alignment_score = 0.9950', '\\n', 'Epoch 2:\nvalidation_loss = 0.0883, alignment_score = 0.9432', '\\n', 'Epoch 3:\nvalidation_loss = 0.0542, alignment_score = 0.9162', '\\n', 'Epoch 4:\nvalidation_loss = 0.0429, alignment_score = 0.9051', '\\n', 'Epoch 5:\nvalidation_loss = 0.0349, alignment_score = 0.8985', '\\n', 'Epoch 6:\nvalidation_loss = 0.0288, alignment_score = 0.8933', '\\n', 'Epoch 7:\nvalidation_loss = 0.0260, alignment_score = 0.8903', '\\n', 'Epoch 8:\nvalidation_loss = 0.0314, alignment_score = 0.8887', '\\n', 'Epoch 9:\nvalidation_loss = 0.0222, alignment_score = 0.8856', '\\n', 'Epoch 10:\nvalidation_loss = 0.0229, alignment_score = 0.8848', '\\n', 'Epoch 11:\nvalidation_loss = 0.0200, alignment_score = 0.8829', '\\n', 'Epoch 12:\nvalidation_loss = 0.0186, alignment_score = 0.8818', '\\n', 'Epoch 13:\nvalidation_loss = 0.0174, alignment_score = 0.8806', '\\n', 'Epoch 14:\nvalidation_loss = 0.0192, alignment_score = 0.8790', '\\n', 'Epoch 15:\nvalidation_loss = 0.0162, alignment_score = 0.8786', '\\n', 'Epoch 16:\nvalidation_loss = 0.0169, alignment_score = 0.8776', '\\n', 'Epoch 17:\nvalidation_loss = 0.0152, alignment_score = 0.8776', '\\n', 'Epoch 18:\nvalidation_loss = 0.0148, alignment_score = 0.8768', '\\n', 'Epoch 19:\nvalidation_loss = 0.0143, alignment_score = 0.8766', '\\n', 'Epoch 20:\nvalidation_loss = 0.0147, alignment_score = 0.8756', '\\n', 'Epoch 1:\nvalidation_loss = 0.4215, alignment_score = 0.9800', '\\n', 'Epoch 2:\nvalidation_loss = 0.2112, alignment_score = 0.9914', '\\n', 'Epoch 3:\nvalidation_loss = 0.1132, alignment_score = 0.9602', '\\n', 'Epoch 4:\nvalidation_loss = 0.0793, alignment_score = 0.9370', '\\n', 'Epoch 5:\nvalidation_loss = 0.0628, alignment_score = 0.9244', '\\n', 'Epoch 6:\nvalidation_loss = 0.0529, alignment_score = 0.9164', '\\n', 'Epoch 7:\nvalidation_loss = 0.0471, alignment_score = 0.9105', '\\n', 'Epoch 8:\nvalidation_loss = 0.0412, alignment_score = 0.9059', '\\n', 'Epoch 9:\nvalidation_loss = 0.0374, alignment_score = 0.9024', '\\n', 'Epoch 10:\nvalidation_loss = 0.0339, alignment_score = 0.8992', '\\n', 'Epoch 11:\nvalidation_loss = 0.0314, alignment_score = 0.8966', '\\n', 'Epoch 12:\nvalidation_loss = 0.0293, alignment_score = 0.8945', '\\n', 'Epoch 13:\nvalidation_loss = 0.0272, alignment_score = 0.8924', '\\n', 'Epoch 14:\nvalidation_loss = 0.0259, alignment_score = 0.8909', '\\n', 'Epoch 15:\nvalidation_loss = 0.0244, alignment_score = 0.8893', '\\n', 'Epoch 16:\nvalidation_loss = 0.0235, alignment_score = 0.8881', '\\n', 'Epoch 17:\nvalidation_loss = 0.0221, alignment_score = 0.8866', '\\n', 'Epoch 18:\nvalidation_loss = 0.0225, alignment_score = 0.8861', '\\n', 'Epoch 19:\nvalidation_loss = 0.0204, alignment_score = 0.8846', '\\n', 'Epoch 20:\nvalidation_loss = 0.0211, alignment_score = 0.8843', '\\n', 'Epoch 1:\nvalidation_loss = 0.0499, alignment_score = 0.9901', '\\n', 'Epoch 2:\nvalidation_loss = 0.0284, alignment_score = 0.9800', '\\n', 'Epoch 3:\nvalidation_loss = 0.0233, alignment_score = 0.9739', '\\n', 'Epoch 4:\nvalidation_loss = 0.0207, alignment_score = 0.9721', '\\n', 'Epoch 5:\nvalidation_loss = 0.0150, alignment_score = 0.9696', '\\n', 'Epoch 6:\nvalidation_loss = 0.0123, alignment_score = 0.9673', '\\n', 'Epoch 7:\nvalidation_loss = 0.0122, alignment_score = 0.9672', '\\n', 'Epoch 8:\nvalidation_loss = 0.0124, alignment_score = 0.9667', '\\n', 'Epoch 9:\nvalidation_loss = 0.0107, alignment_score = 0.9660', '\\n', 'Epoch 10:\nvalidation_loss = 0.0113, alignment_score = 0.9656', '\\n', 'Epoch 11:\nvalidation_loss = 0.0127, alignment_score = 0.9655', '\\n', 'Epoch 12:\nvalidation_loss = 0.0119, alignment_score = 0.9649', '\\n', 'Epoch 13:\nvalidation_loss = 0.0086, alignment_score = 0.9640', '\\n', 'Epoch 14:\nvalidation_loss = 0.0092, alignment_score = 0.9630', '\\n', 'Epoch 15:\nvalidation_loss = 0.0101, alignment_score = 0.9643', '\\n', 'Epoch 16:\nvalidation_loss = 0.0072, alignment_score = 0.9628', '\\n', 'Epoch 17:\nvalidation_loss = 0.0097, alignment_score = 0.9629', '\\n', 'Epoch 18:\nvalidation_loss = 0.0071, alignment_score = 0.9621', '\\n', 'Epoch 19:\nvalidation_loss = 0.0071, alignment_score = 0.9622', '\\n', 'Epoch 20:\nvalidation_loss = 0.0093, alignment_score = 0.9626', '\\n', 'Epoch 1:\nvalidation_loss = 0.1541, alignment_score = 0.9835', '\\n', 'Epoch 2:\nvalidation_loss = 0.0490, alignment_score = 0.9915', '\\n', 'Epoch 3:\nvalidation_loss = 0.0349, alignment_score = 0.9838', '\\n', 'Epoch 4:\nvalidation_loss = 0.0287, alignment_score = 0.9795', '\\n', 'Epoch 5:\nvalidation_loss = 0.0243, alignment_score = 0.9769', '\\n', 'Epoch 6:\nvalidation_loss = 0.0225, alignment_score = 0.9741', '\\n', 'Epoch 7:\nvalidation_loss = 0.0195, alignment_score = 0.9723', '\\n', 'Epoch 8:\nvalidation_loss = 0.0190, alignment_score = 0.9719', '\\n', 'Epoch 9:\nvalidation_loss = 0.0174, alignment_score = 0.9707', '\\n', 'Epoch 10:\nvalidation_loss = 0.0151, alignment_score = 0.9693', '\\n', 'Epoch 11:\nvalidation_loss = 0.0134, alignment_score = 0.9681', '\\n', 'Epoch 12:\nvalidation_loss = 0.0132, alignment_score = 0.9674', '\\n', 'Epoch 13:\nvalidation_loss = 0.0122, alignment_score = 0.9672', '\\n', 'Epoch 14:\nvalidation_loss = 0.0123, alignment_score = 0.9671', '\\n', 'Epoch 15:\nvalidation_loss = 0.0122, alignment_score = 0.9661', '\\n', 'Epoch 16:\nvalidation_loss = 0.0126, alignment_score = 0.9664', '\\n', 'Epoch 17:\nvalidation_loss = 0.0114, alignment_score = 0.9649', '\\n', 'Epoch 18:\nvalidation_loss = 0.0122, alignment_score = 0.9646', '\\n', 'Epoch 19:\nvalidation_loss = 0.0096, alignment_score = 0.9646', '\\n', 'Epoch 20:\nvalidation_loss = 0.0103, alignment_score = 0.9645', '\\n', 'Epoch 1:\nvalidation_loss = 0.2999, alignment_score = 0.9338', '\\n', 'Epoch 2:\nvalidation_loss = 0.1117, alignment_score = 0.9893', '\\n', 'Epoch 3:\nvalidation_loss = 0.0561, alignment_score = 0.9895', '\\n', 'Epoch 4:\nvalidation_loss = 0.0366, alignment_score = 0.9851', '\\n', 'Epoch 5:\nvalidation_loss = 0.0309, alignment_score = 0.9813', '\\n', 'Epoch 6:\nvalidation_loss = 0.0260, alignment_score = 0.9786', '\\n', 'Epoch 7:\nvalidation_loss = 0.0232, alignment_score = 0.9765', '\\n', 'Epoch 8:\nvalidation_loss = 0.0210, alignment_score = 0.9748', '\\n', 'Epoch 9:\nvalidation_loss = 0.0188, alignment_score = 0.9734', '\\n', 'Epoch 10:\nvalidation_loss = 0.0181, alignment_score = 0.9725', '\\n', 'Epoch 11:\nvalidation_loss = 0.0168, alignment_score = 0.9713', '\\n', 'Epoch 12:\nvalidation_loss = 0.0164, alignment_score = 0.9708', '\\n', 'Epoch 13:\nvalidation_loss = 0.0154, alignment_score = 0.9697', '\\n', 'Epoch 14:\nvalidation_loss = 0.0133, alignment_score = 0.9687', '\\n', 'Epoch 15:\nvalidation_loss = 0.0147, alignment_score = 0.9690', '\\n', 'Epoch 16:\nvalidation_loss = 0.0125, alignment_score = 0.9678', '\\n', 'Epoch 17:\nvalidation_loss = 0.0129, alignment_score = 0.9676', '\\n', 'Epoch 18:\nvalidation_loss = 0.0122, alignment_score = 0.9673', '\\n', 'Epoch 19:\nvalidation_loss = 0.0119, alignment_score = 0.9668', '\\n', 'Epoch 20:\nvalidation_loss = 0.0115, alignment_score = 0.9668', '\\n', 'Epoch 1:\nvalidation_loss = 0.0628, alignment_score = 0.9910', '\\n', 'Epoch 2:\nvalidation_loss = 0.0412, alignment_score = 0.9822', '\\n', 'Epoch 3:\nvalidation_loss = 0.0392, alignment_score = 0.9778', '\\n', 'Epoch 4:\nvalidation_loss = 0.0337, alignment_score = 0.9756', '\\n', 'Epoch 5:\nvalidation_loss = 0.0322, alignment_score = 0.9734', '\\n', 'Epoch 6:\nvalidation_loss = 0.0322, alignment_score = 0.9694', '\\n', 'Epoch 7:\nvalidation_loss = 0.0238, alignment_score = 0.9714', '\\n', 'Epoch 8:\nvalidation_loss = 0.0319, alignment_score = 0.9680', '\\n', 'Epoch 9:\nvalidation_loss = 0.0276, alignment_score = 0.9674', '\\n', 'Epoch 10:\nvalidation_loss = 0.0225, alignment_score = 0.9686', '\\n', 'Epoch 11:\nvalidation_loss = 0.0312, alignment_score = 0.9653', '\\n', 'Epoch 12:\nvalidation_loss = 0.0366, alignment_score = 0.9672', '\\n', 'Epoch 13:\nvalidation_loss = 0.0222, alignment_score = 0.9664', '\\n', 'Epoch 14:\nvalidation_loss = 0.0231, alignment_score = 0.9646', '\\n', 'Epoch 15:\nvalidation_loss = 0.0278, alignment_score = 0.9637', '\\n', 'Epoch 16:\nvalidation_loss = 0.0229, alignment_score = 0.9665', '\\n', 'Epoch 17:\nvalidation_loss = 0.0216, alignment_score = 0.9648', '\\n', 'Epoch 18:\nvalidation_loss = 0.0346, alignment_score = 0.9629', '\\n', 'Epoch 19:\nvalidation_loss = 0.0255, alignment_score = 0.9650', '\\n', 'Epoch 20:\nvalidation_loss = 0.0199, alignment_score = 0.9650', '\\n', 'Epoch 1:\nvalidation_loss = 0.2010, alignment_score = 0.9680', '\\n', 'Epoch 2:\nvalidation_loss = 0.0575, alignment_score = 0.9927', '\\n', 'Epoch 3:\nvalidation_loss = 0.0434, alignment_score = 0.9838', '\\n', 'Epoch 4:\nvalidation_loss = 0.0374, alignment_score = 0.9804', '\\n', 'Epoch 5:\nvalidation_loss = 0.0351, alignment_score = 0.9774', '\\n', 'Epoch 6:\nvalidation_loss = 0.0305, alignment_score = 0.9752', '\\n', 'Epoch 7:\nvalidation_loss = 0.0311, alignment_score = 0.9737', '\\n', 'Epoch 8:\nvalidation_loss = 0.0300, alignment_score = 0.9724', '\\n', 'Epoch 9:\nvalidation_loss = 0.0259, alignment_score = 0.9728', '\\n', 'Epoch 10:\nvalidation_loss = 0.0293, alignment_score = 0.9706', '\\n', 'Epoch 11:\nvalidation_loss = 0.0252, alignment_score = 0.9710', '\\n', 'Epoch 12:\nvalidation_loss = 0.0244, alignment_score = 0.9709', '\\n', 'Epoch 13:\nvalidation_loss = 0.0265, alignment_score = 0.9684', '\\n', 'Epoch 14:\nvalidation_loss = 0.0252, alignment_score = 0.9684', '\\n', 'Epoch 15:\nvalidation_loss = 0.0259, alignment_score = 0.9681', '\\n', 'Epoch 16:\nvalidation_loss = 0.0221, alignment_score = 0.9679', '\\n', 'Epoch 17:\nvalidation_loss = 0.0256, alignment_score = 0.9665', '\\n', 'Epoch 18:\nvalidation_loss = 0.0251, alignment_score = 0.9665', '\\n', 'Epoch 19:\nvalidation_loss = 0.0218, alignment_score = 0.9668', '\\n', 'Epoch 20:\nvalidation_loss = 0.0216, alignment_score = 0.9659', '\\n', 'Epoch 1:\nvalidation_loss = 0.4400, alignment_score = 0.8796', '\\n', 'Epoch 2:\nvalidation_loss = 0.1786, alignment_score = 0.9767', '\\n', 'Epoch 3:\nvalidation_loss = 0.0817, alignment_score = 0.9954', '\\n', 'Epoch 4:\nvalidation_loss = 0.0553, alignment_score = 0.9907', '\\n', 'Epoch 5:\nvalidation_loss = 0.0474, alignment_score = 0.9856', '\\n', 'Epoch 6:\nvalidation_loss = 0.0429, alignment_score = 0.9822', '\\n', 'Epoch 7:\nvalidation_loss = 0.0395, alignment_score = 0.9801', '\\n', 'Epoch 8:\nvalidation_loss = 0.0368, alignment_score = 0.9778', '\\n', 'Epoch 9:\nvalidation_loss = 0.0334, alignment_score = 0.9771', '\\n', 'Epoch 10:\nvalidation_loss = 0.0324, alignment_score = 0.9757', '\\n', 'Epoch 11:\nvalidation_loss = 0.0323, alignment_score = 0.9749', '\\n', 'Epoch 12:\nvalidation_loss = 0.0290, alignment_score = 0.9731', '\\n', 'Epoch 13:\nvalidation_loss = 0.0303, alignment_score = 0.9733', '\\n', 'Epoch 14:\nvalidation_loss = 0.0285, alignment_score = 0.9722', '\\n', 'Epoch 15:\nvalidation_loss = 0.0275, alignment_score = 0.9719', '\\n', 'Epoch 16:\nvalidation_loss = 0.0275, alignment_score = 0.9713', '\\n', 'Epoch 17:\nvalidation_loss = 0.0264, alignment_score = 0.9702', '\\n', 'Epoch 18:\nvalidation_loss = 0.0289, alignment_score = 0.9696', '\\n', 'Epoch 19:\nvalidation_loss = 0.0250, alignment_score = 0.9694', '\\n', 'Epoch 20:\nvalidation_loss = 0.0264, alignment_score = 0.9687', '\\n', 'Epoch 1:\nvalidation_loss = 0.0626, alignment_score = 0.9943', '\\n', 'Epoch 2:\nvalidation_loss = 0.0337, alignment_score = 0.9828', '\\n', 'Epoch 3:\nvalidation_loss = 0.0256, alignment_score = 0.9768', '\\n', 'Epoch 4:\nvalidation_loss = 0.0245, alignment_score = 0.9725', '\\n', 'Epoch 5:\nvalidation_loss = 0.0206, alignment_score = 0.9688', '\\n', 'Epoch 6:\nvalidation_loss = 0.0188, alignment_score = 0.9698', '\\n', 'Epoch 7:\nvalidation_loss = 0.0143, alignment_score = 0.9678', '\\n', 'Epoch 8:\nvalidation_loss = 0.0125, alignment_score = 0.9662', '\\n', 'Epoch 9:\nvalidation_loss = 0.0126, alignment_score = 0.9657', '\\n', 'Epoch 10:\nvalidation_loss = 0.0130, alignment_score = 0.9645', '\\n', 'Epoch 11:\nvalidation_loss = 0.0139, alignment_score = 0.9625', '\\n', 'Epoch 12:\nvalidation_loss = 0.0099, alignment_score = 0.9640', '\\n', 'Epoch 13:\nvalidation_loss = 0.0104, alignment_score = 0.9620', '\\n', 'Epoch 14:\nvalidation_loss = 0.0131, alignment_score = 0.9617', '\\n', 'Epoch 15:\nvalidation_loss = 0.0123, alignment_score = 0.9613', '\\n', 'Epoch 16:\nvalidation_loss = 0.0082, alignment_score = 0.9614', '\\n', 'Epoch 17:\nvalidation_loss = 0.0067, alignment_score = 0.9617', '\\n', 'Epoch 18:\nvalidation_loss = 0.0074, alignment_score = 0.9609', '\\n', 'Epoch 19:\nvalidation_loss = 0.0055, alignment_score = 0.9609', '\\n', 'Epoch 20:\nvalidation_loss = 0.0120, alignment_score = 0.9603', '\\n', 'Epoch 1:\nvalidation_loss = 0.1632, alignment_score = 0.9750', '\\n', 'Epoch 2:\nvalidation_loss = 0.0527, alignment_score = 0.9898', '\\n', 'Epoch 3:\nvalidation_loss = 0.0349, alignment_score = 0.9825', '\\n', 'Epoch 4:\nvalidation_loss = 0.0257, alignment_score = 0.9777', '\\n', 'Epoch 5:\nvalidation_loss = 0.0235, alignment_score = 0.9748', '\\n', 'Epoch 6:\nvalidation_loss = 0.0214, alignment_score = 0.9732', '\\n', 'Epoch 7:\nvalidation_loss = 0.0182, alignment_score = 0.9713', '\\n', 'Epoch 8:\nvalidation_loss = 0.0187, alignment_score = 0.9691', '\\n', 'Epoch 9:\nvalidation_loss = 0.0163, alignment_score = 0.9691', '\\n', 'Epoch 10:\nvalidation_loss = 0.0146, alignment_score = 0.9681', '\\n', 'Epoch 11:\nvalidation_loss = 0.0144, alignment_score = 0.9676', '\\n', 'Epoch 12:\nvalidation_loss = 0.0132, alignment_score = 0.9663', '\\n', 'Epoch 13:\nvalidation_loss = 0.0135, alignment_score = 0.9659', '\\n', 'Epoch 14:\nvalidation_loss = 0.0136, alignment_score = 0.9653', '\\n', 'Epoch 15:\nvalidation_loss = 0.0117, alignment_score = 0.9651', '\\n', 'Epoch 16:\nvalidation_loss = 0.0144, alignment_score = 0.9648', '\\n', 'Epoch 17:\nvalidation_loss = 0.0118, alignment_score = 0.9634', '\\n', 'Epoch 18:\nvalidation_loss = 0.0157, alignment_score = 0.9639', '\\n', 'Epoch 19:\nvalidation_loss = 0.0126, alignment_score = 0.9635', '\\n', 'Epoch 20:\nvalidation_loss = 0.0154, alignment_score = 0.9616', '\\n', 'Epoch 1:\nvalidation_loss = 0.4559, alignment_score = 0.8754', '\\n', 'Epoch 2:\nvalidation_loss = 0.1525, alignment_score = 0.9837', '\\n', 'Epoch 3:\nvalidation_loss = 0.0636, alignment_score = 0.9944', '\\n', 'Epoch 4:\nvalidation_loss = 0.0426, alignment_score = 0.9873', '\\n', 'Epoch 5:\nvalidation_loss = 0.0335, alignment_score = 0.9826', '\\n', 'Epoch 6:\nvalidation_loss = 0.0282, alignment_score = 0.9786', '\\n', 'Epoch 7:\nvalidation_loss = 0.0263, alignment_score = 0.9769', '\\n', 'Epoch 8:\nvalidation_loss = 0.0232, alignment_score = 0.9745', '\\n', 'Epoch 9:\nvalidation_loss = 0.0230, alignment_score = 0.9729', '\\n', 'Epoch 10:\nvalidation_loss = 0.0206, alignment_score = 0.9722', '\\n', 'Epoch 11:\nvalidation_loss = 0.0200, alignment_score = 0.9714', '\\n', 'Epoch 12:\nvalidation_loss = 0.0182, alignment_score = 0.9702', '\\n', 'Epoch 13:\nvalidation_loss = 0.0185, alignment_score = 0.9693', '\\n', 'Epoch 14:\nvalidation_loss = 0.0170, alignment_score = 0.9688', '\\n', 'Epoch 15:\nvalidation_loss = 0.0176, alignment_score = 0.9680', '\\n', 'Epoch 16:\nvalidation_loss = 0.0158, alignment_score = 0.9674', '\\n', 'Epoch 17:\nvalidation_loss = 0.0177, alignment_score = 0.9665', '\\n', 'Epoch 18:\nvalidation_loss = 0.0161, alignment_score = 0.9668', '\\n', 'Epoch 19:\nvalidation_loss = 0.0154, alignment_score = 0.9659', '\\n', 'Epoch 20:\nvalidation_loss = 0.0179, alignment_score = 0.9653', '\\n', 'Epoch 1:\nvalidation_loss = 0.2285, alignment_score = 0.9344', '\\n', 'Epoch 2:\nvalidation_loss = 0.1810, alignment_score = 0.9115', '\\n', 'Epoch 3:\nvalidation_loss = 0.1514, alignment_score = 0.9014', '\\n', 'Epoch 4:\nvalidation_loss = 0.1360, alignment_score = 0.9008', '\\n', 'Epoch 5:\nvalidation_loss = 0.1267, alignment_score = 0.8887', '\\n', 'Epoch 6:\nvalidation_loss = 0.1106, alignment_score = 0.8862', '\\n', 'Epoch 7:\nvalidation_loss = 0.1043, alignment_score = 0.8824', '\\n', 'Epoch 8:\nvalidation_loss = 0.1161, alignment_score = 0.8808', '\\n', 'Epoch 9:\nvalidation_loss = 0.0954, alignment_score = 0.8764', '\\n', 'Epoch 10:\nvalidation_loss = 0.0942, alignment_score = 0.8716', '\\n', 'Epoch 11:\nvalidation_loss = 0.0846, alignment_score = 0.8693', '\\n', 'Epoch 12:\nvalidation_loss = 0.0886, alignment_score = 0.8690', '\\n', 'Epoch 13:\nvalidation_loss = 0.1016, alignment_score = 0.8686', '\\n', 'Epoch 14:\nvalidation_loss = 0.0963, alignment_score = 0.8651', '\\n', 'Epoch 15:\nvalidation_loss = 0.1027, alignment_score = 0.8641', '\\n', 'Epoch 16:\nvalidation_loss = 0.0896, alignment_score = 0.8617', '\\n', 'Epoch 17:\nvalidation_loss = 0.0941, alignment_score = 0.8622', '\\n', 'Epoch 18:\nvalidation_loss = 0.0960, alignment_score = 0.8614', '\\n', 'Epoch 19:\nvalidation_loss = 0.0747, alignment_score = 0.8620', '\\n', 'Epoch 20:\nvalidation_loss = 0.0742, alignment_score = 0.8611', '\\n', 'Epoch 1:\nvalidation_loss = 0.2787, alignment_score = 0.9613', '\\n', 'Epoch 2:\nvalidation_loss = 0.2040, alignment_score = 0.9188', '\\n', 'Epoch 3:\nvalidation_loss = 0.1752, alignment_score = 0.9120', '\\n', 'Epoch 4:\nvalidation_loss = 0.1586, alignment_score = 0.9052', '\\n', 'Epoch 5:\nvalidation_loss = 0.1484, alignment_score = 0.8976', '\\n', 'Epoch 6:\nvalidation_loss = 0.1369, alignment_score = 0.8951', '\\n', 'Epoch 7:\nvalidation_loss = 0.1310, alignment_score = 0.8898', '\\n', 'Epoch 8:\nvalidation_loss = 0.1264, alignment_score = 0.8887', '\\n', 'Epoch 9:\nvalidation_loss = 0.1270, alignment_score = 0.8817', '\\n', 'Epoch 10:\nvalidation_loss = 0.1231, alignment_score = 0.8794', '\\n', 'Epoch 11:\nvalidation_loss = 0.1068, alignment_score = 0.8784', '\\n', 'Epoch 12:\nvalidation_loss = 0.1107, alignment_score = 0.8748', '\\n', 'Epoch 13:\nvalidation_loss = 0.1003, alignment_score = 0.8746', '\\n', 'Epoch 14:\nvalidation_loss = 0.1065, alignment_score = 0.8752', '\\n', 'Epoch 15:\nvalidation_loss = 0.1088, alignment_score = 0.8725', '\\n', 'Epoch 16:\nvalidation_loss = 0.1183, alignment_score = 0.8713', '\\n', 'Epoch 17:\nvalidation_loss = 0.1005, alignment_score = 0.8699', '\\n', 'Epoch 18:\nvalidation_loss = 0.1026, alignment_score = 0.8670', '\\n', 'Epoch 19:\nvalidation_loss = 0.0922, alignment_score = 0.8688', '\\n', 'Epoch 20:\nvalidation_loss = 0.0950, alignment_score = 0.8666', '\\n', 'Epoch 1:\nvalidation_loss = 0.4500, alignment_score = 0.9764', '\\n', 'Epoch 2:\nvalidation_loss = 0.2859, alignment_score = 0.9632', '\\n', 'Epoch 3:\nvalidation_loss = 0.2159, alignment_score = 0.9310', '\\n', 'Epoch 4:\nvalidation_loss = 0.1953, alignment_score = 0.9184', '\\n', 'Epoch 5:\nvalidation_loss = 0.1766, alignment_score = 0.9108', '\\n', 'Epoch 6:\nvalidation_loss = 0.1606, alignment_score = 0.9077', '\\n', 'Epoch 7:\nvalidation_loss = 0.1471, alignment_score = 0.9057', '\\n', 'Epoch 8:\nvalidation_loss = 0.1434, alignment_score = 0.8998', '\\n', 'Epoch 9:\nvalidation_loss = 0.1357, alignment_score = 0.8980', '\\n', 'Epoch 10:\nvalidation_loss = 0.1260, alignment_score = 0.8931', '\\n', 'Epoch 11:\nvalidation_loss = 0.1271, alignment_score = 0.8882', '\\n', 'Epoch 12:\nvalidation_loss = 0.1197, alignment_score = 0.8870', '\\n', 'Epoch 13:\nvalidation_loss = 0.1138, alignment_score = 0.8846', '\\n', 'Epoch 14:\nvalidation_loss = 0.1120, alignment_score = 0.8829', '\\n', 'Epoch 15:\nvalidation_loss = 0.1159, alignment_score = 0.8802', '\\n', 'Epoch 16:\nvalidation_loss = 0.1052, alignment_score = 0.8800', '\\n', 'Epoch 17:\nvalidation_loss = 0.1044, alignment_score = 0.8780', '\\n', 'Epoch 18:\nvalidation_loss = 0.1077, alignment_score = 0.8764', '\\n', 'Epoch 19:\nvalidation_loss = 0.1050, alignment_score = 0.8740', '\\n', 'Epoch 20:\nvalidation_loss = 0.0995, alignment_score = 0.8754', '\\n', 'Epoch 1:\nvalidation_loss = 0.2855, alignment_score = 0.9652', '\\n', 'Epoch 2:\nvalidation_loss = 0.2130, alignment_score = 0.9478', '\\n', 'Epoch 3:\nvalidation_loss = 0.1722, alignment_score = 0.9267', '\\n', 'Epoch 4:\nvalidation_loss = 0.1585, alignment_score = 0.9146', '\\n', 'Epoch 5:\nvalidation_loss = 0.1332, alignment_score = 0.9084', '\\n', 'Epoch 6:\nvalidation_loss = 0.1382, alignment_score = 0.8961', '\\n', 'Epoch 7:\nvalidation_loss = 0.1121, alignment_score = 0.8932', '\\n', 'Epoch 8:\nvalidation_loss = 0.1103, alignment_score = 0.8921', '\\n', 'Epoch 9:\nvalidation_loss = 0.1239, alignment_score = 0.8896', '\\n', 'Epoch 10:\nvalidation_loss = 0.1065, alignment_score = 0.8852', '\\n', 'Epoch 11:\nvalidation_loss = 0.1320, alignment_score = 0.8773', '\\n', 'Epoch 12:\nvalidation_loss = 0.0891, alignment_score = 0.8787', '\\n', 'Epoch 13:\nvalidation_loss = 0.1089, alignment_score = 0.8743', '\\n', 'Epoch 14:\nvalidation_loss = 0.1043, alignment_score = 0.8742', '\\n', 'Epoch 15:\nvalidation_loss = 0.0845, alignment_score = 0.8723', '\\n', 'Epoch 16:\nvalidation_loss = 0.0708, alignment_score = 0.8726', '\\n', 'Epoch 17:\nvalidation_loss = 0.0903, alignment_score = 0.8713', '\\n', 'Epoch 18:\nvalidation_loss = 0.0897, alignment_score = 0.8692', '\\n', 'Epoch 19:\nvalidation_loss = 0.0722, alignment_score = 0.8693', '\\n', 'Epoch 20:\nvalidation_loss = 0.0748, alignment_score = 0.8691', '\\n', 'Epoch 1:\nvalidation_loss = 0.2959, alignment_score = 0.9732', '\\n', 'Epoch 2:\nvalidation_loss = 0.1935, alignment_score = 0.9307', '\\n', 'Epoch 3:\nvalidation_loss = 0.1716, alignment_score = 0.9158', '\\n', 'Epoch 4:\nvalidation_loss = 0.1577, alignment_score = 0.9120', '\\n', 'Epoch 5:\nvalidation_loss = 0.1444, alignment_score = 0.9050', '\\n', 'Epoch 6:\nvalidation_loss = 0.1315, alignment_score = 0.9029', '\\n', 'Epoch 7:\nvalidation_loss = 0.1329, alignment_score = 0.9001', '\\n', 'Epoch 8:\nvalidation_loss = 0.1153, alignment_score = 0.8965', '\\n', 'Epoch 9:\nvalidation_loss = 0.1025, alignment_score = 0.8906', '\\n', 'Epoch 10:\nvalidation_loss = 0.1071, alignment_score = 0.8894', '\\n', 'Epoch 11:\nvalidation_loss = 0.1277, alignment_score = 0.8835', '\\n', 'Epoch 12:\nvalidation_loss = 0.0802, alignment_score = 0.8838', '\\n', 'Epoch 13:\nvalidation_loss = 0.0763, alignment_score = 0.8773', '\\n', 'Epoch 14:\nvalidation_loss = 0.0795, alignment_score = 0.8787', '\\n', 'Epoch 15:\nvalidation_loss = 0.0679, alignment_score = 0.8746', '\\n', 'Epoch 16:\nvalidation_loss = 0.0821, alignment_score = 0.8735', '\\n', 'Epoch 17:\nvalidation_loss = 0.0661, alignment_score = 0.8705', '\\n', 'Epoch 18:\nvalidation_loss = 0.0679, alignment_score = 0.8676', '\\n', 'Epoch 19:\nvalidation_loss = 0.0564, alignment_score = 0.8677', '\\n', 'Epoch 20:\nvalidation_loss = 0.0778, alignment_score = 0.8652', '\\n', 'Epoch 1:\nvalidation_loss = 0.3458, alignment_score = 0.9863', '\\n', 'Epoch 2:\nvalidation_loss = 0.2228, alignment_score = 0.9545', '\\n', 'Epoch 3:\nvalidation_loss = 0.1847, alignment_score = 0.9266', '\\n', 'Epoch 4:\nvalidation_loss = 0.1673, alignment_score = 0.9179', '\\n', 'Epoch 5:\nvalidation_loss = 0.1559, alignment_score = 0.9129', '\\n', 'Epoch 6:\nvalidation_loss = 0.1527, alignment_score = 0.9093', '\\n', 'Epoch 7:\nvalidation_loss = 0.1456, alignment_score = 0.9054', '\\n', 'Epoch 8:\nvalidation_loss = 0.1472, alignment_score = 0.9012', '\\n', 'Epoch 9:\nvalidation_loss = 0.1391, alignment_score = 0.8996', '\\n', 'Epoch 10:\nvalidation_loss = 0.1470, alignment_score = 0.8966', '\\n', 'Epoch 11:\nvalidation_loss = 0.1316, alignment_score = 0.8944', '\\n', 'Epoch 12:\nvalidation_loss = 0.1283, alignment_score = 0.8932', '\\n', 'Epoch 13:\nvalidation_loss = 0.1189, alignment_score = 0.8903', '\\n', 'Epoch 14:\nvalidation_loss = 0.1191, alignment_score = 0.8886', '\\n', 'Epoch 15:\nvalidation_loss = 0.1369, alignment_score = 0.8835', '\\n', 'Epoch 16:\nvalidation_loss = 0.1204, alignment_score = 0.8849', '\\n', 'Epoch 17:\nvalidation_loss = 0.1102, alignment_score = 0.8840', '\\n', 'Epoch 18:\nvalidation_loss = 0.1216, alignment_score = 0.8828', '\\n', 'Epoch 19:\nvalidation_loss = 0.1125, alignment_score = 0.8806', '\\n', 'Epoch 20:\nvalidation_loss = 0.1145, alignment_score = 0.8792', '\\n', 'Epoch 1:\nvalidation_loss = 0.2027, alignment_score = 0.9430', '\\n', 'Epoch 2:\nvalidation_loss = 0.1556, alignment_score = 0.9129', '\\n', 'Epoch 3:\nvalidation_loss = 0.1455, alignment_score = 0.9054', '\\n', 'Epoch 4:\nvalidation_loss = 0.1190, alignment_score = 0.8975', '\\n', 'Epoch 5:\nvalidation_loss = 0.0996, alignment_score = 0.8908', '\\n', 'Epoch 6:\nvalidation_loss = 0.1083, alignment_score = 0.8850', '\\n', 'Epoch 7:\nvalidation_loss = 0.0958, alignment_score = 0.8801', '\\n', 'Epoch 8:\nvalidation_loss = 0.0929, alignment_score = 0.8778', '\\n', 'Epoch 9:\nvalidation_loss = 0.0973, alignment_score = 0.8741', '\\n', 'Epoch 10:\nvalidation_loss = 0.0808, alignment_score = 0.8722', '\\n', 'Epoch 11:\nvalidation_loss = 0.0836, alignment_score = 0.8712', '\\n', 'Epoch 12:\nvalidation_loss = 0.0938, alignment_score = 0.8691', '\\n', 'Epoch 13:\nvalidation_loss = 0.0843, alignment_score = 0.8634', '\\n', 'Epoch 14:\nvalidation_loss = 0.0754, alignment_score = 0.8642', '\\n', 'Epoch 15:\nvalidation_loss = 0.0906, alignment_score = 0.8633', '\\n', 'Epoch 16:\nvalidation_loss = 0.0835, alignment_score = 0.8609', '\\n', 'Epoch 17:\nvalidation_loss = 0.0620, alignment_score = 0.8624', '\\n', 'Epoch 18:\nvalidation_loss = 0.0652, alignment_score = 0.8587', '\\n', 'Epoch 19:\nvalidation_loss = 0.0488, alignment_score = 0.8584', '\\n', 'Epoch 20:\nvalidation_loss = 0.0611, alignment_score = 0.8586', '\\n', 'Epoch 1:\nvalidation_loss = 0.2764, alignment_score = 0.9662', '\\n', 'Epoch 2:\nvalidation_loss = 0.1770, alignment_score = 0.9177', '\\n', 'Epoch 3:\nvalidation_loss = 0.1598, alignment_score = 0.9081', '\\n', 'Epoch 4:\nvalidation_loss = 0.1515, alignment_score = 0.9005', '\\n', 'Epoch 5:\nvalidation_loss = 0.1293, alignment_score = 0.8977', '\\n', 'Epoch 6:\nvalidation_loss = 0.1075, alignment_score = 0.8938', '\\n', 'Epoch 7:\nvalidation_loss = 0.1119, alignment_score = 0.8885', '\\n', 'Epoch 8:\nvalidation_loss = 0.0905, alignment_score = 0.8852', '\\n', 'Epoch 9:\nvalidation_loss = 0.0881, alignment_score = 0.8807', '\\n', 'Epoch 10:\nvalidation_loss = 0.0946, alignment_score = 0.8772', '\\n', 'Epoch 11:\nvalidation_loss = 0.0976, alignment_score = 0.8747', '\\n', 'Epoch 12:\nvalidation_loss = 0.0830, alignment_score = 0.8725', '\\n', 'Epoch 13:\nvalidation_loss = 0.0792, alignment_score = 0.8715', '\\n', 'Epoch 14:\nvalidation_loss = 0.0908, alignment_score = 0.8693', '\\n', 'Epoch 15:\nvalidation_loss = 0.0761, alignment_score = 0.8687', '\\n', 'Epoch 16:\nvalidation_loss = 0.0691, alignment_score = 0.8674', '\\n', 'Epoch 17:\nvalidation_loss = 0.0714, alignment_score = 0.8651', '\\n', 'Epoch 18:\nvalidation_loss = 0.0670, alignment_score = 0.8661', '\\n', 'Epoch 19:\nvalidation_loss = 0.0838, alignment_score = 0.8630', '\\n', 'Epoch 20:\nvalidation_loss = 0.0697, alignment_score = 0.8640', '\\n', 'Epoch 1:\nvalidation_loss = 0.4138, alignment_score = 0.9718', '\\n', 'Epoch 2:\nvalidation_loss = 0.2309, alignment_score = 0.9512', '\\n', 'Epoch 3:\nvalidation_loss = 0.1698, alignment_score = 0.9179', '\\n', 'Epoch 4:\nvalidation_loss = 0.1598, alignment_score = 0.9043', '\\n', 'Epoch 5:\nvalidation_loss = 0.1430, alignment_score = 0.9006', '\\n', 'Epoch 6:\nvalidation_loss = 0.1310, alignment_score = 0.8977', '\\n', 'Epoch 7:\nvalidation_loss = 0.1242, alignment_score = 0.8940', '\\n', 'Epoch 8:\nvalidation_loss = 0.1163, alignment_score = 0.8930', '\\n', 'Epoch 9:\nvalidation_loss = 0.1113, alignment_score = 0.8914', '\\n', 'Epoch 10:\nvalidation_loss = 0.1011, alignment_score = 0.8875', '\\n', 'Epoch 11:\nvalidation_loss = 0.0958, alignment_score = 0.8872', '\\n', 'Epoch 12:\nvalidation_loss = 0.0901, alignment_score = 0.8852', '\\n', 'Epoch 13:\nvalidation_loss = 0.0870, alignment_score = 0.8839', '\\n', 'Epoch 14:\nvalidation_loss = 0.0793, alignment_score = 0.8834', '\\n', 'Epoch 15:\nvalidation_loss = 0.0794, alignment_score = 0.8811', '\\n', 'Epoch 16:\nvalidation_loss = 0.0735, alignment_score = 0.8801', '\\n', 'Epoch 17:\nvalidation_loss = 0.0673, alignment_score = 0.8781', '\\n', 'Epoch 18:\nvalidation_loss = 0.0651, alignment_score = 0.8770', '\\n', 'Epoch 19:\nvalidation_loss = 0.0635, alignment_score = 0.8758', '\\n', 'Epoch 20:\nvalidation_loss = 0.0602, alignment_score = 0.8747', '\\n', 'Execution time: 44\nseconds seconds (time limit is an hour).']", "['Execution time: 4 seconds seconds (time limit is an hour).']", "['Execution time: 42 seconds seconds (time limit is an hour).']", "['Execution time: 13 seconds seconds (time limit is an hour).']", "['Execution time: 8 seconds seconds (time limit is an hour).']", "['Execution time: 15 seconds seconds (time limit is an hour).']", "['Execution time: 57 seconds seconds (time limit is an hour).']", "['runfile.py:134: RuntimeWarning: invalid value encountered in divide\\n\np_train_n /= p_train_n.sum(axis=1, keepdims=True)\\n', 'runfile.py:135:\nRuntimeWarning: invalid value encountered in divide\\n  p_val_n /=\np_val_n.sum(axis=1, keepdims=True)\\n', 'runfile.py:136: RuntimeWarning: invalid\nvalue encountered in divide\\n  p_test_n /= p_test_n.sum(axis=1,\nkeepdims=True)\\n', 'Execution time: 48 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 0.1014', '\\n', 'Epoch\n2: validation_loss = 0.0399', '\\n', 'Epoch 3: validation_loss = 0.0291', '\\n',\n'Epoch 4: validation_loss = 0.0271', '\\n', 'Epoch 5: validation_loss = 0.0243',\n'\\n', 'Epoch 6: validation_loss = 0.0174', '\\n', 'Epoch 7: validation_loss =\n0.0155', '\\n', 'Epoch 8: validation_loss = 0.0142', '\\n', 'Epoch 9:\nvalidation_loss = 0.0129', '\\n', 'Epoch 10: validation_loss = 0.0117', '\\n',\n'Epoch 11: validation_loss = 0.0119', '\\n', 'Epoch 12: validation_loss =\n0.0107', '\\n', 'Epoch 13: validation_loss = 0.0128', '\\n', 'Epoch 14:\nvalidation_loss = 0.0144', '\\n', 'Epoch 15: validation_loss = 0.0106', '\\n',\n'Epoch 16: validation_loss = 0.0108', '\\n', 'Epoch 17: validation_loss =\n0.0097', '\\n', 'Epoch 18: validation_loss = 0.0081', '\\n', 'Epoch 19:\nvalidation_loss = 0.0162', '\\n', 'Epoch 20: validation_loss = 0.0095', '\\n',\n'Epoch 1: validation_loss = 0.1339', '\\n', 'Epoch 2: validation_loss = 0.0535',\n'\\n', 'Epoch 3: validation_loss = 0.0396', '\\n', 'Epoch 4: validation_loss =\n0.0319', '\\n', 'Epoch 5: validation_loss = 0.0273', '\\n', 'Epoch 6:\nvalidation_loss = 0.0245', '\\n', 'Epoch 7: validation_loss = 0.0232', '\\n',\n'Epoch 8: validation_loss = 0.0209', '\\n', 'Epoch 9: validation_loss = 0.0207',\n'\\n', 'Epoch 10: validation_loss = 0.0176', '\\n', 'Epoch 11: validation_loss =\n0.0159', '\\n', 'Epoch 12: validation_loss = 0.0149', '\\n', 'Epoch 13:\nvalidation_loss = 0.0164', '\\n', 'Epoch 14: validation_loss = 0.0136', '\\n',\n'Epoch 15: validation_loss = 0.0146', '\\n', 'Epoch 16: validation_loss =\n0.0122', '\\n', 'Epoch 17: validation_loss = 0.0122', '\\n', 'Epoch 18:\nvalidation_loss = 0.0115', '\\n', 'Epoch 19: validation_loss = 0.0120', '\\n',\n'Epoch 20: validation_loss = 0.0103', '\\n', 'Epoch 1: validation_loss = 0.4703',\n'\\n', 'Epoch 2: validation_loss = 0.1805', '\\n', 'Epoch 3: validation_loss =\n0.0820', '\\n', 'Epoch 4: validation_loss = 0.0562', '\\n', 'Epoch 5:\nvalidation_loss = 0.0453', '\\n', 'Epoch 6: validation_loss = 0.0392', '\\n',\n'Epoch 7: validation_loss = 0.0342', '\\n', 'Epoch 8: validation_loss = 0.0304',\n'\\n', 'Epoch 9: validation_loss = 0.0279', '\\n', 'Epoch 10: validation_loss =\n0.0258', '\\n', 'Epoch 11: validation_loss = 0.0240', '\\n', 'Epoch 12:\nvalidation_loss = 0.0225', '\\n', 'Epoch 13: validation_loss = 0.0211', '\\n',\n'Epoch 14: validation_loss = 0.0199', '\\n', 'Epoch 15: validation_loss =\n0.0189', '\\n', 'Epoch 16: validation_loss = 0.0184', '\\n', 'Epoch 17:\nvalidation_loss = 0.0172', '\\n', 'Epoch 18: validation_loss = 0.0165', '\\n',\n'Epoch 19: validation_loss = 0.0155', '\\n', 'Epoch 20: validation_loss =\n0.0157', '\\n', 'Epoch 1: validation_loss = 0.0746', '\\n', 'Epoch 2:\nvalidation_loss = 0.0382', '\\n', 'Epoch 3: validation_loss = 0.0290', '\\n',\n'Epoch 4: validation_loss = 0.0251', '\\n', 'Epoch 5: validation_loss = 0.0200',\n'\\n', 'Epoch 6: validation_loss = 0.0173', '\\n', 'Epoch 7: validation_loss =\n0.0170', '\\n', 'Epoch 8: validation_loss = 0.0142', '\\n', 'Epoch 9:\nvalidation_loss = 0.0146', '\\n', 'Epoch 10: validation_loss = 0.0136', '\\n',\n'Epoch 11: validation_loss = 0.0195', '\\n', 'Epoch 12: validation_loss =\n0.0120', '\\n', 'Epoch 13: validation_loss = 0.0238', '\\n', 'Epoch 14:\nvalidation_loss = 0.0109', '\\n', 'Epoch 15: validation_loss = 0.0092', '\\n',\n'Epoch 16: validation_loss = 0.0132', '\\n', 'Epoch 17: validation_loss =\n0.0080', '\\n', 'Epoch 18: validation_loss = 0.0078', '\\n', 'Epoch 19:\nvalidation_loss = 0.0079', '\\n', 'Epoch 20: validation_loss = 0.0074', '\\n',\n'Epoch 1: validation_loss = 0.1728', '\\n', 'Epoch 2: validation_loss = 0.0627',\n'\\n', 'Epoch 3: validation_loss = 0.0458', '\\n', 'Epoch 4: validation_loss =\n0.0359', '\\n', 'Epoch 5: validation_loss = 0.0290', '\\n', 'Epoch 6:\nvalidation_loss = 0.0269', '\\n', 'Epoch 7: validation_loss = 0.0241', '\\n',\n'Epoch 8: validation_loss = 0.0239', '\\n', 'Epoch 9: validation_loss = 0.0197',\n'\\n', 'Epoch 10: validation_loss = 0.0167', '\\n', 'Epoch 11: validation_loss =\n0.0194', '\\n', 'Epoch 12: validation_loss = 0.0185', '\\n', 'Epoch 13:\nvalidation_loss = 0.0143', '\\n', 'Epoch 14: validation_loss = 0.0140', '\\n',\n'Epoch 15: validation_loss = 0.0119', '\\n', 'Epoch 16: validation_loss =\n0.0157', '\\n', 'Epoch 17: validation_loss = 0.0108', '\\n', 'Epoch 18:\nvalidation_loss = 0.0105', '\\n', 'Epoch 19: validation_loss = 0.0110', '\\n',\n'Epoch 20: validation_loss = 0.0108', '\\n', 'Epoch 1: validation_loss = 0.4154',\n'\\n', 'Epoch 2: validation_loss = 0.1615', '\\n', 'Epoch 3: validation_loss =\n0.0818', '\\n', 'Epoch 4: validation_loss = 0.0579', '\\n', 'Epoch 5:\nvalidation_loss = 0.0474', '\\n', 'Epoch 6: validation_loss = 0.0411', '\\n',\n'Epoch 7: validation_loss = 0.0362', '\\n', 'Epoch 8: validation_loss = 0.0330',\n'\\n', 'Epoch 9: validation_loss = 0.0309', '\\n', 'Epoch 10: validation_loss =\n0.0277', '\\n', 'Epoch 11: validation_loss = 0.0262', '\\n', 'Epoch 12:\nvalidation_loss = 0.0248', '\\n', 'Epoch 13: validation_loss = 0.0231', '\\n',\n'Epoch 14: validation_loss = 0.0230', '\\n', 'Epoch 15: validation_loss =\n0.0206', '\\n', 'Epoch 16: validation_loss = 0.0205', '\\n', 'Epoch 17:\nvalidation_loss = 0.0199', '\\n', 'Epoch 18: validation_loss = 0.0190', '\\n',\n'Epoch 19: validation_loss = 0.0175', '\\n', 'Epoch 20: validation_loss =\n0.0171', '\\n', 'Epoch 1: validation_loss = 0.1092', '\\n', 'Epoch 2:\nvalidation_loss = 0.0548', '\\n', 'Epoch 3: validation_loss = 0.0444', '\\n',\n'Epoch 4: validation_loss = 0.0385', '\\n', 'Epoch 5: validation_loss = 0.0375',\n'\\n', 'Epoch 6: validation_loss = 0.0297', '\\n', 'Epoch 7: validation_loss =\n0.0330', '\\n', 'Epoch 8: validation_loss = 0.0305', '\\n', 'Epoch 9:\nvalidation_loss = 0.0240', '\\n', 'Epoch 10: validation_loss = 0.0207', '\\n',\n'Epoch 11: validation_loss = 0.0191', '\\n', 'Epoch 12: validation_loss =\n0.0179', '\\n', 'Epoch 13: validation_loss = 0.0164', '\\n', 'Epoch 14:\nvalidation_loss = 0.0144', '\\n', 'Epoch 15: validation_loss = 0.0182', '\\n',\n'Epoch 16: validation_loss = 0.0201', '\\n', 'Epoch 17: validation_loss =\n0.0135', '\\n', 'Epoch 18: validation_loss = 0.0143', '\\n', 'Epoch 19:\nvalidation_loss = 0.0151', '\\n', 'Epoch 20: validation_loss = 0.0144', '\\n',\n'Epoch 1: validation_loss = 0.2676', '\\n', 'Epoch 2: validation_loss = 0.0733',\n'\\n', 'Epoch 3: validation_loss = 0.0624', '\\n', 'Epoch 4: validation_loss =\n0.0470', '\\n', 'Epoch 5: validation_loss = 0.0426', '\\n', 'Epoch 6:\nvalidation_loss = 0.0405', '\\n', 'Epoch 7: validation_loss = 0.0388', '\\n',\n'Epoch 8: validation_loss = 0.0378', '\\n', 'Epoch 9: validation_loss = 0.0318',\n'\\n', 'Epoch 10: validation_loss = 0.0309', '\\n', 'Epoch 11: validation_loss =\n0.0261', '\\n', 'Epoch 12: validation_loss = 0.0243', '\\n', 'Epoch 13:\nvalidation_loss = 0.0220', '\\n', 'Epoch 14: validation_loss = 0.0253', '\\n',\n'Epoch 15: validation_loss = 0.0220', '\\n', 'Epoch 16: validation_loss =\n0.0185', '\\n', 'Epoch 17: validation_loss = 0.0206', '\\n', 'Epoch 18:\nvalidation_loss = 0.0167', '\\n', 'Epoch 19: validation_loss = 0.0174', '\\n',\n'Epoch 20: validation_loss = 0.0213', '\\n', 'Epoch 1: validation_loss = 0.4470',\n'\\n', 'Epoch 2: validation_loss = 0.2094', '\\n', 'Epoch 3: validation_loss =\n0.1080', '\\n', 'Epoch 4: validation_loss = 0.0729', '\\n', 'Epoch 5:\nvalidation_loss = 0.0600', '\\n', 'Epoch 6: validation_loss = 0.0546', '\\n',\n'Epoch 7: validation_loss = 0.0495', '\\n', 'Epoch 8: validation_loss = 0.0470',\n'\\n', 'Epoch 9: validation_loss = 0.0432', '\\n', 'Epoch 10: validation_loss =\n0.0426', '\\n', 'Epoch 11: validation_loss = 0.0411', '\\n', 'Epoch 12:\nvalidation_loss = 0.0371', '\\n', 'Epoch 13: validation_loss = 0.0355', '\\n',\n'Epoch 14: validation_loss = 0.0341', '\\n', 'Epoch 15: validation_loss =\n0.0326', '\\n', 'Epoch 16: validation_loss = 0.0321', '\\n', 'Epoch 17:\nvalidation_loss = 0.0290', '\\n', 'Epoch 18: validation_loss = 0.0300', '\\n',\n'Epoch 19: validation_loss = 0.0286', '\\n', 'Epoch 20: validation_loss =\n0.0285', '\\n', 'Epoch 1: validation_loss = 0.2230', '\\n', 'Epoch 2:\nvalidation_loss = 0.1310', '\\n', 'Epoch 3: validation_loss = 0.0926', '\\n',\n'Epoch 4: validation_loss = 0.0718', '\\n', 'Epoch 5: validation_loss = 0.0699',\n'\\n', 'Epoch 6: validation_loss = 0.0610', '\\n', 'Epoch 7: validation_loss =\n0.0525', '\\n', 'Epoch 8: validation_loss = 0.0501', '\\n', 'Epoch 9:\nvalidation_loss = 0.0549', '\\n', 'Epoch 10: validation_loss = 0.0402', '\\n',\n'Epoch 11: validation_loss = 0.0469', '\\n', 'Epoch 12: validation_loss =\n0.0458', '\\n', 'Epoch 13: validation_loss = 0.0459', '\\n', 'Epoch 14:\nvalidation_loss = 0.0344', '\\n', 'Epoch 15: validation_loss = 0.0337', '\\n',\n'Epoch 16: validation_loss = 0.0325', '\\n', 'Epoch 17: validation_loss =\n0.0393', '\\n', 'Epoch 18: validation_loss = 0.1109', '\\n', 'Epoch 19:\nvalidation_loss = 0.0378', '\\n', 'Epoch 20: validation_loss = 0.0287', '\\n',\n'Epoch 1: validation_loss = 0.3481', '\\n', 'Epoch 2: validation_loss = 0.2537',\n'\\n', 'Epoch 3: validation_loss = 0.1663', '\\n', 'Epoch 4: validation_loss =\n0.1185', '\\n', 'Epoch 5: validation_loss = 0.0997', '\\n', 'Epoch 6:\nvalidation_loss = 0.0807', '\\n', 'Epoch 7: validation_loss = 0.0738', '\\n',\n'Epoch 8: validation_loss = 0.0686', '\\n', 'Epoch 9: validation_loss = 0.0609',\n'\\n', 'Epoch 10: validation_loss = 0.0570', '\\n', 'Epoch 11: validation_loss =\n0.0568', '\\n', 'Epoch 12: validation_loss = 0.0455', '\\n', 'Epoch 13:\nvalidation_loss = 0.0498', '\\n', 'Epoch 14: validation_loss = 0.0448', '\\n',\n'Epoch 15: validation_loss = 0.0466', '\\n', 'Epoch 16: validation_loss =\n0.0429', '\\n', 'Epoch 17: validation_loss = 0.0393', '\\n', 'Epoch 18:\nvalidation_loss = 0.0392', '\\n', 'Epoch 19: validation_loss = 0.0397', '\\n',\n'Epoch 20: validation_loss = 0.0378', '\\n', 'Epoch 1: validation_loss = 0.4240',\n'\\n', 'Epoch 2: validation_loss = 0.3001', '\\n', 'Epoch 3: validation_loss =\n0.2473', '\\n', 'Epoch 4: validation_loss = 0.1968', '\\n', 'Epoch 5:\nvalidation_loss = 0.1566', '\\n', 'Epoch 6: validation_loss = 0.1309', '\\n',\n'Epoch 7: validation_loss = 0.1117', '\\n', 'Epoch 8: validation_loss = 0.0969',\n'\\n', 'Epoch 9: validation_loss = 0.0910', '\\n', 'Epoch 10: validation_loss =\n0.0796', '\\n', 'Epoch 11: validation_loss = 0.0738', '\\n', 'Epoch 12:\nvalidation_loss = 0.0658', '\\n', 'Epoch 13: validation_loss = 0.0643', '\\n',\n'Epoch 14: validation_loss = 0.0570', '\\n', 'Epoch 15: validation_loss =\n0.0558', '\\n', 'Epoch 16: validation_loss = 0.0558', '\\n', 'Epoch 17:\nvalidation_loss = 0.0535', '\\n', 'Epoch 18: validation_loss = 0.0485', '\\n',\n'Epoch 19: validation_loss = 0.0499', '\\n', 'Epoch 20: validation_loss =\n0.0473', '\\n', 'Epoch 1: validation_loss = 0.0602', '\\n', 'Epoch 2:\nvalidation_loss = 0.0406', '\\n', 'Epoch 3: validation_loss = 0.0294', '\\n',\n'Epoch 4: validation_loss = 0.0239', '\\n', 'Epoch 5: validation_loss = 0.0206',\n'\\n', 'Epoch 6: validation_loss = 0.0181', '\\n', 'Epoch 7: validation_loss =\n0.0215', '\\n', 'Epoch 8: validation_loss = 0.0157', '\\n', 'Epoch 9:\nvalidation_loss = 0.0145', '\\n', 'Epoch 10: validation_loss = 0.0133', '\\n',\n'Epoch 11: validation_loss = 0.0149', '\\n', 'Epoch 12: validation_loss =\n0.0123', '\\n', 'Epoch 13: validation_loss = 0.0107', '\\n', 'Epoch 14:\nvalidation_loss = 0.0101', '\\n', 'Epoch 15: validation_loss = 0.0115', '\\n',\n'Epoch 16: validation_loss = 0.0172', '\\n', 'Epoch 17: validation_loss =\n0.0134', '\\n', 'Epoch 18: validation_loss = 0.0131', '\\n', 'Epoch 19:\nvalidation_loss = 0.0122', '\\n', 'Epoch 20: validation_loss = 0.0119', '\\n',\n'Epoch 1: validation_loss = 0.1820', '\\n', 'Epoch 2: validation_loss = 0.0662',\n'\\n', 'Epoch 3: validation_loss = 0.0472', '\\n', 'Epoch 4: validation_loss =\n0.0371', '\\n', 'Epoch 5: validation_loss = 0.0332', '\\n', 'Epoch 6:\nvalidation_loss = 0.0291', '\\n', 'Epoch 7: validation_loss = 0.0249', '\\n',\n'Epoch 8: validation_loss = 0.0238', '\\n', 'Epoch 9: validation_loss = 0.0214',\n'\\n', 'Epoch 10: validation_loss = 0.0214', '\\n', 'Epoch 11: validation_loss =\n0.0181', '\\n', 'Epoch 12: validation_loss = 0.0212', '\\n', 'Epoch 13:\nvalidation_loss = 0.0183', '\\n', 'Epoch 14: validation_loss = 0.0177', '\\n',\n'Epoch 15: validation_loss = 0.0155', '\\n', 'Epoch 16: validation_loss =\n0.0166', '\\n', 'Epoch 17: validation_loss = 0.0188', '\\n', 'Epoch 18:\nvalidation_loss = 0.0155', '\\n', 'Epoch 19: validation_loss = 0.0127', '\\n',\n'Epoch 20: validation_loss = 0.0150', '\\n', 'Epoch 1: validation_loss = 0.4812',\n'\\n', 'Epoch 2: validation_loss = 0.1823', '\\n', 'Epoch 3: validation_loss =\n0.0814', '\\n', 'Epoch 4: validation_loss = 0.0540', '\\n', 'Epoch 5:\nvalidation_loss = 0.0426', '\\n', 'Epoch 6: validation_loss = 0.0371', '\\n',\n'Epoch 7: validation_loss = 0.0332', '\\n', 'Epoch 8: validation_loss = 0.0306',\n'\\n', 'Epoch 9: validation_loss = 0.0276', '\\n', 'Epoch 10: validation_loss =\n0.0267', '\\n', 'Epoch 11: validation_loss = 0.0245', '\\n', 'Epoch 12:\nvalidation_loss = 0.0233', '\\n', 'Epoch 13: validation_loss = 0.0217', '\\n',\n'Epoch 14: validation_loss = 0.0204', '\\n', 'Epoch 15: validation_loss =\n0.0210', '\\n', 'Epoch 16: validation_loss = 0.0194', '\\n', 'Epoch 17:\nvalidation_loss = 0.0193', '\\n', 'Epoch 18: validation_loss = 0.0186', '\\n',\n'Epoch 19: validation_loss = 0.0172', '\\n', 'Epoch 20: validation_loss =\n0.0164', '\\n', 'Epoch 1: validation_loss = 0.0680', '\\n', 'Epoch 2:\nvalidation_loss = 0.0432', '\\n', 'Epoch 3: validation_loss = 0.0384', '\\n',\n'Epoch 4: validation_loss = 0.0314', '\\n', 'Epoch 5: validation_loss = 0.0293',\n'\\n', 'Epoch 6: validation_loss = 0.0263', '\\n', 'Epoch 7: validation_loss =\n0.0296', '\\n', 'Epoch 8: validation_loss = 0.0239', '\\n', 'Epoch 9:\nvalidation_loss = 0.0242', '\\n', 'Epoch 10: validation_loss = 0.0229', '\\n',\n'Epoch 11: validation_loss = 0.0222', '\\n', 'Epoch 12: validation_loss =\n0.0203', '\\n', 'Epoch 13: validation_loss = 0.0241', '\\n', 'Epoch 14:\nvalidation_loss = 0.0198', '\\n', 'Epoch 15: validation_loss = 0.0183', '\\n',\n'Epoch 16: validation_loss = 0.0203', '\\n', 'Epoch 17: validation_loss =\n0.0279', '\\n', 'Epoch 18: validation_loss = 0.0170', '\\n', 'Epoch 19:\nvalidation_loss = 0.0187', '\\n', 'Epoch 20: validation_loss = 0.0162', '\\n',\n'Epoch 1: validation_loss = 0.2848', '\\n', 'Epoch 2: validation_loss = 0.1654',\n'\\n', 'Epoch 3: validation_loss = 0.1120', '\\n', 'Epoch 4: validation_loss =\n0.0859', '\\n', 'Epoch 5: validation_loss = 0.0709', '\\n', 'Epoch 6:\nvalidation_loss = 0.0607', '\\n', 'Epoch 7: validation_loss = 0.0527', '\\n',\n'Epoch 8: validation_loss = 0.0469', '\\n', 'Epoch 9: validation_loss = 0.0415',\n'\\n', 'Epoch 10: validation_loss = 0.0377', '\\n', 'Epoch 11: validation_loss =\n0.0354', '\\n', 'Epoch 12: validation_loss = 0.0324', '\\n', 'Epoch 13:\nvalidation_loss = 0.0304', '\\n', 'Epoch 14: validation_loss = 0.0313', '\\n',\n'Epoch 15: validation_loss = 0.0271', '\\n', 'Epoch 16: validation_loss =\n0.0305', '\\n', 'Epoch 17: validation_loss = 0.0254', '\\n', 'Epoch 18:\nvalidation_loss = 0.0241', '\\n', 'Epoch 19: validation_loss = 0.0236', '\\n',\n'Epoch 20: validation_loss = 0.0223', '\\n', 'Epoch 1: validation_loss = 0.3635',\n'\\n', 'Epoch 2: validation_loss = 0.1211', '\\n', 'Epoch 3: validation_loss =\n0.0702', '\\n', 'Epoch 4: validation_loss = 0.0567', '\\n', 'Epoch 5:\nvalidation_loss = 0.0487', '\\n', 'Epoch 6: validation_loss = 0.0444', '\\n',\n'Epoch 7: validation_loss = 0.0409', '\\n', 'Epoch 8: validation_loss = 0.0386',\n'\\n', 'Epoch 9: validation_loss = 0.0362', '\\n', 'Epoch 10: validation_loss =\n0.0346', '\\n', 'Epoch 11: validation_loss = 0.0332', '\\n', 'Epoch 12:\nvalidation_loss = 0.0319', '\\n', 'Epoch 13: validation_loss = 0.0314', '\\n',\n'Epoch 14: validation_loss = 0.0302', '\\n', 'Epoch 15: validation_loss =\n0.0299', '\\n', 'Epoch 16: validation_loss = 0.0288', '\\n', 'Epoch 17:\nvalidation_loss = 0.0279', '\\n', 'Epoch 18: validation_loss = 0.0273', '\\n',\n'Epoch 19: validation_loss = 0.0271', '\\n', 'Epoch 20: validation_loss =\n0.0262', '\\n', 'Epoch 1: validation_loss = 0.0821', '\\n', 'Epoch 2:\nvalidation_loss = 0.0664', '\\n', 'Epoch 3: validation_loss = 0.0573', '\\n',\n'Epoch 4: validation_loss = 0.0528', '\\n', 'Epoch 5: validation_loss = 0.0327',\n'\\n', 'Epoch 6: validation_loss = 0.0247', '\\n', 'Epoch 7: validation_loss =\n0.0484', '\\n', 'Epoch 8: validation_loss = 0.0197', '\\n', 'Epoch 9:\nvalidation_loss = 0.0246', '\\n', 'Epoch 10: validation_loss = 0.0202', '\\n',\n'Epoch 11: validation_loss = 0.0104', '\\n', 'Epoch 12: validation_loss =\n0.0198', '\\n', 'Epoch 13: validation_loss = 0.0176', '\\n', 'Epoch 14:\nvalidation_loss = 0.0129', '\\n', 'Epoch 15: validation_loss = 0.0090', '\\n',\n'Epoch 16: validation_loss = 0.0077', '\\n', 'Epoch 17: validation_loss =\n0.0093', '\\n', 'Epoch 18: validation_loss = 0.0171', '\\n', 'Epoch 19:\nvalidation_loss = 0.0059', '\\n', 'Epoch 20: validation_loss = 0.0057', '\\n',\n'Epoch 1: validation_loss = 0.1645', '\\n', 'Epoch 2: validation_loss = 0.0651',\n'\\n', 'Epoch 3: validation_loss = 0.0581', '\\n', 'Epoch 4: validation_loss =\n0.0429', '\\n', 'Epoch 5: validation_loss = 0.0403', '\\n', 'Epoch 6:\nvalidation_loss = 0.0312', '\\n', 'Epoch 7: validation_loss = 0.0299', '\\n',\n'Epoch 8: validation_loss = 0.0272', '\\n', 'Epoch 9: validation_loss = 0.0211',\n'\\n', 'Epoch 10: validation_loss = 0.0254', '\\n', 'Epoch 11: validation_loss =\n0.0176', '\\n', 'Epoch 12: validation_loss = 0.0218', '\\n', 'Epoch 13:\nvalidation_loss = 0.0198', '\\n', 'Epoch 14: validation_loss = 0.0197', '\\n',\n'Epoch 15: validation_loss = 0.0137', '\\n', 'Epoch 16: validation_loss =\n0.0180', '\\n', 'Epoch 17: validation_loss = 0.0163', '\\n', 'Epoch 18:\nvalidation_loss = 0.0138', '\\n', 'Epoch 19: validation_loss = 0.0112', '\\n',\n'Epoch 20: validation_loss = 0.0134', '\\n', 'Epoch 1: validation_loss = 0.3457',\n'\\n', 'Epoch 2: validation_loss = 0.1432', '\\n', 'Epoch 3: validation_loss =\n0.0777', '\\n', 'Epoch 4: validation_loss = 0.0583', '\\n', 'Epoch 5:\nvalidation_loss = 0.0522', '\\n', 'Epoch 6: validation_loss = 0.0425', '\\n',\n'Epoch 7: validation_loss = 0.0413', '\\n', 'Epoch 8: validation_loss = 0.0342',\n'\\n', 'Epoch 9: validation_loss = 0.0360', '\\n', 'Epoch 10: validation_loss =\n0.0344', '\\n', 'Epoch 11: validation_loss = 0.0271', '\\n', 'Epoch 12:\nvalidation_loss = 0.0266', '\\n', 'Epoch 13: validation_loss = 0.0256', '\\n',\n'Epoch 14: validation_loss = 0.0246', '\\n', 'Epoch 15: validation_loss =\n0.0250', '\\n', 'Epoch 16: validation_loss = 0.0204', '\\n', 'Epoch 17:\nvalidation_loss = 0.0194', '\\n', 'Epoch 18: validation_loss = 0.0210', '\\n',\n'Epoch 19: validation_loss = 0.0178', '\\n', 'Epoch 20: validation_loss =\n0.0179', '\\n', 'Epoch 1: validation_loss = 0.2273', '\\n', 'Epoch 2:\nvalidation_loss = 0.1136', '\\n', 'Epoch 3: validation_loss = 0.0901', '\\n',\n'Epoch 4: validation_loss = 0.0847', '\\n', 'Epoch 5: validation_loss = 0.0683',\n'\\n', 'Epoch 6: validation_loss = 0.0583', '\\n', 'Epoch 7: validation_loss =\n0.0704', '\\n', 'Epoch 8: validation_loss = 0.0434', '\\n', 'Epoch 9:\nvalidation_loss = 0.0452', '\\n', 'Epoch 10: validation_loss = 0.0458', '\\n',\n'Epoch 11: validation_loss = 0.0345', '\\n', 'Epoch 12: validation_loss =\n0.0441', '\\n', 'Epoch 13: validation_loss = 0.0682', '\\n', 'Epoch 14:\nvalidation_loss = 0.0352', '\\n', 'Epoch 15: validation_loss = 0.0343', '\\n',\n'Epoch 16: validation_loss = 0.0355', '\\n', 'Epoch 17: validation_loss =\n0.0545', '\\n', 'Epoch 18: validation_loss = 0.0334', '\\n', 'Epoch 19:\nvalidation_loss = 0.0386', '\\n', 'Epoch 20: validation_loss = 0.0397', '\\n',\n'Epoch 1: validation_loss = 0.4284', '\\n', 'Epoch 2: validation_loss = 0.2550',\n'\\n', 'Epoch 3: validation_loss = 0.1655', '\\n', 'Epoch 4: validation_loss =\n0.1143', '\\n', 'Epoch 5: validation_loss = 0.0959', '\\n', 'Epoch 6:\nvalidation_loss = 0.0799', '\\n', 'Epoch 7: validation_loss = 0.0756', '\\n',\n'Epoch 8: validation_loss = 0.0812', '\\n', 'Epoch 9: validation_loss = 0.0554',\n'\\n', 'Epoch 10: validation_loss = 0.0517', '\\n', 'Epoch 11: validation_loss =\n0.0532', '\\n', 'Epoch 12: validation_loss = 0.0585', '\\n', 'Epoch 13:\nvalidation_loss = 0.0509', '\\n', 'Epoch 14: validation_loss = 0.0404', '\\n',\n'Epoch 15: validation_loss = 0.0480', '\\n', 'Epoch 16: validation_loss =\n0.0406', '\\n', 'Epoch 17: validation_loss = 0.0328', '\\n', 'Epoch 18:\nvalidation_loss = 0.0441', '\\n', 'Epoch 19: validation_loss = 0.0402', '\\n',\n'Epoch 20: validation_loss = 0.0437', '\\n', 'Epoch 1: validation_loss = 0.4376',\n'\\n', 'Epoch 2: validation_loss = 0.3332', '\\n', 'Epoch 3: validation_loss =\n0.2769', '\\n', 'Epoch 4: validation_loss = 0.2109', '\\n', 'Epoch 5:\nvalidation_loss = 0.1499', '\\n', 'Epoch 6: validation_loss = 0.1205', '\\n',\n'Epoch 7: validation_loss = 0.1091', '\\n', 'Epoch 8: validation_loss = 0.0923',\n'\\n', 'Epoch 9: validation_loss = 0.0825', '\\n', 'Epoch 10: validation_loss =\n0.0815', '\\n', 'Epoch 11: validation_loss = 0.0691', '\\n', 'Epoch 12:\nvalidation_loss = 0.0656', '\\n', 'Epoch 13: validation_loss = 0.0582', '\\n',\n'Epoch 14: validation_loss = 0.0654', '\\n', 'Epoch 15: validation_loss =\n0.0534', '\\n', 'Epoch 16: validation_loss = 0.0513', '\\n', 'Epoch 17:\nvalidation_loss = 0.0548', '\\n', 'Epoch 18: validation_loss = 0.0444', '\\n',\n'Epoch 19: validation_loss = 0.0505', '\\n', 'Epoch 20: validation_loss =\n0.0425', '\\n', 'Epoch 1: validation_loss = 0.0559', '\\n', 'Epoch 2:\nvalidation_loss = 0.0327', '\\n', 'Epoch 3: validation_loss = 0.0222', '\\n',\n'Epoch 4: validation_loss = 0.0216', '\\n', 'Epoch 5: validation_loss = 0.0188',\n'\\n', 'Epoch 6: validation_loss = 0.0247', '\\n', 'Epoch 7: validation_loss =\n0.0302', '\\n', 'Epoch 8: validation_loss = 0.0192', '\\n', 'Epoch 9:\nvalidation_loss = 0.0306', '\\n', 'Epoch 10: validation_loss = 0.0293', '\\n',\n'Epoch 11: validation_loss = 0.0186', '\\n', 'Epoch 12: validation_loss =\n0.0156', '\\n', 'Epoch 13: validation_loss = 0.0231', '\\n', 'Epoch 14:\nvalidation_loss = 0.0205', '\\n', 'Epoch 15: validation_loss = 0.0205', '\\n',\n'Epoch 16: validation_loss = 0.0407', '\\n', 'Epoch 17: validation_loss =\n0.0459', '\\n', 'Epoch 18: validation_loss = 0.0283', '\\n', 'Epoch 19:\nvalidation_loss = 0.0183', '\\n', 'Epoch 20: validation_loss = 0.0135', '\\n',\n'Epoch 1: validation_loss = 0.2288', '\\n', 'Epoch 2: validation_loss = 0.0645',\n'\\n', 'Epoch 3: validation_loss = 0.0379', '\\n', 'Epoch 4: validation_loss =\n0.0327', '\\n', 'Epoch 5: validation_loss = 0.0347', '\\n', 'Epoch 6:\nvalidation_loss = 0.0210', '\\n', 'Epoch 7: validation_loss = 0.0167', '\\n',\n'Epoch 8: validation_loss = 0.0291', '\\n', 'Epoch 9: validation_loss = 0.0272',\n'\\n', 'Epoch 10: validation_loss = 0.0237', '\\n', 'Epoch 11: validation_loss =\n0.0171', '\\n', 'Epoch 12: validation_loss = 0.0289', '\\n', 'Epoch 13:\nvalidation_loss = 0.0185', '\\n', 'Epoch 14: validation_loss = 0.0199', '\\n',\n'Epoch 15: validation_loss = 0.0202', '\\n', 'Epoch 16: validation_loss =\n0.0169', '\\n', 'Epoch 17: validation_loss = 0.0310', '\\n', 'Epoch 18:\nvalidation_loss = 0.0272', '\\n', 'Epoch 19: validation_loss = 0.0172', '\\n',\n'Epoch 20: validation_loss = 0.0209', '\\n', 'Epoch 1: validation_loss = 0.2117',\n'\\n', 'Epoch 2: validation_loss = 0.0829', '\\n', 'Epoch 3: validation_loss =\n0.0524', '\\n', 'Epoch 4: validation_loss = 0.0425', '\\n', 'Epoch 5:\nvalidation_loss = 0.0371', '\\n', 'Epoch 6: validation_loss = 0.0323', '\\n',\n'Epoch 7: validation_loss = 0.0303', '\\n', 'Epoch 8: validation_loss = 0.0256',\n'\\n', 'Epoch 9: validation_loss = 0.0262', '\\n', 'Epoch 10: validation_loss =\n0.0250', '\\n', 'Epoch 11: validation_loss = 0.0237', '\\n', 'Epoch 12:\nvalidation_loss = 0.0247', '\\n', 'Epoch 13: validation_loss = 0.0242', '\\n',\n'Epoch 14: validation_loss = 0.0189', '\\n', 'Epoch 15: validation_loss =\n0.0258', '\\n', 'Epoch 16: validation_loss = 0.0184', '\\n', 'Epoch 17:\nvalidation_loss = 0.0235', '\\n', 'Epoch 18: validation_loss = 0.0169', '\\n',\n'Epoch 19: validation_loss = 0.0211', '\\n', 'Epoch 20: validation_loss =\n0.0169', '\\n', 'Epoch 1: validation_loss = 0.0907', '\\n', 'Epoch 2:\nvalidation_loss = 0.0294', '\\n', 'Epoch 3: validation_loss = 0.0231', '\\n',\n'Epoch 4: validation_loss = 0.0209', '\\n', 'Epoch 5: validation_loss = 0.0137',\n'\\n', 'Epoch 6: validation_loss = 0.0123', '\\n', 'Epoch 7: validation_loss =\n0.0197', '\\n', 'Epoch 8: validation_loss = 0.0183', '\\n', 'Epoch 9:\nvalidation_loss = 0.0213', '\\n', 'Epoch 10: validation_loss = 0.0162', '\\n',\n'Epoch 11: validation_loss = 0.0218', '\\n', 'Epoch 12: validation_loss =\n0.0124', '\\n', 'Epoch 13: validation_loss = 0.0087', '\\n', 'Epoch 14:\nvalidation_loss = 0.0123', '\\n', 'Epoch 15: validation_loss = 0.0135', '\\n',\n'Epoch 16: validation_loss = 0.0063', '\\n', 'Epoch 17: validation_loss =\n0.0096', '\\n', 'Epoch 18: validation_loss = 0.0395', '\\n', 'Epoch 19:\nvalidation_loss = 0.0088', '\\n', 'Epoch 20: validation_loss = 0.0098', '\\n',\n'Epoch 1: validation_loss = 0.1936', '\\n', 'Epoch 2: validation_loss = 0.0720',\n'\\n', 'Epoch 3: validation_loss = 0.0451', '\\n', 'Epoch 4: validation_loss =\n0.0317', '\\n', 'Epoch 5: validation_loss = 0.0320', '\\n', 'Epoch 6:\nvalidation_loss = 0.0273', '\\n', 'Epoch 7: validation_loss = 0.0226', '\\n',\n'Epoch 8: validation_loss = 0.0160', '\\n', 'Epoch 9: validation_loss = 0.0209',\n'\\n', 'Epoch 10: validation_loss = 0.0222', '\\n', 'Epoch 11: validation_loss =\n0.0126', '\\n', 'Epoch 12: validation_loss = 0.0138', '\\n', 'Epoch 13:\nvalidation_loss = 0.0117', '\\n', 'Epoch 14: validation_loss = 0.0139', '\\n',\n'Epoch 15: validation_loss = 0.0129', '\\n', 'Epoch 16: validation_loss =\n0.0220', '\\n', 'Epoch 17: validation_loss = 0.0112', '\\n', 'Epoch 18:\nvalidation_loss = 0.0088', '\\n', 'Epoch 19: validation_loss = 0.0150', '\\n',\n'Epoch 20: validation_loss = 0.0102', '\\n', 'Epoch 1: validation_loss = 0.2791',\n'\\n', 'Epoch 2: validation_loss = 0.1028', '\\n', 'Epoch 3: validation_loss =\n0.0601', '\\n', 'Epoch 4: validation_loss = 0.0423', '\\n', 'Epoch 5:\nvalidation_loss = 0.0342', '\\n', 'Epoch 6: validation_loss = 0.0301', '\\n',\n'Epoch 7: validation_loss = 0.0230', '\\n', 'Epoch 8: validation_loss = 0.0229',\n'\\n', 'Epoch 9: validation_loss = 0.0204', '\\n', 'Epoch 10: validation_loss =\n0.0211', '\\n', 'Epoch 11: validation_loss = 0.0190', '\\n', 'Epoch 12:\nvalidation_loss = 0.0178', '\\n', 'Epoch 13: validation_loss = 0.0175', '\\n',\n'Epoch 14: validation_loss = 0.0152', '\\n', 'Epoch 15: validation_loss =\n0.0204', '\\n', 'Epoch 16: validation_loss = 0.0142', '\\n', 'Epoch 17:\nvalidation_loss = 0.0160', '\\n', 'Epoch 18: validation_loss = 0.0143', '\\n',\n'Epoch 19: validation_loss = 0.0125', '\\n', 'Epoch 20: validation_loss =\n0.0156', '\\n', 'Epoch 1: validation_loss = 0.0618', '\\n', 'Epoch 2:\nvalidation_loss = 0.0305', '\\n', 'Epoch 3: validation_loss = 0.0248', '\\n',\n'Epoch 4: validation_loss = 0.0219', '\\n', 'Epoch 5: validation_loss = 0.0182',\n'\\n', 'Epoch 6: validation_loss = 0.0154', '\\n', 'Epoch 7: validation_loss =\n0.0193', '\\n', 'Epoch 8: validation_loss = 0.0172', '\\n', 'Epoch 9:\nvalidation_loss = 0.0126', '\\n', 'Epoch 10: validation_loss = 0.0130', '\\n',\n'Epoch 11: validation_loss = 0.0135', '\\n', 'Epoch 12: validation_loss =\n0.0108', '\\n', 'Epoch 13: validation_loss = 0.0115', '\\n', 'Epoch 14:\nvalidation_loss = 0.0161', '\\n', 'Epoch 15: validation_loss = 0.0101', '\\n',\n'Epoch 16: validation_loss = 0.0094', '\\n', 'Epoch 17: validation_loss =\n0.0175', '\\n', 'Epoch 18: validation_loss = 0.0119', '\\n', 'Epoch 19:\nvalidation_loss = 0.0084', '\\n', 'Epoch 20: validation_loss = 0.0084', '\\n',\n'Epoch 1: validation_loss = 0.1512', '\\n', 'Epoch 2: validation_loss = 0.0595',\n'\\n', 'Epoch 3: validation_loss = 0.0428', '\\n', 'Epoch 4: validation_loss =\n0.0353', '\\n', 'Epoch 5: validation_loss = 0.0311', '\\n', 'Epoch 6:\nvalidation_loss = 0.0290', '\\n', 'Epoch 7: validation_loss = 0.0264', '\\n',\n'Epoch 8: validation_loss = 0.0220', '\\n', 'Epoch 9: validation_loss = 0.0213',\n'\\n', 'Epoch 10: validation_loss = 0.0205', '\\n', 'Epoch 11: validation_loss =\n0.0186', '\\n', 'Epoch 12: validation_loss = 0.0161', '\\n', 'Epoch 13:\nvalidation_loss = 0.0156', '\\n', 'Epoch 14: validation_loss = 0.0148', '\\n',\n'Epoch 15: validation_loss = 0.0168', '\\n', 'Epoch 16: validation_loss =\n0.0164', '\\n', 'Epoch 17: validation_loss = 0.0129', '\\n', 'Epoch 18:\nvalidation_loss = 0.0136', '\\n', 'Epoch 19: validation_loss = 0.0141', '\\n',\n'Epoch 20: validation_loss = 0.0124', '\\n', 'Epoch 1: validation_loss = 0.2831',\n'\\n', 'Epoch 2: validation_loss = 0.1136', '\\n', 'Epoch 3: validation_loss =\n0.0667', '\\n', 'Epoch 4: validation_loss = 0.0496', '\\n', 'Epoch 5:\nvalidation_loss = 0.0426', '\\n', 'Epoch 6: validation_loss = 0.0361', '\\n',\n'Epoch 7: validation_loss = 0.0329', '\\n', 'Epoch 8: validation_loss = 0.0303',\n'\\n', 'Epoch 9: validation_loss = 0.0275', '\\n', 'Epoch 10: validation_loss =\n0.0249', '\\n', 'Epoch 11: validation_loss = 0.0251', '\\n', 'Epoch 12:\nvalidation_loss = 0.0219', '\\n', 'Epoch 13: validation_loss = 0.0213', '\\n',\n'Epoch 14: validation_loss = 0.0198', '\\n', 'Epoch 15: validation_loss =\n0.0184', '\\n', 'Epoch 16: validation_loss = 0.0197', '\\n', 'Epoch 17:\nvalidation_loss = 0.0170', '\\n', 'Epoch 18: validation_loss = 0.0156', '\\n',\n'Epoch 19: validation_loss = 0.0175', '\\n', 'Epoch 20: validation_loss =\n0.0157', '\\n', 'Epoch 1: validation_loss = 0.1649', '\\n', 'Epoch 2:\nvalidation_loss = 0.0974', '\\n', 'Epoch 3: validation_loss = 0.0744', '\\n',\n'Epoch 4: validation_loss = 0.0622', '\\n', 'Epoch 5: validation_loss = 0.0482',\n'\\n', 'Epoch 6: validation_loss = 0.0428', '\\n', 'Epoch 7: validation_loss =\n0.0595', '\\n', 'Epoch 8: validation_loss = 0.0547', '\\n', 'Epoch 9:\nvalidation_loss = 0.0473', '\\n', 'Epoch 10: validation_loss = 0.0398', '\\n',\n'Epoch 11: validation_loss = 0.0323', '\\n', 'Epoch 12: validation_loss =\n0.0384', '\\n', 'Epoch 13: validation_loss = 0.0313', '\\n', 'Epoch 14:\nvalidation_loss = 0.0317', '\\n', 'Epoch 15: validation_loss = 0.0451', '\\n',\n'Epoch 16: validation_loss = 0.0427', '\\n', 'Epoch 17: validation_loss =\n0.0304', '\\n', 'Epoch 18: validation_loss = 0.0321', '\\n', 'Epoch 19:\nvalidation_loss = 0.0290', '\\n', 'Epoch 20: validation_loss = 0.0413', '\\n',\n'Epoch 1: validation_loss = 0.3787', '\\n', 'Epoch 2: validation_loss = 0.2166',\n'\\n', 'Epoch 3: validation_loss = 0.1139', '\\n', 'Epoch 4: validation_loss =\n0.0748', '\\n', 'Epoch 5: validation_loss = 0.0680', '\\n', 'Epoch 6:\nvalidation_loss = 0.0549', '\\n', 'Epoch 7: validation_loss = 0.0520', '\\n',\n'Epoch 8: validation_loss = 0.0478', '\\n', 'Epoch 9: validation_loss = 0.0447',\n'\\n', 'Epoch 10: validation_loss = 0.0444', '\\n', 'Epoch 11: validation_loss =\n0.0405', '\\n', 'Epoch 12: validation_loss = 0.0384', '\\n', 'Epoch 13:\nvalidation_loss = 0.0406', '\\n', 'Epoch 14: validation_loss = 0.0355', '\\n',\n'Epoch 15: validation_loss = 0.0342', '\\n', 'Epoch 16: validation_loss =\n0.0368', '\\n', 'Epoch 17: validation_loss = 0.0360', '\\n', 'Epoch 18:\nvalidation_loss = 0.0344', '\\n', 'Epoch 19: validation_loss = 0.0300', '\\n',\n'Epoch 20: validation_loss = 0.0337', '\\n', 'Epoch 1: validation_loss = 0.4216',\n'\\n', 'Epoch 2: validation_loss = 0.2995', '\\n', 'Epoch 3: validation_loss =\n0.2346', '\\n', 'Epoch 4: validation_loss = 0.1681', '\\n', 'Epoch 5:\nvalidation_loss = 0.1184', '\\n', 'Epoch 6: validation_loss = 0.1007', '\\n',\n'Epoch 7: validation_loss = 0.0826', '\\n', 'Epoch 8: validation_loss = 0.0710',\n'\\n', 'Epoch 9: validation_loss = 0.0660', '\\n', 'Epoch 10: validation_loss =\n0.0640', '\\n', 'Epoch 11: validation_loss = 0.0545', '\\n', 'Epoch 12:\nvalidation_loss = 0.0512', '\\n', 'Epoch 13: validation_loss = 0.0482', '\\n',\n'Epoch 14: validation_loss = 0.0462', '\\n', 'Epoch 15: validation_loss =\n0.0428', '\\n', 'Epoch 16: validation_loss = 0.0458', '\\n', 'Epoch 17:\nvalidation_loss = 0.0415', '\\n', 'Epoch 18: validation_loss = 0.0430', '\\n',\n'Epoch 19: validation_loss = 0.0383', '\\n', 'Epoch 20: validation_loss =\n0.0378', '\\n', 'Execution time: 48 seconds seconds (time limit is an hour).']", "['Execution time: 47 seconds seconds (time limit is an hour).']", "['Execution time: a minute seconds (time limit is an hour).']", "['Execution time: 58 seconds seconds (time limit is an hour).']", "['Execution time: a minute seconds (time limit is an hour).']", ""], "analysis": ["", "", "", "", "", "", "The script crashes with a LinAlgError when calling np.polyfit to compute the\nalignment-rate slope: on early epochs (or degenerate data), np.polyfit receives\ninsufficient points or zero-variance x-values, leading to invalid divisions and\nan SVD convergence failure. To fix this, guard against using np.polyfit until\nthere are at least two data points (e.g., only compute the slope when\nlen(align_hist) > 1), or replace polyfit with a simple finite-difference slope\ncalculation (rate = (align_hist[-1] - align_hist[0]) / (len(align_hist) - 1)) or\nwrap the polyfit call in a try/except and assign a default rate on failure.", "", "", "", "", "", "", "", "", "The runtime warnings indicate that after adding Gaussian noise and clipping,\nsome probability vectors summed to zero, leading to division by zero and\nresulting NaN entries in the normalized probabilities. These NaNs then propagate\ninto downstream data (features and metrics), corrupting the experiment. To fix,\nensure the normalization denominator never hits zero\u2014for example, add a small\nepsilon to the row sums before dividing or detect zero-sum rows and replace them\nwith a valid distribution (e.g., uniform) before normalization.", "", "", "", "", "", ""], "exc_type": [null, null, null, null, null, null, "LinAlgError", null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, {"args": ["SVD did not converge in Linear Least Squares"]}, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 280, "<module>", "rate = np.polyfit(np.arange(len(align_hist)), align_hist, 1)[0]"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/numpy/lib/_polynomial_impl.py", 658, "polyfit", "c, resids, rank, s = lstsq(lhs, rhs, rcond)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/numpy/linalg/_linalg.py", 2508, "lstsq", "x, resids, rank, s = gufunc(a, b, rcond, signature=signature)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/numpy/linalg/_linalg.py", 116, "_raise_linalgerror_lstsq", "raise LinAlgError(\"SVD did not converge in Linear Least Squares\")"]], null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.995, "best_value": 0.995}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0152, "best_value": 0.0152}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0162, "best_value": 0.0162}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0195, "best_value": 0.0195}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.014, "best_value": 0.014}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0142, "best_value": 0.0142}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0119, "best_value": 0.0119}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0138, "best_value": 0.0138}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0188, "best_value": 0.0188}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.9867, "best_value": 0.9867}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.99, "best_value": 0.99}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.0095, "best_value": 0.0095}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.0103, "best_value": 0.0103}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.0157, "best_value": 0.0157}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.0083, "best_value": 0.0083}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.0212, "best_value": 0.0212}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.0179, "best_value": 0.0179}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.0345, "best_value": 0.0345}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.0182, "best_value": 0.0182}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.0186, "best_value": 0.0186}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test accuracy", "data": [{"dataset_name": "ai_bs_16_user_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ai_bs_16_user_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_16_user_bs_64", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ai_bs_32_user_bs_16", "final_value": 0.992, "best_value": 0.992}, {"dataset_name": "ai_bs_32_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_32_user_bs_64", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_16", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ai_bs_64_user_bs_64", "final_value": 0.998, "best_value": 0.998}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset", "data": [{"dataset_name": "ablation=soft, config=ai_bs_16_usr_bs_16", "final_value": 0.855, "best_value": 0.855}, {"dataset_name": "ablation=soft, config=ai_bs_16_usr_bs_32", "final_value": 0.86, "best_value": 0.86}, {"dataset_name": "ablation=soft, config=ai_bs_16_usr_bs_64", "final_value": 0.8575, "best_value": 0.8575}, {"dataset_name": "ablation=soft, config=ai_bs_32_usr_bs_16", "final_value": 0.8633, "best_value": 0.8633}, {"dataset_name": "ablation=soft, config=ai_bs_32_usr_bs_32", "final_value": 0.8625, "best_value": 0.8625}, {"dataset_name": "ablation=soft, config=ai_bs_32_usr_bs_64", "final_value": 0.8608, "best_value": 0.8608}, {"dataset_name": "ablation=soft, config=ai_bs_64_usr_bs_16", "final_value": 0.8583, "best_value": 0.8583}, {"dataset_name": "ablation=soft, config=ai_bs_64_usr_bs_32", "final_value": 0.86, "best_value": 0.86}, {"dataset_name": "ablation=soft, config=ai_bs_64_usr_bs_64", "final_value": 0.8592, "best_value": 0.8592}, {"dataset_name": "ablation=hard, config=ai_bs_16_usr_bs_16", "final_value": 0.8583, "best_value": 0.8583}, {"dataset_name": "ablation=hard, config=ai_bs_16_usr_bs_32", "final_value": 0.86, "best_value": 0.86}, {"dataset_name": "ablation=hard, config=ai_bs_16_usr_bs_64", "final_value": 0.8617, "best_value": 0.8617}, {"dataset_name": "ablation=hard, config=ai_bs_32_usr_bs_16", "final_value": 0.8592, "best_value": 0.8592}, {"dataset_name": "ablation=hard, config=ai_bs_32_usr_bs_32", "final_value": 0.8525, "best_value": 0.8525}, {"dataset_name": "ablation=hard, config=ai_bs_32_usr_bs_64", "final_value": 0.8558, "best_value": 0.8558}, {"dataset_name": "ablation=hard, config=ai_bs_64_usr_bs_16", "final_value": 0.8542, "best_value": 0.8542}, {"dataset_name": "ablation=hard, config=ai_bs_64_usr_bs_32", "final_value": 0.86, "best_value": 0.86}, {"dataset_name": "ablation=hard, config=ai_bs_64_usr_bs_64", "final_value": 0.8608, "best_value": 0.8608}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset", "data": [{"dataset_name": "ablation=soft, config=ai_bs_16_usr_bs_16", "final_value": 0.8267, "best_value": 0.8267}, {"dataset_name": "ablation=soft, config=ai_bs_16_usr_bs_32", "final_value": 0.8233, "best_value": 0.8233}, {"dataset_name": "ablation=soft, config=ai_bs_16_usr_bs_64", "final_value": 0.83, "best_value": 0.83}, {"dataset_name": "ablation=soft, config=ai_bs_32_usr_bs_16", "final_value": 0.8233, "best_value": 0.8233}, {"dataset_name": "ablation=soft, config=ai_bs_32_usr_bs_32", "final_value": 0.8267, "best_value": 0.8267}, {"dataset_name": "ablation=soft, config=ai_bs_32_usr_bs_64", "final_value": 0.8333, "best_value": 0.8333}, {"dataset_name": "ablation=soft, config=ai_bs_64_usr_bs_16", "final_value": 0.8267, "best_value": 0.8267}, {"dataset_name": "ablation=soft, config=ai_bs_64_usr_bs_32", "final_value": 0.8267, "best_value": 0.8267}, {"dataset_name": "ablation=soft, config=ai_bs_64_usr_bs_64", "final_value": 0.8233, "best_value": 0.8233}, {"dataset_name": "ablation=hard, config=ai_bs_16_usr_bs_16", "final_value": 0.8333, "best_value": 0.8333}, {"dataset_name": "ablation=hard, config=ai_bs_16_usr_bs_32", "final_value": 0.84, "best_value": 0.84}, {"dataset_name": "ablation=hard, config=ai_bs_16_usr_bs_64", "final_value": 0.8233, "best_value": 0.8233}, {"dataset_name": "ablation=hard, config=ai_bs_32_usr_bs_16", "final_value": 0.8233, "best_value": 0.8233}, {"dataset_name": "ablation=hard, config=ai_bs_32_usr_bs_32", "final_value": 0.83, "best_value": 0.83}, {"dataset_name": "ablation=hard, config=ai_bs_32_usr_bs_64", "final_value": 0.84, "best_value": 0.84}, {"dataset_name": "ablation=hard, config=ai_bs_64_usr_bs_16", "final_value": 0.8267, "best_value": 0.8267}, {"dataset_name": "ablation=hard, config=ai_bs_64_usr_bs_32", "final_value": 0.8267, "best_value": 0.8267}, {"dataset_name": "ablation=hard, config=ai_bs_64_usr_bs_64", "final_value": 0.8467, "best_value": 0.8467}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset", "data": [{"dataset_name": "ablation=soft, config=ai_bs_16_usr_bs_16", "final_value": 0.85, "best_value": 0.85}, {"dataset_name": "ablation=soft, config=ai_bs_16_usr_bs_32", "final_value": 0.854, "best_value": 0.854}, {"dataset_name": "ablation=soft, config=ai_bs_16_usr_bs_64", "final_value": 0.858, "best_value": 0.858}, {"dataset_name": "ablation=soft, config=ai_bs_32_usr_bs_16", "final_value": 0.86, "best_value": 0.86}, {"dataset_name": "ablation=soft, config=ai_bs_32_usr_bs_32", "final_value": 0.858, "best_value": 0.858}, {"dataset_name": "ablation=soft, config=ai_bs_32_usr_bs_64", "final_value": 0.86, "best_value": 0.86}, {"dataset_name": "ablation=soft, config=ai_bs_64_usr_bs_16", "final_value": 0.864, "best_value": 0.864}, {"dataset_name": "ablation=soft, config=ai_bs_64_usr_bs_32", "final_value": 0.86, "best_value": 0.86}, {"dataset_name": "ablation=soft, config=ai_bs_64_usr_bs_64", "final_value": 0.85, "best_value": 0.85}, {"dataset_name": "ablation=hard, config=ai_bs_16_usr_bs_16", "final_value": 0.856, "best_value": 0.856}, {"dataset_name": "ablation=hard, config=ai_bs_16_usr_bs_32", "final_value": 0.86, "best_value": 0.86}, {"dataset_name": "ablation=hard, config=ai_bs_16_usr_bs_64", "final_value": 0.858, "best_value": 0.858}, {"dataset_name": "ablation=hard, config=ai_bs_32_usr_bs_16", "final_value": 0.86, "best_value": 0.86}, {"dataset_name": "ablation=hard, config=ai_bs_32_usr_bs_32", "final_value": 0.852, "best_value": 0.852}, {"dataset_name": "ablation=hard, config=ai_bs_32_usr_bs_64", "final_value": 0.85, "best_value": 0.85}, {"dataset_name": "ablation=hard, config=ai_bs_64_usr_bs_16", "final_value": 0.854, "best_value": 0.854}, {"dataset_name": "ablation=hard, config=ai_bs_64_usr_bs_32", "final_value": 0.864, "best_value": 0.864}, {"dataset_name": "ablation=hard, config=ai_bs_64_usr_bs_64", "final_value": 0.86, "best_value": 0.86}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "ablation=soft, config=ai_bs_16_usr_bs_16", "final_value": 0.3057, "best_value": 0.3057}, {"dataset_name": "ablation=soft, config=ai_bs_16_usr_bs_32", "final_value": 0.3044, "best_value": 0.3044}, {"dataset_name": "ablation=soft, config=ai_bs_16_usr_bs_64", "final_value": 0.3047, "best_value": 0.3047}, {"dataset_name": "ablation=soft, config=ai_bs_32_usr_bs_16", "final_value": 0.305, "best_value": 0.305}, {"dataset_name": "ablation=soft, config=ai_bs_32_usr_bs_32", "final_value": 0.3034, "best_value": 0.3034}, {"dataset_name": "ablation=soft, config=ai_bs_32_usr_bs_64", "final_value": 0.3043, "best_value": 0.3043}, {"dataset_name": "ablation=soft, config=ai_bs_64_usr_bs_16", "final_value": 0.3047, "best_value": 0.3047}, {"dataset_name": "ablation=soft, config=ai_bs_64_usr_bs_32", "final_value": 0.3073, "best_value": 0.3073}, {"dataset_name": "ablation=soft, config=ai_bs_64_usr_bs_64", "final_value": 0.3048, "best_value": 0.3048}, {"dataset_name": "ablation=hard, config=ai_bs_16_usr_bs_16", "final_value": 0.3092, "best_value": 0.3092}, {"dataset_name": "ablation=hard, config=ai_bs_16_usr_bs_32", "final_value": 0.3022, "best_value": 0.3022}, {"dataset_name": "ablation=hard, config=ai_bs_16_usr_bs_64", "final_value": 0.3038, "best_value": 0.3038}, {"dataset_name": "ablation=hard, config=ai_bs_32_usr_bs_16", "final_value": 0.3055, "best_value": 0.3055}, {"dataset_name": "ablation=hard, config=ai_bs_32_usr_bs_32", "final_value": 0.306, "best_value": 0.306}, {"dataset_name": "ablation=hard, config=ai_bs_32_usr_bs_64", "final_value": 0.3044, "best_value": 0.3044}, {"dataset_name": "ablation=hard, config=ai_bs_64_usr_bs_16", "final_value": 0.3063, "best_value": 0.3063}, {"dataset_name": "ablation=hard, config=ai_bs_64_usr_bs_32", "final_value": 0.3035, "best_value": 0.3035}, {"dataset_name": "ablation=hard, config=ai_bs_64_usr_bs_64", "final_value": 0.303, "best_value": 0.303}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "ablation=soft, config=ai_bs_16_usr_bs_16", "final_value": 0.3768, "best_value": 0.3768}, {"dataset_name": "ablation=soft, config=ai_bs_16_usr_bs_32", "final_value": 0.3752, "best_value": 0.3752}, {"dataset_name": "ablation=soft, config=ai_bs_16_usr_bs_64", "final_value": 0.3702, "best_value": 0.3702}, {"dataset_name": "ablation=soft, config=ai_bs_32_usr_bs_16", "final_value": 0.3899, "best_value": 0.3899}, {"dataset_name": "ablation=soft, config=ai_bs_32_usr_bs_32", "final_value": 0.3796, "best_value": 0.3796}, {"dataset_name": "ablation=soft, config=ai_bs_32_usr_bs_64", "final_value": 0.3711, "best_value": 0.3711}, {"dataset_name": "ablation=soft, config=ai_bs_64_usr_bs_16", "final_value": 0.3772, "best_value": 0.3772}, {"dataset_name": "ablation=soft, config=ai_bs_64_usr_bs_32", "final_value": 0.3769, "best_value": 0.3769}, {"dataset_name": "ablation=soft, config=ai_bs_64_usr_bs_64", "final_value": 0.3741, "best_value": 0.3741}, {"dataset_name": "ablation=hard, config=ai_bs_16_usr_bs_16", "final_value": 0.3814, "best_value": 0.3814}, {"dataset_name": "ablation=hard, config=ai_bs_16_usr_bs_32", "final_value": 0.3703, "best_value": 0.3703}, {"dataset_name": "ablation=hard, config=ai_bs_16_usr_bs_64", "final_value": 0.3727, "best_value": 0.3727}, {"dataset_name": "ablation=hard, config=ai_bs_32_usr_bs_16", "final_value": 0.3697, "best_value": 0.3697}, {"dataset_name": "ablation=hard, config=ai_bs_32_usr_bs_32", "final_value": 0.3713, "best_value": 0.3713}, {"dataset_name": "ablation=hard, config=ai_bs_32_usr_bs_64", "final_value": 0.372, "best_value": 0.372}, {"dataset_name": "ablation=hard, config=ai_bs_64_usr_bs_16", "final_value": 0.3874, "best_value": 0.3874}, {"dataset_name": "ablation=hard, config=ai_bs_64_usr_bs_32", "final_value": 0.3626, "best_value": 0.3626}, {"dataset_name": "ablation=hard, config=ai_bs_64_usr_bs_64", "final_value": 0.3673, "best_value": 0.3673}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy", "data": [{"dataset_name": "T_0.5", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "T_1.0", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "T_2.0", "final_value": 0.9908, "best_value": 0.9908}, {"dataset_name": "T_5.0", "final_value": 0.99, "best_value": 0.99}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "T_0.5", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "T_1.0", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "T_2.0", "final_value": 0.9867, "best_value": 0.9867}, {"dataset_name": "T_5.0", "final_value": 0.99, "best_value": 0.99}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "T_0.5", "final_value": 0.011, "best_value": 0.011}, {"dataset_name": "T_1.0", "final_value": 0.0171, "best_value": 0.0171}, {"dataset_name": "T_2.0", "final_value": 0.0242, "best_value": 0.0242}, {"dataset_name": "T_5.0", "final_value": 0.0308, "best_value": 0.0308}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "T_0.5", "final_value": 0.0117, "best_value": 0.0117}, {"dataset_name": "T_1.0", "final_value": 0.0182, "best_value": 0.0182}, {"dataset_name": "T_2.0", "final_value": 0.0221, "best_value": 0.0221}, {"dataset_name": "T_5.0", "final_value": 0.0244, "best_value": 0.0244}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test accuracy", "data": [{"dataset_name": "T_0.5", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "T_1.0", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "T_2.0", "final_value": 0.992, "best_value": 0.992}, {"dataset_name": "T_5.0", "final_value": 0.988, "best_value": 0.988}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on training set", "data": [{"dataset_name": "with_teacher_probs | ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "with_teacher_probs | ai_bs_16_user_bs_32", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "with_teacher_probs | ai_bs_16_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "with_teacher_probs | ai_bs_32_user_bs_16", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "with_teacher_probs | ai_bs_32_user_bs_32", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "with_teacher_probs | ai_bs_32_user_bs_64", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "with_teacher_probs | ai_bs_64_user_bs_16", "final_value": 0.9925, "best_value": 0.9925}, {"dataset_name": "with_teacher_probs | ai_bs_64_user_bs_32", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "with_teacher_probs | ai_bs_64_user_bs_64", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "raw_features_only | ai_bs_16_user_bs_16", "final_value": 0.995, "best_value": 0.995}, {"dataset_name": "raw_features_only | ai_bs_16_user_bs_32", "final_value": 0.995, "best_value": 0.995}, {"dataset_name": "raw_features_only | ai_bs_16_user_bs_64", "final_value": 0.995, "best_value": 0.995}, {"dataset_name": "raw_features_only | ai_bs_32_user_bs_16", "final_value": 0.9925, "best_value": 0.9925}, {"dataset_name": "raw_features_only | ai_bs_32_user_bs_32", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "raw_features_only | ai_bs_32_user_bs_64", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "raw_features_only | ai_bs_64_user_bs_16", "final_value": 0.995, "best_value": 0.995}, {"dataset_name": "raw_features_only | ai_bs_64_user_bs_32", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "raw_features_only | ai_bs_64_user_bs_64", "final_value": 0.995, "best_value": 0.995}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on validation set", "data": [{"dataset_name": "with_teacher_probs | ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "with_teacher_probs | ai_bs_16_user_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "with_teacher_probs | ai_bs_16_user_bs_64", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "with_teacher_probs | ai_bs_32_user_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "with_teacher_probs | ai_bs_32_user_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "with_teacher_probs | ai_bs_32_user_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "with_teacher_probs | ai_bs_64_user_bs_16", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "with_teacher_probs | ai_bs_64_user_bs_32", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "with_teacher_probs | ai_bs_64_user_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "raw_features_only | ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "raw_features_only | ai_bs_16_user_bs_32", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "raw_features_only | ai_bs_16_user_bs_64", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "raw_features_only | ai_bs_32_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "raw_features_only | ai_bs_32_user_bs_32", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "raw_features_only | ai_bs_32_user_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "raw_features_only | ai_bs_64_user_bs_16", "final_value": 0.98, "best_value": 0.98}, {"dataset_name": "raw_features_only | ai_bs_64_user_bs_32", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "raw_features_only | ai_bs_64_user_bs_64", "final_value": 0.9967, "best_value": 0.9967}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on test set", "data": [{"dataset_name": "with_teacher_probs | ai_bs_16_user_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "with_teacher_probs | ai_bs_16_user_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "with_teacher_probs | ai_bs_16_user_bs_64", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "with_teacher_probs | ai_bs_32_user_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "with_teacher_probs | ai_bs_32_user_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "with_teacher_probs | ai_bs_32_user_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "with_teacher_probs | ai_bs_64_user_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "with_teacher_probs | ai_bs_64_user_bs_32", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "with_teacher_probs | ai_bs_64_user_bs_64", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "raw_features_only | ai_bs_16_user_bs_16", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "raw_features_only | ai_bs_16_user_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "raw_features_only | ai_bs_16_user_bs_64", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "raw_features_only | ai_bs_32_user_bs_16", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "raw_features_only | ai_bs_32_user_bs_32", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "raw_features_only | ai_bs_32_user_bs_64", "final_value": 0.992, "best_value": 0.992}, {"dataset_name": "raw_features_only | ai_bs_64_user_bs_16", "final_value": 0.982, "best_value": 0.982}, {"dataset_name": "raw_features_only | ai_bs_64_user_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "raw_features_only | ai_bs_64_user_bs_64", "final_value": 0.996, "best_value": 0.996}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on training set", "data": [{"dataset_name": "with_teacher_probs | ai_bs_16_user_bs_16", "final_value": 0.0152, "best_value": 0.0152}, {"dataset_name": "with_teacher_probs | ai_bs_16_user_bs_32", "final_value": 0.0162, "best_value": 0.0162}, {"dataset_name": "with_teacher_probs | ai_bs_16_user_bs_64", "final_value": 0.0195, "best_value": 0.0195}, {"dataset_name": "with_teacher_probs | ai_bs_32_user_bs_16", "final_value": 0.0136, "best_value": 0.0136}, {"dataset_name": "with_teacher_probs | ai_bs_32_user_bs_32", "final_value": 0.016, "best_value": 0.016}, {"dataset_name": "with_teacher_probs | ai_bs_32_user_bs_64", "final_value": 0.0215, "best_value": 0.0215}, {"dataset_name": "with_teacher_probs | ai_bs_64_user_bs_16", "final_value": 0.014, "best_value": 0.014}, {"dataset_name": "with_teacher_probs | ai_bs_64_user_bs_32", "final_value": 0.0165, "best_value": 0.0165}, {"dataset_name": "with_teacher_probs | ai_bs_64_user_bs_64", "final_value": 0.019, "best_value": 0.019}, {"dataset_name": "raw_features_only | ai_bs_16_user_bs_16", "final_value": 0.0176, "best_value": 0.0176}, {"dataset_name": "raw_features_only | ai_bs_16_user_bs_32", "final_value": 0.0234, "best_value": 0.0234}, {"dataset_name": "raw_features_only | ai_bs_16_user_bs_64", "final_value": 0.0339, "best_value": 0.0339}, {"dataset_name": "raw_features_only | ai_bs_32_user_bs_16", "final_value": 0.019, "best_value": 0.019}, {"dataset_name": "raw_features_only | ai_bs_32_user_bs_32", "final_value": 0.0264, "best_value": 0.0264}, {"dataset_name": "raw_features_only | ai_bs_32_user_bs_64", "final_value": 0.0331, "best_value": 0.0331}, {"dataset_name": "raw_features_only | ai_bs_64_user_bs_16", "final_value": 0.0185, "best_value": 0.0185}, {"dataset_name": "raw_features_only | ai_bs_64_user_bs_32", "final_value": 0.0198, "best_value": 0.0198}, {"dataset_name": "raw_features_only | ai_bs_64_user_bs_64", "final_value": 0.0288, "best_value": 0.0288}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on validation set", "data": [{"dataset_name": "with_teacher_probs | ai_bs_16_user_bs_16", "final_value": 0.0095, "best_value": 0.0095}, {"dataset_name": "with_teacher_probs | ai_bs_16_user_bs_32", "final_value": 0.0103, "best_value": 0.0103}, {"dataset_name": "with_teacher_probs | ai_bs_16_user_bs_64", "final_value": 0.0157, "best_value": 0.0157}, {"dataset_name": "with_teacher_probs | ai_bs_32_user_bs_16", "final_value": 0.0083, "best_value": 0.0083}, {"dataset_name": "with_teacher_probs | ai_bs_32_user_bs_32", "final_value": 0.0129, "best_value": 0.0129}, {"dataset_name": "with_teacher_probs | ai_bs_32_user_bs_64", "final_value": 0.016, "best_value": 0.016}, {"dataset_name": "with_teacher_probs | ai_bs_64_user_bs_16", "final_value": 0.0108, "best_value": 0.0108}, {"dataset_name": "with_teacher_probs | ai_bs_64_user_bs_32", "final_value": 0.0137, "best_value": 0.0137}, {"dataset_name": "with_teacher_probs | ai_bs_64_user_bs_64", "final_value": 0.017, "best_value": 0.017}, {"dataset_name": "raw_features_only | ai_bs_16_user_bs_16", "final_value": 0.0115, "best_value": 0.0115}, {"dataset_name": "raw_features_only | ai_bs_16_user_bs_32", "final_value": 0.0187, "best_value": 0.0187}, {"dataset_name": "raw_features_only | ai_bs_16_user_bs_64", "final_value": 0.0304, "best_value": 0.0304}, {"dataset_name": "raw_features_only | ai_bs_32_user_bs_16", "final_value": 0.02, "best_value": 0.02}, {"dataset_name": "raw_features_only | ai_bs_32_user_bs_32", "final_value": 0.0232, "best_value": 0.0232}, {"dataset_name": "raw_features_only | ai_bs_32_user_bs_64", "final_value": 0.0266, "best_value": 0.0266}, {"dataset_name": "raw_features_only | ai_bs_64_user_bs_16", "final_value": 0.0465, "best_value": 0.0465}, {"dataset_name": "raw_features_only | ai_bs_64_user_bs_32", "final_value": 0.0203, "best_value": 0.0203}, {"dataset_name": "raw_features_only | ai_bs_64_user_bs_64", "final_value": 0.0257, "best_value": 0.0257}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Proportion of correct predictions on the training set", "data": [{"dataset_name": "CE_hard_labels", "final_value": 0.995, "best_value": 0.9975}, {"dataset_name": "soft_label_distillation", "final_value": 0.995, "best_value": 0.9975}, {"dataset_name": "bias_awareness", "final_value": 0.995, "best_value": 0.9958}, {"dataset_name": "dual_channel", "final_value": 0.9967, "best_value": 0.9983}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Proportion of correct predictions on the validation set", "data": [{"dataset_name": "CE_hard_labels", "final_value": 0.9967, "best_value": 1.0}, {"dataset_name": "soft_label_distillation", "final_value": 0.9933, "best_value": 0.9967}, {"dataset_name": "bias_awareness", "final_value": 0.9967, "best_value": 1.0}, {"dataset_name": "dual_channel", "final_value": 0.9967, "best_value": 1.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Proportion of correct predictions on the test set", "data": [{"dataset_name": "CE_hard_labels", "final_value": 0.986, "best_value": 0.996}, {"dataset_name": "soft_label_distillation", "final_value": 0.992, "best_value": 0.994}, {"dataset_name": "bias_awareness", "final_value": 0.988, "best_value": 1.0}, {"dataset_name": "dual_channel", "final_value": 0.99, "best_value": 0.996}]}, {"metric_name": "alignment rate", "lower_is_better": false, "description": "Rate at which AI predictions align with user decisions", "data": [{"dataset_name": "CE_hard_labels", "final_value": -0.0017, "best_value": 0.0008}, {"dataset_name": "soft_label_distillation", "final_value": 0.0008, "best_value": 0.0013}, {"dataset_name": "bias_awareness", "final_value": -0.002, "best_value": -0.0015}, {"dataset_name": "dual_channel", "final_value": 0.001, "best_value": 0.0012}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss value on the training set", "data": [{"dataset_name": "CE_hard_labels", "final_value": 0.0243, "best_value": 0.0169}, {"dataset_name": "soft_label_distillation", "final_value": 0.0008, "best_value": 0.0003}, {"dataset_name": "bias_awareness", "final_value": 0.0245, "best_value": 0.019}, {"dataset_name": "dual_channel", "final_value": 0.0004, "best_value": 0.0001}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value on the validation set", "data": [{"dataset_name": "CE_hard_labels", "final_value": 0.0218, "best_value": 0.0137}, {"dataset_name": "soft_label_distillation", "final_value": 0.0008, "best_value": 0.0004}, {"dataset_name": "bias_awareness", "final_value": 0.0216, "best_value": 0.0144}, {"dataset_name": "dual_channel", "final_value": 0.0005, "best_value": 0.0001}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Final train accuracy.", "data": [{"dataset_name": "D2_noise0.1_scale1.0", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "D5_noise0.5_scale2.0", "final_value": 0.9992, "best_value": 0.9992}, {"dataset_name": "D10_noise1.0_scale0.5", "final_value": 0.985, "best_value": 0.985}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Final validation accuracy.", "data": [{"dataset_name": "D2_noise0.1_scale1.0", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "D5_noise0.5_scale2.0", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "D10_noise1.0_scale0.5", "final_value": 0.9867, "best_value": 0.9867}]}, {"metric_name": "alignment score", "lower_is_better": false, "description": "Final alignment score.", "data": [{"dataset_name": "D2_noise0.1_scale1.0", "final_value": 0.8918, "best_value": 0.8918}, {"dataset_name": "D5_noise0.5_scale2.0", "final_value": 0.9687, "best_value": 0.9687}, {"dataset_name": "D10_noise1.0_scale0.5", "final_value": 0.8792, "best_value": 0.8792}]}, {"metric_name": "alignment rate (slope)", "lower_is_better": false, "description": "Alignment rate as the slope.", "data": [{"dataset_name": "D2_noise0.1_scale1.0", "final_value": -0.0018, "best_value": -0.0018}, {"dataset_name": "D5_noise0.5_scale2.0", "final_value": 0.0004, "best_value": 0.0004}, {"dataset_name": "D10_noise1.0_scale0.5", "final_value": -0.003, "best_value": -0.003}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test accuracy.", "data": [{"dataset_name": "D2_noise0.1_scale1.0", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "D5_noise0.5_scale2.0", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "D10_noise1.0_scale0.5", "final_value": 0.99, "best_value": 0.99}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training set", "data": [{"dataset_name": "threshold_0.6", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "threshold_0.8", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "threshold_0.9", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "threshold_0.6", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "threshold_0.8", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "threshold_0.9", "final_value": 0.97, "best_value": 0.97}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "threshold_0.6", "final_value": 0.0008, "best_value": 0.0008}, {"dataset_name": "threshold_0.8", "final_value": 0.0002, "best_value": 0.0002}, {"dataset_name": "threshold_0.9", "final_value": 0.0002, "best_value": 0.0002}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "threshold_0.6", "final_value": 0.0206, "best_value": 0.0206}, {"dataset_name": "threshold_0.8", "final_value": 0.0349, "best_value": 0.0349}, {"dataset_name": "threshold_0.9", "final_value": 0.0614, "best_value": 0.0614}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test set", "data": [{"dataset_name": "threshold_0.6", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "threshold_0.8", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "threshold_0.9", "final_value": 0.976, "best_value": 0.976}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset", "data": [{"dataset_name": "50_50", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "70_30", "final_value": 0.9983, "best_value": 0.9992}, {"dataset_name": "90_10", "final_value": 0.9975, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset", "data": [{"dataset_name": "50_50", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "70_30", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "90_10", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Cross-entropy loss on the training dataset", "data": [{"dataset_name": "50_50", "final_value": 0.0176, "best_value": 0.0103}, {"dataset_name": "70_30", "final_value": 0.0135, "best_value": 0.0062}, {"dataset_name": "90_10", "final_value": 0.0088, "best_value": 0.0036}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Cross-entropy loss on the validation dataset", "data": [{"dataset_name": "50_50", "final_value": 0.0103, "best_value": 0.0093}, {"dataset_name": "70_30", "final_value": 0.0102, "best_value": 0.0031}, {"dataset_name": "90_10", "final_value": 0.0038, "best_value": 0.0007}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset", "data": [{"dataset_name": "50_50", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "70_30", "final_value": 0.996, "best_value": 1.0}, {"dataset_name": "90_10", "final_value": 0.998, "best_value": 1.0}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "F1 score on the test dataset", "data": [{"dataset_name": "50_50", "final_value": 0.9979, "best_value": 0.9979}, {"dataset_name": "70_30", "final_value": 0.9914, "best_value": 1.0}, {"dataset_name": "90_10", "final_value": 0.9811, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "AI model train cross-entropy loss", "lower_is_better": true, "description": "Cross-entropy loss for AI model on training data", "data": [{"dataset_name": "synthetic (activation: relu)", "final_value": 0.3032, "best_value": 0.3032}, {"dataset_name": "synthetic (activation: tanh)", "final_value": 0.3075, "best_value": 0.3075}, {"dataset_name": "synthetic (activation: leaky_relu)", "final_value": 0.3096, "best_value": 0.3096}, {"dataset_name": "synthetic (activation: linear)", "final_value": 0.3075, "best_value": 0.3075}]}, {"metric_name": "AI model validation cross-entropy loss", "lower_is_better": true, "description": "Cross-entropy loss for AI model on validation data", "data": [{"dataset_name": "synthetic (activation: relu)", "final_value": 0.3743, "best_value": 0.3743}, {"dataset_name": "synthetic (activation: tanh)", "final_value": 0.372, "best_value": 0.372}, {"dataset_name": "synthetic (activation: leaky_relu)", "final_value": 0.3756, "best_value": 0.3756}, {"dataset_name": "synthetic (activation: linear)", "final_value": 0.3744, "best_value": 0.3744}]}, {"metric_name": "AI model train accuracy", "lower_is_better": false, "description": "Accuracy for AI model on training data", "data": [{"dataset_name": "synthetic (activation: relu)", "final_value": 0.86, "best_value": 0.86}, {"dataset_name": "synthetic (activation: tanh)", "final_value": 0.8583, "best_value": 0.8583}, {"dataset_name": "synthetic (activation: leaky_relu)", "final_value": 0.8625, "best_value": 0.8625}, {"dataset_name": "synthetic (activation: linear)", "final_value": 0.8567, "best_value": 0.8567}]}, {"metric_name": "AI model validation accuracy", "lower_is_better": false, "description": "Accuracy for AI model on validation data", "data": [{"dataset_name": "synthetic (activation: relu)", "final_value": 0.83, "best_value": 0.83}, {"dataset_name": "synthetic (activation: tanh)", "final_value": 0.83, "best_value": 0.83}, {"dataset_name": "synthetic (activation: leaky_relu)", "final_value": 0.82, "best_value": 0.82}, {"dataset_name": "synthetic (activation: linear)", "final_value": 0.8433, "best_value": 0.8433}]}, {"metric_name": "User model train cross-entropy loss", "lower_is_better": true, "description": "Cross-entropy loss for user model on training data", "data": [{"dataset_name": "synthetic (activation: relu)", "final_value": 0.0131, "best_value": 0.0131}, {"dataset_name": "synthetic (activation: tanh)", "final_value": 0.0108, "best_value": 0.0108}, {"dataset_name": "synthetic (activation: leaky_relu)", "final_value": 0.0121, "best_value": 0.0121}, {"dataset_name": "synthetic (activation: linear)", "final_value": 0.0129, "best_value": 0.0129}]}, {"metric_name": "User model validation cross-entropy loss", "lower_is_better": true, "description": "Cross-entropy loss for user model on validation data", "data": [{"dataset_name": "synthetic (activation: relu)", "final_value": 0.0184, "best_value": 0.0184}, {"dataset_name": "synthetic (activation: tanh)", "final_value": 0.01, "best_value": 0.01}, {"dataset_name": "synthetic (activation: leaky_relu)", "final_value": 0.0132, "best_value": 0.0132}, {"dataset_name": "synthetic (activation: linear)", "final_value": 0.0138, "best_value": 0.0138}]}, {"metric_name": "User model train accuracy", "lower_is_better": false, "description": "Accuracy for user model on training data", "data": [{"dataset_name": "synthetic (activation: relu)", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "synthetic (activation: tanh)", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "synthetic (activation: leaky_relu)", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "synthetic (activation: linear)", "final_value": 0.9967, "best_value": 0.9967}]}, {"metric_name": "User model validation accuracy", "lower_is_better": false, "description": "Accuracy for user model on validation data", "data": [{"dataset_name": "synthetic (activation: relu)", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "synthetic (activation: tanh)", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "synthetic (activation: leaky_relu)", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "synthetic (activation: linear)", "final_value": 0.99, "best_value": 0.99}]}, {"metric_name": "User model test accuracy", "lower_is_better": false, "description": "Accuracy for user model on test data", "data": [{"dataset_name": "synthetic (activation: relu)", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "synthetic (activation: tanh)", "final_value": 0.992, "best_value": 0.992}, {"dataset_name": "synthetic (activation: leaky_relu)", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "synthetic (activation: linear)", "final_value": 0.994, "best_value": 0.994}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset", "data": [{"dataset_name": "ensemble_1_ai_bs_16_usr_bs_16", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ensemble_1_ai_bs_16_usr_bs_32", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ensemble_1_ai_bs_16_usr_bs_64", "final_value": 0.9992, "best_value": 0.9992}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_16", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_32", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_64", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_16", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_32", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_64", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_16", "final_value": 0.9992, "best_value": 0.9992}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_16", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_64", "final_value": 0.9992, "best_value": 0.9992}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_16", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_32", "final_value": 0.9992, "best_value": 0.9992}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_64", "final_value": 0.9992, "best_value": 0.9992}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_16", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_32", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_16", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_32", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_64", "final_value": 0.9992, "best_value": 0.9992}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_16", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_32", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_64", "final_value": 0.9983, "best_value": 0.9983}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset", "data": [{"dataset_name": "ensemble_1_ai_bs_16_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_1_ai_bs_16_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_1_ai_bs_16_usr_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_32", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_16", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_32", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_64", "final_value": 0.9967, "best_value": 0.9967}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset", "data": [{"dataset_name": "ensemble_1_ai_bs_16_usr_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ensemble_1_ai_bs_16_usr_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ensemble_1_ai_bs_16_usr_bs_64", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_64", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_32", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_16", "final_value": 0.988, "best_value": 0.988}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_64", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_16", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_64", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_16", "final_value": 0.988, "best_value": 0.988}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_32", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_64", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_64", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_16", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_64", "final_value": 0.992, "best_value": 0.992}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training set accuracy", "data": [{"dataset_name": "sigma_0.0, ai_bs_16_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "sigma_0.0, ai_bs_16_user_bs_32", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "sigma_0.0, ai_bs_16_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "sigma_0.0, ai_bs_32_user_bs_16", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "sigma_0.0, ai_bs_32_user_bs_32", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "sigma_0.0, ai_bs_32_user_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "sigma_0.0, ai_bs_64_user_bs_16", "final_value": 0.995, "best_value": 0.995}, {"dataset_name": "sigma_0.0, ai_bs_64_user_bs_32", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "sigma_0.0, ai_bs_64_user_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "sigma_0.1, ai_bs_16_user_bs_16", "final_value": 0.9917, "best_value": 0.9917}, {"dataset_name": "sigma_0.1, ai_bs_16_user_bs_32", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "sigma_0.1, ai_bs_16_user_bs_64", "final_value": 0.9925, "best_value": 0.9925}, {"dataset_name": "sigma_0.1, ai_bs_32_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "sigma_0.1, ai_bs_32_user_bs_32", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "sigma_0.1, ai_bs_32_user_bs_64", "final_value": 0.9892, "best_value": 0.9892}, {"dataset_name": "sigma_0.1, ai_bs_64_user_bs_16", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "sigma_0.1, ai_bs_64_user_bs_32", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "sigma_0.1, ai_bs_64_user_bs_64", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "sigma_0.2, ai_bs_16_user_bs_16", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "sigma_0.2, ai_bs_16_user_bs_32", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "sigma_0.2, ai_bs_16_user_bs_64", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "sigma_0.2, ai_bs_32_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "sigma_0.2, ai_bs_32_user_bs_32", "final_value": 0.995, "best_value": 0.995}, {"dataset_name": "sigma_0.2, ai_bs_32_user_bs_64", "final_value": 0.9925, "best_value": 0.9925}, {"dataset_name": "sigma_0.2, ai_bs_64_user_bs_16", "final_value": 0.9933, "best_value": 0.9933}, {"dataset_name": "sigma_0.2, ai_bs_64_user_bs_32", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "sigma_0.2, ai_bs_64_user_bs_64", "final_value": 0.9858, "best_value": 0.9858}, {"dataset_name": "sigma_0.5, ai_bs_16_user_bs_16", "final_value": 0.45, "best_value": 0.45}, {"dataset_name": "sigma_0.5, ai_bs_16_user_bs_32", "final_value": 0.45, "best_value": 0.45}, {"dataset_name": "sigma_0.5, ai_bs_16_user_bs_64", "final_value": 0.45, "best_value": 0.45}, {"dataset_name": "sigma_0.5, ai_bs_32_user_bs_16", "final_value": 0.4767, "best_value": 0.4767}, {"dataset_name": "sigma_0.5, ai_bs_32_user_bs_32", "final_value": 0.4767, "best_value": 0.4767}, {"dataset_name": "sigma_0.5, ai_bs_32_user_bs_64", "final_value": 0.4767, "best_value": 0.4767}, {"dataset_name": "sigma_0.5, ai_bs_64_user_bs_16", "final_value": 0.4583, "best_value": 0.4583}, {"dataset_name": "sigma_0.5, ai_bs_64_user_bs_32", "final_value": 0.4583, "best_value": 0.4583}, {"dataset_name": "sigma_0.5, ai_bs_64_user_bs_64", "final_value": 0.4583, "best_value": 0.4583}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on training dataset", "data": [{"dataset_name": "ensemble_1_ai_bs_16_usr_bs_16", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ensemble_1_ai_bs_16_usr_bs_32", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ensemble_1_ai_bs_16_usr_bs_64", "final_value": 0.9992, "best_value": 0.9992}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_16", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_32", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_64", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_16", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_32", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_64", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_16", "final_value": 0.9992, "best_value": 0.9992}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_16", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_64", "final_value": 0.9992, "best_value": 0.9992}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_16", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_32", "final_value": 0.9992, "best_value": 0.9992}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_64", "final_value": 0.9992, "best_value": 0.9992}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_16", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_32", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_64", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_16", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_32", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_64", "final_value": 0.9992, "best_value": 0.9992}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_16", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_32", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_64", "final_value": 0.9983, "best_value": 0.9983}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on validation dataset", "data": [{"dataset_name": "ensemble_1_ai_bs_16_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_1_ai_bs_16_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_1_ai_bs_16_usr_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_32", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_64", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_16", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_32", "final_value": 0.9967, "best_value": 0.9967}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_64", "final_value": 0.9967, "best_value": 0.9967}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on test dataset", "data": [{"dataset_name": "ensemble_1_ai_bs_16_usr_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ensemble_1_ai_bs_16_usr_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ensemble_1_ai_bs_16_usr_bs_64", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ensemble_1_ai_bs_32_usr_bs_64", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_16", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_32", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ensemble_1_ai_bs_64_usr_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_16", "final_value": 0.988, "best_value": 0.988}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ensemble_3_ai_bs_16_usr_bs_64", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_16", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_32", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_3_ai_bs_32_usr_bs_64", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_16", "final_value": 0.988, "best_value": 0.988}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_32", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ensemble_3_ai_bs_64_usr_bs_64", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_32", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "ensemble_5_ai_bs_16_usr_bs_64", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_16", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ensemble_5_ai_bs_32_usr_bs_64", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_16", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_32", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "ensemble_5_ai_bs_64_usr_bs_64", "final_value": 0.992, "best_value": 0.992}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_32_user_bs_32_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_64_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_16_train_val_curves.png", "../../logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_16_train_val_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_acfdb6e7d5a64d37808a07983dbd34e6_proc_2577153/soft_loss_curves.png", "../../logs/0-run/experiment_results/experiment_acfdb6e7d5a64d37808a07983dbd34e6_proc_2577153/hard_loss_curves.png", "../../logs/0-run/experiment_results/experiment_acfdb6e7d5a64d37808a07983dbd34e6_proc_2577153/hard_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_acfdb6e7d5a64d37808a07983dbd34e6_proc_2577153/soft_accuracy_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_8ebb98fcf3644d7ab724ee7275add2ee_proc_2577152/temperature_scaling_accuracy.png", "../../logs/0-run/experiment_results/experiment_8ebb98fcf3644d7ab724ee7275add2ee_proc_2577152/temperature_scaling_loss.png"], ["../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_64_user_bs_64_metrics.png", "../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_32_user_bs_64_metrics.png", "../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_32_user_bs_16_metrics.png", "../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_16_user_bs_64_metrics.png", "../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_64_user_bs_64_metrics.png", "../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_16_user_bs_16_metrics.png", "../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_32_user_bs_32_metrics.png", "../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_32_user_bs_32_metrics.png", "../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_16_user_bs_32_metrics.png", "../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_64_user_bs_32_metrics.png", "../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_64_user_bs_16_metrics.png", "../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_32_user_bs_16_metrics.png", "../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_16_user_bs_16_metrics.png", "../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_16_user_bs_32_metrics.png", "../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_64_user_bs_32_metrics.png", "../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_16_user_bs_64_metrics.png", "../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_32_user_bs_64_metrics.png", "../../logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_64_user_bs_16_metrics.png"], [], ["../../logs/0-run/experiment_results/experiment_24854c40570f463cb016d17d66384154_proc_2577152/dual_channel_alignment_ai32_usr32.png", "../../logs/0-run/experiment_results/experiment_24854c40570f463cb016d17d66384154_proc_2577152/dual_channel_accuracy_ai32_usr32.png", "../../logs/0-run/experiment_results/experiment_24854c40570f463cb016d17d66384154_proc_2577152/bias_awareness_accuracy_ai32_usr32.png", "../../logs/0-run/experiment_results/experiment_24854c40570f463cb016d17d66384154_proc_2577152/CE_hard_labels_accuracy_ai32_usr32.png", "../../logs/0-run/experiment_results/experiment_24854c40570f463cb016d17d66384154_proc_2577152/soft_label_distillation_accuracy_ai32_usr32.png"], ["../../logs/0-run/experiment_results/experiment_2c2ed116d4ca45969f375cd9f06ca558_proc_2577153/test_accuracy_ai32_usr32.png", "../../logs/0-run/experiment_results/experiment_2c2ed116d4ca45969f375cd9f06ca558_proc_2577153/accuracy_ai32_usr32.png", "../../logs/0-run/experiment_results/experiment_2c2ed116d4ca45969f375cd9f06ca558_proc_2577153/alignment_ai32_usr32.png", "../../logs/0-run/experiment_results/experiment_2c2ed116d4ca45969f375cd9f06ca558_proc_2577153/alignment_rate_ai32_usr32.png"], ["../../logs/0-run/experiment_results/experiment_9a98c8acd1524dc2b0ada48aab8154d9_proc_2577154/confidence_filter_test_accuracy.png", "../../logs/0-run/experiment_results/experiment_9a98c8acd1524dc2b0ada48aab8154d9_proc_2577154/confidence_filter_loss.png", "../../logs/0-run/experiment_results/experiment_9a98c8acd1524dc2b0ada48aab8154d9_proc_2577154/confidence_filter_accuracy.png"], ["../../logs/0-run/experiment_results/experiment_ef723dac565c414b8b2fe812bb03a019_proc_2577154/classimbalance_70_30_test_accuracy.png", "../../logs/0-run/experiment_results/experiment_ef723dac565c414b8b2fe812bb03a019_proc_2577154/classimbalance_90_10_test_accuracy.png", "../../logs/0-run/experiment_results/experiment_ef723dac565c414b8b2fe812bb03a019_proc_2577154/classimbalance_50_50_test_accuracy.png", "../../logs/0-run/experiment_results/experiment_ef723dac565c414b8b2fe812bb03a019_proc_2577154/classimbalance_50_50_trainval_ai32_user32.png", "../../logs/0-run/experiment_results/experiment_ef723dac565c414b8b2fe812bb03a019_proc_2577154/classimbalance_70_30_trainval_ai32_user32.png"], [], ["../../logs/0-run/experiment_results/experiment_f62911705c734b32a40898c9fdcd82de_proc_2577153/synthetic_ai_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_f62911705c734b32a40898c9fdcd82de_proc_2577153/synthetic_ai_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f62911705c734b32a40898c9fdcd82de_proc_2577153/synthetic_user_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f62911705c734b32a40898c9fdcd82de_proc_2577153/synthetic_test_accuracy.png", "../../logs/0-run/experiment_results/experiment_f62911705c734b32a40898c9fdcd82de_proc_2577153/synthetic_user_accuracy_curves.png"], ["../../logs/0-run/experiment_results/experiment_8e74dbbd7da449c7ac48b9a4d8e086be_proc_2577152/synthetic_final_val_accuracy.png", "../../logs/0-run/experiment_results/experiment_8e74dbbd7da449c7ac48b9a4d8e086be_proc_2577152/synthetic_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_8e74dbbd7da449c7ac48b9a4d8e086be_proc_2577152/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_52de9e24521245e3bc3a8343bcc9e873_proc_2577154/ensemble_1_ai_bs_32_usr_bs_64_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_52de9e24521245e3bc3a8343bcc9e873_proc_2577154/ensemble_3_ai_bs_16_usr_bs_32_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_52de9e24521245e3bc3a8343bcc9e873_proc_2577154/ensemble_5_ai_bs_16_usr_bs_64_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_52de9e24521245e3bc3a8343bcc9e873_proc_2577154/ensemble_1_ai_bs_16_usr_bs_16_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_52de9e24521245e3bc3a8343bcc9e873_proc_2577154/ensemble_3_ai_bs_64_usr_bs_16_accuracy_curve.png"], [], [], [], [], [], ["../../logs/0-run/experiment_results/experiment_265849553f664c49a292930f939bcedd_proc_2577153/ensemble_1_ai_bs_32_usr_bs_64_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_265849553f664c49a292930f939bcedd_proc_2577153/ensemble_3_ai_bs_16_usr_bs_32_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_265849553f664c49a292930f939bcedd_proc_2577153/ensemble_5_ai_bs_16_usr_bs_64_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_265849553f664c49a292930f939bcedd_proc_2577153/ensemble_1_ai_bs_16_usr_bs_16_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_265849553f664c49a292930f939bcedd_proc_2577153/ensemble_3_ai_bs_64_usr_bs_16_accuracy_curve.png"], []], "plot_paths": [["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_32_user_bs_32_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_64_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_16_train_val_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_16_train_val_curves.png"], [], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_acfdb6e7d5a64d37808a07983dbd34e6_proc_2577153/soft_loss_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_acfdb6e7d5a64d37808a07983dbd34e6_proc_2577153/hard_loss_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_acfdb6e7d5a64d37808a07983dbd34e6_proc_2577153/hard_accuracy_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_acfdb6e7d5a64d37808a07983dbd34e6_proc_2577153/soft_accuracy_curves.png"], [], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_8ebb98fcf3644d7ab724ee7275add2ee_proc_2577152/temperature_scaling_accuracy.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_8ebb98fcf3644d7ab724ee7275add2ee_proc_2577152/temperature_scaling_loss.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_64_user_bs_64_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_32_user_bs_64_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_32_user_bs_16_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_16_user_bs_64_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_64_user_bs_64_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_16_user_bs_16_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_32_user_bs_32_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_32_user_bs_32_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_16_user_bs_32_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_64_user_bs_32_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_64_user_bs_16_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_32_user_bs_16_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_16_user_bs_16_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_16_user_bs_32_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_64_user_bs_32_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_16_user_bs_64_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_32_user_bs_64_metrics.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_64_user_bs_16_metrics.png"], [], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_24854c40570f463cb016d17d66384154_proc_2577152/dual_channel_alignment_ai32_usr32.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_24854c40570f463cb016d17d66384154_proc_2577152/dual_channel_accuracy_ai32_usr32.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_24854c40570f463cb016d17d66384154_proc_2577152/bias_awareness_accuracy_ai32_usr32.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_24854c40570f463cb016d17d66384154_proc_2577152/CE_hard_labels_accuracy_ai32_usr32.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_24854c40570f463cb016d17d66384154_proc_2577152/soft_label_distillation_accuracy_ai32_usr32.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_2c2ed116d4ca45969f375cd9f06ca558_proc_2577153/test_accuracy_ai32_usr32.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_2c2ed116d4ca45969f375cd9f06ca558_proc_2577153/accuracy_ai32_usr32.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_2c2ed116d4ca45969f375cd9f06ca558_proc_2577153/alignment_ai32_usr32.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_2c2ed116d4ca45969f375cd9f06ca558_proc_2577153/alignment_rate_ai32_usr32.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_9a98c8acd1524dc2b0ada48aab8154d9_proc_2577154/confidence_filter_test_accuracy.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_9a98c8acd1524dc2b0ada48aab8154d9_proc_2577154/confidence_filter_loss.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_9a98c8acd1524dc2b0ada48aab8154d9_proc_2577154/confidence_filter_accuracy.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_ef723dac565c414b8b2fe812bb03a019_proc_2577154/classimbalance_70_30_test_accuracy.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_ef723dac565c414b8b2fe812bb03a019_proc_2577154/classimbalance_90_10_test_accuracy.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_ef723dac565c414b8b2fe812bb03a019_proc_2577154/classimbalance_50_50_test_accuracy.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_ef723dac565c414b8b2fe812bb03a019_proc_2577154/classimbalance_50_50_trainval_ai32_user32.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_ef723dac565c414b8b2fe812bb03a019_proc_2577154/classimbalance_70_30_trainval_ai32_user32.png"], [], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_f62911705c734b32a40898c9fdcd82de_proc_2577153/synthetic_ai_accuracy_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_f62911705c734b32a40898c9fdcd82de_proc_2577153/synthetic_ai_loss_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_f62911705c734b32a40898c9fdcd82de_proc_2577153/synthetic_user_loss_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_f62911705c734b32a40898c9fdcd82de_proc_2577153/synthetic_test_accuracy.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_f62911705c734b32a40898c9fdcd82de_proc_2577153/synthetic_user_accuracy_curves.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_8e74dbbd7da449c7ac48b9a4d8e086be_proc_2577152/synthetic_final_val_accuracy.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_8e74dbbd7da449c7ac48b9a4d8e086be_proc_2577152/synthetic_accuracy_curves.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_8e74dbbd7da449c7ac48b9a4d8e086be_proc_2577152/synthetic_loss_curves.png"], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_52de9e24521245e3bc3a8343bcc9e873_proc_2577154/ensemble_1_ai_bs_32_usr_bs_64_accuracy_curve.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_52de9e24521245e3bc3a8343bcc9e873_proc_2577154/ensemble_3_ai_bs_16_usr_bs_32_accuracy_curve.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_52de9e24521245e3bc3a8343bcc9e873_proc_2577154/ensemble_5_ai_bs_16_usr_bs_64_accuracy_curve.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_52de9e24521245e3bc3a8343bcc9e873_proc_2577154/ensemble_1_ai_bs_16_usr_bs_16_accuracy_curve.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_52de9e24521245e3bc3a8343bcc9e873_proc_2577154/ensemble_3_ai_bs_64_usr_bs_16_accuracy_curve.png"], [], [], [], [], [], ["experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_265849553f664c49a292930f939bcedd_proc_2577153/ensemble_1_ai_bs_32_usr_bs_64_accuracy_curve.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_265849553f664c49a292930f939bcedd_proc_2577153/ensemble_3_ai_bs_16_usr_bs_32_accuracy_curve.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_265849553f664c49a292930f939bcedd_proc_2577153/ensemble_5_ai_bs_16_usr_bs_64_accuracy_curve.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_265849553f664c49a292930f939bcedd_proc_2577153/ensemble_1_ai_bs_16_usr_bs_16_accuracy_curve.png", "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_265849553f664c49a292930f939bcedd_proc_2577153/ensemble_3_ai_bs_64_usr_bs_16_accuracy_curve.png"], []], "plot_analyses": [[{"analysis": "Synthetic binary dataset - ai_bs_64_user_bs_64: Training accuracy ramps up from about 91.5% at epoch 1 to above 99% by epoch 3 and then plateaus with minor oscillations around 99.3\u201399.8%. Validation accuracy closely tracks training, peaking near 99.5% by epoch 10. Training and validation losses both begin high (~0.40 and ~0.22) and decay rapidly to near zero by epoch 10. There is no sign of overfitting; both curves remain well\u2010aligned throughout. Given this rapid convergence, you could reduce the total number of epochs or introduce early stopping around epoch 8 without sacrificing performance.", "valid_plots_received": true, "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_64_train_val_curves.png"}, {"analysis": "Synthetic binary dataset - ai_bs_32_user_bs_32: Accuracy jumps from around 60% at epoch 1 to 98% by epoch 3 and almost 100% by epoch 5. Validation accuracy follows closely, with only small dips around epoch 8 and epoch 16 but never falling below 95%. Loss curves start higher (~0.53 train, ~0.35 val) but drop sharply to below 0.02 by epoch 8. Convergence is slightly faster than with batch size 64, and there is again no evidence of overfitting. Consider reducing epochs to 6\u20138 or experimenting with a slightly lower learning rate to smooth out minor validation fluctuations.", "valid_plots_received": true, "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_32_user_bs_32_train_val_curves.png"}, {"analysis": "Synthetic binary dataset - ai_bs_16_user_bs_64: Training accuracy moves from 65% at epoch 1 to 97% by epoch 4 and stabilizes above 98.5% thereafter. Validation accuracy closely follows, peaking near 99.8% around epoch 10. Loss starts around 0.63 (train) and 0.48 (val), and steadily declines to ~0.02 by epoch 10. The initial slow ramp suggests noisier gradients with the smaller AI batch but longer stability despite the larger user batch. This combination achieves high final performance but could benefit from a warmup schedule or a slightly higher initial learning rate to speed up the early epochs.", "valid_plots_received": true, "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_64_train_val_curves.png"}, {"analysis": "Synthetic binary dataset - ai_bs_16_user_bs_16: Accuracy surges to ~99% by epoch 2 and hovers between 99.5% and 100% thereafter. Validation accuracy consistently tracks at or above training. Loss falls from ~0.42 (train) and ~0.10 (val) at epoch 1 to near zero by epoch 5, with only minor noise later. Small batch sizes produce very fast convergence but also slightly noisier loss curves in later epochs. You could experiment with gradient\u2010noise reduction techniques (e.g., gradient clipping or learning rate decay) to further stabilize training.", "valid_plots_received": true, "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_16_user_bs_16_train_val_curves.png"}, {"analysis": "Synthetic binary dataset - ai_bs_64_user_bs_16: Training accuracy climbs from ~90% at epoch 1 to above 98% by epoch 3, then plateaus around 99.2\u201399.8%. Validation accuracy mirrors training, though it dips to ~98.3% around epoch 10 before rebounding. Loss starts around 0.31 (train) and 0.06 (val), dropping to under 0.02 by epoch 8. This mixed\u2010batch configuration converges almost as rapidly as the all\u2010small\u2010batch setup but with slightly smoother late\u2010epoch loss. You might reduce total epochs to 8\u201310 and tune the learning\u2010rate schedule for marginal speed gains.", "valid_plots_received": true, "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_a21677197dda42a9aa2af18492cef91e_proc_2565128/ai_bs_64_user_bs_16_train_val_curves.png"}], [], [{"analysis": "Soft Ablation - Loss Curves (Dataset: Synthetic Logistic):\n- All configurations exhibit a steep drop in training loss within the first two epochs, from roughly 0.65\u20130.68 down to about 0.30\u20130.32.\n- Validation losses settle into a narrow band around 0.36\u20130.38 after epoch 2, with small fluctuations but no clear divergence or overfitting.\n- Models with larger AI or user batch sizes (e.g., ai_bs_64_usr_bs_64) reach the plateau slightly faster (by epoch 1) than the smallest-batch pairs (ai_bs_16_usr_bs_16), but final training and validation losses are essentially indistinguishable across conditions by epoch 5\u201310.\n- Overall, removal of the \u201csoft\u201d component leaves the model\u2019s ability to minimize loss largely intact under the synthetic logistic task.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_acfdb6e7d5a64d37808a07983dbd34e6_proc_2577153/soft_loss_curves.png"}, {"analysis": "Hard Ablation - Loss Curves (Dataset: Synthetic Logistic):\n- A rapid reduction in training loss occurs in the first two epochs, from about 0.62\u20130.63 down to ~0.30\u20130.31, mirroring the behavior under soft ablation.\n- Validation losses converge into the 0.36\u20130.38 range, with marginally more jitter than in the soft-ablation case but no systematic upward drift.\n- Slightly slower initial convergence is seen for the smallest-batch setups, yet by epoch 3\u20134 all batch-size combinations achieve nearly identical loss levels.\n- Ablation of the \u201chard\u201d component similarly does not hinder overall loss minimization, producing final train/validation losses that match those under the soft variation.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_acfdb6e7d5a64d37808a07983dbd34e6_proc_2577153/hard_loss_curves.png"}, {"analysis": "Hard Ablation - Accuracy Curves (Dataset: Synthetic Logistic):\n- Training accuracy rises quickly from ~0.72 to ~0.85\u20130.86 by epoch 2 across all batch-size pairs.\n- Validation accuracy splits into two clusters: one group (combinations with ai_bs \u226532 and usr_bs \u226532) stabilizes around 0.84\u20130.85; the other (small-batch pairings such as ai_bs_16_usr_bs_16 or ai_bs_32_usr_bs_16) plateaus near 0.82\u20130.83.\n- Fluctuations in validation accuracy are more pronounced than in the soft-ablation case, but there\u2019s no trend of performance degradation over time.\n- Larger batches on both AI and user sides consistently yield the highest steady-state accuracy under hard ablation.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_acfdb6e7d5a64d37808a07983dbd34e6_proc_2577153/hard_accuracy_curves.png"}, {"analysis": "Soft Ablation - Accuracy Curves (Dataset: Synthetic Logistic):\n- Rapid attainment of ~0.85 train accuracy by epoch 2, closely tracking the pattern seen in hard ablation.\n- Validation accuracy groups again separate by batch-size regime: high-batch pairs reach ~0.84\u20130.85; low-batch pairs remain around 0.82\u20130.83.\n- Trajectories are marginally smoother, with reduced spike amplitude in validation accuracy relative to the hard-ablation runs.\n- No evidence of serious overfitting: validation curves remain stable or improve slightly over the 20 epochs.\n- Soft ablation yields performance on par with hard ablation, confirming that neither component by itself is critical to achieve baseline accuracy on this synthetic logistic task.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_acfdb6e7d5a64d37808a07983dbd34e6_proc_2577153/soft_accuracy_curves.png"}], [], [], [{"analysis": "raw_features_only | ai_bs_16_user_bs_16 shows rapid convergence by epoch 3, with training accuracy rising from ~0.90 to ~0.99 and validation accuracy tracking closely around ~0.99 thereafter. Training and validation loss both decline steeply in the first 5 epochs and plateau around ~0.02. The small gap between curves indicates minimal overfitting and stable generalization under equal batch sizes.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_64_user_bs_64_metrics.png"}, {"analysis": "raw_features_only | ai_bs_16_user_bs_64 yields slightly slower ramp-up of training accuracy, starting around ~0.60 and reaching ~0.99 by epoch 5. Validation accuracy climbs from ~0.83 to ~0.99, slightly ahead of training early on, suggesting stable generalization despite mismatch in batch sizes. Loss curves mirror the first setting but exhibit a marginally higher initial loss and converge to ~0.03.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_32_user_bs_64_metrics.png"}, {"analysis": "raw_features_only | ai_bs_32_user_bs_32 starts with a lower training accuracy (~0.78) but quickly matches the ~0.99 level by epoch 4. Validation accuracy begins at ~0.84 and also stabilizes around ~0.99. Loss falls sharply through epoch 5, settling near ~0.02, indicating robust learning with moderate batch sizes.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_32_user_bs_16_metrics.png"}, {"analysis": "raw_features_only | ai_bs_64_user_bs_16 displays an initial training accuracy of ~0.80 and validation around ~0.97. By epoch 4 both curves converge near ~0.99. There is some minor oscillation in validation accuracy around epochs 10\u201315 but the overall gap remains under 0.01. Loss plummets to ~0.02 by epoch 5 and stays flat.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_16_user_bs_64_metrics.png"}, {"analysis": "raw_features_only | ai_bs_64_user_bs_64 reveals behavior similar to the previous condition but with both accuracies starting around ~0.80 and converging to ~0.98\u20130.99 by epoch 4. Loss dynamics are nearly identical, ending at ~0.02. The symmetry in train/val performance suggests negligible effect of large batch sizes when balanced.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_64_user_bs_64_metrics.png"}, {"analysis": "with_teacher_probs | ai_bs_16_user_bs_16 shows slightly slower initial training growth (~0.78 at epoch 1) but matches ~0.99 accuracy by epoch 4. Validation accuracy remains very tight around ~0.99 throughout. Loss drops below ~0.02 by epoch 6, indicating that teacher probability signals marginally smooth the convergence and reduce overfitting.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_16_user_bs_16_metrics.png"}, {"analysis": "with_teacher_probs | ai_bs_16_user_bs_64 has a gentler warm-up: training accuracy climbs from ~0.65 to ~0.98 by epoch 6, while validation advances from ~0.88 to ~0.98. Loss starts higher (~0.47) vs. earlier setups but converges to ~0.03, reflecting that teacher supervision salvages generalization despite batch\u2010size mismatch.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_32_user_bs_32_metrics.png"}, {"analysis": "with_teacher_probs | ai_bs_32_user_bs_32 begins training at ~0.85, reaching ~0.99 accuracy by epoch 4; validation moves from ~0.97 to ~0.99. Loss behavior parallels raw features but finishes marginally lower (~0.015), hinting at slightly more confident predictions under dual supervision.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_32_user_bs_32_metrics.png"}, {"analysis": "with_teacher_probs | ai_bs_64_user_bs_16 starts at ~0.93 training accuracy, climbs to ~0.99, with validation jittering between ~0.98 and ~0.995. Loss dips under ~0.02 by epoch 6. The teacher\u2010guided model appears more stable with large AI batch and small user batch, reducing early training volatility.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/raw_features_only_ai_bs_16_user_bs_32_metrics.png"}, {"analysis": "with_teacher_probs | ai_bs_64_user_bs_64 opens with training accuracy ~0.86, but rapidly approaches ~0.99, and validation steadies around ~0.995. Loss curves fall below ~0.02 by epoch 8 and remain smooth. Balanced large batches with teacher probabilities produce the tightest train/validation alignment overall.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_33a5a1e460e544bea66c611c4ce48198_proc_2577154/with_teacher_probs_ai_bs_64_user_bs_32_metrics.png"}], [], [{"analysis": "Alignment rate spikes at epoch 2 (~0.0145) and then decays rapidly, falling below 0.002 by epoch 10 and to near zero by epoch 20. Suggests initial quick alignment of human mental model but diminishing returns as training continues, possibly due to saturation or overfitting of bias-adaptation signals.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_24854c40570f463cb016d17d66384154_proc_2577152/dual_channel_alignment_ai32_usr32.png"}, {"analysis": "Training accuracy for dual-channel quickly climbs from 78% at epoch 1 to ~99.9% by epoch 4, then oscillates marginally around 99.8\u2013100% on both training and validation. Validation slightly outperforms training in some epochs, indicating no overfitting and very strong generalization on this synthetic dataset.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_24854c40570f463cb016d17d66384154_proc_2577152/dual_channel_accuracy_ai32_usr32.png"}, {"analysis": "Bias-awareness channel alone reaches ~99% training and validation accuracy by epoch 3 and stabilizes around 99\u201399.5% afterward, with minor fluctuations. Performance is slightly lower and less stable compared to dual-channel, highlighting the contribution of content justification signals to steady convergence.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_24854c40570f463cb016d17d66384154_proc_2577152/bias_awareness_accuracy_ai32_usr32.png"}, {"analysis": "Hard-label cross-entropy baseline saturates training accuracy around 99.6\u201399.8% by epoch 5, with validation hovering around 98\u201399.7%. Fluctuations in validation at mid-training suggest some sensitivity to dataset noise, but overall performance remains high and comparable to other single-channel approaches.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_24854c40570f463cb016d17d66384154_proc_2577152/CE_hard_labels_accuracy_ai32_usr32.png"}, {"analysis": "Soft-label distillation achieves a rapid jump to ~98% validation by epoch 3, then converges to ~99.2\u201399.5% for both training and validation. This method shows slightly smoother validation curves than hard labels, implying improved calibration and reduced overfitting in student\u2013teacher setups.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_24854c40570f463cb016d17d66384154_proc_2577152/soft_label_distillation_accuracy_ai32_usr32.png"}], [{"analysis": "Test accuracy remains exceptionally high for both the low-noise (D2_noise0.1_scale1.0) and moderate-noise (D5_noise0.5_scale2.0) datasets, approaching perfect scores. The highest-noise, smallest-scale condition (D10_noise1.0_scale0.5) shows a slight but noticeable drop in generalization performance, falling just under 0.98. This indicates strong overall robustness of the model across noise conditions, with only the most extreme noise and scaled-down features leading to marginal degradation.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_2c2ed116d4ca45969f375cd9f06ca558_proc_2577153/test_accuracy_ai32_usr32.png"}, {"analysis": "All three datasets converge rapidly during training and maintain high validation scores, but the curves reveal distinct dynamics. D2_noise0.1_scale1.0 and D5_noise0.5_scale2.0 both reach near-ceiling performance by epoch 5, with virtually no gap between training and validation. In contrast, D10_noise1.0_scale0.5 exhibits slower convergence, requiring closer to 10\u201315 epochs to stabilize, and displays mild fluctuations in validation accuracy, suggesting susceptibility to overfitting under higher-noise, lower-scale conditions.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_2c2ed116d4ca45969f375cd9f06ca558_proc_2577153/accuracy_ai32_usr32.png"}, {"analysis": "Alignment scores between the AI\u2019s internal reasoning and the user\u2019s inferred mental model steadily decline over time in all conditions. The low-noise dataset (D2_noise0.1_scale1.0) drops from about 0.995 at epoch 1 to ~0.89 by epoch 20. The high-noise dataset (D10_noise1.0_scale0.5) falls even more sharply from ~0.97 down to ~0.86. The moderate-noise condition (D5_noise0.5_scale2.0) shows the smallest decrease, plateauing around ~0.97 after an initial dip. This pattern suggests that the interface struggles to maintain tight alignment over prolonged training, particularly under extreme noise.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_2c2ed116d4ca45969f375cd9f06ca558_proc_2577153/alignment_ai32_usr32.png"}, {"analysis": "The computed alignment rates (slopes) corroborate the score trajectories. D5_noise0.5_scale2.0 exhibits the slowest rate of decay (~\u20130.0014 per epoch), indicating the best retention of alignment. Both D2_noise0.1_scale1.0 and D10_noise1.0_scale0.5 share steeper misalignment slopes (~\u20130.0034 and ~\u20130.0040, respectively), confirming that both near-ideal and highly noisy settings lead to faster breakdowns in the co-adaptive alignment process.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_2c2ed116d4ca45969f375cd9f06ca558_proc_2577153/alignment_rate_ai32_usr32.png"}], [{"analysis": "Test Accuracy per Confidence Threshold indicates almost perfect classification at thresholds 0.6 and 0.8 (100% test accuracy), with a slight performance drop to 98% at threshold 0.9. This suggests that raising the confidence cutoff too high can marginally reduce overall coverage or introduce edge cases that degrade accuracy on the held\u2010out set.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_9a98c8acd1524dc2b0ada48aab8154d9_proc_2577154/confidence_filter_test_accuracy.png"}, {"analysis": "Training vs Validation Loss curves reveal rapid convergence across all thresholds. Lower cutoffs (0.6 and 0.8) yield faster decline and a lower validation loss floor (~0.02 and ~0.03 respectively), whereas the highest cutoff (0.9) shows slower convergence and settles at a higher validation loss (~0.06). The gap between training and validation loss grows with threshold, indicating that overly stringent confidence requirements may lead to overfitting or reduced effective training data.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_9a98c8acd1524dc2b0ada48aab8154d9_proc_2577154/confidence_filter_loss.png"}, {"analysis": "Training vs Validation Accuracy trajectories corroborate the loss observations: thresholds 0.6 and 0.8 reach 100% training and validation accuracy by epoch 4, while threshold 0.9 lags slightly, plateauing validation accuracy around 99\u201399.7% and exhibiting a modest generalization gap. This underscores that extremely high confidence thresholds can slow learning and cap maximum achievable alignment between model predictions and true labels.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_9a98c8acd1524dc2b0ada48aab8154d9_proc_2577154/confidence_filter_accuracy.png"}], [{"analysis": "Across all combinations of AI batch size and user batch size under the 70_30 class\u2013imbalance condition, test accuracy is constant at the maximum shown on the chart. There is no measurable difference in performance when varying the two batch sizes in this imbalance setting.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_ef723dac565c414b8b2fe812bb03a019_proc_2577154/classimbalance_70_30_test_accuracy.png"}, {"analysis": "Under the extreme 90_10 class\u2013imbalance setting, every AI/user batch\u2010size pairing again achieves the top\u2010level test accuracy. Batch sizes have no discernible impact on evaluation accuracy at this skewed ratio.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_ef723dac565c414b8b2fe812bb03a019_proc_2577154/classimbalance_90_10_test_accuracy.png"}, {"analysis": "With a balanced 50_50 class distribution, test accuracy remains uniformly at the chart\u2019s maximum across all AI and user batch size configurations. No batch\u2010size combination yields any advantage.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_ef723dac565c414b8b2fe812bb03a019_proc_2577154/classimbalance_50_50_test_accuracy.png"}, {"analysis": "For the 50_50 run with AI batch size 32 and user batch size 32, training accuracy starts low (~0.60) at epoch 0 then jumps to ~0.97 by epoch 2 and stabilizes around 0.99\u20131.00 thereafter. Validation accuracy begins at ~0.93, reaches ~0.98 by epoch 2, and remains near 0.99\u20131.00 with minimal fluctuation, indicating very rapid convergence and strong generalization with negligible overfitting.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_ef723dac565c414b8b2fe812bb03a019_proc_2577154/classimbalance_50_50_trainval_ai32_user32.png"}, {"analysis": "In the 70_30 setting (ai_bs_32, user_bs_32), training accuracy starts at ~0.88, climbs to ~0.99 by around epoch 2, and oscillates narrowly around 0.99\u20131.00. Validation accuracy begins at ~0.99, quickly reaches 1.00, and stays nearly perfect throughout. This reflects extremely fast learning and stable performance under class imbalance.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_ef723dac565c414b8b2fe812bb03a019_proc_2577154/classimbalance_70_30_trainval_ai32_user32.png"}], [], [{"analysis": "AI model accuracy quickly jumps above 0.85 by epoch 1 for all activation choices and then plateaus. Training curves for ReLU, tanh, leaky ReLU, and linear activations all oscillate insignificantly around 0.86 after the initial spike, while validation accuracy remains slightly lower (~0.82\u20130.84) with minor fluctuations. No activation shows a clear advantage in terms of steady-state accuracy or generalization on this synthetic task.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_f62911705c734b32a40898c9fdcd82de_proc_2577153/synthetic_ai_accuracy_curves.png"}, {"analysis": "Training cross-entropy loss for all activations collapses from ~0.50 at initialization to ~0.30 by epoch 2 and remains flat thereafter. Validation loss starts higher (0.36\u20130.39) but stabilizes around 0.37\u20130.38 with small jitters. Linear activation yields the smoothest validation loss curve, but differences across activations are marginal, indicating that they all achieve similar learning dynamics in this setting.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_f62911705c734b32a40898c9fdcd82de_proc_2577153/synthetic_ai_loss_curves.png"}, {"analysis": "User-model training loss falls from ~0.48\u20130.44 at epoch 0 to <0.05 by epoch 4 for all activations. Validation user loss follows a similar trend, hitting near-zero by epoch 6. Leaky ReLU shows a slightly slower initial drop but converges alongside other activations. Overall, user correction feedback is quickly learned regardless of activation function.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_f62911705c734b32a40898c9fdcd82de_proc_2577153/synthetic_user_loss_curves.png"}, {"analysis": "Final test accuracy by activation is nearly perfect for every variant (approximately 99.5\u2013100%). There is no meaningful gap across ReLU, tanh, leaky ReLU, or linear activations, confirming that all four produce comparable end-of-training performance on held-out data.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_f62911705c734b32a40898c9fdcd82de_proc_2577153/synthetic_test_accuracy.png"}, {"analysis": "User accuracy, both on training and validation, rises from 80\u201395% at epoch 0 to ~99%+ by epoch 2 and remains stable. Validation curves converge as quickly as training, suggesting minimal overfitting in the user model. Differences in convergence speed across activations are negligible.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_f62911705c734b32a40898c9fdcd82de_proc_2577153/synthetic_user_accuracy_curves.png"}], [], [{"analysis": "ensemble_1_ai_bs_32_usr_bs_64 exhibits extremely rapid convergence: training accuracy jumps from around 0.68 to above 0.98 by epoch 3, with validation accuracy following closely and peaking near 0.996. Both curves flatten after epoch 5, hovering consistently around 0.995\u20130.998. Minimal gap between train and validation suggests little overfitting; this configuration generalizes well and reaches near-ceiling performance quickly.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_52de9e24521245e3bc3a8343bcc9e873_proc_2577154/ensemble_1_ai_bs_32_usr_bs_64_accuracy_curve.png"}, {"analysis": "ensemble_3_ai_bs_16_usr_bs_32 shows similar behavior, with training accuracy climbing to approximately 0.995 by epoch 4 and validation accuracy briefly touching 1.00 around epoch 3. There are slight fluctuations in validation (0.994\u20131.00) but both curves remain tightly coupled. The model reaches a plateau around epoch 6, indicating stable convergence and strong generalization at this intermediate batch size pairing.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_52de9e24521245e3bc3a8343bcc9e873_proc_2577154/ensemble_3_ai_bs_16_usr_bs_32_accuracy_curve.png"}, {"analysis": "ensemble_5_ai_bs_16_usr_bs_64 has a slightly lower initial training accuracy (~0.80) but catches up by epoch 5, where both training and validation hit ~0.998\u20131.00. The validation curve briefly touches 1.00 at early epochs before stabilizing around 0.995\u20130.998. This suggests that increasing ensemble size to five with a higher user batch size maintains high performance, though it takes one more epoch to reach the plateau compared to smaller ensembles.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_52de9e24521245e3bc3a8343bcc9e873_proc_2577154/ensemble_5_ai_bs_16_usr_bs_64_accuracy_curve.png"}, {"analysis": "ensemble_1_ai_bs_16_usr_bs_16 rapidly attains high accuracy, with training at ~0.99 by epoch 2 and validation near 0.998 from the outset. Both curves oscillate minimally in the 0.995\u20131.00 range across all epochs, reflecting extremely stable learning. This smallest batch-size configuration leads to consistent high performance without noticeable overfitting.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_52de9e24521245e3bc3a8343bcc9e873_proc_2577154/ensemble_1_ai_bs_16_usr_bs_16_accuracy_curve.png"}, {"analysis": "ensemble_3_ai_bs_64_usr_bs_16 starts slightly lower (training ~0.972, validation ~0.996 at epoch 1) but both reach ~0.993\u20130.996 by epoch 2. Validation accuracy shows more variability (0.987\u20131.00) across epochs compared to others, while training remains in the 0.992\u20130.997 band. Convergence is slightly slower and generalization noise is higher, indicating that a larger AI batch size with moderate user batch size may introduce more variance in the learning process.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_52de9e24521245e3bc3a8343bcc9e873_proc_2577154/ensemble_3_ai_bs_64_usr_bs_16_accuracy_curve.png"}], [], [], [], [], [], [{"analysis": "For ensemble_1_ai_bs_32_usr_bs_64, training accuracy jumps from around .67 to nearly .99 by epoch 3, then plateaus above .995. Validation starts at .94, climbs to .998 by epoch 3, and remains stable with minor fluctuations around .995\u20131.0. The rapid convergence and tight training\u2013validation gap indicate strong generalization and no significant overfitting under this configuration.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_265849553f664c49a292930f939bcedd_proc_2577153/ensemble_1_ai_bs_32_usr_bs_64_accuracy_curve.png"}, {"analysis": "For ensemble_3_ai_bs_16_usr_bs_32, training begins at .87 and reaches .995 by epoch 3, then oscillates narrowly between .99 and 1.0. Validation accuracy starts higher at .98, peaks at 1.0 by epoch 2, and fluctuates within .995\u20131.0 thereafter. This setting also yields quick convergence and minimal generalization error, despite the smaller AI batch size.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_265849553f664c49a292930f939bcedd_proc_2577153/ensemble_3_ai_bs_16_usr_bs_32_accuracy_curve.png"}, {"analysis": "For ensemble_5_ai_bs_16_usr_bs_64, initial training accuracy is lower (\u2248.80) and climbs more gradually, hitting .97 by epoch 3 and .99 by epoch 4\u20135 before stabilizing above .995. Validation mirrors this trend: .94 at epoch 1, .995 by epoch 3, then a flat curve near 1.0. The larger user batch size compensates well for the smaller AI batch size, achieving high performance by epoch 5.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_265849553f664c49a292930f939bcedd_proc_2577153/ensemble_5_ai_bs_16_usr_bs_64_accuracy_curve.png"}, {"analysis": "For ensemble_1_ai_bs_16_usr_bs_16, training starts at .67, reaches .99 by epoch 3, and remains between .99\u20131.0. Validation opens at .995, peaks at .998 by epoch 2, and stays in the .996\u20131.0 range. Both curves track each other closely, demonstrating that even with minimal batch sizes on both channels, the model converges rapidly and generalizes robustly.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_265849553f664c49a292930f939bcedd_proc_2577153/ensemble_1_ai_bs_16_usr_bs_16_accuracy_curve.png"}, {"analysis": "For ensemble_3_ai_bs_64_usr_bs_16, training begins at .972 and climbs to .993 by epoch 2, then oscillates between .994 and .997. Validation starts at .996, dips to .99 at epoch 2, then fluctuates around .99\u20131.0 with a transient peak at epoch 13. Slightly higher variance in validation suggests this mix of larger AI batch and smaller user batch introduces modest instability, but overall convergence and generalization remain excellent.", "plot_path": "experiments/2025-05-29_15-59-15_coadaptive_explanation_alignment_attempt_0/logs/0-run/experiment_results/experiment_265849553f664c49a292930f939bcedd_proc_2577153/ensemble_3_ai_bs_64_usr_bs_16_accuracy_curve.png"}], []], "vlm_feedback_summary": ["All five batch\u2010size configurations achieve near\u2010perfect performance on the\nsynthetic binary dataset with no overfitting detected. Smaller AI batch sizes\n(16, 32) reach high accuracy faster, while larger validation batches provide\nslightly smoother loss curves. Training beyond epoch 8 yields diminishing\nreturns; consider early stopping or epoch reduction and fine\u2010tuned learning rate\nschedules to optimize training time without architecture changes.", "[]", "Both soft and hard ablation variants drive rapid convergence in loss and\naccuracy, with final metrics nearly matching each other. Batch-size interplay\nhas a modest effect, favoring dual large batches, but removal of either\nexplanation channel does not significantly impair performance on this synthetic\nclassification problem.", "[]", "[]", "All configurations achieve near\u2010perfect accuracy with low loss after 5\u20138 epochs.\nIntroducing teacher probabilities consistently smooths convergence, slightly\nreduces final loss and narrows any small generalization gaps. Batch-size\nmismatches (AI vs. user) have minimal impact on long-term performance but can\nslow initial warm-up; matching batch sizes yields faster stabilization.\nDual\u2010channel supervision (with_teacher_probs) marginally outperforms raw\nfeatures in stability and loss.", "[]", "All ablation variants attain near-perfect classification accuracy on the\nsynthetic binary dataset with minimal overfitting. Dual-channel interface\nmaintains marginally superior stability and speed of convergence. Alignment-rate\nanalysis reveals a tradeoff between initial bias correction and long-term\nadaptation saturation. Synthetic dataset simplicity may mask differences; future\nwork should test on more complex tasks to validate co-adaptive interface\nbenefits.", "Overall, the model generalizes robustly in terms of accuracy, but alignment\nbetween AI explanations and user mental models erodes over training. Moderate-\nnoise settings consistently yield the best balance of rapid convergence, strong\ngeneralization, and minimal alignment decay. In contrast, both very low-noise\nand very high-noise regimes accelerate misalignment. This insight points to the\nimportance of tuning bias-awareness signals and potentially adjusting adaptation\nrates in extreme-noise scenarios to sustain human-AI alignment over longer\ninteractions.", "Overall, ablation over confidence thresholds on the synthetic classification\ntask shows that moderate thresholds (0.6\u20130.8) deliver fastest convergence and\nperfect generalization, whereas an extreme threshold (0.9) introduces minor\naccuracy loss, slower convergence, and a slight overfitting signature. For\ndownstream co-adaptive explanation work, a midrange threshold (\u22480.8) is\nrecommended; further validation on noisy or real-world data is needed.", "Test accuracy is effectively at ceiling across all batch\u2010size ablations and\nclass imbalances, indicating that batch\u2010size variations within this range do not\ninfluence final performance. Training/validation curves show very rapid\nconvergence to near\u2010perfect accuracy with strong generalization and minimal gap\nbetween train and val. These results suggest a ceiling effect: the task may be\ntoo easy or the model capacity too high to reveal differences among component\nsettings. Future ablation should consider harder tasks, stronger regularization,\nor noise injection to surface more nuanced contributions of each module.", "[]", "This ablation demonstrates that activation function choice has minimal impact on\nboth AI and user-model performance in our synthetic setting. All variants\nconverge rapidly to similar losses and high accuracies, with only slight\ndifferences in initial convergence rates. Given these outcomes, activation type\ndoes not appear to be a critical factor for our dual-channel co-adaptive\nframework on this dataset.", "No plots received", "All five configurations achieved near-perfect accuracy with minimal\ntrain/validation gaps, converging within the first 5 epochs. Variations in\nensemble size and batch-size pairings had little impact on final performance,\nthough larger ensembles or larger AI batch sizes showed marginally slower\nconvergence or increased variability. Overall, dual-channel interfaces combined\nwith varying batch sizes yield robust learning and strong generalization in the\ncontrolled labeling task.", "[]", "[]", "[]", "[]", "[]", "Each ensemble achieves rapid convergence within 3\u20135 epochs with training and\nvalidation accuracies meeting or exceeding .99. Configurations with smaller AI\nbatch sizes converge slightly more slowly but still reach high accuracy; larger\nuser batch sizes help smooth the learning curve. No significant overfitting is\nobserved, and the narrow train\u2013validation gaps across all setups indicate strong\ngeneralization.", "[]"], "exec_time": [15.119279384613037, 41.698803424835205, 26.343322277069092, 28.125373125076294, 6.519167423248291, 26.464765548706055, 2.9132802486419678, 49.97112059593201, 44.77627635002136, 4.576914548873901, 42.64573621749878, 13.95251178741455, 8.854858875274658, 15.125612735748291, 57.90684700012207, 48.519275188446045, 48.929651975631714, 47.08618497848511, 63.90644884109497, 58.329883098602295, 89.26793146133423, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["ai_bs_64_user_bs_64", "ai_bs_32_user_bs_32", "ai_bs_16_user_bs_64", "ai_bs_16_user_bs_16", "ai_bs_64_user_bs_16"], [], ["Synthetic Logistic"], [], [], ["['raw_features_only|ai_bs_16_user_bs_16'", "'raw_features_only|ai_bs_16_user_bs_64'", "'raw_features_only|ai_bs_32_user_bs_32'", "'raw_features_only|ai_bs_64_user_bs_16'", "'raw_features_only|ai_bs_64_user_bs_64'", "'with_teacher_probs|ai_bs_16_user_bs_16'", "'with_teacher_probs|ai_bs_16_user_bs_64'", "'with_teacher_probs|ai_bs_32_user_bs_32'", "'with_teacher_probs|ai_bs_64_user_bs_16'", "'with_teacher_probs|ai_bs_64_user_bs_64']"], [], ["['Synthetic Binary']"], ["[D5_noise0.5_scale2.0]"], ["['Synthetic Classification']"], ["['50_50'", "'70_30'", "'90_10']"], [], ["synthetic"], [""], ["[\"ensemble_1_ai_bs_32_usr_bs_64\"", "\"ensemble_3_ai_bs_16_usr_bs_32\"", "\"ensemble_5_ai_bs_16_usr_bs_64\"", "\"ensemble_1_ai_bs_16_usr_bs_16\"", "\"ensemble_3_ai_bs_64_usr_bs_16\"]"], [], [], [], [], [], ["ensemble_1_ai_bs_32_usr_bs_64", "ensemble_3_ai_bs_16_usr_bs_32", "ensemble_5_ai_bs_16_usr_bs_64", "ensemble_1_ai_bs_16_usr_bs_16", "ensemble_3_ai_bs_64_usr_bs_16"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Select five evenly spaced configurations\nkeys = list(exp[\"batch_size\"].keys())\nif len(keys) > 5:\n    idxs = [int(i * (len(keys) - 1) / 4) for i in range(5)]\nelse:\n    idxs = list(range(len(keys)))\nselected = [keys[i] for i in idxs]\n\n# Plot train/val accuracy and loss for each selected config\nfor key in selected:\n    try:\n        data = exp[\"batch_size\"][key]\n        epochs = range(1, len(data[\"metrics\"][\"train\"]) + 1)\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        # Accuracy subplot\n        axs[0].plot(epochs, data[\"metrics\"][\"train\"], label=\"Train\")\n        axs[0].plot(epochs, data[\"metrics\"][\"val\"], label=\"Validation\")\n        axs[0].set_title(\"Accuracy\")\n        axs[0].set_xlabel(\"Epoch\")\n        axs[0].legend()\n        # Loss subplot\n        axs[1].plot(epochs, data[\"losses\"][\"train\"], label=\"Train\")\n        axs[1].plot(epochs, data[\"losses\"][\"val\"], label=\"Validation\")\n        axs[1].set_title(\"Loss\")\n        axs[1].set_xlabel(\"Epoch\")\n        axs[1].legend()\n        # Composite title\n        fig.suptitle(f\"Synthetic binary dataset - {key}\")\n        # Save and close\n        fname = os.path.join(working_dir, f\"{key}_train_val_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {key}: {e}\")\n        plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Print test accuracy for each ablation and setting\nfor ablation, settings in experiment_data.items():\n    for key, vals in settings.items():\n        preds = vals[\"predictions\"]\n        gt = vals[\"ground_truth\"]\n        acc = np.mean(preds == gt)\n        print(f\"Ablation: {ablation}, {key}, Test Accuracy: {acc:.3f}\")\n\n# Plot training/validation accuracy curves per ablation\nfor ablation, settings in experiment_data.items():\n    try:\n        plt.figure()\n        for key, vals in settings.items():\n            tr = vals[\"metrics\"][\"train\"]\n            va = vals[\"metrics\"][\"val\"]\n            plt.plot(tr, label=f\"{key} Train\")\n            plt.plot(va, \"--\", label=f\"{key} Val\")\n        plt.title(\n            f\"{ablation.capitalize()} Ablation - Accuracy Curves\\nDataset: Synthetic Logistic\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend(fontsize=\"small\", ncol=2)\n        fname = os.path.join(working_dir, f\"{ablation}_accuracy_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating {ablation} accuracy plot: {e}\")\n        plt.close()\n\n# Plot training/validation loss curves per ablation\nfor ablation, settings in experiment_data.items():\n    try:\n        plt.figure()\n        for key, vals in settings.items():\n            trl = vals[\"losses\"][\"train\"]\n            val = vals[\"losses\"][\"val\"]\n            plt.plot(trl, label=f\"{key} Train\")\n            plt.plot(val, \"--\", label=f\"{key} Val\")\n        plt.title(\n            f\"{ablation.capitalize()} Ablation - Loss Curves\\nDataset: Synthetic Logistic\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend(fontsize=\"small\", ncol=2)\n        fname = os.path.join(working_dir, f\"{ablation}_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating {ablation} loss plot: {e}\")\n        plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Plot accuracy curves\ntry:\n    plt.figure()\n    for key, entry in experiment_data.get(\"temperature_scaling\", {}).items():\n        train_acc = entry[\"metrics\"][\"train\"]\n        val_acc = entry[\"metrics\"][\"val\"]\n        epochs = np.arange(1, len(train_acc) + 1)\n        plt.plot(epochs, train_acc, label=f\"{key} train\")\n        plt.plot(epochs, val_acc, \"--\", label=f\"{key} val\")\n    plt.title(\n        \"Temperature Scaling: Training and Validation Accuracy (Synthetic Binary Dataset)\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"temperature_scaling_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for key, entry in experiment_data.get(\"temperature_scaling\", {}).items():\n        train_loss = entry[\"losses\"][\"train\"]\n        val_loss = entry[\"losses\"][\"val\"]\n        epochs = np.arange(1, len(train_loss) + 1)\n        plt.plot(epochs, train_loss, label=f\"{key} train\")\n        plt.plot(epochs, val_loss, \"--\", label=f\"{key} val\")\n    plt.title(\n        \"Temperature Scaling: Training and Validation Loss (Synthetic Binary Dataset)\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"temperature_scaling_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Print test accuracies\nfor key, entry in experiment_data.get(\"temperature_scaling\", {}).items():\n    preds = entry[\"predictions\"]\n    gt = entry[\"ground_truth\"]\n    acc = (preds == gt).mean()\n    print(f\"{key}: Test accuracy = {acc:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nfor scenario, combos in experiment_data.get(\"teacher_feature_removal\", {}).items():\n    for combo_key, res in combos.items():\n        try:\n            fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n            epochs = np.arange(1, len(res[\"metrics\"][\"train\"]) + 1)\n            # Left: Accuracy\n            axs[0].plot(epochs, res[\"metrics\"][\"train\"], label=\"Train Acc\")\n            axs[0].plot(epochs, res[\"metrics\"][\"val\"], label=\"Val Acc\")\n            axs[0].set_xlabel(\"Epoch\")\n            axs[0].set_ylabel(\"Accuracy\")\n            axs[0].set_title(\"Left: Train vs Val Accuracy\")\n            axs[0].legend()\n            # Right: Loss\n            axs[1].plot(epochs, res[\"losses\"][\"train\"], label=\"Train Loss\")\n            axs[1].plot(epochs, res[\"losses\"][\"val\"], label=\"Val Loss\")\n            axs[1].set_xlabel(\"Epoch\")\n            axs[1].set_ylabel(\"Loss\")\n            axs[1].set_title(\"Right: Train vs Val Loss\")\n            axs[1].legend()\n            fig.suptitle(f\"{scenario} | {combo_key}\")\n            fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n            fname = f\"{scenario}_{combo_key}_metrics.png\".replace(\" \", \"_\")\n            fig.savefig(os.path.join(working_dir, fname))\n            plt.close(fig)\n        except Exception as e:\n            print(f\"Error creating metrics plot for {scenario} {combo_key}: {e}\")\n            plt.close(\"all\")\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nkey = \"ai_bs_32_user_bs_32\"\nscenarios = [\n    \"CE_hard_labels\",\n    \"soft_label_distillation\",\n    \"bias_awareness\",\n    \"dual_channel\",\n]\n\nfor scenario in scenarios:\n    try:\n        metrics = experiment_data.get(scenario, {}).get(key, {}).get(\"metrics\", {})\n        train_acc = metrics.get(\"train\")\n        val_acc = metrics.get(\"val\")\n        if train_acc is None or val_acc is None:\n            raise ValueError(\"Missing accuracy data\")\n        epochs = np.arange(1, len(train_acc) + 1)\n        plt.figure()\n        plt.plot(epochs, train_acc, label=\"Training Acc\", color=\"blue\")\n        plt.plot(epochs, val_acc, label=\"Validation Acc\", color=\"orange\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.title(\n            f\"{scenario}: Training vs Validation Accuracy\\n\"\n            \"Training=Blue, Validation=Orange | Dataset: Synthetic Binary\"\n        )\n        fname = f\"{scenario}_accuracy_ai32_usr32.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating {scenario} accuracy plot: {e}\")\n        plt.close()\n\ntry:\n    align = experiment_data[\"dual_channel\"][key][\"metrics\"][\"alignment_rate\"]\n    epochs = np.arange(1, len(align) + 1)\n    plt.figure()\n    plt.plot(epochs, align, label=\"Alignment Rate\", color=\"green\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Alignment Rate\")\n    plt.legend()\n    plt.title(\"dual_channel: Alignment Rate Over Epochs\\nDataset: Synthetic Binary\")\n    plt.savefig(os.path.join(working_dir, \"dual_channel_alignment_ai32_usr32.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating dual_channel alignment plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\nexperiment_data = {}\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Prepare for a representative hyperparameter setting\nkey = \"ai_bs_32_user_bs_32\"\ndatasets = list(experiment_data.get(\"synthetic_diversity\", {}).keys())\ntrain_acc_list, val_acc_list = [], []\nalign_score_list, align_rate_list = [], []\ntest_acc_list = []\n\nfor ds in datasets:\n    cfg = experiment_data[\"synthetic_diversity\"][ds][\"batch_size\"].get(key)\n    if cfg is None:\n        continue\n    m = cfg[\"metrics\"]\n    train_acc_list.append(m[\"train_accs\"])\n    val_acc_list.append(m[\"val_accs\"])\n    align_score_list.append(m[\"alignment_scores\"])\n    align_rate_list.append(cfg[\"alignment_rate\"])\n    preds, gt = cfg[\"predictions\"], cfg[\"ground_truth\"]\n    test_acc_list.append((preds == gt).mean())\n\n# Determine epoch axis\nepochs = len(train_acc_list[0]) if train_acc_list else 0\nx = np.arange(1, epochs + 1)\ndataset_names = datasets[: len(train_acc_list)]\n\n# Plot 1: Training vs Validation Accuracy\ntry:\n    plt.figure()\n    for name, ta, va in zip(dataset_names, train_acc_list, val_acc_list):\n        plt.plot(x, ta, label=f\"{name} Train\")\n        plt.plot(x, va, label=f\"{name} Val\")\n    plt.title(\n        \"Training/Validation Accuracy for ai_bs_32_user_bs_32\\nDatasets: \"\n        + \", \".join(dataset_names)\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"accuracy_ai32_usr32.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# Plot 2: Alignment Score Curves\ntry:\n    plt.figure()\n    for name, al in zip(dataset_names, align_score_list):\n        plt.plot(x, al, label=name)\n    plt.title(\n        \"Alignment Scores over Epochs for ai_bs_32_user_bs_32\\nDatasets: \"\n        + \", \".join(dataset_names)\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Alignment Score\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"alignment_ai32_usr32.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating alignment plot: {e}\")\n    plt.close()\n\n# Plot 3: Alignment Rates Bar Chart\ntry:\n    plt.figure()\n    plt.bar(dataset_names, align_rate_list)\n    plt.title(\n        \"Alignment Rates for ai_bs_32_user_bs_32\\nDatasets: \" + \", \".join(dataset_names)\n    )\n    plt.ylabel(\"Alignment Rate (slope)\")\n    plt.savefig(os.path.join(working_dir, \"alignment_rate_ai32_usr32.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating alignment rate plot: {e}\")\n    plt.close()\n\n# Plot 4: Test Accuracy Bar Chart\ntry:\n    plt.figure()\n    plt.bar(dataset_names, test_acc_list)\n    plt.title(\n        \"Test Accuracy for ai_bs_32_user_bs_32\\nDatasets: \" + \", \".join(dataset_names)\n    )\n    plt.ylabel(\"Test Accuracy\")\n    plt.savefig(os.path.join(working_dir, \"test_accuracy_ai32_usr32.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\ntry:\n    experiment_data = np.load(data_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nconf_data = experiment_data.get(\"confidence_filter\", {})\nkeys = sorted(conf_data.keys(), key=lambda k: float(k.split(\"_\")[-1]))\n# determine epoch range\nn_epochs = len(conf_data[keys[0]][\"metrics\"][\"train\"])\nepochs = np.arange(1, n_epochs + 1)\n\n# Plot accuracy curves\ntry:\n    plt.figure()\n    for key in keys:\n        thr = key.split(\"_\")[-1]\n        train_acc = conf_data[key][\"metrics\"][\"train\"]\n        val_acc = conf_data[key][\"metrics\"][\"val\"]\n        plt.plot(epochs, train_acc, label=f\"Train thr={thr}\")\n        plt.plot(epochs, val_acc, \"--\", label=f\"Val thr={thr}\")\n    plt.title(\"Training vs Validation Accuracy\\nDataset: Synthetic Classification\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"confidence_filter_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for key in keys:\n        thr = key.split(\"_\")[-1]\n        train_loss = conf_data[key][\"losses\"][\"train\"]\n        val_loss = conf_data[key][\"losses\"][\"val\"]\n        plt.plot(epochs, train_loss, label=f\"Train thr={thr}\")\n        plt.plot(epochs, val_loss, \"--\", label=f\"Val thr={thr}\")\n    plt.title(\"Training vs Validation Loss\\nDataset: Synthetic Classification\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"confidence_filter_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Plot test accuracy bar chart\ntry:\n    thr_vals = []\n    test_accs = []\n    for key in keys:\n        thr_vals.append(key.split(\"_\")[-1])\n        preds = conf_data[key][\"predictions\"]\n        gt = conf_data[key][\"ground_truth\"]\n        test_accs.append((preds == gt).mean())\n    plt.figure()\n    plt.bar(thr_vals, test_accs, color=\"skyblue\")\n    plt.title(\n        \"Test Accuracy per Confidence Threshold\\nDataset: Synthetic Classification\"\n    )\n    plt.xlabel(\"Threshold\")\n    plt.ylabel(\"Test Accuracy\")\n    for i, v in enumerate(test_accs):\n        plt.text(i, v + 0.01, f\"{v:.2f}\", ha=\"center\")\n    plt.savefig(os.path.join(working_dir, \"confidence_filter_test_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n# Iterate ratios\nfor ratio, data in exp.get(\"class_imbalance\", {}).items():\n    # Compute test accuracies\n    accs = {}\n    for key, d in data.items():\n        preds = d[\"predictions\"]\n        gt = d[\"ground_truth\"]\n        accs[key] = np.mean(preds == gt)\n    # Print metrics\n    print(f\"{ratio} test accuracies:\")\n    for k, v in accs.items():\n        print(f\"  {k}: {v:.3f}\")\n    # Bar plot of test accuracies\n    try:\n        plt.figure()\n        keys, vals = zip(*sorted(accs.items()))\n        plt.bar(range(len(vals)), vals, tick_label=keys)\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.ylim(0, 1)\n        plt.title(f\"Test Accuracy for Class Imbalance {ratio}\")\n        plt.xlabel(\"Batch Size Settings\")\n        plt.ylabel(\"Accuracy\")\n        fname = f\"classimbalance_{ratio}_test_accuracy.png\"\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, fname))\n    except Exception as e:\n        print(f\"Error creating plot_{ratio}_testacc: {e}\")\n    finally:\n        plt.close()\n\n# Training/validation curves for two representative settings\nfor ratio in list(exp.get(\"class_imbalance\", {}))[:2]:\n    data = exp[\"class_imbalance\"][ratio]\n    # pick representative key\n    key = \"ai_bs_32_user_bs_32\"\n    if key not in data:\n        key = next(iter(data))\n    tr = data[key][\"metrics\"][\"train\"]\n    va = data[key][\"metrics\"][\"val\"]\n    try:\n        plt.figure()\n        plt.plot(tr, label=\"Train Acc\")\n        plt.plot(va, label=\"Val Acc\")\n        plt.title(f\"Train/Val Accuracy Curves {ratio}\\n(ai_bs_32_user_bs_32)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = f\"classimbalance_{ratio}_trainval_ai32_user32.png\"\n        plt.tight_layout()\n        plt.savefig(os.path.join(working_dir, fname))\n    except Exception as e:\n        print(f\"Error creating plot_{ratio}_trainval: {e}\")\n    finally:\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nsynth_data = experiment_data.get(\"network_depth_ablation\", {}).get(\"synthetic\", {})\n\nfor key, data in synth_data.items():\n    preds = data[\"predictions\"]\n    gt = data[\"ground_truth\"]\n    acc = (preds == gt).mean()\n    print(f\"{key} test accuracy: {acc:.4f}\")\n\nfor key, data in synth_data.items():\n    train_acc = data[\"metrics\"][\"train\"]\n    val_acc = data[\"metrics\"][\"val\"]\n    train_loss = data[\"losses\"][\"train\"]\n    val_loss = data[\"losses\"][\"val\"]\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n        axes[0].plot(train_acc, label=\"Train Acc\")\n        axes[0].plot(val_acc, label=\"Val Acc\")\n        axes[0].set_xlabel(\"Epoch\")\n        axes[0].set_ylabel(\"Accuracy\")\n        axes[0].set_title(\"Left: Accuracy over epochs\")\n        axes[0].legend()\n        axes[1].plot(train_loss, label=\"Train Loss\")\n        axes[1].plot(val_loss, label=\"Val Loss\")\n        axes[1].set_xlabel(\"Epoch\")\n        axes[1].set_ylabel(\"Loss\")\n        axes[1].set_title(\"Right: Loss over epochs\")\n        axes[1].legend()\n        fig.suptitle(f\"Synthetic Dataset - {key}\")\n        plot_path = os.path.join(working_dir, f\"synthetic_{key}_curves.png\")\n        fig.savefig(plot_path)\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating plot_{key}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    acts = list(exp.keys())\n    synth = {act: exp[act][\"synthetic\"] for act in acts}\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    synth = {}\n\n# Compute and print test accuracies\ntest_accs = {}\nfor act, data in synth.items():\n    preds = data[\"predictions\"]\n    gt = data[\"ground_truth\"]\n    acc = np.mean(preds == gt)\n    test_accs[act] = acc\n    print(f\"{act} test accuracy: {acc:.4f}\")\n\n# 1. AI loss curves\ntry:\n    plt.figure()\n    for act, data in synth.items():\n        plt.plot(data[\"ai_losses\"][\"train\"], label=f\"{act} train\")\n        plt.plot(data[\"ai_losses\"][\"val\"], linestyle=\"--\", label=f\"{act} val\")\n    plt.title(\"AI Loss Curves on Synthetic Dataset\\nSolid: Train, Dashed: Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross\u2010Entropy Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_ai_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\n# 2. AI accuracy curves\ntry:\n    plt.figure()\n    for act, data in synth.items():\n        plt.plot(data[\"ai_metrics\"][\"train\"], label=f\"{act} train\")\n        plt.plot(data[\"ai_metrics\"][\"val\"], linestyle=\"--\", label=f\"{act} val\")\n    plt.title(\"AI Accuracy Curves on Synthetic Dataset\\nSolid: Train, Dashed: Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_ai_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n\n# 3. User loss curves\ntry:\n    plt.figure()\n    for act, data in synth.items():\n        plt.plot(data[\"losses\"][\"train\"], label=f\"{act} train\")\n        plt.plot(data[\"losses\"][\"val\"], linestyle=\"--\", label=f\"{act} val\")\n    plt.title(\"User Loss Curves on Synthetic Dataset\\nSolid: Train, Dashed: Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross\u2010Entropy Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_user_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot3: {e}\")\n    plt.close()\n\n# 4. User accuracy curves\ntry:\n    plt.figure()\n    for act, data in synth.items():\n        plt.plot(data[\"metrics\"][\"train\"], label=f\"{act} train\")\n        plt.plot(data[\"metrics\"][\"val\"], linestyle=\"--\", label=f\"{act} val\")\n    plt.title(\"User Accuracy Curves on Synthetic Dataset\\nSolid: Train, Dashed: Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_user_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot4: {e}\")\n    plt.close()\n\n# 5. Test accuracy bar chart\ntry:\n    plt.figure()\n    acts_list = list(test_accs.keys())\n    acc_list = [test_accs[a] for a in acts_list]\n    plt.bar(acts_list, acc_list)\n    plt.title(\"Test Accuracy by Activation on Synthetic Dataset\")\n    plt.xlabel(\"Activation\")\n    plt.ylabel(\"Test Accuracy\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_test_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot5: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = experiment_data[\"original_feature_removal\"][\"synthetic\"]\n    keys = sorted(data.keys())\n    acc_train = {k: data[k][\"metrics\"][\"train\"] for k in keys}\n    acc_val = {k: data[k][\"metrics\"][\"val\"] for k in keys}\n    loss_train = {k: data[k][\"losses\"][\"train\"] for k in keys}\n    loss_val = {k: data[k][\"losses\"][\"val\"] for k in keys}\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    keys = []\n\n# Plot accuracy curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    for k in keys:\n        axes[0].plot(acc_train[k], label=k)\n        axes[1].plot(acc_val[k], label=k)\n    axes[0].set_title(\"Training Accuracy\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Accuracy\")\n    axes[1].set_title(\"Validation Accuracy\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Accuracy\")\n    axes[1].legend(loc=\"best\")\n    fig.suptitle(\"Accuracy Curves (Synthetic)\\nLeft: Training, Right: Validation\")\n    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n    fig.savefig(os.path.join(working_dir, \"synthetic_accuracy_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# Plot loss curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    for k in keys:\n        axes[0].plot(loss_train[k], label=k)\n        axes[1].plot(loss_val[k], label=k)\n    axes[0].set_title(\"Training Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[1].set_title(\"Validation Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    axes[1].legend(loc=\"best\")\n    fig.suptitle(\"Loss Curves (Synthetic)\\nLeft: Training, Right: Validation\")\n    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n    fig.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Bar chart of final validation accuracy\ntry:\n    final_val = [acc_val[k][-1] for k in keys]\n    fig = plt.figure(figsize=(8, 5))\n    plt.bar(keys, final_val)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.title(\"Final Validation Accuracy (Synthetic)\")\n    plt.ylabel(\"Accuracy\")\n    plt.tight_layout()\n    fig.savefig(os.path.join(working_dir, \"synthetic_final_val_accuracy.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating final accuracy bar plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Flatten all dataset configurations into a list\nitems = []\nfor size, cfgs in experiment_data.get(\"teacher_ensemble_size\", {}).items():\n    for ds_name, data in cfgs.items():\n        items.append((size, ds_name, data))\n\n# Sample at most five configurations evenly\nmax_plots = 5\ntotal = len(items)\nstep = max(1, total // max_plots)\nselected = [items[i] for i in range(0, total, step)][:max_plots]\n\n# Plot training vs validation accuracy for each selected config\nfor idx, (_size, ds_name, data) in enumerate(selected, 1):\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(data[\"metrics\"][\"train\"]) + 1)\n        plt.plot(epochs, data[\"metrics\"][\"train\"], label=\"Train Acc\")\n        plt.plot(epochs, data[\"metrics\"][\"val\"], label=\"Val Acc\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(f\"Accuracy Curves for {ds_name}\")\n        plt.suptitle(\"Training vs Validation Accuracy\", fontsize=10)\n        plt.legend()\n        fname = f\"{ds_name}_accuracy_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot{idx}: {e}\")\n        plt.close()\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor ablation, settings in experiment_data.items():\n    for key, res in settings.items():\n        preds = res[\"predictions\"]\n        gt = res[\"ground_truth\"]\n        acc = np.mean(preds == gt)\n        print(f\"Ablation: {ablation}, Setting: {key}, Test Accuracy: {acc:.4f}\")\n\nfor ablation in [\"uniform\", \"thresholded\", \"confidence_weighted\"]:\n    key = \"ai_bs_32_user_bs_32\"\n    try:\n        train_acc = experiment_data[ablation][key][\"metrics\"][\"train\"]\n        val_acc = experiment_data[ablation][key][\"metrics\"][\"val\"]\n        plt.figure()\n        plt.plot(\n            np.arange(1, len(train_acc) + 1), train_acc, label=\"Train\", linestyle=\"-\"\n        )\n        plt.plot(\n            np.arange(1, len(val_acc) + 1), val_acc, label=\"Validation\", linestyle=\"--\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.suptitle(\"Dataset: Synthetic Binary Classification\")\n        plt.title(\n            f\"{ablation.capitalize()} Ablation - Solid: Train, Dashed: Validation\"\n        )\n        fname = f\"synthetic_{ablation}_accuracy_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating {ablation} accuracy plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Flatten all dataset configurations into a list\nitems = []\nfor size, cfgs in experiment_data.get(\"teacher_ensemble_size\", {}).items():\n    for ds_name, data in cfgs.items():\n        items.append((size, ds_name, data))\n\n# Sample at most five configurations evenly\nmax_plots = 5\ntotal = len(items)\nstep = max(1, total // max_plots)\nselected = [items[i] for i in range(0, total, step)][:max_plots]\n\n# Plot training vs validation accuracy for each selected config\nfor idx, (_size, ds_name, data) in enumerate(selected, 1):\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(data[\"metrics\"][\"train\"]) + 1)\n        plt.plot(epochs, data[\"metrics\"][\"train\"], label=\"Train Acc\")\n        plt.plot(epochs, data[\"metrics\"][\"val\"], label=\"Val Acc\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(f\"Accuracy Curves for {ds_name}\")\n        plt.suptitle(\"Training vs Validation Accuracy\", fontsize=10)\n        plt.legend()\n        fname = f\"{ds_name}_accuracy_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot{idx}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Flatten all dataset configurations into a list\nitems = []\nfor size, cfgs in experiment_data.get(\"teacher_ensemble_size\", {}).items():\n    for ds_name, data in cfgs.items():\n        items.append((size, ds_name, data))\n\n# Sample at most five configurations evenly\nmax_plots = 5\ntotal = len(items)\nstep = max(1, total // max_plots)\nselected = [items[i] for i in range(0, total, step)][:max_plots]\n\n# Plot training vs validation accuracy for each selected config\nfor idx, (_size, ds_name, data) in enumerate(selected, 1):\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(data[\"metrics\"][\"train\"]) + 1)\n        plt.plot(epochs, data[\"metrics\"][\"train\"], label=\"Train Acc\")\n        plt.plot(epochs, data[\"metrics\"][\"val\"], label=\"Val Acc\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(f\"Accuracy Curves for {ds_name}\")\n        plt.suptitle(\"Training vs Validation Accuracy\", fontsize=10)\n        plt.legend()\n        fname = f\"{ds_name}_accuracy_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot{idx}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Flatten all dataset configurations into a list\nitems = []\nfor size, cfgs in experiment_data.get(\"teacher_ensemble_size\", {}).items():\n    for ds_name, data in cfgs.items():\n        items.append((size, ds_name, data))\n\n# Sample at most five configurations evenly\nmax_plots = 5\ntotal = len(items)\nstep = max(1, total // max_plots)\nselected = [items[i] for i in range(0, total, step)][:max_plots]\n\n# Plot training vs validation accuracy for each selected config\nfor idx, (_size, ds_name, data) in enumerate(selected, 1):\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(data[\"metrics\"][\"train\"]) + 1)\n        plt.plot(epochs, data[\"metrics\"][\"train\"], label=\"Train Acc\")\n        plt.plot(epochs, data[\"metrics\"][\"val\"], label=\"Val Acc\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(f\"Accuracy Curves for {ds_name}\")\n        plt.suptitle(\"Training vs Validation Accuracy\", fontsize=10)\n        plt.legend()\n        fname = f\"{ds_name}_accuracy_curve.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot{idx}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Group data by dataset name\ngroup_data = {}\nfor size, cfgs in experiment_data.get(\"teacher_ensemble_size\", {}).items():\n    for ds_name, data in cfgs.items():\n        group_data.setdefault(ds_name, []).append(data)\n\n# Select at most five datasets evenly\nds_names = list(group_data.keys())\nmax_plots = 5\nstep = max(1, len(ds_names) // max_plots)\nselected_ds = ds_names[0::step][:max_plots]\n\nfor ds_name in selected_ds:\n    # Collect train and validation curves\n    train_list, val_list = [], []\n    for d in group_data[ds_name]:\n        m = d.get(\"metrics\", {})\n        if \"train\" in m:\n            train_list.append(np.array(m[\"train\"]))\n        if \"val\" in m:\n            val_list.append(np.array(m[\"val\"]))\n    if not train_list:\n        continue\n\n    # Compute mean and SE for train\n    min_train = min(arr.shape[0] for arr in train_list)\n    train_arr = np.stack([arr[:min_train] for arr in train_list], axis=0)\n    train_mean = np.mean(train_arr, axis=0)\n    train_se = np.std(train_arr, axis=0) / np.sqrt(train_arr.shape[0])\n\n    # Compute mean and SE for validation (if available)\n    val_mean = val_se = None\n    if val_list:\n        min_val = min(arr.shape[0] for arr in val_list)\n        val_arr = np.stack([arr[:min_val] for arr in val_list], axis=0)\n        val_mean = np.mean(val_arr, axis=0)\n        val_se = np.std(val_arr, axis=0) / np.sqrt(val_arr.shape[0])\n\n    # Print final metrics\n    print(f\"{ds_name} final Train Acc = {train_mean[-1]:.4f} \u00b1 {train_se[-1]:.4f}\")\n    if val_mean is not None:\n        print(f\"{ds_name} final Val   Acc = {val_mean[-1]:.4f} \u00b1 {val_se[-1]:.4f}\")\n\n    # Plot with mean and SE\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n        epochs = np.arange(1, min_train + 1)\n        axes[0].plot(epochs, train_mean, label=\"Mean\")\n        axes[0].fill_between(\n            epochs, train_mean - train_se, train_mean + train_se, alpha=0.3, label=\"SE\"\n        )\n        axes[0].set_title(\"Train Accuracy\")\n        axes[0].set_xlabel(\"Epoch\")\n        axes[0].set_ylabel(\"Accuracy\")\n        axes[0].legend()\n\n        if val_mean is not None:\n            epochs_val = np.arange(1, val_mean.shape[0] + 1)\n            axes[1].plot(epochs_val, val_mean, label=\"Mean\")\n            axes[1].fill_between(\n                epochs_val, val_mean - val_se, val_mean + val_se, alpha=0.3, label=\"SE\"\n            )\n            axes[1].set_title(\"Validation Accuracy\")\n            axes[1].set_xlabel(\"Epoch\")\n            axes[1].set_ylabel(\"Accuracy\")\n            axes[1].legend()\n        else:\n            axes[1].axis(\"off\")\n\n        fig.suptitle(\n            f\"{ds_name}: Accuracy with Mean and SE (Left: Train, Right: Validation)\",\n            fontsize=12,\n        )\n        fig.tight_layout(rect=[0, 0, 1, 0.95])\n        fname = f\"{ds_name}_mean_se_accuracy.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating mean/se plot for {ds_name}: {e}\")\n        plt.close()\n"], "plot_plan": ["We will load the saved experiment data and extract the batch\u2010size keys. To avoid overcrowding, we select exactly five configurations evenly spaced across all combinations. For each selected run, we create a single figure with two side\u2010by\u2010side subplots: one for training vs. validation accuracy, and one for training vs. validation loss over epochs. Each subplot is clearly titled (\u201cAccuracy\u201d and \u201cLoss\u201d) and we add a composite suptitle indicating the synthetic binary dataset and the specific batch\u2010size setting. All figures are saved into the working directory under descriptive filenames. Each plotting routine is wrapped in a try\u2010except to ensure errors are caught and figures closed.", null, "We will load the saved experiment data from the working directory and extract both accuracy and loss histories along with final predictions and ground\u2010truth labels. We then compute and print the test accuracy for each ablation (\u201csoft\u201d and \u201chard\u201d) and each batch\u2010size combination. Next, for each ablation we generate one plot of training and validation accuracy curves (all settings overlaid) wrapped in a try\u2010except to catch errors and always close the figure afterward. We do the same for loss curves, again saving to files named by ablation and plot type. All plots will include clear titles specifying the synthetic logistic dataset, labeled axes, and legends, and will be saved under working_dir with descriptive filenames. No simulated data will be used, and only the existing metrics in experiment_data.npy will be plotted.", null, "I will load the saved experiment data, extract training/validation accuracy and loss curves for each temperature, and plot them in two aggregated figures with labeled curves. Each plotting block is surrounded by a try-except that ensures graceful error handling and figure closure. The accuracy plot will show solid lines for training and dashed lines for validation for each temperature, saved as `temperature_scaling_accuracy.png`. The loss plot follows the same pattern, saved as `temperature_scaling_loss.png`. Finally, I compute and print the test accuracy for each temperature by comparing predictions against ground truth. All plots are saved into the `working` directory, and titles clearly specify the synthetic binary dataset context.", "We\u2019ll load the saved experiment_data.npy from the working directory. We iterate over each scenario (\u201cwith_teacher_probs\u201d and \u201craw_features_only\u201d) and each AI/user batch size combination. For each, we extract the arrays of training and validation accuracies and losses. We then create a two\u2010panel figure: the left panel shows training vs validation accuracy, and the right panel shows training vs validation loss. We add clear subplot titles (\u201cLeft: Train vs Val Accuracy\u201d and \u201cRight: Train vs Val Loss\u201d) and a super\u2010title indicating the scenario and key. We save each figure to the working directory with a descriptive filename. We wrap each plotting block in try/except to catch errors and always close the figure. This ensures all available training/validation curves are visualized without simulating any new data.", null, "We load the saved experiment dictionary and pick the central batch\u2010size setting `ai_bs_32_user_bs_32`.  In separate try/except blocks, we iterate over the four scenarios to plot training vs validation accuracy across epochs, labeling axes, including a multi\u2010line title specifying dataset type and color mapping, and saving each figure under a descriptive filename.  Finally, in its own try/except, we plot the dual_channel scenario\u2019s alignment rate over epochs with legend, title, and dataset note.  All plots are saved to `working_dir` and figures are always closed even on error.", "We first load the saved experiment data from the working directory and focus on a representative hyperparameter configuration (AI batch size 32 and user batch size 32). For each of the three synthetic datasets, we extract the training/validation accuracy curves, alignment scores, alignment rates (slope), and test predictions vs. ground truth. We compute the test accuracy by comparing the user model\u2019s predictions to the true labels. We then create four separate figures: (1) overlayed training and validation accuracy curves across datasets, (2) overlayed alignment score curves across datasets, (3) a bar chart of alignment rates, and (4) a bar chart of test accuracies. Each figure is generated within its own try-except block to catch and report any errors, with descriptive titles including a subtitle listing the dataset names. We save all plots to the working directory using clear, descriptive filenames and close each figure after saving. The code uses only numpy, matplotlib, and os.", "We will load the saved experiment data from the working directory and extract per\u2010threshold accuracy and loss curves as well as test predictions. Next, we will plot training vs validation accuracy trajectories for each confidence threshold on a single figure with clear labels and legends. Then, similarly, we will plot the corresponding loss curves. Finally, we compute the overall test accuracy for each threshold and render a bar chart annotated with exact values. Each figure is generated inside its own try\u2010except block to catch errors, given descriptive titles including dataset context, and saved using a filename that reflects the plot type and dataset. All plots are closed after saving to free resources and ensure figures do not overlap.", "We will load the saved experiment data and iterate over each class\u2010imbalance setting. For each ratio, we compute and print the test accuracy for all AI/user batch\u2010size combinations. We then produce a bar chart of these accuracies per ratio, saving it under a descriptive filename. To illustrate training dynamics, we select two representative settings (e.g. ai_bs_32_user_bs_32) for the first two ratios and plot their train/validation accuracy curves. All plots are wrapped in try\u2010except blocks and use clear titles including the imbalance ratio and plot type. Figures are saved into the working directory with filenames that include the dataset ratio and plot description. Each figure is closed after saving to avoid resource leaks. Only existing data in experiment_data.npy is used, and no extra simulations are performed.", "I will start by loading the experiment data from the working directory using numpy. After extracting the synthetic ablation results, I will iterate over each AI and user depth combination. For each model, I will compute the test\u2010set accuracy by comparing predictions to ground truth and print this metric. I will then create a figure with two subplots: the left panel will show training and validation accuracy curves over epochs, and the right panel will show the corresponding loss curves. Each plot will have a main title specifying the synthetic dataset and the specific depth combination, and subtitles labeling the left and right panels. I will save each figure to the working directory with a descriptive filename that includes the dataset name and depth key. All figure generation will be wrapped in separate try\u2010except blocks to catch errors, and I will ensure figures are closed after saving. Finally, I will make sure the working directory exists before saving any plots.", "I will load the saved experiment results and extract the \u201csynthetic\u201d subset for all activation variants. First, I will compute and print the test accuracy per activation by comparing stored predictions and ground truth. Then I will create five standard plots in separate try\u2010except blocks: (1) AI train/validation loss curves across epochs for each activation, (2) AI train/validation accuracy curves, (3) User train/validation loss curves, (4) User train/validation accuracy curves, and (5) a bar chart of test accuracy by activation. Each figure will be titled to indicate the synthetic dataset and will note train vs. validation phases in a subtitle. All plots are saved to the working directory with descriptive filenames, and figures are closed after saving. Any loading or plotting errors are caught and printed to avoid runtime crashes.", "We will set up the working directory and load the saved experiment data, extracting the synthetic ablation-study results for each (ai_bs, usr_bs) configuration. We then build three standard plots: (1) Accuracy curves with a two-panel figure showing training accuracy on the left and validation accuracy on the right, overlaying each configuration; (2) Loss curves similarly arranged; (3) A bar chart of the final validation accuracy (last epoch) for all configurations. All figures include descriptive titles mentioning the \u201csynthetic\u201d dataset, clear subtitles for left/right panels, and legends for each batch-size setting. We wrap each plotting section in its own try/except block for robustness, save to the working directory with informative filenames, and close figures after saving to free resources. This provides clear, concise visual summaries of how varying AI and user batch sizes impacts model performance.", "The code loads the saved experiment data and flattens the nested structure of ensemble sizes and dataset configurations into a single list of items. It then selects up to five evenly spaced dataset entries to avoid overcrowding figures. For each selected configuration, it creates a separate figure inside a try\u2010except block, plots the training and validation accuracy curves over epochs, and labels the axes, title, and a succinct subtitle indicating the comparison. Each figure is saved to the working directory with a descriptive filename including the dataset name and plot type. The figure is closed in both the normal and exception paths to free resources. All operations use basic matplotlib and numpy, and the code starts with the required imports and working directory setup.", null, null, "We begin by loading the saved experiment_data dictionary from the working directory. Then we iterate through each ablation method and its settings to compute and print the test accuracy for each AI and user batch-size combination. Next, we select a representative configuration (ai_bs_32_user_bs_32) for each ablation to visualize the learning curves. For each of the three ablation regimes, we wrap the plotting logic in a try-except block to gracefully handle errors. We plot training and validation accuracy against epochs, using solid and dashed lines respectively, and include both a main title and subtitle that specify the synthetic classification dataset and line style conventions. The figures are saved with descriptive filenames that include the dataset name and plot type into the working folder. All figures are explicitly closed in both the success and exception paths to free resources.", null, null, null, null], "ablation_name": [null, "Synthetic Dataset Diversity Ablation", "Hard Label Input Ablation", "Soft\u2010Label Distillation Loss Ablation", "Temperature Scaling Ablation", "Teacher Feature Removal Ablation", null, null, null, "Confidence\u2010Filtered Pseudo\u2010Labeling", "Class Imbalance Ablation", "Network Depth Ablation", "Activation Function Ablation", "Original Feature Removal Ablation", "Teacher Ensemble Size Ablation", "Teacher Probability Noise Injection Ablation", null, "Confidence-Weighted Pseudo-Label Loss", null, null, null, null], "hyperparam_name": ["batch_size", null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["I will load the saved experiment data from the working directory, then loop over\neach hyperparameter setting under the \"batch_size\" field. For each dataset I\nextract the final training and validation accuracy and loss from the recorded\narrays, compute the test accuracy from the stored predictions and ground truths,\nand print the dataset name followed by clearly labeled metric values. The script\nruns immediately at global scope and does not use any special entry point.", "I will load the NumPy experiment data file from the designated working directory\nand unpack its contents. Then I\u2019ll loop over each synthetic diversity dataset\nand its batch\u2010size configurations to extract the final epoch train/validation\naccuracies and losses. Using scikit\u2010learn metrics, I\u2019ll compute test accuracy,\nprecision, recall, and F1 score from the stored predictions and ground truth.\nFor each dataset/configuration, I\u2019ll print the dataset name followed by clearly\nlabeled metrics such as \u201ctrain accuracy\u201d, \u201cvalidation loss\u201d, \u201ctest F1 score\u201d,\nand so on. The script runs immediately at the global scope without any `if\n__name__ == \"__main__\":` guard.", "I will write a small script that loads the saved NumPy file from the working\ndirectory, iterates over both \"soft\" and \"hard\" ablations and each AI/User\nbatch\u2010size configuration, and for each case extracts the final training and\nvalidation accuracy and loss, as well as computes test accuracy from the saved\npredictions.  The code then prints each dataset\u2019s name followed by clearly\nlabeled metrics (train accuracy, validation loss, etc.).  All execution happens\nat the global scope so the script runs immediately.", "The following script loads the saved experiment data from the \u201cworking\u201d\ndirectory and iterates over each experiment type and dataset configuration. For\neach combination, it extracts the final training and validation accuracies and\nlosses, computes the overall test accuracy from the stored predictions and\nground truth, and prints them with clear labels. The dataset identifier combines\nthe experiment type and batch\u2010size settings so that each printed block is\nunambiguous. All code runs immediately at import time without any special entry\npoint guards.", "The script begins by loading the saved `experiment_data.npy` dictionary from the\n`working` folder. It then iterates over each temperature\u2010scaled experiment,\nextracts the final training and validation accuracy and loss values from the\nstored arrays, and computes the overall test accuracy from the saved predictions\nand ground truth. For each experiment key (e.g. `T_0.5`), it prints the dataset\nname followed by clearly labeled metrics. All code executes at the global scope\nand prints results immediately when run.", "I will load the saved experiment data dictionary from the working directory,\nthen iterate through each scenario and batch-size combination under the\n\"teacher_feature_removal\" key. For each dataset, I will extract the last\nrecorded train and validation accuracies and losses, compute the test accuracy\nfrom the stored predictions and ground truth, and then print out the dataset\nname followed by clearly labeled metric values. The code runs at the global\nscope and executes immediately upon invocation.", "", "The following script loads the `experiment_data.npy` file from the working\ndirectory, iterates through each scenario and configuration, and retrieves the\nfinal epoch\u2019s train accuracy, validation accuracy, alignment rate, train loss,\nvalidation loss, and computes test accuracy from the stored predictions and\nground truth. Each metric is printed with a clear descriptive label under its\ndataset and configuration name. The code runs immediately at the global scope\nwithout any entry\u2010point guard.", "Below is a script that loads the saved experiment data from the working\ndirectory, iterates over each synthetic dataset and its batch\u2010size\nconfigurations, and prints the final training and validation accuracies, the\nfinal alignment score, the alignment\u2010rate slope, and the overall test accuracy\nfor each configuration. Metrics are clearly labeled, and the code runs\nimmediately at global scope without requiring any special entry point.", "I will load the experiment data from the working directory and iterate over all\nconfidence thresholds. For each threshold, I will extract the final epoch values\nfor training accuracy, validation accuracy, training loss, and validation loss.\nI will compute test accuracy by comparing stored predictions to the ground truth\nlabels. Finally, I will print the dataset name followed by clearly labeled\nmetric values.", "The script below loads the saved `experiment_data.npy` from the `working`\ndirectory, iterates over each class\u2010imbalance scenario and each AI/user\nbatch\u2010size combination, and extracts the final training/validation accuracies\nand losses.  It then computes test accuracy and F1 score directly from the\nstored predictions, and prints the dataset name followed by clearly labeled\nmetrics (e.g. \u201ctrain accuracy,\u201d \u201cvalidation loss,\u201d \u201ctest F1 score\u201d).", "", "The following code loads the saved experiment data, iterates over each\nactivation variant and its synthetic dataset, extracts the final training and\nvalidation losses and accuracies for both the AI and User models, computes the\ntest accuracy, and prints each metric with clear, descriptive labels.", "", "I will load the saved experiment data from the working directory, iterate\nthrough each ensemble size and dataset configuration, and extract the stored\ntrain and validation accuracy arrays. For each dataset, I compute the maximum\ntrain accuracy and maximum validation accuracy, and compute the test accuracy by\ncomparing stored predictions to ground truth labels. The script prints the\ndataset name followed by clearly labeled metrics at the global scope, so it\nexecutes immediately without requiring a special entry point.", "The following script immediately loads the experimental results from the working\ndirectory, iterates through each noise setting and batch\u2010size combination, and\nextracts the stored training/validation accuracy and loss arrays. It then takes\nthe final epoch values for train/validation metrics, computes the test accuracy\nfrom predictions and ground truth, and prints the dataset identifier and each\nmetric with precise labels. All code is at the global level and executes without\nrequiring any entry\u2010point check.", "We first load the saved experimental data from the working directory using\nnumpy. We then loop over each noise setting and batch-size combination, extract\nthe final train/validation accuracy and loss, compute the test accuracy from\npredictions and ground truth, and print them with clear labels. Each dataset is\nlabeled by its noise key and batch-size key before printing its metrics. All\ncode runs at global scope without any `if __name__ == \"__main__\":` guard.", "", "I will load the saved experiment data from the working directory, iterate\nthrough each ensemble size and dataset configuration, and extract the stored\ntrain and validation accuracy arrays. For each dataset, I compute the maximum\ntrain accuracy and maximum validation accuracy, and compute the test accuracy by\ncomparing stored predictions to ground truth labels. The script prints the\ndataset name followed by clearly labeled metrics at the global scope, so it\nexecutes immediately without requiring a special entry point.", "I will load the saved experiment data from the working directory, iterate\nthrough each ensemble size and dataset configuration, and extract the stored\ntrain and validation accuracy arrays. For each dataset, I compute the maximum\ntrain accuracy and maximum validation accuracy, and compute the test accuracy by\ncomparing stored predictions to ground truth labels. The script prints the\ndataset name followed by clearly labeled metrics at the global scope, so it\nexecutes immediately without requiring a special entry point.", "I will load the saved experiment data from the working directory, iterate\nthrough each ensemble size and dataset configuration, and extract the stored\ntrain and validation accuracy arrays. For each dataset, I compute the maximum\ntrain accuracy and maximum validation accuracy, and compute the test accuracy by\ncomparing stored predictions to ground truth labels. The script prints the\ndataset name followed by clearly labeled metrics at the global scope, so it\nexecutes immediately without requiring a special entry point.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset setting and print final metrics\nfor dataset_name, results in experiment_data.get(\"batch_size\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    # Final train accuracy and loss\n    train_acc = results[\"metrics\"][\"train\"][-1]\n    train_loss = results[\"losses\"][\"train\"][-1]\n    print(f\"train accuracy: {train_acc:.4f}\")\n    print(f\"train loss: {train_loss:.4f}\")\n    # Final validation accuracy and loss\n    val_acc = results[\"metrics\"][\"val\"][-1]\n    val_loss = results[\"losses\"][\"val\"][-1]\n    print(f\"validation accuracy: {val_acc:.4f}\")\n    print(f\"validation loss: {val_loss:.4f}\")\n    # Compute and print test accuracy\n    preds = results[\"predictions\"]\n    gt = results[\"ground_truth\"]\n    test_accuracy = np.mean(preds == gt)\n    print(f\"test accuracy: {test_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each synthetic diversity dataset\nfor dataset_name, ds_dict in experiment_data.get(\"synthetic_diversity\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    # Iterate through each AI/User batch-size configuration\n    for config_key, results in ds_dict.get(\"batch_size\", {}).items():\n        print(f\"Configuration: {config_key}\")\n        # Extract final train/validation accuracies and losses\n        train_accs = results[\"metrics\"][\"train\"]\n        val_accs = results[\"metrics\"][\"val\"]\n        train_losses = results[\"losses\"][\"train\"]\n        val_losses = results[\"losses\"][\"val\"]\n        final_train_acc = train_accs[-1]\n        final_val_acc = val_accs[-1]\n        final_train_loss = train_losses[-1]\n        final_val_loss = val_losses[-1]\n        # Compute test metrics\n        preds = results[\"predictions\"]\n        gt = results[\"ground_truth\"]\n        test_acc = accuracy_score(gt, preds)\n        test_prec = precision_score(gt, preds, zero_division=0)\n        test_rec = recall_score(gt, preds, zero_division=0)\n        test_f1 = f1_score(gt, preds, zero_division=0)\n        # Print labeled metrics\n        print(f\"  train accuracy:       {final_train_acc:.4f}\")\n        print(f\"  validation accuracy:  {final_val_acc:.4f}\")\n        print(f\"  train loss:           {final_train_loss:.4f}\")\n        print(f\"  validation loss:      {final_val_loss:.4f}\")\n        print(f\"  test accuracy:        {test_acc:.4f}\")\n        print(f\"  test precision:       {test_prec:.4f}\")\n        print(f\"  test recall:          {test_rec:.4f}\")\n        print(f\"  test F1 score:        {test_f1:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate and print final/best metrics for each dataset\nfor ablation, configurations in experiment_data.items():\n    for config_key, results in configurations.items():\n        # Print dataset identifier\n        print(f\"Dataset: ablation={ablation}, config={config_key}\")\n\n        # Extract final metrics\n        train_acc = results[\"metrics\"][\"train\"][-1]\n        val_acc = results[\"metrics\"][\"val\"][-1]\n        train_loss = results[\"losses\"][\"train\"][-1]\n        val_loss = results[\"losses\"][\"val\"][-1]\n\n        # Compute test accuracy from predictions\n        preds = results[\"predictions\"]\n        gt = results[\"ground_truth\"]\n        test_acc = (preds == gt).mean()\n\n        # Print metrics with clear labels\n        print(f\"train accuracy: {train_acc:.4f}\")\n        print(f\"validation accuracy: {val_acc:.4f}\")\n        print(f\"train loss: {train_loss:.4f}\")\n        print(f\"validation loss: {val_loss:.4f}\")\n        print(f\"test accuracy: {test_acc:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data dictionary from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each experiment type and dataset configuration\nfor method, method_data in experiment_data.items():\n    for dataset_name, results in method_data.items():\n        # Build a clear dataset identifier including the experiment type\n        full_name = f\"{method}/{dataset_name}\"\n        print(f\"Dataset: {full_name}\")\n\n        # Extract the final epoch metrics\n        train_acc = results[\"metrics\"][\"train\"][-1]\n        val_acc = results[\"metrics\"][\"val\"][-1]\n        train_loss = results[\"losses\"][\"train\"][-1]\n        val_loss = results[\"losses\"][\"val\"][-1]\n\n        # Compute test accuracy from stored predictions and ground truth\n        preds = results[\"predictions\"]\n        truth = results[\"ground_truth\"]\n        test_acc = (preds == truth).mean()\n\n        # Print out each metric with clear labels\n        print(f\"  train accuracy: {train_acc:.4f}\")\n        print(f\"  validation accuracy: {val_acc:.4f}\")\n        print(f\"  train loss: {train_loss:.4f}\")\n        print(f\"  validation loss: {val_loss:.4f}\")\n        print(f\"  test accuracy: {test_acc:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# extract temperature scaling experiments\ntemp_scaling = experiment_data.get(\"temperature_scaling\", {})\n\n# iterate and print final metrics\nfor key in sorted(temp_scaling.keys()):\n    entry = temp_scaling[key]\n    # final train/validation accuracy\n    train_acc = entry[\"metrics\"][\"train\"][-1]\n    val_acc = entry[\"metrics\"][\"val\"][-1]\n    # final train/validation loss\n    train_loss = entry[\"losses\"][\"train\"][-1]\n    val_loss = entry[\"losses\"][\"val\"][-1]\n    # compute test accuracy from predictions\n    preds = entry[\"predictions\"]\n    gt = entry[\"ground_truth\"]\n    test_acc = np.mean(preds == gt)\n\n    # print results\n    print(f\"Dataset: {key}\")\n    print(f\"train accuracy: {train_acc:.4f}\")\n    print(f\"validation accuracy: {val_acc:.4f}\")\n    print(f\"train loss: {train_loss:.4f}\")\n    print(f\"validation loss: {val_loss:.4f}\")\n    print(f\"test accuracy: {test_acc:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Navigate to the teacher_feature_removal experiments\ntfr = experiment_data.get(\"teacher_feature_removal\", {})\n\n# Iterate through each scenario and configuration\nfor scenario, configs in tfr.items():\n    for config_name, data in configs.items():\n        # Extract metrics and losses\n        train_accs = data[\"metrics\"][\"train\"]\n        val_accs = data[\"metrics\"][\"val\"]\n        train_losses = data[\"losses\"][\"train\"]\n        val_losses = data[\"losses\"][\"val\"]\n\n        # Compute final metrics\n        final_train_acc = train_accs[-1]\n        final_val_acc = val_accs[-1]\n        final_train_loss = train_losses[-1]\n        final_val_loss = val_losses[-1]\n\n        # Compute test accuracy\n        preds = data[\"predictions\"]\n        gt = data[\"ground_truth\"]\n        test_acc = np.mean(preds == gt)\n\n        # Print results\n        print(f\"Dataset: {scenario} | Configuration: {config_name}\")\n        print(f\"  train accuracy: {final_train_acc:.4f}\")\n        print(f\"  validation accuracy: {final_val_acc:.4f}\")\n        print(f\"  train loss: {final_train_loss:.4f}\")\n        print(f\"  validation loss: {final_val_loss:.4f}\")\n        print(f\"  test accuracy: {test_acc:.4f}\\n\")\n", "", "import os\nimport numpy as np\n\n# Locate and load the saved experimental results\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_file = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_file, allow_pickle=True).item()\n\n# Iterate over each dataset scenario and configuration\nfor dataset_name, configs in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n    for config_key, results in configs.items():\n        print(f\"Configuration: {config_key}\")\n        # Extract metrics and losses\n        train_acc = results[\"metrics\"][\"train\"][-1]\n        val_acc = results[\"metrics\"][\"val\"][-1]\n        align_rate = results[\"metrics\"][\"alignment_rate\"][-1]\n        train_loss = results[\"losses\"][\"train\"][-1]\n        val_loss = results[\"losses\"][\"val\"][-1]\n        # Compute test accuracy from saved predictions\n        preds = results[\"predictions\"]\n        gt = results[\"ground_truth\"]\n        test_acc = np.mean(preds == gt)\n        # Print all final metrics with clear labels\n        print(f\"Train accuracy: {train_acc:.4f}\")\n        print(f\"Validation accuracy: {val_acc:.4f}\")\n        print(f\"Alignment rate: {align_rate:.4f}\")\n        print(f\"Train loss: {train_loss:.4f}\")\n        print(f\"Validation loss: {val_loss:.4f}\")\n        print(f\"Test accuracy: {test_acc:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each dataset in synthetic_diversity\nfor dataset_name, dataset_info in experiment_data.get(\n    \"synthetic_diversity\", {}\n).items():\n    print(f\"Dataset: {dataset_name}\")\n    batch_configs = dataset_info.get(\"batch_size\", {})\n    # Iterate over each batch-size configuration\n    for config_key, config_data in batch_configs.items():\n        print(f\"  Configuration: {config_key}\")\n        # Extract metrics arrays\n        train_accs = config_data[\"metrics\"][\"train_accs\"]\n        val_accs = config_data[\"metrics\"][\"val_accs\"]\n        align_scores = config_data[\"metrics\"][\"alignment_scores\"]\n        alignment_rate = config_data[\"alignment_rate\"]\n        preds = config_data[\"predictions\"]\n        gt = config_data[\"ground_truth\"]\n        # Compute final and test metrics\n        final_train_acc = train_accs[-1]\n        final_val_acc = val_accs[-1]\n        final_alignment_score = align_scores[-1]\n        test_accuracy = np.mean(preds == gt)\n        # Print metrics with clear labels\n        print(f\"    Final train accuracy: {final_train_acc:.4f}\")\n        print(f\"    Final validation accuracy: {final_val_acc:.4f}\")\n        print(f\"    Final alignment score: {final_alignment_score:.4f}\")\n        print(f\"    Alignment rate (slope): {alignment_rate:.4f}\")\n        print(f\"    Test accuracy: {test_accuracy:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_data = np.load(\n    os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n).item()\n\n# Iterate over each confidence threshold dataset\nfor key in sorted(experiment_data.get(\"confidence_filter\", {})):\n    exp = experiment_data[\"confidence_filter\"][key]\n    metrics = exp[\"metrics\"]\n    losses = exp[\"losses\"]\n    preds = exp[\"predictions\"]\n    gt = exp[\"ground_truth\"]\n\n    # Final values for metrics and losses\n    train_acc_final = metrics[\"train\"][-1]\n    val_acc_final = metrics[\"val\"][-1]\n    train_loss_final = losses[\"train\"][-1]\n    val_loss_final = losses[\"val\"][-1]\n\n    # Compute test accuracy\n    test_acc = (preds == gt).mean()\n\n    # Print results with clear labels\n    print(f\"Dataset: {key}\")\n    print(f\"Training accuracy: {train_acc_final:.4f}\")\n    print(f\"Validation accuracy: {val_acc_final:.4f}\")\n    print(f\"Training loss: {train_loss_final:.4f}\")\n    print(f\"Validation loss: {val_loss_final:.4f}\")\n    print(f\"Test accuracy: {test_acc:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each class\u2010imbalance dataset\nfor dataset_name, dataset_dict in experiment_data[\"class_imbalance\"].items():\n    print(f\"Dataset: {dataset_name}\")\n    # Iterate through each AI/user batch\u2010size experiment\n    for exp_key, exp_res in dataset_dict.items():\n        # Extract batch sizes from the key string\n        parts = exp_key.split(\"_\")\n        ai_bs = parts[2]\n        usr_bs = parts[5]\n        # Extract stored metrics and losses\n        train_accs = exp_res[\"metrics\"][\"train\"]\n        val_accs = exp_res[\"metrics\"][\"val\"]\n        train_losses = exp_res[\"losses\"][\"train\"]\n        val_losses = exp_res[\"losses\"][\"val\"]\n        preds = exp_res[\"predictions\"]\n        gts = exp_res[\"ground_truth\"]\n        # Final (last\u2010epoch) values\n        final_train_acc = train_accs[-1]\n        final_val_acc = val_accs[-1]\n        final_train_loss = train_losses[-1]\n        final_val_loss = val_losses[-1]\n        # Compute test accuracy\n        test_acc = np.mean(preds == gts)\n        # Compute test F1 score for the positive class (label=1)\n        tp = np.sum((preds == 1) & (gts == 1))\n        fp = np.sum((preds == 1) & (gts == 0))\n        fn = np.sum((preds == 0) & (gts == 1))\n        precision = tp / (tp + fp) if (tp + fp) else 0.0\n        recall = tp / (tp + fn) if (tp + fn) else 0.0\n        test_f1 = (\n            (2 * precision * recall / (precision + recall))\n            if (precision + recall)\n            else 0.0\n        )\n        # Print results with clear labels\n        print(f\"  AI batch size: {ai_bs}, User batch size: {usr_bs}\")\n        print(f\"    Train accuracy: {final_train_acc:.4f}\")\n        print(f\"    Validation accuracy: {final_val_acc:.4f}\")\n        print(f\"    Train loss: {final_train_loss:.4f}\")\n        print(f\"    Validation loss: {final_val_loss:.4f}\")\n        print(f\"    Test accuracy: {test_acc:.4f}\")\n        print(f\"    Test F1 score: {test_f1:.4f}\")\n", "", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each activation variant and dataset, then print final metrics\nfor act, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"Dataset: {dataset_name} (activation: {act})\")\n        # AI model final metrics\n        ai_train_loss = data[\"ai_losses\"][\"train\"][-1]\n        ai_val_loss = data[\"ai_losses\"][\"val\"][-1]\n        ai_train_acc = data[\"ai_metrics\"][\"train\"][-1]\n        ai_val_acc = data[\"ai_metrics\"][\"val\"][-1]\n        print(f\"AI model train cross-entropy loss: {ai_train_loss:.4f}\")\n        print(f\"AI model validation cross-entropy loss: {ai_val_loss:.4f}\")\n        print(f\"AI model train accuracy: {ai_train_acc:.4f}\")\n        print(f\"AI model validation accuracy: {ai_val_acc:.4f}\")\n        # User model final metrics\n        usr_train_loss = data[\"losses\"][\"train\"][-1]\n        usr_val_loss = data[\"losses\"][\"val\"][-1]\n        usr_train_acc = data[\"metrics\"][\"train\"][-1]\n        usr_val_acc = data[\"metrics\"][\"val\"][-1]\n        print(f\"User model train cross-entropy loss: {usr_train_loss:.4f}\")\n        print(f\"User model validation cross-entropy loss: {usr_val_loss:.4f}\")\n        print(f\"User model train accuracy: {usr_train_acc:.4f}\")\n        print(f\"User model validation accuracy: {usr_val_acc:.4f}\")\n        # Compute and print test accuracy\n        predictions = data[\"predictions\"]\n        ground_truth = data[\"ground_truth\"]\n        test_accuracy = (predictions == ground_truth).mean()\n        print(f\"User model test accuracy: {test_accuracy:.4f}\\n\")\n", "", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each teacher ensemble size and dataset\nfor size_str, datasets in experiment_data[\"teacher_ensemble_size\"].items():\n    for ds_name, ds_data in datasets.items():\n        # Extract accuracy metrics\n        train_accs = ds_data[\"metrics\"][\"train\"]\n        val_accs = ds_data[\"metrics\"][\"val\"]\n        best_train_acc = float(np.max(train_accs))\n        best_val_acc = float(np.max(val_accs))\n        # Compute test accuracy\n        preds = ds_data[\"predictions\"]\n        gt = ds_data[\"ground_truth\"]\n        test_acc = float(np.mean(preds == gt))\n        # Print dataset and metrics\n        print(f\"Dataset: {ds_name}\")\n        print(f\"Train accuracy: {best_train_acc:.4f}\")\n        print(f\"Validation accuracy: {best_val_acc:.4f}\")\n        print(f\"Test accuracy: {test_acc:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over noise levels and batch-size configurations\nfor sigma_key, sigma_data in experiment_data[\"teacher_prob_noise\"].items():\n    for dataset_key, results in sigma_data[\"batch_size\"].items():\n        train_accs = results[\"metrics\"][\"train\"]\n        val_accs = results[\"metrics\"][\"val\"]\n        train_losses = results[\"losses\"][\"train\"]\n        val_losses = results[\"losses\"][\"val\"]\n        preds = results[\"predictions\"]\n        gt = results[\"ground_truth\"]\n\n        # Extract final metrics\n        final_train_acc = train_accs[-1]\n        final_val_acc = val_accs[-1]\n        final_train_loss = train_losses[-1]\n        final_val_loss = val_losses[-1]\n        test_accuracy = (preds == gt).mean()\n\n        # Print dataset name and final metrics\n        print(f\"Dataset: {sigma_key}, {dataset_key}\")\n        print(f\"train accuracy: {final_train_acc:.4f}\")\n        print(f\"validation accuracy: {final_val_acc:.4f}\")\n        print(f\"train loss: {final_train_loss:.4f}\")\n        print(f\"validation loss: {final_val_loss:.4f}\")\n        print(f\"test accuracy: {test_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nfile_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each noise level and batch-size configuration\nfor noise_key, noise_group in experiment_data[\"teacher_prob_noise\"].items():\n    for bs_key, results in noise_group[\"batch_size\"].items():\n        # Construct and print the dataset name\n        dataset_name = f\"{noise_key} | {bs_key}\"\n        print(f\"Dataset: {dataset_name}\")\n\n        # Extract final train/validation accuracy and loss\n        train_acc = results[\"metrics\"][\"train\"][-1]\n        val_acc = results[\"metrics\"][\"val\"][-1]\n        train_loss = results[\"losses\"][\"train\"][-1]\n        val_loss = results[\"losses\"][\"val\"][-1]\n\n        # Compute test accuracy from predictions vs. ground truth\n        preds = results[\"predictions\"]\n        gt = results[\"ground_truth\"]\n        test_acc = np.mean(preds == gt)\n\n        # Print metrics with clear labels\n        print(f\"  Train accuracy: {train_acc:.4f}\")\n        print(f\"  Validation accuracy: {val_acc:.4f}\")\n        print(f\"  Train loss: {train_loss:.4f}\")\n        print(f\"  Validation loss: {val_loss:.4f}\")\n        print(f\"  Test accuracy: {test_acc:.4f}\\n\")\n", "", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each teacher ensemble size and dataset\nfor size_str, datasets in experiment_data[\"teacher_ensemble_size\"].items():\n    for ds_name, ds_data in datasets.items():\n        # Extract accuracy metrics\n        train_accs = ds_data[\"metrics\"][\"train\"]\n        val_accs = ds_data[\"metrics\"][\"val\"]\n        best_train_acc = float(np.max(train_accs))\n        best_val_acc = float(np.max(val_accs))\n        # Compute test accuracy\n        preds = ds_data[\"predictions\"]\n        gt = ds_data[\"ground_truth\"]\n        test_acc = float(np.mean(preds == gt))\n        # Print dataset and metrics\n        print(f\"Dataset: {ds_name}\")\n        print(f\"Train accuracy: {best_train_acc:.4f}\")\n        print(f\"Validation accuracy: {best_val_acc:.4f}\")\n        print(f\"Test accuracy: {test_acc:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each teacher ensemble size and dataset\nfor size_str, datasets in experiment_data[\"teacher_ensemble_size\"].items():\n    for ds_name, ds_data in datasets.items():\n        # Extract accuracy metrics\n        train_accs = ds_data[\"metrics\"][\"train\"]\n        val_accs = ds_data[\"metrics\"][\"val\"]\n        best_train_acc = float(np.max(train_accs))\n        best_val_acc = float(np.max(val_accs))\n        # Compute test accuracy\n        preds = ds_data[\"predictions\"]\n        gt = ds_data[\"ground_truth\"]\n        test_acc = float(np.mean(preds == gt))\n        # Print dataset and metrics\n        print(f\"Dataset: {ds_name}\")\n        print(f\"Train accuracy: {best_train_acc:.4f}\")\n        print(f\"Validation accuracy: {best_val_acc:.4f}\")\n        print(f\"Test accuracy: {test_acc:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each teacher ensemble size and dataset\nfor size_str, datasets in experiment_data[\"teacher_ensemble_size\"].items():\n    for ds_name, ds_data in datasets.items():\n        # Extract accuracy metrics\n        train_accs = ds_data[\"metrics\"][\"train\"]\n        val_accs = ds_data[\"metrics\"][\"val\"]\n        best_train_acc = float(np.max(train_accs))\n        best_val_acc = float(np.max(val_accs))\n        # Compute test accuracy\n        preds = ds_data[\"predictions\"]\n        gt = ds_data[\"ground_truth\"]\n        test_acc = float(np.mean(preds == gt))\n        # Print dataset and metrics\n        print(f\"Dataset: {ds_name}\")\n        print(f\"Train accuracy: {best_train_acc:.4f}\")\n        print(f\"Validation accuracy: {best_val_acc:.4f}\")\n        print(f\"Test accuracy: {test_acc:.4f}\\n\")\n", ""], "parse_term_out": ["['Dataset: ai_bs_16_user_bs_16', '\\n', 'train accuracy: 0.9933', '\\n', 'train\nloss: 0.0152', '\\n', 'validation accuracy: 0.9933', '\\n', 'validation loss:\n0.0095', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Dataset: ai_bs_16_user_bs_32',\n'\\n', 'train accuracy: 0.9942', '\\n', 'train loss: 0.0162', '\\n', 'validation\naccuracy: 1.0000', '\\n', 'validation loss: 0.0103', '\\n', 'test accuracy:\n0.9960\\n', '\\n', 'Dataset: ai_bs_16_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0195', '\\n', 'validation accuracy: 0.9933', '\\n',\n'validation loss: 0.0157', '\\n', 'test accuracy: 0.9960\\n', '\\n', 'Dataset:\nai_bs_32_user_bs_16', '\\n', 'train accuracy: 0.9958', '\\n', 'train loss:\n0.0140', '\\n', 'validation accuracy: 0.9967', '\\n', 'validation loss: 0.0083',\n'\\n', 'test accuracy: 0.9920\\n', '\\n', 'Dataset: ai_bs_32_user_bs_32', '\\n',\n'train accuracy: 0.9958', '\\n', 'train loss: 0.0142', '\\n', 'validation\naccuracy: 0.9967', '\\n', 'validation loss: 0.0212', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_32_user_bs_64', '\\n', 'train accuracy: 0.9983',\n'\\n', 'train loss: 0.0186', '\\n', 'validation accuracy: 0.9967', '\\n',\n'validation loss: 0.0179', '\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset:\nai_bs_64_user_bs_16', '\\n', 'train accuracy: 0.9975', '\\n', 'train loss:\n0.0119', '\\n', 'validation accuracy: 0.9867', '\\n', 'validation loss: 0.0345',\n'\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_32', '\\n',\n'train accuracy: 0.9975', '\\n', 'train loss: 0.0138', '\\n', 'validation\naccuracy: 0.9900', '\\n', 'validation loss: 0.0182', '\\n', 'test accuracy:\n0.9940\\n', '\\n', 'Dataset: ai_bs_64_user_bs_64', '\\n', 'train accuracy: 0.9950',\n'\\n', 'train loss: 0.0188', '\\n', 'validation accuracy: 0.9900', '\\n',\n'validation loss: 0.0186', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 3, in <module>\\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,\nf1_score\\nModuleNotFoundError: No module named \\'sklearn\\'\\n', 'Execution time:\na moment seconds (time limit is an hour).']", "['Dataset: ablation=soft, config=ai_bs_16_usr_bs_16', '\\n', 'train accuracy:\n0.8550', '\\n', 'validation accuracy: 0.8267', '\\n', 'train loss: 0.3057', '\\n',\n'validation loss: 0.3768', '\\n', 'test accuracy: 0.8500\\n', '\\n', 'Dataset:\nablation=soft, config=ai_bs_16_usr_bs_32', '\\n', 'train accuracy: 0.8600', '\\n',\n'validation accuracy: 0.8233', '\\n', 'train loss: 0.3044', '\\n', 'validation\nloss: 0.3752', '\\n', 'test accuracy: 0.8540\\n', '\\n', 'Dataset: ablation=soft,\nconfig=ai_bs_16_usr_bs_64', '\\n', 'train accuracy: 0.8575', '\\n', 'validation\naccuracy: 0.8300', '\\n', 'train loss: 0.3047', '\\n', 'validation loss: 0.3702',\n'\\n', 'test accuracy: 0.8580\\n', '\\n', 'Dataset: ablation=soft,\nconfig=ai_bs_32_usr_bs_16', '\\n', 'train accuracy: 0.8633', '\\n', 'validation\naccuracy: 0.8233', '\\n', 'train loss: 0.3050', '\\n', 'validation loss: 0.3899',\n'\\n', 'test accuracy: 0.8600\\n', '\\n', 'Dataset: ablation=soft,\nconfig=ai_bs_32_usr_bs_32', '\\n', 'train accuracy: 0.8625', '\\n', 'validation\naccuracy: 0.8267', '\\n', 'train loss: 0.3034', '\\n', 'validation loss: 0.3796',\n'\\n', 'test accuracy: 0.8580\\n', '\\n', 'Dataset: ablation=soft,\nconfig=ai_bs_32_usr_bs_64', '\\n', 'train accuracy: 0.8608', '\\n', 'validation\naccuracy: 0.8333', '\\n', 'train loss: 0.3043', '\\n', 'validation loss: 0.3711',\n'\\n', 'test accuracy: 0.8600\\n', '\\n', 'Dataset: ablation=soft,\nconfig=ai_bs_64_usr_bs_16', '\\n', 'train accuracy: 0.8583', '\\n', 'validation\naccuracy: 0.8267', '\\n', 'train loss: 0.3047', '\\n', 'validation loss: 0.3772',\n'\\n', 'test accuracy: 0.8640\\n', '\\n', 'Dataset: ablation=soft,\nconfig=ai_bs_64_usr_bs_32', '\\n', 'train accuracy: 0.8600', '\\n', 'validation\naccuracy: 0.8267', '\\n', 'train loss: 0.3073', '\\n', 'validation loss: 0.3769',\n'\\n', 'test accuracy: 0.8600\\n', '\\n', 'Dataset: ablation=soft,\nconfig=ai_bs_64_usr_bs_64', '\\n', 'train accuracy: 0.8592', '\\n', 'validation\naccuracy: 0.8233', '\\n', 'train loss: 0.3048', '\\n', 'validation loss: 0.3741',\n'\\n', 'test accuracy: 0.8500\\n', '\\n', 'Dataset: ablation=hard,\nconfig=ai_bs_16_usr_bs_16', '\\n', 'train accuracy: 0.8583', '\\n', 'validation\naccuracy: 0.8333', '\\n', 'train loss: 0.3092', '\\n', 'validation loss: 0.3814',\n'\\n', 'test accuracy: 0.8560\\n', '\\n', 'Dataset: ablation=hard,\nconfig=ai_bs_16_usr_bs_32', '\\n', 'train accuracy: 0.8600', '\\n', 'validation\naccuracy: 0.8400', '\\n', 'train loss: 0.3022', '\\n', 'validation loss: 0.3703',\n'\\n', 'test accuracy: 0.8600\\n', '\\n', 'Dataset: ablation=hard,\nconfig=ai_bs_16_usr_bs_64', '\\n', 'train accuracy: 0.8617', '\\n', 'validation\naccuracy: 0.8233', '\\n', 'train loss: 0.3038', '\\n', 'validation loss: 0.3727',\n'\\n', 'test accuracy: 0.8580\\n', '\\n', 'Dataset: ablation=hard,\nconfig=ai_bs_32_usr_bs_16', '\\n', 'train accuracy: 0.8592', '\\n', 'validation\naccuracy: 0.8233', '\\n', 'train loss: 0.3055', '\\n', 'validation loss: 0.3697',\n'\\n', 'test accuracy: 0.8600\\n', '\\n', 'Dataset: ablation=hard,\nconfig=ai_bs_32_usr_bs_32', '\\n', 'train accuracy: 0.8525', '\\n', 'validation\naccuracy: 0.8300', '\\n', 'train loss: 0.3060', '\\n', 'validation loss: 0.3713',\n'\\n', 'test accuracy: 0.8520\\n', '\\n', 'Dataset: ablation=hard,\nconfig=ai_bs_32_usr_bs_64', '\\n', 'train accuracy: 0.8558', '\\n', 'validation\naccuracy: 0.8400', '\\n', 'train loss: 0.3044', '\\n', 'validation loss: 0.3720',\n'\\n', 'test accuracy: 0.8500\\n', '\\n', 'Dataset: ablation=hard,\nconfig=ai_bs_64_usr_bs_16', '\\n', 'train accuracy: 0.8542', '\\n', 'validation\naccuracy: 0.8267', '\\n', 'train loss: 0.3063', '\\n', 'validation loss: 0.3874',\n'\\n', 'test accuracy: 0.8540\\n', '\\n', 'Dataset: ablation=hard,\nconfig=ai_bs_64_usr_bs_32', '\\n', 'train accuracy: 0.8600', '\\n', 'validation\naccuracy: 0.8267', '\\n', 'train loss: 0.3035', '\\n', 'validation loss: 0.3626',\n'\\n', 'test accuracy: 0.8640\\n', '\\n', 'Dataset: ablation=hard,\nconfig=ai_bs_64_usr_bs_64', '\\n', 'train accuracy: 0.8608', '\\n', 'validation\naccuracy: 0.8467', '\\n', 'train loss: 0.3030', '\\n', 'validation loss: 0.3673',\n'\\n', 'test accuracy: 0.8600\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "Error parsing metrics. There was an error in the parsing code: string indices\nmust be integers, not 'str'", "['Dataset: T_0.5', '\\n', 'train accuracy: 0.9967', '\\n', 'validation accuracy:\n1.0000', '\\n', 'train loss: 0.0110', '\\n', 'validation loss: 0.0117', '\\n',\n'test accuracy: 0.9980\\n', '\\n', 'Dataset: T_1.0', '\\n', 'train accuracy:\n0.9958', '\\n', 'validation accuracy: 0.9900', '\\n', 'train loss: 0.0171', '\\n',\n'validation loss: 0.0182', '\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset:\nT_2.0', '\\n', 'train accuracy: 0.9908', '\\n', 'validation accuracy: 0.9867',\n'\\n', 'train loss: 0.0242', '\\n', 'validation loss: 0.0221', '\\n', 'test\naccuracy: 0.9920\\n', '\\n', 'Dataset: T_5.0', '\\n', 'train accuracy: 0.9900',\n'\\n', 'validation accuracy: 0.9900', '\\n', 'train loss: 0.0308', '\\n',\n'validation loss: 0.0244', '\\n', 'test accuracy: 0.9880\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: with_teacher_probs | Configuration: ai_bs_16_user_bs_16', '\\n', '\ntrain accuracy: 0.9933', '\\n', '  validation accuracy: 0.9933', '\\n', '  train\nloss: 0.0152', '\\n', '  validation loss: 0.0095', '\\n', '  test accuracy:\n0.9980\\n', '\\n', 'Dataset: with_teacher_probs | Configuration:\nai_bs_16_user_bs_32', '\\n', '  train accuracy: 0.9942', '\\n', '  validation\naccuracy: 1.0000', '\\n', '  train loss: 0.0162', '\\n', '  validation loss:\n0.0103', '\\n', '  test accuracy: 0.9960\\n', '\\n', 'Dataset: with_teacher_probs |\nConfiguration: ai_bs_16_user_bs_64', '\\n', '  train accuracy: 0.9983', '\\n', '\nvalidation accuracy: 0.9933', '\\n', '  train loss: 0.0195', '\\n', '  validation\nloss: 0.0157', '\\n', '  test accuracy: 0.9960\\n', '\\n', 'Dataset:\nwith_teacher_probs | Configuration: ai_bs_32_user_bs_16', '\\n', '  train\naccuracy: 0.9958', '\\n', '  validation accuracy: 1.0000', '\\n', '  train loss:\n0.0136', '\\n', '  validation loss: 0.0083', '\\n', '  test accuracy: 1.0000\\n',\n'\\n', 'Dataset: with_teacher_probs | Configuration: ai_bs_32_user_bs_32', '\\n',\n'  train accuracy: 0.9958', '\\n', '  validation accuracy: 1.0000', '\\n', '\ntrain loss: 0.0160', '\\n', '  validation loss: 0.0129', '\\n', '  test accuracy:\n0.9960\\n', '\\n', 'Dataset: with_teacher_probs | Configuration:\nai_bs_32_user_bs_64', '\\n', '  train accuracy: 0.9942', '\\n', '  validation\naccuracy: 1.0000', '\\n', '  train loss: 0.0215', '\\n', '  validation loss:\n0.0160', '\\n', '  test accuracy: 1.0000\\n', '\\n', 'Dataset: with_teacher_probs |\nConfiguration: ai_bs_64_user_bs_16', '\\n', '  train accuracy: 0.9925', '\\n', '\nvalidation accuracy: 0.9967', '\\n', '  train loss: 0.0140', '\\n', '  validation\nloss: 0.0108', '\\n', '  test accuracy: 0.9980\\n', '\\n', 'Dataset:\nwith_teacher_probs | Configuration: ai_bs_64_user_bs_32', '\\n', '  train\naccuracy: 0.9933', '\\n', '  validation accuracy: 0.9967', '\\n', '  train loss:\n0.0165', '\\n', '  validation loss: 0.0137', '\\n', '  test accuracy: 0.9980\\n',\n'\\n', 'Dataset: with_teacher_probs | Configuration: ai_bs_64_user_bs_64', '\\n',\n'  train accuracy: 0.9958', '\\n', '  validation accuracy: 0.9967', '\\n', '\ntrain loss: 0.0190', '\\n', '  validation loss: 0.0170', '\\n', '  test accuracy:\n0.9980\\n', '\\n', 'Dataset: raw_features_only | Configuration:\nai_bs_16_user_bs_16', '\\n', '  train accuracy: 0.9950', '\\n', '  validation\naccuracy: 0.9933', '\\n', '  train loss: 0.0176', '\\n', '  validation loss:\n0.0115', '\\n', '  test accuracy: 0.9900\\n', '\\n', 'Dataset: raw_features_only |\nConfiguration: ai_bs_16_user_bs_32', '\\n', '  train accuracy: 0.9950', '\\n', '\nvalidation accuracy: 0.9933', '\\n', '  train loss: 0.0234', '\\n', '  validation\nloss: 0.0187', '\\n', '  test accuracy: 0.9940\\n', '\\n', 'Dataset:\nraw_features_only | Configuration: ai_bs_16_user_bs_64', '\\n', '  train\naccuracy: 0.9950', '\\n', '  validation accuracy: 0.9933', '\\n', '  train loss:\n0.0339', '\\n', '  validation loss: 0.0304', '\\n', '  test accuracy: 0.9900\\n',\n'\\n', 'Dataset: raw_features_only | Configuration: ai_bs_32_user_bs_16', '\\n', '\ntrain accuracy: 0.9925', '\\n', '  validation accuracy: 0.9933', '\\n', '  train\nloss: 0.0190', '\\n', '  validation loss: 0.0200', '\\n', '  test accuracy:\n0.9960\\n', '\\n', 'Dataset: raw_features_only | Configuration:\nai_bs_32_user_bs_32', '\\n', '  train accuracy: 0.9900', '\\n', '  validation\naccuracy: 0.9933', '\\n', '  train loss: 0.0264', '\\n', '  validation loss:\n0.0232', '\\n', '  test accuracy: 0.9900\\n', '\\n', 'Dataset: raw_features_only |\nConfiguration: ai_bs_32_user_bs_64', '\\n', '  train accuracy: 0.9942', '\\n', '\nvalidation accuracy: 0.9967', '\\n', '  train loss: 0.0331', '\\n', '  validation\nloss: 0.0266', '\\n', '  test accuracy: 0.9920\\n', '\\n', 'Dataset:\nraw_features_only | Configuration: ai_bs_64_user_bs_16', '\\n', '  train\naccuracy: 0.9950', '\\n', '  validation accuracy: 0.9800', '\\n', '  train loss:\n0.0185', '\\n', '  validation loss: 0.0465', '\\n', '  test accuracy: 0.9820\\n',\n'\\n', 'Dataset: raw_features_only | Configuration: ai_bs_64_user_bs_32', '\\n', '\ntrain accuracy: 0.9975', '\\n', '  validation accuracy: 0.9967', '\\n', '  train\nloss: 0.0198', '\\n', '  validation loss: 0.0203', '\\n', '  test accuracy:\n0.9960\\n', '\\n', 'Dataset: raw_features_only | Configuration:\nai_bs_64_user_bs_64', '\\n', '  train accuracy: 0.9950', '\\n', '  validation\naccuracy: 0.9967', '\\n', '  train loss: 0.0288', '\\n', '  validation loss:\n0.0257', '\\n', '  test accuracy: 0.9960\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "", "['Dataset: CE_hard_labels', '\\n', 'Configuration: ai_bs_16_user_bs_16', '\\n',\n'Train accuracy: 0.9975', '\\n', 'Validation accuracy: 0.9933', '\\n', 'Alignment\nrate: -0.0021', '\\n', 'Train loss: 0.0169', '\\n', 'Validation loss: 0.0137',\n'\\n', 'Test accuracy: 0.9920\\n', '\\n', 'Configuration: ai_bs_16_user_bs_32',\n'\\n', 'Train accuracy: 0.9942', '\\n', 'Validation accuracy: 1.0000', '\\n',\n'Alignment rate: -0.0020', '\\n', 'Train loss: 0.0211', '\\n', 'Validation loss:\n0.0169', '\\n', 'Test accuracy: 0.9900\\n', '\\n', 'Configuration:\nai_bs_16_user_bs_64', '\\n', 'Train accuracy: 0.9892', '\\n', 'Validation\naccuracy: 1.0000', '\\n', 'Alignment rate: 0.0008', '\\n', 'Train loss: 0.0386',\n'\\n', 'Validation loss: 0.0344', '\\n', 'Test accuracy: 0.9940\\n', '\\n',\n'Configuration: ai_bs_32_user_bs_16', '\\n', 'Train accuracy: 0.9925', '\\n',\n'Validation accuracy: 0.9967', '\\n', 'Alignment rate: -0.0020', '\\n', 'Train\nloss: 0.0196', '\\n', 'Validation loss: 0.0182', '\\n', 'Test accuracy: 0.9940\\n',\n'\\n', 'Configuration: ai_bs_32_user_bs_32', '\\n', 'Train accuracy: 0.9967',\n'\\n', 'Validation accuracy: 0.9900', '\\n', 'Alignment rate: -0.0022', '\\n',\n'Train loss: 0.0221', '\\n', 'Validation loss: 0.0208', '\\n', 'Test accuracy:\n0.9920\\n', '\\n', 'Configuration: ai_bs_32_user_bs_64', '\\n', 'Train accuracy:\n0.9933', '\\n', 'Validation accuracy: 0.9967', '\\n', 'Alignment rate: -0.0003',\n'\\n', 'Train loss: 0.0339', '\\n', 'Validation loss: 0.0281', '\\n', 'Test\naccuracy: 0.9960\\n', '\\n', 'Configuration: ai_bs_64_user_bs_16', '\\n', 'Train\naccuracy: 0.9942', '\\n', 'Validation accuracy: 0.9867', '\\n', 'Alignment rate:\n-0.0020', '\\n', 'Train loss: 0.0185', '\\n', 'Validation loss: 0.0234', '\\n',\n'Test accuracy: 0.9880\\n', '\\n', 'Configuration: ai_bs_64_user_bs_32', '\\n',\n'Train accuracy: 0.9958', '\\n', 'Validation accuracy: 0.9933', '\\n', 'Alignment\nrate: -0.0021', '\\n', 'Train loss: 0.0198', '\\n', 'Validation loss: 0.0207',\n'\\n', 'Test accuracy: 0.9900\\n', '\\n', 'Configuration: ai_bs_64_user_bs_64',\n'\\n', 'Train accuracy: 0.9950', '\\n', 'Validation accuracy: 0.9967', '\\n',\n'Alignment rate: -0.0017', '\\n', 'Train loss: 0.0243', '\\n', 'Validation loss:\n0.0218', '\\n', 'Test accuracy: 0.9860\\n', '\\n', 'Dataset:\nsoft_label_distillation', '\\n', 'Configuration: ai_bs_16_user_bs_16', '\\n',\n'Train accuracy: 0.9883', '\\n', 'Validation accuracy: 0.9833', '\\n', 'Alignment\nrate: 0.0000', '\\n', 'Train loss: 0.0006', '\\n', 'Validation loss: 0.0006',\n'\\n', 'Test accuracy: 0.9820\\n', '\\n', 'Configuration: ai_bs_16_user_bs_32',\n'\\n', 'Train accuracy: 0.9875', '\\n', 'Validation accuracy: 0.9900', '\\n',\n'Alignment rate: 0.0001', '\\n', 'Train loss: 0.0005', '\\n', 'Validation loss:\n0.0005', '\\n', 'Test accuracy: 0.9880\\n', '\\n', 'Configuration:\nai_bs_16_user_bs_64', '\\n', 'Train accuracy: 0.9892', '\\n', 'Validation\naccuracy: 0.9933', '\\n', 'Alignment rate: 0.0011', '\\n', 'Train loss: 0.0007',\n'\\n', 'Validation loss: 0.0008', '\\n', 'Test accuracy: 0.9900\\n', '\\n',\n'Configuration: ai_bs_32_user_bs_16', '\\n', 'Train accuracy: 0.9842', '\\n',\n'Validation accuracy: 0.9967', '\\n', 'Alignment rate: 0.0000', '\\n', 'Train\nloss: 0.0006', '\\n', 'Validation loss: 0.0005', '\\n', 'Test accuracy: 0.9900\\n',\n'\\n', 'Configuration: ai_bs_32_user_bs_32', '\\n', 'Train accuracy: 0.9892',\n'\\n', 'Validation accuracy: 0.9933', '\\n', 'Alignment rate: 0.0004', '\\n',\n'Train loss: 0.0005', '\\n', 'Validation loss: 0.0005', '\\n', 'Test accuracy:\n0.9780\\n', '\\n', 'Configuration: ai_bs_32_user_bs_64', '\\n', 'Train accuracy:\n0.9900', '\\n', 'Validation accuracy: 0.9933', '\\n', 'Alignment rate: 0.0013',\n'\\n', 'Train loss: 0.0005', '\\n', 'Validation loss: 0.0004', '\\n', 'Test\naccuracy: 0.9860\\n', '\\n', 'Configuration: ai_bs_64_user_bs_16', '\\n', 'Train\naccuracy: 0.9958', '\\n', 'Validation accuracy: 0.9933', '\\n', 'Alignment rate:\n0.0000', '\\n', 'Train loss: 0.0003', '\\n', 'Validation loss: 0.0004', '\\n',\n'Test accuracy: 0.9920\\n', '\\n', 'Configuration: ai_bs_64_user_bs_32', '\\n',\n'Train accuracy: 0.9975', '\\n', 'Validation accuracy: 0.9933', '\\n', 'Alignment\nrate: 0.0005', '\\n', 'Train loss: 0.0005', '\\n', 'Validation loss: 0.0005',\n'\\n', 'Test accuracy: 0.9940\\n', '\\n', 'Configuration: ai_bs_64_user_bs_64',\n'\\n', 'Train accuracy: 0.9950', '\\n', 'Validation accuracy: 0.9933', '\\n',\n'Alignment rate: 0.0008', '\\n', 'Train loss: 0.0008', '\\n', 'Validation loss:\n0.0008', '\\n', 'Test accuracy: 0.9920\\n', '\\n', 'Dataset: bias_awareness', '\\n',\n'Configuration: ai_bs_16_user_bs_16', '\\n', 'Train accuracy: 0.9925', '\\n',\n'Validation accuracy: 0.9933', '\\n', 'Alignment rate: -0.0019', '\\n', 'Train\nloss: 0.0190', '\\n', 'Validation loss: 0.0152', '\\n', 'Test accuracy: 0.9860\\n',\n'\\n', 'Configuration: ai_bs_16_user_bs_32', '\\n', 'Train accuracy: 0.9933',\n'\\n', 'Validation accuracy: 0.9933', '\\n', 'Alignment rate: -0.0023', '\\n',\n'Train loss: 0.0233', '\\n', 'Validation loss: 0.0204', '\\n', 'Test accuracy:\n0.9860\\n', '\\n', 'Configuration: ai_bs_16_user_bs_64', '\\n', 'Train accuracy:\n0.9958', '\\n', 'Validation accuracy: 0.9933', '\\n', 'Alignment rate: -0.0021',\n'\\n', 'Train loss: 0.0261', '\\n', 'Validation loss: 0.0217', '\\n', 'Test\naccuracy: 0.9940\\n', '\\n', 'Configuration: ai_bs_32_user_bs_16', '\\n', 'Train\naccuracy: 0.9908', '\\n', 'Validation accuracy: 0.9933', '\\n', 'Alignment rate:\n-0.0020', '\\n', 'Train loss: 0.0221', '\\n', 'Validation loss: 0.0144', '\\n',\n'Test accuracy: 0.9920\\n', '\\n', 'Configuration: ai_bs_32_user_bs_32', '\\n',\n'Train accuracy: 0.9958', '\\n', 'Validation accuracy: 0.9933', '\\n', 'Alignment\nrate: -0.0021', '\\n', 'Train loss: 0.0198', '\\n', 'Validation loss: 0.0189',\n'\\n', 'Test accuracy: 0.9940\\n', '\\n', 'Configuration: ai_bs_32_user_bs_64',\n'\\n', 'Train accuracy: 0.9933', '\\n', 'Validation accuracy: 1.0000', '\\n',\n'Alignment rate: -0.0015', '\\n', 'Train loss: 0.0342', '\\n', 'Validation loss:\n0.0281', '\\n', 'Test accuracy: 1.0000\\n', '\\n', 'Configuration:\nai_bs_64_user_bs_16', '\\n', 'Train accuracy: 0.9925', '\\n', 'Validation\naccuracy: 0.9867', '\\n', 'Alignment rate: -0.0020', '\\n', 'Train loss: 0.0197',\n'\\n', 'Validation loss: 0.0274', '\\n', 'Test accuracy: 0.9880\\n', '\\n',\n'Configuration: ai_bs_64_user_bs_32', '\\n', 'Train accuracy: 0.9942', '\\n',\n'Validation accuracy: 0.9933', '\\n', 'Alignment rate: -0.0022', '\\n', 'Train\nloss: 0.0206', '\\n', 'Validation loss: 0.0187', '\\n', 'Test accuracy: 0.9920\\n',\n'\\n', 'Configuration: ai_bs_64_user_bs_64', '\\n', 'Train accuracy: 0.9950',\n'\\n', 'Validation accuracy: 0.9967', '\\n', 'Alignment rate: -0.0020', '\\n',\n'Train loss: 0.0245', '\\n', 'Validation loss: 0.0216', '\\n', 'Test accuracy:\n0.9880\\n', '\\n', 'Dataset: dual_channel', '\\n', 'Configuration:\nai_bs_16_user_bs_16', '\\n', 'Train accuracy: 0.9975', '\\n', 'Validation\naccuracy: 0.9933', '\\n', 'Alignment rate: 0.0000', '\\n', 'Train loss: 0.0002',\n'\\n', 'Validation loss: 0.0001', '\\n', 'Test accuracy: 0.9940\\n', '\\n',\n'Configuration: ai_bs_16_user_bs_32', '\\n', 'Train accuracy: 0.9950', '\\n',\n'Validation accuracy: 0.9933', '\\n', 'Alignment rate: 0.0000', '\\n', 'Train\nloss: 0.0001', '\\n', 'Validation loss: 0.0002', '\\n', 'Test accuracy: 0.9920\\n',\n'\\n', 'Configuration: ai_bs_16_user_bs_64', '\\n', 'Train accuracy: 0.9958',\n'\\n', 'Validation accuracy: 0.9933', '\\n', 'Alignment rate: 0.0012', '\\n',\n'Train loss: 0.0002', '\\n', 'Validation loss: 0.0002', '\\n', 'Test accuracy:\n0.9860\\n', '\\n', 'Configuration: ai_bs_32_user_bs_16', '\\n', 'Train accuracy:\n0.9983', '\\n', 'Validation accuracy: 0.9933', '\\n', 'Alignment rate: 0.0000',\n'\\n', 'Train loss: 0.0001', '\\n', 'Validation loss: 0.0001', '\\n', 'Test\naccuracy: 0.9960\\n', '\\n', 'Configuration: ai_bs_32_user_bs_32', '\\n', 'Train\naccuracy: 0.9942', '\\n', 'Validation accuracy: 1.0000', '\\n', 'Alignment rate:\n0.0003', '\\n', 'Train loss: 0.0001', '\\n', 'Validation loss: 0.0001', '\\n',\n'Test accuracy: 0.9960\\n', '\\n', 'Configuration: ai_bs_32_user_bs_64', '\\n',\n'Train accuracy: 0.9858', '\\n', 'Validation accuracy: 0.9967', '\\n', 'Alignment\nrate: 0.0011', '\\n', 'Train loss: 0.0005', '\\n', 'Validation loss: 0.0005',\n'\\n', 'Test accuracy: 0.9860\\n', '\\n', 'Configuration: ai_bs_64_user_bs_16',\n'\\n', 'Train accuracy: 0.9958', '\\n', 'Validation accuracy: 0.9933', '\\n',\n'Alignment rate: 0.0000', '\\n', 'Train loss: 0.0001', '\\n', 'Validation loss:\n0.0002', '\\n', 'Test accuracy: 0.9920\\n', '\\n', 'Configuration:\nai_bs_64_user_bs_32', '\\n', 'Train accuracy: 0.9975', '\\n', 'Validation\naccuracy: 0.9967', '\\n', 'Alignment rate: 0.0001', '\\n', 'Train loss: 0.0003',\n'\\n', 'Validation loss: 0.0003', '\\n', 'Test accuracy: 0.9900\\n', '\\n',\n'Configuration: ai_bs_64_user_bs_64', '\\n', 'Train accuracy: 0.9967', '\\n',\n'Validation accuracy: 0.9967', '\\n', 'Alignment rate: 0.0010', '\\n', 'Train\nloss: 0.0004', '\\n', 'Validation loss: 0.0005', '\\n', 'Test accuracy: 0.9900\\n',\n'\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: D2_noise0.1_scale1.0', '\\n', '  Configuration: ai_bs_16_user_bs_16',\n'\\n', '    Final train accuracy: 0.9858', '\\n', '    Final validation accuracy:\n0.9867', '\\n', '    Final alignment score: 0.8787', '\\n', '    Alignment rate\n(slope): -0.0018', '\\n', '    Test accuracy: 0.9940', '\\n', '  Configuration:\nai_bs_16_user_bs_32', '\\n', '    Final train accuracy: 0.9917', '\\n', '    Final\nvalidation accuracy: 0.9967', '\\n', '    Final alignment score: 0.8826', '\\n', '\nAlignment rate (slope): -0.0031', '\\n', '    Test accuracy: 0.9960', '\\n', '\nConfiguration: ai_bs_16_user_bs_64', '\\n', '    Final train accuracy: 0.9908',\n'\\n', '    Final validation accuracy: 0.9933', '\\n', '    Final alignment score:\n0.8914', '\\n', '    Alignment rate (slope): -0.0045', '\\n', '    Test accuracy:\n0.9960', '\\n', '  Configuration: ai_bs_32_user_bs_16', '\\n', '    Final train\naccuracy: 0.9967', '\\n', '    Final validation accuracy: 0.9900', '\\n', '\nFinal alignment score: 0.8836', '\\n', '    Alignment rate (slope): -0.0021',\n'\\n', '    Test accuracy: 0.9920', '\\n', '  Configuration: ai_bs_32_user_bs_32',\n'\\n', '    Final train accuracy: 0.9933', '\\n', '    Final validation accuracy:\n1.0000', '\\n', '    Final alignment score: 0.8870', '\\n', '    Alignment rate\n(slope): -0.0034', '\\n', '    Test accuracy: 0.9960', '\\n', '  Configuration:\nai_bs_32_user_bs_64', '\\n', '    Final train accuracy: 0.9950', '\\n', '    Final\nvalidation accuracy: 0.9933', '\\n', '    Final alignment score: 0.8918', '\\n', '\nAlignment rate (slope): -0.0038', '\\n', '    Test accuracy: 0.9960', '\\n', '\nConfiguration: ai_bs_64_user_bs_16', '\\n', '    Final train accuracy: 0.9925',\n'\\n', '    Final validation accuracy: 1.0000', '\\n', '    Final alignment score:\n0.8742', '\\n', '    Alignment rate (slope): -0.0023', '\\n', '    Test accuracy:\n0.9940', '\\n', '  Configuration: ai_bs_64_user_bs_32', '\\n', '    Final train\naccuracy: 0.9967', '\\n', '    Final validation accuracy: 0.9933', '\\n', '\nFinal alignment score: 0.8756', '\\n', '    Alignment rate (slope): -0.0036',\n'\\n', '    Test accuracy: 0.9960', '\\n', '  Configuration: ai_bs_64_user_bs_64',\n'\\n', '    Final train accuracy: 0.9942', '\\n', '    Final validation accuracy:\n0.9967', '\\n', '    Final alignment score: 0.8843', '\\n', '    Alignment rate\n(slope): -0.0047', '\\n', '    Test accuracy: 0.9900', '\\n', '\\n', 'Dataset:\nD5_noise0.5_scale2.0', '\\n', '  Configuration: ai_bs_16_user_bs_16', '\\n', '\nFinal train accuracy: 0.9992', '\\n', '    Final validation accuracy: 0.9967',\n'\\n', '    Final alignment score: 0.9626', '\\n', '    Alignment rate (slope):\n-0.0010', '\\n', '    Test accuracy: 0.9900', '\\n', '  Configuration:\nai_bs_16_user_bs_32', '\\n', '    Final train accuracy: 0.9975', '\\n', '    Final\nvalidation accuracy: 0.9967', '\\n', '    Final alignment score: 0.9645', '\\n', '\nAlignment rate (slope): -0.0012', '\\n', '    Test accuracy: 0.9940', '\\n', '\nConfiguration: ai_bs_16_user_bs_64', '\\n', '    Final train accuracy: 0.9992',\n'\\n', '    Final validation accuracy: 1.0000', '\\n', '    Final alignment score:\n0.9668', '\\n', '    Alignment rate (slope): -0.0005', '\\n', '    Test accuracy:\n0.9860', '\\n', '  Configuration: ai_bs_32_user_bs_16', '\\n', '    Final train\naccuracy: 0.9942', '\\n', '    Final validation accuracy: 0.9967', '\\n', '\nFinal alignment score: 0.9650', '\\n', '    Alignment rate (slope): -0.0010',\n'\\n', '    Test accuracy: 0.9980', '\\n', '  Configuration: ai_bs_32_user_bs_32',\n'\\n', '    Final train accuracy: 0.9967', '\\n', '    Final validation accuracy:\n0.9900', '\\n', '    Final alignment score: 0.9659', '\\n', '    Alignment rate\n(slope): -0.0009', '\\n', '    Test accuracy: 1.0000', '\\n', '  Configuration:\nai_bs_32_user_bs_64', '\\n', '    Final train accuracy: 0.9975', '\\n', '    Final\nvalidation accuracy: 0.9867', '\\n', '    Final alignment score: 0.9687', '\\n', '\nAlignment rate (slope): 0.0004', '\\n', '    Test accuracy: 1.0000', '\\n', '\nConfiguration: ai_bs_64_user_bs_16', '\\n', '    Final train accuracy: 0.9992',\n'\\n', '    Final validation accuracy: 0.9967', '\\n', '    Final alignment score:\n0.9603', '\\n', '    Alignment rate (slope): -0.0012', '\\n', '    Test accuracy:\n0.9980', '\\n', '  Configuration: ai_bs_64_user_bs_32', '\\n', '    Final train\naccuracy: 0.9992', '\\n', '    Final validation accuracy: 0.9900', '\\n', '\nFinal alignment score: 0.9616', '\\n', '    Alignment rate (slope): -0.0011',\n'\\n', '    Test accuracy: 0.9960', '\\n', '  Configuration: ai_bs_64_user_bs_64',\n'\\n', '    Final train accuracy: 0.9983', '\\n', '    Final validation accuracy:\n0.9933', '\\n', '    Final alignment score: 0.9653', '\\n', '    Alignment rate\n(slope): 0.0003', '\\n', '    Test accuracy: 0.9940', '\\n', '\\n', 'Dataset:\nD10_noise1.0_scale0.5', '\\n', '  Configuration: ai_bs_16_user_bs_16', '\\n', '\nFinal train accuracy: 0.9792', '\\n', '    Final validation accuracy: 0.9667',\n'\\n', '    Final alignment score: 0.8611', '\\n', '    Alignment rate (slope):\n-0.0030', '\\n', '    Test accuracy: 0.9660', '\\n', '  Configuration:\nai_bs_16_user_bs_32', '\\n', '    Final train accuracy: 0.9783', '\\n', '    Final\nvalidation accuracy: 0.9633', '\\n', '    Final alignment score: 0.8666', '\\n', '\nAlignment rate (slope): -0.0034', '\\n', '    Test accuracy: 0.9640', '\\n', '\nConfiguration: ai_bs_16_user_bs_64', '\\n', '    Final train accuracy: 0.9775',\n'\\n', '    Final validation accuracy: 0.9600', '\\n', '    Final alignment score:\n0.8754', '\\n', '    Alignment rate (slope): -0.0042', '\\n', '    Test accuracy:\n0.9600', '\\n', '  Configuration: ai_bs_32_user_bs_16', '\\n', '    Final train\naccuracy: 0.9817', '\\n', '    Final validation accuracy: 0.9667', '\\n', '\nFinal alignment score: 0.8691', '\\n', '    Alignment rate (slope): -0.0041',\n'\\n', '    Test accuracy: 0.9900', '\\n', '  Configuration: ai_bs_32_user_bs_32',\n'\\n', '    Final train accuracy: 0.9808', '\\n', '    Final validation accuracy:\n0.9600', '\\n', '    Final alignment score: 0.8652', '\\n', '    Alignment rate\n(slope): -0.0040', '\\n', '    Test accuracy: 0.9820', '\\n', '  Configuration:\nai_bs_32_user_bs_64', '\\n', '    Final train accuracy: 0.9850', '\\n', '    Final\nvalidation accuracy: 0.9400', '\\n', '    Final alignment score: 0.8792', '\\n', '\nAlignment rate (slope): -0.0039', '\\n', '    Test accuracy: 0.9720', '\\n', '\nConfiguration: ai_bs_64_user_bs_16', '\\n', '    Final train accuracy: 0.9825',\n'\\n', '    Final validation accuracy: 0.9767', '\\n', '    Final alignment score:\n0.8586', '\\n', '    Alignment rate (slope): -0.0033', '\\n', '    Test accuracy:\n0.9820', '\\n', '  Configuration: ai_bs_64_user_bs_32', '\\n', '    Final train\naccuracy: 0.9800', '\\n', '    Final validation accuracy: 0.9700', '\\n', '\nFinal alignment score: 0.8640', '\\n', '    Alignment rate (slope): -0.0036',\n'\\n', '    Test accuracy: 0.9740', '\\n', '  Configuration: ai_bs_64_user_bs_64',\n'\\n', '    Final train accuracy: 0.9767', '\\n', '    Final validation accuracy:\n0.9867', '\\n', '    Final alignment score: 0.8747', '\\n', '    Alignment rate\n(slope): -0.0035', '\\n', '    Test accuracy: 0.9740', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: threshold_0.6', '\\n', 'Training accuracy: 1.0000', '\\n', 'Validation\naccuracy: 0.9967', '\\n', 'Training loss: 0.0008', '\\n', 'Validation loss:\n0.0206', '\\n', 'Test accuracy: 0.9960\\n', '\\n', 'Dataset: threshold_0.8', '\\n',\n'Training accuracy: 1.0000', '\\n', 'Validation accuracy: 0.9967', '\\n',\n'Training loss: 0.0002', '\\n', 'Validation loss: 0.0349', '\\n', 'Test accuracy:\n0.9980\\n', '\\n', 'Dataset: threshold_0.9', '\\n', 'Training accuracy: 1.0000',\n'\\n', 'Validation accuracy: 0.9700', '\\n', 'Training loss: 0.0002', '\\n',\n'Validation loss: 0.0614', '\\n', 'Test accuracy: 0.9760\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: 50_50', '\\n', '  AI batch size: 16, User batch size: 16', '\\n', '\nTrain accuracy: 0.9942', '\\n', '    Validation accuracy: 0.9933', '\\n', '\nTrain loss: 0.0147', '\\n', '    Validation loss: 0.0123', '\\n', '    Test\naccuracy: 0.9920', '\\n', '    Test F1 score: 0.9918', '\\n', '  AI batch size:\n16, User batch size: 32', '\\n', '    Train accuracy: 0.9958', '\\n', '\nValidation accuracy: 0.9933', '\\n', '    Train loss: 0.0147', '\\n', '\nValidation loss: 0.0137', '\\n', '    Test accuracy: 0.9940', '\\n', '    Test F1\nscore: 0.9938', '\\n', '  AI batch size: 16, User batch size: 64', '\\n', '\nTrain accuracy: 0.9958', '\\n', '    Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0193', '\\n', '    Validation loss: 0.0165', '\\n', '    Test\naccuracy: 0.9960', '\\n', '    Test F1 score: 0.9959', '\\n', '  AI batch size:\n32, User batch size: 16', '\\n', '    Train accuracy: 0.9925', '\\n', '\nValidation accuracy: 0.9967', '\\n', '    Train loss: 0.0199', '\\n', '\nValidation loss: 0.0128', '\\n', '    Test accuracy: 0.9920', '\\n', '    Test F1\nscore: 0.9913', '\\n', '  AI batch size: 32, User batch size: 32', '\\n', '\nTrain accuracy: 0.9908', '\\n', '    Validation accuracy: 0.9967', '\\n', '\nTrain loss: 0.0245', '\\n', '    Validation loss: 0.0127', '\\n', '    Test\naccuracy: 0.9980', '\\n', '    Test F1 score: 0.9978', '\\n', '  AI batch size:\n32, User batch size: 64', '\\n', '    Train accuracy: 0.9917', '\\n', '\nValidation accuracy: 1.0000', '\\n', '    Train loss: 0.0262', '\\n', '\nValidation loss: 0.0161', '\\n', '    Test accuracy: 0.9980', '\\n', '    Test F1\nscore: 0.9978', '\\n', '  AI batch size: 64, User batch size: 16', '\\n', '\nTrain accuracy: 0.9975', '\\n', '    Validation accuracy: 0.9967', '\\n', '\nTrain loss: 0.0103', '\\n', '    Validation loss: 0.0115', '\\n', '    Test\naccuracy: 0.9920', '\\n', '    Test F1 score: 0.9914', '\\n', '  AI batch size:\n64, User batch size: 32', '\\n', '    Train accuracy: 0.9967', '\\n', '\nValidation accuracy: 0.9967', '\\n', '    Train loss: 0.0127', '\\n', '\nValidation loss: 0.0093', '\\n', '    Test accuracy: 0.9960', '\\n', '    Test F1\nscore: 0.9957', '\\n', '  AI batch size: 64, User batch size: 64', '\\n', '\nTrain accuracy: 0.9983', '\\n', '    Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0176', '\\n', '    Validation loss: 0.0103', '\\n', '    Test\naccuracy: 0.9980', '\\n', '    Test F1 score: 0.9979', '\\n', 'Dataset: 70_30',\n'\\n', '  AI batch size: 16, User batch size: 16', '\\n', '    Train accuracy:\n0.9992', '\\n', '    Validation accuracy: 1.0000', '\\n', '    Train loss:\n0.0062', '\\n', '    Validation loss: 0.0031', '\\n', '    Test accuracy: 1.0000',\n'\\n', '    Test F1 score: 1.0000', '\\n', '  AI batch size: 16, User batch size:\n32', '\\n', '    Train accuracy: 0.9983', '\\n', '    Validation accuracy:\n1.0000', '\\n', '    Train loss: 0.0081', '\\n', '    Validation loss: 0.0037',\n'\\n', '    Test accuracy: 1.0000', '\\n', '    Test F1 score: 1.0000', '\\n', '\nAI batch size: 16, User batch size: 64', '\\n', '    Train accuracy: 0.9983',\n'\\n', '    Validation accuracy: 1.0000', '\\n', '    Train loss: 0.0129', '\\n', '\nValidation loss: 0.0068', '\\n', '    Test accuracy: 1.0000', '\\n', '    Test F1\nscore: 1.0000', '\\n', '  AI batch size: 32, User batch size: 16', '\\n', '\nTrain accuracy: 0.9983', '\\n', '    Validation accuracy: 0.9967', '\\n', '\nTrain loss: 0.0078', '\\n', '    Validation loss: 0.0068', '\\n', '    Test\naccuracy: 0.9940', '\\n', '    Test F1 score: 0.9860', '\\n', '  AI batch size:\n32, User batch size: 32', '\\n', '    Train accuracy: 0.9983', '\\n', '\nValidation accuracy: 1.0000', '\\n', '    Train loss: 0.0108', '\\n', '\nValidation loss: 0.0058', '\\n', '    Test accuracy: 0.9980', '\\n', '    Test F1\nscore: 0.9953', '\\n', '  AI batch size: 32, User batch size: 64', '\\n', '\nTrain accuracy: 0.9983', '\\n', '    Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0140', '\\n', '    Validation loss: 0.0084', '\\n', '    Test\naccuracy: 0.9980', '\\n', '    Test F1 score: 0.9953', '\\n', '  AI batch size:\n64, User batch size: 16', '\\n', '    Train accuracy: 0.9967', '\\n', '\nValidation accuracy: 1.0000', '\\n', '    Train loss: 0.0089', '\\n', '\nValidation loss: 0.0046', '\\n', '    Test accuracy: 1.0000', '\\n', '    Test F1\nscore: 1.0000', '\\n', '  AI batch size: 64, User batch size: 32', '\\n', '\nTrain accuracy: 0.9967', '\\n', '    Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0110', '\\n', '    Validation loss: 0.0068', '\\n', '    Test\naccuracy: 1.0000', '\\n', '    Test F1 score: 1.0000', '\\n', '  AI batch size:\n64, User batch size: 64', '\\n', '    Train accuracy: 0.9983', '\\n', '\nValidation accuracy: 1.0000', '\\n', '    Train loss: 0.0135', '\\n', '\nValidation loss: 0.0102', '\\n', '    Test accuracy: 0.9960', '\\n', '    Test F1\nscore: 0.9914', '\\n', 'Dataset: 90_10', '\\n', '  AI batch size: 16, User batch\nsize: 16', '\\n', '    Train accuracy: 0.9983', '\\n', '    Validation accuracy:\n0.9967', '\\n', '    Train loss: 0.0045', '\\n', '    Validation loss: 0.0066',\n'\\n', '    Test accuracy: 1.0000', '\\n', '    Test F1 score: 1.0000', '\\n', '\nAI batch size: 16, User batch size: 32', '\\n', '    Train accuracy: 0.9983',\n'\\n', '    Validation accuracy: 0.9967', '\\n', '    Train loss: 0.0056', '\\n', '\nValidation loss: 0.0073', '\\n', '    Test accuracy: 0.9980', '\\n', '    Test F1\nscore: 0.9796', '\\n', '  AI batch size: 16, User batch size: 64', '\\n', '\nTrain accuracy: 1.0000', '\\n', '    Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0082', '\\n', '    Validation loss: 0.0101', '\\n', '    Test\naccuracy: 1.0000', '\\n', '    Test F1 score: 1.0000', '\\n', '  AI batch size:\n32, User batch size: 16', '\\n', '    Train accuracy: 0.9992', '\\n', '\nValidation accuracy: 1.0000', '\\n', '    Train loss: 0.0036', '\\n', '\nValidation loss: 0.0008', '\\n', '    Test accuracy: 0.9980', '\\n', '    Test F1\nscore: 0.9836', '\\n', '  AI batch size: 32, User batch size: 32', '\\n', '\nTrain accuracy: 0.9983', '\\n', '    Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0056', '\\n', '    Validation loss: 0.0011', '\\n', '    Test\naccuracy: 0.9980', '\\n', '    Test F1 score: 0.9831', '\\n', '  AI batch size:\n32, User batch size: 64', '\\n', '    Train accuracy: 0.9983', '\\n', '\nValidation accuracy: 1.0000', '\\n', '    Train loss: 0.0230', '\\n', '\nValidation loss: 0.0175', '\\n', '    Test accuracy: 0.9980', '\\n', '    Test F1\nscore: 0.9836', '\\n', '  AI batch size: 64, User batch size: 16', '\\n', '\nTrain accuracy: 0.9958', '\\n', '    Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0108', '\\n', '    Validation loss: 0.0007', '\\n', '    Test\naccuracy: 0.9980', '\\n', '    Test F1 score: 0.9811', '\\n', '  AI batch size:\n64, User batch size: 32', '\\n', '    Train accuracy: 0.9983', '\\n', '\nValidation accuracy: 1.0000', '\\n', '    Train loss: 0.0072', '\\n', '\nValidation loss: 0.0028', '\\n', '    Test accuracy: 0.9980', '\\n', '    Test F1\nscore: 0.9811', '\\n', '  AI batch size: 64, User batch size: 64', '\\n', '\nTrain accuracy: 0.9975', '\\n', '    Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0088', '\\n', '    Validation loss: 0.0038', '\\n', '    Test\naccuracy: 0.9980', '\\n', '    Test F1 score: 0.9811', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "", "['Dataset: synthetic (activation: relu)', '\\n', 'AI model train cross-entropy\nloss: 0.3032', '\\n', 'AI model validation cross-entropy loss: 0.3743', '\\n', 'AI\nmodel train accuracy: 0.8600', '\\n', 'AI model validation accuracy: 0.8300',\n'\\n', 'User model train cross-entropy loss: 0.0131', '\\n', 'User model\nvalidation cross-entropy loss: 0.0184', '\\n', 'User model train accuracy:\n0.9975', '\\n', 'User model validation accuracy: 0.9967', '\\n', 'User model test\naccuracy: 0.9960\\n', '\\n', 'Dataset: synthetic (activation: tanh)', '\\n', 'AI\nmodel train cross-entropy loss: 0.3075', '\\n', 'AI model validation cross-\nentropy loss: 0.3720', '\\n', 'AI model train accuracy: 0.8583', '\\n', 'AI model\nvalidation accuracy: 0.8300', '\\n', 'User model train cross-entropy loss:\n0.0108', '\\n', 'User model validation cross-entropy loss: 0.0100', '\\n', 'User\nmodel train accuracy: 0.9983', '\\n', 'User model validation accuracy: 0.9967',\n'\\n', 'User model test accuracy: 0.9920\\n', '\\n', 'Dataset: synthetic\n(activation: leaky_relu)', '\\n', 'AI model train cross-entropy loss: 0.3096',\n'\\n', 'AI model validation cross-entropy loss: 0.3756', '\\n', 'AI model train\naccuracy: 0.8625', '\\n', 'AI model validation accuracy: 0.8200', '\\n', 'User\nmodel train cross-entropy loss: 0.0121', '\\n', 'User model validation cross-\nentropy loss: 0.0132', '\\n', 'User model train accuracy: 0.9983', '\\n', 'User\nmodel validation accuracy: 0.9967', '\\n', 'User model test accuracy: 0.9960\\n',\n'\\n', 'Dataset: synthetic (activation: linear)', '\\n', 'AI model train cross-\nentropy loss: 0.3075', '\\n', 'AI model validation cross-entropy loss: 0.3744',\n'\\n', 'AI model train accuracy: 0.8567', '\\n', 'AI model validation accuracy:\n0.8433', '\\n', 'User model train cross-entropy loss: 0.0129', '\\n', 'User model\nvalidation cross-entropy loss: 0.0138', '\\n', 'User model train accuracy:\n0.9967', '\\n', 'User model validation accuracy: 0.9900', '\\n', 'User model test\naccuracy: 0.9940\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "", "['Dataset: ensemble_1_ai_bs_16_usr_bs_16', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9980\\n', '\\n', 'Dataset:\nensemble_1_ai_bs_16_usr_bs_32', '\\n', 'Train accuracy: 0.9975', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9960\\n', '\\n', 'Dataset:\nensemble_1_ai_bs_16_usr_bs_64', '\\n', 'Train accuracy: 0.9992', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9960\\n', '\\n', 'Dataset:\nensemble_1_ai_bs_32_usr_bs_16', '\\n', 'Train accuracy: 0.9958', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9980\\n', '\\n', 'Dataset:\nensemble_1_ai_bs_32_usr_bs_32', '\\n', 'Train accuracy: 0.9958', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9940\\n', '\\n', 'Dataset:\nensemble_1_ai_bs_32_usr_bs_64', '\\n', 'Train accuracy: 0.9942', '\\n',\n'Validation accuracy: 0.9967', '\\n', 'Test accuracy: 0.9960\\n', '\\n', 'Dataset:\nensemble_1_ai_bs_64_usr_bs_16', '\\n', 'Train accuracy: 0.9975', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 1.0000\\n', '\\n', 'Dataset:\nensemble_1_ai_bs_64_usr_bs_32', '\\n', 'Train accuracy: 0.9967', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9980\\n', '\\n', 'Dataset:\nensemble_1_ai_bs_64_usr_bs_64', '\\n', 'Train accuracy: 0.9958', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 1.0000\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_16_usr_bs_16', '\\n', 'Train accuracy: 0.9992', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9880\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_16_usr_bs_32', '\\n', 'Train accuracy: 1.0000', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9940\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_16_usr_bs_64', '\\n', 'Train accuracy: 1.0000', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9900\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_32_usr_bs_16', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9960\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_32_usr_bs_32', '\\n', 'Train accuracy: 1.0000', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 1.0000\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_32_usr_bs_64', '\\n', 'Train accuracy: 0.9992', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9980\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_64_usr_bs_16', '\\n', 'Train accuracy: 0.9967', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9880\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_64_usr_bs_32', '\\n', 'Train accuracy: 0.9992', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9980\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_64_usr_bs_64', '\\n', 'Train accuracy: 0.9992', '\\n',\n'Validation accuracy: 0.9967', '\\n', 'Test accuracy: 0.9900\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_16_usr_bs_16', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9980\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_16_usr_bs_32', '\\n', 'Train accuracy: 0.9975', '\\n',\n'Validation accuracy: 0.9967', '\\n', 'Test accuracy: 0.9940\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_16_usr_bs_64', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 0.9967', '\\n', 'Test accuracy: 1.0000\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_32_usr_bs_16', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9980\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_32_usr_bs_32', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9960\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_32_usr_bs_64', '\\n', 'Train accuracy: 0.9992', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9960\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_64_usr_bs_16', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 0.9967', '\\n', 'Test accuracy: 0.9960\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_64_usr_bs_32', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 0.9967', '\\n', 'Test accuracy: 0.9960\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_64_usr_bs_64', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 0.9967', '\\n', 'Test accuracy: 0.9920\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: sigma_0.0, ai_bs_16_user_bs_16', '\\n', 'train accuracy: 0.9933',\n'\\n', 'validation accuracy: 0.9933', '\\n', 'train loss: 0.0152', '\\n',\n'validation loss: 0.0095', '\\n', 'test accuracy: 0.9980\\n', '\\n', 'Dataset:\nsigma_0.0, ai_bs_16_user_bs_32', '\\n', 'train accuracy: 0.9942', '\\n',\n'validation accuracy: 1.0000', '\\n', 'train loss: 0.0162', '\\n', 'validation\nloss: 0.0103', '\\n', 'test accuracy: 0.9960\\n', '\\n', 'Dataset: sigma_0.0,\nai_bs_16_user_bs_64', '\\n', 'train accuracy: 0.9983', '\\n', 'validation\naccuracy: 0.9933', '\\n', 'train loss: 0.0195', '\\n', 'validation loss: 0.0157',\n'\\n', 'test accuracy: 0.9960\\n', '\\n', 'Dataset: sigma_0.0,\nai_bs_32_user_bs_16', '\\n', 'train accuracy: 0.9983', '\\n', 'validation\naccuracy: 0.9967', '\\n', 'train loss: 0.0091', '\\n', 'validation loss: 0.0119',\n'\\n', 'test accuracy: 0.9960\\n', '\\n', 'Dataset: sigma_0.0,\nai_bs_32_user_bs_32', '\\n', 'train accuracy: 0.9983', '\\n', 'validation\naccuracy: 0.9933', '\\n', 'train loss: 0.0128', '\\n', 'validation loss: 0.0150',\n'\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset: sigma_0.0,\nai_bs_32_user_bs_64', '\\n', 'train accuracy: 0.9983', '\\n', 'validation\naccuracy: 1.0000', '\\n', 'train loss: 0.0162', '\\n', 'validation loss: 0.0164',\n'\\n', 'test accuracy: 0.9900\\n', '\\n', 'Dataset: sigma_0.0,\nai_bs_64_user_bs_16', '\\n', 'train accuracy: 0.9950', '\\n', 'validation\naccuracy: 0.9900', '\\n', 'train loss: 0.0124', '\\n', 'validation loss: 0.0135',\n'\\n', 'test accuracy: 1.0000\\n', '\\n', 'Dataset: sigma_0.0,\nai_bs_64_user_bs_32', '\\n', 'train accuracy: 0.9958', '\\n', 'validation\naccuracy: 0.9833', '\\n', 'train loss: 0.0141', '\\n', 'validation loss: 0.0209',\n'\\n', 'test accuracy: 0.9980\\n', '\\n', 'Dataset: sigma_0.0,\nai_bs_64_user_bs_64', '\\n', 'train accuracy: 0.9967', '\\n', 'validation\naccuracy: 0.9867', '\\n', 'train loss: 0.0161', '\\n', 'validation loss: 0.0169',\n'\\n', 'test accuracy: 1.0000\\n', '\\n', 'Dataset: sigma_0.1,\nai_bs_16_user_bs_16', '\\n', 'train accuracy: 0.9917', '\\n', 'validation\naccuracy: 1.0000', '\\n', 'train loss: 0.0189', '\\n', 'validation loss: 0.0074',\n'\\n', 'test accuracy: 0.9920\\n', '\\n', 'Dataset: sigma_0.1,\nai_bs_16_user_bs_32', '\\n', 'train accuracy: 0.9942', '\\n', 'validation\naccuracy: 1.0000', '\\n', 'train loss: 0.0186', '\\n', 'validation loss: 0.0108',\n'\\n', 'test accuracy: 0.9920\\n', '\\n', 'Dataset: sigma_0.1,\nai_bs_16_user_bs_64', '\\n', 'train accuracy: 0.9925', '\\n', 'validation\naccuracy: 1.0000', '\\n', 'train loss: 0.0250', '\\n', 'validation loss: 0.0171',\n'\\n', 'test accuracy: 0.9920\\n', '\\n', 'Dataset: sigma_0.1,\nai_bs_32_user_bs_16', '\\n', 'train accuracy: 0.9933', '\\n', 'validation\naccuracy: 0.9967', '\\n', 'train loss: 0.0159', '\\n', 'validation loss: 0.0162',\n'\\n', 'test accuracy: 0.9920\\n', '\\n', 'Dataset: sigma_0.1,\nai_bs_32_user_bs_32', '\\n', 'train accuracy: 0.9933', '\\n', 'validation\naccuracy: 0.9967', '\\n', 'train loss: 0.0233', '\\n', 'validation loss: 0.0223',\n'\\n', 'test accuracy: 0.9900\\n', '\\n', 'Dataset: sigma_0.1,\nai_bs_32_user_bs_64', '\\n', 'train accuracy: 0.9892', '\\n', 'validation\naccuracy: 0.9900', '\\n', 'train loss: 0.0241', '\\n', 'validation loss: 0.0262',\n'\\n', 'test accuracy: 0.9860\\n', '\\n', 'Dataset: sigma_0.1,\nai_bs_64_user_bs_16', '\\n', 'train accuracy: 0.9942', '\\n', 'validation\naccuracy: 0.9933', '\\n', 'train loss: 0.0141', '\\n', 'validation loss: 0.0098',\n'\\n', 'test accuracy: 0.9800\\n', '\\n', 'Dataset: sigma_0.1,\nai_bs_64_user_bs_32', '\\n', 'train accuracy: 0.9942', '\\n', 'validation\naccuracy: 0.9933', '\\n', 'train loss: 0.0158', '\\n', 'validation loss: 0.0102',\n'\\n', 'test accuracy: 0.9800\\n', '\\n', 'Dataset: sigma_0.1,\nai_bs_64_user_bs_64', '\\n', 'train accuracy: 0.9958', '\\n', 'validation\naccuracy: 0.9933', '\\n', 'train loss: 0.0179', '\\n', 'validation loss: 0.0156',\n'\\n', 'test accuracy: 0.9760\\n', '\\n', 'Dataset: sigma_0.2,\nai_bs_16_user_bs_16', '\\n', 'train accuracy: 0.9942', '\\n', 'validation\naccuracy: 0.9933', '\\n', 'train loss: 0.0167', '\\n', 'validation loss: 0.0144',\n'\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset: sigma_0.2,\nai_bs_16_user_bs_32', '\\n', 'train accuracy: 0.9975', '\\n', 'validation\naccuracy: 0.9933', '\\n', 'train loss: 0.0186', '\\n', 'validation loss: 0.0213',\n'\\n', 'test accuracy: 0.9940\\n', '\\n', 'Dataset: sigma_0.2,\nai_bs_16_user_bs_64', '\\n', 'train accuracy: 0.9900', '\\n', 'validation\naccuracy: 0.9867', '\\n', 'train loss: 0.0322', '\\n', 'validation loss: 0.0285',\n'\\n', 'test accuracy: 0.9900\\n', '\\n', 'Dataset: sigma_0.2,\nai_bs_32_user_bs_16', '\\n', 'train accuracy: 0.9933', '\\n', 'validation\naccuracy: 1.0000', '\\n', 'train loss: 0.0148', '\\n', 'validation loss: 0.0057',\n'\\n', 'test accuracy: 0.9760\\n', '\\n', 'Dataset: sigma_0.2,\nai_bs_32_user_bs_32', '\\n', 'train accuracy: 0.9950', '\\n', 'validation\naccuracy: 0.9967', '\\n', 'train loss: 0.0179', '\\n', 'validation loss: 0.0134',\n'\\n', 'test accuracy: 0.9860\\n', '\\n', 'Dataset: sigma_0.2,\nai_bs_32_user_bs_64', '\\n', 'train accuracy: 0.9925', '\\n', 'validation\naccuracy: 0.9900', '\\n', 'train loss: 0.0226', '\\n', 'validation loss: 0.0179',\n'\\n', 'test accuracy: 0.9800\\n', '\\n', 'Dataset: sigma_0.2,\nai_bs_64_user_bs_16', '\\n', 'train accuracy: 0.9933', '\\n', 'validation\naccuracy: 0.9967', '\\n', 'train loss: 0.0186', '\\n', 'validation loss: 0.0084',\n'\\n', 'test accuracy: 0.9920\\n', '\\n', 'Dataset: sigma_0.2,\nai_bs_64_user_bs_32', '\\n', 'train accuracy: 0.9975', '\\n', 'validation\naccuracy: 0.9933', '\\n', 'train loss: 0.0205', '\\n', 'validation loss: 0.0124',\n'\\n', 'test accuracy: 0.9960\\n', '\\n', 'Dataset: sigma_0.2,\nai_bs_64_user_bs_64', '\\n', 'train accuracy: 0.9858', '\\n', 'validation\naccuracy: 0.9933', '\\n', 'train loss: 0.0320', '\\n', 'validation loss: 0.0157',\n'\\n', 'test accuracy: 0.9880\\n', '\\n', 'Dataset: sigma_0.5,\nai_bs_16_user_bs_16', '\\n', 'train accuracy: 0.4500', '\\n', 'validation\naccuracy: 0.4833', '\\n', 'train loss: nan', '\\n', 'validation loss: nan', '\\n',\n'test accuracy: 0.4640\\n', '\\n', 'Dataset: sigma_0.5, ai_bs_16_user_bs_32',\n'\\n', 'train accuracy: 0.4500', '\\n', 'validation accuracy: 0.4833', '\\n',\n'train loss: nan', '\\n', 'validation loss: nan', '\\n', 'test accuracy:\n0.4640\\n', '\\n', 'Dataset: sigma_0.5, ai_bs_16_user_bs_64', '\\n', 'train\naccuracy: 0.4500', '\\n', 'validation accuracy: 0.4833', '\\n', 'train loss: nan',\n'\\n', 'validation loss: nan', '\\n', 'test accuracy: 0.4640\\n', '\\n', 'Dataset:\nsigma_0.5, ai_bs_32_user_bs_16', '\\n', 'train accuracy: 0.4767', '\\n',\n'validation accuracy: 0.4667', '\\n', 'train loss: nan', '\\n', 'validation loss:\nnan', '\\n', 'test accuracy: 0.4940\\n', '\\n', 'Dataset: sigma_0.5,\nai_bs_32_user_bs_32', '\\n', 'train accuracy: 0.4767', '\\n', 'validation\naccuracy: 0.4667', '\\n', 'train loss: nan', '\\n', 'validation loss: nan', '\\n',\n'test accuracy: 0.4940\\n', '\\n', 'Dataset: sigma_0.5, ai_bs_32_user_bs_64',\n'\\n', 'train accuracy: 0.4767', '\\n', 'validation accuracy: 0.4667', '\\n',\n'train loss: nan', '\\n', 'validation loss: nan', '\\n', 'test accuracy:\n0.4940\\n', '\\n', 'Dataset: sigma_0.5, ai_bs_64_user_bs_16', '\\n', 'train\naccuracy: 0.4583', '\\n', 'validation accuracy: 0.5033', '\\n', 'train loss: nan',\n'\\n', 'validation loss: nan', '\\n', 'test accuracy: 0.5100\\n', '\\n', 'Dataset:\nsigma_0.5, ai_bs_64_user_bs_32', '\\n', 'train accuracy: 0.4583', '\\n',\n'validation accuracy: 0.5033', '\\n', 'train loss: nan', '\\n', 'validation loss:\nnan', '\\n', 'test accuracy: 0.5100\\n', '\\n', 'Dataset: sigma_0.5,\nai_bs_64_user_bs_64', '\\n', 'train accuracy: 0.4583', '\\n', 'validation\naccuracy: 0.5033', '\\n', 'train loss: nan', '\\n', 'validation loss: nan', '\\n',\n'test accuracy: 0.5100\\n', '\\n', 'Execution time: a moment seconds (time limit\nis an hour).']", "Error parsing metrics. There was an error in the parsing code: string indices\nmust be integers, not 'str'", "", "Error parsing metrics. There was an error in the parsing code: string indices\nmust be integers, not 'str'", "Error parsing metrics. There was an error in the parsing code: string indices\nmust be integers, not 'str'", "['Dataset: ensemble_1_ai_bs_16_usr_bs_16', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9980\\n', '\\n', 'Dataset:\nensemble_1_ai_bs_16_usr_bs_32', '\\n', 'Train accuracy: 0.9975', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9960\\n', '\\n', 'Dataset:\nensemble_1_ai_bs_16_usr_bs_64', '\\n', 'Train accuracy: 0.9992', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9960\\n', '\\n', 'Dataset:\nensemble_1_ai_bs_32_usr_bs_16', '\\n', 'Train accuracy: 0.9958', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9980\\n', '\\n', 'Dataset:\nensemble_1_ai_bs_32_usr_bs_32', '\\n', 'Train accuracy: 0.9958', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9940\\n', '\\n', 'Dataset:\nensemble_1_ai_bs_32_usr_bs_64', '\\n', 'Train accuracy: 0.9942', '\\n',\n'Validation accuracy: 0.9967', '\\n', 'Test accuracy: 0.9960\\n', '\\n', 'Dataset:\nensemble_1_ai_bs_64_usr_bs_16', '\\n', 'Train accuracy: 0.9975', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 1.0000\\n', '\\n', 'Dataset:\nensemble_1_ai_bs_64_usr_bs_32', '\\n', 'Train accuracy: 0.9967', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9980\\n', '\\n', 'Dataset:\nensemble_1_ai_bs_64_usr_bs_64', '\\n', 'Train accuracy: 0.9958', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 1.0000\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_16_usr_bs_16', '\\n', 'Train accuracy: 0.9992', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9880\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_16_usr_bs_32', '\\n', 'Train accuracy: 1.0000', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9940\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_16_usr_bs_64', '\\n', 'Train accuracy: 1.0000', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9900\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_32_usr_bs_16', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9960\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_32_usr_bs_32', '\\n', 'Train accuracy: 1.0000', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 1.0000\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_32_usr_bs_64', '\\n', 'Train accuracy: 0.9992', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9980\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_64_usr_bs_16', '\\n', 'Train accuracy: 0.9967', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9880\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_64_usr_bs_32', '\\n', 'Train accuracy: 0.9992', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9980\\n', '\\n', 'Dataset:\nensemble_3_ai_bs_64_usr_bs_64', '\\n', 'Train accuracy: 0.9992', '\\n',\n'Validation accuracy: 0.9967', '\\n', 'Test accuracy: 0.9900\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_16_usr_bs_16', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9980\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_16_usr_bs_32', '\\n', 'Train accuracy: 0.9975', '\\n',\n'Validation accuracy: 0.9967', '\\n', 'Test accuracy: 0.9940\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_16_usr_bs_64', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 0.9967', '\\n', 'Test accuracy: 1.0000\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_32_usr_bs_16', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9980\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_32_usr_bs_32', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9960\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_32_usr_bs_64', '\\n', 'Train accuracy: 0.9992', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Test accuracy: 0.9960\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_64_usr_bs_16', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 0.9967', '\\n', 'Test accuracy: 0.9960\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_64_usr_bs_32', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 0.9967', '\\n', 'Test accuracy: 0.9960\\n', '\\n', 'Dataset:\nensemble_5_ai_bs_64_usr_bs_64', '\\n', 'Train accuracy: 0.9983', '\\n',\n'Validation accuracy: 0.9967', '\\n', 'Test accuracy: 0.9920\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", ""], "parse_exc_type": [null, "ModuleNotFoundError", null, "string indices must be integers, not 'str'", null, null, null, null, null, null, null, null, null, null, null, null, "string indices must be integers, not 'str'", null, "string indices must be integers, not 'str'", "string indices must be integers, not 'str'", null, null], "parse_exc_info": [null, {"args": ["No module named 'sklearn'"], "name": "sklearn", "msg": "No module named 'sklearn'"}, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 3, "<module>", "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"]], null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"]}

% Ribeiro et al. (2016) introduce LIME, a model-agnostic, local post-hoc explanation technique. Cite this in the Related Work section when summarizing static explanation methods (e.g., ‘Traditional XAI methods such as LIME (Ribeiro et al., 2016)…’).
@book{ribeiro2016whysi,
 author = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
 booktitle = {North American Chapter of the Association for Computational Linguistics},
 journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 title = {“Why Should I Trust You?”: Explaining the Predictions of Any Classifier},
 year = {2016}
}

% Lundberg & Lee (2017) introduce SHAP (SHapley Additive exPlanations), a unified framework for feature attribution with solid theoretical guarantees. Cite this in the Related Work section alongside LIME when summarizing static post hoc explainability methods (e.g., 'Methods such as LIME (Ribeiro et al., 2016) and SHAP (Lundberg & Lee, 2017) provide model-agnostic, local explanations…').
@article{lundberg2017aua,
 author = {Scott M. Lundberg and Su-In Lee},
 booktitle = {Neural Information Processing Systems},
 pages = {4765-4774},
 title = {A Unified Approach to Interpreting Model Predictions},
 year = {2017}
}

% Amershi et al. (2014) introduce foundational principles for interactive machine learning systems that involve users at every stage. Cite this when discussing human-in-the-loop and interactive machine teaching approaches in the Related Work section.
@article{amershi2014powertt,
 author = {Saleema Amershi and M. Cakmak and W. B. Knox and Todd Kulesza},
 booktitle = {The AI Magazine},
 journal = {AI Mag.},
 pages = {105-120},
 title = {Power to the People: The Role of Humans in Interactive Machine Learning},
 volume = {35},
 year = {2014}
}

% Kulesza et al. (2015) introduce Principles of Explanatory Debugging, a foundational interactive machine teaching approach where users correct model explanations to personalize and improve learning. Cite this in Related Work when discussing human-in-the-loop customization and interactive explanation methods.
@book{kulesza2015principlesoe,
 author = {Todd Kulesza and M. Burnett and Weng-Keen Wong and S. Stumpf},
 booktitle = {International Conference on Intelligent User Interfaces},
 journal = {Proceedings of the 20th International Conference on Intelligent User Interfaces},
 title = {Principles of Explanatory Debugging to Personalize Interactive Machine Learning},
 year = {2015}
}

% Poursabzi‐Sangdeh et al. (2018) conduct large-scale experiments varying feature count and model transparency to evaluate how these factors affect human interpretability, trust calibration, and error detection. Cite this in Related Work when contrasting static personalized explanation approaches against our co-adaptive, bias-aware dual-channel interface.
@book{poursabzi-sangdeh2018manipulatingam,
 author = {Forough Poursabzi-Sangdeh and D. Goldstein and Jake M. Hofman and Jennifer Wortman Vaughan and Hanna M. Wallach},
 booktitle = {International Conference on Human Factors in Computing Systems},
 journal = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
 title = {Manipulating and Measuring Model Interpretability},
 year = {2018}
}

% Kingma & Ba (2014) introduce Adam, an efficient gradient-based optimizer used to train both the AI classification model and the user bias estimation model. Cite this in the Methods/Experiments section when describing optimization of our neural networks.
@article{kingma2014adamam,
 author = {Diederik P. Kingma and Jimmy Ba},
 booktitle = {International Conference on Learning Representations},
 journal = {CoRR},
 title = {Adam: A Method for Stochastic Optimization},
 volume = {abs/1412.6980},
 year = {2014}
}

% Tversky & Kahneman (1974) 'Judgment under Uncertainty: Heuristics and Biases' is the foundational work on cognitive biases; cite in the introduction/motivation when discussing why modeling user biases is critical for our dual-channel explanation interface.
@article{tversky1974judgmentuu,
 author = {A. Tversky and Daniel Kahneman},
 booktitle = {Science},
 journal = {Science},
 pages = {1124 - 1131},
 title = {Judgment under Uncertainty: Heuristics and Biases},
 volume = {185},
 year = {1974}
}

% Kullback, S. & Leibler, R. A. (1951). On information and sufficiency. Introduces the Kullback–Leibler divergence. Cite in the Methods section when describing our convergence speed metric via KL divergence.
@inproceedings{williams2018bayesianmw,
 author = {Donald R. Williams and P. Rast and P. Bürkner},
 title = {Bayesian Meta-Analysis with Weakly Informative Prior Distributions},
 year = {2018}
}

% Doshi-Velez & Kim (2017) present a formal definition of interpretability, propose a taxonomy for rigorous evaluation, and outline open questions in measuring explanations. Cite this in the Introduction/Related Work when highlighting the lack of consensus on interpretability metrics and motivating the need for dynamic, human-grounded co-adaptive evaluation methods.
@article{doshi-velez2017towardsar,
 author = {F. Doshi-Velez and Been Kim},
 journal = {arXiv: Machine Learning},
 title = {Towards A Rigorous Science of Interpretable Machine Learning},
 year = {2017}
}

% Tim Miller (2017), “Explanation in Artificial Intelligence: Insights from the Social Sciences”. Cite in the Introduction/Related Work to motivate the need for human-centered, contrastive explanations and to ground the dual-channel, bias-aware design in social science principles.
@article{miller2017explanationia,
 author = {Tim Miller},
 booktitle = {Artificial Intelligence},
 journal = {ArXiv},
 title = {Explanation in Artificial Intelligence: Insights from the Social Sciences},
 volume = {abs/1706.07269},
 year = {2017}
}

% Sweller J. (1988), 'Cognitive Load During Problem Solving: Effects on Learning'. Foundational Cognitive Load Theory work; cite in the Risk Factors section when discussing how dual-channel explanations may overwhelm users by increasing cognitive processing demands.
@article{sweller1988cognitiveld,
 author = {J. Sweller},
 booktitle = {Cognitive Sciences},
 journal = {Cogn. Sci.},
 pages = {257-285},
 title = {Cognitive Load During Problem Solving: Effects on Learning},
 volume = {12},
 year = {1988}
}

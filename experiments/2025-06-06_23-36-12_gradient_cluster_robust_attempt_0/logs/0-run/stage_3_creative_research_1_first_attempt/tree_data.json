{"edges": [[0, 6], [0, 5], [0, 14], [0, 13], [0, 16], [0, 1], [0, 11], [0, 3], [0, 15], [0, 2], [0, 9], [2, 4], [4, 10], [5, 8], [6, 7], [7, 12]], "layout": [[0.5, 0.0], [0.0, 0.33333333333333337], [0.1, 0.33333333333333337], [0.2, 0.33333333333333337], [0.1, 0.6666666666666667], [0.3, 0.33333333333333337], [0.4, 0.33333333333333337], [0.4, 0.6666666666666667], [0.3, 0.6666666666666667], [0.5, 0.33333333333333337], [0.1, 1.0], [0.6, 0.33333333333333337], [0.4, 1.0], [0.7, 0.33333333333333337], [0.8, 0.33333333333333337], [0.9, 0.33333333333333337], [1.0, 0.33333333333333337]], "plan": ["Hyperparam tuning name: learning_rate. We extend the baseline by looping over a\nlog\u2010scale grid of learning rates, reinitializing the model and optimizer for\neach value. During each run we apply the same warmup + gradient\u2010clustering\nreweighting schedule, recording per\u2010epoch train/val worst\u2010group accuracies and\nlosses. After training, we evaluate on the test split to obtain predictions and\nground truth. All these arrays are collated into an `experiment_data` dict under\nthe \u201clearning_rate\u201d tuning type and saved to `experiment_data.npy`.", "We enhance UGC by dynamically selecting the number of clusters via silhouette\nscores on PCA\u2010reduced gradient embeddings from a small subset of samples and by\ntracking Normalized Mutual Information (NMI) between these pseudo\u2010group\nassignments and true labels at each epoch. We apply this pipeline to three\nHugging Face vision tasks (MNIST, FashionMNIST, CIFAR10) with a lightweight CNN:\nwe warm up for a few epochs to collect per\u2010sample gradients, reduce them to 10D,\nand choose k\u2208{2,3} that maximizes silhouette. We then compute sample weights\ninversely proportional to cluster sizes and continue robust reweighted training,\nlogging training/validation losses, validation accuracy, and NMI at each epoch.\nWe print validation loss and NMI for monitoring and finally save a structured\n`experiment_data` dict\u2014including all metrics, per\u2010epoch losses, predictions, and\nground truths\u2014to `./working/experiment_data.npy` for downstream analysis.", "We dynamically select k via silhouette score at each epoch, cluster per-sample\ngradients, and update sample weights for robust re-training. We inject a binary\nspurious bit into MNIST, Fashion-MNIST, and KMNIST (restricted to digits 0/1)\nthat is highly correlated with the label in training but de-correlated in\nval/test. At the end of every epoch we extract per-sample grads on the final\nlinear layer, run k-means with k\u2208{2,3,4,5}, pick the k with maximum silhouette,\nand compute NMI against the true spurious groups. Sample weights are set\ninversely proportional to cluster sizes, driving group-robust optimization in\nsubsequent epochs. We track and print validation loss, worst\u2010group accuracy,\nNMI, and silhouette each epoch, and save all metrics, losses, predictions, and\nground\u2010truth labels into `working/experiment_data.npy` for later analysis. This\nworkflow scales across multiple HuggingFace datasets and provides temporal\ninsights into pseudo\u2010group discovery.", "We load three HuggingFace image datasets (MNIST, FashionMNIST, and CIFAR-10) and\nrestrict each to a binary classification task by selecting two classes. To\nintroduce spurious correlation, we overlay a colored patch that matches the\nclass label with high probability (95%). Each dataset is randomly split into\ntraining, validation, and test subsets (50/25/25) to mimic realistic evaluation,\nand all images are normalized to the [0,1] range. We define a lightweight CNN\nand train it with a warmup epoch followed by a clustering step: we extract per-\nsample gradients on the classification head, reduce their dimension via PCA, and\nuse silhouette-guided k-means to discover the best number of pseudo-groups. We\ncompute inverse-frequency weights from these clusters and incorporate them into\nsubsequent cross-entropy training to emphasize underrepresented clusters. At\neach epoch, we record validation loss, worst-group accuracy, and the normalized\nmutual information (NMI) between cluster assignments and true spurious labels.\nFinally, we evaluate on a held-out test split, save predictions and all per-\nepoch metrics in a numpy archive, and ensure full GPU support and proper\nnormalization. The overall script runs immediately, moves all tensors and models\nto GPU if available, and tracks metrics in a structured dictionary which is\nsaved to the working directory for later analysis.", "We replace the sklearn calls with our own NumPy implementations of silhouette\nand NMI to eliminate the missing\u2010package error. Our silhouette_score_np computes\npairwise distances and silhouette for each valid sample in pure NumPy, while\nnormalized_mutual_info_score_np builds a contingency table to compute mutual\ninformation and entropies. We then swap out the sklearn calls in the clustering\nloop for these functions, keeping all existing logic intact and preserving GPU\nmoves and saving. This ensures the code runs completely self\u2010contained without\nexternal metric dependencies.", "We load three HuggingFace image datasets (MNIST, Fashion-MNIST, CIFAR-10 limited\nto classes 0/1) as binary tasks, subsample 2K train, 500-500 val/test splits,\nand inject a small colored patch as a spurious feature highly correlated with\nthe label in training. We define a lightweight CNN, train it for one warmup\nepoch, then extract per-sample gradients of the final layer over the training\nset, apply a custom K-means to form two pseudo-groups, and compute sample\nweights inversely proportional to cluster sizes. We then continue training for\ntwo more epochs with a weighted cross-entropy loss to mitigate reliance on the\nspurious patch. At each epoch we evaluate on validation data, print the average\nvalidation loss, and record worst-group accuracy and the normalized mutual\ninformation (NMI) between the clusters and true spurious groups. We save\ntrain/val losses, worst-group metrics, and NMI per epoch in an `experiment_data`\ndict. After training each dataset we evaluate on the held-out test set to\ncollect predictions and ground truth labels. Finally we save all metrics and\ndata as a NumPy file in `./working`. The pipeline moves all tensors/models to\nGPU if available, carefully normalizes inputs to [-1,1], and adheres to the\nrequired code structure.", "We propose to extend UGC by concatenating gradient signatures from both a hidden\nconvolutional layer and the final classifier to form richer per-sample\nembeddings, then adaptively determining the optimal number of clusters (k\u2208{2,3})\nby maximizing an inter\u2010to\u2010intra cluster dispersion ratio.  After a brief\nwarm\u2010up, we sample a subset of training points, collect their gradient features,\nrun k\u2010means for each k and pick the best clustering, then compute per\u2010cluster\nweights via inverse frequency for Group DRO reweighting.  To demonstrate\ngenerality, we inject a synthetic color\u2010background spurious feature into three\nHuggingFace vision datasets (MNIST, Fashion\u2010MNIST, CIFAR\u201010), and treat the\ncolor bit as the true group label.  We track and print validation loss,\nworst\u2010group accuracy, and normalized mutual information (NMI) between clusters\nand true color groups at each epoch.  Finally, we save all per\u2010epoch metrics,\nlosses, and NMI scores into the working directory for further analysis.  This\ncreative experiment will reveal how multi\u2010layer gradients and adaptive cluster\nselection jointly improve pseudo\u2010group discovery and robust performance across\ndiverse data regimes.", "I have updated the dataset wrapper to correctly pass the `dtype` keyword to\n`torch.tensor` for both `label` and `group`. This fixes the `TypeError` when\ncreating tensors. No other changes were required\u2014everything else remains the\nsame and the script can now execute through the data loader without error.", "We can fix the KeyError by detecting whether the dataset uses the \u201cimage\u201d or\n\u201cimg\u201d column and indexing accordingly. In make_split, we inspect ds.column_names\nto pick the correct image key. We also update batch tensor movement to use\nisinstance checks and ensure all model, optimizer, and data are moved to the\ncorrect device. This resolves the crash on Fashion\u2010MNIST while preserving\nfunctionality on MNIST and CIFAR\u201010.", "We enhance UGC by injecting synthetic spurious correlations into three\nHuggingFace image tasks (MNIST even\u2010vs\u2010odd, FashionMNIST classes 0\u20131, CIFAR-10\nplane vs car) to demonstrate generality. After a one\u2010epoch warmup of a\nlightweight CNN, we dynamically cluster per\u2010sample gradients at the classifier\nlayer on all 5 k training samples using k-means for k\u2208[2,5], selecting the best\nk by silhouette score each epoch. We compute NMI between pseudo\u2010groups and true\nspurious labels at every epoch to track clustering quality. Cluster sizes drive\ninverse\u2010frequency sample weights for group-robust reweighting in subsequent\ntraining steps. We train for five epochs per dataset, printing validation loss\nat each epoch, and tracking worst\u2010group accuracy and NMI. All per-epoch losses,\naccuracies, and NMI values are recorded in an experiment_data dict and saved as\na NumPy file at the end. This adaptive clustering strategy shows improved\nrobustness without any true group supervision.", "We replace the non\u2010existent \u201ckmnist\u201d dataset with \u201ccifar10\u201d so that our loop\nuses three valid HuggingFace datasets. This change ensures the dataset loading\nstep no longer errors out, while preserving the binary filtering, spurious\nfeature injection, and gradient\u2010clustering pipeline unchanged. With (\u201cmnist\u201d,\n\u201cfashion_mnist\u201d, \u201ccifar10\u201d) we still get three diverse vision datasets and can\nproceed to evaluate our Unsupervised Gradient Clustering approach.", "We adaptively choose the number of gradient\u2010clusters by maximizing the\nsilhouette score on PCA\u2010reduced gradient embeddings, eliminating manual k\nselection. After a brief warmup epoch, we extract per\u2010sample gradients of the\nlast linear layer, perform PCA, and scan k\u2208[2,5] to find the best cluster count.\nWe then reweight each sample inversely by its cluster frequency and continue\nrobust training to improve worst\u2010group accuracy. This pipeline is applied to\nthree HuggingFace datasets\u2014Colored MNIST (with injected color spur), Waterbirds,\nand CelebA hair classification\u2014using a small CNN for MNIST and fine\u2010tuned\nResNet18 for the other two. We track validation loss, worst\u2010group accuracy, and\nnormalized mutual information (NMI) between pseudo\u2010groups and true spurious\ngroups at every epoch. All tensors are moved to GPU when available, and we save\nthe final per\u2010epoch metrics, predictions, and ground truths to\n``working/experiment_data.npy``.", "I will convert the randomly\u2010sampled subset indices from a NumPy array of\n`numpy.int64` into a Python list of ints before building the `Subset`, ensuring\nthat HuggingFace\u2019s arrow dataset sees pure Python ints for indexing and avoids\nthe \u201cWrong key type\u201d error. This small change preserves all subsequent logic for\ngradient extraction, clustering, and group\u2010robust weighting unchanged. The rest\nof the training and evaluation loops remain intact, with GPU handling and metric\nlogging as before.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# k-means on CPU\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper: returns (sum_loss, worst-group accuracy)\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameter sweep over learning rates\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# Containers for results\nmetrics_train_all = []\nmetrics_val_all = []\nlosses_train_all = []\nlosses_val_all = []\npredictions_list = []\nground_truth = None\n\nfor lr in lrs:\n    print(f\"\\n=== Training with learning rate = {lr} ===\")\n    # re-init model, loss, optimizer\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    # per-epoch logs\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # cluster-based reweighting after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out_i = model(batch[\"features\"])\n                loss_i = criterion(out_i, batch[\"label\"]).mean()\n                loss_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # evaluate after epoch\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(\n            f\"lr={lr} epoch={epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n        )\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # final test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n\n    preds_npy = np.array(preds)\n    truths_npy = np.array(truths)\n    predictions_list.append(preds_npy)\n    if ground_truth is None:\n        ground_truth = truths_npy\n\n    metrics_train_all.append(m_tr)\n    metrics_val_all.append(m_val)\n    losses_train_all.append(l_tr)\n    losses_val_all.append(l_val)\n\n# Assemble and save experiment data\nexperiment_data = {\n    \"learning_rate\": {\n        \"synthetic\": {\n            \"lrs\": np.array(lrs),\n            \"metrics\": {\n                \"train\": np.array(metrics_train_all),\n                \"val\": np.array(metrics_val_all),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_all),\n                \"val\": np.array(losses_val_all),\n            },\n            \"predictions\": np.stack(predictions_list),\n            \"ground_truth\": ground_truth,\n        }\n    }\n}\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\nfrom datasets import load_dataset\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import normalized_mutual_info_score, silhouette_score\nfrom torchvision import transforms\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, in_channels, n_classes):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels, 32, 3, 1, 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, 1, 1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n        )\n        self.fc = nn.Linear(64, n_classes)\n\n    def forward(self, x):\n        feat = self.features(x)\n        return self.fc(feat)\n\n\nexperiment_data = {}\ndatasets_list = [\"mnist\", \"fashion_mnist\", \"cifar10\"]\n\nfor ds_name in datasets_list:\n    print(f\"\\n=== Dataset: {ds_name} ===\")\n    ds = load_dataset(ds_name)\n    train_val = ds[\"train\"].train_test_split(test_size=0.1, seed=0)\n    train_hf, val_hf = train_val[\"train\"], train_val[\"test\"]\n    test_hf = ds[\"test\"]\n\n    class HFDataset(Dataset):\n        def __init__(self, hf_ds):\n            self.ds = hf_ds\n\n        def __len__(self):\n            return len(self.ds)\n\n        def __getitem__(self, idx):\n            item = self.ds[idx]\n            img = np.array(item[\"image\"], dtype=np.float32) / 255.0\n            if img.ndim == 2:\n                img = np.expand_dims(img, 2)\n            img_t = torch.from_numpy(img).permute(2, 0, 1)\n            lbl = torch.tensor(item[\"label\"], dtype=torch.long)\n            return {\"features\": img_t, \"label\": lbl, \"idx\": idx}\n\n    train_ds = HFDataset(train_hf)\n    val_ds = HFDataset(val_hf)\n    test_ds = HFDataset(test_hf)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n\n    n_train = len(train_ds)\n    cluster_indices = list(range(min(1000, n_train)))\n    cluster_loader = DataLoader(\n        train_ds, batch_size=1, sampler=SubsetRandomSampler(cluster_indices)\n    )\n\n    in_ch = 3 if ds_name == \"cifar10\" else 1\n    n_cls = len(set(train_hf[\"label\"]))\n    model = SimpleCNN(in_ch, n_cls).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    warmup_epochs, train_epochs = 3, 10\n    total_epochs = warmup_epochs + train_epochs\n    sample_weights = None\n    cluster_labels = None\n    y_true = None\n\n    exp = {\n        \"metrics\": {\"train_loss\": [], \"val_loss\": [], \"val_acc\": [], \"nmi\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            losses = criterion(out, batch[\"label\"])\n            if epoch >= warmup_epochs and sample_weights is not None:\n                w = sample_weights[batch[\"idx\"]]\n                loss = (losses * w).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads_list, y_list = [], []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out = model(batch[\"features\"])\n                l = criterion(out, batch[\"label\"]).mean()\n                l.backward()\n                grad = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads_list.append(grad)\n                y_list.append(batch[\"label\"].item())\n            grads = np.stack(grads_list)\n            emb = PCA(n_components=10, random_state=0).fit_transform(grads)\n            best_k, best_score = 2, -1\n            for k in [2, 3]:\n                lbls = KMeans(n_clusters=k, random_state=0).fit_predict(emb)\n                s = silhouette_score(emb, lbls)\n                if s > best_score:\n                    best_score, best_k = s, k\n            kmeans = KMeans(n_clusters=best_k, random_state=0).fit(emb)\n            cluster_labels = kmeans.labels_\n            y_true = y_list\n            counts = np.bincount(cluster_labels)\n            sw = np.array(\n                [1.0 / counts[lab] for lab in cluster_labels], dtype=np.float32\n            )\n            sw_full = np.ones(n_train, dtype=np.float32)\n            for i, idx in enumerate(cluster_indices):\n                sw_full[idx] = sw[i]\n            sample_weights = torch.tensor(sw_full, device=device)\n\n        # Evaluate\n        train_loss_sum, train_count = 0, 0\n        model.eval()\n        with torch.no_grad():\n            for batch in train_loader:\n                batch = {\n                    k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                }\n                out = model(batch[\"features\"])\n                train_loss_sum += criterion(out, batch[\"label\"]).sum().item()\n                train_count += batch[\"label\"].size(0)\n        val_loss_sum, val_count, correct = 0, 0, 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {\n                    k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                }\n                out = model(batch[\"features\"])\n                val_loss_sum += criterion(out, batch[\"label\"]).sum().item()\n                preds = out.argmax(1)\n                correct += (preds == batch[\"label\"]).sum().item()\n                val_count += batch[\"label\"].size(0)\n        val_acc = correct / val_count\n        train_loss = train_loss_sum / train_count\n        val_loss = val_loss_sum / val_count\n        nmi = (\n            normalized_mutual_info_score(y_true, cluster_labels)\n            if cluster_labels is not None\n            else 0.0\n        )\n\n        print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, NMI = {nmi:.4f}\")\n\n        exp[\"metrics\"][\"train_loss\"].append(train_loss)\n        exp[\"metrics\"][\"val_loss\"].append(val_loss)\n        exp[\"metrics\"][\"val_acc\"].append(val_acc)\n        exp[\"metrics\"][\"nmi\"].append(nmi)\n        exp[\"losses\"][\"train\"].append(train_loss)\n        exp[\"losses\"][\"val\"].append(val_loss)\n\n    # Test predictions\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n    exp[\"predictions\"] = preds\n    exp[\"ground_truth\"] = truths\n    experiment_data[ds_name] = exp\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom sklearn.metrics import silhouette_score, normalized_mutual_info_score\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Simple k-means\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    centroids = X[rng.choice(N, n_clusters, replace=False)].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(-1)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid, num_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, num_classes)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Dataset wrapper\nclass SpuriousDataset(Dataset):\n    def __init__(self, feats, labels, groups):\n        self.X = torch.tensor(feats, dtype=torch.float32)\n        self.y = torch.tensor(labels, dtype=torch.long)\n        self.g = torch.tensor(groups, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return self.X[i], self.y[i], self.g[i], i\n\n\n# Evaluation\ndef evaluate(loader, model, criterion):\n    model.eval()\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for Xb, yb, gb, _ in loader:\n            Xb, yb, gb = Xb.to(device), yb.to(device), gb.to(device)\n            out = model(Xb)\n            l = criterion(out, yb).sum().item()\n            loss_sum += l\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = gb == g\n                cnt = mask.sum().item()\n                total[g] += cnt\n                if cnt > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg = min(correct[g] / total[g] if total[g] > 0 else 0 for g in (0, 1))\n    return loss_sum, wg\n\n\nexperiment_data = {}\nfor ds_name in (\"mnist\", \"fashion_mnist\", \"kmnist\"):\n    # load & filter binary classes 0/1\n    ds_full = load_dataset(ds_name)\n    ds_tr = ds_full[\"train\"].filter(lambda x: x[\"label\"] < 2).shuffle(0)\n    ds_te = ds_full[\"test\"].filter(lambda x: x[\"label\"] < 2).shuffle(0)\n    # subsample\n    n_tr, n_val, n_te = 2000, 500, 500\n    ds_tr = ds_tr.select(range(min(len(ds_tr), n_tr + n_val)))\n    ds_train = ds_tr.select(range(min(len(ds_tr), n_tr)))\n    ds_val = ds_tr.select(range(min(len(ds_tr), n_tr), min(len(ds_tr), n_tr + n_val)))\n    ds_test = ds_te.select(range(min(len(ds_te), n_te)))\n\n    # build arrays\n    def build(ds, p):\n        feats, labs, grs = [], [], []\n        for ex in ds:\n            img = np.array(ex[\"image\"], dtype=np.float32).flatten() / 255.0\n            y = ex[\"label\"]\n            sp = y if (np.random.rand() < p) else 1 - y\n            feats.append(np.concatenate([img, [sp]]))\n            labs.append(y)\n            grs.append(sp)\n        return np.stack(feats), np.array(labs), np.array(grs)\n\n    Xtr, ytr, gtr = build(ds_train, 0.9)\n    Xval, yval, gval = build(ds_val, 0.5)\n    Xte, yte, gte = build(ds_test, 0.5)\n    # datasets & loaders\n    train_ds = SpuriousDataset(Xtr, ytr, gtr)\n    val_ds = SpuriousDataset(Xval, yval, gval)\n    test_ds = SpuriousDataset(Xte, yte, gte)\n    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n    cluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n    # model, criterion, optimizer\n    model = MLP(Xtr.shape[1], 128, 2).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    sample_weights = None\n\n    # recorders\n    train_wg, val_wg = [], []\n    train_ls, val_ls = [], []\n    nmi_list, silh_list = [], []\n\n    total_epochs = 4\n    for epoch in range(total_epochs):\n        # training\n        model.train()\n        for Xb, yb, gb, idb in train_loader:\n            Xb, yb, gb = Xb.to(device), yb.to(device), gb.to(device)\n            idb = torch.tensor(idb, dtype=torch.long, device=device)\n            out = model(Xb)\n            losses = criterion(out, yb)\n            if sample_weights is not None:\n                w = sample_weights[idb]\n                loss = (losses * w).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        # eval\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_w = evaluate(val_loader, model, criterion)\n        print(\n            f\"{ds_name} Epoch {epoch}: validation_loss = {v_loss/len(val_ds):.4f}, val_wg = {v_w:.4f}\"\n        )\n        train_wg.append(tr_wg)\n        val_wg.append(v_w)\n        train_ls.append(tr_loss / len(train_ds))\n        val_ls.append(v_loss / len(val_ds))\n\n        # gradient clustering\n        model.eval()\n        grads = []\n        for Xb, yb, gb, _ in cluster_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            l = criterion(out, yb).mean()\n            l.backward()\n            grads.append(model.fc.weight.grad.detach().cpu().numpy().ravel())\n        grads = np.stack(grads)\n        # dynamic k\n        best_k, best_s = None, -1\n        best_lbl = None\n        for k in (2, 3, 4, 5):\n            if k >= 2 and k <= len(grads):\n                lbl = kmeans_np(grads, n_clusters=k, n_iters=20)\n                uc, cnt = np.unique(lbl, return_counts=True)\n                if np.min(cnt) < 2:\n                    continue\n                s_score = silhouette_score(grads, lbl)\n                if s_score > best_s:\n                    best_s, best_k, best_lbl = s_score, k, lbl\n        if best_lbl is None:\n            best_lbl = kmeans_np(grads, 2, 10)\n            best_s = (\n                silhouette_score(grads, best_lbl)\n                if len(np.unique(best_lbl)) > 1\n                else -1\n            )\n            best_k = 2\n        # weights & metrics\n        counts = np.bincount(best_lbl, minlength=best_k)\n        w_arr = np.array(\n            [1.0 / counts[l] if counts[l] > 0 else 1.0 for l in best_lbl],\n            dtype=np.float32,\n        )\n        sample_weights = torch.tensor(w_arr, device=device)\n        nmi = normalized_mutual_info_score(train_ds.g.numpy(), best_lbl)\n        nmi_list.append(nmi)\n        silh_list.append(best_s)\n\n    # final test\n    preds, truths = [], []\n    model.eval()\n    with torch.no_grad():\n        for Xb, yb, gb, _ in test_loader:\n            Xb = Xb.to(device)\n            out = model(Xb).argmax(1).cpu().numpy().tolist()\n            preds.extend(out)\n            truths.extend(yb.numpy().tolist())\n\n    experiment_data[ds_name] = {\n        \"metrics\": {\n            \"train\": np.array(train_wg),\n            \"val\": np.array(val_wg),\n            \"nmi\": np.array(nmi_list),\n            \"silhouette\": np.array(silh_list),\n        },\n        \"losses\": {\"train\": np.array(train_ls), \"val\": np.array(val_ls)},\n        \"predictions\": np.array(preds),\n        \"ground_truth\": np.array(truths),\n    }\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Subset, Dataset\nimport torch.nn as nn\nimport torch.optim as optim\nfrom datasets import load_dataset\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, normalized_mutual_info_score\nfrom sklearn.decomposition import PCA\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Configuration for three datasets and binary classes\ndataset_configs = [\n    {\"name\": \"mnist\", \"hf_name\": \"mnist\", \"class1\": 0, \"class2\": 1},\n    {\"name\": \"fashion_mnist\", \"hf_name\": \"fashion_mnist\", \"class1\": 0, \"class2\": 1},\n    {\"name\": \"cifar10\", \"hf_name\": \"cifar10\", \"class1\": 3, \"class2\": 5},\n]\n\nexperiment_data = {}\nfor cfg in dataset_configs:\n    # Load and filter to two classes\n    ds = load_dataset(cfg[\"hf_name\"], split=\"train\")\n    ds = ds.filter(lambda x: x[\"label\"] in (cfg[\"class1\"], cfg[\"class2\"]))\n    ds = ds.map(lambda x: {\"label\": 0 if x[\"label\"] == cfg[\"class1\"] else 1})\n    # Subsample for speed\n    N_total = min(len(ds), 2000)\n    ds = ds.select(range(N_total))\n    # Create spurious labels with 95% correlation\n    np.random.seed(0)\n    labels_np = np.array(ds[\"label\"])\n    sp = np.where(np.random.rand(N_total) < 0.95, labels_np, 1 - labels_np).astype(\n        np.int64\n    )\n\n    # PyTorch Dataset inserting a colored patch as spurious feature\n    class HFSpuriousDataset(Dataset):\n        def __init__(self, hf_ds, sp_arr):\n            self.ds = hf_ds\n            self.sp = sp_arr\n\n        def __len__(self):\n            return len(self.ds)\n\n        def __getitem__(self, idx):\n            img = self.ds[idx][\"image\"]\n            arr = np.array(img)\n            if arr.ndim == 2:\n                arr = np.stack([arr] * 3, axis=-1)\n            # red patch for sp=1, green patch for sp=0\n            if self.sp[idx] == 1:\n                arr[:4, :4, 0] = 255\n            else:\n                arr[:4, :4, 1] = 255\n            tensor = torch.tensor(arr, dtype=torch.float32).permute(2, 0, 1) / 255.0\n            return {\n                \"features\": tensor,\n                \"label\": torch.tensor(self.ds[idx][\"label\"], dtype=torch.long),\n                \"group\": torch.tensor(self.sp[idx], dtype=torch.long),\n                \"idx\": idx,\n            }\n\n    full_ds = HFSpuriousDataset(ds, sp)\n    idxs = np.arange(N_total)\n    np.random.shuffle(idxs)\n    n1 = int(0.5 * N_total)\n    n2 = int(0.25 * N_total)\n    train_idx, val_idx, test_idx = idxs[:n1], idxs[n1 : n1 + n2], idxs[n1 + n2 :]\n    train_ds = Subset(full_ds, train_idx)\n    val_ds = Subset(full_ds, val_idx)\n    test_ds = Subset(full_ds, test_idx)\n\n    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n    cluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n    # Simple CNN architecture\n    class SimpleCNN(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv = nn.Sequential(\n                nn.Conv2d(3, 16, 3, padding=1),\n                nn.ReLU(),\n                nn.Conv2d(16, 32, 3, padding=1),\n                nn.ReLU(),\n                nn.AdaptiveAvgPool2d(1),\n            )\n            self.fc = nn.Linear(32, 2)\n\n        def forward(self, x):\n            f = self.conv(x).view(x.size(0), -1)\n            return self.fc(f)\n\n    model = SimpleCNN().to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n    # Containers for metrics\n    metrics_train, metrics_val, metrics_nmi = [], [], []\n    losses_train, losses_val = [], []\n    sample_weights = None\n    cluster_labels = None\n\n    warmup_epochs = 1\n    total_epochs = 6\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # Unsupervised gradient clustering after warmup\n        if epoch == warmup_epochs:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out = model(batch[\"features\"])\n                l = criterion(out, batch[\"label\"]).mean()\n                l.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            red = PCA(n_components=min(10, grads.shape[1])).fit_transform(grads)\n            best_k, best_score = 2, -1\n            for k in [2, 3, 4, 5]:\n                labs = KMeans(n_clusters=k, n_init=5).fit_predict(red)\n                score = silhouette_score(red, labs)\n                if score > best_score:\n                    best_score, best_k = score, k\n            cluster_labels = KMeans(n_clusters=best_k, n_init=5).fit_predict(red)\n            counts = np.bincount(cluster_labels, minlength=best_k)\n            sw = np.array(\n                [1.0 / counts[lab] for lab in cluster_labels], dtype=np.float32\n            )\n            sample_weights = torch.tensor(sw, device=device)\n\n        # Evaluation function\n        def evaluate(loader):\n            loss_sum = 0.0\n            correct = {0: 0, 1: 0}\n            total = {0: 0, 1: 0}\n            with torch.no_grad():\n                for b in loader:\n                    batch = {\n                        k: v.to(device) for k, v in b.items() if torch.is_tensor(v)\n                    }\n                    x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n                    out = model(x)\n                    ls = criterion(out, yb)\n                    loss_sum += ls.sum().item()\n                    preds = out.argmax(1)\n                    for g in (0, 1):\n                        mask = grp == g\n                        total[g] += mask.sum().item()\n                        if mask.sum() > 0:\n                            correct[g] += (preds[mask] == yb[mask]).sum().item()\n            val_loss = loss_sum / len(loader.dataset)\n            wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0 for g in (0, 1))\n            return val_loss, wg_acc\n\n        tr_loss, tr_wg = evaluate(train_loader)\n        v_loss, v_wg = evaluate(val_loader)\n        if cluster_labels is not None:\n            nmi = normalized_mutual_info_score(sp[train_idx], cluster_labels)\n        else:\n            nmi = 0.0\n        print(f\"Epoch {epoch}: validation_loss = {v_loss:.4f}\")\n        metrics_train.append(tr_wg)\n        metrics_val.append(v_wg)\n        metrics_nmi.append(nmi)\n        losses_train.append(tr_loss)\n        losses_val.append(v_loss)\n\n    # Final test predictions\n    model.eval()\n    preds, truth, groups = [], [], []\n    with torch.no_grad():\n        for b in test_loader:\n            batch = {k: v.to(device) for k, v in b.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truth.extend(batch[\"label\"].cpu().tolist())\n            groups.extend(batch[\"group\"].cpu().tolist())\n\n    experiment_data[cfg[\"name\"]] = {\n        \"metrics\": {\n            \"train\": np.array(metrics_train),\n            \"val\": np.array(metrics_val),\n            \"nmi\": np.array(metrics_nmi),\n        },\n        \"losses\": {\"train\": np.array(losses_train), \"val\": np.array(losses_val)},\n        \"predictions\": np.array(preds),\n        \"ground_truth\": np.array(truth),\n        \"test_group\": np.array(groups),\n    }\n\n# Save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Custom metrics to avoid sklearn dependency\ndef silhouette_score_np(X, labels):\n    N = X.shape[0]\n    if N <= 1:\n        return -1.0\n    # pairwise distance matrix\n    D = np.sqrt(((X[:, None, :] - X[None, :, :]) ** 2).sum(axis=2))\n    sils = []\n    for i in range(N):\n        li = labels[i]\n        same = np.where(labels == li)[0]\n        same = same[same != i]\n        if same.size == 0:\n            continue\n        a = D[i, same].mean()\n        b = np.inf\n        for lj in np.unique(labels):\n            if lj == li:\n                continue\n            other = np.where(labels == lj)[0]\n            if other.size == 0:\n                continue\n            b = min(b, D[i, other].mean())\n        sils.append((b - a) / max(a, b))\n    return float(np.mean(sils)) if sils else -1.0\n\n\ndef normalized_mutual_info_score_np(labels_true, labels_pred):\n    labels_true = np.asarray(labels_true)\n    labels_pred = np.asarray(labels_pred)\n    N = labels_true.size\n    ut = np.unique(labels_true)\n    up = np.unique(labels_pred)\n    # build contingency table\n    contingency = np.zeros((ut.size, up.size), dtype=np.int64)\n    for i, t in enumerate(ut):\n        for j, p in enumerate(up):\n            contingency[i, j] = np.sum((labels_true == t) & (labels_pred == p))\n    a = contingency.sum(axis=1)\n    b = contingency.sum(axis=0)\n    # mutual information\n    nz = contingency > 0\n    MI = np.sum(\n        contingency[nz]\n        / N\n        * np.log(\n            (contingency[nz] * N)\n            / (\n                a.repeat(b.shape[0]).reshape(a.shape[0], b.shape[0])[nz]\n                * b.repeat(a.shape[0]).reshape(b.shape[0], a.shape[0]).T[nz]\n            )\n            + 1e-10\n        )\n    )\n    Ht = -np.sum((a / N) * np.log(a / N + 1e-10))\n    Hp = -np.sum((b / N) * np.log(b / N + 1e-10))\n    return float(MI / ((Ht + Hp) / 2 + 1e-10))\n\n\n# k-means\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    centroids = X[rng.choice(N, n_clusters, replace=False)].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(-1)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if pts.shape[0] > 0:\n                centroids[k] = pts.mean(0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid, num_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, num_classes)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Dataset wrapper\nclass SpuriousDataset(Dataset):\n    def __init__(self, feats, labels, groups):\n        self.X = torch.tensor(feats, dtype=torch.float32)\n        self.y = torch.tensor(labels, dtype=torch.long)\n        self.g = torch.tensor(groups, dtype=torch.long)\n\n    def __len__(self):\n        return self.y.size(0)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx], self.g[idx], idx\n\n\n# evaluation\ndef evaluate(loader, model, criterion):\n    model.eval()\n    loss_sum = 0.0\n    correct = {}\n    total = {}\n    with torch.no_grad():\n        for Xb, yb, gb, _ in loader:\n            Xb, yb, gb = Xb.to(device), yb.to(device), gb.to(device)\n            out = model(Xb)\n            l = criterion(out, yb).sum().item()\n            loss_sum += l\n            preds = out.argmax(1)\n            for g in torch.unique(gb).tolist():\n                mask = gb == g\n                total[g] = total.get(g, 0) + mask.sum().item()\n                correct[g] = correct.get(g, 0) + (preds[mask] == yb[mask]).sum().item()\n    wg = min(correct[g] / total[g] for g in total) if total else 0.0\n    return loss_sum, wg\n\n\nexperiment_data = {}\nfor ds_name in (\"mnist\", \"fashion_mnist\", \"kmnist\"):\n    # load & filter binary\n    ds_full = load_dataset(ds_name)\n    ds_tr = ds_full[\"train\"].filter(lambda x: x[\"label\"] < 2).shuffle(0)\n    ds_te = ds_full[\"test\"].filter(lambda x: x[\"label\"] < 2).shuffle(0)\n    n_tr, n_val, n_te = 2000, 500, 500\n    ds_tr = ds_tr.select(range(min(len(ds_tr), n_tr + n_val)))\n    ds_train = ds_tr.select(range(min(len(ds_tr), n_tr)))\n    ds_val = ds_tr.select(range(min(len(ds_tr), n_tr), min(len(ds_tr), n_tr + n_val)))\n    ds_test = ds_te.select(range(min(len(ds_te), n_te)))\n\n    def build(ds, p):\n        feats, labs, grs = [], [], []\n        for ex in ds:\n            img = np.array(ex[\"image\"], dtype=np.float32).flatten() / 255.0\n            y = ex[\"label\"]\n            sp = y if (np.random.rand() < p) else 1 - y\n            feats.append(np.concatenate([img, [sp]]))\n            labs.append(y)\n            grs.append(sp)\n        return np.stack(feats), np.array(labs), np.array(grs)\n\n    Xtr, ytr, gtr = build(ds_train, 0.9)\n    Xval, yval, gval = build(ds_val, 0.5)\n    Xte, yte, gte = build(ds_test, 0.5)\n\n    train_ds = SpuriousDataset(Xtr, ytr, gtr)\n    val_ds = SpuriousDataset(Xval, yval, gval)\n    test_ds = SpuriousDataset(Xte, yte, gte)\n\n    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n    cluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n    model = MLP(Xtr.shape[1], 128, 2).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    sample_weights = None\n\n    train_wg, val_wg = [], []\n    train_ls, val_ls = [], []\n    nmi_list, silh_list = [], []\n\n    total_epochs = 4\n    for epoch in range(total_epochs):\n        model.train()\n        for Xb, yb, gb, idb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            idb = idb.to(device)\n            out = model(Xb)\n            losses = criterion(out, yb)\n            if sample_weights is not None:\n                w = sample_weights[idb]\n                loss = (losses * w).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(\n            f\"{ds_name} Epoch {epoch}: validation_loss = {v_loss/len(val_ds):.4f}, val_wg = {v_wg:.4f}\"\n        )\n        train_wg.append(tr_wg)\n        val_wg.append(v_wg)\n        train_ls.append(tr_loss / len(train_ds))\n        val_ls.append(v_loss / len(val_ds))\n\n        # gradient clustering\n        model.eval()\n        grads = []\n        for Xb, yb, _, _ in cluster_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            l = criterion(out, yb).mean()\n            l.backward()\n            grads.append(model.fc.weight.grad.detach().cpu().numpy().ravel())\n        grads = np.stack(grads)\n\n        best_k, best_s, best_lbl = None, -1, None\n        for k in (2, 3, 4, 5):\n            if k <= len(grads):\n                lbl = kmeans_np(grads, n_clusters=k, n_iters=20)\n                counts = np.bincount(lbl, minlength=k)\n                if np.min(counts) < 2:\n                    continue\n                s_score = silhouette_score_np(grads, lbl)\n                if s_score > best_s:\n                    best_s, best_k, best_lbl = s_score, k, lbl\n        if best_lbl is None:\n            best_lbl = kmeans_np(grads, n_clusters=2, n_iters=10)\n            best_s = silhouette_score_np(grads, best_lbl)\n            best_k = 2\n\n        counts = np.bincount(best_lbl, minlength=best_k)\n        w_arr = np.array([1.0 / counts[l] for l in best_lbl], dtype=np.float32)\n        sample_weights = torch.tensor(w_arr, device=device)\n\n        nmi = normalized_mutual_info_score_np(train_ds.g.numpy(), best_lbl)\n        nmi_list.append(nmi)\n        silh_list.append(best_s)\n\n    # final test\n    preds, truths = [], []\n    model.eval()\n    with torch.no_grad():\n        for Xb, yb, _, _ in test_loader:\n            Xb = Xb.to(device)\n            out = model(Xb).argmax(1).cpu().numpy()\n            preds.extend(out.tolist())\n            truths.extend(yb.numpy().tolist())\n\n    experiment_data[ds_name] = {\n        \"metrics\": {\n            \"train\": np.array(train_wg),\n            \"val\": np.array(val_wg),\n            \"nmi\": np.array(nmi_list),\n            \"silhouette\": np.array(silh_list),\n        },\n        \"losses\": {\"train\": np.array(train_ls), \"val\": np.array(val_ls)},\n        \"predictions\": np.array(preds),\n        \"ground_truth\": np.array(truths),\n    }\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset\nimport random\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n# K-means on NumPy\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    centroids = X[rng.choice(N, n_clusters, replace=False)].copy()\n    labels = np.zeros(N, int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n        new_labels = dists.argmin(axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            centroids[k] = pts.mean(axis=0) if len(pts) > 0 else X[rng.randint(N)]\n    return labels\n\n\n# NMI computation\ndef compute_nmi(labels, true):\n    labels = np.array(labels)\n    true = np.array(true)\n    eps = 1e-10\n    N = len(labels)\n    clusters = np.unique(labels)\n    classes = np.unique(true)\n    MI = 0.0\n    for c in clusters:\n        p_x = (labels == c).sum() / N\n        for k in classes:\n            p_y = (true == k).sum() / N\n            p_xy = ((labels == c) & (true == k)).sum() / N\n            if p_xy > 0:\n                MI += p_xy * np.log(p_xy / (p_x * p_y) + eps)\n    Hx = -sum(\n        (labels == c).sum() / N * np.log((labels == c).sum() / N + eps)\n        for c in clusters\n    )\n    Hy = -sum(\n        (true == k).sum() / N * np.log((true == k).sum() / N + eps) for k in classes\n    )\n    return 2 * MI / (Hx + Hy + eps)\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\ndef evaluate(loader, model, criterion):\n    model.eval()\n    total_loss = 0.0\n    total_samples = 0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            total_loss += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                correct[g] += (preds[mask] == yb[mask]).sum().item()\n            total_samples += yb.size(0)\n    avg_loss = total_loss / total_samples\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return avg_loss, wg_acc\n\n\nclass SmallCNN(nn.Module):\n    def __init__(self, flat_dim):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(3, 16, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(16, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n        )\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(flat_dim, 64)\n        self.relu2 = nn.ReLU()\n        self.fc2 = nn.Linear(64, 2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.flatten(x)\n        x = self.relu2(self.fc1(x))\n        return self.fc2(x)\n\n\n# Experiment settings\nsp_corr_train = 0.95\nsp_corr_eval = 0.5\nwarmup_epochs = 1\ntrain_epochs = 2\ntotal_epochs = warmup_epochs + train_epochs\nbatch_size = 64\ndatasets_list = [\"mnist\", \"fashion_mnist\", \"cifar10\"]\nexperiment_data = {}\n\nfor name in datasets_list:\n    raw = load_dataset(name)\n    # filter to binary classes 0/1\n    train_raw = (\n        raw[\"train\"]\n        .filter(lambda ex: ex[\"label\"] in [0, 1])\n        .shuffle(seed=0)\n        .select(range(2000))\n    )\n    test_all = (\n        raw[\"test\"]\n        .filter(lambda ex: ex[\"label\"] in [0, 1])\n        .shuffle(seed=0)\n        .select(range(1000))\n    )\n    val_raw = test_all.select(range(500))\n    test_raw = test_all.select(range(500, 1000))\n\n    # function to build arrays\n    def make_split(ds, corr):\n        imgs = ds[\"image\"]\n        ys = ds[\"label\"]\n        X, y, g = [], [], []\n        for im, lab in zip(imgs, ys):\n            arr = np.array(im)\n            if arr.ndim == 2:\n                arr = np.stack([arr] * 3, axis=2)\n            grp = lab if random.random() < corr else 1 - lab\n            patch = 5\n            if grp == 1:\n                arr[:patch, :patch, 0] = 255\n                arr[:patch, :patch, 1:] = 0\n            else:\n                arr[:patch, :patch, 2] = 255\n                arr[:patch, :patch, :2] = 0\n            arr = arr.astype(np.float32) / 255.0\n            arr = (arr - 0.5) / 0.5\n            arr = arr.transpose(2, 0, 1)\n            X.append(arr)\n            y.append(lab)\n            g.append(grp)\n        return np.stack(X), np.array(y), np.array(g)\n\n    X_train, y_train, g_train = make_split(train_raw, sp_corr_train)\n    X_val, y_val, g_val = make_split(val_raw, sp_corr_eval)\n    X_test, y_test, g_test = make_split(test_raw, sp_corr_eval)\n    # DataLoaders\n    train_ds = CustomDataset(X_train, y_train, g_train)\n    val_ds = CustomDataset(X_val, y_val, g_val)\n    test_ds = CustomDataset(X_test, y_test, g_test)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n    cluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n    # model & optimizer\n    img_size = 28 if name != \"cifar10\" else 32\n    flat_dim = 32 * (img_size // 4) * (img_size // 4)\n    model = SmallCNN(flat_dim).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    sample_weights = None\n    nmi_val = np.nan\n    experiment_data[name] = {\n        \"metrics\": {\n            \"train_loss\": [],\n            \"val_loss\": [],\n            \"train_wg\": [],\n            \"val_wg\": [],\n            \"nmi\": [],\n        },\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # training loop\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        # clustering at end of warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out = model(batch[\"features\"])\n                l = criterion(out, batch[\"label\"]).mean()\n                l.backward()\n                grads.append(model.fc2.weight.grad.detach().cpu().view(-1).numpy())\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, 2, 10)\n            nmi_val = compute_nmi(labels, g_train)\n            counts = np.bincount(labels, minlength=2)\n            sw = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw, device=device)\n        # eval\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(f\"Epoch {epoch}: validation_loss = {v_loss:.4f}\")\n        exp = experiment_data[name][\"metrics\"]\n        exp[\"train_loss\"].append(tr_loss)\n        exp[\"val_loss\"].append(v_loss)\n        exp[\"train_wg\"].append(tr_wg)\n        exp[\"val_wg\"].append(v_wg)\n        exp[\"nmi\"].append(nmi_val)\n    # test predictions\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            preds.extend(model(batch[\"features\"]).argmax(1).cpu().tolist())\n    experiment_data[name][\"predictions\"] = preds\n    experiment_data[name][\"ground_truth\"] = y_test.tolist()\n\n# save all data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.transforms as T\nfrom datasets import load_dataset\nimport math\n\n# setup working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# simple k-means + centroids\ndef kmeans_np(X, n_clusters, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    idx = rng.choice(N, n_clusters, replace=False)\n    centroids = X[idx].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n        new = dists.argmin(axis=1)\n        if np.all(new == labels):\n            break\n        labels = new\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels, centroids\n\n\n# compute NMI\ndef compute_nmi(labels, groups):\n    N = len(labels)\n    cnt_u = {}\n    cnt_v = {}\n    for u in labels:\n        cnt_u[u] = cnt_u.get(u, 0) + 1\n    for v in groups:\n        cnt_v[v] = cnt_v.get(v, 0) + 1\n    p_u = {u: cnt_u[u] / N for u in cnt_u}\n    p_v = {v: cnt_v[v] / N for v in cnt_v}\n    joint = {}\n    for u, v in zip(labels, groups):\n        joint[(u, v)] = joint.get((u, v), 0) + 1\n    p_uv = {k: joint[k] / N for k in joint}\n    H_u = -sum(p * math.log(p + 1e-8) for p in p_u.values())\n    H_v = -sum(p * math.log(p + 1e-8) for p in p_v.values())\n    I = 0.0\n    for (u, v), p in p_uv.items():\n        I += p * (\n            math.log(p + 1e-8) - math.log(p_u[u] + 1e-8) - math.log(p_v[v] + 1e-8)\n        )\n    return I / (((H_u + H_v) / 2) + 1e-8)\n\n\n# evaluation: worst-group accuracy & loss sum\ndef evaluate(loader, model, criterion):\n    model.eval()\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, y, g = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, y)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for gg in (0, 1):\n                m = g == gg\n                total[gg] += m.sum().item()\n                if m.sum().item() > 0:\n                    correct[gg] += (preds[m] == y[m]).sum().item()\n    wg = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg\n\n\n# small CNN\nclass SmallCNN(nn.Module):\n    def __init__(self, in_ch, num_classes):\n        super().__init__()\n        self.hidden = nn.Sequential(\n            nn.Conv2d(in_ch, 32, 3, 1, 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, 1, 1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d(1),\n        )\n        self.classifier = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        h = self.hidden(x).view(x.size(0), -1)\n        return self.classifier(h)\n\n\n# colored spurious dataset wrapper\nclass ColoredHFImageDataset(Dataset):\n    def __init__(self, hf_ds, sp_corr, mean, std):\n        self.ds = hf_ds\n        self.sp = sp_corr\n        self.norm = T.Normalize(mean, std)\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        item = self.ds[idx]\n        img = item.get(\"image\", item.get(\"img\"))\n        label = item[\"label\"]\n        # spurious color bit\n        flip = np.random.rand() < self.sp\n        sp = (label % 2) if flip else 1 - (label % 2)\n        arr = np.array(img)\n        if arr.ndim == 2:\n            arr = np.stack([arr] * 3, 2)\n        col = np.zeros_like(arr)\n        if sp == 0:\n            col[..., 0] = arr[..., 0]\n        else:\n            col[..., 1] = arr[..., 0]\n        img2 = Image.fromarray(col)\n        x = T.ToTensor()(img2)\n        x = self.norm(x)\n        return {\n            \"features\": x,\n            \"label\": torch.tensor(label, torch.long),\n            \"group\": torch.tensor(sp, torch.long),\n            \"idx\": idx,\n        }\n\n\n# experiment across three HF datasets\ndatasets_cfg = {\n    \"mnist\": ([0.5] * 3, [0.5] * 3),\n    \"fashion_mnist\": ([0.5] * 3, [0.5] * 3),\n    \"cifar10\": ([0.5] * 3, [0.5] * 3),\n}\nspurious_corr = 0.9\nresults = {}\n\nfor name, (mean, std) in datasets_cfg.items():\n    print(f\"\\n*** DATASET {name} ***\")\n    ds = load_dataset(name)\n    split = ds[\"train\"].train_test_split(0.2, seed=0)\n    train_hf, val_hf = split[\"train\"], split[\"test\"]\n    test_hf = ds[\"test\"]\n    train_ds = ColoredHFImageDataset(train_hf, spurious_corr, mean, std)\n    val_ds = ColoredHFImageDataset(val_hf, spurious_corr, mean, std)\n    test_ds = ColoredHFImageDataset(test_hf, spurious_corr, mean, std)\n    N = len(train_ds)\n    bs = 128\n    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n\n    model = SmallCNN(\n        3,\n        (\n            len(ds[\"train\"].features[\"label\"].names)\n            if hasattr(ds[\"train\"], \"features\")\n            else len(set(ds[\"train\"][\"label\"]))\n        ),\n    ).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    warmup, train_ep = 1, 2\n    total = warmup + train_ep\n    sw = None\n    tr_wg, val_wg, nmi_list = [], [], []\n    for epoch in range(total):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, y, idx = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, y)\n            if epoch > warmup and sw is not None:\n                loss = (losses * sw[idx]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # after warmup, build clusters\n        if epoch == warmup:\n            # sample subset\n            sub = np.random.choice(N, min(N, 2000), replace=False)\n            subset = Subset(train_ds, sub)\n            loader = DataLoader(subset, batch_size=1, shuffle=False)\n            # collect gradients from two layers\n            grads = []\n            true_g = []\n            for b in loader:\n                b = {k: v.to(device) for k, v in b.items() if torch.is_tensor(v)}\n                optimizer.zero_grad()\n                out = model.hidden(b[\"features\"])\n                out = out.view(1, -1)\n                logits = model.classifier(out)\n                l = criterion(logits, b[\"label\"])\n                l.mean().backward()\n                g1 = model.classifier.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g1)\n                true_g.append(int(b[\"group\"].cpu().item()))\n            grads = np.stack(grads)\n            # pick best k\n            best = (-1, None, None)\n            for k in (2, 3):\n                lbls, cents = kmeans_np(grads, k)\n                # intra\n                d_intra = ((grads - cents[lbls]) ** 2).sum(1).mean()\n                # inter\n                d_inter = (\n                    np.mean(\n                        [\n                            ((cents[i] - cents[j]) ** 2).sum()\n                            for i in range(k)\n                            for j in range(i + 1, k)\n                        ]\n                    )\n                    if k > 1\n                    else 0\n                )\n                score = d_inter / (d_intra + 1e-8)\n                if score > best[0]:\n                    best = (score, lbls, cents)\n            _, sub_lbls, cents = best\n            # assign weights\n            cnt = np.bincount(sub_lbls, minlength=cents.shape[0])\n            sw_arr = np.ones(N, dtype=np.float32)\n            for i, lab in enumerate(sub_lbls):\n                sw_arr[sub[i]] = 1.0 / (cnt[lab] + 1e-8)\n            sw = torch.tensor(sw_arr, device=device)\n            # compute NMI\n            nmi0 = compute_nmi(sub_lbls, true_g)\n        # evaluate\n        lval, wg = evaluate(val_loader, model, criterion)\n        val_wg.append(wg)\n        nmi_list.append(nmi0 if epoch >= warmup else 0.0)\n        print(\n            f\"[{name}] Epoch {epoch}: validation_loss = {lval/len(val_ds):.4f}, val_wg = {wg:.4f}, NMI = {nmi_list[-1]:.4f}\"\n        )\n\n    results[name] = {\"metrics\": {\"val_wg\": val_wg}, \"nmi\": nmi_list}\n\n# save data\nexperiment_data = results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Saved experiment data to {working_dir}/experiment_data.npy\")\n", "import os\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.transforms as T\nfrom datasets import load_dataset\nimport math\n\n# setup working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# simple k-means + centroids\ndef kmeans_np(X, n_clusters, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    idx = rng.choice(N, n_clusters, replace=False)\n    centroids = X[idx].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n        new = dists.argmin(axis=1)\n        if np.all(new == labels):\n            break\n        labels = new\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels, centroids\n\n\n# compute NMI\ndef compute_nmi(labels, groups):\n    N = len(labels)\n    cnt_u = {}\n    cnt_v = {}\n    for u in labels:\n        cnt_u[u] = cnt_u.get(u, 0) + 1\n    for v in groups:\n        cnt_v[v] = cnt_v.get(v, 0) + 1\n    p_u = {u: cnt_u[u] / N for u in cnt_u}\n    p_v = {v: cnt_v[v] / N for v in cnt_v}\n    joint = {}\n    for u, v in zip(labels, groups):\n        joint[(u, v)] = joint.get((u, v), 0) + 1\n    p_uv = {k: joint[k] / N for k in joint}\n    H_u = -sum(p * math.log(p + 1e-8) for p in p_u.values())\n    H_v = -sum(p * math.log(p + 1e-8) for p in p_v.values())\n    I = 0.0\n    for (u, v), p in p_uv.items():\n        I += p * (\n            math.log(p + 1e-8) - math.log(p_u[u] + 1e-8) - math.log(p_v[v] + 1e-8)\n        )\n    return I / (((H_u + H_v) / 2) + 1e-8)\n\n\n# evaluation: worst-group accuracy & loss sum\ndef evaluate(loader, model, criterion):\n    model.eval()\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            x, y, g = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, y)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for gg in (0, 1):\n                m = g == gg\n                total[gg] += m.sum().item()\n                if m.sum().item() > 0:\n                    correct[gg] += (preds[m] == y[m]).sum().item()\n    wg = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg\n\n\n# small CNN\nclass SmallCNN(nn.Module):\n    def __init__(self, in_ch, num_classes):\n        super().__init__()\n        self.hidden = nn.Sequential(\n            nn.Conv2d(in_ch, 32, 3, 1, 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, 1, 1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d(1),\n        )\n        self.classifier = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        h = self.hidden(x).view(x.size(0), -1)\n        return self.classifier(h)\n\n\n# colored spurious dataset wrapper\nclass ColoredHFImageDataset(Dataset):\n    def __init__(self, hf_ds, sp_corr, mean, std):\n        self.ds = hf_ds\n        self.sp = sp_corr\n        self.norm = T.Normalize(mean, std)\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        item = self.ds[idx]\n        img = item.get(\"image\", item.get(\"img\"))\n        label = item[\"label\"]\n        # spurious color bit\n        flip = np.random.rand() < self.sp\n        sp = (label % 2) if flip else 1 - (label % 2)\n        arr = np.array(img)\n        if arr.ndim == 2:\n            arr = np.stack([arr] * 3, 2)\n        col = np.zeros_like(arr)\n        if sp == 0:\n            col[..., 0] = arr[..., 0]\n        else:\n            col[..., 1] = arr[..., 0]\n        img2 = Image.fromarray(col)\n        x = T.ToTensor()(img2)\n        x = self.norm(x)\n        return {\n            \"features\": x,\n            \"label\": torch.tensor(label, dtype=torch.long),\n            \"group\": torch.tensor(sp, dtype=torch.long),\n            \"idx\": idx,\n        }\n\n\n# experiment across three HF datasets\ndatasets_cfg = {\n    \"mnist\": ([0.5] * 3, [0.5] * 3),\n    \"fashion_mnist\": ([0.5] * 3, [0.5] * 3),\n    \"cifar10\": ([0.5] * 3, [0.5] * 3),\n}\nspurious_corr = 0.9\nresults = {}\n\nfor name, (mean, std) in datasets_cfg.items():\n    print(f\"\\n*** DATASET {name} ***\")\n    ds = load_dataset(name)\n    split = ds[\"train\"].train_test_split(0.2, seed=0)\n    train_hf, val_hf = split[\"train\"], split[\"test\"]\n    test_hf = ds[\"test\"]\n    train_ds = ColoredHFImageDataset(train_hf, spurious_corr, mean, std)\n    val_ds = ColoredHFImageDataset(val_hf, spurious_corr, mean, std)\n    test_ds = ColoredHFImageDataset(test_hf, spurious_corr, mean, std)\n    N = len(train_ds)\n    bs = 128\n    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n\n    model = SmallCNN(\n        3,\n        (\n            len(ds[\"train\"].features[\"label\"].names)\n            if hasattr(ds[\"train\"], \"features\")\n            else len(set(ds[\"train\"][\"label\"]))\n        ),\n    ).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    warmup, train_ep = 1, 2\n    total = warmup + train_ep\n    sw = None\n    tr_wg, val_wg, nmi_list = [], [], []\n    for epoch in range(total):\n        model.train()\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            x, y, idx = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, y)\n            if epoch > warmup and sw is not None:\n                loss = (losses * sw[idx]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        if epoch == warmup:\n            sub = np.random.choice(N, min(N, 2000), replace=False)\n            subset = Subset(train_ds, sub)\n            loader = DataLoader(subset, batch_size=1, shuffle=False)\n            grads, true_g = [], []\n            for b in loader:\n                b = {\n                    k: v.to(device) for k, v in b.items() if isinstance(v, torch.Tensor)\n                }\n                optimizer.zero_grad()\n                out = model.hidden(b[\"features\"]).view(1, -1)\n                logits = model.classifier(out)\n                l = criterion(logits, b[\"label\"])\n                l.mean().backward()\n                g1 = model.classifier.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g1)\n                true_g.append(int(b[\"group\"].cpu().item()))\n            grads = np.stack(grads)\n            best = (-1, None, None)\n            for k in (2, 3):\n                lbls, cents = kmeans_np(grads, k)\n                d_intra = ((grads - cents[lbls]) ** 2).sum(1).mean()\n                d_inter = (\n                    np.mean(\n                        [\n                            ((cents[i] - cents[j]) ** 2).sum()\n                            for i in range(k)\n                            for j in range(i + 1, k)\n                        ]\n                    )\n                    if k > 1\n                    else 0\n                )\n                score = d_inter / (d_intra + 1e-8)\n                if score > best[0]:\n                    best = (score, lbls, cents)\n            _, sub_lbls, cents = best\n            cnt = np.bincount(sub_lbls, minlength=cents.shape[0])\n            sw_arr = np.ones(N, dtype=np.float32)\n            for i, lab in enumerate(sub_lbls):\n                sw_arr[sub[i]] = 1.0 / (cnt[lab] + 1e-8)\n            sw = torch.tensor(sw_arr, device=device)\n            nmi0 = compute_nmi(sub_lbls, true_g)\n\n        lval, wg = evaluate(val_loader, model, criterion)\n        val_wg.append(wg)\n        nmi_list.append(nmi0 if epoch >= warmup else 0.0)\n        print(\n            f\"[{name}] Epoch {epoch}: validation_loss = {lval/len(val_ds):.4f}, val_wg = {wg:.4f}, NMI = {nmi_list[-1]:.4f}\"\n        )\n\n    results[name] = {\"metrics\": {\"val_wg\": val_wg}, \"nmi\": nmi_list}\n\n# save data\nexperiment_data = results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Saved experiment data to {working_dir}/experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset\nimport random\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n# K-means on NumPy\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    centroids = X[rng.choice(N, n_clusters, replace=False)].copy()\n    labels = np.zeros(N, int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n        new_labels = dists.argmin(axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            centroids[k] = pts.mean(axis=0) if len(pts) > 0 else X[rng.randint(N)]\n    return labels\n\n\n# NMI computation\ndef compute_nmi(labels, true):\n    labels = np.array(labels)\n    true = np.array(true)\n    eps = 1e-10\n    N = len(labels)\n    clusters = np.unique(labels)\n    classes = np.unique(true)\n    MI = 0.0\n    for c in clusters:\n        p_x = (labels == c).sum() / N\n        for k in classes:\n            p_y = (true == k).sum() / N\n            p_xy = ((labels == c) & (true == k)).sum() / N\n            if p_xy > 0:\n                MI += p_xy * np.log(p_xy / (p_x * p_y) + eps)\n    Hx = -sum(\n        (labels == c).sum() / N * np.log((labels == c).sum() / N + eps)\n        for c in clusters\n    )\n    Hy = -sum(\n        (true == k).sum() / N * np.log((true == k).sum() / N + eps) for k in classes\n    )\n    return 2 * MI / (Hx + Hy + eps)\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\ndef evaluate(loader, model, criterion):\n    model.eval()\n    total_loss = 0.0\n    total_samples = 0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            total_loss += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                correct[g] += (preds[mask] == yb[mask]).sum().item()\n            total_samples += yb.size(0)\n    avg_loss = total_loss / total_samples\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return avg_loss, wg_acc\n\n\nclass SmallCNN(nn.Module):\n    def __init__(self, flat_dim):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(3, 16, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(16, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n        )\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(flat_dim, 64)\n        self.relu2 = nn.ReLU()\n        self.fc2 = nn.Linear(64, 2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.flatten(x)\n        x = self.relu2(self.fc1(x))\n        return self.fc2(x)\n\n\n# Experiment settings\nsp_corr_train = 0.95\nsp_corr_eval = 0.5\nwarmup_epochs = 1\ntrain_epochs = 2\ntotal_epochs = warmup_epochs + train_epochs\nbatch_size = 64\n\ndatasets_list = [\"mnist\", \"fashion_mnist\", \"cifar10\"]\nexperiment_data = {}\n\nfor name in datasets_list:\n    raw = load_dataset(name)\n    train_raw = (\n        raw[\"train\"]\n        .filter(lambda ex: ex[\"label\"] in [0, 1])\n        .shuffle(seed=0)\n        .select(range(2000))\n    )\n    test_all = (\n        raw[\"test\"]\n        .filter(lambda ex: ex[\"label\"] in [0, 1])\n        .shuffle(seed=0)\n        .select(range(1000))\n    )\n    val_raw = test_all.select(range(500))\n    test_raw = test_all.select(range(500, 1000))\n\n    def make_split(ds, corr):\n        # pick correct image column\n        img_col = \"image\" if \"image\" in ds.column_names else \"img\"\n        imgs = ds[img_col]\n        ys = ds[\"label\"]\n        X, y, g = [], [], []\n        for im, lab in zip(imgs, ys):\n            arr = np.array(im)\n            if arr.ndim == 2:\n                arr = np.stack([arr] * 3, axis=2)\n            grp = lab if random.random() < corr else 1 - lab\n            patch = 5\n            if grp == 1:\n                arr[:patch, :patch, 0] = 255\n                arr[:patch, :patch, 1:] = 0\n            else:\n                arr[:patch, :patch, 2] = 255\n                arr[:patch, :patch, :2] = 0\n            arr = arr.astype(np.float32) / 255.0\n            arr = (arr - 0.5) / 0.5\n            arr = arr.transpose(2, 0, 1)\n            X.append(arr)\n            y.append(lab)\n            g.append(grp)\n        return np.stack(X), np.array(y), np.array(g)\n\n    X_train, y_train, g_train = make_split(train_raw, sp_corr_train)\n    X_val, y_val, g_val = make_split(val_raw, sp_corr_eval)\n    X_test, y_test, g_test = make_split(test_raw, sp_corr_eval)\n\n    train_ds = CustomDataset(X_train, y_train, g_train)\n    val_ds = CustomDataset(X_val, y_val, g_val)\n    test_ds = CustomDataset(X_test, y_test, g_test)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n    cluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n    img_size = 28 if name != \"cifar10\" else 32\n    flat_dim = 32 * (img_size // 4) * (img_size // 4)\n    model = SmallCNN(flat_dim).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    sample_weights = None\n    nmi_val = np.nan\n    experiment_data[name] = {\n        \"metrics\": {\n            \"train_loss\": [],\n            \"val_loss\": [],\n            \"train_wg\": [],\n            \"val_wg\": [],\n            \"nmi\": [],\n        },\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device)\n                    for k, v in sample.items()\n                    if isinstance(v, torch.Tensor)\n                }\n                optimizer.zero_grad()\n                out = model(batch[\"features\"])\n                l = criterion(out, batch[\"label\"]).mean()\n                l.backward()\n                grads.append(model.fc2.weight.grad.detach().cpu().view(-1).numpy())\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, 2, 10)\n            nmi_val = compute_nmi(labels, g_train)\n            counts = np.bincount(labels, minlength=2)\n            sw = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw, device=device)\n\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(f\"Epoch {epoch}: validation_loss = {v_loss:.4f}\")\n        m = experiment_data[name][\"metrics\"]\n        m[\"train_loss\"].append(tr_loss)\n        m[\"val_loss\"].append(v_loss)\n        m[\"train_wg\"].append(tr_wg)\n        m[\"val_wg\"].append(v_wg)\n        m[\"nmi\"].append(nmi_val)\n\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            preds.extend(model(batch[\"features\"]).argmax(1).cpu().tolist())\n    experiment_data[name][\"predictions\"] = preds\n    experiment_data[name][\"ground_truth\"] = y_test.tolist()\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nimport numpy as np\nfrom datasets import load_dataset\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torch.nn as nn, torch.optim as optim\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, normalized_mutual_info_score\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\nclass SpuriousHFDataset(Dataset):\n    def __init__(self, hf_data, transform, sp_corr, binary_task, max_samples=None):\n        self.hf = hf_data\n        self.transform = transform\n        self.sp_corr = sp_corr\n        self.binary_task = binary_task\n        N = len(self.hf)\n        self.ids = list(range(min(max_samples if max_samples else N, N)))\n        self._precompute()\n\n    def _precompute(self):\n        ys, gs = [], []\n        for idx in self.ids:\n            lab = self.hf[idx][\"label\"]\n            yb = (lab % 2) if self.binary_task == \"even-odd\" else lab\n            ys.append(yb)\n            gs.append(yb if np.random.rand() < self.sp_corr else 1 - yb)\n        self.y = np.array(ys)\n        self.groups = np.array(gs)\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, i):\n        item = self.hf[self.ids[i]]\n        img = item[\"image\"]\n        x = self.transform(img)\n        return {\n            \"features\": x,\n            \"label\": torch.tensor(int(self.y[i]), dtype=torch.long),\n            \"group\": torch.tensor(int(self.groups[i]), dtype=torch.long),\n            \"idx\": torch.tensor(i, dtype=torch.long),\n        }\n\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, in_ch):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(in_ch, 32, 3, 1, 1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 3, 1, 1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n        )\n        self.classifier = nn.Linear(64, 2)\n\n    def forward(self, x):\n        return self.classifier(self.features(x))\n\n\ndef evaluate(loader, model, criterion):\n    model.eval()\n    loss_sum = 0.0\n    corr = {0: 0, 1: 0}\n    tot = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, y, g = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, y)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for grp in (0, 1):\n                mask = g == grp\n                tot[grp] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    corr[grp] += (preds[mask] == y[mask]).sum().item()\n    wg = min(\n        corr[0] / tot[0] if tot[0] > 0 else 0.0, corr[1] / tot[1] if tot[1] > 0 else 0.0\n    )\n    return loss_sum, wg\n\n\ndataset_names = [\"mnist\", \"fashion_mnist\", \"cifar10\"]\nexperiment_data = {}\ntrain_samples, val_samples = 5000, 1000\nsp_corr_train, sp_corr_val = 0.95, 0.5\nwarmup_epochs, total_epochs, max_k = 1, 5, 5\n\nfor name in dataset_names:\n    # load and filter binary tasks\n    ds_train = load_dataset(name, split=\"train\")\n    ds_val = load_dataset(name, split=\"test\")\n    if name != \"mnist\":\n        ds_train = ds_train.filter(lambda x: x[\"label\"] in [0, 1])\n        ds_val = ds_val.filter(lambda x: x[\"label\"] in [0, 1])\n    ds_train = ds_train.select(range(min(train_samples, len(ds_train))))\n    ds_val = ds_val.select(range(min(val_samples, len(ds_val))))\n    # transforms\n    if name in (\"mnist\", \"fashion_mnist\"):\n        in_ch = 1\n        transform = transforms.Compose([transforms.Resize(28), transforms.ToTensor()])\n    else:\n        in_ch = 3\n        transform = transforms.Compose([transforms.ToTensor()])\n    task = \"even-odd\" if name == \"mnist\" else \"binary\"\n    train_ds = SpuriousHFDataset(ds_train, transform, sp_corr_train, task)\n    val_ds = SpuriousHFDataset(ds_val, transform, sp_corr_val, task)\n    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=128, shuffle=False)\n    cluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n    # model setup\n    model = SimpleCNN(in_ch).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    sample_weights = None\n    metrics = {\n        \"loss_train\": [],\n        \"loss_val\": [],\n        \"train_wg\": [],\n        \"val_wg\": [],\n        \"nmi\": [],\n    }\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, y, idxs = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, y)\n            if sample_weights is not None:\n                w = sample_weights[idxs]\n                loss = (losses * w).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        # gradient clustering & reweight\n        if epoch >= warmup_epochs:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                sample = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out0 = model(sample[\"features\"])\n                l0 = criterion(out0, sample[\"label\"]).mean()\n                l0.backward()\n                g = model.classifier.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            sil, lbls = [], {}\n            for k in range(2, max_k + 1):\n                km = KMeans(n_clusters=k, random_state=0).fit(grads)\n                labs = km.labels_\n                sil.append(silhouette_score(grads, labs))\n                lbls[k] = labs\n            best_k = list(range(2, max_k + 1))[int(np.argmax(sil))]\n            pseudo = lbls[best_k]\n            counts = np.bincount(pseudo, minlength=best_k)\n            sw = np.array([1.0 / counts[l] for l in pseudo], dtype=np.float32)\n            sample_weights = torch.tensor(sw, device=device)\n            nmi = normalized_mutual_info_score(train_ds.groups, pseudo)\n        else:\n            nmi = np.nan\n        # evaluation\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        val_loss, val_wg = evaluate(val_loader, model, criterion)\n        avg_val_loss = val_loss / len(val_ds)\n        print(f\"Epoch {epoch}: validation_loss = {avg_val_loss:.4f}\")\n        metrics[\"loss_train\"].append(tr_loss / len(train_ds))\n        metrics[\"loss_val\"].append(val_loss / len(val_ds))\n        metrics[\"train_wg\"].append(tr_wg)\n        metrics[\"val_wg\"].append(val_wg)\n        metrics[\"nmi\"].append(nmi)\n    experiment_data[name] = metrics\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Custom metrics to avoid sklearn dependency\ndef silhouette_score_np(X, labels):\n    N = X.shape[0]\n    if N <= 1:\n        return -1.0\n    D = np.sqrt(((X[:, None, :] - X[None, :, :]) ** 2).sum(axis=2))\n    sils = []\n    for i in range(N):\n        li = labels[i]\n        same = np.where(labels == li)[0]\n        same = same[same != i]\n        if same.size == 0:\n            continue\n        a = D[i, same].mean()\n        b = np.inf\n        for lj in np.unique(labels):\n            if lj == li:\n                continue\n            other = np.where(labels == lj)[0]\n            if other.size == 0:\n                continue\n            b = min(b, D[i, other].mean())\n        sils.append((b - a) / max(a, b))\n    return float(np.mean(sils)) if sils else -1.0\n\n\ndef normalized_mutual_info_score_np(labels_true, labels_pred):\n    labels_true = np.asarray(labels_true)\n    labels_pred = np.asarray(labels_pred)\n    N = labels_true.size\n    ut = np.unique(labels_true)\n    up = np.unique(labels_pred)\n    contingency = np.zeros((ut.size, up.size), dtype=np.int64)\n    for i, t in enumerate(ut):\n        for j, p in enumerate(up):\n            contingency[i, j] = np.sum((labels_true == t) & (labels_pred == p))\n    a = contingency.sum(axis=1)\n    b = contingency.sum(axis=0)\n    nz = contingency > 0\n    MI = np.sum(\n        contingency[nz]\n        / N\n        * np.log(\n            (contingency[nz] * N)\n            / (\n                a.repeat(b.shape[0]).reshape(a.shape[0], b.shape[0])[nz]\n                * b.repeat(a.shape[0]).reshape(b.shape[0], a.shape[0]).T[nz]\n            )\n            + 1e-10\n        )\n    )\n    Ht = -np.sum((a / N) * np.log(a / N + 1e-10))\n    Hp = -np.sum((b / N) * np.log(b / N + 1e-10))\n    return float(MI / ((Ht + Hp) / 2 + 1e-10))\n\n\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    centroids = X[rng.choice(N, n_clusters, replace=False)].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(-1)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if pts.shape[0] > 0:\n                centroids[k] = pts.mean(0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid, num_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, num_classes)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\nclass SpuriousDataset(Dataset):\n    def __init__(self, feats, labels, groups):\n        self.X = torch.tensor(feats, dtype=torch.float32)\n        self.y = torch.tensor(labels, dtype=torch.long)\n        self.g = torch.tensor(groups, dtype=torch.long)\n\n    def __len__(self):\n        return self.y.size(0)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx], self.g[idx], idx\n\n\ndef evaluate(loader, model, criterion):\n    model.eval()\n    loss_sum = 0.0\n    correct = {}\n    total = {}\n    with torch.no_grad():\n        for Xb, yb, gb, _ in loader:\n            Xb, yb, gb = Xb.to(device), yb.to(device), gb.to(device)\n            out = model(Xb)\n            l = criterion(out, yb).sum().item()\n            loss_sum += l\n            preds = out.argmax(1)\n            for g in torch.unique(gb).tolist():\n                mask = gb == g\n                total[g] = total.get(g, 0) + mask.sum().item()\n                correct[g] = correct.get(g, 0) + (preds[mask] == yb[mask]).sum().item()\n    wg = min(correct[g] / total[g] for g in total) if total else 0.0\n    return loss_sum, wg\n\n\nexperiment_data = {}\nfor ds_name in (\"mnist\", \"fashion_mnist\", \"cifar10\"):\n    ds_full = load_dataset(ds_name)\n    ds_tr = ds_full[\"train\"].filter(lambda x: x[\"label\"] < 2).shuffle(0)\n    ds_te = ds_full[\"test\"].filter(lambda x: x[\"label\"] < 2).shuffle(0)\n    n_tr, n_val, n_te = 2000, 500, 500\n    ds_tr = ds_tr.select(range(min(len(ds_tr), n_tr + n_val)))\n    ds_train = ds_tr.select(range(min(len(ds_tr), n_tr)))\n    ds_val = ds_tr.select(range(min(len(ds_tr), n_tr), min(len(ds_tr), n_tr + n_val)))\n    ds_test = ds_te.select(range(min(len(ds_te), n_te)))\n\n    def build(ds, p):\n        feats, labs, grs = [], [], []\n        for ex in ds:\n            img = np.array(ex[\"image\"], dtype=np.float32).flatten() / 255.0\n            y = ex[\"label\"]\n            sp = y if (np.random.rand() < p) else 1 - y\n            feats.append(np.concatenate([img, [sp]]))\n            labs.append(y)\n            grs.append(sp)\n        return np.stack(feats), np.array(labs), np.array(grs)\n\n    Xtr, ytr, gtr = build(ds_train, 0.9)\n    Xval, yval, gval = build(ds_val, 0.5)\n    Xte, yte, gte = build(ds_test, 0.5)\n\n    train_ds = SpuriousDataset(Xtr, ytr, gtr)\n    val_ds = SpuriousDataset(Xval, yval, gval)\n    test_ds = SpuriousDataset(Xte, yte, gte)\n\n    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n    cluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n    model = MLP(Xtr.shape[1], 128, 2).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    sample_weights = None\n\n    train_wg, val_wg = [], []\n    train_ls, val_ls = [], []\n    nmi_list, silh_list = [], []\n\n    total_epochs = 4\n    for epoch in range(total_epochs):\n        model.train()\n        for Xb, yb, gb, idb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            idb = idb.to(device)\n            out = model(Xb)\n            losses = criterion(out, yb)\n            if sample_weights is not None:\n                w = sample_weights[idb]\n                loss = (losses * w).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(\n            f\"{ds_name} Epoch {epoch}: validation_loss = {v_loss/len(val_ds):.4f}, val_wg = {v_wg:.4f}\"\n        )\n        train_wg.append(tr_wg)\n        val_wg.append(v_wg)\n        train_ls.append(tr_loss / len(train_ds))\n        val_ls.append(v_loss / len(val_ds))\n\n        # gradient clustering\n        model.eval()\n        grads = []\n        for Xb, yb, _, _ in cluster_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            l = criterion(out, yb).mean()\n            l.backward()\n            grads.append(model.fc.weight.grad.detach().cpu().numpy().ravel())\n        grads = np.stack(grads)\n\n        best_k, best_s, best_lbl = None, -1, None\n        for k in (2, 3, 4, 5):\n            if k <= len(grads):\n                lbl = kmeans_np(grads, n_clusters=k, n_iters=20)\n                counts = np.bincount(lbl, minlength=k)\n                if np.min(counts) < 2:\n                    continue\n                s_score = silhouette_score_np(grads, lbl)\n                if s_score > best_s:\n                    best_s, best_k, best_lbl = s_score, k, lbl\n        if best_lbl is None:\n            best_lbl = kmeans_np(grads, n_clusters=2, n_iters=10)\n            best_s = silhouette_score_np(grads, best_lbl)\n            best_k = 2\n\n        counts = np.bincount(best_lbl, minlength=best_k)\n        w_arr = np.array([1.0 / counts[l] for l in best_lbl], dtype=np.float32)\n        sample_weights = torch.tensor(w_arr, device=device)\n\n        nmi = normalized_mutual_info_score_np(train_ds.g.numpy(), best_lbl)\n        nmi_list.append(nmi)\n        silh_list.append(best_s)\n\n    # final test\n    preds, truths = [], []\n    model.eval()\n    with torch.no_grad():\n        for Xb, yb, _, _ in test_loader:\n            Xb = Xb.to(device)\n            out = model(Xb).argmax(1).cpu().numpy()\n            preds.extend(out.tolist())\n            truths.extend(yb.numpy().tolist())\n\n    experiment_data[ds_name] = {\n        \"metrics\": {\n            \"train\": np.array(train_wg),\n            \"val\": np.array(val_wg),\n            \"nmi\": np.array(nmi_list),\n            \"silhouette\": np.array(silh_list),\n        },\n        \"losses\": {\n            \"train\": np.array(train_ls),\n            \"val\": np.array(val_ls),\n        },\n        \"predictions\": np.array(preds),\n        \"ground_truth\": np.array(truths),\n    }\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom PIL import Image\nfrom torchvision import transforms, models\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import normalized_mutual_info_score, silhouette_score\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\nclass HFImageDataset(Dataset):\n    def __init__(self, hf_ds, transform):\n        self.ds = hf_ds\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        item = self.ds[idx]\n        img = item[\"image\"]\n        if isinstance(img, np.ndarray):\n            img = Image.fromarray(img)\n        img = self.transform(img)\n        return {\n            \"features\": img,\n            \"label\": torch.tensor(item[\"label\"], dtype=torch.long),\n            \"group\": torch.tensor(item[\"group\"], dtype=torch.long),\n            \"idx\": torch.tensor(idx, dtype=torch.long),\n        }\n\n\ndef evaluate(loader, model, criterion):\n    model.eval()\n    loss_sum, group_correct, group_total = 0.0, {}, {}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, y, g = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, y)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for gid in g.unique().tolist():\n                mask = g == gid\n                group_total[gid] = group_total.get(gid, 0) + mask.sum().item()\n                group_correct[gid] = (\n                    group_correct.get(gid, 0) + (preds[mask] == y[mask]).sum().item()\n                )\n    wg_acc = min(group_correct[gid] / group_total[gid] for gid in group_total)\n    return loss_sum, wg_acc\n\n\nexperiment_data = {}\nfor dataset_name in [\"mnist\", \"nateraw/Waterbirds\", \"celebA\"]:\n    print(f\"\\n=== Dataset: {dataset_name} ===\")\n    if dataset_name == \"mnist\":\n        ds = load_dataset(\"mnist\")\n        ds[\"train\"] = ds[\"train\"].filter(lambda x: x[\"label\"] < 2)\n        ds[\"test\"] = ds[\"test\"].filter(lambda x: x[\"label\"] < 2)\n\n        def add_color(ex, corr):\n            img = np.array(ex[\"image\"])\n            lab = ex[\"label\"]\n            r = np.random.rand()\n            color = (\n                \"red\"\n                if (r < corr and lab == 1) or (r >= corr and lab == 0)\n                else \"green\"\n            )\n            arr = np.zeros((28, 28, 3), dtype=np.uint8)\n            if color == \"red\":\n                arr[..., 0] = img\n            else:\n                arr[..., 2] = img\n            ex[\"image\"], ex[\"group\"] = arr, (1 if color == \"red\" else 0)\n            return ex\n\n        train_hf = ds[\"train\"].map(\n            lambda ex: add_color(ex, 0.95), remove_columns=[\"image\"]\n        )\n        tmp = (\n            ds[\"test\"]\n            .map(lambda ex: add_color(ex, 0.5), remove_columns=[\"image\"])\n            .shuffle(seed=0)\n        )\n        val_hf = tmp.select(range(500))\n        test_hf = tmp.select(range(500, 1000))\n        transform = transforms.Compose(\n            [\n                transforms.Resize((32, 32)),\n                transforms.ToTensor(),\n                transforms.Normalize([0.5] * 3, [0.5] * 3),\n            ]\n        )\n    else:\n        hf = load_dataset(dataset_name)\n        if dataset_name == \"nateraw/Waterbirds\":\n\n            def add_group_wb(ex):\n                ex[\"group\"] = ex[\"label\"] * 2 + ex[\"place\"]\n                return ex\n\n            train_hf = hf[\"train\"].map(add_group_wb)\n            val_hf = hf[\"validation\"].map(add_group_wb)\n            test_hf = hf[\"test\"].map(add_group_wb)\n        else:  # celebA\n\n            def proc_cb(ex):\n                lab = 1 if ex[\"attributes\"][\"Blond_Hair\"] == 1 else 0\n                male = 1 if ex[\"attributes\"][\"Male\"] == 1 else 0\n                ex[\"label\"], ex[\"group\"] = lab, lab * 2 + male\n                return ex\n\n            train_hf = hf[\"train\"].map(proc_cb)\n            val_hf = hf[\"validation\"].map(proc_cb)\n            test_hf = hf[\"test\"].map(proc_cb)\n        transform = transforms.Compose(\n            [\n                transforms.Resize((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n            ]\n        )\n\n    # subsample for speed\n    train_hf = train_hf.shuffle(seed=0).select(range(2000))\n    val_hf = val_hf.shuffle(seed=0).select(range(500))\n    test_hf = test_hf.shuffle(seed=0).select(range(500))\n\n    train_ds, val_ds, test_ds = (\n        HFImageDataset(train_hf, transform),\n        HFImageDataset(val_hf, transform),\n        HFImageDataset(test_hf, transform),\n    )\n    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=64)\n    test_loader = DataLoader(test_ds, batch_size=64)\n    cluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n    # model init\n    if dataset_name == \"mnist\":\n        model = nn.Sequential(\n            nn.Conv2d(3, 16, 3, 1, 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(16, 32, 3, 1, 1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(32 * 16 * 16, 64),\n            nn.ReLU(),\n            nn.Linear(64, 2),\n        )\n    else:\n        model = models.resnet18(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, 2)\n    model, criterion = model.to(device), nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # UGC + silhouette\u2010driven k\n    warmup, epochs, k_max = 1, 4, 5\n    experiment_data[dataset_name] = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"nmi\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    sample_weights, labels, true_groups = None, None, None\n\n    for epoch in range(epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, y, idx = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, y)\n            loss = (\n                (losses * sample_weights[idx]).mean()\n                if sample_weights is not None\n                else losses.mean()\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        if epoch == warmup - 1:\n            model.eval()\n            grads, idxs, true_groups = [], [], []\n            for batch in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                }\n                x, y, g, idx = (\n                    batch[\"features\"],\n                    batch[\"label\"],\n                    batch[\"group\"],\n                    batch[\"idx\"],\n                )\n                i = idx.item()\n                idxs.append(i)\n                true_groups.append(int(g.item()))\n                optimizer.zero_grad()\n                out = model(x)\n                li = criterion(out, y).mean()\n                li.backward()\n                last = model.fc if hasattr(model, \"fc\") else model[-1]\n                grads.append(last.weight.grad.detach().cpu().numpy().flatten())\n            grads = np.stack(grads)\n            comp = PCA(n_components=min(50, grads.shape[1])).fit_transform(grads)\n            best_k, best_sc = 2, -1\n            for k in range(2, k_max + 1):\n                labs = KMeans(n_clusters=k, random_state=0).fit_predict(comp)\n                if len(set(labs)) < 2:\n                    continue\n                sc = silhouette_score(comp, labs)\n                if sc > best_sc:\n                    best_sc, best_k = sc, k\n            labels = KMeans(n_clusters=best_k, random_state=0).fit_predict(comp)\n            counts = np.bincount(labels, minlength=best_k)\n            sw = np.array([1.0 / counts[l] for l in labels], dtype=np.float32)\n            sw_tensor = torch.zeros(len(train_ds), dtype=torch.float32, device=device)\n            for pos, i in enumerate(idxs):\n                sw_tensor[i] = sw[pos]\n            sample_weights = sw_tensor\n\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        nmi = (\n            normalized_mutual_info_score(true_groups, labels)\n            if labels is not None\n            else 0.0\n        )\n        print(\n            f\"{dataset_name} epoch {epoch}: val_loss={v_loss:.4f}, val_wg={v_wg:.4f}, NMI={nmi:.4f}\"\n        )\n        experiment_data[dataset_name][\"losses\"][\"train\"].append(tr_loss / len(train_ds))\n        experiment_data[dataset_name][\"losses\"][\"val\"].append(v_loss / len(val_ds))\n        experiment_data[dataset_name][\"metrics\"][\"train\"].append(tr_wg)\n        experiment_data[dataset_name][\"metrics\"][\"val\"].append(v_wg)\n        experiment_data[dataset_name][\"nmi\"].append(nmi)\n\n    test_loss, test_wg = evaluate(test_loader, model, criterion)\n    print(f\"{dataset_name} test_wg={test_wg:.4f}\")\n    preds, truths = [], []\n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n    experiment_data[dataset_name][\"predictions\"] = np.array(preds)\n    experiment_data[dataset_name][\"ground_truth\"] = np.array(truths)\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.transforms as T\nfrom datasets import load_dataset\nimport math\n\n# setup working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# simple k-means + centroids\ndef kmeans_np(X, n_clusters, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    idx = rng.choice(N, n_clusters, replace=False)\n    centroids = X[idx].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n        new = dists.argmin(axis=1)\n        if np.all(new == labels):\n            break\n        labels = new\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels, centroids\n\n\n# compute NMI\ndef compute_nmi(labels, groups):\n    N = len(labels)\n    cnt_u = {}\n    cnt_v = {}\n    for u in labels:\n        cnt_u[u] = cnt_u.get(u, 0) + 1\n    for v in groups:\n        cnt_v[v] = cnt_v.get(v, 0) + 1\n    p_u = {u: cnt_u[u] / N for u in cnt_u}\n    p_v = {v: cnt_v[v] / N for v in cnt_v}\n    joint = {}\n    for u, v in zip(labels, groups):\n        joint[(u, v)] = joint.get((u, v), 0) + 1\n    p_uv = {k: joint[k] / N for k in joint}\n    H_u = -sum(p * math.log(p + 1e-8) for p in p_u.values())\n    H_v = -sum(p * math.log(p + 1e-8) for p in p_v.values())\n    I = 0.0\n    for (u, v), p in p_uv.items():\n        I += p * (\n            math.log(p + 1e-8) - math.log(p_u[u] + 1e-8) - math.log(p_v[v] + 1e-8)\n        )\n    return I / (((H_u + H_v) / 2) + 1e-8)\n\n\n# evaluation: worst-group accuracy & loss sum\ndef evaluate(loader, model, criterion):\n    model.eval()\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            x, y, g = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, y)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for gg in (0, 1):\n                m = g == gg\n                total[gg] += m.sum().item()\n                if m.sum().item() > 0:\n                    correct[gg] += (preds[m] == y[m]).sum().item()\n    wg = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg\n\n\n# small CNN\nclass SmallCNN(nn.Module):\n    def __init__(self, in_ch, num_classes):\n        super().__init__()\n        self.hidden = nn.Sequential(\n            nn.Conv2d(in_ch, 32, 3, 1, 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, 1, 1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d(1),\n        )\n        self.classifier = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        h = self.hidden(x).view(x.size(0), -1)\n        return self.classifier(h)\n\n\n# colored spurious dataset wrapper\nclass ColoredHFImageDataset(Dataset):\n    def __init__(self, hf_ds, sp_corr, mean, std):\n        self.ds = hf_ds\n        self.sp = sp_corr\n        self.norm = T.Normalize(mean, std)\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        idx = int(idx)\n        item = self.ds[idx]\n        img = item.get(\"image\", item.get(\"img\"))\n        label = item[\"label\"]\n        flip = np.random.rand() < self.sp\n        sp = (label % 2) if flip else 1 - (label % 2)\n        arr = np.array(img)\n        if arr.ndim == 2:\n            arr = np.stack([arr] * 3, 2)\n        col = np.zeros_like(arr)\n        if sp == 0:\n            col[..., 0] = arr[..., 0]\n        else:\n            col[..., 1] = arr[..., 0]\n        img2 = Image.fromarray(col)\n        x = T.ToTensor()(img2)\n        x = self.norm(x)\n        return {\n            \"features\": x,\n            \"label\": torch.tensor(label, dtype=torch.long),\n            \"group\": torch.tensor(sp, dtype=torch.long),\n            \"idx\": torch.tensor(idx, dtype=torch.long),\n        }\n\n\n# experiment across three HF datasets\ndatasets_cfg = {\n    \"mnist\": ([0.5] * 3, [0.5] * 3),\n    \"fashion_mnist\": ([0.5] * 3, [0.5] * 3),\n    \"cifar10\": ([0.5] * 3, [0.5] * 3),\n}\nspurious_corr = 0.9\nresults = {}\n\nfor name, (mean, std) in datasets_cfg.items():\n    print(f\"\\n*** DATASET {name} ***\")\n    ds = load_dataset(name)\n    split = ds[\"train\"].train_test_split(0.2, seed=0)\n    train_hf, val_hf = split[\"train\"], split[\"test\"]\n    test_hf = ds[\"test\"]\n    train_ds = ColoredHFImageDataset(train_hf, spurious_corr, mean, std)\n    val_ds = ColoredHFImageDataset(val_hf, spurious_corr, mean, std)\n    test_ds = ColoredHFImageDataset(test_hf, spurious_corr, mean, std)\n    N = len(train_ds)\n    bs = 128\n    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n\n    model = SmallCNN(\n        3,\n        (\n            len(ds[\"train\"].features[\"label\"].names)\n            if hasattr(ds[\"train\"], \"features\")\n            else len(set(ds[\"train\"][\"label\"]))\n        ),\n    ).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    warmup, train_ep = 1, 2\n    total = warmup + train_ep\n    sw = None\n    val_wg, nmi_list = [], []\n    for epoch in range(total):\n        model.train()\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            x, y, idx = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, y)\n            if epoch > warmup and sw is not None:\n                loss = (losses * sw[idx]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        if epoch == warmup:\n            sub = np.random.choice(N, min(N, 2000), replace=False).tolist()\n            subset = Subset(train_ds, sub)\n            loader = DataLoader(subset, batch_size=1, shuffle=False)\n            grads, true_g = [], []\n            for b in loader:\n                b = {\n                    k: v.to(device) for k, v in b.items() if isinstance(v, torch.Tensor)\n                }\n                optimizer.zero_grad()\n                out = model.hidden(b[\"features\"]).view(1, -1)\n                logits = model.classifier(out)\n                l = criterion(logits, b[\"label\"])\n                l.mean().backward()\n                g1 = model.classifier.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g1)\n                true_g.append(int(b[\"group\"].cpu().item()))\n            grads = np.stack(grads)\n            best = (-1, None, None)\n            for k in (2, 3):\n                lbls, cents = kmeans_np(grads, k)\n                d_intra = ((grads - cents[lbls]) ** 2).sum(1).mean()\n                d_inter = (\n                    np.mean(\n                        [\n                            ((cents[i] - cents[j]) ** 2).sum()\n                            for i in range(k)\n                            for j in range(i + 1, k)\n                        ]\n                    )\n                    if k > 1\n                    else 0\n                )\n                score = d_inter / (d_intra + 1e-8)\n                if score > best[0]:\n                    best = (score, lbls, cents)\n            _, sub_lbls, cents = best\n            cnt = np.bincount(sub_lbls, minlength=cents.shape[0])\n            sw_arr = np.ones(N, dtype=np.float32)\n            for i, lab in enumerate(sub_lbls):\n                sw_arr[sub[i]] = 1.0 / (cnt[lab] + 1e-8)\n            sw = torch.tensor(sw_arr, device=device)\n            nmi0 = compute_nmi(sub_lbls, true_g)\n\n        lval, wg = evaluate(val_loader, model, criterion)\n        val_wg.append(wg)\n        nmi_list.append(nmi0 if epoch >= warmup else 0.0)\n        print(\n            f\"[{name}] Epoch {epoch}: validation_loss = {lval/len(val_ds):.4f}, val_wg = {wg:.4f}, NMI = {nmi_list[-1]:.4f}\"\n        )\n\n    results[name] = {\"metrics\": {\"val_wg\": val_wg}, \"nmi\": nmi_list}\n\n# save data\nexperiment_data = results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Saved experiment data to {working_dir}/experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# k-means on CPU\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper: returns (sum_loss, worst-group accuracy)\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameter sweep over learning rates\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# Containers for results\nmetrics_train_all = []\nmetrics_val_all = []\nlosses_train_all = []\nlosses_val_all = []\npredictions_list = []\nground_truth = None\n\nfor lr in lrs:\n    print(f\"\\n=== Training with learning rate = {lr} ===\")\n    # re-init model, loss, optimizer\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    # per-epoch logs\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # cluster-based reweighting after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out_i = model(batch[\"features\"])\n                loss_i = criterion(out_i, batch[\"label\"]).mean()\n                loss_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # evaluate after epoch\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(\n            f\"lr={lr} epoch={epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n        )\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # final test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n\n    preds_npy = np.array(preds)\n    truths_npy = np.array(truths)\n    predictions_list.append(preds_npy)\n    if ground_truth is None:\n        ground_truth = truths_npy\n\n    metrics_train_all.append(m_tr)\n    metrics_val_all.append(m_val)\n    losses_train_all.append(l_tr)\n    losses_val_all.append(l_val)\n\n# Assemble and save experiment data\nexperiment_data = {\n    \"learning_rate\": {\n        \"synthetic\": {\n            \"lrs\": np.array(lrs),\n            \"metrics\": {\n                \"train\": np.array(metrics_train_all),\n                \"val\": np.array(metrics_val_all),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_all),\n                \"val\": np.array(losses_val_all),\n            },\n            \"predictions\": np.stack(predictions_list),\n            \"ground_truth\": ground_truth,\n        }\n    }\n}\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# k-means on CPU\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper: returns (sum_loss, worst-group accuracy)\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameter sweep over learning rates\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# Containers for results\nmetrics_train_all = []\nmetrics_val_all = []\nlosses_train_all = []\nlosses_val_all = []\npredictions_list = []\nground_truth = None\n\nfor lr in lrs:\n    print(f\"\\n=== Training with learning rate = {lr} ===\")\n    # re-init model, loss, optimizer\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    # per-epoch logs\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # cluster-based reweighting after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out_i = model(batch[\"features\"])\n                loss_i = criterion(out_i, batch[\"label\"]).mean()\n                loss_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # evaluate after epoch\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(\n            f\"lr={lr} epoch={epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n        )\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # final test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n\n    preds_npy = np.array(preds)\n    truths_npy = np.array(truths)\n    predictions_list.append(preds_npy)\n    if ground_truth is None:\n        ground_truth = truths_npy\n\n    metrics_train_all.append(m_tr)\n    metrics_val_all.append(m_val)\n    losses_train_all.append(l_tr)\n    losses_val_all.append(l_val)\n\n# Assemble and save experiment data\nexperiment_data = {\n    \"learning_rate\": {\n        \"synthetic\": {\n            \"lrs\": np.array(lrs),\n            \"metrics\": {\n                \"train\": np.array(metrics_train_all),\n                \"val\": np.array(metrics_val_all),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_all),\n                \"val\": np.array(losses_val_all),\n            },\n            \"predictions\": np.stack(predictions_list),\n            \"ground_truth\": ground_truth,\n        }\n    }\n}\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# k-means on CPU\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper: returns (sum_loss, worst-group accuracy)\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameter sweep over learning rates\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# Containers for results\nmetrics_train_all = []\nmetrics_val_all = []\nlosses_train_all = []\nlosses_val_all = []\npredictions_list = []\nground_truth = None\n\nfor lr in lrs:\n    print(f\"\\n=== Training with learning rate = {lr} ===\")\n    # re-init model, loss, optimizer\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    # per-epoch logs\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # cluster-based reweighting after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out_i = model(batch[\"features\"])\n                loss_i = criterion(out_i, batch[\"label\"]).mean()\n                loss_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # evaluate after epoch\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(\n            f\"lr={lr} epoch={epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n        )\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # final test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n\n    preds_npy = np.array(preds)\n    truths_npy = np.array(truths)\n    predictions_list.append(preds_npy)\n    if ground_truth is None:\n        ground_truth = truths_npy\n\n    metrics_train_all.append(m_tr)\n    metrics_val_all.append(m_val)\n    losses_train_all.append(l_tr)\n    losses_val_all.append(l_val)\n\n# Assemble and save experiment data\nexperiment_data = {\n    \"learning_rate\": {\n        \"synthetic\": {\n            \"lrs\": np.array(lrs),\n            \"metrics\": {\n                \"train\": np.array(metrics_train_all),\n                \"val\": np.array(metrics_val_all),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_all),\n                \"val\": np.array(losses_val_all),\n            },\n            \"predictions\": np.stack(predictions_list),\n            \"ground_truth\": ground_truth,\n        }\n    }\n}\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\n=== Training with learning rate = 0.0001 ===',\n'\\n', 'lr=0.0001 epoch=0: val_loss=0.6977, val_wg=0.3168', '\\n', 'lr=0.0001\nepoch=1: val_loss=0.6917, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=2:\nval_loss=0.6904, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=3: val_loss=0.6900,\nval_wg=0.3588', '\\n', 'lr=0.0001 epoch=4: val_loss=0.6899, val_wg=0.3588', '\\n',\n'lr=0.0001 epoch=5: val_loss=0.6898, val_wg=0.3588', '\\n', '\\n=== Training with\nlearning rate = 0.001 ===', '\\n', 'lr=0.001 epoch=0: val_loss=0.5822,\nval_wg=0.8866', '\\n', 'lr=0.001 epoch=1: val_loss=0.5093, val_wg=0.9538', '\\n',\n'lr=0.001 epoch=2: val_loss=0.4929, val_wg=0.9622', '\\n', 'lr=0.001 epoch=3:\nval_loss=0.4887, val_wg=0.9622', '\\n', 'lr=0.001 epoch=4: val_loss=0.4872,\nval_wg=0.9622', '\\n', 'lr=0.001 epoch=5: val_loss=0.4860, val_wg=0.9622', '\\n',\n'\\n=== Training with learning rate = 0.01 ===', '\\n', 'lr=0.01 epoch=0:\nval_loss=0.0228, val_wg=0.9885', '\\n', 'lr=0.01 epoch=1: val_loss=0.0305,\nval_wg=0.9885', '\\n', 'lr=0.01 epoch=2: val_loss=0.0348, val_wg=0.9885', '\\n',\n'lr=0.01 epoch=3: val_loss=0.0380, val_wg=0.9885', '\\n', 'lr=0.01 epoch=4:\nval_loss=0.0405, val_wg=0.9885', '\\n', 'lr=0.01 epoch=5: val_loss=0.0430,\nval_wg=0.9924', '\\n', 'Execution time: 6 seconds seconds (time limit is an\nhour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 11, in\n<module>\\n    from sklearn.decomposition import PCA\\nModuleNotFoundError: No\nmodule named \\'sklearn\\'\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 10, in\n<module>\\n    from sklearn.metrics import silhouette_score,\nnormalized_mutual_info_score\\nModuleNotFoundError: No module named\n\\'sklearn\\'\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 12, in\n<module>\\n    from sklearn.cluster import KMeans\\nModuleNotFoundError: No module\nnamed \\'sklearn\\'\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', '\\rFilter:   0%|          | 0/60000 [00:00<?, ?\nexamples/s]', '\\rFilter:   5%|5         | 3000/60000 [00:00<00:02, 20148.55\nexamples/s]', '\\rFilter:  12%|#1        | 7000/60000 [00:00<00:02, 22337.11\nexamples/s]', '\\rFilter:  17%|#6        | 10000/60000 [00:00<00:03, 14523.04\nexamples/s]', '\\rFilter:  23%|##3       | 14000/60000 [00:00<00:02, 17476.53\nexamples/s]', '\\rFilter:  30%|###       | 18000/60000 [00:00<00:02, 19483.81\nexamples/s]', '\\rFilter:  37%|###6      | 22000/60000 [00:01<00:01, 20800.51\nexamples/s]', '\\rFilter:  43%|####3     | 26000/60000 [00:01<00:01, 21705.99\nexamples/s]', '\\rFilter:  50%|#####     | 30000/60000 [00:01<00:01, 22349.76\nexamples/s]', '\\rFilter:  57%|#####6    | 34000/60000 [00:01<00:01, 22789.46\nexamples/s]', '\\rFilter:  63%|######3   | 38000/60000 [00:01<00:00, 23067.30\nexamples/s]', '\\rFilter:  70%|#######   | 42000/60000 [00:01<00:00, 23282.09\nexamples/s]', '\\rFilter:  77%|#######6  | 46000/60000 [00:02<00:00, 19364.91\nexamples/s]', '\\rFilter:  83%|########3 | 50000/60000 [00:02<00:00, 20508.49\nexamples/s]', '\\rFilter:  90%|######### | 54000/60000 [00:02<00:00, 21402.93\nexamples/s]', '\\rFilter:  97%|#########6| 58000/60000 [00:02<00:00, 22075.93\nexamples/s]', '', '\\rFilter: 100%|##########| 60000/60000 [00:02<00:00, 21048.30\nexamples/s]', '\\n', '\\rFilter:   0%|          | 0/10000 [00:00<?, ?\nexamples/s]', '\\rFilter:  40%|####      | 4000/10000 [00:00<00:00, 23540.96\nexamples/s]', '\\rFilter:  80%|########  | 8000/10000 [00:00<00:00, 23613.56\nexamples/s]', '', '\\rFilter: 100%|##########| 10000/10000 [00:00<00:00, 23515.76\nexamples/s]', '\\n', 'mnist Epoch 0: validation_loss = 0.0049, val_wg = 0.9960',\n'\\n', 'mnist Epoch 1: validation_loss = 0.0022, val_wg = 0.9960', '\\n', 'mnist\nEpoch 2: validation_loss = 0.0060, val_wg = 0.9960', '\\n', 'mnist Epoch 3:\nvalidation_loss = 0.0020, val_wg = 1.0000', '\\n', '\\rFilter:   0%|          |\n0/60000 [00:00<?, ? examples/s]', '\\rFilter:   5%|5         | 3000/60000\n[00:00<00:03, 18683.17 examples/s]', '\\rFilter:  12%|#1        | 7000/60000\n[00:00<00:02, 20003.98 examples/s]', '\\rFilter:  18%|#8        | 11000/60000\n[00:00<00:02, 20351.25 examples/s]', '\\rFilter:  25%|##5       | 15000/60000\n[00:00<00:02, 20509.27 examples/s]', '\\rFilter:  30%|###       | 18000/60000\n[00:01<00:02, 15012.24 examples/s]', '\\rFilter:  37%|###6      | 22000/60000\n[00:01<00:02, 16670.93 examples/s]', '\\rFilter:  43%|####3     | 26000/60000\n[00:01<00:01, 17866.15 examples/s]', '\\rFilter:  50%|#####     | 30000/60000\n[00:01<00:01, 18703.21 examples/s]', '\\rFilter:  57%|#####6    | 34000/60000\n[00:01<00:01, 19302.26 examples/s]', '\\rFilter:  63%|######3   | 38000/60000\n[00:02<00:01, 19711.45 examples/s]', '\\rFilter:  70%|#######   | 42000/60000\n[00:02<00:00, 19969.27 examples/s]', '\\rFilter:  77%|#######6  | 46000/60000\n[00:02<00:00, 20184.55 examples/s]', '\\rFilter:  83%|########3 | 50000/60000\n[00:02<00:00, 20339.23 examples/s]', '\\rFilter:  90%|######### | 54000/60000\n[00:02<00:00, 20441.70 examples/s]', '\\rFilter:  97%|#########6| 58000/60000\n[00:02<00:00, 20504.18 examples/s]', '', '\\rFilter: 100%|##########| 60000/60000\n[00:03<00:00, 18508.85 examples/s]', '\\n', '\\rFilter:   0%|          | 0/10000\n[00:00<?, ? examples/s]', '\\rFilter:  40%|####      | 4000/10000 [00:00<00:00,\n20627.38 examples/s]', '\\rFilter:  80%|########  | 8000/10000 [00:00<00:00,\n20703.86 examples/s]', '', '\\rFilter: 100%|##########| 10000/10000 [00:00<00:00,\n20660.77 examples/s]', '\\n', 'fashion_mnist Epoch 0: validation_loss = 0.1016,\nval_wg = 0.9614', '\\n', 'fashion_mnist Epoch 1: validation_loss = 0.7944, val_wg\n= 0.6432', '\\n', 'fashion_mnist Epoch 2: validation_loss = 0.3826, val_wg =\n0.8465', '\\n', 'fashion_mnist Epoch 3: validation_loss = 0.1784, val_wg =\n0.9305', '\\n', 'Traceback (most recent call last):\\n  File \"runfile.py\", line\n148, in <module>\\n    ds_full = load_dataset(ds_name)\\n\n^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1652, in dataset_module_factory\\n    raise e1\nfrom None\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1578, in dataset_module_factory\\n    raise\nDatasetNotFoundError(f\"Dataset \\'{path}\\' doesn\\'t exist on the Hub or cannot be\naccessed.\") from e\\ndatasets.exceptions.DatasetNotFoundError: Dataset \\'kmnist\\'\ndoesn\\'t exist on the Hub or cannot be accessed.\\n', 'Execution time: a minute\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\rFilter:   0%|          | 0/60000 [00:00<?, ?\nexamples/s]', '\\rFilter:   5%|5         | 3000/60000 [00:00<00:02, 19957.89\nexamples/s]', '\\rFilter:  10%|#         | 6000/60000 [00:00<00:04, 11245.69\nexamples/s]', '\\rFilter:  17%|#6        | 10000/60000 [00:00<00:03, 15219.99\nexamples/s]', '\\rFilter:  23%|##3       | 14000/60000 [00:00<00:02, 17610.92\nexamples/s]', '\\rFilter:  30%|###       | 18000/60000 [00:01<00:02, 19166.95\nexamples/s]', '\\rFilter:  37%|###6      | 22000/60000 [00:01<00:01, 20159.66\nexamples/s]', '\\rFilter:  43%|####3     | 26000/60000 [00:01<00:01, 20837.56\nexamples/s]', '\\rFilter:  50%|#####     | 30000/60000 [00:01<00:01, 21302.82\nexamples/s]', '\\rFilter:  57%|#####6    | 34000/60000 [00:01<00:01, 21663.70\nexamples/s]', '\\rFilter:  62%|######1   | 37000/60000 [00:02<00:01, 17519.52\nexamples/s]', '\\rFilter:  68%|######8   | 41000/60000 [00:02<00:00, 19008.92\nexamples/s]', '\\rFilter:  75%|#######5  | 45000/60000 [00:02<00:00, 20202.00\nexamples/s]', '\\rFilter:  82%|########1 | 49000/60000 [00:02<00:00, 21100.62\nexamples/s]', '\\rFilter:  88%|########8 | 53000/60000 [00:02<00:00, 21768.53\nexamples/s]', '\\rFilter:  95%|#########5| 57000/60000 [00:02<00:00, 22275.85\nexamples/s]', '\\rFilter: 100%|##########| 60000/60000 [00:03<00:00, 22441.16\nexamples/s]', '', '\\rFilter: 100%|##########| 60000/60000 [00:03<00:00, 19937.87\nexamples/s]', '\\n', '\\rFilter:   0%|          | 0/10000 [00:00<?, ?\nexamples/s]', '\\rFilter:  40%|####      | 4000/10000 [00:00<00:00, 23682.28\nexamples/s]', '\\rFilter:  80%|########  | 8000/10000 [00:00<00:00, 16208.85\nexamples/s]', '', '\\rFilter: 100%|##########| 10000/10000 [00:00<00:00, 17916.54\nexamples/s]', '\\n', 'Epoch 0: validation_loss = 0.0151', '\\n', 'Epoch 1:\nvalidation_loss = 0.0099', '\\n', 'Epoch 2: validation_loss = 0.0427', '\\n',\n'\\rFilter:   0%|          | 0/60000 [00:00<?, ? examples/s]', '\\rFilter:   5%|5\n| 3000/60000 [00:00<00:03, 18769.51 examples/s]', '\\rFilter:  12%|#1        |\n7000/60000 [00:00<00:02, 20055.61 examples/s]', '\\rFilter:  18%|#8        |\n11000/60000 [00:00<00:02, 20411.94 examples/s]', '\\rFilter:  25%|##5       |\n15000/60000 [00:00<00:02, 20561.79 examples/s]', '\\rFilter:  30%|###       |\n18000/60000 [00:00<00:02, 20512.18 examples/s]', '\\rFilter:  37%|###6      |\n22000/60000 [00:01<00:01, 20621.43 examples/s]', '\\rFilter:  43%|####3     |\n26000/60000 [00:01<00:01, 20671.95 examples/s]', '\\rFilter:  50%|#####     |\n30000/60000 [00:01<00:01, 20712.45 examples/s]', '\\rFilter:  57%|#####6    |\n34000/60000 [00:01<00:01, 20736.40 examples/s]', '\\rFilter:  63%|######3   |\n38000/60000 [00:01<00:01, 20675.53 examples/s]', '\\rFilter:  70%|#######   |\n42000/60000 [00:02<00:00, 20701.23 examples/s]', '\\rFilter:  75%|#######5  |\n45000/60000 [00:02<00:00, 15776.13 examples/s]', '\\rFilter:  82%|########1 |\n49000/60000 [00:02<00:00, 17085.13 examples/s]', '\\rFilter:  88%|########8 |\n53000/60000 [00:02<00:00, 18077.17 examples/s]', '\\rFilter:  95%|#########5|\n57000/60000 [00:02<00:00, 18806.19 examples/s]', '\\rFilter: 100%|##########|\n60000/60000 [00:03<00:00, 19237.66 examples/s]', '', '\\rFilter: 100%|##########|\n60000/60000 [00:03<00:00, 19391.78 examples/s]', '\\n', '\\rFilter:   0%|\n| 0/10000 [00:00<?, ? examples/s]', '\\rFilter:  40%|####      | 4000/10000\n[00:00<00:00, 20634.66 examples/s]', '\\rFilter:  80%|########  | 8000/10000\n[00:00<00:00, 20700.30 examples/s]', '', '\\rFilter: 100%|##########| 10000/10000\n[00:00<00:00, 20658.94 examples/s]', '\\n', 'Epoch 0: validation_loss = 0.1236',\n'\\n', 'Epoch 1: validation_loss = 0.1603', '\\n', 'Epoch 2: validation_loss =\n0.2964', '\\n', '\\rFilter:   0%|          | 0/50000 [00:00<?, ? examples/s]',\n'\\rFilter:   4%|4         | 2000/50000 [00:00<00:04, 11043.69 examples/s]',\n'\\rFilter:   8%|8         | 4000/50000 [00:00<00:03, 11738.28 examples/s]',\n'\\rFilter:  12%|#2        | 6000/50000 [00:00<00:03, 11921.61 examples/s]',\n'\\rFilter:  16%|#6        | 8000/50000 [00:00<00:03, 12056.38 examples/s]',\n'\\rFilter:  20%|##        | 10000/50000 [00:00<00:03, 12125.96 examples/s]',\n'\\rFilter:  24%|##4       | 12000/50000 [00:00<00:03, 12179.96 examples/s]',\n'\\rFilter:  28%|##8       | 14000/50000 [00:01<00:03, 9063.67 examples/s] ',\n'\\rFilter:  32%|###2      | 16000/50000 [00:01<00:03, 9853.38 examples/s]',\n'\\rFilter:  36%|###6      | 18000/50000 [00:01<00:03, 10475.86 examples/s]',\n'\\rFilter:  40%|####      | 20000/50000 [00:01<00:02, 10914.39 examples/s]',\n'\\rFilter:  44%|####4     | 22000/50000 [00:01<00:02, 11277.39 examples/s]',\n'\\rFilter:  48%|####8     | 24000/50000 [00:02<00:02, 11547.59 examples/s]',\n'\\rFilter:  52%|#####2    | 26000/50000 [00:02<00:02, 11737.84 examples/s]',\n'\\rFilter:  56%|#####6    | 28000/50000 [00:02<00:01, 11873.78 examples/s]',\n'\\rFilter:  60%|######    | 30000/50000 [00:02<00:01, 11966.97 examples/s]',\n'\\rFilter:  64%|######4   | 32000/50000 [00:02<00:01, 12038.46 examples/s]',\n'\\rFilter:  68%|######8   | 34000/50000 [00:02<00:01, 12086.78 examples/s]',\n'\\rFilter:  72%|#######2  | 36000/50000 [00:03<00:01, 12123.59 examples/s]',\n'\\rFilter:  76%|#######6  | 38000/50000 [00:03<00:00, 12104.82 examples/s]',\n'\\rFilter:  80%|########  | 40000/50000 [00:03<00:00, 12135.26 examples/s]',\n'\\rFilter:  84%|########4 | 42000/50000 [00:03<00:00, 12148.96 examples/s]',\n'\\rFilter:  88%|########8 | 44000/50000 [00:03<00:00, 12170.38 examples/s]',\n'\\rFilter:  92%|#########2| 46000/50000 [00:03<00:00, 12184.39 examples/s]',\n'\\rFilter:  96%|#########6| 48000/50000 [00:04<00:00, 12194.55 examples/s]',\n'\\rFilter: 100%|##########| 50000/50000 [00:04<00:00, 12194.55 examples/s]', '',\n'\\rFilter: 100%|##########| 50000/50000 [00:04<00:00, 11649.69 examples/s]',\n'\\n', '\\rFilter:   0%|          | 0/10000 [00:00<?, ? examples/s]', '\\rFilter:\n20%|##        | 2000/10000 [00:00<00:00, 12170.10 examples/s]', '\\rFilter:\n40%|####      | 4000/10000 [00:00<00:00, 12235.73 examples/s]', '\\rFilter:\n60%|######    | 6000/10000 [00:00<00:00, 12261.99 examples/s]', '\\rFilter:\n80%|########  | 8000/10000 [00:00<00:00, 12202.56 examples/s]', '\\rFilter:\n100%|##########| 10000/10000 [00:00<00:00, 8855.62 examples/s]', '', '\\rFilter:\n100%|##########| 10000/10000 [00:00<00:00, 10080.91 examples/s]', '\\n',\n'Traceback (most recent call last):\\n  File \"runfile.py\", line 183, in\n<module>\\n    X_train, y_train, g_train = make_split(train_raw, sp_corr_train)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 160, in\nmake_split\\n    imgs = ds[\"image\"]\\n           ~~^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/arrow_dataset.py\", line 2777, in __getitem__\\n    return\nself._getitem(key)\\n           ^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/arrow_dataset.py\", line 2761, in _getitem\\n    pa_subtable =\nquery_table(self._data, key, indices=self._indices)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/formatting/formatting.py\", line 604, in query_table\\n\n_check_valid_column_key(key, table.column_names)\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/formatting/formatting.py\", line 541, in\n_check_valid_column_key\\n    raise KeyError(f\"Column {key} not in the dataset.\nCurrent columns in the dataset: {columns}\")\\nKeyError: \"Column image not in the\ndataset. Current columns in the dataset: [\\'img\\', \\'label\\']\"\\n', 'Execution\ntime: 52 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n*** DATASET mnist ***', '\\n', 'Traceback (most\nrecent call last):\\n  File \"runfile.py\", line 186, in <module>\\n    for batch in\ntrain_loader:\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/dataloader.py\", line 701, in __next__\\n    data =\nself._next_data()\\n           ^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/dataloader.py\", line 757, in _next_data\\n    data =\nself._dataset_fetcher.fetch(index)  # may raise StopIteration\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\\n    data =\n[self.dataset[idx] for idx in possibly_batched_index]\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\\n    data =\n[self.dataset[idx] for idx in possibly_batched_index]\\n\n~~~~~~~~~~~~^^^^^\\n  File \"runfile.py\", line 140, in __getitem__\\n    \"label\":\ntorch.tensor(label, torch.long),\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nTypeError: tensor() takes 1 positional argument\nbut 2 were given\\n', 'Execution time: 10 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', '\\n*** DATASET mnist ***', '\\n', '[mnist] Epoch 0:\nvalidation_loss = 1.6071, val_wg = 0.3153, NMI = 0.0000', '\\n', 'Traceback (most\nrecent call last):\\n  File \"runfile.py\", line 208, in <module>\\n    for b in\nloader:\\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/dataloader.py\", line 701, in __next__\\n    data =\nself._next_data()\\n           ^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/dataloader.py\", line 757, in _next_data\\n    data =\nself._dataset_fetcher.fetch(index)  # may raise StopIteration\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\\n    data =\nself.dataset.__getitems__(possibly_batched_index)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/dataset.py\", line 420, in __getitems__\\n    return\n[self.dataset[self.indices[idx]] for idx in indices]\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/utils/data/dataset.py\", line 420, in <listcomp>\\n    return\n[self.dataset[self.indices[idx]] for idx in indices]\\n\n~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 123, in __getitem__\\n\nitem = self.ds[idx]\\n           ~~~~~~~^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/arrow_dataset.py\", line 2777, in __getitem__\\n    return\nself._getitem(key)\\n           ^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/arrow_dataset.py\", line 2762, in _getitem\\n\nformatted_output = format_table(\\n                       ^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/formatting/formatting.py\", line 650, in format_table\\n\nquery_type = key_to_query_type(key)\\n                 ^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/formatting/formatting.py\", line 570, in key_to_query_type\\n\n_raise_bad_key_type(key)\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/formatting/formatting.py\", line 44, in _raise_bad_key_type\\n\nraise TypeError(\\nTypeError: Wrong key type: \\'4802\\' of type \\'<class\n\\'numpy.int64\\'>\\'. Expected one of int, slice, range, str or Iterable.\\n',\n'Execution time: a minute seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 0.0151', '\\n', 'Epoch\n1: validation_loss = 0.0099', '\\n', 'Epoch 2: validation_loss = 0.0427', '\\n',\n'Epoch 0: validation_loss = 0.1236', '\\n', 'Epoch 1: validation_loss = 0.1603',\n'\\n', 'Epoch 2: validation_loss = 0.2964', '\\n', 'Epoch 0: validation_loss =\n1.5571', '\\n', 'Epoch 1: validation_loss = 1.9842', '\\n', 'Epoch 2:\nvalidation_loss = 1.4346', '\\n', 'Execution time: 47 seconds seconds (time limit\nis an hour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 12, in\n<module>\\n    from sklearn.cluster import KMeans\\nModuleNotFoundError: No module\nnamed \\'sklearn\\'\\n', 'Execution time: a second seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'mnist Epoch 0: validation_loss = 0.0049, val_wg =\n0.9960', '\\n', 'mnist Epoch 1: validation_loss = 0.0022, val_wg = 0.9960', '\\n',\n'mnist Epoch 2: validation_loss = 0.0060, val_wg = 0.9960', '\\n', 'mnist Epoch\n3: validation_loss = 0.0020, val_wg = 1.0000', '\\n', 'fashion_mnist Epoch 0:\nvalidation_loss = 0.1016, val_wg = 0.9614', '\\n', 'fashion_mnist Epoch 1:\nvalidation_loss = 0.7944, val_wg = 0.6432', '\\n', 'fashion_mnist Epoch 2:\nvalidation_loss = 0.3826, val_wg = 0.8465', '\\n', 'fashion_mnist Epoch 3:\nvalidation_loss = 0.1784, val_wg = 0.9305', '\\n', '\\rFilter:   0%|          |\n0/50000 [00:00<?, ? examples/s]', '\\rFilter:   2%|2         | 1000/50000\n[00:00<00:05, 9652.26 examples/s]', '\\rFilter:   6%|6         | 3000/50000\n[00:00<00:04, 10948.04 examples/s]', '\\rFilter:  10%|#         | 5000/50000\n[00:00<00:04, 11230.40 examples/s]', '\\rFilter:  14%|#4        | 7000/50000\n[00:00<00:03, 11382.45 examples/s]', '\\rFilter:  18%|#8        | 9000/50000\n[00:00<00:03, 11384.76 examples/s]', '\\rFilter:  22%|##2       | 11000/50000\n[00:00<00:03, 11466.81 examples/s]', '\\rFilter:  26%|##6       | 13000/50000\n[00:01<00:03, 11498.70 examples/s]', '\\rFilter:  30%|###       | 15000/50000\n[00:01<00:04, 8475.38 examples/s] ', '\\rFilter:  34%|###4      | 17000/50000\n[00:01<00:03, 9200.97 examples/s]', '\\rFilter:  38%|###8      | 19000/50000\n[00:01<00:03, 9821.51 examples/s]', '\\rFilter:  42%|####2     | 21000/50000\n[00:02<00:02, 10310.04 examples/s]', '\\rFilter:  46%|####6     | 23000/50000\n[00:02<00:02, 10641.77 examples/s]', '\\rFilter:  50%|#####     | 25000/50000\n[00:02<00:02, 10804.16 examples/s]', '\\rFilter:  54%|#####4    | 27000/50000\n[00:02<00:02, 11032.71 examples/s]', '\\rFilter:  58%|#####8    | 29000/50000\n[00:02<00:01, 11164.74 examples/s]', '\\rFilter:  62%|######2   | 31000/50000\n[00:02<00:01, 11287.86 examples/s]', '\\rFilter:  66%|######6   | 33000/50000\n[00:03<00:01, 11335.29 examples/s]', '\\rFilter:  70%|#######   | 35000/50000\n[00:03<00:01, 11405.72 examples/s]', '\\rFilter:  74%|#######4  | 37000/50000\n[00:03<00:01, 11468.44 examples/s]', '\\rFilter:  78%|#######8  | 39000/50000\n[00:03<00:00, 11497.40 examples/s]', '\\rFilter:  82%|########2 | 41000/50000\n[00:03<00:00, 11510.03 examples/s]', '\\rFilter:  86%|########6 | 43000/50000\n[00:03<00:00, 11530.80 examples/s]', '\\rFilter:  90%|######### | 45000/50000\n[00:04<00:00, 11559.05 examples/s]', '\\rFilter:  94%|#########3| 47000/50000\n[00:04<00:00, 11536.48 examples/s]', '\\rFilter:  98%|#########8| 49000/50000\n[00:04<00:00, 11562.70 examples/s]', '', '\\rFilter: 100%|##########| 50000/50000\n[00:04<00:00, 10987.76 examples/s]', '\\n', '\\rFilter:   0%|          | 0/10000\n[00:00<?, ? examples/s]', '\\rFilter:  20%|##        | 2000/10000 [00:00<00:00,\n11489.07 examples/s]', '\\rFilter:  40%|####      | 4000/10000 [00:00<00:00,\n11471.26 examples/s]', '\\rFilter:  60%|######    | 6000/10000 [00:00<00:00,\n7699.38 examples/s] ', '\\rFilter:  80%|########  | 8000/10000 [00:00<00:00,\n8870.91 examples/s]', '\\rFilter: 100%|##########| 10000/10000 [00:01<00:00,\n9666.42 examples/s]', '', '\\rFilter: 100%|##########| 10000/10000 [00:01<00:00,\n9448.21 examples/s]', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 160, in <module>\\n    Xtr, ytr, gtr = build(ds_train, 0.9)\\n\n^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 152, in build\\n    img =\nnp.array(ex[\"image\"], dtype=np.float32).flatten() / 255.0\\n\n~~^^^^^^^^^\\nKeyError: \\'image\\'\\n', 'Execution time: a minute seconds (time\nlimit is an hour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 12, in\n<module>\\n    from sklearn.decomposition import PCA\\nModuleNotFoundError: No\nmodule named \\'sklearn\\'\\n', 'Execution time: a second seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', '\\n*** DATASET mnist ***', '\\n', '[mnist] Epoch 0:\nvalidation_loss = 1.6121, val_wg = 0.3162, NMI = 0.0000', '\\n', '[mnist] Epoch\n1: validation_loss = 1.4516, val_wg = 0.4311, NMI = 0.1255', '\\n', '[mnist]\nEpoch 2: validation_loss = 1.3053, val_wg = 0.5261, NMI = 0.1255', '\\n', '\\n***\nDATASET fashion_mnist ***', '\\n', '[fashion_mnist] Epoch 0: validation_loss =\n1.2996, val_wg = 0.4198, NMI = 0.0000', '\\n', '[fashion_mnist] Epoch 1:\nvalidation_loss = 1.0106, val_wg = 0.5029, NMI = 0.0801', '\\n', '[fashion_mnist]\nEpoch 2: validation_loss = 0.8909, val_wg = 0.5564, NMI = 0.0801', '\\n', '\\n***\nDATASET cifar10 ***', '\\n', '[cifar10] Epoch 0: validation_loss = 1.9109, val_wg\n= 0.2339, NMI = 0.0000', '\\n', '[cifar10] Epoch 1: validation_loss = 1.8209,\nval_wg = 0.2705, NMI = 0.1550', '\\n', '[cifar10] Epoch 2: validation_loss =\n1.7503, val_wg = 0.3103, NMI = 0.1550', '\\n', 'Saved experiment data to\n/data/chenhui/AI-Scientist-v2/experiments/2025-06-06_23-36-\n12_gradient_cluster_robust_attempt_0/0-run/process_ForkProcess-\n9/working/experiment_data.npy', '\\n', 'Execution time: 4 minutes seconds (time\nlimit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Training with learning rate = 0.0001 ===',\n'\\n', 'lr=0.0001 epoch=0: val_loss=0.6977, val_wg=0.3168', '\\n', 'lr=0.0001\nepoch=1: val_loss=0.6917, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=2:\nval_loss=0.6904, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=3: val_loss=0.6900,\nval_wg=0.3588', '\\n', 'lr=0.0001 epoch=4: val_loss=0.6899, val_wg=0.3588', '\\n',\n'lr=0.0001 epoch=5: val_loss=0.6898, val_wg=0.3588', '\\n', '\\n=== Training with\nlearning rate = 0.001 ===', '\\n', 'lr=0.001 epoch=0: val_loss=0.5822,\nval_wg=0.8866', '\\n', 'lr=0.001 epoch=1: val_loss=0.5093, val_wg=0.9538', '\\n',\n'lr=0.001 epoch=2: val_loss=0.4929, val_wg=0.9622', '\\n', 'lr=0.001 epoch=3:\nval_loss=0.4887, val_wg=0.9622', '\\n', 'lr=0.001 epoch=4: val_loss=0.4872,\nval_wg=0.9622', '\\n', 'lr=0.001 epoch=5: val_loss=0.4860, val_wg=0.9622', '\\n',\n'\\n=== Training with learning rate = 0.01 ===', '\\n', 'lr=0.01 epoch=0:\nval_loss=0.0228, val_wg=0.9885', '\\n', 'lr=0.01 epoch=1: val_loss=0.0305,\nval_wg=0.9885', '\\n', 'lr=0.01 epoch=2: val_loss=0.0348, val_wg=0.9885', '\\n',\n'lr=0.01 epoch=3: val_loss=0.0380, val_wg=0.9885', '\\n', 'lr=0.01 epoch=4:\nval_loss=0.0405, val_wg=0.9885', '\\n', 'lr=0.01 epoch=5: val_loss=0.0430,\nval_wg=0.9924', '\\n', 'Execution time: 8 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', '\\n=== Training with learning rate = 0.0001 ===',\n'\\n', 'lr=0.0001 epoch=0: val_loss=0.6977, val_wg=0.3168', '\\n', 'lr=0.0001\nepoch=1: val_loss=0.6917, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=2:\nval_loss=0.6904, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=3: val_loss=0.6900,\nval_wg=0.3588', '\\n', 'lr=0.0001 epoch=4: val_loss=0.6899, val_wg=0.3588', '\\n',\n'lr=0.0001 epoch=5: val_loss=0.6898, val_wg=0.3588', '\\n', '\\n=== Training with\nlearning rate = 0.001 ===', '\\n', 'lr=0.001 epoch=0: val_loss=0.5822,\nval_wg=0.8866', '\\n', 'lr=0.001 epoch=1: val_loss=0.5093, val_wg=0.9538', '\\n',\n'lr=0.001 epoch=2: val_loss=0.4929, val_wg=0.9622', '\\n', 'lr=0.001 epoch=3:\nval_loss=0.4887, val_wg=0.9622', '\\n', 'lr=0.001 epoch=4: val_loss=0.4872,\nval_wg=0.9622', '\\n', 'lr=0.001 epoch=5: val_loss=0.4860, val_wg=0.9622', '\\n',\n'\\n=== Training with learning rate = 0.01 ===', '\\n', 'lr=0.01 epoch=0:\nval_loss=0.0228, val_wg=0.9885', '\\n', 'lr=0.01 epoch=1: val_loss=0.0305,\nval_wg=0.9885', '\\n', 'lr=0.01 epoch=2: val_loss=0.0348, val_wg=0.9885', '\\n',\n'lr=0.01 epoch=3: val_loss=0.0380, val_wg=0.9885', '\\n', 'lr=0.01 epoch=4:\nval_loss=0.0405, val_wg=0.9885', '\\n', 'lr=0.01 epoch=5: val_loss=0.0430,\nval_wg=0.9924', '\\n', 'Execution time: 8 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', '\\n=== Training with learning rate = 0.0001 ===',\n'\\n', 'lr=0.0001 epoch=0: val_loss=0.6977, val_wg=0.3168', '\\n', 'lr=0.0001\nepoch=1: val_loss=0.6917, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=2:\nval_loss=0.6904, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=3: val_loss=0.6900,\nval_wg=0.3588', '\\n', 'lr=0.0001 epoch=4: val_loss=0.6899, val_wg=0.3588', '\\n',\n'lr=0.0001 epoch=5: val_loss=0.6898, val_wg=0.3588', '\\n', '\\n=== Training with\nlearning rate = 0.001 ===', '\\n', 'lr=0.001 epoch=0: val_loss=0.5822,\nval_wg=0.8866', '\\n', 'lr=0.001 epoch=1: val_loss=0.5093, val_wg=0.9538', '\\n',\n'lr=0.001 epoch=2: val_loss=0.4929, val_wg=0.9622', '\\n', 'lr=0.001 epoch=3:\nval_loss=0.4887, val_wg=0.9622', '\\n', 'lr=0.001 epoch=4: val_loss=0.4872,\nval_wg=0.9622', '\\n', 'lr=0.001 epoch=5: val_loss=0.4860, val_wg=0.9622', '\\n',\n'\\n=== Training with learning rate = 0.01 ===', '\\n', 'lr=0.01 epoch=0:\nval_loss=0.0228, val_wg=0.9885', '\\n', 'lr=0.01 epoch=1: val_loss=0.0305,\nval_wg=0.9885', '\\n', 'lr=0.01 epoch=2: val_loss=0.0348, val_wg=0.9885', '\\n',\n'lr=0.01 epoch=3: val_loss=0.0380, val_wg=0.9885', '\\n', 'lr=0.01 epoch=4:\nval_loss=0.0405, val_wg=0.9885', '\\n', 'lr=0.01 epoch=5: val_loss=0.0430,\nval_wg=0.9924', '\\n', 'Execution time: 8 seconds seconds (time limit is an\nhour).']", ""], "analysis": ["", "Execution failed with ModuleNotFoundError: No module named 'sklearn'. The\nenvironment is missing scikit-learn. To fix, install the scikit-learn package\n(e.g., add 'pip install scikit-learn' to your setup or requirements) or replace\nPCA and KMeans calls with alternatives available in the current environment.", "The training script fails immediately with ModuleNotFoundError: No module named\n'sklearn'. The code depends on scikit-learn for silhouette_score and\nnormalized_mutual_info_score but scikit-learn is not installed in the\nenvironment. To fix, add scikit-learn to the project's dependencies (e.g., pip\ninstall scikit-learn) or include custom implementations of the required metrics.", "The script fails immediately due to ModuleNotFoundError: No module named\n'sklearn'. This indicates scikit-learn is not installed in the environment. To\nfix this, add scikit-learn to the dependencies (e.g., `pip install scikit-\nlearn`) or include it in the project's requirements file.", "The script attempted to load the Hugging Face dataset 'kmnist', which does not\nexist on the Hub, resulting in a DatasetNotFoundError and aborting the run. To\nfix this, replace 'kmnist' with the correct dataset name (e.g.,\n'kuzushiji_mnist') or another valid HF dataset identifier.", "The script crashes when processing the \u2018fashion_mnist\u2019 dataset because it\nassumes all HuggingFace image datasets have an \u201cimage\u201d column. In reality,\n\u2018fashion_mnist\u2019 uses the key \u2018img\u2019 (columns are [\u2018img\u2019, \u2018label\u2019]), triggering a\nKeyError in make_split(). Proposed fix: detect or standardize the image column\nname (e.g. check if \u2018image\u2019 in ds.column_names else use \u2018img\u2019), or rename the\ncolumn immediately after loading: raw = raw.rename_column('img','image'). This\nensures make_split() always accesses the correct key.", "The data wrapper\u2019s __getitem__ incorrectly calls torch.tensor(label,\ntorch.long), passing the dtype as a second positional argument, which PyTorch\nrequires to be passed as a keyword (dtype=...). This causes a TypeError and\ncrashes data loading. Fix by changing both torch.tensor(label, torch.long) and\ntorch.tensor(sp, torch.long) to torch.tensor(label, dtype=torch.long) and\ntorch.tensor(sp, dtype=torch.long). Additionally, wrap idx in a tensor if you\nintend to keep it through DataLoader (e.g., torch.tensor(idx,\ndtype=torch.long)).", "The script crashes during the pseudo\u2010group extraction step because Subset yields\nnumpy.int64 indices when sampling, and these numpy.int64 values are passed\ndirectly to the HuggingFace Dataset __getitem__, which only accepts Python ints\nor other supported types. This raises a TypeError. Proposed fix: convert sampled\nindices to Python int by calling sub.tolist() instead of using a NumPy array, or\ncast idx to int inside ColoredHFImageDataset.__getitem__ before indexing the HF\ndataset.", "", "The script failed immediately due to a missing dependency: scikit-learn\n(sklearn) is not installed in the environment. To fix this, add scikit-learn to\nthe environment (e.g., via `pip install scikit-learn`) or use an alternative\nclustering implementation available in the runtime.", "The script assumes each dataset has an 'image' field, but CIFAR-10 samples use\nthe key 'img', causing a KeyError at build(...). To fix, adapt the data-loading\nstep to reference ex.get('image', ex.get('img')), or rename the 'img' column to\n'image' for CIFAR-10, ensuring consistent key access across datasets.", "Execution failed due to missing scikit-learn ('sklearn') dependency. Install\nscikit-learn (e.g., pip install scikit-learn) or include it in the environment's\ndependencies to resolve the error.", "", "", "The implementation only uses a synthetic dataset and does not load or test on\nthe three HuggingFace datasets (Colored MNIST, Waterbirds, CelebA) as required.\nTo fix this, import the datasets from the HuggingFace library, preprocess them\nsimilarly, wrap them in DataLoaders, and include them in the experiment loops.", "", ""], "exc_type": [null, "ModuleNotFoundError", "ModuleNotFoundError", "ModuleNotFoundError", "DatasetNotFoundError", "KeyError", "TypeError", "TypeError", null, "ModuleNotFoundError", "KeyError", "ModuleNotFoundError", null, null, null, null, null], "exc_info": [null, {"args": ["No module named 'sklearn'"], "name": "sklearn", "msg": "No module named 'sklearn'"}, {"args": ["No module named 'sklearn'"], "name": "sklearn", "msg": "No module named 'sklearn'"}, {"args": ["No module named 'sklearn'"], "name": "sklearn", "msg": "No module named 'sklearn'"}, {"args": ["Dataset 'kmnist' doesn't exist on the Hub or cannot be accessed."]}, {"args": ["Column image not in the dataset. Current columns in the dataset: ['img', 'label']"]}, {"args": ["tensor() takes 1 positional argument but 2 were given"]}, {"args": ["Wrong key type: '4802' of type '<class 'numpy.int64'>'. Expected one of int, slice, range, str or Iterable."]}, null, {"args": ["No module named 'sklearn'"], "name": "sklearn", "msg": "No module named 'sklearn'"}, {"args": ["image"]}, {"args": ["No module named 'sklearn'"], "name": "sklearn", "msg": "No module named 'sklearn'"}, null, null, null, null, null], "exc_stack": [null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 11, "<module>", "from sklearn.decomposition import PCA"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 10, "<module>", "from sklearn.metrics import silhouette_score, normalized_mutual_info_score"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 12, "<module>", "from sklearn.cluster import KMeans"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 148, "<module>", "ds_full = load_dataset(ds_name)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1652, "dataset_module_factory", "raise e1 from None"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1578, "dataset_module_factory", "raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 183, "<module>", "X_train, y_train, g_train = make_split(train_raw, sp_corr_train)"], ["runfile.py", 160, "make_split", "imgs = ds[\"image\"]"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/arrow_dataset.py", 2777, "__getitem__", "return self._getitem(key)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/arrow_dataset.py", 2761, "_getitem", "pa_subtable = query_table(self._data, key, indices=self._indices)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/formatting/formatting.py", 604, "query_table", "_check_valid_column_key(key, table.column_names)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/formatting/formatting.py", 541, "_check_valid_column_key", "raise KeyError(f\"Column {key} not in the dataset. Current columns in the dataset: {columns}\")"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 186, "<module>", "for batch in train_loader:"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/dataloader.py", 701, "__next__", "data = self._next_data()"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/dataloader.py", 757, "_next_data", "data = self._dataset_fetcher.fetch(index)  # may raise StopIteration"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", 52, "fetch", "data = [self.dataset[idx] for idx in possibly_batched_index]"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", 52, "<listcomp>", "data = [self.dataset[idx] for idx in possibly_batched_index]"], ["runfile.py", 140, "__getitem__", "\"label\": torch.tensor(label, torch.long),"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 208, "<module>", "for b in loader:"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/dataloader.py", 701, "__next__", "data = self._next_data()"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/dataloader.py", 757, "_next_data", "data = self._dataset_fetcher.fetch(index)  # may raise StopIteration"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", 50, "fetch", "data = self.dataset.__getitems__(possibly_batched_index)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/dataset.py", 420, "__getitems__", "return [self.dataset[self.indices[idx]] for idx in indices]"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/utils/data/dataset.py", 420, "<listcomp>", "return [self.dataset[self.indices[idx]] for idx in indices]"], ["runfile.py", 123, "__getitem__", "item = self.ds[idx]"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/arrow_dataset.py", 2777, "__getitem__", "return self._getitem(key)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/arrow_dataset.py", 2762, "_getitem", "formatted_output = format_table("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/formatting/formatting.py", 650, "format_table", "query_type = key_to_query_type(key)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/formatting/formatting.py", 570, "key_to_query_type", "_raise_bad_key_type(key)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/formatting/formatting.py", 44, "_raise_bad_key_type", "raise TypeError("]], null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 12, "<module>", "from sklearn.cluster import KMeans"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 160, "<module>", "Xtr, ytr, gtr = build(ds_train, 0.9)"], ["runfile.py", 152, "build", "img = np.array(ex[\"image\"], dtype=np.float32).flatten() / 255.0"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 12, "<module>", "from sklearn.decomposition import PCA"]], null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "worst-group accuracy", "lower_is_better": false, "description": "Proportion of correct predictions on the worst-performing subgroup in the dataset at final evaluation.", "data": [{"dataset_name": "in-sample", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "development", "final_value": 0.9924, "best_value": 0.9924}]}, {"metric_name": "average loss", "lower_is_better": true, "description": "Average loss across all examples in the dataset at final evaluation.", "data": [{"dataset_name": "in-sample", "final_value": 0.0125, "best_value": 0.0125}, {"dataset_name": "development", "final_value": 0.043, "best_value": 0.043}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Proportion of correct predictions at evaluation time on the out-of-sample dataset.", "data": [{"dataset_name": "out-of-sample", "final_value": 0.996, "best_value": 0.996}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "training loss", "data": [{"dataset_name": "mnist", "final_value": 0.0098, "best_value": 0.0098}, {"dataset_name": "fashion_mnist", "final_value": 0.0348, "best_value": 0.0348}, {"dataset_name": "cifar10", "final_value": 0.1354, "best_value": 0.1354}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "validation loss", "data": [{"dataset_name": "mnist", "final_value": 0.0427, "best_value": 0.0427}, {"dataset_name": "fashion_mnist", "final_value": 0.2964, "best_value": 0.2964}, {"dataset_name": "cifar10", "final_value": 1.4346, "best_value": 1.4346}]}, {"metric_name": "train worst-group accuracy", "lower_is_better": false, "description": "training worst-group accuracy", "data": [{"dataset_name": "mnist", "final_value": 0.9962, "best_value": 0.9962}, {"dataset_name": "fashion_mnist", "final_value": 0.9809, "best_value": 0.9809}, {"dataset_name": "cifar10", "final_value": 0.9551, "best_value": 0.9551}]}, {"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "validation worst-group accuracy", "data": [{"dataset_name": "mnist", "final_value": 0.963, "best_value": 0.963}, {"dataset_name": "fashion_mnist", "final_value": 0.8548, "best_value": 0.8548}, {"dataset_name": "cifar10", "final_value": 0.4869, "best_value": 0.4869}]}, {"metric_name": "normalized mutual information", "lower_is_better": false, "description": "NMI score", "data": [{"dataset_name": "mnist", "final_value": 0.0024, "best_value": 0.0024}, {"dataset_name": "fashion_mnist", "final_value": 0.0002, "best_value": 0.0002}, {"dataset_name": "cifar10", "final_value": 0.0117, "best_value": 0.0117}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on validation set", "data": [{"dataset_name": "mnist", "final_value": 0.5261, "best_value": 0.5261}, {"dataset_name": "fashion_mnist", "final_value": 0.5564, "best_value": 0.5564}, {"dataset_name": "cifar10", "final_value": 0.3103, "best_value": 0.3103}]}, {"metric_name": "NMI score", "lower_is_better": false, "description": "Normalized mutual information score on validation set", "data": [{"dataset_name": "mnist", "final_value": 0.1255, "best_value": 0.1255}, {"dataset_name": "fashion_mnist", "final_value": 0.0801, "best_value": 0.0801}, {"dataset_name": "cifar10", "final_value": 0.155, "best_value": 0.155}]}]}, {"metric_names": [{"metric_name": "worst-group accuracy", "lower_is_better": false, "description": "The accuracy on the worst-performing group of samples in the dataset", "data": [{"dataset_name": "Dataset 1", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "Dataset 2", "final_value": 0.9924, "best_value": 0.9924}]}, {"metric_name": "average loss", "lower_is_better": true, "description": "The average loss on the dataset", "data": [{"dataset_name": "Dataset 1", "final_value": 0.0125, "best_value": 0.0125}, {"dataset_name": "Dataset 2", "final_value": 0.043, "best_value": 0.043}]}, {"metric_name": "accuracy", "lower_is_better": false, "description": "The classification accuracy on the dataset", "data": [{"dataset_name": "Dataset 3", "final_value": 0.996, "best_value": 0.996}]}]}, {"metric_names": [{"metric_name": "worst-group accuracy", "lower_is_better": false, "description": "Accuracy of the worst-performing subgroup on the dataset", "data": [{"dataset_name": "Primary Dataset", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "Development Dataset", "final_value": 0.9924, "best_value": 0.9924}]}, {"metric_name": "average loss", "lower_is_better": true, "description": "Average loss value over the dataset", "data": [{"dataset_name": "Primary Dataset", "final_value": 0.0125, "best_value": 0.0125}, {"dataset_name": "Development Dataset", "final_value": 0.043, "best_value": 0.043}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Overall accuracy on the held-out dataset", "data": [{"dataset_name": "Held-out Dataset", "final_value": 0.996, "best_value": 0.996}]}]}, {"metric_names": [{"metric_name": "worst-group accuracy", "lower_is_better": false, "description": "Accuracy of the worst-performing subgroup", "data": [{"dataset_name": "Development dataset", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "Tuning dataset", "final_value": 0.9924, "best_value": 0.9924}]}, {"metric_name": "average loss", "lower_is_better": true, "description": "Average loss per example", "data": [{"dataset_name": "Development dataset", "final_value": 0.0125, "best_value": 0.0125}, {"dataset_name": "Tuning dataset", "final_value": 0.043, "best_value": 0.043}]}, {"metric_name": "accuracy", "lower_is_better": false, "description": "Overall test-set accuracy", "data": [{"dataset_name": "Holdout dataset", "final_value": 0.996, "best_value": 0.996}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_worst_group_accuracy.png", "../../logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_loss_curves.png"], [], [], [], [], [], [], [], ["../../logs/0-run/experiment_results/experiment_49fcd875f877404d9be92813ea5221d0_proc_11502/loss_curves_comparison.png", "../../logs/0-run/experiment_results/experiment_49fcd875f877404d9be92813ea5221d0_proc_11502/wg_accuracy_comparison.png", "../../logs/0-run/experiment_results/experiment_49fcd875f877404d9be92813ea5221d0_proc_11502/nmi_comparison.png"], [], [], [], ["../../logs/0-run/experiment_results/experiment_84fb6109282b4ec59b949dc2dd561db4_proc_11502/fashion_mnist_validation_wg_nmi.png", "../../logs/0-run/experiment_results/experiment_84fb6109282b4ec59b949dc2dd561db4_proc_11502/cifar10_validation_wg_nmi.png", "../../logs/0-run/experiment_results/experiment_84fb6109282b4ec59b949dc2dd561db4_proc_11502/mnist_validation_wg_nmi.png", "../../logs/0-run/experiment_results/experiment_84fb6109282b4ec59b949dc2dd561db4_proc_11502/comparison_validation_wg_accuracy.png", "../../logs/0-run/experiment_results/experiment_84fb6109282b4ec59b949dc2dd561db4_proc_11502/comparison_nmi_curves.png"], ["../../logs/0-run/experiment_results/experiment_d9f6b86a6c8248cbab4aa90ba817f260_proc_11504/synthetic_worst_group_accuracy.png", "../../logs/0-run/experiment_results/experiment_d9f6b86a6c8248cbab4aa90ba817f260_proc_11504/synthetic_loss_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_cc3e08fa12a544a69ec69d976d9e1189_proc_11502/synthetic_worst_group_accuracy.png", "../../logs/0-run/experiment_results/experiment_cc3e08fa12a544a69ec69d976d9e1189_proc_11502/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/seed_aggregation_ae7c2420b83345f38a377a80cc4136b7/synthetic_worst_group_accuracy_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_ae7c2420b83345f38a377a80cc4136b7/synthetic_loss_curves_agg.png"]], "plot_paths": [["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_worst_group_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_loss_curves.png"], [], [], [], [], [], [], [], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_49fcd875f877404d9be92813ea5221d0_proc_11502/loss_curves_comparison.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_49fcd875f877404d9be92813ea5221d0_proc_11502/wg_accuracy_comparison.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_49fcd875f877404d9be92813ea5221d0_proc_11502/nmi_comparison.png"], [], [], [], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_84fb6109282b4ec59b949dc2dd561db4_proc_11502/fashion_mnist_validation_wg_nmi.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_84fb6109282b4ec59b949dc2dd561db4_proc_11502/cifar10_validation_wg_nmi.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_84fb6109282b4ec59b949dc2dd561db4_proc_11502/mnist_validation_wg_nmi.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_84fb6109282b4ec59b949dc2dd561db4_proc_11502/comparison_validation_wg_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_84fb6109282b4ec59b949dc2dd561db4_proc_11502/comparison_nmi_curves.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_d9f6b86a6c8248cbab4aa90ba817f260_proc_11504/synthetic_worst_group_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_d9f6b86a6c8248cbab4aa90ba817f260_proc_11504/synthetic_loss_curves.png"], [], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_cc3e08fa12a544a69ec69d976d9e1189_proc_11502/synthetic_worst_group_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_cc3e08fa12a544a69ec69d976d9e1189_proc_11502/synthetic_loss_curves.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ae7c2420b83345f38a377a80cc4136b7/synthetic_worst_group_accuracy_agg.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ae7c2420b83345f38a377a80cc4136b7/synthetic_loss_curves_agg.png"]], "plot_analyses": [[{"analysis": "Training worst-group accuracy curves show that with learning rate 0.0001 the model only improves slowly from ~0.30 to ~0.35 over six epochs. With lr=0.001 accuracy jumps quickly from ~0.90 to ~0.98 by epoch 2 and then plateaus around ~0.97\u20130.98. With lr=0.01 the model achieves near-perfect worst-group accuracy (~0.99\u20131.00) from the first epoch onward. The validation worst-group curves mirror the training trends closely, indicating stable generalization across these rates.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_worst_group_accuracy.png"}, {"analysis": "Training loss curves reveal that lr=0.0001 yields a minor decrease from ~0.70 to ~0.69, matching its slow accuracy gains. With lr=0.001 the loss drops sharply from ~0.58 to ~0.48 by epoch 3 before flattening, consistent with rapid accuracy improvements. The high lr=0.01 setting drives the loss close to zero immediately (~0.01) but then slightly increases, suggesting possible overshooting or instability. Validation losses follow similar patterns, confirming that the training dynamics transfer to held-out data.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_loss_curves.png"}], [], [], [], [], [], [], [], [{"analysis": "Loss curves comparison shows stable convergence for MNIST with training loss around 0.01\u20130.02 and validation loss near 0.01 across epochs, indicating minimal overfitting. Fashion-MNIST exhibits a U-shaped training loss (decrease to ~0.01 then rise to ~0.035) and steadily increasing validation loss from ~0.12 to ~0.30, signaling overfitting beginning around epoch 2. CIFAR10 training loss spikes at epoch 2 (up to ~0.48) before recovering by epoch 3, and validation loss peaks at epoch 2 (~2.0) then falls to ~1.4, reflecting unstable training dynamics, likely due to premature clustering perturbations or hyperparameter settings not suited for this more complex dataset.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_49fcd875f877404d9be92813ea5221d0_proc_11502/loss_curves_comparison.png"}, {"analysis": "Worst-group accuracy comparison reveals near-perfect training and validation performance on MNIST (above 99%), while Fashion-MNIST holds around 98\u201399% in training but drops to ~85% validation by epoch 3, further evidence of overfitting. CIFAR10 worst-group accuracy collapses to ~70% at epoch 2 during training and plateaued validation around ~48\u201349%, showing that pseudo-group reweighting under current clustering fails to generalize for this dataset.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_49fcd875f877404d9be92813ea5221d0_proc_11502/wg_accuracy_comparison.png"}, {"analysis": "Normalized mutual information (NMI) remains extremely low across all epochs\u2014around 0.0024 for MNIST, 0.0002 for Fashion-MNIST, and 0.0117 for CIFAR10\u2014indicating that unsupervised clustering of gradient signatures does not recover the true spurious or core group structure. This poor cluster\u2013ground-truth alignment explains the subpar worst-group performance, especially on more complex data.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_49fcd875f877404d9be92813ea5221d0_proc_11502/nmi_comparison.png"}], [], [], [], [], [{"analysis": "Training and validation worst-group accuracy curves show a strong dependence on learning rate. At lr=0.01 models achieve near-perfect worst-group performance almost immediately, indicating rapid convergence that may mask underlying group biases. At lr=0.001, worst-group accuracy rises quickly from ~0.90 to ~0.97 on training and ~0.96 on validation by epoch 2, then plateaus, suggesting a good balance between convergence speed and stable generalization. At lr=0.0001, worst-group accuracy remains low (~0.35) throughout, implying underfitting and inability to recover spurious-group structure.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_d9f6b86a6c8248cbab4aa90ba817f260_proc_11504/synthetic_worst_group_accuracy.png"}, {"analysis": "Loss curves mirror the accuracy trends: lr=0.01 yields extremely low training and validation loss (~0.01), signaling possible over-optimization or collapse; lr=0.001 reduces loss from ~0.58 to ~0.48 by epoch 3 then levels off, aligning with its high worst-group accuracy; lr=0.0001 barely decreases loss (~0.70 to ~0.69), confirming slow learning and poor worst-group performance.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_d9f6b86a6c8248cbab4aa90ba817f260_proc_11504/synthetic_loss_curves.png"}], [], [{"analysis": "Synthetic dataset \u2013 worst-group accuracy curves over epochs for three learning rates (0.0001, 0.001, 0.01). On the training split, lr=0.01 (green) starts at ~98% worst-group accuracy in epoch 1, quickly plateaus near 99\u2013100%, indicating very fast and sustained improvement on the hardest subpopulation. lr=0.001 (orange) begins at ~90%, jumps to ~98% by epoch 2, then stabilizes slightly below the lr=0.01 curve for the remaining epochs. lr=0.0001 (blue) exhibits sluggish progress: it starts near 30%, slowly rises to ~35% by epoch 6, suggesting that the low learning rate cannot effectively optimize for worst-group performance within six epochs.\n\nOn the validation split, the same pattern holds. The highest learning rate almost instantly reaches ~99% worst-group accuracy and remains stable. The mid-range rate climbs from ~89% to ~96% by epoch 3 and stays around that level. The lowest rate lingers near ~32\u201336% across all epochs, indicating underfitting for the hardest group under a very small step size. These curves show that, for this synthetic task, larger learning rates dramatically accelerate both optimization and robustness towards spurious correlations, while very small rates fail to escape the spurious shortcut within the given budget.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_cc3e08fa12a544a69ec69d976d9e1189_proc_11502/synthetic_worst_group_accuracy.png"}, {"analysis": "Synthetic dataset \u2013 loss curves over epochs for the same three learning rates. On the training split, lr=0.01 (green) achieves a near-zero loss (~0.02 at epoch 1, dipping to ~0.005 by epoch 3) but then shows a slight uptick toward ~0.01 by epoch 6, hinting at potential early over-optimization or onset of overfitting. lr=0.001 (orange) reduces loss from ~0.58 at epoch 1 to ~0.48 by epoch 3 and then marginally improves to ~0.48\u20130.49, reflecting steady convergence without drastic overfitting. lr=0.0001 (blue) only decreases from ~0.70 to ~0.68 across six epochs, indicating very slow learning.\n\nOn validation, lr=0.01 again attains extremely low loss (~0.02\u21920.04), tracking the training loss closely, so generalization to held-out data appears strong. The mid-level rate lowers validation loss from ~0.59 to ~0.49, mirroring its training behavior and confirming consistent improvement. The smallest rate essentially stagnates around ~0.70\u21920.69. Combined with the accuracy curves, these losses reinforce that only sufficiently large learning rates deliver both rapid convergence and robust worst-group performance, while tiny rates yield neither low loss nor improved minority-group accuracy.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_cc3e08fa12a544a69ec69d976d9e1189_proc_11502/synthetic_loss_curves.png"}], []], "vlm_feedback_summary": ["Larger learning rates accelerate convergence of both loss minimization and\nworst-group accuracy, enabling the model to quickly exploit gradient clustering\nfor robust subgroup performance. Medium lr balances speed and stability, while\nvery low lr fails to adapt group weights effectively within six epochs, and very\nhigh lr risks potential instability despite achieving nearly perfect worst-group\nmetrics.", "[]", "[]", "[]", "[]", "[]", "[]", "[]", "Unsupervised gradient clustering provides robust outcomes on simple benchmarks\nlike MNIST but struggles on Fashion-MNIST and especially CIFAR10, as shown by\nunstable loss curves, overfitting trends, degraded worst-group accuracy, and\nnear-zero NMI. Clustering hyperparameters, gradient feature extraction\nstrategies, dimensionality reduction settings, and choice of clustering\nalgorithm need refinement to better uncover latent group structures and improve\nrobust generalization.", "[]", "[]", "[]", "[]", "The synthetic data results highlight that moderate learning rates (e.g., 1e-3)\nbest support UGC\u2019s ability to uncover latent spurious groups and boost worst-\ngroup accuracy without collapsing the training dynamics. Very low rates underfit\nand fail to reveal group structure, while very high rates converge too quickly,\nrisking over-optimization and potential loss of generalizable features.", "[]", "Both worst-group accuracy and loss curves unequivocally favor higher learning\nrates (0.01 and 0.001) over the smallest (0.0001). The largest rate achieves\nnear-oracle performance by epoch 2 but risks slight overfitting, whereas the\nmid-rate presents a balanced, stable improvement. Low rates underfit entirely.\nFor future runs, consider momentum or warm restarts to harness large-step\nbenefits while mitigating over-optimization, and evaluate unsupervised gradient\nclustering under both mid and high learning rates.", "[]"], "exec_time": [6.182882070541382, 0.4455888271331787, 0.2440016269683838, 0.24448156356811523, 109.65931177139282, 52.11668300628662, 10.278892517089844, 62.163214683532715, 47.036861181259155, 1.3944478034973145, 117.73834013938904, 1.2182745933532715, 295.6392123699188, 8.639209508895874, 8.58534550666809, 8.582160472869873, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['synthetic']"], [], [], [], [], [], [], [], ["['MNIST']"], [], [], [], [], ["[\"synthetic\"]"], [], ["[\"synthetic\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    data = experiment_data[\"learning_rate\"][\"synthetic\"]\n    lrs = data[\"lrs\"]\n    metrics_train = data[\"metrics\"][\"train\"]\n    metrics_val = data[\"metrics\"][\"val\"]\n    losses_train = data[\"losses\"][\"train\"]\n    losses_val = data[\"losses\"][\"val\"]\nexcept Exception as e:\n    print(f\"Error loading or extracting experiment data: {e}\")\n\n# Plot worst\u2010group accuracy curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, metrics_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, metrics_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, metrics_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training WG Accuracy\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Worst\u2010group Accuracy\")\n    axes[1].set_title(\"Validation WG Accuracy\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Worst\u2010group Accuracy\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\n        \"Synthetic dataset - Worst\u2010group Accuracy\\nLeft: Training, Right: Validation\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_worst_group_accuracy.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close(\"all\")\n\n# Plot loss curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, losses_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, losses_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, losses_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[1].set_title(\"Validation Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\"Synthetic dataset - Loss Curves\\nLeft: Training, Right: Validation\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close(\"all\")\n", null, null, null, null, null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    datasets = list(experiment_data.keys())\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Loss curves comparison\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for ds in datasets:\n        metrics = experiment_data[ds][\"metrics\"]\n        epochs = np.arange(1, len(metrics[\"train_loss\"]) + 1)\n        axes[0].plot(epochs, metrics[\"train_loss\"], label=ds)\n        axes[1].plot(epochs, metrics[\"val_loss\"], label=ds)\n    axes[0].set_title(\"Training Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[1].set_title(\"Validation Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\"Loss Curves Comparison\\nLeft: Training, Right: Validation\")\n    plt.savefig(os.path.join(working_dir, \"loss_curves_comparison.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating loss comparison plot: {e}\")\n    plt.close(\"all\")\n\n# Worst\u2010group accuracy comparison\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for ds in datasets:\n        metrics = experiment_data[ds][\"metrics\"]\n        epochs = np.arange(1, len(metrics[\"train_wg\"]) + 1)\n        axes[0].plot(epochs, metrics[\"train_wg\"], label=ds)\n        axes[1].plot(epochs, metrics[\"val_wg\"], label=ds)\n    axes[0].set_title(\"Training Worst\u2010group Accuracy\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Accuracy\")\n    axes[1].set_title(\"Validation Worst\u2010group Accuracy\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Accuracy\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\"Worst\u2010group Accuracy Comparison\\nLeft: Training, Right: Validation\")\n    plt.savefig(os.path.join(working_dir, \"wg_accuracy_comparison.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating wg accuracy comparison plot: {e}\")\n    plt.close(\"all\")\n\n# NMI across epochs comparison\ntry:\n    plt.figure(figsize=(6, 4))\n    for ds in datasets:\n        nmi = experiment_data[ds][\"metrics\"][\"nmi\"]\n        epochs = np.arange(1, len(nmi) + 1)\n        plt.plot(epochs, nmi, label=ds)\n    plt.title(\"NMI across Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"NMI\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"nmi_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating NMI comparison plot: {e}\")\n    plt.close(\"all\")\n", null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\ndatasets = [\"mnist\", \"fashion_mnist\", \"cifar10\"]\nval_wg = {d: np.array(experiment_data[d][\"metrics\"][\"val_wg\"]) for d in datasets}\nnmi = {d: np.array(experiment_data[d][\"nmi\"]) for d in datasets}\nepochs = np.arange(1, next(iter(val_wg.values())).shape[0] + 1)\n\n# Comparison plot: Validation worst-group accuracy\ntry:\n    plt.figure()\n    for d in datasets:\n        plt.plot(epochs, val_wg[d], marker=\"o\", label=d)\n    plt.title(\"Validation Worst-group Accuracy\\nComparison across datasets\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Worst-group Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"comparison_validation_wg_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating comparison_validation_wg_accuracy: {e}\")\n    plt.close()\n\n# Comparison plot: NMI curves\ntry:\n    plt.figure()\n    for d in datasets:\n        plt.plot(epochs, nmi[d], marker=\"o\", label=d)\n    plt.title(\"NMI over Epochs\\nComparison across datasets\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Normalized Mutual Information\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"comparison_nmi_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating comparison_nmi_curves: {e}\")\n    plt.close()\n\n# Per-dataset plots: Validation WG and NMI side by side\nfor d in datasets:\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n        axes[0].plot(epochs, val_wg[d], marker=\"o\")\n        axes[0].set_title(f\"Validation WG Accuracy\\nDataset: {d}\")\n        axes[0].set_xlabel(\"Epoch\")\n        axes[0].set_ylabel(\"Worst-group Accuracy\")\n        axes[1].plot(epochs, nmi[d], marker=\"o\")\n        axes[1].set_title(f\"NMI over Epochs\\nDataset: {d}\")\n        axes[1].set_xlabel(\"Epoch\")\n        axes[1].set_ylabel(\"Normalized Mutual Information\")\n        fig.suptitle(f\"{d} - Validation WG and NMI\")\n        plt.savefig(os.path.join(working_dir, f\"{d}_validation_wg_nmi.png\"))\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating {d}_validation_wg_nmi: {e}\")\n        plt.close(\"all\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    data = experiment_data[\"learning_rate\"][\"synthetic\"]\n    lrs = data[\"lrs\"]\n    metrics_train = data[\"metrics\"][\"train\"]\n    metrics_val = data[\"metrics\"][\"val\"]\n    losses_train = data[\"losses\"][\"train\"]\n    losses_val = data[\"losses\"][\"val\"]\nexcept Exception as e:\n    print(f\"Error loading or extracting experiment data: {e}\")\n\n# Plot worst\u2010group accuracy curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, metrics_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, metrics_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, metrics_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training WG Accuracy\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Worst\u2010group Accuracy\")\n    axes[1].set_title(\"Validation WG Accuracy\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Worst\u2010group Accuracy\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\n        \"Synthetic dataset - Worst\u2010group Accuracy\\nLeft: Training, Right: Validation\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_worst_group_accuracy.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close(\"all\")\n\n# Plot loss curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, losses_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, losses_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, losses_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[1].set_title(\"Validation Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\"Synthetic dataset - Loss Curves\\nLeft: Training, Right: Validation\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close(\"all\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    data = experiment_data[\"learning_rate\"][\"synthetic\"]\n    lrs = data[\"lrs\"]\n    metrics_train = data[\"metrics\"][\"train\"]\n    metrics_val = data[\"metrics\"][\"val\"]\n    losses_train = data[\"losses\"][\"train\"]\n    losses_val = data[\"losses\"][\"val\"]\nexcept Exception as e:\n    print(f\"Error loading or extracting experiment data: {e}\")\n\n# Plot worst\u2010group accuracy curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, metrics_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, metrics_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, metrics_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training WG Accuracy\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Worst\u2010group Accuracy\")\n    axes[1].set_title(\"Validation WG Accuracy\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Worst\u2010group Accuracy\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\n        \"Synthetic dataset - Worst\u2010group Accuracy\\nLeft: Training, Right: Validation\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_worst_group_accuracy.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close(\"all\")\n\n# Plot loss curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, losses_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, losses_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, losses_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[1].set_title(\"Validation Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\"Synthetic dataset - Loss Curves\\nLeft: Training, Right: Validation\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close(\"all\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    data = experiment_data[\"learning_rate\"][\"synthetic\"]\n    lrs = data[\"lrs\"]\n    metrics_train = data[\"metrics\"][\"train\"]\n    metrics_val = data[\"metrics\"][\"val\"]\n    losses_train = data[\"losses\"][\"train\"]\n    losses_val = data[\"losses\"][\"val\"]\nexcept Exception as e:\n    print(f\"Error loading or extracting experiment data: {e}\")\n\n# Plot worst\u2010group accuracy curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, metrics_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, metrics_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, metrics_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training WG Accuracy\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Worst\u2010group Accuracy\")\n    axes[1].set_title(\"Validation WG Accuracy\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Worst\u2010group Accuracy\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\n        \"Synthetic dataset - Worst\u2010group Accuracy\\nLeft: Training, Right: Validation\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_worst_group_accuracy.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close(\"all\")\n\n# Plot loss curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, losses_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, losses_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, losses_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[1].set_title(\"Validation Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\"Synthetic dataset - Loss Curves\\nLeft: Training, Right: Validation\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close(\"all\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data from multiple runs\nexperiment_data_paths = [\n    \"experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_d9f6b86a6c8248cbab4aa90ba817f260_proc_11504/experiment_data.npy\",\n    \"None/experiment_data.npy\",\n    \"experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_cc3e08fa12a544a69ec69d976d9e1189_proc_11502/experiment_data.npy\",\n]\nall_data = []\ntry:\n    for path in experiment_data_paths:\n        try:\n            data = np.load(\n                os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\"), path), allow_pickle=True\n            ).item()\n            all_data.append(data)\n        except Exception as e:\n            print(f\"Error loading experiment data from {path}: {e}\")\nexcept Exception as e:\n    print(f\"Error initializing data loading: {e}\")\n\n# Aggregate and plot per dataset\ndataset_names = all_data[0].get(\"learning_rate\", {}).keys() if all_data else []\nfor dataset in dataset_names:\n    # collect runs for this dataset\n    run_list = [\n        exp[\"learning_rate\"][dataset]\n        for exp in all_data\n        if dataset in exp.get(\"learning_rate\", {})\n    ]\n    if not run_list:\n        continue\n    # stack metrics and losses: shape (runs, n_lrs, epochs)\n    metrics_tr = np.stack([r[\"metrics\"][\"train\"] for r in run_list], axis=0)\n    metrics_val = np.stack([r[\"metrics\"][\"val\"] for r in run_list], axis=0)\n    loss_tr = np.stack([r[\"losses\"][\"train\"] for r in run_list], axis=0)\n    loss_val = np.stack([r[\"losses\"][\"val\"] for r in run_list], axis=0)\n    lrs = run_list[0][\"lrs\"]\n    # compute mean and standard error\n    m_tr_mean = metrics_tr.mean(axis=0)\n    m_tr_sem = metrics_tr.std(axis=0) / np.sqrt(metrics_tr.shape[0])\n    m_val_mean = metrics_val.mean(axis=0)\n    m_val_sem = metrics_val.std(axis=0) / np.sqrt(metrics_val.shape[0])\n    l_tr_mean = loss_tr.mean(axis=0)\n    l_tr_sem = loss_tr.std(axis=0) / np.sqrt(loss_tr.shape[0])\n    l_val_mean = loss_val.mean(axis=0)\n    l_val_sem = loss_val.std(axis=0) / np.sqrt(loss_val.shape[0])\n    # print final\u2010epoch metrics\n    for i, lr in enumerate(lrs):\n        print(\n            f\"{dataset} lr={lr} - Final Train WG Acc: {m_tr_mean[i,-1]:.4f} \u00b1 {m_tr_sem[i,-1]:.4f}, \"\n            f\"Val WG Acc: {m_val_mean[i,-1]:.4f} \u00b1 {m_val_sem[i,-1]:.4f}\"\n        )\n        print(\n            f\"{dataset} lr={lr} - Final Train Loss: {l_tr_mean[i,-1]:.4f} \u00b1 {l_tr_sem[i,-1]:.4f}, \"\n            f\"Val Loss: {l_val_mean[i,-1]:.4f} \u00b1 {l_val_sem[i,-1]:.4f}\"\n        )\n    # Plot aggregated worst\u2010group accuracy\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n        epochs = np.arange(1, m_tr_mean.shape[1] + 1)\n        for i, lr in enumerate(lrs):\n            axes[0].errorbar(epochs, m_tr_mean[i], yerr=m_tr_sem[i], label=f\"lr={lr}\")\n            axes[1].errorbar(epochs, m_val_mean[i], yerr=m_val_sem[i], label=f\"lr={lr}\")\n        axes[0].set_title(\"Training WG Accuracy\")\n        axes[0].set_xlabel(\"Epoch\")\n        axes[0].set_ylabel(\"Worst\u2010group Accuracy\")\n        axes[1].set_title(\"Validation WG Accuracy\")\n        axes[1].set_xlabel(\"Epoch\")\n        axes[1].set_ylabel(\"Worst\u2010group Accuracy\")\n        axes[0].legend()\n        axes[1].legend()\n        fig.suptitle(\n            f\"{dataset.capitalize()} dataset - Worst\u2010group Accuracy (Aggregated)\\nLeft: Training, Right: Validation\"\n        )\n        plt.savefig(\n            os.path.join(working_dir, f\"{dataset}_worst_group_accuracy_agg.png\")\n        )\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating aggregated accuracy plot for {dataset}: {e}\")\n        plt.close(\"all\")\n    # Plot aggregated loss curves\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n        epochs = np.arange(1, l_tr_mean.shape[1] + 1)\n        for i, lr in enumerate(lrs):\n            axes[0].errorbar(epochs, l_tr_mean[i], yerr=l_tr_sem[i], label=f\"lr={lr}\")\n            axes[1].errorbar(epochs, l_val_mean[i], yerr=l_val_sem[i], label=f\"lr={lr}\")\n        axes[0].set_title(\"Training Loss\")\n        axes[0].set_xlabel(\"Epoch\")\n        axes[0].set_ylabel(\"Loss\")\n        axes[1].set_title(\"Validation Loss\")\n        axes[1].set_xlabel(\"Epoch\")\n        axes[1].set_ylabel(\"Loss\")\n        axes[0].legend()\n        axes[1].legend()\n        fig.suptitle(\n            f\"{dataset.capitalize()} dataset - Loss Curves (Aggregated)\\nLeft: Training, Right: Validation\"\n        )\n        plt.savefig(os.path.join(working_dir, f\"{dataset}_loss_curves_agg.png\"))\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating aggregated loss plot for {dataset}: {e}\")\n        plt.close(\"all\")\n"], "plot_plan": ["We will load the saved experiment data from the working directory and extract the metrics and losses for the synthetic dataset across learning rates. Then we construct two summary figures: one showing worst\u2010group accuracy and another showing losses, each as side\u2010by\u2010side subplots for training vs validation curves over epochs. We label axes, include legends for each learning rate, and add a main title with a clear subtitle indicating which side is training vs validation. Each figure is generated inside its own try\u2010except block to catch errors, save to the working directory with descriptive filenames, and always close the figure afterward.", null, null, null, null, null, null, null, "I will load the saved experiment data from the working directory and extract the metrics for each dataset. I\u2019ll then generate three comparison plots: one for loss curves (training vs validation), one for worst\u2010group accuracy (training vs validation), and one for NMI across epochs. Each plot will include all datasets with a legend and clear axis labels and titles. All plotting blocks will be wrapped in try\u2010except to catch errors and ensure figures are closed properly. The figures will be saved under descriptive filenames in the working directory. This approach provides a concise overview of model performance across datasets without overplotting individual dataset figures. The code strictly follows the data structure and uses basic matplotlib plotting.", null, null, null, "First, we load the experimental results from experiment_data.npy, which holds validation worst\u2010group accuracy and NMI curves for three datasets. Next, we extract the val_wg and nmi lists for 'mnist', 'fashion_mnist', and 'cifar10' and build a common epoch index. In the first try-except block, we plot validation worst-group accuracy over epochs for all datasets in a single figure, add titles, axis labels, and a legend, then save the plot. The second try-except block generates a comparison figure of NMI curves across epochs for the same datasets with similar annotations. Finally, within a loop over each dataset, each iteration is wrapped in its own try-except block to produce a two-panel figure showing that dataset\u2019s validation WG accuracy on the left and its NMI curve on the right, complete with suptitles and axis labels. Every plot is saved to working_dir using a descriptive filename that includes the dataset name and plot type. We ensure each figure is closed after saving to avoid resource leaks. Any errors during loading or plotting are caught and printed.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": ["learning_rate", null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["This script loads the NumPy file saved under the `working` directory to extract\nthe final-epoch worst\u2010group accuracies and average losses for the training and\nvalidation splits, and selects the learning rate that yields the highest\nvalidation worst\u2010group accuracy. It then prints clearly labeled metrics for the\ntraining and validation sets. Finally, it computes and prints the overall test\naccuracy by comparing the saved test predictions with the ground truth labels.", "", "", "", "", "", "", "", "The following script immediately loads the saved `experiment_data.npy` file from\nthe `working` directory, iterates over each dataset entry, and prints out the\nfinal epoch values for train loss, validation loss, train worst-group accuracy,\nvalidation worst-group accuracy, and NMI with clear, descriptive labels.", "", "", "", "I will write a small script that locates the working directory, loads the saved\nNumPy file into a Python dictionary, and then iterates over each dataset to pull\nout the final values for validation worst-group accuracy and NMI. For clarity,\nit prints the dataset name followed by clearly labeled final metrics. The script\nruns immediately when executed without any special entry point.", "This script loads the NumPy file saved under the `working` directory to extract\nthe final-epoch worst\u2010group accuracies and average losses for the training and\nvalidation splits, and selects the learning rate that yields the highest\nvalidation worst\u2010group accuracy. It then prints clearly labeled metrics for the\ntraining and validation sets. Finally, it computes and prints the overall test\naccuracy by comparing the saved test predictions with the ground truth labels.", "This script loads the NumPy file saved under the `working` directory to extract\nthe final-epoch worst\u2010group accuracies and average losses for the training and\nvalidation splits, and selects the learning rate that yields the highest\nvalidation worst\u2010group accuracy. It then prints clearly labeled metrics for the\ntraining and validation sets. Finally, it computes and prints the overall test\naccuracy by comparing the saved test predictions with the ground truth labels.", "This script loads the NumPy file saved under the `working` directory to extract\nthe final-epoch worst\u2010group accuracies and average losses for the training and\nvalidation splits, and selects the learning rate that yields the highest\nvalidation worst\u2010group accuracy. It then prints clearly labeled metrics for the\ntraining and validation sets. Finally, it computes and prints the overall test\naccuracy by comparing the saved test predictions with the ground truth labels.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Load the saved experiment data\ndata_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Navigate to the synthetic learning-rate sweep results\nsynthetic = experiment_data[\"learning_rate\"][\"synthetic\"]\nlrs = synthetic[\"lrs\"]\nmetrics_train = synthetic[\"metrics\"][\"train\"]  # shape: (num_lrs, epochs)\nmetrics_val = synthetic[\"metrics\"][\"val\"]\nlosses_train = synthetic[\"losses\"][\"train\"]\nlosses_val = synthetic[\"losses\"][\"val\"]\npredictions = synthetic[\"predictions\"]  # shape: (num_lrs, num_test_samples)\nground_truth = synthetic[\"ground_truth\"]\n\n# Extract final-epoch values\nfinal_train_wg = metrics_train[:, -1]\nfinal_val_wg = metrics_val[:, -1]\nfinal_train_loss = losses_train[:, -1]\nfinal_val_loss = losses_val[:, -1]\n\n# Select the best learning rate by highest final validation worst-group accuracy\nbest_idx = np.argmax(final_val_wg)\n\n# Print metrics with clear labels\nprint(\"Training Dataset Metrics:\")\nprint(f\"  Final training worst-group accuracy: {final_train_wg[best_idx]:.4f}\")\nprint(f\"  Final training average loss: {final_train_loss[best_idx]:.4f}\\n\")\n\nprint(\"Validation Dataset Metrics:\")\nprint(f\"  Final validation worst-group accuracy: {final_val_wg[best_idx]:.4f}\")\nprint(f\"  Final validation average loss: {final_val_loss[best_idx]:.4f}\\n\")\n\n# Compute and print test accuracy\ntest_acc = np.mean(predictions[best_idx] == ground_truth)\nprint(\"Test Dataset Metrics:\")\nprint(f\"  Test accuracy: {test_acc:.4f}\")\n", "", "", "", "", "", "", "", "import os\nimport numpy as np\n\n# Load the experimental results dictionary\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over datasets and print final metrics\nfor dataset_name, data in experiment_data.items():\n    metrics = data[\"metrics\"]\n    # Extract the final epoch values for each metric\n    final_train_loss = metrics[\"train_loss\"][-1]\n    final_val_loss = metrics[\"val_loss\"][-1]\n    final_train_wg_acc = metrics[\"train_wg\"][-1]\n    final_val_wg_acc = metrics[\"val_wg\"][-1]\n    final_nmi = metrics[\"nmi\"][-1]\n\n    # Print dataset name and each metric with clear labels\n    print(f\"{dataset_name}\")\n    print(f\"train loss: {final_train_loss:.4f}\")\n    print(f\"validation loss: {final_val_loss:.4f}\")\n    print(f\"train worst-group accuracy: {final_train_wg_acc:.4f}\")\n    print(f\"validation worst-group accuracy: {final_val_wg_acc:.4f}\")\n    print(f\"NMI: {final_nmi:.4f}\\n\")\n", "", "", "", "import os\nimport numpy as np\n\n# Locate the working directory and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each dataset, extract final metrics, and print them\nfor dataset_name, results in experiment_data.items():\n    final_val_wg = results[\"metrics\"][\"val_wg\"][-1]\n    final_nmi = results[\"nmi\"][-1]\n    print(f\"{dataset_name}\")\n    print(f\"final validation worst-group accuracy: {final_val_wg:.4f}\")\n    print(f\"final NMI score: {final_nmi:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\ndata_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Navigate to the synthetic learning-rate sweep results\nsynthetic = experiment_data[\"learning_rate\"][\"synthetic\"]\nlrs = synthetic[\"lrs\"]\nmetrics_train = synthetic[\"metrics\"][\"train\"]  # shape: (num_lrs, epochs)\nmetrics_val = synthetic[\"metrics\"][\"val\"]\nlosses_train = synthetic[\"losses\"][\"train\"]\nlosses_val = synthetic[\"losses\"][\"val\"]\npredictions = synthetic[\"predictions\"]  # shape: (num_lrs, num_test_samples)\nground_truth = synthetic[\"ground_truth\"]\n\n# Extract final-epoch values\nfinal_train_wg = metrics_train[:, -1]\nfinal_val_wg = metrics_val[:, -1]\nfinal_train_loss = losses_train[:, -1]\nfinal_val_loss = losses_val[:, -1]\n\n# Select the best learning rate by highest final validation worst-group accuracy\nbest_idx = np.argmax(final_val_wg)\n\n# Print metrics with clear labels\nprint(\"Training Dataset Metrics:\")\nprint(f\"  Final training worst-group accuracy: {final_train_wg[best_idx]:.4f}\")\nprint(f\"  Final training average loss: {final_train_loss[best_idx]:.4f}\\n\")\n\nprint(\"Validation Dataset Metrics:\")\nprint(f\"  Final validation worst-group accuracy: {final_val_wg[best_idx]:.4f}\")\nprint(f\"  Final validation average loss: {final_val_loss[best_idx]:.4f}\\n\")\n\n# Compute and print test accuracy\ntest_acc = np.mean(predictions[best_idx] == ground_truth)\nprint(\"Test Dataset Metrics:\")\nprint(f\"  Test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\ndata_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Navigate to the synthetic learning-rate sweep results\nsynthetic = experiment_data[\"learning_rate\"][\"synthetic\"]\nlrs = synthetic[\"lrs\"]\nmetrics_train = synthetic[\"metrics\"][\"train\"]  # shape: (num_lrs, epochs)\nmetrics_val = synthetic[\"metrics\"][\"val\"]\nlosses_train = synthetic[\"losses\"][\"train\"]\nlosses_val = synthetic[\"losses\"][\"val\"]\npredictions = synthetic[\"predictions\"]  # shape: (num_lrs, num_test_samples)\nground_truth = synthetic[\"ground_truth\"]\n\n# Extract final-epoch values\nfinal_train_wg = metrics_train[:, -1]\nfinal_val_wg = metrics_val[:, -1]\nfinal_train_loss = losses_train[:, -1]\nfinal_val_loss = losses_val[:, -1]\n\n# Select the best learning rate by highest final validation worst-group accuracy\nbest_idx = np.argmax(final_val_wg)\n\n# Print metrics with clear labels\nprint(\"Training Dataset Metrics:\")\nprint(f\"  Final training worst-group accuracy: {final_train_wg[best_idx]:.4f}\")\nprint(f\"  Final training average loss: {final_train_loss[best_idx]:.4f}\\n\")\n\nprint(\"Validation Dataset Metrics:\")\nprint(f\"  Final validation worst-group accuracy: {final_val_wg[best_idx]:.4f}\")\nprint(f\"  Final validation average loss: {final_val_loss[best_idx]:.4f}\\n\")\n\n# Compute and print test accuracy\ntest_acc = np.mean(predictions[best_idx] == ground_truth)\nprint(\"Test Dataset Metrics:\")\nprint(f\"  Test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\ndata_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Navigate to the synthetic learning-rate sweep results\nsynthetic = experiment_data[\"learning_rate\"][\"synthetic\"]\nlrs = synthetic[\"lrs\"]\nmetrics_train = synthetic[\"metrics\"][\"train\"]  # shape: (num_lrs, epochs)\nmetrics_val = synthetic[\"metrics\"][\"val\"]\nlosses_train = synthetic[\"losses\"][\"train\"]\nlosses_val = synthetic[\"losses\"][\"val\"]\npredictions = synthetic[\"predictions\"]  # shape: (num_lrs, num_test_samples)\nground_truth = synthetic[\"ground_truth\"]\n\n# Extract final-epoch values\nfinal_train_wg = metrics_train[:, -1]\nfinal_val_wg = metrics_val[:, -1]\nfinal_train_loss = losses_train[:, -1]\nfinal_val_loss = losses_val[:, -1]\n\n# Select the best learning rate by highest final validation worst-group accuracy\nbest_idx = np.argmax(final_val_wg)\n\n# Print metrics with clear labels\nprint(\"Training Dataset Metrics:\")\nprint(f\"  Final training worst-group accuracy: {final_train_wg[best_idx]:.4f}\")\nprint(f\"  Final training average loss: {final_train_loss[best_idx]:.4f}\\n\")\n\nprint(\"Validation Dataset Metrics:\")\nprint(f\"  Final validation worst-group accuracy: {final_val_wg[best_idx]:.4f}\")\nprint(f\"  Final validation average loss: {final_val_loss[best_idx]:.4f}\\n\")\n\n# Compute and print test accuracy\ntest_acc = np.mean(predictions[best_idx] == ground_truth)\nprint(\"Test Dataset Metrics:\")\nprint(f\"  Test accuracy: {test_acc:.4f}\")\n", ""], "parse_term_out": ["['Training Dataset Metrics:', '\\n', '  Final training worst-group accuracy:\n0.9942', '\\n', '  Final training average loss: 0.0125\\n', '\\n', 'Validation\nDataset Metrics:', '\\n', '  Final validation worst-group accuracy: 0.9924',\n'\\n', '  Final validation average loss: 0.0430\\n', '\\n', 'Test Dataset\nMetrics:', '\\n', '  Test accuracy: 0.9960', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "", "", "", "", "", "", "", "['mnist', '\\n', 'train loss: 0.0098', '\\n', 'validation loss: 0.0427', '\\n',\n'train worst-group accuracy: 0.9962', '\\n', 'validation worst-group accuracy:\n0.9630', '\\n', 'NMI: 0.0024\\n', '\\n', 'fashion_mnist', '\\n', 'train loss:\n0.0348', '\\n', 'validation loss: 0.2964', '\\n', 'train worst-group accuracy:\n0.9809', '\\n', 'validation worst-group accuracy: 0.8548', '\\n', 'NMI: 0.0002\\n',\n'\\n', 'cifar10', '\\n', 'train loss: 0.1354', '\\n', 'validation loss: 1.4346',\n'\\n', 'train worst-group accuracy: 0.9551', '\\n', 'validation worst-group\naccuracy: 0.4869', '\\n', 'NMI: 0.0117\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "", "", "", "['mnist', '\\n', 'final validation worst-group accuracy: 0.5261', '\\n', 'final\nNMI score: 0.1255\\n', '\\n', 'fashion_mnist', '\\n', 'final validation worst-group\naccuracy: 0.5564', '\\n', 'final NMI score: 0.0801\\n', '\\n', 'cifar10', '\\n',\n'final validation worst-group accuracy: 0.3103', '\\n', 'final NMI score:\n0.1550\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Training Dataset Metrics:', '\\n', '  Final training worst-group accuracy:\n0.9942', '\\n', '  Final training average loss: 0.0125\\n', '\\n', 'Validation\nDataset Metrics:', '\\n', '  Final validation worst-group accuracy: 0.9924',\n'\\n', '  Final validation average loss: 0.0430\\n', '\\n', 'Test Dataset\nMetrics:', '\\n', '  Test accuracy: 0.9960', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Training Dataset Metrics:', '\\n', '  Final training worst-group accuracy:\n0.9942', '\\n', '  Final training average loss: 0.0125\\n', '\\n', 'Validation\nDataset Metrics:', '\\n', '  Final validation worst-group accuracy: 0.9924',\n'\\n', '  Final validation average loss: 0.0430\\n', '\\n', 'Test Dataset\nMetrics:', '\\n', '  Test accuracy: 0.9960', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Training Dataset Metrics:', '\\n', '  Final training worst-group accuracy:\n0.9942', '\\n', '  Final training average loss: 0.0125\\n', '\\n', 'Validation\nDataset Metrics:', '\\n', '  Final validation worst-group accuracy: 0.9924',\n'\\n', '  Final validation average loss: 0.0430\\n', '\\n', 'Test Dataset\nMetrics:', '\\n', '  Test accuracy: 0.9960', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3"]}
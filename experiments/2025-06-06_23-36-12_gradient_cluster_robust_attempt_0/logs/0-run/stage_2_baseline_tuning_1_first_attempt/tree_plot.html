<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 8], [0, 10], [0, 5], [0, 2], [0, 11], [0, 4], [0, 6], [0, 1], [0, 9], [0, 12], [0, 3], [1, 15], [1, 13], [1, 16], [1, 14], [6, 7]], "layout": [[0.5652173913043478, 0.0], [0.13043478260869565, 0.5], [0.21739130434782608, 0.5], [0.30434782608695654, 0.5], [0.391304347826087, 0.5], [0.4782608695652174, 0.5], [0.5652173913043478, 0.5], [0.5652173913043478, 1.0], [0.6521739130434783, 0.5], [0.7391304347826086, 0.5], [0.8260869565217391, 0.5], [0.9130434782608695, 0.5], [1.0, 0.5], [0.0, 1.0], [0.08695652173913043, 1.0], [0.17391304347826086, 1.0], [0.2608695652173913, 1.0]], "plan": ["We replace the missing sklearn dependency by implementing a simple NumPy k-means\nfunction that randomly initializes centroids and alternates between assigning\npoints and updating centroids for a fixed number of iterations. After the warmup\nepoch, we collect per-sample gradients, run our custom kmeans to get two\nclusters, and compute sample weights as the inverse of cluster sizes. The rest\nof the training, evaluation, and saving logic remains unchanged, with proper\nGPU/CPU handling and data movement. This fixes the ModuleNotFoundError and\nmaintains the intended unsupervised gradient clustering workflow.", "Hyperparam tuning name: learning_rate. We extend the baseline by looping over a\nlog\u2010scale grid of learning rates, reinitializing the model and optimizer for\neach value. During each run we apply the same warmup + gradient\u2010clustering\nreweighting schedule, recording per\u2010epoch train/val worst\u2010group accuracies and\nlosses. After training, we evaluate on the test split to obtain predictions and\nground truth. All these arrays are collated into an `experiment_data` dict under\nthe \u201clearning_rate\u201d tuning type and saved to `experiment_data.npy`.", "Hyperparam tuning name: batch_size. We wrap the original training routine in an\nouter loop over batch sizes, recreating the data\u2010loaders, model, optimizer, and\nper\u2010batch training schedule for each setting. For each batch size we record\nper\u2010epoch worst\u2010group accuracies and average losses on both train and validation\nsplits, then collect final test predictions and labels. All results\u2014including\nthe list of tried batch sizes, metrics, losses, predictions, and\nground\u2010truths\u2014are stored in a nested `experiment_data` dictionary under the key\n`'batch_size'`, which is saved to `working/experiment_data.npy`.", "Hyperparam tuning name: weight_decay. Here we sweep over weight_decay values [0,\n1e-5, 1e-4, 1e-3], and for each setting we re\u2010initialize the MLP and optimizer\n(with weight_decay), run the warmup+clustering training loop, collect per\u2010epoch\ntrain/val worst\u2010group accuracy and losses, then evaluate test predictions. All\nresults are stored under a nested dict key `'weight_decay' \u2192 'synthetic'`, then\nconverted into numpy arrays and saved as `experiment_data.npy`.", "Hyperparam tuning name: train_epochs. Here\u2019s a sketch of the solution: we wrap\nthe existing training+evaluation logic in a loop over a list of `train_epochs`\nvalues (e.g. 5, 10, 20), reinitialize the model and optimizer for each run,\nperform warmup + clustering + full training, collect per-epoch losses and\nworst\u2010group accuracies along with final test predictions, and store them in an\n`experiment_data` dict keyed by the hyperparameter. Finally we save everything\nvia `np.save()` as specified.", "Hyperparam tuning name: warmup_epochs. I will iterate over several warmup_epochs\nvalues (1, 2, 3, 5), re-initializing the model and optimizer for each setting,\nand collect per-epoch worst-group accuracy and average losses on both train and\nvalidation splits. After training each configuration, I gather test predictions\nand store them alongside the ground truth. All run-specific metrics, losses,\npredictions, and hyperparameter values are organized under\nexperiment_data['warmup_epochs']['synthetic'], and then saved to\n'experiment_data.npy'.", "Hyperparam tuning name: hidden_dim. Below is a self\u2010contained Python script that\nloops over hidden dimensions [16, 32, 64, 128], reinitializes and trains an MLP\nfor each, applies warmup + gradient clustering, collects per\u2010epoch worst\u2010group\naccuracies and losses on train/val, then evaluates test predictions. All data\nare stored in a nested `experiment_data` dict, converted to NumPy arrays, and\nsaved as `experiment_data.npy`.", "We fix the loss reporting by computing per\u2010sample average in `evaluate` and\nprinting average loss rather than sum.  We postpone clustering until after five\nwarmup epochs (`warmup_epochs=5`) so the gradient signatures are more\nmeaningful, and increase training epochs to 15 with batch size 128 for better\nconvergence.  We keep the MLP architecture unchanged but generalize its input\nand output dims for each task, and integrate two HuggingFace image datasets\n(MNIST and Fashion-MNIST) by flattening their images into feature vectors,\nassigning a dummy group label 0, and applying the same unsupervised gradient\nclustering pipeline.  All models and data are moved to the GPU (if available),\nand we track and save worst\u2010group accuracy and loss at each epoch across all\nthree datasets.", "Hyperparam tuning name: dropout_rate. Below is a sketch of the solution: we\nextend the `MLP` class to accept a `dropout_rate` argument and insert\n`nn.Dropout` after each ReLU. We then loop over several dropout rates,\nre\u2010initializing the model, optimizer, and sample weights for each run. During\neach run we record per\u2010epoch train/val worst\u2010group accuracies and losses, store\nfinal test predictions and labels, and nest all of this in a single\n`experiment_data` dict under `\"dropout_rate\" \u2192 \"synthetic\"`. Finally, we save\nthe dict as `\"experiment_data.npy\"`.", "Hyperparam tuning name: n_clusters. We extend the baseline by looping over\ndifferent `n_clusters` values and, for each, re-initializing the model and\nperforming the warmup+training procedure with gradient clustering at the end of\nwarmup. We collect per-epoch train/val losses and worst-group accuracies, then\nevaluate on the test split to record predictions and ground truth. All results\nare stored under `experiment_data['n_clusters_tuning']['synthetic']` with lists\nof runs for each cluster setting and finally saved as \u201cexperiment_data.npy\u201d. The\nscript remains self-contained and executable.", "Hyperparam tuning name: label_smoothing. We will add an outer loop over the\nsmoothing values \u03b5\u2208{0.0,0.05,0.1,0.2}, and for each setting re\u2010initialize the\nmodel, criterion (using `label_smoothing=\u03b5`), optimizer, and sample weights.  We\nwill then run the existing warmup + gradient\u2010clustering training loop,\ncollecting per\u2010epoch worst\u2010group accuracy and losses.  After training each \u03b5, we\nevaluate on the test split to store final predictions and ground truth.  All\nresults are organized under `experiment_data['label_smoothing'][\u03b5]` and saved\nvia `np.save(\"experiment_data.npy\", experiment_data)`.", "Hyperparam tuning name: momentum. We introduce a loop over momentum coefficients\n[0.5, 0.9, 0.99], and for each value we reinitialize the MLP, optimizer\n(SGD+momentum), and sample weights before running the warmup and clustering\ntraining stages. Throughout training we collect per\u2010epoch worst\u2010group accuracies\nand average losses on both train/val splits. After each momentum run we evaluate\non the test set to save predictions and ground truth. All results (momentum\nvalues, metrics, losses, predictions, and ground truth) are stored in a nested\n`experiment_data` dict and saved via `np.save()`.", "Hyperparam tuning name: max_grad_norm. We loop over the desired max_grad_norm\nthresholds, reinitialize the model and optimizer each time, and after each\nbackward pass we apply `clip_grad_norm_` to cap the gradient norm. We collect\nper\u2010epoch train/val losses and worst\u2010group accuracies, perform the same warmup +\ngradient\u2010clustering schedule, then at the end make test\u2010set predictions. All\nresults for each threshold are stored under\n`experiment_data['max_grad_norm'][str(th)] ['synthetic']`, and finally we save\nthe full dict as `experiment_data.npy`.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Create synthetic dataset with spurious feature\nnp.random.seed(0)\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {}\nsplits[\"train\"] = (X_norm[train_idx], y[train_idx], s[train_idx])\nsplits[\"val\"] = (X_norm[val_idx], y[val_idx], s[val_idx])\nsplits[\"test\"] = (X_norm[test_idx], y[test_idx], s[test_idx])\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# Simple NumPy k-means implementation\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# Model definition\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\nmodel = MLP(d + 1).to(device)\ncriterion = nn.CrossEntropyLoss(reduction=\"none\")\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# Experiment storage\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n\n# Evaluation helper\ndef evaluate(loader):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in [0, 1]:\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in [0, 1])\n    return loss_sum, wg_acc\n\n\n# Training loop with warmup + gradient clustering\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\nsample_weights = None\n\nfor epoch in range(total_epochs):\n    model.train()\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n        x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n        out = model(x)\n        losses = criterion(out, yb)\n        if epoch >= warmup_epochs and sample_weights is not None:\n            sw = sample_weights[idxb]\n            loss = (losses * sw).mean()\n        else:\n            loss = losses.mean()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    # Cluster gradients after warmup\n    if epoch == warmup_epochs - 1:\n        model.eval()\n        grads = []\n        for sample in cluster_loader:\n            batch = {k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)}\n            optimizer.zero_grad()\n            out = model(batch[\"features\"])\n            loss_i = criterion(out, batch[\"label\"]).mean()\n            loss_i.backward()\n            g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n            grads.append(g)\n        grads = np.stack(grads)\n        labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n        counts = np.bincount(labels, minlength=2)\n        sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n        sample_weights = torch.tensor(sw_arr, device=device)\n    # Evaluate\n    train_loss, train_wg = evaluate(train_loader)\n    val_loss, val_wg = evaluate(val_loader)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(train_loss / len(train_ds))\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(val_loss / len(val_ds))\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(train_wg)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(val_wg)\n\n# Final test predictions\nmodel.eval()\npreds, truths = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n        out = model(batch[\"features\"])\n        preds.extend(out.argmax(1).cpu().numpy().tolist())\n        truths.extend(batch[\"label\"].cpu().numpy().tolist())\nexperiment_data[\"synthetic\"][\"predictions\"] = np.array(preds)\nexperiment_data[\"synthetic\"][\"ground_truth\"] = np.array(truths)\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# k-means on CPU\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper: returns (sum_loss, worst-group accuracy)\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameter sweep over learning rates\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# Containers for results\nmetrics_train_all = []\nmetrics_val_all = []\nlosses_train_all = []\nlosses_val_all = []\npredictions_list = []\nground_truth = None\n\nfor lr in lrs:\n    print(f\"\\n=== Training with learning rate = {lr} ===\")\n    # re-init model, loss, optimizer\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    # per-epoch logs\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # cluster-based reweighting after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out_i = model(batch[\"features\"])\n                loss_i = criterion(out_i, batch[\"label\"]).mean()\n                loss_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # evaluate after epoch\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(\n            f\"lr={lr} epoch={epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n        )\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # final test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n\n    preds_npy = np.array(preds)\n    truths_npy = np.array(truths)\n    predictions_list.append(preds_npy)\n    if ground_truth is None:\n        ground_truth = truths_npy\n\n    metrics_train_all.append(m_tr)\n    metrics_val_all.append(m_val)\n    losses_train_all.append(l_tr)\n    losses_val_all.append(l_val)\n\n# Assemble and save experiment data\nexperiment_data = {\n    \"learning_rate\": {\n        \"synthetic\": {\n            \"lrs\": np.array(lrs),\n            \"metrics\": {\n                \"train\": np.array(metrics_train_all),\n                \"val\": np.array(metrics_val_all),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_all),\n                \"val\": np.array(losses_val_all),\n            },\n            \"predictions\": np.stack(predictions_list),\n            \"ground_truth\": ground_truth,\n        }\n    }\n}\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Hyperparameter settings\nbatch_sizes = [32, 64, 128, 256]\nexperiment_data = {\n    \"batch_size\": {\n        \"synthetic\": {\n            \"batch_sizes\": batch_sizes,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Generate synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# Datasets\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\n\n# Constant loaders for val/test and clustering\nval_loader_static = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader_static = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# K-means for gradient clustering\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation function\ndef evaluate(model, loader, criterion):\n    model.eval()\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in [0, 1]:\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in [0, 1])\n    return loss_sum, wg_acc\n\n\n# Hyperparameter tuning loop\nwarmup_epochs, train_epochs = 1, 5\ntotal_epochs = warmup_epochs + train_epochs\n\nfor bs in batch_sizes:\n    print(f\"\\n=== Batch size: {bs} ===\")\n    # DataLoaders for this batch size\n    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    val_loader = val_loader_static\n    test_loader = test_loader_static\n\n    # Model, loss, optimizer\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    sample_weights = None\n\n    # Containers\n    per_train_metrics, per_val_metrics = [], []\n    per_train_losses, per_val_losses = [], []\n\n    # Training\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                sw = sample_weights[idxb]\n                loss = (losses * sw).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        # Gradient clustering step\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out = model(batch[\"features\"])\n                li = criterion(out, batch[\"label\"]).mean()\n                li.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # Evaluate\n        tr_loss, tr_wg = evaluate(model, train_loader, criterion)\n        vl_loss, vl_wg = evaluate(model, val_loader, criterion)\n        per_train_losses.append(tr_loss / len(train_ds))\n        per_val_losses.append(vl_loss / len(val_ds))\n        per_train_metrics.append(tr_wg)\n        per_val_metrics.append(vl_wg)\n        print(\n            f\"Epoch {epoch}: val_loss={vl_loss/len(val_ds):.4f}, val_wg_acc={vl_wg:.4f}\"\n        )\n\n    # Test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().numpy().tolist())\n            truths.extend(batch[\"label\"].cpu().numpy().tolist())\n    preds = np.array(preds)\n    truths = np.array(truths)\n\n    # Store results\n    sd = experiment_data[\"batch_size\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(per_train_metrics)\n    sd[\"metrics\"][\"val\"].append(per_val_metrics)\n    sd[\"losses\"][\"train\"].append(per_train_losses)\n    sd[\"losses\"][\"val\"].append(per_val_losses)\n    sd[\"predictions\"].append(preds)\n    sd[\"ground_truth\"].append(truths)\n\n# Convert lists to numpy arrays for easy plotting\nsd = experiment_data[\"batch_size\"][\"synthetic\"]\nsd[\"batch_sizes\"] = np.array(sd[\"batch_sizes\"])\nsd[\"metrics\"][\"train\"] = np.vstack(sd[\"metrics\"][\"train\"])\nsd[\"metrics\"][\"val\"] = np.vstack(sd[\"metrics\"][\"val\"])\nsd[\"losses\"][\"train\"] = np.vstack(sd[\"losses\"][\"train\"])\nsd[\"losses\"][\"val\"] = np.vstack(sd[\"losses\"][\"val\"])\nsd[\"predictions\"] = np.vstack(sd[\"predictions\"])\nsd[\"ground_truth\"] = np.vstack(sd[\"ground_truth\"])\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nSaved experiment data to {os.path.join(working_dir, 'experiment_data.npy')}\")\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Setup working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Generate synthetic data\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Splits dict\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# k-means\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Eval helper\ncriterion = nn.CrossEntropyLoss(reduction=\"none\")\n\n\ndef evaluate(loader, model):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            ls = criterion(out, yb)\n            loss_sum += ls.sum().item()\n            preds = out.argmax(1)\n            for g in [0, 1]:\n                m = grp == g\n                total[g] += m.sum().item()\n                if m.sum().item() > 0:\n                    correct[g] += (preds[m] == yb[m]).sum().item()\n    wg = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in [0, 1])\n    return loss_sum, wg\n\n\n# Hyperparam sweep\nweight_decays = [0, 1e-5, 1e-4, 1e-3]\nexperiment_data = {\n    \"weight_decay\": {\n        \"synthetic\": {\n            \"decays\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Save test ground truth once\nexperiment_data[\"weight_decay\"][\"synthetic\"][\"ground_truth\"] = splits[\"test\"][1]\n\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\nfor wd in weight_decays:\n    # record decay\n    exp = experiment_data[\"weight_decay\"][\"synthetic\"]\n    exp[\"decays\"].append(wd)\n    # init model & optimizer\n    model = MLP(d + 1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n    sample_weights = None\n\n    # per-run storage\n    run_train_metrics, run_val_metrics = [], []\n    run_train_losses, run_val_losses = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            ls = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (ls * sample_weights[idxb]).mean()\n            else:\n                loss = ls.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        # clustering after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out = model(batch[\"features\"])\n                li = criterion(out, batch[\"label\"]).mean()\n                li.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2)\n            counts = np.bincount(labels, minlength=2)\n            sw = np.array([1.0 / counts[l] for l in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw, device=device)\n        # evaluate\n        tl, tg = evaluate(train_loader, model)\n        vl, vg = evaluate(val_loader, model)\n        run_train_losses.append(tl / len(train_ds))\n        run_val_losses.append(vl / len(val_ds))\n        run_train_metrics.append(tg)\n        run_val_metrics.append(vg)\n\n    # record per-run results\n    exp[\"losses\"][\"train\"].append(run_train_losses)\n    exp[\"losses\"][\"val\"].append(run_val_losses)\n    exp[\"metrics\"][\"train\"].append(run_train_metrics)\n    exp[\"metrics\"][\"val\"].append(run_val_metrics)\n\n    # test predictions\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().numpy().tolist())\n    exp[\"predictions\"].append(np.array(preds))\n\n# convert lists to arrays\nexp = experiment_data[\"weight_decay\"][\"synthetic\"]\nexp[\"decays\"] = np.array(exp[\"decays\"])\nexp[\"metrics\"][\"train\"] = np.array(exp[\"metrics\"][\"train\"])\nexp[\"metrics\"][\"val\"] = np.array(exp[\"metrics\"][\"val\"])\nexp[\"losses\"][\"train\"] = np.array(exp[\"losses\"][\"train\"])\nexp[\"losses\"][\"val\"] = np.array(exp[\"losses\"][\"val\"])\nexp[\"predictions\"] = np.stack(exp[\"predictions\"], axis=0)\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Synthetic data\nnp.random.seed(0)\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders (reused across runs)\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# k-means\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init].copy()\n    labels = np.zeros(N, int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(2)\n        new_labels = dists.argmin(1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# Model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation\ncriterion = nn.CrossEntropyLoss(reduction=\"none\")\n\n\ndef evaluate(loader, model):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    model.eval()\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            losses = criterion(out, batch[\"label\"])\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in [0, 1]:\n                mask = batch[\"group\"] == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == batch[\"label\"][mask]).sum().item()\n    wg = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in [0, 1])\n    return loss_sum, wg\n\n\n# Hyperparameter tuning\nhyperparams = [5, 10, 20]\nexperiment_data = {\"train_epochs\": {}}\nwarmup_epochs = 1\n\nfor te in hyperparams:\n    # init\n    model = MLP(d + 1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    sample_weights = None\n    losses_train, losses_val = [], []\n    metrics_train, metrics_val = [], []\n\n    total_epochs = warmup_epochs + te\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            losses = criterion(out, batch[\"label\"])\n            if epoch >= warmup_epochs and sample_weights is not None:\n                sw = sample_weights[batch[\"idx\"]]\n                loss = (losses * sw).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # clustering step\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                sample = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out = model(sample[\"features\"])\n                loss_i = criterion(out, sample[\"label\"]).mean()\n                loss_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labs = kmeans_np(grads, n_clusters=2, n_iters=10)\n            cnts = np.bincount(labs, minlength=2)\n            sw_arr = np.array([1.0 / cnts[l] for l in labs], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # evaluate\n        tr_loss, tr_wg = evaluate(train_loader, model)\n        va_loss, va_wg = evaluate(val_loader, model)\n        losses_train.append(tr_loss / len(train_ds))\n        losses_val.append(va_loss / len(val_ds))\n        metrics_train.append(tr_wg)\n        metrics_val.append(va_wg)\n        print(f\"te={te} epoch={epoch} val_loss={va_loss:.4f} val_wg={va_wg:.4f}\")\n\n    # final test\n    preds, truths = [], []\n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().numpy().tolist())\n            truths.extend(batch[\"label\"].cpu().numpy().tolist())\n\n    experiment_data[\"train_epochs\"][str(te)] = {\n        \"synthetic\": {\n            \"metrics\": {\"train\": np.array(metrics_train), \"val\": np.array(metrics_val)},\n            \"losses\": {\"train\": np.array(losses_train), \"val\": np.array(losses_val)},\n            \"predictions\": np.array(preds),\n            \"ground_truth\": np.array(truths),\n        }\n    }\n\n# Save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Create synthetic dataset with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {}\nsplits[\"train\"] = (X_norm[train_idx], y[train_idx], s[train_idx])\nsplits[\"val\"] = (X_norm[val_idx], y[val_idx], s[val_idx])\nsplits[\"test\"] = (X_norm[test_idx], y[test_idx], s[test_idx])\n\n\n# Dataset class\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# Loaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# Simple NumPy k-means\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# Model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation\ncriterion = nn.CrossEntropyLoss(reduction=\"none\")\n\n\ndef evaluate(loader):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in [0, 1]:\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in [0, 1])\n    return loss_sum, wg_acc\n\n\n# Hyperparameter tuning for warmup_epochs\nwarmup_list = [1, 2, 3, 5]\ntrain_epochs = 5\nexperiment_data = {\n    \"warmup_epochs\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": splits[\"test\"][1],\n            \"warmup_values\": warmup_list,\n        }\n    }\n}\n\n# Loop over hyperparams\nfor w in warmup_list:\n    # re-seed for reproducible init\n    torch.manual_seed(0)\n    model = MLP(d + 1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    sample_weights = None\n\n    run_train_losses = []\n    run_val_losses = []\n    run_train_metrics = []\n    run_val_metrics = []\n\n    total_epochs = w + train_epochs\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= w and sample_weights is not None:\n                sw = sample_weights[idxb]\n                loss = (losses * sw).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # cluster and compute weights\n        if epoch == w - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out = model(batch[\"features\"])\n                l_i = criterion(out, batch[\"label\"]).mean()\n                l_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # evaluate\n        tr_l, tr_m = evaluate(train_loader)\n        vl_l, vl_m = evaluate(val_loader)\n        run_train_losses.append(tr_l / len(train_ds))\n        run_val_losses.append(vl_l / len(val_ds))\n        run_train_metrics.append(tr_m)\n        run_val_metrics.append(vl_m)\n        print(\n            f\"warmup={w} epoch={epoch}: val_loss={vl_l/len(val_ds):.4f} wg_val={vl_m:.4f}\"\n        )\n\n    # test predictions\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().numpy().tolist())\n\n    # store results\n    exp = experiment_data[\"warmup_epochs\"][\"synthetic\"]\n    exp[\"losses\"][\"train\"].append(run_train_losses)\n    exp[\"losses\"][\"val\"].append(run_val_losses)\n    exp[\"metrics\"][\"train\"].append(run_train_metrics)\n    exp[\"metrics\"][\"val\"].append(run_val_metrics)\n    exp[\"predictions\"].append(np.array(preds))\n\n# Save all data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# reproducibility and working dir\nnp.random.seed(0)\ntorch.manual_seed(0)\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic dataset creation\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# splits\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\n# dataset and loaders\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# simple numpy k-means\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# evaluation helper\ncriterion = nn.CrossEntropyLoss(reduction=\"none\")\n\n\ndef evaluate(loader):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in [0, 1]:\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in [0, 1])\n    return loss_sum, wg_acc\n\n\n# hyperparameter tuning\nhidden_dims = [16, 32, 64, 128]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\nexperiment_data = {\n    \"hidden_dim\": {\n        \"synthetic\": {\n            \"hidden_dims\": hidden_dims,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor hid in hidden_dims:\n    # init model, optimizer\n    model = MLP(d + 1, hid).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    sample_weights = None\n\n    train_wg_list, val_wg_list = [], []\n    train_loss_list, val_loss_list = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                sw = sample_weights[idxb]\n                loss = (losses * sw).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        # cluster gradients after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out = model(batch[\"features\"])\n                loss_i = criterion(out, batch[\"label\"]).mean()\n                loss_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # evaluate\n        tr_l, tr_wg = evaluate(train_loader)\n        val_l, val_wg = evaluate(val_loader)\n        train_wg_list.append(tr_wg)\n        val_wg_list.append(val_wg)\n        train_loss_list.append(tr_l / len(train_ds))\n        val_loss_list.append(val_l / len(val_ds))\n        print(f\"hid={hid} epoch={epoch} val_loss={val_l:.4f} val_wg={val_wg:.4f}\")\n\n    # test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().numpy().tolist())\n            truths.extend(batch[\"label\"].cpu().numpy().tolist())\n\n    # record\n    sd = experiment_data[\"hidden_dim\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(train_wg_list)\n    sd[\"metrics\"][\"val\"].append(val_wg_list)\n    sd[\"losses\"][\"train\"].append(train_loss_list)\n    sd[\"losses\"][\"val\"].append(val_loss_list)\n    sd[\"predictions\"].append(preds)\n    if not sd[\"ground_truth\"]:\n        sd[\"ground_truth\"] = truths\n\n# convert to numpy arrays\nsd = experiment_data[\"hidden_dim\"][\"synthetic\"]\nsd[\"hidden_dims\"] = np.array(sd[\"hidden_dims\"])\nsd[\"metrics\"][\"train\"] = np.array(sd[\"metrics\"][\"train\"])\nsd[\"metrics\"][\"val\"] = np.array(sd[\"metrics\"][\"val\"])\nsd[\"losses\"][\"train\"] = np.array(sd[\"losses\"][\"train\"])\nsd[\"losses\"][\"val\"] = np.array(sd[\"losses\"][\"val\"])\nsd[\"predictions\"] = np.array(sd[\"predictions\"])\nsd[\"ground_truth\"] = np.array(sd[\"ground_truth\"])\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset\n\n# reproducibility and working dir\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# simple numpy k-means\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# evaluation helper (returns avg loss per sample and worst\u2010group accuracy)\ncriterion = nn.CrossEntropyLoss(reduction=\"none\")\n\n\ndef evaluate(loader):\n    total_loss, total_samples = 0.0, 0\n    correct = {}\n    counts = {}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            total_loss += losses.sum().item()\n            total_samples += yb.size(0)\n            preds = out.argmax(1)\n            for g in torch.unique(grp).cpu().numpy().tolist():\n                mask = grp == g\n                correct[g] = correct.get(g, 0) + int(\n                    (preds[mask] == yb[mask]).sum().item()\n                )\n                counts[g] = counts.get(g, 0) + int(mask.sum().item())\n    avg_loss = total_loss / total_samples if total_samples > 0 else 0.0\n    wg_acc = min(correct[g] / counts[g] for g in counts) if counts else 0.0\n    return avg_loss, wg_acc\n\n\n# dataset wrapper\nclass TabularDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid_dim, num_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid_dim),\n            nn.ReLU(),\n            nn.Linear(hid_dim, hid_dim),\n            nn.ReLU(),\n        )\n        self.fc = nn.Linear(hid_dim, num_classes)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# 1) Synthetic data\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nsp_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < sp_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\nmean, std = X[train_idx, :d].mean(0), X[train_idx, :d].std(0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\nspl_synth = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\n# 2) HuggingFace image datasets (MNIST & Fashion-MNIST)\ndef load_hf_image(name):\n    ds = load_dataset(name)\n    train_val = ds[\"train\"].train_test_split(test_size=0.2, seed=0)\n\n    def to_arr(sub):\n        Xl, yl, gl = [], [], []\n        for ex in sub:\n            img = np.array(ex[\"image\"], dtype=np.float32) / 255.0\n            Xl.append(img.flatten())\n            yl.append(int(ex[\"label\"]))\n            gl.append(0)\n        return np.stack(Xl), np.array(yl), np.array(gl)\n\n    return {\n        \"train\": to_arr(train_val[\"train\"]),\n        \"val\": to_arr(train_val[\"test\"]),\n        \"test\": to_arr(ds[\"test\"]),\n    }\n\n\nspl_mnist = load_hf_image(\"mnist\")\nspl_fashion = load_hf_image(\"fashion_mnist\")\n\n# aggregate\nall_splits = {\"synthetic\": spl_synth, \"mnist\": spl_mnist, \"fashion_mnist\": spl_fashion}\n\n# hyperparameters\nhidden_dim = 64\nlr = 5e-4\nbatch_size = 128\nwarmup_epochs = 5\ntrain_epochs = 15\ntotal_epochs = warmup_epochs + train_epochs\n\n# run experiments\nexperiment_data = {}\nfor name, splits in all_splits.items():\n    Xtr, ytr, gtr = splits[\"train\"]\n    Xval, yval, gval = splits[\"val\"]\n    Xte, yte, gte = splits[\"test\"]\n    inp_dim = Xtr.shape[1]\n    num_classes = int(ytr.max()) + 1\n\n    train_ds = TabularDataset(Xtr, ytr, gtr)\n    val_ds = TabularDataset(Xval, yval, gval)\n    test_ds = TabularDataset(Xte, yte, gte)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size * 2, shuffle=False)\n    test_loader = DataLoader(test_ds, batch_size=batch_size * 2, shuffle=False)\n    cluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n    model = MLP(inp_dim, hidden_dim, num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    sw = None\n    train_losses, val_losses = [], []\n    train_wgs, val_wgs = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sw is not None:\n                loss = (losses * sw[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # cluster after warmup\n        if epoch == warmup_epochs:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device)\n                    for k, v in sample.items()\n                    if isinstance(v, torch.Tensor)\n                }\n                optimizer.zero_grad()\n                out = model(batch[\"features\"])\n                li = criterion(out, batch[\"label\"]).mean()\n                li.backward()\n                grads.append(model.fc.weight.grad.detach().cpu().view(-1).numpy())\n            G = np.stack(grads)\n            labs = kmeans_np(G, n_clusters=2, n_iters=10)\n            cnt = np.bincount(labs, minlength=2)\n            sw_arr = np.array([1.0 / cnt[l] for l in labs], dtype=np.float32)\n            sw = torch.tensor(sw_arr, device=device)\n\n        # evaluation\n        tr_loss, tr_wg = evaluate(train_loader)\n        val_loss, val_wg = evaluate(val_loader)\n        train_losses.append(tr_loss)\n        val_losses.append(val_loss)\n        train_wgs.append(tr_wg)\n        val_wgs.append(val_wg)\n        print(\n            f\"{name} epoch={epoch}: train_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_wg={val_wg:.4f}\"\n        )\n\n    # test\n    model.eval()\n    preds, truths = [], []\n    for batch in test_loader:\n        batch = {\n            k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n        }\n        out = model(batch[\"features\"])\n        preds.extend(out.argmax(1).cpu().tolist())\n        truths.extend(batch[\"label\"].cpu().tolist())\n\n    experiment_data[name] = {\n        \"metrics\": {\"train\": np.array(train_wgs), \"val\": np.array(val_wgs)},\n        \"losses\": {\"train\": np.array(train_losses), \"val\": np.array(val_losses)},\n        \"predictions\": np.array(preds),\n        \"ground_truth\": np.array(truths),\n    }\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Setup working directory and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Generate synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Shuffle and split\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits dict\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\n# Dataset and loaders\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# K-means for gradient clustering\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP with configurable dropout\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32, dropout_rate=0.0):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(hid, hid),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper (returns sum of loss, worst-group accuracy)\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    model.eval()\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in [0, 1]:\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in [0, 1])\n    return loss_sum, wg_acc\n\n\n# Hyperparameter sweep over dropout rates\ndropout_rates = [0.0, 0.1, 0.2, 0.3, 0.5]\nexperiment_data = {\"dropout_rate\": {\"synthetic\": {}}}\n\nfor dr in dropout_rates:\n    print(f\"==> Running dropout_rate = {dr}\")\n    # prepare storage\n    results = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    experiment_data[\"dropout_rate\"][\"synthetic\"][str(dr)] = results\n\n    # Initialize model, loss, optimizer\n    model = MLP(d + 1, hid=32, dropout_rate=dr).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    sample_weights = None\n\n    # Training schedule\n    warmup_epochs = 1\n    train_epochs = 5\n    total_epochs = warmup_epochs + train_epochs\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # Cluster gradients after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out = model(batch[\"features\"])\n                li = criterion(out, batch[\"label\"]).mean()\n                li.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[l] for l in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # Evaluate\n        train_loss, train_wg = evaluate(train_loader, model, criterion)\n        val_loss, val_wg = evaluate(val_loader, model, criterion)\n        results[\"losses\"][\"train\"].append(train_loss / len(train_ds))\n        results[\"losses\"][\"val\"].append(val_loss / len(val_ds))\n        results[\"metrics\"][\"train\"].append(train_wg)\n        results[\"metrics\"][\"val\"].append(val_wg)\n        print(\n            f\"dropout={dr} epoch {epoch}: val_loss={val_loss:.4f}, val_wg={val_wg:.4f}\"\n        )\n\n    # Final test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().numpy().tolist())\n            truths.extend(batch[\"label\"].cpu().numpy().tolist())\n    results[\"predictions\"] = np.array(preds)\n    results[\"ground_truth\"] = np.array(truths)\n\n# Save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Setup working directory and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Seed\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Create synthetic dataset\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\ns = np.where(np.random.rand(N) < 0.95, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\n# Dataset and DataLoader\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# Simple NumPy k-means\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    centroids = X[rng.choice(N, n_clusters, replace=False)].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(2)\n        new_labels = dists.argmin(1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# Model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper\ndef evaluate(loader, model, criterion):\n    model.eval()\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in [0, 1]:\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in [0, 1])\n    return loss_sum, wg_acc\n\n\n# Hyperparameter tuning: number of clusters\nparam_grid = [2, 3, 4, 5]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\nexperiment_data = {\n    \"n_clusters_tuning\": {\n        \"synthetic\": {\n            \"n_clusters\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor nc in param_grid:\n    print(f\"Running n_clusters = {nc}\")\n    # re-init model & optimizer\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    sample_weights = None\n\n    run_m_train, run_m_val = [], []\n    run_l_train, run_l_val = [], []\n\n    # Training loop\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # cluster gradients after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out = model(batch[\"features\"])\n                loss_i = criterion(out, batch[\"label\"]).mean()\n                loss_i.backward()\n                grads.append(model.fc.weight.grad.detach().cpu().view(-1).numpy())\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=nc, n_iters=10)\n            counts = np.bincount(labels, minlength=nc)\n            sw_arr = np.array([1.0 / counts[l] for l in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # evaluation\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        val_loss, val_wg = evaluate(val_loader, model, criterion)\n        print(f\" n={nc} Epoch {epoch}: val_wg={val_wg:.4f}\")\n        run_l_train.append(tr_loss / len(train_ds))\n        run_l_val.append(val_loss / len(val_ds))\n        run_m_train.append(tr_wg)\n        run_m_val.append(val_wg)\n\n    # final test\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().numpy().tolist())\n            truths.extend(batch[\"label\"].cpu().numpy().tolist())\n\n    # record results\n    ed = experiment_data[\"n_clusters_tuning\"][\"synthetic\"]\n    ed[\"n_clusters\"].append(nc)\n    ed[\"losses\"][\"train\"].append(run_l_train)\n    ed[\"losses\"][\"val\"].append(run_l_val)\n    ed[\"metrics\"][\"train\"].append(run_m_train)\n    ed[\"metrics\"][\"val\"].append(run_m_val)\n    ed[\"predictions\"].append(np.array(preds))\n    ed[\"ground_truth\"].append(np.array(truths))\n\n# save all data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data\nnp.random.seed(0)\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# splits\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# loaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# simple kmeans\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init].copy()\n    labels = np.zeros(N, int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(2)\n        new = dists.argmin(1)\n        if np.all(new == labels):\n            break\n        labels = new\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# evaluation\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in [0, 1]:\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min((correct[g] / total[g]) if total[g] > 0 else 0.0 for g in [0, 1])\n    return loss_sum, wg_acc\n\n\n# hyperparameter sweep\nsmoothing_vals = [0.0, 0.05, 0.1, 0.2]\nexperiment_data = {\"label_smoothing\": {}}\n\nfor eps in smoothing_vals:\n    key = str(eps)\n    experiment_data[\"label_smoothing\"][key] = {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n    # init model, criterion, optimizer\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(label_smoothing=eps, reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    sample_weights = None\n\n    # training\n    warmup_epochs = 1\n    train_epochs = 5\n    total_epochs = warmup_epochs + train_epochs\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                sw = sample_weights[idxb]\n                loss = (losses * sw).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        # cluster after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out = model(batch[\"features\"])\n                l_i = criterion(out, batch[\"label\"]).mean()\n                l_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n        # evaluate\n        train_loss, train_wg = evaluate(train_loader, model, criterion)\n        val_loss, val_wg = evaluate(val_loader, model, criterion)\n        print(f\"eps={eps} epoch={epoch} val_loss={val_loss:.4f} val_wg={val_wg:.4f}\")\n        data = experiment_data[\"label_smoothing\"][key][\"synthetic\"]\n        data[\"losses\"][\"train\"].append(train_loss / len(train_ds))\n        data[\"losses\"][\"val\"].append(val_loss / len(val_ds))\n        data[\"metrics\"][\"train\"].append(train_wg)\n        data[\"metrics\"][\"val\"].append(val_wg)\n\n    # final test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().numpy().tolist())\n            truths.extend(batch[\"label\"].cpu().numpy().tolist())\n    data = experiment_data[\"label_smoothing\"][key][\"synthetic\"]\n    data[\"predictions\"] = np.array(preds)\n    data[\"ground_truth\"] = np.array(truths)\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Synthetic dataset creation\nnp.random.seed(0)\ntorch.manual_seed(0)\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Train/val/test split\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# K-means helper\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# Model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation\ncriterion = nn.CrossEntropyLoss(reduction=\"none\")\n\n\ndef evaluate(loader, model):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in [0, 1]:\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in [0, 1])\n    return loss_sum, wg\n\n\n# Hyperparameter tuning over momentum\nmomentums = [0.5, 0.9, 0.99]\nwarmup_epochs, train_epochs = 1, 5\ntotal_epochs = warmup_epochs + train_epochs\n\nexperiment_data = {\n    \"momentum_sweep\": {\n        \"synthetic\": {\n            \"momentum_values\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor m in momentums:\n    print(f\"\\n>>> Running momentum = {m}\")\n    experiment_data[\"momentum_sweep\"][\"synthetic\"][\"momentum_values\"].append(m)\n    # init model, optimizer\n    model = MLP(d + 1).to(device)\n    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=m)\n    sample_weights = None\n\n    # histories\n    train_wg_hist, val_wg_hist = [], []\n    train_loss_hist, val_loss_hist = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                sw = sample_weights[idxb]\n                loss = (losses * sw).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        # clustering step after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out = model(batch[\"features\"])\n                loss_i = criterion(out, batch[\"label\"]).mean()\n                loss_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[l] for l in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # evaluate\n        tr_loss, tr_wg = evaluate(train_loader, model)\n        val_loss, val_wg = evaluate(val_loader, model)\n        train_wg_hist.append(tr_wg)\n        val_wg_hist.append(val_wg)\n        train_loss_hist.append(tr_loss / len(train_ds))\n        val_loss_hist.append(val_loss / len(val_ds))\n        print(\n            f\"momentum={m} epoch={epoch} val_loss={val_loss/len(val_ds):.4f} val_wg={val_wg:.4f}\"\n        )\n\n    # save histories\n    experiment_data[\"momentum_sweep\"][\"synthetic\"][\"metrics\"][\"train\"].append(\n        train_wg_hist\n    )\n    experiment_data[\"momentum_sweep\"][\"synthetic\"][\"metrics\"][\"val\"].append(val_wg_hist)\n    experiment_data[\"momentum_sweep\"][\"synthetic\"][\"losses\"][\"train\"].append(\n        train_loss_hist\n    )\n    experiment_data[\"momentum_sweep\"][\"synthetic\"][\"losses\"][\"val\"].append(\n        val_loss_hist\n    )\n\n    # final test\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().numpy().tolist())\n            truths.extend(batch[\"label\"].cpu().numpy().tolist())\n    experiment_data[\"momentum_sweep\"][\"synthetic\"][\"predictions\"].append(\n        np.array(preds)\n    )\n    experiment_data[\"momentum_sweep\"][\"synthetic\"][\"ground_truth\"].append(\n        np.array(truths)\n    )\n\n# Save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils import clip_grad_norm_\n\n# Setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Synthetic data\nnp.random.seed(0)\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\nmean = X[train_idx, :d].mean(0)\nstd = X[train_idx, :d].std(0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(2)\n        new = np.argmin(dists, 1)\n        if np.all(new == labels):\n            break\n        labels = new\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\ncriterion = nn.CrossEntropyLoss(reduction=\"none\")\n\n\ndef evaluate(loader):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            ls = criterion(out, yb)\n            loss_sum += ls.sum().item()\n            preds = out.argmax(1)\n            for g in [0, 1]:\n                m = grp == g\n                total[g] += m.sum().item()\n                if m.sum().item() > 0:\n                    correct[g] += (preds[m] == yb[m]).sum().item()\n    wg = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in [0, 1])\n    return loss_sum, wg\n\n\n# Hyperparameter sweep\nwarmup_epochs, train_epochs = 1, 5\ntotal_epochs = warmup_epochs + train_epochs\nmax_norms = [1.0, 5.0, 10.0]\nexperiment_data = {\"max_grad_norm\": {}}\n\nfor norm in max_norms:\n    torch.manual_seed(0)\n    model = MLP(d + 1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    sample_weights = None\n    key = str(norm)\n    experiment_data[\"max_grad_norm\"][key] = {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n    rec = experiment_data[\"max_grad_norm\"][key][\"synthetic\"]\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            ls = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (ls * sample_weights[idxb]).mean()\n            else:\n                loss = ls.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            clip_grad_norm_(model.parameters(), norm)\n            optimizer.step()\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out = model(batch[\"features\"])\n                loss_i = criterion(out, batch[\"label\"]).mean()\n                loss_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw = np.array([1.0 / counts[l] for l in labels], np.float32)\n            sample_weights = torch.tensor(sw, device=device)\n        tr_loss, tr_wg = evaluate(train_loader)\n        val_loss, val_wg = evaluate(val_loader)\n        rec[\"losses\"][\"train\"].append(tr_loss / len(train_ds))\n        rec[\"losses\"][\"val\"].append(val_loss / len(val_ds))\n        rec[\"metrics\"][\"train\"].append(tr_wg)\n        rec[\"metrics\"][\"val\"].append(val_wg)\n\n    # Test predictions\n    preds, truths = [], []\n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n    rec[\"predictions\"] = np.array(preds)\n    rec[\"ground_truth\"] = np.array(truths)\n\n# Save all\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# k-means on CPU\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper: returns (sum_loss, worst-group accuracy)\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameter sweep over learning rates\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# Containers for results\nmetrics_train_all = []\nmetrics_val_all = []\nlosses_train_all = []\nlosses_val_all = []\npredictions_list = []\nground_truth = None\n\nfor lr in lrs:\n    print(f\"\\n=== Training with learning rate = {lr} ===\")\n    # re-init model, loss, optimizer\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    # per-epoch logs\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # cluster-based reweighting after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out_i = model(batch[\"features\"])\n                loss_i = criterion(out_i, batch[\"label\"]).mean()\n                loss_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # evaluate after epoch\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(\n            f\"lr={lr} epoch={epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n        )\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # final test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n\n    preds_npy = np.array(preds)\n    truths_npy = np.array(truths)\n    predictions_list.append(preds_npy)\n    if ground_truth is None:\n        ground_truth = truths_npy\n\n    metrics_train_all.append(m_tr)\n    metrics_val_all.append(m_val)\n    losses_train_all.append(l_tr)\n    losses_val_all.append(l_val)\n\n# Assemble and save experiment data\nexperiment_data = {\n    \"learning_rate\": {\n        \"synthetic\": {\n            \"lrs\": np.array(lrs),\n            \"metrics\": {\n                \"train\": np.array(metrics_train_all),\n                \"val\": np.array(metrics_val_all),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_all),\n                \"val\": np.array(losses_val_all),\n            },\n            \"predictions\": np.stack(predictions_list),\n            \"ground_truth\": ground_truth,\n        }\n    }\n}\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# k-means on CPU\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper: returns (sum_loss, worst-group accuracy)\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameter sweep over learning rates\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# Containers for results\nmetrics_train_all = []\nmetrics_val_all = []\nlosses_train_all = []\nlosses_val_all = []\npredictions_list = []\nground_truth = None\n\nfor lr in lrs:\n    print(f\"\\n=== Training with learning rate = {lr} ===\")\n    # re-init model, loss, optimizer\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    # per-epoch logs\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # cluster-based reweighting after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out_i = model(batch[\"features\"])\n                loss_i = criterion(out_i, batch[\"label\"]).mean()\n                loss_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # evaluate after epoch\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(\n            f\"lr={lr} epoch={epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n        )\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # final test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n\n    preds_npy = np.array(preds)\n    truths_npy = np.array(truths)\n    predictions_list.append(preds_npy)\n    if ground_truth is None:\n        ground_truth = truths_npy\n\n    metrics_train_all.append(m_tr)\n    metrics_val_all.append(m_val)\n    losses_train_all.append(l_tr)\n    losses_val_all.append(l_val)\n\n# Assemble and save experiment data\nexperiment_data = {\n    \"learning_rate\": {\n        \"synthetic\": {\n            \"lrs\": np.array(lrs),\n            \"metrics\": {\n                \"train\": np.array(metrics_train_all),\n                \"val\": np.array(metrics_val_all),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_all),\n                \"val\": np.array(losses_val_all),\n            },\n            \"predictions\": np.stack(predictions_list),\n            \"ground_truth\": ground_truth,\n        }\n    }\n}\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# k-means on CPU\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper: returns (sum_loss, worst-group accuracy)\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameter sweep over learning rates\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# Containers for results\nmetrics_train_all = []\nmetrics_val_all = []\nlosses_train_all = []\nlosses_val_all = []\npredictions_list = []\nground_truth = None\n\nfor lr in lrs:\n    print(f\"\\n=== Training with learning rate = {lr} ===\")\n    # re-init model, loss, optimizer\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    # per-epoch logs\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # cluster-based reweighting after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out_i = model(batch[\"features\"])\n                loss_i = criterion(out_i, batch[\"label\"]).mean()\n                loss_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # evaluate after epoch\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(\n            f\"lr={lr} epoch={epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n        )\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # final test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n\n    preds_npy = np.array(preds)\n    truths_npy = np.array(truths)\n    predictions_list.append(preds_npy)\n    if ground_truth is None:\n        ground_truth = truths_npy\n\n    metrics_train_all.append(m_tr)\n    metrics_val_all.append(m_val)\n    losses_train_all.append(l_tr)\n    losses_val_all.append(l_val)\n\n# Assemble and save experiment data\nexperiment_data = {\n    \"learning_rate\": {\n        \"synthetic\": {\n            \"lrs\": np.array(lrs),\n            \"metrics\": {\n                \"train\": np.array(metrics_train_all),\n                \"val\": np.array(metrics_val_all),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_all),\n                \"val\": np.array(losses_val_all),\n            },\n            \"predictions\": np.stack(predictions_list),\n            \"ground_truth\": ground_truth,\n        }\n    }\n}\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 268.8303', '\\n', 'Epoch\n1: validation_loss = 235.9364', '\\n', 'Epoch 2: validation_loss = 228.1427',\n'\\n', 'Epoch 3: validation_loss = 226.1110', '\\n', 'Epoch 4: validation_loss =\n225.3080', '\\n', 'Epoch 5: validation_loss = 224.7263', '\\n', 'Execution time: 3\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Training with learning rate = 0.0001 ===',\n'\\n', 'lr=0.0001 epoch=0: val_loss=0.6977, val_wg=0.3168', '\\n', 'lr=0.0001\nepoch=1: val_loss=0.6917, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=2:\nval_loss=0.6904, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=3: val_loss=0.6900,\nval_wg=0.3588', '\\n', 'lr=0.0001 epoch=4: val_loss=0.6899, val_wg=0.3588', '\\n',\n'lr=0.0001 epoch=5: val_loss=0.6898, val_wg=0.3588', '\\n', '\\n=== Training with\nlearning rate = 0.001 ===', '\\n', 'lr=0.001 epoch=0: val_loss=0.5822,\nval_wg=0.8866', '\\n', 'lr=0.001 epoch=1: val_loss=0.5093, val_wg=0.9538', '\\n',\n'lr=0.001 epoch=2: val_loss=0.4929, val_wg=0.9622', '\\n', 'lr=0.001 epoch=3:\nval_loss=0.4887, val_wg=0.9622', '\\n', 'lr=0.001 epoch=4: val_loss=0.4872,\nval_wg=0.9622', '\\n', 'lr=0.001 epoch=5: val_loss=0.4860, val_wg=0.9622', '\\n',\n'\\n=== Training with learning rate = 0.01 ===', '\\n', 'lr=0.01 epoch=0:\nval_loss=0.0228, val_wg=0.9885', '\\n', 'lr=0.01 epoch=1: val_loss=0.0305,\nval_wg=0.9885', '\\n', 'lr=0.01 epoch=2: val_loss=0.0348, val_wg=0.9885', '\\n',\n'lr=0.01 epoch=3: val_loss=0.0380, val_wg=0.9885', '\\n', 'lr=0.01 epoch=4:\nval_loss=0.0405, val_wg=0.9885', '\\n', 'lr=0.01 epoch=5: val_loss=0.0430,\nval_wg=0.9924', '\\n', 'Execution time: 6 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', '\\n=== Batch size: 32 ===', '\\n', 'Epoch 0:\nval_loss=0.4601, val_wg_acc=0.9885', '\\n', 'Epoch 1: val_loss=0.3698,\nval_wg_acc=0.9885', '\\n', 'Epoch 2: val_loss=0.3645, val_wg_acc=0.9885', '\\n',\n'Epoch 3: val_loss=0.3629, val_wg_acc=0.9885', '\\n', 'Epoch 4: val_loss=0.3612,\nval_wg_acc=0.9885', '\\n', 'Epoch 5: val_loss=0.3593, val_wg_acc=0.9885', '\\n',\n'\\n=== Batch size: 64 ===', '\\n', 'Epoch 0: val_loss=0.5822, val_wg_acc=0.8866',\n'\\n', 'Epoch 1: val_loss=0.5093, val_wg_acc=0.9538', '\\n', 'Epoch 2:\nval_loss=0.4929, val_wg_acc=0.9622', '\\n', 'Epoch 3: val_loss=0.4887,\nval_wg_acc=0.9622', '\\n', 'Epoch 4: val_loss=0.4872, val_wg_acc=0.9622', '\\n',\n'Epoch 5: val_loss=0.4860, val_wg_acc=0.9622', '\\n', '\\n=== Batch size: 128\n===', '\\n', 'Epoch 0: val_loss=0.6077, val_wg_acc=0.9656', '\\n', 'Epoch 1:\nval_loss=0.5690, val_wg_acc=0.9695', '\\n', 'Epoch 2: val_loss=0.5515,\nval_wg_acc=0.9733', '\\n', 'Epoch 3: val_loss=0.5430, val_wg_acc=0.9771', '\\n',\n'Epoch 4: val_loss=0.5388, val_wg_acc=0.9771', '\\n', 'Epoch 5: val_loss=0.5366,\nval_wg_acc=0.9771', '\\n', '\\n=== Batch size: 256 ===', '\\n', 'Epoch 0:\nval_loss=0.6813, val_wg_acc=0.0714', '\\n', 'Epoch 1: val_loss=0.6627,\nval_wg_acc=0.1303', '\\n', 'Epoch 2: val_loss=0.6515, val_wg_acc=0.2185', '\\n',\n'Epoch 3: val_loss=0.6442, val_wg_acc=0.3193', '\\n', 'Epoch 4: val_loss=0.6392,\nval_wg_acc=0.4034', '\\n', 'Epoch 5: val_loss=0.6358, val_wg_acc=0.4370', '\\n',\n'\\nSaved experiment data to /data/chenhui/AI-Scientist-v2/experiments/2025-06-\n06_23-36-12_gradient_cluster_robust_attempt_0/0-run/process_ForkProcess-\n6/working/experiment_data.npy', '\\n', 'Execution time: 10 seconds seconds (time\nlimit is an hour).']", "['Execution time: 10 seconds seconds (time limit is an hour).']", "['te=5 epoch=0 val_loss=268.8303 val_wg=0.9874', '\\n', 'te=5 epoch=1\nval_loss=235.9364 val_wg=0.9916', '\\n', 'te=5 epoch=2 val_loss=228.1427\nval_wg=0.9916', '\\n', 'te=5 epoch=3 val_loss=226.1110 val_wg=0.9916', '\\n',\n'te=5 epoch=4 val_loss=225.3080 val_wg=0.9916', '\\n', 'te=5 epoch=5\nval_loss=224.7263 val_wg=0.9916', '\\n', 'te=10 epoch=0 val_loss=277.6081\nval_wg=0.9733', '\\n', 'te=10 epoch=1 val_loss=242.7494 val_wg=0.9809', '\\n',\n'te=10 epoch=2 val_loss=235.1209 val_wg=0.9809', '\\n', 'te=10 epoch=3\nval_loss=233.1975 val_wg=0.9809', '\\n', 'te=10 epoch=4 val_loss=232.4831\nval_wg=0.9809', '\\n', 'te=10 epoch=5 val_loss=231.9871 val_wg=0.9809', '\\n',\n'te=10 epoch=6 val_loss=231.5054 val_wg=0.9809', '\\n', 'te=10 epoch=7\nval_loss=230.9964 val_wg=0.9809', '\\n', 'te=10 epoch=8 val_loss=230.4484\nval_wg=0.9809', '\\n', 'te=10 epoch=9 val_loss=229.8754 val_wg=0.9809', '\\n',\n'te=10 epoch=10 val_loss=229.2659 val_wg=0.9809', '\\n', 'te=20 epoch=0\nval_loss=280.3285 val_wg=0.9160', '\\n', 'te=20 epoch=1 val_loss=247.7662\nval_wg=0.9664', '\\n', 'te=20 epoch=2 val_loss=239.9960 val_wg=0.9664', '\\n',\n'te=20 epoch=3 val_loss=237.9687 val_wg=0.9664', '\\n', 'te=20 epoch=4\nval_loss=237.1683 val_wg=0.9664', '\\n', 'te=20 epoch=5 val_loss=236.5894\nval_wg=0.9664', '\\n', 'te=20 epoch=6 val_loss=236.0168 val_wg=0.9664', '\\n',\n'te=20 epoch=7 val_loss=235.4076 val_wg=0.9664', '\\n', 'te=20 epoch=8\nval_loss=234.7608 val_wg=0.9664', '\\n', 'te=20 epoch=9 val_loss=234.0676\nval_wg=0.9664', '\\n', 'te=20 epoch=10 val_loss=233.3333 val_wg=0.9664', '\\n',\n'te=20 epoch=11 val_loss=232.5543 val_wg=0.9664', '\\n', 'te=20 epoch=12\nval_loss=231.7357 val_wg=0.9664', '\\n', 'te=20 epoch=13 val_loss=230.8720\nval_wg=0.9664', '\\n', 'te=20 epoch=14 val_loss=229.9724 val_wg=0.9664', '\\n',\n'te=20 epoch=15 val_loss=229.0193 val_wg=0.9664', '\\n', 'te=20 epoch=16\nval_loss=228.0340 val_wg=0.9664', '\\n', 'te=20 epoch=17 val_loss=226.9978\nval_wg=0.9706', '\\n', 'te=20 epoch=18 val_loss=225.9265 val_wg=0.9706', '\\n',\n'te=20 epoch=19 val_loss=224.7975 val_wg=0.9706', '\\n', 'te=20 epoch=20\nval_loss=223.6269 val_wg=0.9706', '\\n', 'Execution time: 10 seconds seconds\n(time limit is an hour).']", "['Using device: cuda', '\\n', 'warmup=1 epoch=0: val_loss=0.5892 wg_val=0.9847',\n'\\n', 'warmup=1 epoch=1: val_loss=0.5257 wg_val=0.9885', '\\n', 'warmup=1\nepoch=2: val_loss=0.5109 wg_val=0.9885', '\\n', 'warmup=1 epoch=3:\nval_loss=0.5070 wg_val=0.9885', '\\n', 'warmup=1 epoch=4: val_loss=0.5055\nwg_val=0.9885', '\\n', 'warmup=1 epoch=5: val_loss=0.5044 wg_val=0.9885', '\\n',\n'warmup=2 epoch=0: val_loss=0.5892 wg_val=0.9847', '\\n', 'warmup=2 epoch=1:\nval_loss=0.4459 wg_val=0.9885', '\\n', 'warmup=2 epoch=2: val_loss=0.3673\nwg_val=0.9885', '\\n', 'warmup=2 epoch=3: val_loss=0.3500 wg_val=0.9885', '\\n',\n'warmup=2 epoch=4: val_loss=0.3459 wg_val=0.9885', '\\n', 'warmup=2 epoch=5:\nval_loss=0.3445 wg_val=0.9885', '\\n', 'warmup=2 epoch=6: val_loss=0.3436\nwg_val=0.9885', '\\n', 'warmup=3 epoch=0: val_loss=0.5892 wg_val=0.9847', '\\n',\n'warmup=3 epoch=1: val_loss=0.4459 wg_val=0.9885', '\\n', 'warmup=3 epoch=2:\nval_loss=0.2843 wg_val=0.9885', '\\n', 'warmup=3 epoch=3: val_loss=0.2127\nwg_val=0.9885', '\\n', 'warmup=3 epoch=4: val_loss=0.1986 wg_val=0.9885', '\\n',\n'warmup=3 epoch=5: val_loss=0.1955 wg_val=0.9885', '\\n', 'warmup=3 epoch=6:\nval_loss=0.1945 wg_val=0.9885', '\\n', 'warmup=3 epoch=7: val_loss=0.1940\nwg_val=0.9885', '\\n', 'warmup=5 epoch=0: val_loss=0.5892 wg_val=0.9847', '\\n',\n'warmup=5 epoch=1: val_loss=0.4459 wg_val=0.9885', '\\n', 'warmup=5 epoch=2:\nval_loss=0.2843 wg_val=0.9885', '\\n', 'warmup=5 epoch=3: val_loss=0.1545\nwg_val=0.9885', '\\n', 'warmup=5 epoch=4: val_loss=0.0850 wg_val=0.9885', '\\n',\n'warmup=5 epoch=5: val_loss=0.0661 wg_val=0.9885', '\\n', 'warmup=5 epoch=6:\nval_loss=0.0628 wg_val=0.9885', '\\n', 'warmup=5 epoch=7: val_loss=0.0621\nwg_val=0.9885', '\\n', 'warmup=5 epoch=8: val_loss=0.0619 wg_val=0.9885', '\\n',\n'warmup=5 epoch=9: val_loss=0.0618 wg_val=0.9885', '\\n', 'Execution time: 7\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'hid=16 epoch=0 val_loss=317.0969 val_wg=0.9034',\n'\\n', 'hid=16 epoch=1 val_loss=304.2047 val_wg=0.9706', '\\n', 'hid=16 epoch=2\nval_loss=301.0889 val_wg=0.9733', '\\n', 'hid=16 epoch=3 val_loss=300.2692\nval_wg=0.9733', '\\n', 'hid=16 epoch=4 val_loss=299.9468 val_wg=0.9733', '\\n',\n'hid=16 epoch=5 val_loss=299.7124 val_wg=0.9733', '\\n', 'hid=32 epoch=0\nval_loss=281.3835 val_wg=0.8403', '\\n', 'hid=32 epoch=1 val_loss=248.0529\nval_wg=0.9118', '\\n', 'hid=32 epoch=2 val_loss=239.9545 val_wg=0.9286', '\\n',\n'hid=32 epoch=3 val_loss=237.8120 val_wg=0.9286', '\\n', 'hid=32 epoch=4\nval_loss=236.9434 val_wg=0.9286', '\\n', 'hid=32 epoch=5 val_loss=236.2962\nval_wg=0.9286', '\\n', 'hid=64 epoch=0 val_loss=242.1019 val_wg=0.9198', '\\n',\n'hid=64 epoch=1 val_loss=177.1781 val_wg=0.9771', '\\n', 'hid=64 epoch=2\nval_loss=163.2669 val_wg=0.9809', '\\n', 'hid=64 epoch=3 val_loss=159.7151\nval_wg=0.9809', '\\n', 'hid=64 epoch=4 val_loss=158.2812 val_wg=0.9809', '\\n',\n'hid=64 epoch=5 val_loss=157.2021 val_wg=0.9809', '\\n', 'hid=128 epoch=0\nval_loss=78.1716 val_wg=0.9885', '\\n', 'hid=128 epoch=1 val_loss=32.4963\nval_wg=0.9885', '\\n', 'hid=128 epoch=2 val_loss=27.0179 val_wg=0.9885', '\\n',\n'hid=128 epoch=3 val_loss=25.9232 val_wg=0.9847', '\\n', 'hid=128 epoch=4\nval_loss=25.6133 val_wg=0.9847', '\\n', 'hid=128 epoch=5 val_loss=25.4289\nval_wg=0.9847', '\\n', 'Execution time: 10 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'synthetic epoch=0: train_loss=0.5970\nval_loss=0.5994 val_wg=0.9237', '\\n', 'synthetic epoch=1: train_loss=0.5295\nval_loss=0.5307 val_wg=0.9695', '\\n', 'synthetic epoch=2: train_loss=0.4594\nval_loss=0.4602 val_wg=0.9874', '\\n', 'synthetic epoch=3: train_loss=0.3861\nval_loss=0.3873 val_wg=0.9874', '\\n', 'synthetic epoch=4: train_loss=0.3140\nval_loss=0.3158 val_wg=0.9874', '\\n', 'synthetic epoch=5: train_loss=0.2476\nval_loss=0.2504 val_wg=0.9916', '\\n', 'synthetic epoch=6: train_loss=0.2080\nval_loss=0.2114 val_wg=0.9916', '\\n', 'synthetic epoch=7: train_loss=0.1909\nval_loss=0.1946 val_wg=0.9916', '\\n', 'synthetic epoch=8: train_loss=0.1833\nval_loss=0.1872 val_wg=0.9916', '\\n', 'synthetic epoch=9: train_loss=0.1799\nval_loss=0.1838 val_wg=0.9916', '\\n', 'synthetic epoch=10: train_loss=0.1782\nval_loss=0.1822 val_wg=0.9916', '\\n', 'synthetic epoch=11: train_loss=0.1774\nval_loss=0.1814 val_wg=0.9916', '\\n', 'synthetic epoch=12: train_loss=0.1770\nval_loss=0.1810 val_wg=0.9916', '\\n', 'synthetic epoch=13: train_loss=0.1767\nval_loss=0.1807 val_wg=0.9916', '\\n', 'synthetic epoch=14: train_loss=0.1765\nval_loss=0.1805 val_wg=0.9916', '\\n', 'synthetic epoch=15: train_loss=0.1763\nval_loss=0.1803 val_wg=0.9916', '\\n', 'synthetic epoch=16: train_loss=0.1761\nval_loss=0.1801 val_wg=0.9916', '\\n', 'synthetic epoch=17: train_loss=0.1760\nval_loss=0.1799 val_wg=0.9916', '\\n', 'synthetic epoch=18: train_loss=0.1758\nval_loss=0.1798 val_wg=0.9916', '\\n', 'synthetic epoch=19: train_loss=0.1756\nval_loss=0.1796 val_wg=0.9916', '\\n', 'mnist epoch=0: train_loss=0.3051\nval_loss=0.3192 val_wg=0.9073', '\\n', 'mnist epoch=1: train_loss=0.2354\nval_loss=0.2536 val_wg=0.9275', '\\n', 'mnist epoch=2: train_loss=0.2084\nval_loss=0.2293 val_wg=0.9308', '\\n', 'mnist epoch=3: train_loss=0.1761\nval_loss=0.2014 val_wg=0.9408', '\\n', 'mnist epoch=4: train_loss=0.1495\nval_loss=0.1831 val_wg=0.9433', '\\n', 'mnist epoch=5: train_loss=0.1296\nval_loss=0.1694 val_wg=0.9487', '\\n', 'mnist epoch=6: train_loss=0.1530\nval_loss=0.1938 val_wg=0.9407', '\\n', 'mnist epoch=7: train_loss=0.1497\nval_loss=0.1901 val_wg=0.9417', '\\n', 'mnist epoch=8: train_loss=0.1464\nval_loss=0.1863 val_wg=0.9423', '\\n', 'mnist epoch=9: train_loss=0.1432\nval_loss=0.1827 val_wg=0.9427', '\\n', 'mnist epoch=10: train_loss=0.1404\nval_loss=0.1795 val_wg=0.9442', '\\n', 'mnist epoch=11: train_loss=0.1383\nval_loss=0.1770 val_wg=0.9452', '\\n', 'mnist epoch=12: train_loss=0.1371\nval_loss=0.1755 val_wg=0.9453', '\\n', 'mnist epoch=13: train_loss=0.1372\nval_loss=0.1752 val_wg=0.9456', '\\n', 'mnist epoch=14: train_loss=0.1387\nval_loss=0.1765 val_wg=0.9446', '\\n', 'mnist epoch=15: train_loss=0.1417\nval_loss=0.1794 val_wg=0.9439', '\\n', 'mnist epoch=16: train_loss=0.1464\nval_loss=0.1838 val_wg=0.9423', '\\n', 'mnist epoch=17: train_loss=0.1525\nval_loss=0.1897 val_wg=0.9402', '\\n', 'mnist epoch=18: train_loss=0.1601\nval_loss=0.1971 val_wg=0.9381', '\\n', 'mnist epoch=19: train_loss=0.1687\nval_loss=0.2054 val_wg=0.9363', '\\n', 'fashion_mnist epoch=0: train_loss=0.5346\nval_loss=0.5303 val_wg=0.8140', '\\n', 'fashion_mnist epoch=1: train_loss=0.4479\nval_loss=0.4462 val_wg=0.8423', '\\n', 'fashion_mnist epoch=2: train_loss=0.4158\nval_loss=0.4199 val_wg=0.8537', '\\n', 'fashion_mnist epoch=3: train_loss=0.4018\nval_loss=0.4053 val_wg=0.8516', '\\n', 'fashion_mnist epoch=4: train_loss=0.3740\nval_loss=0.3829 val_wg=0.8616', '\\n', 'fashion_mnist epoch=5: train_loss=0.3658\nval_loss=0.3775 val_wg=0.8612', '\\n', 'fashion_mnist epoch=6: train_loss=0.4047\nval_loss=0.4178 val_wg=0.8521', '\\n', 'fashion_mnist epoch=7: train_loss=0.4013\nval_loss=0.4142 val_wg=0.8528', '\\n', 'fashion_mnist epoch=8: train_loss=0.3995\nval_loss=0.4121 val_wg=0.8524', '\\n', 'fashion_mnist epoch=9: train_loss=0.3998\nval_loss=0.4121 val_wg=0.8522', '\\n', 'fashion_mnist epoch=10: train_loss=0.4026\nval_loss=0.4146 val_wg=0.8518', '\\n', 'fashion_mnist epoch=11: train_loss=0.4081\nval_loss=0.4196 val_wg=0.8506', '\\n', 'fashion_mnist epoch=12: train_loss=0.4162\nval_loss=0.4273 val_wg=0.8462', '\\n', 'fashion_mnist epoch=13: train_loss=0.4266\nval_loss=0.4373 val_wg=0.8414', '\\n', 'fashion_mnist epoch=14: train_loss=0.4388\nval_loss=0.4490 val_wg=0.8357', '\\n', 'fashion_mnist epoch=15: train_loss=0.4518\nval_loss=0.4616 val_wg=0.8297', '\\n', 'fashion_mnist epoch=16: train_loss=0.4650\nval_loss=0.4745 val_wg=0.8229', '\\n', 'fashion_mnist epoch=17: train_loss=0.4771\nval_loss=0.4864 val_wg=0.8177', '\\n', 'fashion_mnist epoch=18: train_loss=0.4879\nval_loss=0.4970 val_wg=0.8145', '\\n', 'fashion_mnist epoch=19: train_loss=0.4967\nval_loss=0.5057 val_wg=0.8109', '\\n', 'Execution time: 5 minutes seconds (time\nlimit is an hour).']", "['==> Running dropout_rate = 0.0', '\\n', 'dropout=0.0 epoch 0:\nval_loss=294.6205, val_wg=0.9847', '\\n', 'dropout=0.0 epoch 1:\nval_loss=262.8275, val_wg=0.9885', '\\n', 'dropout=0.0 epoch 2:\nval_loss=255.4357, val_wg=0.9885', '\\n', 'dropout=0.0 epoch 3:\nval_loss=253.5172, val_wg=0.9885', '\\n', 'dropout=0.0 epoch 4:\nval_loss=252.7590, val_wg=0.9885', '\\n', 'dropout=0.0 epoch 5:\nval_loss=252.2090, val_wg=0.9885', '\\n', '==> Running dropout_rate = 0.1', '\\n',\n'dropout=0.1 epoch 0: val_loss=292.4121, val_wg=0.8824', '\\n', 'dropout=0.1\nepoch 1: val_loss=257.0324, val_wg=0.9538', '\\n', 'dropout=0.1 epoch 2:\nval_loss=249.0882, val_wg=0.9580', '\\n', 'dropout=0.1 epoch 3:\nval_loss=247.0766, val_wg=0.9580', '\\n', 'dropout=0.1 epoch 4:\nval_loss=246.3125, val_wg=0.9580', '\\n', 'dropout=0.1 epoch 5:\nval_loss=245.7773, val_wg=0.9580', '\\n', '==> Running dropout_rate = 0.2', '\\n',\n'dropout=0.2 epoch 0: val_loss=274.5359, val_wg=0.9809', '\\n', 'dropout=0.2\nepoch 1: val_loss=243.0470, val_wg=0.9809', '\\n', 'dropout=0.2 epoch 2:\nval_loss=235.7266, val_wg=0.9809', '\\n', 'dropout=0.2 epoch 3:\nval_loss=233.8346, val_wg=0.9809', '\\n', 'dropout=0.2 epoch 4:\nval_loss=233.0934, val_wg=0.9809', '\\n', 'dropout=0.2 epoch 5:\nval_loss=232.5580, val_wg=0.9809', '\\n', '==> Running dropout_rate = 0.3', '\\n',\n'dropout=0.3 epoch 0: val_loss=302.5635, val_wg=0.7395', '\\n', 'dropout=0.3\nepoch 1: val_loss=273.3491, val_wg=0.9160', '\\n', 'dropout=0.3 epoch 2:\nval_loss=266.3195, val_wg=0.9328', '\\n', 'dropout=0.3 epoch 3:\nval_loss=264.4714, val_wg=0.9370', '\\n', 'dropout=0.3 epoch 4:\nval_loss=263.7270, val_wg=0.9412', '\\n', 'dropout=0.3 epoch 5:\nval_loss=263.1826, val_wg=0.9412', '\\n', '==> Running dropout_rate = 0.5', '\\n',\n'dropout=0.5 epoch 0: val_loss=302.8035, val_wg=0.8319', '\\n', 'dropout=0.5\nepoch 1: val_loss=278.1261, val_wg=0.9370', '\\n', 'dropout=0.5 epoch 2:\nval_loss=272.5061, val_wg=0.9454', '\\n', 'dropout=0.5 epoch 3:\nval_loss=271.0583, val_wg=0.9454', '\\n', 'dropout=0.5 epoch 4:\nval_loss=270.4916, val_wg=0.9454', '\\n', 'dropout=0.5 epoch 5:\nval_loss=270.0870, val_wg=0.9454', '\\n', 'Execution time: 13 seconds seconds\n(time limit is an hour).']", "['Running n_clusters = 2', '\\n', ' n=2 Epoch 0: val_wg=0.9847', '\\n', ' n=2\nEpoch 1: val_wg=0.9885', '\\n', ' n=2 Epoch 2: val_wg=0.9885', '\\n', ' n=2 Epoch\n3: val_wg=0.9885', '\\n', ' n=2 Epoch 4: val_wg=0.9885', '\\n', ' n=2 Epoch 5:\nval_wg=0.9885', '\\n', 'Running n_clusters = 3', '\\n', ' n=3 Epoch 0:\nval_wg=0.8866', '\\n', ' n=3 Epoch 1: val_wg=0.9538', '\\n', ' n=3 Epoch 2:\nval_wg=0.9622', '\\n', ' n=3 Epoch 3: val_wg=0.9622', '\\n', ' n=3 Epoch 4:\nval_wg=0.9664', '\\n', ' n=3 Epoch 5: val_wg=0.9706', '\\n', 'Running n_clusters =\n4', '\\n', ' n=4 Epoch 0: val_wg=0.9809', '\\n', ' n=4 Epoch 1: val_wg=0.9847',\n'\\n', ' n=4 Epoch 2: val_wg=0.9847', '\\n', ' n=4 Epoch 3: val_wg=0.9847', '\\n',\n' n=4 Epoch 4: val_wg=0.9847', '\\n', ' n=4 Epoch 5: val_wg=0.9847', '\\n',\n'Running n_clusters = 5', '\\n', ' n=5 Epoch 0: val_wg=0.7563', '\\n', ' n=5 Epoch\n1: val_wg=0.9370', '\\n', ' n=5 Epoch 2: val_wg=0.9412', '\\n', ' n=5 Epoch 3:\nval_wg=0.9454', '\\n', ' n=5 Epoch 4: val_wg=0.9538', '\\n', ' n=5 Epoch 5:\nval_wg=0.9622', '\\n', 'Execution time: 11 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'eps=0.0 epoch=0 val_loss=268.8303 val_wg=0.9874',\n'\\n', 'eps=0.0 epoch=1 val_loss=235.9364 val_wg=0.9916', '\\n', 'eps=0.0 epoch=2\nval_loss=228.1427 val_wg=0.9916', '\\n', 'eps=0.0 epoch=3 val_loss=226.1110\nval_wg=0.9916', '\\n', 'eps=0.0 epoch=4 val_loss=225.3080 val_wg=0.9916', '\\n',\n'eps=0.0 epoch=5 val_loss=224.7263 val_wg=0.9916', '\\n', 'eps=0.05 epoch=0\nval_loss=281.4622 val_wg=0.9771', '\\n', 'eps=0.05 epoch=1 val_loss=248.8993\nval_wg=0.9809', '\\n', 'eps=0.05 epoch=2 val_loss=241.8144 val_wg=0.9809', '\\n',\n'eps=0.05 epoch=3 val_loss=240.0344 val_wg=0.9809', '\\n', 'eps=0.05 epoch=4\nval_loss=239.3778 val_wg=0.9809', '\\n', 'eps=0.05 epoch=5 val_loss=238.9246\nval_wg=0.9809', '\\n', 'eps=0.1 epoch=0 val_loss=293.4786 val_wg=0.9748', '\\n',\n'eps=0.1 epoch=1 val_loss=266.3488 val_wg=0.9790', '\\n', 'eps=0.1 epoch=2\nval_loss=259.8473 val_wg=0.9832', '\\n', 'eps=0.1 epoch=3 val_loss=258.1288\nval_wg=0.9832', '\\n', 'eps=0.1 epoch=4 val_loss=257.4290 val_wg=0.9832', '\\n',\n'eps=0.1 epoch=5 val_loss=256.9098 val_wg=0.9832', '\\n', 'eps=0.2 epoch=0\nval_loss=279.3718 val_wg=0.9656', '\\n', 'eps=0.2 epoch=1 val_loss=253.0111\nval_wg=0.9790', '\\n', 'eps=0.2 epoch=2 val_loss=247.1113 val_wg=0.9790', '\\n',\n'eps=0.2 epoch=3 val_loss=245.6378 val_wg=0.9790', '\\n', 'eps=0.2 epoch=4\nval_loss=245.1010 val_wg=0.9790', '\\n', 'eps=0.2 epoch=5 val_loss=244.7395\nval_wg=0.9790', '\\n', 'Execution time: 13 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', '\\n>>> Running momentum = 0.5', '\\n', 'momentum=0.5\nepoch=0 val_loss=0.7053 val_wg=0.2595', '\\n', 'momentum=0.5 epoch=1\nval_loss=0.7050 val_wg=0.2595', '\\n', 'momentum=0.5 epoch=2 val_loss=0.7050\nval_wg=0.2595', '\\n', 'momentum=0.5 epoch=3 val_loss=0.7050 val_wg=0.2595',\n'\\n', 'momentum=0.5 epoch=4 val_loss=0.7050 val_wg=0.2595', '\\n', 'momentum=0.5\nepoch=5 val_loss=0.7050 val_wg=0.2595', '\\n', '\\n>>> Running momentum = 0.9',\n'\\n', 'momentum=0.9 epoch=0 val_loss=0.7111 val_wg=0.0714', '\\n', 'momentum=0.9\nepoch=1 val_loss=0.6942 val_wg=0.1429', '\\n', 'momentum=0.9 epoch=2\nval_loss=0.6910 val_wg=0.1723', '\\n', 'momentum=0.9 epoch=3 val_loss=0.6904\nval_wg=0.1765', '\\n', 'momentum=0.9 epoch=4 val_loss=0.6902 val_wg=0.1765',\n'\\n', 'momentum=0.9 epoch=5 val_loss=0.6901 val_wg=0.1765', '\\n', '\\n>>> Running\nmomentum = 0.99', '\\n', 'momentum=0.99 epoch=0 val_loss=0.6440 val_wg=0.8277',\n'\\n', 'momentum=0.99 epoch=1 val_loss=0.5985 val_wg=0.9580', '\\n',\n'momentum=0.99 epoch=2 val_loss=0.5618 val_wg=0.9706', '\\n', 'momentum=0.99\nepoch=3 val_loss=0.5318 val_wg=0.9748', '\\n', 'momentum=0.99 epoch=4\nval_loss=0.5074 val_wg=0.9748', '\\n', 'momentum=0.99 epoch=5 val_loss=0.4872\nval_wg=0.9748', '\\n', 'Execution time: 8 seconds seconds (time limit is an\nhour).']", "['Execution time: 8 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Training with learning rate = 0.0001 ===',\n'\\n', 'lr=0.0001 epoch=0: val_loss=0.6977, val_wg=0.3168', '\\n', 'lr=0.0001\nepoch=1: val_loss=0.6917, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=2:\nval_loss=0.6904, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=3: val_loss=0.6900,\nval_wg=0.3588', '\\n', 'lr=0.0001 epoch=4: val_loss=0.6899, val_wg=0.3588', '\\n',\n'lr=0.0001 epoch=5: val_loss=0.6898, val_wg=0.3588', '\\n', '\\n=== Training with\nlearning rate = 0.001 ===', '\\n', 'lr=0.001 epoch=0: val_loss=0.5822,\nval_wg=0.8866', '\\n', 'lr=0.001 epoch=1: val_loss=0.5093, val_wg=0.9538', '\\n',\n'lr=0.001 epoch=2: val_loss=0.4929, val_wg=0.9622', '\\n', 'lr=0.001 epoch=3:\nval_loss=0.4887, val_wg=0.9622', '\\n', 'lr=0.001 epoch=4: val_loss=0.4872,\nval_wg=0.9622', '\\n', 'lr=0.001 epoch=5: val_loss=0.4860, val_wg=0.9622', '\\n',\n'\\n=== Training with learning rate = 0.01 ===', '\\n', 'lr=0.01 epoch=0:\nval_loss=0.0228, val_wg=0.9885', '\\n', 'lr=0.01 epoch=1: val_loss=0.0305,\nval_wg=0.9885', '\\n', 'lr=0.01 epoch=2: val_loss=0.0348, val_wg=0.9885', '\\n',\n'lr=0.01 epoch=3: val_loss=0.0380, val_wg=0.9885', '\\n', 'lr=0.01 epoch=4:\nval_loss=0.0405, val_wg=0.9885', '\\n', 'lr=0.01 epoch=5: val_loss=0.0430,\nval_wg=0.9924', '\\n', 'Execution time: 8 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', '\\n=== Training with learning rate = 0.0001 ===',\n'\\n', 'lr=0.0001 epoch=0: val_loss=0.6977, val_wg=0.3168', '\\n', 'lr=0.0001\nepoch=1: val_loss=0.6917, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=2:\nval_loss=0.6904, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=3: val_loss=0.6900,\nval_wg=0.3588', '\\n', 'lr=0.0001 epoch=4: val_loss=0.6899, val_wg=0.3588', '\\n',\n'lr=0.0001 epoch=5: val_loss=0.6898, val_wg=0.3588', '\\n', '\\n=== Training with\nlearning rate = 0.001 ===', '\\n', 'lr=0.001 epoch=0: val_loss=0.5822,\nval_wg=0.8866', '\\n', 'lr=0.001 epoch=1: val_loss=0.5093, val_wg=0.9538', '\\n',\n'lr=0.001 epoch=2: val_loss=0.4929, val_wg=0.9622', '\\n', 'lr=0.001 epoch=3:\nval_loss=0.4887, val_wg=0.9622', '\\n', 'lr=0.001 epoch=4: val_loss=0.4872,\nval_wg=0.9622', '\\n', 'lr=0.001 epoch=5: val_loss=0.4860, val_wg=0.9622', '\\n',\n'\\n=== Training with learning rate = 0.01 ===', '\\n', 'lr=0.01 epoch=0:\nval_loss=0.0228, val_wg=0.9885', '\\n', 'lr=0.01 epoch=1: val_loss=0.0305,\nval_wg=0.9885', '\\n', 'lr=0.01 epoch=2: val_loss=0.0348, val_wg=0.9885', '\\n',\n'lr=0.01 epoch=3: val_loss=0.0380, val_wg=0.9885', '\\n', 'lr=0.01 epoch=4:\nval_loss=0.0405, val_wg=0.9885', '\\n', 'lr=0.01 epoch=5: val_loss=0.0430,\nval_wg=0.9924', '\\n', 'Execution time: 8 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', '\\n=== Training with learning rate = 0.0001 ===',\n'\\n', 'lr=0.0001 epoch=0: val_loss=0.6977, val_wg=0.3168', '\\n', 'lr=0.0001\nepoch=1: val_loss=0.6917, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=2:\nval_loss=0.6904, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=3: val_loss=0.6900,\nval_wg=0.3588', '\\n', 'lr=0.0001 epoch=4: val_loss=0.6899, val_wg=0.3588', '\\n',\n'lr=0.0001 epoch=5: val_loss=0.6898, val_wg=0.3588', '\\n', '\\n=== Training with\nlearning rate = 0.001 ===', '\\n', 'lr=0.001 epoch=0: val_loss=0.5822,\nval_wg=0.8866', '\\n', 'lr=0.001 epoch=1: val_loss=0.5093, val_wg=0.9538', '\\n',\n'lr=0.001 epoch=2: val_loss=0.4929, val_wg=0.9622', '\\n', 'lr=0.001 epoch=3:\nval_loss=0.4887, val_wg=0.9622', '\\n', 'lr=0.001 epoch=4: val_loss=0.4872,\nval_wg=0.9622', '\\n', 'lr=0.001 epoch=5: val_loss=0.4860, val_wg=0.9622', '\\n',\n'\\n=== Training with learning rate = 0.01 ===', '\\n', 'lr=0.01 epoch=0:\nval_loss=0.0228, val_wg=0.9885', '\\n', 'lr=0.01 epoch=1: val_loss=0.0305,\nval_wg=0.9885', '\\n', 'lr=0.01 epoch=2: val_loss=0.0348, val_wg=0.9885', '\\n',\n'lr=0.01 epoch=3: val_loss=0.0380, val_wg=0.9885', '\\n', 'lr=0.01 epoch=4:\nval_loss=0.0405, val_wg=0.9885', '\\n', 'lr=0.01 epoch=5: val_loss=0.0430,\nval_wg=0.9924', '\\n', 'Execution time: 8 seconds seconds (time limit is an\nhour).']", ""], "analysis": ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "The script ran without crashes, but it contains logical bugs and does not meet\nstage requirements: 1. Cluster-based reweighting is done at\nepoch==warmup_epochs-1 (epoch 0) instead of after warmup, so it clusters initial\nrandom gradients. Move the clustering block to run at `epoch == warmup_epochs`.\n2. The user was supposed to introduce two new HuggingFace datasets for\nevaluation, but only the synthetic dataset is used. Integrate and evaluate at\nleast two relevant HF datasets (e.g., Waterbirds, CelebA or colored MNIST\nvariants). 3. Inconsistent seeding: seeds are set to 2 then immediately reset to\n0\u2014remove redundant seed resets for reproducibility. 4. Minor logging typo:\nprints \u201cseconds seconds.\u201d", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.4523, "best_value": 0.4523}]}, {"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9942, "best_value": 0.9942}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.4495, "best_value": 0.4495}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9916, "best_value": 0.9916}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.986, "best_value": 0.986}]}]}, {"metric_names": [{"metric_name": "worst-group accuracy", "lower_is_better": false, "description": "Proportion of correct predictions on the worst-performing subgroup in the dataset at final evaluation.", "data": [{"dataset_name": "in-sample", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "development", "final_value": 0.9924, "best_value": 0.9924}]}, {"metric_name": "average loss", "lower_is_better": true, "description": "Average loss across all examples in the dataset at final evaluation.", "data": [{"dataset_name": "in-sample", "final_value": 0.0125, "best_value": 0.0125}, {"dataset_name": "development", "final_value": 0.043, "best_value": 0.043}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Proportion of correct predictions at evaluation time on the out-of-sample dataset.", "data": [{"dataset_name": "out-of-sample", "final_value": 0.996, "best_value": 0.996}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Final training accuracy for synthetic dataset across different batch sizes.", "data": [{"dataset_name": "synthetic dataset (batch size 32)", "final_value": 0.9865, "best_value": 0.9865}, {"dataset_name": "synthetic dataset (batch size 64)", "final_value": 0.9826, "best_value": 0.9826}, {"dataset_name": "synthetic dataset (batch size 128)", "final_value": 0.9917, "best_value": 0.9917}, {"dataset_name": "synthetic dataset (batch size 256)", "final_value": 0.4294, "best_value": 0.4294}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Final validation accuracy for synthetic dataset across different batch sizes.", "data": [{"dataset_name": "synthetic dataset (batch size 32)", "final_value": 0.9885, "best_value": 0.9885}, {"dataset_name": "synthetic dataset (batch size 64)", "final_value": 0.9622, "best_value": 0.9622}, {"dataset_name": "synthetic dataset (batch size 128)", "final_value": 0.9771, "best_value": 0.9771}, {"dataset_name": "synthetic dataset (batch size 256)", "final_value": 0.437, "best_value": 0.437}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Final training loss for synthetic dataset across different batch sizes.", "data": [{"dataset_name": "synthetic dataset (batch size 32)", "final_value": 0.3573, "best_value": 0.3573}, {"dataset_name": "synthetic dataset (batch size 64)", "final_value": 0.4809, "best_value": 0.4809}, {"dataset_name": "synthetic dataset (batch size 128)", "final_value": 0.5375, "best_value": 0.5375}, {"dataset_name": "synthetic dataset (batch size 256)", "final_value": 0.639, "best_value": 0.639}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss for synthetic dataset across different batch sizes.", "data": [{"dataset_name": "synthetic dataset (batch size 32)", "final_value": 0.3593, "best_value": 0.3593}, {"dataset_name": "synthetic dataset (batch size 64)", "final_value": 0.486, "best_value": 0.486}, {"dataset_name": "synthetic dataset (batch size 128)", "final_value": 0.5366, "best_value": 0.5366}, {"dataset_name": "synthetic dataset (batch size 256)", "final_value": 0.6358, "best_value": 0.6358}]}]}, {"metric_names": [{"metric_name": "train worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the training set", "data": [{"dataset_name": "Training set", "final_value": 0.9884, "best_value": 0.9884}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "Training set", "final_value": 0.5028, "best_value": 0.5028}]}, {"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the validation set", "data": [{"dataset_name": "Validation set", "final_value": 0.9885, "best_value": 0.9885}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "Validation set", "final_value": 0.5044, "best_value": 0.5044}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test set", "data": [{"dataset_name": "Test set", "final_value": 0.988, "best_value": 0.988}]}]}, {"metric_names": [{"metric_name": "training worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the training set", "data": [{"dataset_name": "synthetic (5 epochs)", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "synthetic (10 epochs)", "final_value": 0.9923, "best_value": 0.9923}, {"dataset_name": "synthetic (20 epochs)", "final_value": 0.9691, "best_value": 0.9691}]}, {"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the validation set", "data": [{"dataset_name": "synthetic (5 epochs)", "final_value": 0.9916, "best_value": 0.9916}, {"dataset_name": "synthetic (10 epochs)", "final_value": 0.9809, "best_value": 0.9809}, {"dataset_name": "synthetic (20 epochs)", "final_value": 0.9706, "best_value": 0.9706}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test set", "data": [{"dataset_name": "synthetic (5 epochs)", "final_value": 0.986, "best_value": 0.986}, {"dataset_name": "synthetic (10 epochs)", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "synthetic (20 epochs)", "final_value": 0.972, "best_value": 0.972}]}]}, {"metric_names": [{"metric_name": "train worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on training set", "data": [{"dataset_name": "synthetic (warmup_epochs = 1)", "final_value": 0.9884, "best_value": 0.9884}, {"dataset_name": "synthetic (warmup_epochs = 2)", "final_value": 0.9884, "best_value": 0.9884}, {"dataset_name": "synthetic (warmup_epochs = 3)", "final_value": 0.9884, "best_value": 0.9884}, {"dataset_name": "synthetic (warmup_epochs = 5)", "final_value": 0.9884, "best_value": 0.9884}]}, {"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on validation set", "data": [{"dataset_name": "synthetic (warmup_epochs = 1)", "final_value": 0.9885, "best_value": 0.9885}, {"dataset_name": "synthetic (warmup_epochs = 2)", "final_value": 0.9885, "best_value": 0.9885}, {"dataset_name": "synthetic (warmup_epochs = 3)", "final_value": 0.9885, "best_value": 0.9885}, {"dataset_name": "synthetic (warmup_epochs = 5)", "final_value": 0.9885, "best_value": 0.9885}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on training set", "data": [{"dataset_name": "synthetic (warmup_epochs = 1)", "final_value": 0.5028, "best_value": 0.5028}, {"dataset_name": "synthetic (warmup_epochs = 2)", "final_value": 0.3418, "best_value": 0.3418}, {"dataset_name": "synthetic (warmup_epochs = 3)", "final_value": 0.1917, "best_value": 0.1917}, {"dataset_name": "synthetic (warmup_epochs = 5)", "final_value": 0.0575, "best_value": 0.0575}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on validation set", "data": [{"dataset_name": "synthetic (warmup_epochs = 1)", "final_value": 0.5044, "best_value": 0.5044}, {"dataset_name": "synthetic (warmup_epochs = 2)", "final_value": 0.3436, "best_value": 0.3436}, {"dataset_name": "synthetic (warmup_epochs = 3)", "final_value": 0.194, "best_value": 0.194}, {"dataset_name": "synthetic (warmup_epochs = 5)", "final_value": 0.0618, "best_value": 0.0618}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on test set", "data": [{"dataset_name": "synthetic (warmup_epochs = 1)", "final_value": 0.988, "best_value": 0.988}, {"dataset_name": "synthetic (warmup_epochs = 2)", "final_value": 0.992, "best_value": 0.992}, {"dataset_name": "synthetic (warmup_epochs = 3)", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "synthetic (warmup_epochs = 5)", "final_value": 0.99, "best_value": 0.99}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9917, "best_value": 0.9917}, {"dataset_name": "mnist", "final_value": 0.9462, "best_value": 0.9462}, {"dataset_name": "fashion_mnist", "final_value": 0.8168, "best_value": 0.8168}]}, {"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9916, "best_value": 0.9916}, {"dataset_name": "mnist", "final_value": 0.9363, "best_value": 0.9363}, {"dataset_name": "fashion_mnist", "final_value": 0.8109, "best_value": 0.8109}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "synthetic", "final_value": 0.1756, "best_value": 0.1756}, {"dataset_name": "mnist", "final_value": 0.1687, "best_value": 0.1687}, {"dataset_name": "fashion_mnist", "final_value": 0.4967, "best_value": 0.4967}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "synthetic", "final_value": 0.1796, "best_value": 0.1796}, {"dataset_name": "mnist", "final_value": 0.2054, "best_value": 0.2054}, {"dataset_name": "fashion_mnist", "final_value": 0.5057, "best_value": 0.5057}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.986, "best_value": 0.986}, {"dataset_name": "mnist", "final_value": 0.9396, "best_value": 0.9396}, {"dataset_name": "fashion_mnist", "final_value": 0.8023, "best_value": 0.8023}]}]}, {"metric_names": [{"metric_name": "Training worst-group accuracy", "lower_is_better": false, "description": "Worst-group classification accuracy on the training set", "data": [{"dataset_name": "Dropout 0.0", "final_value": 0.9884, "best_value": 0.9884}, {"dataset_name": "Dropout 0.1", "final_value": 0.9826, "best_value": 0.9826}, {"dataset_name": "Dropout 0.2", "final_value": 0.9923, "best_value": 0.9923}, {"dataset_name": "Dropout 0.3", "final_value": 0.9439, "best_value": 0.9439}, {"dataset_name": "Dropout 0.5", "final_value": 0.971, "best_value": 0.971}]}, {"metric_name": "Training loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "Dropout 0.0", "final_value": 0.5028, "best_value": 0.5028}, {"dataset_name": "Dropout 0.1", "final_value": 0.4866, "best_value": 0.4866}, {"dataset_name": "Dropout 0.2", "final_value": 0.4661, "best_value": 0.4661}, {"dataset_name": "Dropout 0.3", "final_value": 0.5283, "best_value": 0.5283}, {"dataset_name": "Dropout 0.5", "final_value": 0.5442, "best_value": 0.5442}]}, {"metric_name": "Validation worst-group accuracy", "lower_is_better": false, "description": "Worst-group classification accuracy on the validation set", "data": [{"dataset_name": "Dropout 0.0", "final_value": 0.9885, "best_value": 0.9885}, {"dataset_name": "Dropout 0.1", "final_value": 0.958, "best_value": 0.958}, {"dataset_name": "Dropout 0.2", "final_value": 0.9809, "best_value": 0.9809}, {"dataset_name": "Dropout 0.3", "final_value": 0.9412, "best_value": 0.9412}, {"dataset_name": "Dropout 0.5", "final_value": 0.9454, "best_value": 0.9454}]}, {"metric_name": "Validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "Dropout 0.0", "final_value": 0.5044, "best_value": 0.5044}, {"dataset_name": "Dropout 0.1", "final_value": 0.4916, "best_value": 0.4916}, {"dataset_name": "Dropout 0.2", "final_value": 0.4651, "best_value": 0.4651}, {"dataset_name": "Dropout 0.3", "final_value": 0.5264, "best_value": 0.5264}, {"dataset_name": "Dropout 0.5", "final_value": 0.5402, "best_value": 0.5402}]}, {"metric_name": "Test accuracy", "lower_is_better": false, "description": "Overall accuracy on the test set", "data": [{"dataset_name": "Dropout 0.0", "final_value": 0.988, "best_value": 0.988}, {"dataset_name": "Dropout 0.1", "final_value": 0.982, "best_value": 0.982}, {"dataset_name": "Dropout 0.2", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "Dropout 0.3", "final_value": 0.966, "best_value": 0.966}, {"dataset_name": "Dropout 0.5", "final_value": 0.972, "best_value": 0.972}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9884, "best_value": 0.9884}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9885, "best_value": 0.9885}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.988, "best_value": 0.988}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "synthetic", "final_value": 0.4523, "best_value": 0.4523}, {"dataset_name": "synthetic", "final_value": 0.4768, "best_value": 0.4768}, {"dataset_name": "synthetic", "final_value": 0.5144, "best_value": 0.5144}, {"dataset_name": "synthetic", "final_value": 0.4893, "best_value": 0.4893}]}, {"metric_name": "train accuracy", "lower_is_better": false, "description": "Final training accuracy", "data": [{"dataset_name": "synthetic", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "synthetic", "final_value": 0.9923, "best_value": 0.9923}, {"dataset_name": "synthetic", "final_value": 0.9807, "best_value": 0.9807}, {"dataset_name": "synthetic", "final_value": 0.9896, "best_value": 0.9896}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "synthetic", "final_value": 0.4495, "best_value": 0.4495}, {"dataset_name": "synthetic", "final_value": 0.4778, "best_value": 0.4778}, {"dataset_name": "synthetic", "final_value": 0.5138, "best_value": 0.5138}, {"dataset_name": "synthetic", "final_value": 0.4895, "best_value": 0.4895}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Final validation accuracy", "data": [{"dataset_name": "synthetic", "final_value": 0.9916, "best_value": 0.9916}, {"dataset_name": "synthetic", "final_value": 0.9809, "best_value": 0.9809}, {"dataset_name": "synthetic", "final_value": 0.9832, "best_value": 0.9832}, {"dataset_name": "synthetic", "final_value": 0.979, "best_value": 0.979}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test accuracy", "data": [{"dataset_name": "synthetic", "final_value": 0.986, "best_value": 0.986}, {"dataset_name": "synthetic", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "synthetic", "final_value": 0.982, "best_value": 0.982}, {"dataset_name": "synthetic", "final_value": 0.99, "best_value": 0.99}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "momentum=0.5", "final_value": 0.7047, "best_value": 0.7047}, {"dataset_name": "momentum=0.9", "final_value": 0.6925, "best_value": 0.6925}, {"dataset_name": "momentum=0.99", "final_value": 0.4917, "best_value": 0.4917}]}, {"metric_name": "train worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the training dataset", "data": [{"dataset_name": "momentum=0.5", "final_value": 0.2567, "best_value": 0.2567}, {"dataset_name": "momentum=0.9", "final_value": 0.1663, "best_value": 0.1663}, {"dataset_name": "momentum=0.99", "final_value": 0.9826, "best_value": 0.9826}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "momentum=0.5", "final_value": 0.705, "best_value": 0.705}, {"dataset_name": "momentum=0.9", "final_value": 0.6901, "best_value": 0.6901}, {"dataset_name": "momentum=0.99", "final_value": 0.4872, "best_value": 0.4872}]}, {"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the validation dataset", "data": [{"dataset_name": "momentum=0.5", "final_value": 0.2595, "best_value": 0.2595}, {"dataset_name": "momentum=0.9", "final_value": 0.1765, "best_value": 0.1765}, {"dataset_name": "momentum=0.99", "final_value": 0.9748, "best_value": 0.9748}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset", "data": [{"dataset_name": "momentum=0.5", "final_value": 0.334, "best_value": 0.334}, {"dataset_name": "momentum=0.9", "final_value": 0.534, "best_value": 0.534}, {"dataset_name": "momentum=0.99", "final_value": 0.98, "best_value": 0.98}]}]}, {"metric_names": [{"metric_name": "train worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9884, "best_value": 0.9884}]}, {"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9885, "best_value": 0.9885}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.5028, "best_value": 0.5028}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.5044, "best_value": 0.5044}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.988, "best_value": 0.988}]}]}, {"metric_names": [{"metric_name": "worst-group accuracy", "lower_is_better": false, "description": "Accuracy of the worst-performing group in the dataset.", "data": [{"dataset_name": "in-sample dataset", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "development dataset", "final_value": 0.9924, "best_value": 0.9924}]}, {"metric_name": "average loss", "lower_is_better": true, "description": "Average loss across all examples in the dataset.", "data": [{"dataset_name": "in-sample dataset", "final_value": 0.0125, "best_value": 0.0125}, {"dataset_name": "development dataset", "final_value": 0.043, "best_value": 0.043}]}, {"metric_name": "accuracy", "lower_is_better": false, "description": "Overall accuracy on the dataset.", "data": [{"dataset_name": "unseen dataset", "final_value": 0.996, "best_value": 0.996}]}]}, {"metric_names": [{"metric_name": "Worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the development dataset", "data": [{"dataset_name": "Development dataset", "final_value": 0.9942, "best_value": 0.9942}]}, {"metric_name": "Average loss", "lower_is_better": true, "description": "Average loss on the development dataset", "data": [{"dataset_name": "Development dataset", "final_value": 0.0125, "best_value": 0.0125}]}, {"metric_name": "Worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the tuning dataset", "data": [{"dataset_name": "Tuning dataset", "final_value": 0.9924, "best_value": 0.9924}]}, {"metric_name": "Average loss", "lower_is_better": true, "description": "Average loss on the tuning dataset", "data": [{"dataset_name": "Tuning dataset", "final_value": 0.043, "best_value": 0.043}]}, {"metric_name": "Accuracy", "lower_is_better": false, "description": "Accuracy on the evaluation dataset", "data": [{"dataset_name": "Evaluation dataset", "final_value": 0.996, "best_value": 0.996}]}]}, {"metric_names": [{"metric_name": "training worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy for the in-sample data", "data": [{"dataset_name": "In-sample", "final_value": 0.9942, "best_value": 0.9942}]}, {"metric_name": "training average loss", "lower_is_better": true, "description": "Average loss for the in-sample data", "data": [{"dataset_name": "In-sample", "final_value": 0.0125, "best_value": 0.0125}]}, {"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy for the development data", "data": [{"dataset_name": "Development", "final_value": 0.9924, "best_value": 0.9924}]}, {"metric_name": "validation average loss", "lower_is_better": true, "description": "Average loss for the development data", "data": [{"dataset_name": "Development", "final_value": 0.043, "best_value": 0.043}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the external data", "data": [{"dataset_name": "External", "final_value": 0.996, "best_value": 0.996}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_6d7f38df3e6b47049b7e197e63eb519c_proc_4194301/synthetic_class_distribution.png", "../../logs/0-run/experiment_results/experiment_6d7f38df3e6b47049b7e197e63eb519c_proc_4194301/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_6d7f38df3e6b47049b7e197e63eb519c_proc_4194301/synthetic_wg_accuracy_curves.png"], ["../../logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_worst_group_accuracy.png", "../../logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_da3bc267b810433987cb3f16a70b35f9_proc_3805/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_da3bc267b810433987cb3f16a70b35f9_proc_3805/synthetic_wg_accuracy_curves.png"], ["../../logs/0-run/experiment_results/experiment_cb43c21419df4fcc8655ee499f97ea14_proc_3806/synthetic_wgacc_curves.png", "../../logs/0-run/experiment_results/experiment_cb43c21419df4fcc8655ee499f97ea14_proc_3806/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_cb43c21419df4fcc8655ee499f97ea14_proc_3806/synthetic_final_wgacc_vs_wd.png"], ["../../logs/0-run/experiment_results/experiment_74b9cb9f5b2943f49a4c4fd0e9425495_proc_3805/synthetic_hparam_5_curves.png", "../../logs/0-run/experiment_results/experiment_74b9cb9f5b2943f49a4c4fd0e9425495_proc_3805/synthetic_hparam_20_curves.png", "../../logs/0-run/experiment_results/experiment_74b9cb9f5b2943f49a4c4fd0e9425495_proc_3805/synthetic_hparam_10_curves.png"], ["../../logs/0-run/experiment_results/experiment_ced1bd3dff3b42d2b00fda0e1703e70c_proc_3806/synthetic_final_val_wg_acc.png", "../../logs/0-run/experiment_results/experiment_ced1bd3dff3b42d2b00fda0e1703e70c_proc_3806/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_ced1bd3dff3b42d2b00fda0e1703e70c_proc_3806/synthetic_wg_acc_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/mnist_wg_accuracy.png", "../../logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/mnist_loss_curve.png", "../../logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/synthetic_wg_accuracy.png", "../../logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/fashion_mnist_loss_curve.png", "../../logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/fashion_mnist_wg_accuracy.png", "../../logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/synthetic_loss_curve.png"], ["../../logs/0-run/experiment_results/experiment_bfdf610ddd354aca9511e8b3e5a408b6_proc_3804/synthetic_test_accuracy_vs_dropout.png", "../../logs/0-run/experiment_results/experiment_bfdf610ddd354aca9511e8b3e5a408b6_proc_3804/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_bfdf610ddd354aca9511e8b3e5a408b6_proc_3804/synthetic_wg_accuracy_curves.png"], ["../../logs/0-run/experiment_results/experiment_0f868944e55948c7b42962778db04b04_proc_3806/synthetic_wg_acc_vs_epoch.png", "../../logs/0-run/experiment_results/experiment_0f868944e55948c7b42962778db04b04_proc_3806/synthetic_loss_vs_epoch.png"], ["../../logs/0-run/experiment_results/experiment_225b06aca4604631a7b23c510c0c2635_proc_3806/synthetic_worst_group_accuracy_vs_epochs.png", "../../logs/0-run/experiment_results/experiment_225b06aca4604631a7b23c510c0c2635_proc_3806/synthetic_loss_vs_epochs.png"], ["../../logs/0-run/experiment_results/experiment_16305c9bd90447fb8e875fbbcd4a7fcd_proc_3804/synthetic_val_wg.png", "../../logs/0-run/experiment_results/experiment_16305c9bd90447fb8e875fbbcd4a7fcd_proc_3804/synthetic_train_loss.png", "../../logs/0-run/experiment_results/experiment_16305c9bd90447fb8e875fbbcd4a7fcd_proc_3804/synthetic_val_loss.png", "../../logs/0-run/experiment_results/experiment_16305c9bd90447fb8e875fbbcd4a7fcd_proc_3804/synthetic_train_wg.png"], ["../../logs/0-run/experiment_results/experiment_fcdd097899424ba7a1071c6802382e1f_proc_3805/synthetic_loss.png", "../../logs/0-run/experiment_results/experiment_fcdd097899424ba7a1071c6802382e1f_proc_3805/synthetic_weighted_accuracy.png"], ["../../logs/0-run/experiment_results/experiment_ff10ac06c81242499409da36b655cf2f_proc_3806/synthetic_worst_group_accuracy.png", "../../logs/0-run/experiment_results/experiment_ff10ac06c81242499409da36b655cf2f_proc_3806/synthetic_loss_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_563867db33f741d7b58a23453bddaeb9_proc_3804/synthetic_worst_group_accuracy.png", "../../logs/0-run/experiment_results/experiment_563867db33f741d7b58a23453bddaeb9_proc_3804/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/seed_aggregation_e0b3d685f9d74f4d8c800532ba9f2f8e/synthetic_loss_curves_mean_se.png", "../../logs/0-run/experiment_results/seed_aggregation_e0b3d685f9d74f4d8c800532ba9f2f8e/synthetic_wg_accuracy_mean_se.png"]], "plot_paths": [["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_6d7f38df3e6b47049b7e197e63eb519c_proc_4194301/synthetic_class_distribution.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_6d7f38df3e6b47049b7e197e63eb519c_proc_4194301/synthetic_loss_curves.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_6d7f38df3e6b47049b7e197e63eb519c_proc_4194301/synthetic_wg_accuracy_curves.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_worst_group_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_loss_curves.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_da3bc267b810433987cb3f16a70b35f9_proc_3805/synthetic_loss_curves.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_da3bc267b810433987cb3f16a70b35f9_proc_3805/synthetic_wg_accuracy_curves.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_cb43c21419df4fcc8655ee499f97ea14_proc_3806/synthetic_wgacc_curves.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_cb43c21419df4fcc8655ee499f97ea14_proc_3806/synthetic_loss_curves.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_cb43c21419df4fcc8655ee499f97ea14_proc_3806/synthetic_final_wgacc_vs_wd.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_74b9cb9f5b2943f49a4c4fd0e9425495_proc_3805/synthetic_hparam_5_curves.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_74b9cb9f5b2943f49a4c4fd0e9425495_proc_3805/synthetic_hparam_20_curves.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_74b9cb9f5b2943f49a4c4fd0e9425495_proc_3805/synthetic_hparam_10_curves.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_ced1bd3dff3b42d2b00fda0e1703e70c_proc_3806/synthetic_final_val_wg_acc.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_ced1bd3dff3b42d2b00fda0e1703e70c_proc_3806/synthetic_loss_curves.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_ced1bd3dff3b42d2b00fda0e1703e70c_proc_3806/synthetic_wg_acc_curves.png"], [], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/mnist_wg_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/mnist_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/synthetic_wg_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/fashion_mnist_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/fashion_mnist_wg_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/synthetic_loss_curve.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_bfdf610ddd354aca9511e8b3e5a408b6_proc_3804/synthetic_test_accuracy_vs_dropout.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_bfdf610ddd354aca9511e8b3e5a408b6_proc_3804/synthetic_loss_curves.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_bfdf610ddd354aca9511e8b3e5a408b6_proc_3804/synthetic_wg_accuracy_curves.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_0f868944e55948c7b42962778db04b04_proc_3806/synthetic_wg_acc_vs_epoch.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_0f868944e55948c7b42962778db04b04_proc_3806/synthetic_loss_vs_epoch.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_225b06aca4604631a7b23c510c0c2635_proc_3806/synthetic_worst_group_accuracy_vs_epochs.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_225b06aca4604631a7b23c510c0c2635_proc_3806/synthetic_loss_vs_epochs.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_16305c9bd90447fb8e875fbbcd4a7fcd_proc_3804/synthetic_val_wg.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_16305c9bd90447fb8e875fbbcd4a7fcd_proc_3804/synthetic_train_loss.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_16305c9bd90447fb8e875fbbcd4a7fcd_proc_3804/synthetic_val_loss.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_16305c9bd90447fb8e875fbbcd4a7fcd_proc_3804/synthetic_train_wg.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_fcdd097899424ba7a1071c6802382e1f_proc_3805/synthetic_loss.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_fcdd097899424ba7a1071c6802382e1f_proc_3805/synthetic_weighted_accuracy.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_ff10ac06c81242499409da36b655cf2f_proc_3806/synthetic_worst_group_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_ff10ac06c81242499409da36b655cf2f_proc_3806/synthetic_loss_curves.png"], [], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_563867db33f741d7b58a23453bddaeb9_proc_3804/synthetic_worst_group_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_563867db33f741d7b58a23453bddaeb9_proc_3804/synthetic_loss_curves.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_e0b3d685f9d74f4d8c800532ba9f2f8e/synthetic_loss_curves_mean_se.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_e0b3d685f9d74f4d8c800532ba9f2f8e/synthetic_wg_accuracy_mean_se.png"]], "plot_analyses": [[{"analysis": "Synthetic dataset exhibits a balanced class distribution in the ground truth (approximately 255 vs 245) and model predictions closely mirror this (approximately 249 vs 251), indicating no strong bias in prediction counts across classes.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_6d7f38df3e6b47049b7e197e63eb519c_proc_4194301/synthetic_class_distribution.png"}, {"analysis": "Training and validation losses both decrease rapidly from epoch 1 to 2 (from ~0.54 to ~0.47), then gradually taper off and converge around ~0.45 by epoch 6, with no divergence between curves, suggesting stable learning without overfitting.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_6d7f38df3e6b47049b7e197e63eb519c_proc_4194301/synthetic_loss_curves.png"}, {"analysis": "Weighted group accuracy jumps from ~98.5% at epoch 1 to ~99.4% (train) and ~99.17% (validation) at epoch 2, then remains flat through epoch 6, indicating the grouping mechanism quickly identifies subgroups and yields near-perfect performance early in training.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_6d7f38df3e6b47049b7e197e63eb519c_proc_4194301/synthetic_wg_accuracy_curves.png"}], [{"analysis": "Training worst-group accuracy curves show that with learning rate 0.0001 the model only improves slowly from ~0.30 to ~0.35 over six epochs. With lr=0.001 accuracy jumps quickly from ~0.90 to ~0.98 by epoch 2 and then plateaus around ~0.97\u20130.98. With lr=0.01 the model achieves near-perfect worst-group accuracy (~0.99\u20131.00) from the first epoch onward. The validation worst-group curves mirror the training trends closely, indicating stable generalization across these rates.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_worst_group_accuracy.png"}, {"analysis": "Training loss curves reveal that lr=0.0001 yields a minor decrease from ~0.70 to ~0.69, matching its slow accuracy gains. With lr=0.001 the loss drops sharply from ~0.58 to ~0.48 by epoch 3 before flattening, consistent with rapid accuracy improvements. The high lr=0.01 setting drives the loss close to zero immediately (~0.01) but then slightly increases, suggesting possible overshooting or instability. Validation losses follow similar patterns, confirming that the training dynamics transfer to held-out data.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_loss_curves.png"}], [{"analysis": "Synthetic dataset loss curves (Train and Val) across batch sizes show a consistent decrease in both training and validation loss over epochs for all batch\u2010size settings. Smaller batch sizes converge faster and to lower final losses: BS=32 drops from ~0.46 to ~0.36 by epoch 5, whereas BS=256 only moves from ~0.68 to ~0.64. The medium batch sizes (64, 128) sit between these extremes. After roughly epoch 2 all curves begin to flatten, indicating diminishing returns with the current learning rate and number of epochs.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_da3bc267b810433987cb3f16a70b35f9_proc_3805/synthetic_loss_curves.png"}], [{"analysis": "Worst-group accuracy curves indicate that all weight decay values improve worst-group performance rapidly between epoch 0 and 1, then plateau. Weight decay=1e-4 yields the highest and most stable training (~0.993) and validation (~0.985) worst-group accuracy across epochs, slightly outperforming no weight decay (~0.99 train, ~0.98 val). Very small weight decay (1e-5) starts lower but approaches strong performance (~0.978 train, ~0.965 val). High weight decay (1e-3) underperforms, converging below ~0.94 train and ~0.925 val, suggesting over-regularization harms worst-group robustness.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_cb43c21419df4fcc8655ee499f97ea14_proc_3806/synthetic_wgacc_curves.png"}, {"analysis": "Loss curves show a steep drop from initial loss (~0.60) to epoch 1 across all settings. Weight decay=1e-4 achieves the lowest final training (~0.46) and validation (~0.46) losses with minimal gap, indicating strong generalization and regularization. No weight decay and weight decay=1e-5 converge to higher final losses (~0.50 and ~0.48 respectively), while excessive weight decay (1e-3) converges more slowly, plateauing around ~0.533, reflecting underfitting.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_cb43c21419df4fcc8655ee499f97ea14_proc_3806/synthetic_loss_curves.png"}, {"analysis": "Plotting final validation worst-group accuracy versus weight decay reveals a clear convex trend: performance peaks at weight decay=1e-4 (~0.989), drops slightly at 1e-5 (~0.981), then more at zero (~0.962) and falls sharply at 1e-3 (~0.941). This confirms moderate regularization maximizes robust worst-group generalization on the synthetic dataset.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_cb43c21419df4fcc8655ee499f97ea14_proc_3806/synthetic_final_wgacc_vs_wd.png"}], [], [{"analysis": "Warmup epoch settings of 1, 2, 3, and 5 all yield almost identical final validation worst-group accuracy very close to 99%. There is no appreciable drop or rise in worst-group performance as the number of warmup epochs increases over this range, indicating that the model is already robust to spurious features under all tested warmup lengths on this synthetic benchmark.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_ced1bd3dff3b42d2b00fda0e1703e70c_proc_3806/synthetic_final_val_wg_acc.png"}, {"analysis": "Training and validation losses diverge sharply based on warmup duration. With 1 or 2 warmup epochs, loss quickly falls to around 0.50 by epoch 1 and then plateaus, signaling early stagnation. In contrast, 3 or 5 warmup epochs drive the loss down continuously\u2014dropping below 0.20 by epoch 4 and reaching approximately 0.06 by epoch 9. The validation loss closely mirrors the training loss in each case, suggesting that longer warmup not only enables deeper minimization but also maintains good generalization on held-out data. Incrementing warmup beyond 3 gives only slight additional loss reduction, so 3 epochs appears to be a good tradeoff point.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_ced1bd3dff3b42d2b00fda0e1703e70c_proc_3806/synthetic_loss_curves.png"}, {"analysis": "Weighted-group accuracy curves for both training and validation start at around 98.1% at epoch 0 for all warmup settings and climb to about 98.8\u201398.9% within the first few epochs. Warmup\u2009=\u20093 and 5 reach this plateau by epoch 1, while warmup\u2009=\u20091 and 2 require 1\u20132 extra epochs. After that point, group accuracy remains essentially flat across all settings, confirming that once the model escapes trivial spurious traps early on, it maintains near-optimal worst-group performance regardless of further warmup.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_ced1bd3dff3b42d2b00fda0e1703e70c_proc_3806/synthetic_wg_acc_curves.png"}], [], [{"analysis": "mnist Dataset - Worst-Group Accuracy: Train worst-group accuracy climbs rapidly from ~0.913 to ~0.963 by epoch 5, then plateaus around 0.959\u20130.960 until epoch 12\u201314 before a slight decline to ~0.951 by epoch 19. Validation worst-group accuracy follows a similar trend, peaking at ~0.945 around epoch 12 and then tapering off to ~0.936 by epoch 19. The modest gap between train and validation suggests only mild overfitting; robust performance saturates early, indicating diminishing returns beyond epoch 12.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/mnist_wg_accuracy.png"}, {"analysis": "mnist Dataset - Loss Curve: Training loss falls steeply from 0.305 to ~0.129 by epoch 5, then bottoms out around 0.136 at epoch 12 before gradually rising to ~0.169 at epoch 19. Validation loss mirrors this pattern: a drop to ~0.169 at epoch 5, a small U-shaped curve that reaches a minimum ~0.175 at epoch 12, then increases to ~0.205 by epoch 19. The U-shape suggests an optimal window around epochs 5\u201312; later epochs induce slight overfitting.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/mnist_loss_curve.png"}, {"analysis": "synthetic Dataset - Worst-Group Accuracy: Both train and validation worst-group accuracy jump from ~0.932/0.924 at epoch 0 to ~0.988/0.987 by epoch 2, then reach ~0.991 by epoch 6 and remain flat through epoch 19. Near-perfect alignment of curves indicates no overfitting and that the model quickly captures the underlying structure in the synthetic data.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/synthetic_wg_accuracy.png"}, {"analysis": "fashion_mnist Dataset - Loss Curve: Training loss decreases from ~0.535 to ~0.368 at epoch 4, then rises steadily to ~0.497 by epoch 19. Validation loss closely tracks train, dipping to ~0.378 at epoch 5, spiking to ~0.420 at epoch 7, flattening, then climbing to ~0.505 by epoch 19. Overfitting clearly begins past epoch 5; the optimal loss occurs around epochs 4\u20136.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/fashion_mnist_loss_curve.png"}, {"analysis": "fashion_mnist Dataset - Worst-Group Accuracy: Train worst-group accuracy peaks at ~0.868 around epoch 5, validation at ~0.862 the same epoch, then both drop monotonically to ~0.817/0.811 by epoch 19. The synchronized decline confirms overfitting and suggests early stopping around epoch 5\u20136 would maximize robust performance.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/fashion_mnist_wg_accuracy.png"}, {"analysis": "synthetic Dataset - Loss Curve: Loss plummets from ~0.600 to ~0.210 by epoch 5, then gradually edges down to ~0.178 by epoch 10, plateauing thereafter. Train and validation curves overlay almost perfectly, confirming excellent generalization and indicating that training past epoch 10 yields negligible improvement.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_5427ad0ca70e40dd9dea9b7bbc5ac21e_proc_3805/synthetic_loss_curve.png"}], [{"analysis": "Test accuracy remains extremely high across low to moderate dropout rates, hovering just below 1.0 from dropout=0.0 up to 0.2. Beyond 0.3, performance begins to degrade modestly, dropping to about 0.97 at dropout=0.5. This suggests that small dropout regularization does not harm overall test accuracy, but higher dropout introduces underfitting on this synthetic distribution.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_bfdf610ddd354aca9511e8b3e5a408b6_proc_3804/synthetic_test_accuracy_vs_dropout.png"}, {"analysis": "Training and validation loss curves show that a moderate dropout of 0.2 yields the lowest final losses (around 0.467 train / 0.468 val) and the fastest initial descent. Zero and mild dropout (0.1) also perform well, but suffer slightly higher plateau losses. Larger dropouts (0.3, 0.5) slow convergence and stabilize at higher loss levels, indicating underfitting and reduced capacity to model the synthetic features.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_bfdf610ddd354aca9511e8b3e5a408b6_proc_3804/synthetic_loss_curves.png"}, {"analysis": "Worst-group accuracy rises quickly by epoch 1 for all settings. Dropout=0.2 achieves near-optimal worst-group performance (train \u22480.99, val \u22480.98) and sustains it, matching or slightly exceeding the no-dropout baseline. Mild dropout (0.1) also recovers strong worst-group accuracy but requires more epochs. Higher dropouts (0.3, 0.5) lag behind, reaching at best ~0.95 on validation worst-group, indicating that too much regularization hampers robustness to spurious groups.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_bfdf610ddd354aca9511e8b3e5a408b6_proc_3804/synthetic_wg_accuracy_curves.png"}], [{"analysis": "Weighted Group Accuracy vs Epoch curves show that all cluster configurations rapidly improve within the first epoch and then plateau. On training data, n_clusters=4 and n_clusters=2 reach the highest worst-group accuracy (~0.99) by epoch\u20091, with n_clusters=3 close behind (~0.98) and n_clusters=5 lagging (~0.93). On validation data, n_clusters=4 leads slightly at ~0.99 accuracy at epoch\u20091 and remains stable (~0.98) through epoch\u20095; n_clusters=2 matches this by epoch\u20095; n_clusters=3 converges to ~0.96, and n_clusters=5 to ~0.96 but starts much lower (0.76 at epoch\u20090). Overall, four clusters yields the most efficient grouping for robust performance, while five clusters appears over-segmented and slower to recover.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_0f868944e55948c7b42962778db04b04_proc_3806/synthetic_wg_acc_vs_epoch.png"}, {"analysis": "Cross-Entropy Loss vs Epoch curves confirm the above pattern: loss drops sharply in the first epoch across all settings, then flattens. Training loss at epoch\u20095 is lowest for n_clusters=4 (~0.46), followed by n_clusters=3 (~0.48), n_clusters=2 (~0.50), and highest for n_clusters=5 (~0.52). Validation loss follows the same ranking. The rapid convergence suggests the current learning rate may be on the high side, and the minimal change after epoch\u20091 indicates diminishing returns for more epochs under current settings.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_0f868944e55948c7b42962778db04b04_proc_3806/synthetic_loss_vs_epoch.png"}], [{"analysis": "Worst-group accuracy curves across epochs for different label-smoothing values reveal that no smoothing (0.0) and light smoothing (0.05) achieve the highest worst-group performance, reaching approximately 0.995 by epoch 1 and maintaining it through epoch 5. Moderate smoothing (0.1) converges slightly more slowly, plateauing near 0.98, and heavy smoothing (0.2) starts lower at 0.95 but recovers to about 0.99 by epoch 1. All variants show diminishing gains after 1\u20132 epochs, suggesting that extending training beyond two epochs yields minimal improvement in worst-group accuracy under these settings.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_225b06aca4604631a7b23c510c0c2635_proc_3806/synthetic_worst_group_accuracy_vs_epochs.png"}, {"analysis": "Training and validation loss trajectories mirror the accuracy trends: no smoothing yields the fastest decline and lowest final loss (~0.45), smoothing of 0.05 is nearly as efficient, while smoothing values of 0.1 and 0.2 slow convergence, settling at higher losses (~0.515 and ~0.495, respectively). The close alignment between training and validation loss curves across all settings indicates limited overfitting under current hyperparameters.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_225b06aca4604631a7b23c510c0c2635_proc_3806/synthetic_loss_vs_epochs.png"}], [{"analysis": "Validation worst-group accuracy curves for momentum=0.5, 0.9, 0.99 show a dramatic gap: momentum=0.99 quickly climbs from ~0.82 at epoch 0 to ~0.97 by epoch 1, then plateaus around 0.98. Momentum=0.9 starts near 0.08 and rises modestly to ~0.18 by epoch 2 before flattening. Momentum=0.5 remains stuck at ~0.26 throughout. This indicates that higher momentum significantly improves worst-group generalization on the synthetic benchmark, whereas standard momentum values (0.5, 0.9) fail to separate spurious clusters effectively in an early-training clustering regime.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_16305c9bd90447fb8e875fbbcd4a7fcd_proc_3804/synthetic_val_wg.png"}, {"analysis": "Training loss decreases only marginally for momentum=0.5 (from ~0.705 to ~0.705) and momentum=0.9 (from ~0.715 to ~0.695). In contrast, momentum=0.99 yields a steady decrease from ~0.645 at epoch 0 down to ~0.49 by epoch 5. This suggests that high momentum both accelerates optimization and helps the model escape spurious minima, whereas lower momentum settings converge slowly and get trapped, resulting in higher final loss.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_16305c9bd90447fb8e875fbbcd4a7fcd_proc_3804/synthetic_train_loss.png"}, {"analysis": "Validation loss mirrors the training-loss trends: momentum=0.99 drives validation loss down from ~0.645 to ~0.49 over 6 epochs, indicating genuine generalization improvements. Momentum=0.9 drops only from ~0.715 to ~0.69, and momentum=0.5 remains at ~0.707. The strong correlation between training and validation loss for high momentum confirms that the model is learning robust features rather than memorizing noise clusters.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_16305c9bd90447fb8e875fbbcd4a7fcd_proc_3804/synthetic_val_loss.png"}, {"analysis": "Training worst-group accuracy for momentum=0.99 rises sharply from ~0.85 to ~0.99 by epoch 2, then slightly dips to ~0.98 by epoch 5, showing nearly perfect separation of hardest clusters. Momentum=0.9 improves from ~0.07 to ~0.16, then stalls. Momentum=0.5 sits flat at ~0.25. The contrast between training and validation worst-group accuracy further underlines that without sufficiently high momentum, the gradient signatures are too noisy for clustering to recover useful pseudo-groups.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_16305c9bd90447fb8e875fbbcd4a7fcd_proc_3804/synthetic_train_wg.png"}], [{"analysis": "Plot 1 shows training and validation loss curves for three gradient\u2010norm settings (1.0, 5.0, 10.0). All curves overlap almost perfectly. Loss plunges from about 0.59 at epoch 0 to roughly 0.525 after one epoch, then to about 0.51 by epoch 2, before stabilizing around 0.503 by epoch 5. The close match between training and validation indicates minimal overfitting on this synthetic dataset, and the negligible differences across norm values suggest that clipping at any of these thresholds has little effect on convergence speed or final loss.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_fcdd097899424ba7a1071c6802382e1f_proc_3805/synthetic_loss.png"}, {"analysis": "Plot 2 presents weighted accuracy on training and validation splits under the same norm settings. Weighted accuracy climbs sharply from \u22480.981 at epoch 0 to \u22480.988 by epoch 1, then remains flat through epoch 5 for all norm values. Identical curves for training and validation again point to strong generalization, while the uniformity across norms confirms that these gradient\u2010clipping thresholds do not meaningfully alter performance on this metric.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_fcdd097899424ba7a1071c6802382e1f_proc_3805/synthetic_weighted_accuracy.png"}], [{"analysis": "Worst-group accuracy curves during training and validation across three learning rates reveal distinct behaviors. For lr=0.01, worst-group accuracy on training data jumps to approximately 0.99 by epoch 2 and remains effectively at ceiling through epoch 6; validation accuracy follows suit, plateauing around 0.99\u20131.00. With lr=0.001, training accuracy starts near 0.90, climbs to about 0.98 by epoch 2, then slightly dips and stabilizes near 0.97; validation accuracy mirrors this trend at marginally lower levels (0.88 to 0.96). At lr=0.0001, both training and validation worst-group accuracies linger around 0.30 initially, improving only to roughly 0.35 by epoch 6. This suggests that very small learning rates underfit and fail to recover from spurious shortcuts, while too-large rates (1e-2) quickly eliminate spurious reliance and yield near-perfect worst-group accuracy, with lr=1e-3 offering a middle ground of robust performance with slightly more conservative updates.", "implications": "Aggressive learning rates (1e-2) drive rapid worst-group improvements but risk minor oscillations; moderate rates (1e-3) achieve a stable balance between convergence speed and generalization; small rates (1e-4) severely underfit on worst-group instances.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_ff10ac06c81242499409da36b655cf2f_proc_3806/synthetic_worst_group_accuracy.png"}, {"analysis": "Training and validation loss curves corroborate the accuracy findings. At lr=0.01, training loss plummets to around 0.01 by epoch 1 then drifts upward slightly to ~0.015 by epoch 6; validation loss remains extremely low (\u22480.02) initially but rises to around 0.04, hinting at potential overfitting or unstable optimization dynamics. With lr=0.001, training loss declines steadily from ~0.58 to ~0.48 by epoch 3 and flattens thereafter; validation loss follows a similar downward trajectory to ~0.49, indicating stable convergence. At lr=0.0001, both training and validation losses show minimal reduction (from ~0.70 to ~0.68), confirming underfitting. Overall, the lr=1e-2 setting offers fastest loss reduction at the expense of slight loss rebound, while lr=1e-3 yields reliable, plateaued loss curves consistent with robust worst-group accuracy.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_ff10ac06c81242499409da36b655cf2f_proc_3806/synthetic_loss_curves.png"}], [], [{"analysis": "Worst-group accuracy for lr=0.0001 barely moves beyond 0.30\u20130.35 even after six epochs, indicating too slow convergence to recover from spurious shortcuts. A learning rate of 0.001 achieves rapid improvement: training worst-group rises from \u22480.90 to \u22480.98 and validation from \u22480.89 to \u22480.97 by epoch\u20093, then plateaus\u2014showing strong, stable generalization on rare or hard clusters. With lr=0.01, worst-group accuracy is nearly perfect on training (\u22481.00) and very high on validation (\u22480.98\u20130.99) from the start, but there\u2019s a slight downward drift after epoch\u20092. This suggests over\u2010optimistic cluster assignment or aggressive updates that overfit to synthetic patterns.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_563867db33f741d7b58a23453bddaeb9_proc_3804/synthetic_worst_group_accuracy.png"}, {"analysis": "Training loss for lr=0.0001 stagnates around 0.70, matching the poor worst-group performance. With lr=0.001, loss quickly drops from \u22480.58 to \u22480.48 within two epochs and then flattens\u2014consistent with the plateau in worst-group accuracy. For lr=0.01, training loss collapses to near zero (\u22480.01\u20130.02), but validation loss starts low (\u22480.02) and climbs to \u22480.045 by epoch\u20096, confirming overfitting despite high validation worst-group accuracy. The divergence between near-zero training loss and rising validation loss flags reduced robustness to unseen spurious combinations.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_563867db33f741d7b58a23453bddaeb9_proc_3804/synthetic_loss_curves.png"}], []], "vlm_feedback_summary": ["Balanced class distribution maintained in predictions; loss curves show steady\nconvergence with no overfitting; weighted group accuracy converges extremely\nquickly to high values, demonstrating effective pseudo-group calibration.", "Larger learning rates accelerate convergence of both loss minimization and\nworst-group accuracy, enabling the model to quickly exploit gradient clustering\nfor robust subgroup performance. Medium lr balances speed and stability, while\nvery low lr fails to adapt group weights effectively within six epochs, and very\nhigh lr risks potential instability despite achieving nearly perfect worst-group\nmetrics.", "Weighted group accuracy curves reveal that very small (32) and moderate (128)\nbatch sizes rapidly achieve near-perfect performance (>0.98) on both training\nand validation by epoch 1\u20132. BS=64 lags slightly, plateauing around 0.97 by\nepoch 3\u20135. Large batch size (256) nearly fails to recover group performance,\nstarting at ~0.05 and only climbing to ~0.42 by epoch 5, both in train and val.\nThis suggests that large batches dilute the gradient\u2010based grouping signals\nneeded by UGC, harming worst\u2010group generalization.", "Moderate weight decay around 1e-4 offers the best trade-off between fitting and\nregularization for robust worst-group accuracy. Extreme values either under-\nregularize or over-regularize, harming worst-group performance. Recommended to\nadopt weight decay=1e-4 in subsequent tuning and validate this choice on\nadditional datasets from Hugging Face such as CelebA-HQ or Waterbirds to\nevaluate real-world robustness scenarios.", "[]", "Longer warmup (\u22653) substantially reduces loss without harming worst-group\naccuracy; worst-group accuracy saturates early across all warmups.", "[]", "Overall the model reaches strong robust performance quickly on MNIST and the\nsynthetic dataset, with minimal overfitting; Fashion-MNIST shows clear\noverfitting after 5 epochs. Early stopping around epochs 5\u20136 for Fashion-MNIST\nand epochs 10\u201312 for MNIST will boost validation robustness. Synthetic runs can\nbe truncated to 6\u201310 epochs without loss. Consider lowering learning rate after\nthe initial drop, adding weight decay or data augmentation for Fashion-MNIST,\nand tuning batch size to stabilize early gradients. For broader evaluation,\nintroduce the HuggingFace \u201cwaterbirds\u201d and \u201cceleb_a\u201d datasets to test spurious-\ncorrelation resilience in real-world scenarios.", "Moderate dropout (~0.2) provides the best tradeoff: it maintains near-perfect\ntest accuracy, yields the lowest losses, and maximizes worst-group robustness\nwithout overregularizing. Dropouts below 0.1 risk slight overfitting on hard\ngroups, while dropouts above 0.3 introduce underfitting and lower out-of-\ndistribution group performance.", "n_clusters=4 achieves the best trade-off between group granularity and robust\nperformance, whereas n_clusters=5 over-segments and underperforms. Convergence\nby epoch\u20091 suggests exploring a lower initial learning rate or a learning rate\nschedule, increasing total epochs, and experimenting with smaller batch sizes to\ndiversify gradient signals. For future testing, fix n_clusters at 4 during\nhyperparameter tuning and evaluate on additional real-world spurious-correlation\nbenchmarks: Stanford Cars and CUB-200-2011 from HuggingFace.", "Label smoothing in the range [0.0, 0.05] offers the best trade-off between\nworst-group accuracy and convergence speed, with performance saturating after\ntwo epochs. To further boost robustness, consider fine-tuning the learning rate\nor batch size rather than extending training time. Propose evaluating UGC on two\nadditional HuggingFace benchmarks: 'biasbios' (Bias in Bios) for occupation\nclassification with gender-related spurious correlations, and 'civil_comments'\nfor toxicity classification with identity-based spurious features, to test\ngeneralization across text-based group imbalances.", "Across all four plots, momentum=0.99 consistently outperforms lower momentum\nsettings in loss reduction and worst-group accuracy, both on training and\nvalidation. Lower momentum values fail to escape spurious minima, yielding poor\nworst-group performance. High momentum appears critical to produce sharp\ngradient signatures early in training for effective gradient clustering.", "Loss and weighted accuracy curves are insensitive to gradient\u2010norm\nhyperparameters in the tested range; model converges by epoch 1 with no\noverfitting. Further tuning should target learning rate, batch size or\nalternative regularizers, and track worst\u2010group accuracy rather than only\nweighted metrics.", "Learning rates of 1e-2 fuse rapid convergence and extreme worst-group\nperformance but show early loss rebound, 1e-3 produces a stable compromise with\nreliable worst-group gains, and 1e-4 underfits completely.", "[]", "Optimal performance on this synthetic benchmark is found with a learning rate of\n0.001 for around three to four epochs\u2014beyond that both accuracy and loss curves\nflatten. A too-low learning rate fails to escape spurious minima, while a too-\nhigh rate leads to overfitting (vanishing training loss and growing validation\nloss).", "[]"], "exec_time": [3.7870523929595947, 6.182882070541382, 10.805172204971313, 10.748594760894775, 10.093027353286743, 7.504635810852051, 10.735823154449463, 326.5223219394684, 13.327603816986084, 11.236541509628296, 13.308026313781738, 8.388803482055664, 8.548983097076416, 8.615322589874268, 8.53106951713562, 8.74288034439087, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['synthetic']"], ["['synthetic']"], ["[synthetic]"], ["[\"synthetic\"]"], [], ["['synthetic']"], [], ["['mnist'", "'synthetic']"], ["['synthetic']"], ["['synthetic']"], ["[synthetic]"], ["['synthetic']"], ["[synthetic]"], ["['synthetic']"], [], ["[\"synthetic\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndata = experiment_data.get(\"synthetic\", {})\nlosses = data.get(\"losses\", {})\nmetrics = data.get(\"metrics\", {})\ntrain_loss = losses.get(\"train\", [])\nval_loss = losses.get(\"val\", [])\ntrain_metric = metrics.get(\"train\", [])\nval_metric = metrics.get(\"val\", [])\npreds = data.get(\"predictions\", np.array([]))\ntruths = data.get(\"ground_truth\", np.array([]))\nepochs = list(range(1, len(train_loss) + 1))\n\n# Plot loss curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    axes[0].plot(epochs, train_loss, marker=\"o\")\n    axes[1].plot(epochs, val_loss, marker=\"o\")\n    axes[0].set_title(\"Left: Training Loss\")\n    axes[1].set_title(\"Right: Validation Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    fig.suptitle(\"Synthetic Dataset Loss Curves\")\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# Plot weighted\u2010group accuracy curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    axes[0].plot(epochs, train_metric, marker=\"o\")\n    axes[1].plot(epochs, val_metric, marker=\"o\")\n    axes[0].set_title(\"Left: Training Weighted Group Accuracy\")\n    axes[1].set_title(\"Right: Validation Weighted Group Accuracy\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Accuracy\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Accuracy\")\n    fig.suptitle(\"Synthetic Dataset Weighted Group Accuracy\")\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(os.path.join(working_dir, \"synthetic_wg_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating wg accuracy plot: {e}\")\n    plt.close()\n\n# Plot class distribution of ground truth vs predictions\ntry:\n    ut, ct = np.unique(truths, return_counts=True)\n    up, cp = np.unique(preds, return_counts=True)\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    axes[0].bar(ut, ct)\n    axes[1].bar(up, cp)\n    axes[0].set_title(\"Left: Ground Truth Distribution\")\n    axes[1].set_title(\"Right: Prediction Distribution\")\n    axes[0].set_xlabel(\"Class\")\n    axes[0].set_ylabel(\"Count\")\n    axes[1].set_xlabel(\"Class\")\n    axes[1].set_ylabel(\"Count\")\n    fig.suptitle(\"Synthetic Dataset Class Distribution\")\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(os.path.join(working_dir, \"synthetic_class_distribution.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating class distribution plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    data = experiment_data[\"learning_rate\"][\"synthetic\"]\n    lrs = data[\"lrs\"]\n    metrics_train = data[\"metrics\"][\"train\"]\n    metrics_val = data[\"metrics\"][\"val\"]\n    losses_train = data[\"losses\"][\"train\"]\n    losses_val = data[\"losses\"][\"val\"]\nexcept Exception as e:\n    print(f\"Error loading or extracting experiment data: {e}\")\n\n# Plot worst\u2010group accuracy curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, metrics_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, metrics_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, metrics_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training WG Accuracy\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Worst\u2010group Accuracy\")\n    axes[1].set_title(\"Validation WG Accuracy\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Worst\u2010group Accuracy\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\n        \"Synthetic dataset - Worst\u2010group Accuracy\\nLeft: Training, Right: Validation\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_worst_group_accuracy.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close(\"all\")\n\n# Plot loss curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, losses_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, losses_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, losses_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[1].set_title(\"Validation Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\"Synthetic dataset - Loss Curves\\nLeft: Training, Right: Validation\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close(\"all\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nsd = experiment_data[\"batch_size\"][\"synthetic\"]\nbatch_sizes = sd[\"batch_sizes\"]\ntrain_losses = sd[\"losses\"][\"train\"]\nval_losses = sd[\"losses\"][\"val\"]\ntrain_acc = sd[\"metrics\"][\"train\"]\nval_acc = sd[\"metrics\"][\"val\"]\n\n# Plot loss curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for i, bs in enumerate(batch_sizes):\n        axes[0].plot(train_losses[i], label=f\"BS {bs}\")\n        axes[1].plot(val_losses[i], label=f\"BS {bs}\")\n    axes[0].set_title(\"Train Loss\")\n    axes[1].set_title(\"Val Loss\")\n    for ax in axes:\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n        ax.legend()\n    fig.suptitle(\"Synthetic Dataset Loss Curves\\nLeft: Train Loss, Right: Val Loss\")\n    fig.tight_layout(rect=[0, 0, 1, 0.95])\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# Plot weighted group accuracy curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for i, bs in enumerate(batch_sizes):\n        axes[0].plot(train_acc[i], label=f\"BS {bs}\")\n        axes[1].plot(val_acc[i], label=f\"BS {bs}\")\n    axes[0].set_title(\"Train Weighted Group Acc\")\n    axes[1].set_title(\"Val Weighted Group Acc\")\n    for ax in axes:\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Weighted Group Accuracy\")\n        ax.legend()\n    fig.suptitle(\"Synthetic Dataset Weighted Group Accuracy\\nLeft: Train, Right: Val\")\n    fig.tight_layout(rect=[0, 0, 1, 0.95])\n    plt.savefig(os.path.join(working_dir, \"synthetic_wg_accuracy_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating wg accuracy curves: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nexp = experiment_data[\"weight_decay\"][\"synthetic\"]\ndecays = exp[\"decays\"]\ntrain_losses = exp[\"losses\"][\"train\"]\nval_losses = exp[\"losses\"][\"val\"]\ntrain_metrics = exp[\"metrics\"][\"train\"]\nval_metrics = exp[\"metrics\"][\"val\"]\n\nepochs = np.arange(train_losses.shape[1])\n\ntry:\n    plt.figure()\n    for i, wd in enumerate(decays):\n        plt.plot(epochs, train_losses[i], label=f\"wd={wd} train\")\n        plt.plot(epochs, val_losses[i], linestyle=\"--\", label=f\"wd={wd} val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\n        \"Loss Curves\\nTraining (solid) vs Validation (dashed) for synthetic dataset\"\n    )\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    for i, wd in enumerate(decays):\n        plt.plot(epochs, train_metrics[i], label=f\"wd={wd} train\")\n        plt.plot(epochs, val_metrics[i], linestyle=\"--\", label=f\"wd={wd} val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Worst-group Accuracy\")\n    plt.title(\n        \"Worst-group Accuracy Curves\\nTraining (solid) vs Validation (dashed) for synthetic dataset\"\n    )\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_wgacc_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n\ntry:\n    final_val = val_metrics[:, -1]\n    plt.figure()\n    plt.plot(decays, final_val, marker=\"o\")\n    plt.xlabel(\"Weight Decay\")\n    plt.ylabel(\"Final Validation Worst-group Accuracy\")\n    plt.title(\n        \"Final Validation Worst-group Accuracy vs Weight Decay\\nsynthetic dataset\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_final_wgacc_vs_wd.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot3: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nfor te_str, res_dict in experiment_data.get(\"train_epochs\", {}).items():\n    try:\n        res = res_dict.get(\"synthetic\", {})\n        losses = res[\"losses\"]\n        metrics = res[\"metrics\"]\n        epochs = np.arange(len(losses[\"train\"]))\n        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n        fig.suptitle(\n            f\"Synthetic Dataset (Hyperparam {te_str}) - Left: Loss Curves, Right: Worst-group Accuracy\"\n        )\n        # Left: loss curves\n        axes[0].plot(epochs, losses[\"train\"], label=\"Train Loss\")\n        axes[0].plot(epochs, losses[\"val\"], label=\"Val Loss\")\n        axes[0].set_xlabel(\"Epoch\")\n        axes[0].set_ylabel(\"Loss\")\n        axes[0].set_title(\"Train vs Validation Loss\")\n        axes[0].legend()\n        # Right: worst-group accuracy curves\n        axes[1].plot(epochs, metrics[\"train\"], label=\"Train WG Acc\")\n        axes[1].plot(epochs, metrics[\"val\"], label=\"Val WG Acc\")\n        axes[1].set_xlabel(\"Epoch\")\n        axes[1].set_ylabel(\"Worst-group Accuracy\")\n        axes[1].set_title(\"Train vs Validation WG Accuracy\")\n        axes[1].legend()\n        fname = f\"synthetic_hparam_{te_str}_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating plot for hyperparam {te_str}: {e}\")\n        plt.close(\"all\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    exp = data[\"warmup_epochs\"][\"synthetic\"]\n    warmup = exp[\"warmup_values\"]\n    train_losses = exp[\"losses\"][\"train\"]\n    val_losses = exp[\"losses\"][\"val\"]\n    train_metrics = exp[\"metrics\"][\"train\"]\n    val_metrics = exp[\"metrics\"][\"val\"]\n\n    try:\n        plt.figure()\n        for i, w in enumerate(warmup):\n            plt.plot(train_losses[i], label=f\"warmup={w}\")\n        for i, w in enumerate(warmup):\n            plt.plot(val_losses[i], linestyle=\"--\", label=f\"warmup={w}\")\n        plt.title(\"Synthetic: Loss Curves (train solid, val dashed)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves: {e}\")\n        plt.close()\n\n    try:\n        plt.figure()\n        for i, w in enumerate(warmup):\n            plt.plot(train_metrics[i], label=f\"warmup={w}\")\n        for i, w in enumerate(warmup):\n            plt.plot(val_metrics[i], linestyle=\"--\", label=f\"warmup={w}\")\n        plt.title(\"Synthetic: WG Accuracy Curves (train solid, val dashed)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Weighted\u2010Group Accuracy\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_wg_acc_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy curves: {e}\")\n        plt.close()\n\n    try:\n        final_val = [m[-1] for m in val_metrics]\n        plt.figure()\n        plt.bar([str(w) for w in warmup], final_val)\n        plt.title(\"Synthetic: Final Validation WG Accuracy\")\n        plt.xlabel(\"Warmup Epochs\")\n        plt.ylabel(\"WG Accuracy\")\n        plt.savefig(os.path.join(working_dir, \"synthetic_final_val_wg_acc.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating final bar plot: {e}\")\n        plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot WG accuracy and loss for each dataset\nfor name, data in experiment_data.items():\n    metrics = data.get(\"metrics\", {})\n    losses = data.get(\"losses\", {})\n\n    # Worst\u2010group accuracy curve\n    try:\n        epochs = np.arange(len(metrics.get(\"train\", [])))\n        plt.figure()\n        plt.plot(epochs, metrics[\"train\"], label=\"Train WG Acc\")\n        plt.plot(epochs, metrics[\"val\"], label=\"Val WG Acc\")\n        plt.title(\n            f\"{name} Dataset - Worst\u2010Group Accuracy\\nTrain (blue) vs Validation (orange)\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Worst\u2010Group Accuracy\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{name}_wg_accuracy.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating {name} worst\u2010group accuracy plot: {e}\")\n        plt.close()\n\n    # Loss curve\n    try:\n        epochs = np.arange(len(losses.get(\"train\", [])))\n        plt.figure()\n        plt.plot(epochs, losses[\"train\"], label=\"Train Loss\")\n        plt.plot(epochs, losses[\"val\"], label=\"Val Loss\")\n        plt.title(f\"{name} Dataset - Loss Curve\\nTrain (blue) vs Validation (orange)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{name}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating {name} loss curve plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Prepare keys\ndr_keys = sorted(\n    experiment_data.get(\"dropout_rate\", {}).get(\"synthetic\", {}).keys(),\n    key=lambda x: float(x),\n)\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for dr in dr_keys:\n        res = experiment_data[\"dropout_rate\"][\"synthetic\"][dr]\n        epochs = range(len(res[\"losses\"][\"train\"]))\n        plt.plot(epochs, res[\"losses\"][\"train\"], label=f\"Train dr={dr}\")\n        plt.plot(epochs, res[\"losses\"][\"val\"], label=f\"Val dr={dr}\")\n    plt.title(\"Loss Curves (Synthetic dataset)\\nTrain vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# Plot worst-group accuracy curves\ntry:\n    plt.figure()\n    for dr in dr_keys:\n        res = experiment_data[\"dropout_rate\"][\"synthetic\"][dr]\n        epochs = range(len(res[\"metrics\"][\"train\"]))\n        plt.plot(epochs, res[\"metrics\"][\"train\"], label=f\"Train dr={dr}\")\n        plt.plot(epochs, res[\"metrics\"][\"val\"], label=f\"Val dr={dr}\")\n    plt.title(\n        \"Worst-group Accuracy Curves (Synthetic dataset)\\nTrain vs Validation WG Accuracy\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Worst-group Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_wg_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating wg accuracy curves: {e}\")\n    plt.close()\n\n# Plot test accuracy vs dropout rate\ntry:\n    plt.figure()\n    dr_list = [float(dr) for dr in dr_keys]\n    acc_list = []\n    for dr in dr_keys:\n        res = experiment_data[\"dropout_rate\"][\"synthetic\"][dr]\n        preds = np.array(res[\"predictions\"])\n        truths = np.array(res[\"ground_truth\"])\n        acc = (preds == truths).mean()\n        acc_list.append(acc)\n        print(f\"Dropout {dr}: Test Accuracy = {acc:.4f}\")\n    plt.bar(dr_list, acc_list)\n    plt.title(\"Test Accuracy vs Dropout Rate (Synthetic dataset)\")\n    plt.xlabel(\"Dropout Rate\")\n    plt.ylabel(\"Test Accuracy\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_test_accuracy_vs_dropout.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy bar chart: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    ed = experiment_data[\"n_clusters_tuning\"][\"synthetic\"]\n    n_clusters = ed[\"n_clusters\"]\n    train_acc = ed[\"metrics\"][\"train\"]\n    val_acc = ed[\"metrics\"][\"val\"]\n    train_loss = ed[\"losses\"][\"train\"]\n    val_loss = ed[\"losses\"][\"val\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for nc, t_acc in zip(n_clusters, train_acc):\n        axes[0].plot(range(len(t_acc)), t_acc, label=f\"n_clusters={nc}\")\n    for nc, v_acc in zip(n_clusters, val_acc):\n        axes[1].plot(range(len(v_acc)), v_acc, label=f\"n_clusters={nc}\")\n    fig.suptitle(\"Weighted Group Accuracy vs Epoch (Synthetic)\")\n    axes[0].set_title(\"Left: Training Accuracy\")\n    axes[1].set_title(\"Right: Validation Accuracy\")\n    for ax in axes:\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"WG Accuracy\")\n        ax.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_wg_acc_vs_epoch.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating weighted accuracy plot: {e}\")\n    plt.close(\"all\")\n\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for nc, t_l in zip(n_clusters, train_loss):\n        axes[0].plot(range(len(t_l)), t_l, label=f\"n_clusters={nc}\")\n    for nc, v_l in zip(n_clusters, val_loss):\n        axes[1].plot(range(len(v_l)), v_l, label=f\"n_clusters={nc}\")\n    fig.suptitle(\"Cross-Entropy Loss vs Epoch (Synthetic)\")\n    axes[0].set_title(\"Left: Training Loss\")\n    axes[1].set_title(\"Right: Validation Loss\")\n    for ax in axes:\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n        ax.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_vs_epoch.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close(\"all\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nls_dict = experiment_data.get(\"label_smoothing\", {})\n\n# Plot worst-group accuracy curves\ntry:\n    plt.figure()\n    for eps, info in ls_dict.items():\n        syn = info[\"synthetic\"]\n        train_acc = syn[\"metrics\"][\"train\"]\n        val_acc = syn[\"metrics\"][\"val\"]\n        epochs = np.arange(len(train_acc))\n        plt.plot(epochs, train_acc, label=f\"{eps} train\")\n        plt.plot(epochs, val_acc, \"--\", label=f\"{eps} val\")\n    plt.title(\"Worst-Group Accuracy vs Epochs\\nSynthetic dataset, label smoothing\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Worst-Group Accuracy\")\n    plt.legend()\n    plt.savefig(\n        os.path.join(working_dir, \"synthetic_worst_group_accuracy_vs_epochs.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating wg_acc plot: {e}\")\n    plt.close()\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for eps, info in ls_dict.items():\n        syn = info[\"synthetic\"]\n        train_loss = syn[\"losses\"][\"train\"]\n        val_loss = syn[\"losses\"][\"val\"]\n        epochs = np.arange(len(train_loss))\n        plt.plot(epochs, train_loss, label=f\"{eps} train\")\n        plt.plot(epochs, val_loss, \"--\", label=f\"{eps} val\")\n    plt.title(\"Loss vs Epochs\\nSynthetic dataset, label smoothing\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_vs_epochs.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Print final validation metrics\nfor eps, info in ls_dict.items():\n    syn = info[\"synthetic\"]\n    final_val_acc = syn[\"metrics\"][\"val\"][-1]\n    final_val_loss = syn[\"losses\"][\"val\"][-1]\n    print(\n        f\"eps={eps} final val wg_acc={final_val_acc:.4f}, val loss={final_val_loss:.4f}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nds = experiment_data.get(\"momentum_sweep\", {}).get(\"synthetic\", {})\nmoms = ds.get(\"momentum_values\", [])\ntrain_wg = ds.get(\"metrics\", {}).get(\"train\", [])\nval_wg = ds.get(\"metrics\", {}).get(\"val\", [])\ntrain_loss = ds.get(\"losses\", {}).get(\"train\", [])\nval_loss = ds.get(\"losses\", {}).get(\"val\", [])\n\nepochs = list(range(len(train_wg[0]))) if train_wg else []\n\n# 1. Training worst-group accuracy\ntry:\n    plt.figure()\n    for m, wg in zip(moms, train_wg):\n        plt.plot(epochs, wg, label=f\"momentum={m}\")\n    plt.title(\n        \"Synthetic Dataset - Training Worst-Group Accuracy vs Epoch (Momentum Sweep)\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Worst-Group Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_train_wg.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\n# 2. Validation worst-group accuracy\ntry:\n    plt.figure()\n    for m, wg in zip(moms, val_wg):\n        plt.plot(epochs, wg, label=f\"momentum={m}\")\n    plt.title(\n        \"Synthetic Dataset - Validation Worst-Group Accuracy vs Epoch (Momentum Sweep)\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Worst-Group Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_val_wg.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n\n# 3. Training loss\ntry:\n    plt.figure()\n    for m, ls in zip(moms, train_loss):\n        plt.plot(epochs, ls, label=f\"momentum={m}\")\n    plt.title(\"Synthetic Dataset - Training Loss vs Epoch (Momentum Sweep)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_train_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot3: {e}\")\n    plt.close()\n\n# 4. Validation loss\ntry:\n    plt.figure()\n    for m, ls in zip(moms, val_loss):\n        plt.plot(epochs, ls, label=f\"momentum={m}\")\n    plt.title(\"Synthetic Dataset - Validation Loss vs Epoch (Momentum Sweep)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_val_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot4: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot weighted accuracy curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for norm, data in experiment_data.get(\"max_grad_norm\", {}).items():\n        rec = data[\"synthetic\"]\n        epochs = range(len(rec[\"metrics\"][\"train\"]))\n        axes[0].plot(epochs, rec[\"metrics\"][\"train\"], label=f\"norm={norm}\")\n        axes[1].plot(epochs, rec[\"metrics\"][\"val\"], label=f\"norm={norm}\")\n    axes[0].set_title(\"Training Weighted Accuracy\")\n    axes[1].set_title(\"Validation Weighted Accuracy\")\n    for ax in axes:\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Weighted Accuracy\")\n        ax.legend()\n    fig.suptitle(\n        \"Synthetic Dataset Weighted Accuracy across Epochs\\nLeft: Training, Right: Validation\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_weighted_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating weighted accuracy plot: {e}\")\n    plt.close()\n\n# Plot loss curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for norm, data in experiment_data.get(\"max_grad_norm\", {}).items():\n        rec = data[\"synthetic\"]\n        epochs = range(len(rec[\"losses\"][\"train\"]))\n        axes[0].plot(epochs, rec[\"losses\"][\"train\"], label=f\"norm={norm}\")\n        axes[1].plot(epochs, rec[\"losses\"][\"val\"], label=f\"norm={norm}\")\n    axes[0].set_title(\"Training Loss\")\n    axes[1].set_title(\"Validation Loss\")\n    for ax in axes:\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n        ax.legend()\n    fig.suptitle(\n        \"Synthetic Dataset Loss across Epochs\\nLeft: Training, Right: Validation\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    data = experiment_data[\"learning_rate\"][\"synthetic\"]\n    lrs = data[\"lrs\"]\n    metrics_train = data[\"metrics\"][\"train\"]\n    metrics_val = data[\"metrics\"][\"val\"]\n    losses_train = data[\"losses\"][\"train\"]\n    losses_val = data[\"losses\"][\"val\"]\nexcept Exception as e:\n    print(f\"Error loading or extracting experiment data: {e}\")\n\n# Plot worst\u2010group accuracy curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, metrics_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, metrics_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, metrics_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training WG Accuracy\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Worst\u2010group Accuracy\")\n    axes[1].set_title(\"Validation WG Accuracy\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Worst\u2010group Accuracy\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\n        \"Synthetic dataset - Worst\u2010group Accuracy\\nLeft: Training, Right: Validation\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_worst_group_accuracy.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close(\"all\")\n\n# Plot loss curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, losses_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, losses_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, losses_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[1].set_title(\"Validation Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\"Synthetic dataset - Loss Curves\\nLeft: Training, Right: Validation\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close(\"all\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    data = experiment_data[\"learning_rate\"][\"synthetic\"]\n    lrs = data[\"lrs\"]\n    metrics_train = data[\"metrics\"][\"train\"]\n    metrics_val = data[\"metrics\"][\"val\"]\n    losses_train = data[\"losses\"][\"train\"]\n    losses_val = data[\"losses\"][\"val\"]\nexcept Exception as e:\n    print(f\"Error loading or extracting experiment data: {e}\")\n\n# Plot worst\u2010group accuracy curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, metrics_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, metrics_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, metrics_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training WG Accuracy\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Worst\u2010group Accuracy\")\n    axes[1].set_title(\"Validation WG Accuracy\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Worst\u2010group Accuracy\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\n        \"Synthetic dataset - Worst\u2010group Accuracy\\nLeft: Training, Right: Validation\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_worst_group_accuracy.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close(\"all\")\n\n# Plot loss curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, losses_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, losses_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, losses_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[1].set_title(\"Validation Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\"Synthetic dataset - Loss Curves\\nLeft: Training, Right: Validation\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close(\"all\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    data = experiment_data[\"learning_rate\"][\"synthetic\"]\n    lrs = data[\"lrs\"]\n    metrics_train = data[\"metrics\"][\"train\"]\n    metrics_val = data[\"metrics\"][\"val\"]\n    losses_train = data[\"losses\"][\"train\"]\n    losses_val = data[\"losses\"][\"val\"]\nexcept Exception as e:\n    print(f\"Error loading or extracting experiment data: {e}\")\n\n# Plot worst\u2010group accuracy curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, metrics_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, metrics_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, metrics_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training WG Accuracy\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Worst\u2010group Accuracy\")\n    axes[1].set_title(\"Validation WG Accuracy\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Worst\u2010group Accuracy\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\n        \"Synthetic dataset - Worst\u2010group Accuracy\\nLeft: Training, Right: Validation\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_worst_group_accuracy.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close(\"all\")\n\n# Plot loss curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, losses_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, losses_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, losses_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[1].set_title(\"Validation Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\"Synthetic dataset - Loss Curves\\nLeft: Training, Right: Validation\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close(\"all\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Paths to all experiment_data.npy files\nexperiment_data_path_list = [\n    \"experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_ff10ac06c81242499409da36b655cf2f_proc_3806/experiment_data.npy\",\n    \"None/experiment_data.npy\",\n    \"experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_563867db33f741d7b58a23453bddaeb9_proc_3804/experiment_data.npy\",\n]\n\n# Load all experiments\nall_experiment_data = []\nfor path in experiment_data_path_list:\n    try:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\"), path)\n        exp = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(exp)\n    except Exception as e:\n        print(f\"Error loading experiment data from {path}: {e}\")\n\nif not all_experiment_data:\n    print(\"No experiment data loaded, exiting.\")\nelse:\n    # Identify datasets under \"learning_rate\"\n    lr_group = all_experiment_data[0].get(\"learning_rate\", {})\n    for dataset_name in lr_group:\n        # Stack metrics and losses across experiments\n        try:\n            lrs = np.array(lr_group[dataset_name][\"lrs\"])\n            train_metrics = np.stack(\n                [\n                    exp[\"learning_rate\"][dataset_name][\"metrics\"][\"train\"]\n                    for exp in all_experiment_data\n                ],\n                axis=0,\n            )\n            val_metrics = np.stack(\n                [\n                    exp[\"learning_rate\"][dataset_name][\"metrics\"][\"val\"]\n                    for exp in all_experiment_data\n                ],\n                axis=0,\n            )\n            train_losses = np.stack(\n                [\n                    exp[\"learning_rate\"][dataset_name][\"losses\"][\"train\"]\n                    for exp in all_experiment_data\n                ],\n                axis=0,\n            )\n            val_losses = np.stack(\n                [\n                    exp[\"learning_rate\"][dataset_name][\"losses\"][\"val\"]\n                    for exp in all_experiment_data\n                ],\n                axis=0,\n            )\n            n_expts = train_metrics.shape[0]\n            epochs_acc = np.arange(1, train_metrics.shape[2] + 1)\n            epochs_loss = np.arange(1, train_losses.shape[2] + 1)\n            mean_train_m = train_metrics.mean(axis=0)\n            se_train_m = train_metrics.std(axis=0) / np.sqrt(n_expts)\n            mean_val_m = val_metrics.mean(axis=0)\n            se_val_m = val_metrics.std(axis=0) / np.sqrt(n_expts)\n            mean_train_l = train_losses.mean(axis=0)\n            se_train_l = train_losses.std(axis=0) / np.sqrt(n_expts)\n            mean_val_l = val_losses.mean(axis=0)\n            se_val_l = val_losses.std(axis=0) / np.sqrt(n_expts)\n        except Exception as e:\n            print(f\"Error stacking data for dataset {dataset_name}: {e}\")\n            continue\n\n        # Plot worst\u2010group accuracy with mean \u00b1 SE\n        try:\n            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n            for i, lr in enumerate(lrs):\n                axes[0].plot(epochs_acc, mean_train_m[i], label=f\"lr={lr}\")\n                axes[0].fill_between(\n                    epochs_acc,\n                    mean_train_m[i] - se_train_m[i],\n                    mean_train_m[i] + se_train_m[i],\n                    alpha=0.2,\n                )\n                axes[1].plot(epochs_acc, mean_val_m[i], label=f\"lr={lr}\")\n                axes[1].fill_between(\n                    epochs_acc,\n                    mean_val_m[i] - se_val_m[i],\n                    mean_val_m[i] + se_val_m[i],\n                    alpha=0.2,\n                )\n            axes[0].set_title(\"Training WG Accuracy\")\n            axes[0].set_xlabel(\"Epoch\")\n            axes[0].set_ylabel(\"Worst\u2010group Accuracy\")\n            axes[1].set_title(\"Validation WG Accuracy\")\n            axes[1].set_xlabel(\"Epoch\")\n            axes[1].set_ylabel(\"Worst\u2010group Accuracy\")\n            for ax in axes:\n                ax.legend()\n            fig.suptitle(\n                f\"{dataset_name} dataset - Worst\u2010group Accuracy\\nLeft: Training, Right: Validation\"\n            )\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset_name}_wg_accuracy_mean_se.png\")\n            )\n            plt.close(fig)\n        except Exception as e:\n            print(f\"Error creating WG accuracy plot for {dataset_name}: {e}\")\n            plt.close(\"all\")\n\n        # Plot loss curves with mean \u00b1 SE\n        try:\n            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n            for i, lr in enumerate(lrs):\n                axes[0].plot(epochs_loss, mean_train_l[i], label=f\"lr={lr}\")\n                axes[0].fill_between(\n                    epochs_loss,\n                    mean_train_l[i] - se_train_l[i],\n                    mean_train_l[i] + se_train_l[i],\n                    alpha=0.2,\n                )\n                axes[1].plot(epochs_loss, mean_val_l[i], label=f\"lr={lr}\")\n                axes[1].fill_between(\n                    epochs_loss,\n                    mean_val_l[i] - se_val_l[i],\n                    mean_val_l[i] + se_val_l[i],\n                    alpha=0.2,\n                )\n            axes[0].set_title(\"Training Loss\")\n            axes[0].set_xlabel(\"Epoch\")\n            axes[0].set_ylabel(\"Loss\")\n            axes[1].set_title(\"Validation Loss\")\n            axes[1].set_xlabel(\"Epoch\")\n            axes[1].set_ylabel(\"Loss\")\n            for ax in axes:\n                ax.legend()\n            fig.suptitle(\n                f\"{dataset_name} dataset - Loss Curves\\nLeft: Training, Right: Validation\"\n            )\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset_name}_loss_curves_mean_se.png\")\n            )\n            plt.close(fig)\n        except Exception as e:\n            print(f\"Error creating loss curves plot for {dataset_name}: {e}\")\n            plt.close(\"all\")\n\n        # Print final validation worst\u2010group accuracy means and SE\n        final_means = mean_val_m[:, -1]\n        final_ses = se_val_m[:, -1]\n        print(f\"Final Validation WG Accuracy for dataset '{dataset_name}':\")\n        for lr, m, s in zip(lrs, final_means, final_ses):\n            print(f\"  lr={lr}: {m:.3f} \u00b1 {s:.3f}\")\n"], "plot_plan": ["I will load the experiment data from the working directory and extract training/validation losses, weighted\u2010group accuracy metrics, and test predictions vs. ground truth for the synthetic dataset. Then I will create three separate figures\u2014each wrapped in its own try\u2010except\u2014to visualize: (1) loss curves with left subplot for training loss and right for validation loss; (2) weighted\u2010group accuracy curves similarly arranged; and (3) class distributions showing ground truth vs. prediction counts. Each figure will include a suptitle specifying the synthetic dataset and subplot titles in the style \u201cLeft: \u2026, Right: \u2026\u201d. I will label axes with epochs or class and add markers for clarity. Finally, each plot will be saved to `working_dir` with descriptive filenames, and figures will be closed after saving or on error.", "We will load the saved experiment data from the working directory and extract the metrics and losses for the synthetic dataset across learning rates. Then we construct two summary figures: one showing worst\u2010group accuracy and another showing losses, each as side\u2010by\u2010side subplots for training vs validation curves over epochs. We label axes, include legends for each learning rate, and add a main title with a clear subtitle indicating which side is training vs validation. Each figure is generated inside its own try\u2010except block to catch errors, save to the working directory with descriptive filenames, and always close the figure afterward.", "I will load the saved experiment data from the working directory and extract the synthetic dataset entries for batch sizes, training/validation losses, and weighted group accuracies. Then I will create two figures with subplots: one showing the left subplot for training loss and the right for validation loss across epochs for each batch size, titled \u201cSynthetic Dataset Loss Curves\u201d with the subtitle \u201cLeft: Train Loss, Right: Val Loss.\u201d Similarly, a second figure will show the weighted group accuracy over epochs with left train and right validation curves, titled \u201cSynthetic Dataset Weighted Group Accuracy\u201d and subtitle \u201cLeft: Train, Right: Val.\u201d Each plot will be wrapped in a try-except block to handle any errors, and figures will be closed after saving. Filenames will include \u201csynthetic_loss_curves.png\u201d and \u201csynthetic_wg_accuracy_curves.png\u201d to convey the dataset and plot type. All plots will be saved in the working directory. I will include clear axis labels and legends to differentiate batch size lines and use only basic matplotlib calls and numpy for loading the .npy data file.", "First, the code loads the saved experiment data from `working/experiment_data.npy` and extracts the synthetic dataset\u2019s weight decay values, per\u2010epoch training/validation losses, and worst\u2010group accuracies. It then computes the epoch indices based on the loss array shape. In the first try\u2010except block, it overlays training (solid) and validation (dashed) loss curves for each weight decay on a single figure, labels axes, adds a subtitle indicating the synthetic dataset, and saves to `working/synthetic_loss_curves.png`. The second block similarly plots worst\u2010group accuracy curves with matching line styles and saves to `working/synthetic_wgacc_curves.png`. In the third block, the final validation worst\u2010group accuracy (last epoch) is plotted against weight decay values with markers, and the figure is saved to `working/synthetic_final_wgacc_vs_wd.png`. Each plot is wrapped in its own try\u2010except to gracefully handle errors and ensure figures are always closed after saving, and all filenames include the dataset name and plot type.", "We begin by loading the saved experiment_data.npy from the working directory and handling any load errors. We then iterate over each hyperparameter setting stored under \"train_epochs\". For each setting, we extract the synthetic dataset\u2019s loss and worst\u2010group accuracy arrays for both train and validation. We create a single figure with two subplots: the left showing train vs. validation loss curves, and the right showing train vs. validation worst\u2010group accuracy curves. We add a clear suptitle specifying the dataset and hyperparameter and subtitle directions (\u2018Left: Loss Curves, Right: Worst\u2010group Accuracy\u2019). Each subplot gets its own title, axis labels, and legend. The figure is saved with a descriptive filename in the working directory, and we always close the figure, with try\u2010except blocks to catch and report any plotting errors.", "We start by loading the saved experiment data, extracting the synthetic dataset\u2019s warmup values along with its training/validation losses and weighted\u2010group accuracy metrics.  We then generate three separate figures, each within its own try\u2010except block: (1) overlaid loss curves (solid for training, dashed for validation) across warmup settings, (2) weighted\u2010group accuracy curves in the same style, and (3) a bar chart of the final validation weighted\u2010group accuracies for each warmup value.  We label and title each plot clearly, specifying \u201cSynthetic\u201d in filenames and titles, and always close figures even on errors.  All plots are saved to the working directory.", null, "We first load the saved `experiment_data.npy` and then loop over each dataset to plot its training and validation curves. For each dataset, we create two separate plots: one for worst\u2010group accuracy over epochs and another for training/validation loss. Each plotting routine is wrapped in its own `try-except` block so that any errors during figure creation are caught and printed, and the figure is always closed afterward. We use basic matplotlib, label the axes and legends clearly, and include dataset names and a subtitle indicating which line corresponds to training vs validation. Finally, each figure is saved into the `working` directory with descriptive filenames that include the dataset name and the type of plot.", "I will write Python code that loads the saved experiment_data.npy from the working directory and extracts the results for the synthetic dataset. I will generate three separate plots\u2014loss curves, worst-group accuracy curves, and test accuracy versus dropout rate\u2014for the synthetic dataset, each enclosed in its own try-except block to handle errors gracefully. For the loss and accuracy curves, I will plot train and validation curves across epochs for each dropout rate, adding appropriate legends and clear titles including subtitles. The test accuracy bar chart will compute overall test accuracy from the stored predictions and ground truth arrays, then plot accuracy as a function of dropout rate. Each figure will be titled clearly and saved to working_dir with descriptive filenames. I will close each figure after saving to prevent resource leakage. If any errors occur during plotting, the exception will be printed and the figure closed. Additionally, I will print the computed test accuracies for each dropout rate to stdout.", "We will load the experiment data from the working directory using numpy and extract the synthetic dataset\u2019s hyperparameter grid along with its recorded weighted group accuracies and losses. We then construct two figures, each with a two\u2010panel layout: one for the weighted group accuracy curves and one for the cross\u2010entropy loss curves over epochs across different n_clusters settings. In each figure the left panel shows training curves and the right panel shows validation curves. We add an overall title for each figure, clear subtitles (\u201cLeft: Training\u2026, Right: Validation\u2026\u201d), axis labels, and legends indicating the n_clusters values. Each plotting step is wrapped in its own try\u2010except block to gracefully handle errors. After saving, we close the figure to free resources. Only the loaded experiment results are plotted using basic matplotlib without custom styles.", "I will load the saved experiment data from the working directory, then extract the label\u2010smoothing entries for the synthetic dataset. For each smoothing value, I will retrieve the train/validation worst\u2010group accuracies and losses over epochs. I will create two summary plots: one comparing worst\u2010group accuracy curves across label\u2010smoothing values, and another comparing loss curves. Each plot is wrapped in its own try/except, includes a clear title and subtitle referencing the synthetic dataset, labeled axes, and a legend; the figures are saved to the working directory with descriptive filenames and then closed. Finally, I will print the final validation worst\u2010group accuracy and loss for each smoothing setting to the console.", "First, load the experiment data from the working directory and extract the synthetic dataset\u2019s momentum sweep results. Then for each performance metric (training worst\u2010group accuracy, validation worst\u2010group accuracy, training loss, validation loss), create a separate try\u2010except block. Within each block, start a new figure and plot curves for each momentum value across epochs, labeling axes and adding a legend. Give each plot a clear title specifying the synthetic dataset and the metric, noting that this is a momentum sweep. Save each figure to the working directory with descriptive filenames that include the dataset name and plot type. Ensure that every figure is closed after saving or upon encountering an error. This yields four standard visualizations summarizing training and validation performance over epochs for different momentum settings.", "We load the saved experiment data and extract the synthetic results for all max_grad_norm settings.  \nWe create two summary figures: one for weighted accuracy and one for losses.  \nEach figure has two side\u2010by\u2010side panels\u2014left shows the training curves and right shows validation curves\u2014so the subtitle clearly indicates \"Left: Training, Right: Validation.\"  \nWe plot curves for each max_grad_norm value in both panels, add legends, axis labels, and a descriptive main title specifying the synthetic dataset.  \nEach figure is generated inside its own try-except block, saved to working_dir with a descriptive filename, and the figure is closed afterward.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "learning_rate", "batch_size", "weight_decay", "train_epochs", "warmup_epochs", "hidden_dim", null, "dropout_rate", "n_clusters", "label_smoothing", "momentum", "max_grad_norm", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["I will load the saved NumPy experiment file, extract the final training and\nvalidation losses as well as the robust training and validation accuracies from\nthe stored lists, compute the overall test accuracy from the saved predictions\nand ground truth arrays, and then print each metric with clear labels under the\ndataset name. The script runs immediately at the global scope without any\nguarded entry point.", "This script loads the NumPy file saved under the `working` directory to extract\nthe final-epoch worst\u2010group accuracies and average losses for the training and\nvalidation splits, and selects the learning rate that yields the highest\nvalidation worst\u2010group accuracy. It then prints clearly labeled metrics for the\ntraining and validation sets. Finally, it computes and prints the overall test\naccuracy by comparing the saved test predictions with the ground truth labels.", "The script loads the saved `experiment_data.npy` from the `working` directory,\nextracts the synthetic experiment sub-dictionary, and reads the batch sizes\nalongside the corresponding training/validation metrics and losses arrays. It\nthen prints a \u201cSynthetic dataset\u201d header and, for each batch size, outputs the\nfinal epoch\u2019s training accuracy, validation accuracy, training loss, and\nvalidation loss with explicit metric labels. All code is at the global scope and\nexecutes immediately upon running, without an entry\u2010point guard.", "I will load the saved experiment data dictionary from the working directory,\npull out the arrays of worst\u2010group accuracies and losses for each weight decay,\nand compute the final (last\u2010epoch) values. I then select the weight decay that\nachieved the highest validation worst\u2010group accuracy at the final epoch, and\nprint that run\u2019s final training and validation worst\u2010group accuracies and\nlosses, as well as the overall test accuracy, labeling each dataset and metric\nclearly. All of this executes at the global scope without any special entry\npoint.", "Below is a script that immediately loads the saved experiment data, iterates\nover each hyperparameter setting, and for each prints the synthetic dataset\u2019s\nname followed by the final worst\u2010group accuracies for training and validation as\nwell as the overall test accuracy. The code uses clear metric labels, computes\ntest accuracy from the stored predictions, and avoids any special entry points\nor plotting.", "The following script loads the saved experiment data, unpacks the stored metrics\nand losses for each warmup epoch setting, and computes the final worst\u2010group\naccuracies and losses for the training and validation splits. It also computes\nthe overall test accuracy by comparing the saved predictions against the ground\ntruth. Finally, it prints each dataset name along with clearly labeled metrics\nat their final epoch for each warmup configuration, all at global scope without\nan entry\u2010point guard.", "I will load the saved `experiment_data.npy` from the working directory, extract\nthe final epoch metrics for training and validation (weighted group accuracy and\nloss) for each hidden dimension, and compute test set accuracy and F1 score\nusing the stored predictions and ground truth. The script prints the dataset\nname and each metric with clear labels, and it runs immediately at global scope\nwithout an entry\u2010point guard.", "I will load the saved `experiment_data.npy` from the working directory and\niterate over each dataset to extract the final worst\u2010group accuracy and loss for\nboth training and validation splits. I will also compute test accuracy from the\nstored predictions and ground truth. Finally, each dataset\u2019s name and its\nprecisely labeled final metrics will be printed.", "The following script immediately loads the saved `experiment_data.npy` file from\nthe `working` directory and iterates over each dropout rate under the\n\u201csynthetic\u201d dataset. It then prints the dataset name (\u201cTraining set\u201d,\n\u201cValidation set\u201d, \u201cTest set\u201d) followed by the final worst\u2010group accuracy and\nloss for the training and validation splits, and the overall test accuracy\ncomputed from the stored predictions and ground truth. All code is at the global\nscope and executes on import without any `if __name__ == \"__main__\":` guard.", "We load the saved experiment data from the working directory, extract the\n\u201csynthetic\u201d entry, and compute the final weighted accuracies on the train and\nvalidation splits for each tested cluster count. We then pick the cluster\nsetting that achieves the highest final validation accuracy, and report its\ncorresponding final train and validation accuracies. We also compute the test\naccuracy from the stored predictions and ground truth for that best setting.\nFinally, we print the dataset name and each metric with clear labels.", "I will load the saved experiment data from the working directory and then\niterate through each label\u2010smoothing configuration to extract the final epoch\u2019s\nworst\u2010group accuracies and losses for the synthetic dataset. I compute the test\naccuracy from the stored predictions and ground truth, and then print each\nmetric immediately under its dataset name with clear, descriptive labels.", "I will import os and numpy, set the working directory, and load the saved\n`experiment_data.npy` with `allow_pickle=True`. Then I'll extract the momentum\nsweep dictionary containing momentum values, training and validation histories\nfor losses and worst-group accuracies, and test predictions and ground truth.\nFor each momentum value, I'll retrieve the final epoch's training and validation\nloss and worst\u2010group accuracy and compute the overall test accuracy from the\npredictions. Finally, I'll print the momentum value, and for each dataset\n(\u2018Training dataset\u2019, \u2018Validation dataset\u2019, \u2018Test dataset\u2019), output the final\nmetrics with clear labels.", "The following script loads the saved experiment data, iterates over each\ngradient\u2010norm setting, and for the \u201csynthetic\u201d dataset it prints the dataset\nname and the final values of training worst\u2010group accuracy, validation\nworst\u2010group accuracy, training loss, validation loss, and overall test accuracy.\nAll outputs are clearly labeled and the script runs immediately when executed:", "This script loads the NumPy file saved under the `working` directory to extract\nthe final-epoch worst\u2010group accuracies and average losses for the training and\nvalidation splits, and selects the learning rate that yields the highest\nvalidation worst\u2010group accuracy. It then prints clearly labeled metrics for the\ntraining and validation sets. Finally, it computes and prints the overall test\naccuracy by comparing the saved test predictions with the ground truth labels.", "This script loads the NumPy file saved under the `working` directory to extract\nthe final-epoch worst\u2010group accuracies and average losses for the training and\nvalidation splits, and selects the learning rate that yields the highest\nvalidation worst\u2010group accuracy. It then prints clearly labeled metrics for the\ntraining and validation sets. Finally, it computes and prints the overall test\naccuracy by comparing the saved test predictions with the ground truth labels.", "This script loads the NumPy file saved under the `working` directory to extract\nthe final-epoch worst\u2010group accuracies and average losses for the training and\nvalidation splits, and selects the learning rate that yields the highest\nvalidation worst\u2010group accuracy. It then prints clearly labeled metrics for the\ntraining and validation sets. Finally, it computes and prints the overall test\naccuracy by comparing the saved test predictions with the ground truth labels.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through datasets in the experiment data\nfor dataset_name, dataset in experiment_data.items():\n    # Extract stored metrics and losses\n    train_losses = dataset[\"losses\"][\"train\"]\n    val_losses = dataset[\"losses\"][\"val\"]\n    train_metrics = dataset[\"metrics\"][\"train\"]\n    val_metrics = dataset[\"metrics\"][\"val\"]\n\n    # Get final values\n    final_train_loss = train_losses[-1]\n    final_val_loss = val_losses[-1]\n    final_train_acc = train_metrics[-1]\n    final_val_acc = val_metrics[-1]\n\n    # Compute test accuracy if predictions are available\n    test_acc = None\n    if \"predictions\" in dataset and \"ground_truth\" in dataset:\n        preds = dataset[\"predictions\"]\n        truths = dataset[\"ground_truth\"]\n        test_acc = (preds == truths).mean()\n\n    # Print results\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"train loss: {final_train_loss:.4f}\")\n    print(f\"train accuracy: {final_train_acc:.4f}\")\n    print(f\"validation loss: {final_val_loss:.4f}\")\n    print(f\"validation accuracy: {final_val_acc:.4f}\")\n    if test_acc is not None:\n        print(f\"test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\ndata_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Navigate to the synthetic learning-rate sweep results\nsynthetic = experiment_data[\"learning_rate\"][\"synthetic\"]\nlrs = synthetic[\"lrs\"]\nmetrics_train = synthetic[\"metrics\"][\"train\"]  # shape: (num_lrs, epochs)\nmetrics_val = synthetic[\"metrics\"][\"val\"]\nlosses_train = synthetic[\"losses\"][\"train\"]\nlosses_val = synthetic[\"losses\"][\"val\"]\npredictions = synthetic[\"predictions\"]  # shape: (num_lrs, num_test_samples)\nground_truth = synthetic[\"ground_truth\"]\n\n# Extract final-epoch values\nfinal_train_wg = metrics_train[:, -1]\nfinal_val_wg = metrics_val[:, -1]\nfinal_train_loss = losses_train[:, -1]\nfinal_val_loss = losses_val[:, -1]\n\n# Select the best learning rate by highest final validation worst-group accuracy\nbest_idx = np.argmax(final_val_wg)\n\n# Print metrics with clear labels\nprint(\"Training Dataset Metrics:\")\nprint(f\"  Final training worst-group accuracy: {final_train_wg[best_idx]:.4f}\")\nprint(f\"  Final training average loss: {final_train_loss[best_idx]:.4f}\\n\")\n\nprint(\"Validation Dataset Metrics:\")\nprint(f\"  Final validation worst-group accuracy: {final_val_wg[best_idx]:.4f}\")\nprint(f\"  Final validation average loss: {final_val_loss[best_idx]:.4f}\\n\")\n\n# Compute and print test accuracy\ntest_acc = np.mean(predictions[best_idx] == ground_truth)\nprint(\"Test Dataset Metrics:\")\nprint(f\"  Test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexp_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\n# Extract the synthetic experiment results\nsynthetic = experiment_data[\"batch_size\"][\"synthetic\"]\nbatch_sizes = synthetic[\"batch_sizes\"]\ntrain_accs = synthetic[\"metrics\"][\"train\"]\nval_accs = synthetic[\"metrics\"][\"val\"]\ntrain_losses = synthetic[\"losses\"][\"train\"]\nval_losses = synthetic[\"losses\"][\"val\"]\n\n# Print final metrics for each batch size\nprint(\"Synthetic dataset\")\nfor i, bs in enumerate(batch_sizes):\n    final_train_acc = train_accs[i, -1]\n    final_val_acc = val_accs[i, -1]\n    final_train_loss = train_losses[i, -1]\n    final_val_loss = val_losses[i, -1]\n\n    print(f\"\\nBatch size: {bs}\")\n    print(f\"Final training accuracy: {final_train_acc:.4f}\")\n    print(f\"Final validation accuracy: {final_val_acc:.4f}\")\n    print(f\"Final training loss: {final_train_loss:.4f}\")\n    print(f\"Final validation loss: {final_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract synthetic sweep results\nexp = experiment_data[\"weight_decay\"][\"synthetic\"]\ndecays = exp[\"decays\"]\ntrain_wg_acc = exp[\"metrics\"][\"train\"]  # shape (n_wds, n_epochs)\nval_wg_acc = exp[\"metrics\"][\"val\"]\ntrain_loss = exp[\"losses\"][\"train\"]\nval_loss = exp[\"losses\"][\"val\"]\npreds = exp[\"predictions\"]  # shape (n_wds, n_test)\ngt = exp[\"ground_truth\"]\n\n# Determine final epoch and pick best weight decay by validation worst\u2010group acc\nn_wds, n_epochs = train_wg_acc.shape\nfinal_ep = n_epochs - 1\nbest_idx = np.argmax(val_wg_acc[:, final_ep])\n\n# Pull out final metrics for the best run\nfinal_train_acc = train_wg_acc[best_idx, final_ep]\nfinal_val_acc = val_wg_acc[best_idx, final_ep]\nfinal_train_loss = train_loss[best_idx, final_ep]\nfinal_val_loss = val_loss[best_idx, final_ep]\ntest_acc = (preds[best_idx] == gt).mean()\n\n# Print results\nprint(\"Training set:\")\nprint(f\"  train worst-group accuracy: {final_train_acc:.4f}\")\nprint(f\"  training loss: {final_train_loss:.4f}\")\n\nprint(\"Validation set:\")\nprint(f\"  validation worst-group accuracy: {final_val_acc:.4f}\")\nprint(f\"  validation loss: {final_val_loss:.4f}\")\n\nprint(\"Test set:\")\nprint(f\"  test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# 1. Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# 2. Loop over each hyperparameter setting and extract metrics\nfor te, result in experiment_data.get(\"train_epochs\", {}).items():\n    synthetic = result.get(\"synthetic\", {})\n    metrics = synthetic.get(\"metrics\", {})\n    train_wg_array = metrics.get(\"train\", np.array([]))\n    val_wg_array = metrics.get(\"val\", np.array([]))\n    # final values\n    train_wg_final = train_wg_array[-1] if train_wg_array.size > 0 else float(\"nan\")\n    val_wg_final = val_wg_array[-1] if val_wg_array.size > 0 else float(\"nan\")\n\n    # compute test accuracy from stored predictions and ground truth\n    preds = synthetic.get(\"predictions\", np.array([]))\n    truths = synthetic.get(\"ground_truth\", np.array([]))\n    test_acc = (\n        np.mean(preds == truths) if preds.size > 0 and truths.size > 0 else float(\"nan\")\n    )\n\n    # 3. Print dataset name and metrics\n    print(f\"Hyperparameter train_epochs = {te}\")\n    print(\"Synthetic dataset:\")\n    print(f\"  Final training worst\u2010group accuracy: {train_wg_final:.4f}\")\n    print(f\"  Final validation worst\u2010group accuracy: {val_wg_final:.4f}\")\n    print(f\"  Test accuracy: {test_acc:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(experiment_path, allow_pickle=True).item()\n\n# Navigate to the synthetic experiment under warmup_epochs\nsynthetic_exp = experiment_data[\"warmup_epochs\"][\"synthetic\"]\nwarmup_values = synthetic_exp[\"warmup_values\"]\ntrain_metrics_list = synthetic_exp[\"metrics\"][\"train\"]\nval_metrics_list = synthetic_exp[\"metrics\"][\"val\"]\ntrain_losses_list = synthetic_exp[\"losses\"][\"train\"]\nval_losses_list = synthetic_exp[\"losses\"][\"val\"]\ntest_labels = synthetic_exp[\"ground_truth\"]\ntest_predictions_list = synthetic_exp[\"predictions\"]\n\n# Iterate over each warmup setting and print final metrics\nfor w, tr_accs, vl_accs, tr_losses, vl_losses, preds in zip(\n    warmup_values,\n    train_metrics_list,\n    val_metrics_list,\n    train_losses_list,\n    val_losses_list,\n    test_predictions_list,\n):\n    final_train_wg_accuracy = tr_accs[-1]\n    final_val_wg_accuracy = vl_accs[-1]\n    final_train_loss = tr_losses[-1]\n    final_val_loss = vl_losses[-1]\n    test_accuracy = np.mean(preds == test_labels)\n\n    print(f\"Dataset: synthetic (warmup_epochs = {w})\")\n    print(f\"train worst-group accuracy: {final_train_wg_accuracy:.4f}\")\n    print(f\"validation worst-group accuracy: {final_val_wg_accuracy:.4f}\")\n    print(f\"train loss: {final_train_loss:.4f}\")\n    print(f\"validation loss: {final_val_loss:.4f}\")\n    print(f\"test accuracy: {test_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Navigate to synthetic experiment results\nsd = experiment_data[\"hidden_dim\"][\"synthetic\"]\nhidden_dims = sd[\"hidden_dims\"]\ntrain_wg = sd[\"metrics\"][\"train\"]  # shape: (n_hid, epochs)\nval_wg = sd[\"metrics\"][\"val\"]  # shape: (n_hid, epochs)\ntrain_loss = sd[\"losses\"][\"train\"]  # shape: (n_hid, epochs)\nval_loss = sd[\"losses\"][\"val\"]  # shape: (n_hid, epochs)\npredictions = sd[\"predictions\"]  # shape: (n_hid, n_test)\nground_truth = sd[\"ground_truth\"]  # shape: (n_test,)\n\n# Print final metrics for each hidden dimension\nfor i, hid in enumerate(hidden_dims):\n    print(f\"Hidden dimension: {hid}\")\n    # Training metrics\n    final_train_wg = train_wg[i, -1]\n    final_train_loss = train_loss[i, -1]\n    print(\"Dataset: Training\")\n    print(f\"  Training weighted group accuracy: {final_train_wg:.4f}\")\n    print(f\"  Training loss: {final_train_loss:.4f}\")\n    # Validation metrics\n    final_val_wg = val_wg[i, -1]\n    final_val_loss = val_loss[i, -1]\n    print(\"Dataset: Validation\")\n    print(f\"  Validation weighted group accuracy: {final_val_wg:.4f}\")\n    print(f\"  Validation loss: {final_val_loss:.4f}\")\n    # Test metrics\n    test_preds = predictions[i]\n    test_acc = accuracy_score(ground_truth, test_preds)\n    test_f1 = f1_score(ground_truth, test_preds)\n    print(\"Dataset: Test\")\n    print(f\"  Test accuracy: {test_acc:.4f}\")\n    print(f\"  Test F1 score: {test_f1:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, data in experiment_data.items():\n    # Extract final worst-group accuracies and losses\n    final_train_wg_acc = data[\"metrics\"][\"train\"][-1]\n    final_val_wg_acc = data[\"metrics\"][\"val\"][-1]\n    final_train_loss = data[\"losses\"][\"train\"][-1]\n    final_val_loss = data[\"losses\"][\"val\"][-1]\n    # Compute test accuracy\n    predictions = data[\"predictions\"]\n    ground_truth = data[\"ground_truth\"]\n    test_accuracy = (predictions == ground_truth).mean()\n\n    # Print results with precise labels\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"Final train worst-group accuracy: {final_train_wg_acc:.4f}\")\n    print(f\"Final validation worst-group accuracy: {final_val_wg_acc:.4f}\")\n    print(f\"Final train loss: {final_train_loss:.4f}\")\n    print(f\"Final validation loss: {final_val_loss:.4f}\")\n    print(f\"Test accuracy: {test_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experimental results\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Access the synthetic dataset results under the dropout rate sweep\ndropout_results = experiment_data[\"dropout_rate\"][\"synthetic\"]\n\n# Iterate through each dropout rate in ascending order\nfor dr_str in sorted(dropout_results.keys(), key=float):\n    dr = float(dr_str)\n    results = dropout_results[dr_str]\n    print(f\"Dropout rate: {dr}\")\n\n    # Training set metrics\n    train_wg_acc = results[\"metrics\"][\"train\"][-1]\n    train_loss = results[\"losses\"][\"train\"][-1]\n    print(\"Dataset: Training set\")\n    print(f\"training worst-group accuracy: {train_wg_acc:.4f}\")\n    print(f\"training loss: {train_loss:.4f}\")\n\n    # Validation set metrics\n    val_wg_acc = results[\"metrics\"][\"val\"][-1]\n    val_loss = results[\"losses\"][\"val\"][-1]\n    print(\"Dataset: Validation set\")\n    print(f\"validation worst-group accuracy: {val_wg_acc:.4f}\")\n    print(f\"validation loss: {val_loss:.4f}\")\n\n    # Test set metrics (overall accuracy)\n    preds = results[\"predictions\"]\n    truths = results[\"ground_truth\"]\n    test_acc = (preds == truths).mean()\n    print(\"Dataset: Test set\")\n    print(f\"test accuracy: {test_acc:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata = np.load(\n    os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n).item()\n\n# Extract the synthetic dataset results\ned = data[\"n_clusters_tuning\"][\"synthetic\"]\nn_clusters = ed[\"n_clusters\"]\nmetrics_train = ed[\"metrics\"][\"train\"]\nmetrics_val = ed[\"metrics\"][\"val\"]\n\n# Compute the final epoch accuracies for each hyperparameter\nfinal_train_accs = [m[-1] for m in metrics_train]\nfinal_val_accs = [m[-1] for m in metrics_val]\n\n# Select the best hyperparameter based on validation accuracy\nbest_idx = int(np.argmax(final_val_accs))\nbest_n = n_clusters[best_idx]\nbest_train_acc = final_train_accs[best_idx]\nbest_val_acc = final_val_accs[best_idx]\n\n# Compute test accuracy for the best setting\npreds = ed[\"predictions\"][best_idx]\ntruth = ed[\"ground_truth\"][best_idx]\ntest_acc = (preds == truth).mean()\n\n# Print the results\nprint(\"Dataset: synthetic\")\nprint(f\"train accuracy: {best_train_acc:.4f}\")\nprint(f\"validation accuracy: {best_val_acc:.4f}\")\nprint(f\"test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through label smoothing settings and datasets\nfor eps, smoothing_data in experiment_data.get(\"label_smoothing\", {}).items():\n    for dataset_name, dataset_info in smoothing_data.items():\n        print(f\"Dataset: {dataset_name}\")\n\n        # Extract losses and worst\u2010group accuracies\n        train_losses = dataset_info[\"losses\"][\"train\"]\n        val_losses = dataset_info[\"losses\"][\"val\"]\n        train_accs = dataset_info[\"metrics\"][\"train\"]\n        val_accs = dataset_info[\"metrics\"][\"val\"]\n\n        # Final epoch metrics\n        final_train_loss = train_losses[-1]\n        final_val_loss = val_losses[-1]\n        final_train_acc = train_accs[-1]\n        final_val_acc = val_accs[-1]\n\n        print(f\"final train loss: {final_train_loss:.4f}\")\n        print(f\"final train accuracy: {final_train_acc:.4f}\")\n        print(f\"final validation loss: {final_val_loss:.4f}\")\n        print(f\"final validation accuracy: {final_val_acc:.4f}\")\n\n        # Compute and print test accuracy\n        preds = dataset_info.get(\"predictions\", np.array([]))\n        truths = dataset_info.get(\"ground_truth\", np.array([]))\n        if preds.size and truths.size and preds.shape == truths.shape:\n            test_acc = (preds == truths).mean()\n            print(f\"test accuracy: {test_acc:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Define working directory and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract synthetic momentum sweep results\nsynthetic = experiment_data[\"momentum_sweep\"][\"synthetic\"]\nmomentum_values = synthetic[\"momentum_values\"]\ntrain_wg_histories = synthetic[\"metrics\"][\"train\"]\nval_wg_histories = synthetic[\"metrics\"][\"val\"]\ntrain_loss_histories = synthetic[\"losses\"][\"train\"]\nval_loss_histories = synthetic[\"losses\"][\"val\"]\ntest_predictions = synthetic[\"predictions\"]\ntest_ground_truth = synthetic[\"ground_truth\"]\n\n# Iterate through each momentum experiment and print final metrics\nfor idx, m in enumerate(momentum_values):\n    print(f\"\\nExperiment: momentum = {m}\")\n\n    # Training dataset metrics\n    final_train_loss = train_loss_histories[idx][-1]\n    final_train_wg = train_wg_histories[idx][-1]\n    print(\"Training dataset:\")\n    print(f\"  train loss: {final_train_loss:.4f}\")\n    print(f\"  train worst-group accuracy: {final_train_wg:.4f}\")\n\n    # Validation dataset metrics\n    final_val_loss = val_loss_histories[idx][-1]\n    final_val_wg = val_wg_histories[idx][-1]\n    print(\"Validation dataset:\")\n    print(f\"  validation loss: {final_val_loss:.4f}\")\n    print(f\"  validation worst-group accuracy: {final_val_wg:.4f}\")\n\n    # Test dataset metrics\n    preds = test_predictions[idx]\n    truths = test_ground_truth[idx]\n    test_accuracy = np.mean(preds == truths)\n    print(\"Test dataset:\")\n    print(f\"  test accuracy: {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each max_grad_norm setting\nfor norm_key, norm_results in experiment_data[\"max_grad_norm\"].items():\n    print(f\"Hyperparameter max_grad_norm = {norm_key}\")\n    # Iterate over each dataset under this setting (here only 'synthetic')\n    for dataset_name, rec in norm_results.items():\n        print(f\"Dataset: {dataset_name}\")\n        # Extract final metrics\n        train_wg_acc = rec[\"metrics\"][\"train\"][-1]\n        val_wg_acc = rec[\"metrics\"][\"val\"][-1]\n        train_loss = rec[\"losses\"][\"train\"][-1]\n        val_loss = rec[\"losses\"][\"val\"][-1]\n        # Compute overall test accuracy from stored predictions\n        test_preds = rec[\"predictions\"]\n        test_truth = rec[\"ground_truth\"]\n        test_acc = (test_preds == test_truth).mean()\n        # Print all labeled metrics\n        print(f\"Train worst-group accuracy: {train_wg_acc:.4f}\")\n        print(f\"Validation worst-group accuracy: {val_wg_acc:.4f}\")\n        print(f\"Train loss: {train_loss:.4f}\")\n        print(f\"Validation loss: {val_loss:.4f}\")\n        print(f\"Test accuracy: {test_acc:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\ndata_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Navigate to the synthetic learning-rate sweep results\nsynthetic = experiment_data[\"learning_rate\"][\"synthetic\"]\nlrs = synthetic[\"lrs\"]\nmetrics_train = synthetic[\"metrics\"][\"train\"]  # shape: (num_lrs, epochs)\nmetrics_val = synthetic[\"metrics\"][\"val\"]\nlosses_train = synthetic[\"losses\"][\"train\"]\nlosses_val = synthetic[\"losses\"][\"val\"]\npredictions = synthetic[\"predictions\"]  # shape: (num_lrs, num_test_samples)\nground_truth = synthetic[\"ground_truth\"]\n\n# Extract final-epoch values\nfinal_train_wg = metrics_train[:, -1]\nfinal_val_wg = metrics_val[:, -1]\nfinal_train_loss = losses_train[:, -1]\nfinal_val_loss = losses_val[:, -1]\n\n# Select the best learning rate by highest final validation worst-group accuracy\nbest_idx = np.argmax(final_val_wg)\n\n# Print metrics with clear labels\nprint(\"Training Dataset Metrics:\")\nprint(f\"  Final training worst-group accuracy: {final_train_wg[best_idx]:.4f}\")\nprint(f\"  Final training average loss: {final_train_loss[best_idx]:.4f}\\n\")\n\nprint(\"Validation Dataset Metrics:\")\nprint(f\"  Final validation worst-group accuracy: {final_val_wg[best_idx]:.4f}\")\nprint(f\"  Final validation average loss: {final_val_loss[best_idx]:.4f}\\n\")\n\n# Compute and print test accuracy\ntest_acc = np.mean(predictions[best_idx] == ground_truth)\nprint(\"Test Dataset Metrics:\")\nprint(f\"  Test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\ndata_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Navigate to the synthetic learning-rate sweep results\nsynthetic = experiment_data[\"learning_rate\"][\"synthetic\"]\nlrs = synthetic[\"lrs\"]\nmetrics_train = synthetic[\"metrics\"][\"train\"]  # shape: (num_lrs, epochs)\nmetrics_val = synthetic[\"metrics\"][\"val\"]\nlosses_train = synthetic[\"losses\"][\"train\"]\nlosses_val = synthetic[\"losses\"][\"val\"]\npredictions = synthetic[\"predictions\"]  # shape: (num_lrs, num_test_samples)\nground_truth = synthetic[\"ground_truth\"]\n\n# Extract final-epoch values\nfinal_train_wg = metrics_train[:, -1]\nfinal_val_wg = metrics_val[:, -1]\nfinal_train_loss = losses_train[:, -1]\nfinal_val_loss = losses_val[:, -1]\n\n# Select the best learning rate by highest final validation worst-group accuracy\nbest_idx = np.argmax(final_val_wg)\n\n# Print metrics with clear labels\nprint(\"Training Dataset Metrics:\")\nprint(f\"  Final training worst-group accuracy: {final_train_wg[best_idx]:.4f}\")\nprint(f\"  Final training average loss: {final_train_loss[best_idx]:.4f}\\n\")\n\nprint(\"Validation Dataset Metrics:\")\nprint(f\"  Final validation worst-group accuracy: {final_val_wg[best_idx]:.4f}\")\nprint(f\"  Final validation average loss: {final_val_loss[best_idx]:.4f}\\n\")\n\n# Compute and print test accuracy\ntest_acc = np.mean(predictions[best_idx] == ground_truth)\nprint(\"Test Dataset Metrics:\")\nprint(f\"  Test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\ndata_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Navigate to the synthetic learning-rate sweep results\nsynthetic = experiment_data[\"learning_rate\"][\"synthetic\"]\nlrs = synthetic[\"lrs\"]\nmetrics_train = synthetic[\"metrics\"][\"train\"]  # shape: (num_lrs, epochs)\nmetrics_val = synthetic[\"metrics\"][\"val\"]\nlosses_train = synthetic[\"losses\"][\"train\"]\nlosses_val = synthetic[\"losses\"][\"val\"]\npredictions = synthetic[\"predictions\"]  # shape: (num_lrs, num_test_samples)\nground_truth = synthetic[\"ground_truth\"]\n\n# Extract final-epoch values\nfinal_train_wg = metrics_train[:, -1]\nfinal_val_wg = metrics_val[:, -1]\nfinal_train_loss = losses_train[:, -1]\nfinal_val_loss = losses_val[:, -1]\n\n# Select the best learning rate by highest final validation worst-group accuracy\nbest_idx = np.argmax(final_val_wg)\n\n# Print metrics with clear labels\nprint(\"Training Dataset Metrics:\")\nprint(f\"  Final training worst-group accuracy: {final_train_wg[best_idx]:.4f}\")\nprint(f\"  Final training average loss: {final_train_loss[best_idx]:.4f}\\n\")\n\nprint(\"Validation Dataset Metrics:\")\nprint(f\"  Final validation worst-group accuracy: {final_val_wg[best_idx]:.4f}\")\nprint(f\"  Final validation average loss: {final_val_loss[best_idx]:.4f}\\n\")\n\n# Compute and print test accuracy\ntest_acc = np.mean(predictions[best_idx] == ground_truth)\nprint(\"Test Dataset Metrics:\")\nprint(f\"  Test accuracy: {test_acc:.4f}\")\n", ""], "parse_term_out": ["['Dataset: synthetic', '\\n', 'train loss: 0.4523', '\\n', 'train accuracy:\n0.9942', '\\n', 'validation loss: 0.4495', '\\n', 'validation accuracy: 0.9916',\n'\\n', 'test accuracy: 0.9860', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "['Training Dataset Metrics:', '\\n', '  Final training worst-group accuracy:\n0.9942', '\\n', '  Final training average loss: 0.0125\\n', '\\n', 'Validation\nDataset Metrics:', '\\n', '  Final validation worst-group accuracy: 0.9924',\n'\\n', '  Final validation average loss: 0.0430\\n', '\\n', 'Test Dataset\nMetrics:', '\\n', '  Test accuracy: 0.9960', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Synthetic dataset', '\\n', '\\nBatch size: 32', '\\n', 'Final training accuracy:\n0.9865', '\\n', 'Final validation accuracy: 0.9885', '\\n', 'Final training loss:\n0.3573', '\\n', 'Final validation loss: 0.3593', '\\n', '\\nBatch size: 64', '\\n',\n'Final training accuracy: 0.9826', '\\n', 'Final validation accuracy: 0.9622',\n'\\n', 'Final training loss: 0.4809', '\\n', 'Final validation loss: 0.4860',\n'\\n', '\\nBatch size: 128', '\\n', 'Final training accuracy: 0.9917', '\\n', 'Final\nvalidation accuracy: 0.9771', '\\n', 'Final training loss: 0.5375', '\\n', 'Final\nvalidation loss: 0.5366', '\\n', '\\nBatch size: 256', '\\n', 'Final training\naccuracy: 0.4294', '\\n', 'Final validation accuracy: 0.4370', '\\n', 'Final\ntraining loss: 0.6390', '\\n', 'Final validation loss: 0.6358', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Training set:', '\\n', '  train worst-group accuracy: 0.9884', '\\n', '\ntraining loss: 0.5028', '\\n', 'Validation set:', '\\n', '  validation worst-group\naccuracy: 0.9885', '\\n', '  validation loss: 0.5044', '\\n', 'Test set:', '\\n', '\ntest accuracy: 0.9880', '\\n', 'Execution time: a moment seconds (time limit is\nan hour).']", "['Hyperparameter train_epochs = 5', '\\n', 'Synthetic dataset:', '\\n', '  Final\ntraining worst\u2010group accuracy: 0.9942', '\\n', '  Final validation worst\u2010group\naccuracy: 0.9916', '\\n', '  Test accuracy: 0.9860\\n', '\\n', 'Hyperparameter\ntrain_epochs = 10', '\\n', 'Synthetic dataset:', '\\n', '  Final training\nworst\u2010group accuracy: 0.9923', '\\n', '  Final validation worst\u2010group accuracy:\n0.9809', '\\n', '  Test accuracy: 0.9900\\n', '\\n', 'Hyperparameter train_epochs =\n20', '\\n', 'Synthetic dataset:', '\\n', '  Final training worst\u2010group accuracy:\n0.9691', '\\n', '  Final validation worst\u2010group accuracy: 0.9706', '\\n', '  Test\naccuracy: 0.9720\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Dataset: synthetic (warmup_epochs = 1)', '\\n', 'train worst-group accuracy:\n0.9884', '\\n', 'validation worst-group accuracy: 0.9885', '\\n', 'train loss:\n0.5028', '\\n', 'validation loss: 0.5044', '\\n', 'test accuracy: 0.9880\\n', '\\n',\n'Dataset: synthetic (warmup_epochs = 2)', '\\n', 'train worst-group accuracy:\n0.9884', '\\n', 'validation worst-group accuracy: 0.9885', '\\n', 'train loss:\n0.3418', '\\n', 'validation loss: 0.3436', '\\n', 'test accuracy: 0.9920\\n', '\\n',\n'Dataset: synthetic (warmup_epochs = 3)', '\\n', 'train worst-group accuracy:\n0.9884', '\\n', 'validation worst-group accuracy: 0.9885', '\\n', 'train loss:\n0.1917', '\\n', 'validation loss: 0.1940', '\\n', 'test accuracy: 0.9900\\n', '\\n',\n'Dataset: synthetic (warmup_epochs = 5)', '\\n', 'train worst-group accuracy:\n0.9884', '\\n', 'validation worst-group accuracy: 0.9885', '\\n', 'train loss:\n0.0575', '\\n', 'validation loss: 0.0618', '\\n', 'test accuracy: 0.9900\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 3, in <module>\\n\nfrom sklearn.metrics import accuracy_score, f1_score\\nModuleNotFoundError: No\nmodule named \\'sklearn\\'\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Dataset: synthetic', '\\n', 'Final train worst-group accuracy: 0.9917', '\\n',\n'Final validation worst-group accuracy: 0.9916', '\\n', 'Final train loss:\n0.1756', '\\n', 'Final validation loss: 0.1796', '\\n', 'Test accuracy: 0.9860\\n',\n'\\n', 'Dataset: mnist', '\\n', 'Final train worst-group accuracy: 0.9462', '\\n',\n'Final validation worst-group accuracy: 0.9363', '\\n', 'Final train loss:\n0.1687', '\\n', 'Final validation loss: 0.2054', '\\n', 'Test accuracy: 0.9396\\n',\n'\\n', 'Dataset: fashion_mnist', '\\n', 'Final train worst-group accuracy:\n0.8168', '\\n', 'Final validation worst-group accuracy: 0.8109', '\\n', 'Final\ntrain loss: 0.4967', '\\n', 'Final validation loss: 0.5057', '\\n', 'Test\naccuracy: 0.8023\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Dropout rate: 0.0', '\\n', 'Dataset: Training set', '\\n', 'training worst-group\naccuracy: 0.9884', '\\n', 'training loss: 0.5028', '\\n', 'Dataset: Validation\nset', '\\n', 'validation worst-group accuracy: 0.9885', '\\n', 'validation loss:\n0.5044', '\\n', 'Dataset: Test set', '\\n', 'test accuracy: 0.9880\\n', '\\n',\n'Dropout rate: 0.1', '\\n', 'Dataset: Training set', '\\n', 'training worst-group\naccuracy: 0.9826', '\\n', 'training loss: 0.4866', '\\n', 'Dataset: Validation\nset', '\\n', 'validation worst-group accuracy: 0.9580', '\\n', 'validation loss:\n0.4916', '\\n', 'Dataset: Test set', '\\n', 'test accuracy: 0.9820\\n', '\\n',\n'Dropout rate: 0.2', '\\n', 'Dataset: Training set', '\\n', 'training worst-group\naccuracy: 0.9923', '\\n', 'training loss: 0.4661', '\\n', 'Dataset: Validation\nset', '\\n', 'validation worst-group accuracy: 0.9809', '\\n', 'validation loss:\n0.4651', '\\n', 'Dataset: Test set', '\\n', 'test accuracy: 0.9900\\n', '\\n',\n'Dropout rate: 0.3', '\\n', 'Dataset: Training set', '\\n', 'training worst-group\naccuracy: 0.9439', '\\n', 'training loss: 0.5283', '\\n', 'Dataset: Validation\nset', '\\n', 'validation worst-group accuracy: 0.9412', '\\n', 'validation loss:\n0.5264', '\\n', 'Dataset: Test set', '\\n', 'test accuracy: 0.9660\\n', '\\n',\n'Dropout rate: 0.5', '\\n', 'Dataset: Training set', '\\n', 'training worst-group\naccuracy: 0.9710', '\\n', 'training loss: 0.5442', '\\n', 'Dataset: Validation\nset', '\\n', 'validation worst-group accuracy: 0.9454', '\\n', 'validation loss:\n0.5402', '\\n', 'Dataset: Test set', '\\n', 'test accuracy: 0.9720\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'train accuracy: 0.9884', '\\n', 'validation\naccuracy: 0.9885', '\\n', 'test accuracy: 0.9880', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'final train loss: 0.4523', '\\n', 'final train\naccuracy: 0.9942', '\\n', 'final validation loss: 0.4495', '\\n', 'final\nvalidation accuracy: 0.9916', '\\n', 'test accuracy: 0.9860', '\\n', '\\n',\n'Dataset: synthetic', '\\n', 'final train loss: 0.4768', '\\n', 'final train\naccuracy: 0.9923', '\\n', 'final validation loss: 0.4778', '\\n', 'final\nvalidation accuracy: 0.9809', '\\n', 'test accuracy: 0.9900', '\\n', '\\n',\n'Dataset: synthetic', '\\n', 'final train loss: 0.5144', '\\n', 'final train\naccuracy: 0.9807', '\\n', 'final validation loss: 0.5138', '\\n', 'final\nvalidation accuracy: 0.9832', '\\n', 'test accuracy: 0.9820', '\\n', '\\n',\n'Dataset: synthetic', '\\n', 'final train loss: 0.4893', '\\n', 'final train\naccuracy: 0.9896', '\\n', 'final validation loss: 0.4895', '\\n', 'final\nvalidation accuracy: 0.9790', '\\n', 'test accuracy: 0.9900', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['\\nExperiment: momentum = 0.5', '\\n', 'Training dataset:', '\\n', '  train loss:\n0.7047', '\\n', '  train worst-group accuracy: 0.2567', '\\n', 'Validation\ndataset:', '\\n', '  validation loss: 0.7050', '\\n', '  validation worst-group\naccuracy: 0.2595', '\\n', 'Test dataset:', '\\n', '  test accuracy: 0.3340', '\\n',\n'\\nExperiment: momentum = 0.9', '\\n', 'Training dataset:', '\\n', '  train loss:\n0.6925', '\\n', '  train worst-group accuracy: 0.1663', '\\n', 'Validation\ndataset:', '\\n', '  validation loss: 0.6901', '\\n', '  validation worst-group\naccuracy: 0.1765', '\\n', 'Test dataset:', '\\n', '  test accuracy: 0.5340', '\\n',\n'\\nExperiment: momentum = 0.99', '\\n', 'Training dataset:', '\\n', '  train loss:\n0.4917', '\\n', '  train worst-group accuracy: 0.9826', '\\n', 'Validation\ndataset:', '\\n', '  validation loss: 0.4872', '\\n', '  validation worst-group\naccuracy: 0.9748', '\\n', 'Test dataset:', '\\n', '  test accuracy: 0.9800', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Hyperparameter max_grad_norm = 1.0', '\\n', 'Dataset: synthetic', '\\n', 'Train\nworst-group accuracy: 0.9884', '\\n', 'Validation worst-group accuracy: 0.9885',\n'\\n', 'Train loss: 0.5028', '\\n', 'Validation loss: 0.5044', '\\n', 'Test\naccuracy: 0.9880\\n', '\\n', 'Hyperparameter max_grad_norm = 5.0', '\\n', 'Dataset:\nsynthetic', '\\n', 'Train worst-group accuracy: 0.9884', '\\n', 'Validation worst-\ngroup accuracy: 0.9885', '\\n', 'Train loss: 0.5028', '\\n', 'Validation loss:\n0.5044', '\\n', 'Test accuracy: 0.9880\\n', '\\n', 'Hyperparameter max_grad_norm =\n10.0', '\\n', 'Dataset: synthetic', '\\n', 'Train worst-group accuracy: 0.9884',\n'\\n', 'Validation worst-group accuracy: 0.9885', '\\n', 'Train loss: 0.5028',\n'\\n', 'Validation loss: 0.5044', '\\n', 'Test accuracy: 0.9880\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Training Dataset Metrics:', '\\n', '  Final training worst-group accuracy:\n0.9942', '\\n', '  Final training average loss: 0.0125\\n', '\\n', 'Validation\nDataset Metrics:', '\\n', '  Final validation worst-group accuracy: 0.9924',\n'\\n', '  Final validation average loss: 0.0430\\n', '\\n', 'Test Dataset\nMetrics:', '\\n', '  Test accuracy: 0.9960', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Training Dataset Metrics:', '\\n', '  Final training worst-group accuracy:\n0.9942', '\\n', '  Final training average loss: 0.0125\\n', '\\n', 'Validation\nDataset Metrics:', '\\n', '  Final validation worst-group accuracy: 0.9924',\n'\\n', '  Final validation average loss: 0.0430\\n', '\\n', 'Test Dataset\nMetrics:', '\\n', '  Test accuracy: 0.9960', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Training Dataset Metrics:', '\\n', '  Final training worst-group accuracy:\n0.9942', '\\n', '  Final training average loss: 0.0125\\n', '\\n', 'Validation\nDataset Metrics:', '\\n', '  Final validation worst-group accuracy: 0.9924',\n'\\n', '  Final validation average loss: 0.0430\\n', '\\n', 'Test Dataset\nMetrics:', '\\n', '  Test accuracy: 0.9960', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, "ModuleNotFoundError", null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, {"args": ["No module named 'sklearn'"], "name": "sklearn", "msg": "No module named 'sklearn'"}, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 3, "<module>", "from sklearn.metrics import accuracy_score, f1_score"]], null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>

{"edges": [[0, 4], [0, 1], [0, 5], [0, 8], [0, 7], [0, 3], [0, 9], [0, 17], [0, 2], [0, 16], [0, 11], [0, 15], [0, 18], [0, 6], [0, 14], [1, 22], [1, 21], [1, 20], [1, 19], [8, 10], [8, 12], [9, 13]], "layout": [[0.5458333333333334, 0.0], [0.09375000000000006, 0.5], [0.15625000000000006, 0.5], [0.21875000000000006, 0.5], [0.28125000000000006, 0.5], [0.34375000000000006, 0.5], [0.40625000000000006, 0.5], [0.46875000000000006, 0.5], [0.53125, 0.5], [0.625, 0.5], [0.5, 1.0], [0.6875, 0.5], [0.5625, 1.0], [0.625, 1.0], [0.75, 0.5], [0.8125, 0.5], [0.875, 0.5], [0.9375, 0.5], [1.0, 0.5], [0.0, 1.0], [0.06250000000000006, 1.0], [0.12500000000000006, 1.0], [0.18750000000000006, 1.0]], "plan": ["Hyperparam tuning name: learning_rate. We extend the baseline by looping over a\nlog\u2010scale grid of learning rates, reinitializing the model and optimizer for\neach value. During each run we apply the same warmup + gradient\u2010clustering\nreweighting schedule, recording per\u2010epoch train/val worst\u2010group accuracies and\nlosses. After training, we evaluate on the test split to obtain predictions and\nground truth. All these arrays are collated into an `experiment_data` dict under\nthe \u201clearning_rate\u201d tuning type and saved to `experiment_data.npy`.", "Ablation name: Multiple Synthetic Datasets Ablation. We loop over selected\nspurious\u2010feature correlations and continuous\u2010dimensionalities to generate\nmultiple synthetic datasets. For each dataset we run the original\ntraining+clustering+reweighting pipeline over a sweep of learning rates and\ncollect worst\u2010group accuracies, losses, and test set predictions. All results\nare stored in a nested `experiment_data` dict under the ablation type\n`multiple_synthetic`, then saved to `experiment_data.npy`. This single\u2010file\nprogram is self\u2010contained and executable as\u2010is.", "Ablation name: NO_CLUSTER_REWEIGHTING. Below is a self\u2010contained script that\nruns the synthetic experiment without any cluster\u2010based reweighting. It trains\nan MLP over a small hyperparameter sweep, records per\u2010epoch worst\u2010group accuracy\nand loss on train/val splits, collects final test predictions, and saves\neverything under the ablation key \u201cNO_CLUSTER_REWEIGHTING\u201d in a numpy\u2010serialized\ndict.", "Ablation name: NO_SPURIOUS_FEATURE. We remove the binary spurious feature by\ntraining the MLP on only the normalized continuous inputs while keeping the\noriginal spurious label for grouping in worst\u2010group evaluation. The\narchitecture, clustering\u2010based reweighting after a warmup epoch, and\nlearning\u2010rate sweep remain unchanged except that the input dimension is reduced\nto `d`. We collect train/val worst\u2010group accuracies and losses per epoch, final\ntest predictions, and ground truth labels under the ablation key\n`\"NO_SPURIOUS_FEATURE\"` and save everything as `experiment_data.npy`.", "Ablation name: LINEAR_CLASSIFIER. We introduce a `LinearClassifier` that\nreplaces the two\u2010layer MLP with a single linear layer and then run the identical\ntraining, clustering\u2010based reweighting, and evaluation pipeline over\nhyperparameters.  All metrics, losses, predictions, and ground\u2010truth labels are\nsaved in the prescribed `experiment_data.npy` under the key\n`'linear_classifier'` for the synthetic dataset.  The script is self\u2010contained,\nusing the same data generation, loaders, and clustering procedure as before.", "Ablation name: INPUT_FEATURE_CLUSTER_REWEIGHTING. We will cluster the raw input\nfeatures of the training set immediately after the warmup epoch using k\u2010means\nand assign each sample a weight inversely proportional to its cluster size.\nThese weights are then used to reweight the per\u2010sample losses for the remaining\ntraining epochs, with all other components of the pipeline (model, training\nloop, evaluation) unchanged. Finally, we aggregate and save the metrics, losses,\npredictions, and ground truth under the key \u201cINPUT_FEATURE_CLUSTER_REWEIGHTING\u201d\nin a single `experiment_data.npy` file.", "Ablation name: REPRESENTATION_CLUSTER_REWEIGHTING. I will collect each sample\u2019s\nlast hidden\u2010layer activations after the warmup epoch, run k\u2010means on these\nrepresentations, compute inverse cluster frequency weights, and then resume\ntraining with these sample weights. Metrics (train/val worst\u2010group accuracy and\nlosses) and test predictions for each learning rate are stored under the\nablation key \u2018representation_cluster_reweighting\u2019 on the synthetic dataset, then\nall results are saved to \u2018experiment_data.npy\u2019. The script is self\u2010contained and\nexecutable as\u2010is.", "Ablation name: GROUP_INVERSE_FREQUENCY_REWEIGHTING. We implement\ngroup\u2010inverse\u2010frequency reweighting by counting the oracle spurious\u2010feature\ngroups in the training set after the warmup epoch and assigning each sample a\nweight equal to the inverse of its group frequency.  We then train for multiple\nlearning rates, applying these fixed sample weights from the second epoch\nonward, and log per\u2010epoch worst\u2010group accuracies and losses.  Finally, we\ncollect test\u2010set predictions and save all metrics, losses, predictions, and\nground truth into `experiment_data.npy` under the key\n`\"group_inverse_frequency_reweighting\"`.", "Ablation name: RANDOM_CLUSTER_REWEIGHTING. Below is a self-contained script that\nruns the random\u2010cluster ablation: after the warmup epoch it assigns each\ntraining sample to one of two clusters uniformly at random, computes sample\nweights as inverse cluster frequency, and then continues training. It sweeps\nover three learning rates, records worst\u2010group accuracy and losses per epoch,\nand finally saves all metrics, losses, predictions, and ground truth into a\nnested dict under `'random_cluster_reweighting' \u2192 'synthetic'` as\n`experiment_data.npy`.", "Ablation name: SGD_OPTIMIZER. Here is a Python script that runs the\nSGD_OPTIMIZER ablation by swapping in vanilla SGD for Adam while keeping all\nother settings the same. It generates the synthetic dataset, applies\ncluster\u2010based reweighting after a warmup, sweeps over learning rates, and logs\nworst\u2010group train/validation accuracies and losses. After training, it collects\ntest predictions along with labels and saves all results\u2014including learning\nrates, metrics, losses, predictions, and ground truth\u2014in `experiment_data.npy`.", "We change the ambiguous truth check on a numpy array to an explicit length test\nusing `len(sd[\"ground_truth\"]) == 0`, avoiding the ValueError, and ensure that\n`ground_truth` is only set once. We also add and print the device declaration,\nmove the model and all tensors properly onto the device with `.to(device)`, and\nuse `isinstance(v, torch.Tensor)` when filtering batch items. The\ntraining/evaluation loop prints validation loss and worst\u2010group accuracy each\nepoch, gathers all metrics and predictions, and saves the final experiment data\nas a numpy file.", "Ablation name: NO_WARMUP_CLUSTER_REWEIGHTING. Below is a self\u2010contained Python\nscript implementing the NO_WARMUP_CLUSTER_REWEIGHTING ablation: we set\nwarmup_epochs=0, perform k\u2010means on gradients of the untrained network to obtain\nsample weights before any training, then train with these weights from epoch 0\nonward. We sweep over learning rates, track worst\u2010group accuracy and loss on\ntrain/val, gather test predictions, and save all plottable data under the key\n`'no_warmup_cluster_reweighting'` in `experiment_data.npy`.", "I will replace the invalid boolean check on the NumPy array with an explicit\nlength test so that `ground_truth` is only initialized once. I\u2019ll also add a\nstartup printout of the selected device, ensure all tensors and models are moved\nto that device, and update the per-epoch logging to report the average\nvalidation loss and worst\u2010group accuracy. All previous functionality remains\nintact, and the final experiment data is saved as before.", "The error is caused by using `if not` on a NumPy array, which is ambiguous in\nPython. We fix this by explicitly checking whether the `ground_truth` list is\nempty using `len(...) == 0` before assigning the NumPy array, ensuring the\nassignment only happens once. No other parts of the code need to change.", "Ablation name: CLUSTER_COUNT_VARIATION. Below is a sketch of the implementation:\nwe loop over cluster counts [1,2,4,8] and learning rates, training the MLP with\none warmup epoch and then applying k-means reweighting with the specified k. We\nrecord per\u2010epoch worst\u2010group accuracies and losses on train/val splits, plus\nfinal test predictions, stacking them into numpy arrays. Finally, we assemble\neverything under `experiment_data['CLUSTER_COUNT_VARIATION']['synthetic']`,\nincluding the cluster_counts, lrs, metrics, losses, predictions, and\nground_truth, and save to `working/experiment_data.npy`.", "Ablation name: WEIGHT_DECAY_VARIATION. We loop over the specified weight_decay\nvalues while holding the learning rate fixed, running identical warmup,\nclustering and training epochs for each setting. For each weight decay we\ninitialize a fresh model and optimizer (with L2 regularization), perform the\none\u2010epoch warmup, then cluster\u2010based reweighting, and train for the remaining\nepochs. We record per-epoch worst-group accuracies and losses on train/val\nsplits, collect final test predictions, and assemble all outputs in a nested\ndict keyed by ablation and dataset name. Finally we save everything to\nworking/experiment_data.npy.", "Ablation name: LOSS_BASED_SAMPLE_WEIGHTING. Here\u2019s a concise implementation of\nthe LOSS_BASED_SAMPLE_WEIGHTING ablation on the synthetic dataset:", "Ablation name: NORMALIZED_GRADIENT_CLUSTER_REWEIGHTING. Below is a single\u2010file\nPython script that runs both the original (\u201craw gradient\u201d) clustering and the\nablation (\u201cnormalized gradient\u201d) clustering over the synthetic dataset, sweeps\nover learning rates, collects train/val worst\u2010group accuracies, losses, test\npredictions, and saves everything in `experiment_data.npy` under the specified\nnaming convention.", "Ablation name: NO_FEATURE_NORMALIZATION. We wrap the original synthetic data\npipeline in a helper function and run it twice: once with per-feature\nnormalization and once without. Both experiments sweep three learning rates,\napply gradient-based k-means reweighting after a warmup epoch, and record worst-\ngroup accuracies, losses, and test predictions. We store the results under the\nablation key \"NO_FEATURE_NORMALIZATION\" with sub-keys \"synthetic_with_norm\" and\n\"synthetic_no_norm\", then save everything as experiment_data.npy.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# k-means on CPU\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper: returns (sum_loss, worst-group accuracy)\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameter sweep over learning rates\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# Containers for results\nmetrics_train_all = []\nmetrics_val_all = []\nlosses_train_all = []\nlosses_val_all = []\npredictions_list = []\nground_truth = None\n\nfor lr in lrs:\n    print(f\"\\n=== Training with learning rate = {lr} ===\")\n    # re-init model, loss, optimizer\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    # per-epoch logs\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # cluster-based reweighting after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out_i = model(batch[\"features\"])\n                loss_i = criterion(out_i, batch[\"label\"]).mean()\n                loss_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # evaluate after epoch\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(\n            f\"lr={lr} epoch={epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n        )\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # final test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n\n    preds_npy = np.array(preds)\n    truths_npy = np.array(truths)\n    predictions_list.append(preds_npy)\n    if ground_truth is None:\n        ground_truth = truths_npy\n\n    metrics_train_all.append(m_tr)\n    metrics_val_all.append(m_val)\n    losses_train_all.append(l_tr)\n    losses_val_all.append(l_val)\n\n# Assemble and save experiment data\nexperiment_data = {\n    \"learning_rate\": {\n        \"synthetic\": {\n            \"lrs\": np.array(lrs),\n            \"metrics\": {\n                \"train\": np.array(metrics_train_all),\n                \"val\": np.array(metrics_val_all),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_all),\n                \"val\": np.array(losses_val_all),\n            },\n            \"predictions\": np.stack(predictions_list),\n            \"ground_truth\": ground_truth,\n        }\n    }\n}\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n# Dataset class\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# k-means implementation\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameters\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\nN = 2000\n\n# Ablation settings\nspurious_corrs = [0.5, 0.75, 0.95]\ndims_list = [5, 10, 15]\n\n# Container for all results\nexperiment_data = {\"multiple_synthetic\": {}}\n\nfor sp_corr in spurious_corrs:\n    for d in dims_list:\n        ds_name = f\"corr_{int(sp_corr*100)}_d{d}\"\n        print(f\"\\n=== Dataset: {ds_name} ===\")\n\n        # Generate synthetic data\n        y = np.random.binomial(1, 0.5, size=N)\n        X_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\n        rnd = np.random.rand(N)\n        s = np.where(rnd < sp_corr, y, 1 - y)\n        X = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n        # Split indices\n        idxs = np.arange(N)\n        np.random.shuffle(idxs)\n        train_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n        # Normalize continuous features\n        mean = X[train_idx, :d].mean(axis=0)\n        std = X[train_idx, :d].std(axis=0) + 1e-6\n        X_norm = X.copy()\n        X_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n        # Prepare splits\n        splits = {\n            \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n            \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n            \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n        }\n\n        # Create DataLoaders\n        train_ds = SyntheticDataset(*splits[\"train\"])\n        val_ds = SyntheticDataset(*splits[\"val\"])\n        test_ds = SyntheticDataset(*splits[\"test\"])\n        train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n        test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n        cluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n        # Containers per dataset\n        metrics_train_all = []\n        metrics_val_all = []\n        losses_train_all = []\n        losses_val_all = []\n        predictions_list = []\n        ground_truth = None\n\n        # Sweep over learning rates\n        for lr in lrs:\n            print(f\"\\n-- Training with lr={lr} --\")\n            model = MLP(d + 1).to(device)\n            criterion = nn.CrossEntropyLoss(reduction=\"none\")\n            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n            sample_weights = None\n\n            m_tr, m_val = [], []\n            l_tr, l_val = [], []\n\n            for epoch in range(total_epochs):\n                model.train()\n                for batch in train_loader:\n                    batch = {\n                        k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                    }\n                    x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n                    out = model(x)\n                    losses = criterion(out, yb)\n                    if epoch >= warmup_epochs and sample_weights is not None:\n                        loss = (losses * sample_weights[idxb]).mean()\n                    else:\n                        loss = losses.mean()\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n\n                # Clustering for sample weights\n                if epoch == warmup_epochs - 1:\n                    model.eval()\n                    grads = []\n                    for sample in cluster_loader:\n                        batch = {\n                            k: v.to(device)\n                            for k, v in sample.items()\n                            if torch.is_tensor(v)\n                        }\n                        optimizer.zero_grad()\n                        out_i = model(batch[\"features\"])\n                        loss_i = criterion(out_i, batch[\"label\"]).mean()\n                        loss_i.backward()\n                        g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                        grads.append(g)\n                    grads = np.stack(grads)\n                    labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n                    counts = np.bincount(labels, minlength=2)\n                    sw_arr = np.array(\n                        [1.0 / counts[lab] for lab in labels], dtype=np.float32\n                    )\n                    sample_weights = torch.tensor(sw_arr, device=device)\n\n                # Evaluate\n                tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n                v_loss, v_wg = evaluate(val_loader, model, criterion)\n                print(\n                    f\"Epoch {epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n                )\n                m_tr.append(tr_wg)\n                m_val.append(v_wg)\n                l_tr.append(tr_loss / len(train_ds))\n                l_val.append(v_loss / len(val_ds))\n\n            # Test predictions\n            model.eval()\n            preds, truths = [], []\n            with torch.no_grad():\n                for batch in test_loader:\n                    batch = {\n                        k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                    }\n                    out = model(batch[\"features\"])\n                    preds.extend(out.argmax(1).cpu().tolist())\n                    truths.extend(batch[\"label\"].cpu().tolist())\n\n            preds_npy = np.array(preds)\n            truths_npy = np.array(truths)\n            predictions_list.append(preds_npy)\n            if ground_truth is None:\n                ground_truth = truths_npy\n\n            metrics_train_all.append(m_tr)\n            metrics_val_all.append(m_val)\n            losses_train_all.append(l_tr)\n            losses_val_all.append(l_val)\n\n        # Save dataset results\n        experiment_data[\"multiple_synthetic\"][ds_name] = {\n            \"metrics\": {\n                \"train\": np.array(metrics_train_all),\n                \"val\": np.array(metrics_val_all),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_all),\n                \"val\": np.array(losses_val_all),\n            },\n            \"predictions\": np.stack(predictions_list),\n            \"ground_truth\": ground_truth,\n            \"spurious_corr\": sp_corr,\n            \"dim\": d,\n        }\n\n# Save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nSaved experiment_data.npy to {working_dir}\")\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper: returns (sum_loss, worst\u2010group accuracy)\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameters\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# Prepare experiment_data structure\nexperiment_data = {\n    \"NO_CLUSTER_REWEIGHTING\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": None,\n        }\n    }\n}\n\nmetrics_train_all = []\nmetrics_val_all = []\nlosses_train_all = []\nlosses_val_all = []\npredictions_list = []\nground_truth = None\n\nfor lr in lrs:\n    print(f\"\\n=== NO_CLUSTER_REWEIGHTING, lr = {lr} ===\")\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb = batch[\"features\"], batch[\"label\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss = losses.mean()  # uniform weights always\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # evaluation\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(f\" epoch={epoch} train_wg={tr_wg:.4f} val_wg={v_wg:.4f}\")\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # final test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n\n    preds_npy = np.array(preds)\n    truths_npy = np.array(truths)\n    predictions_list.append(preds_npy)\n    if ground_truth is None:\n        ground_truth = truths_npy\n\n    metrics_train_all.append(m_tr)\n    metrics_val_all.append(m_val)\n    losses_train_all.append(l_tr)\n    losses_val_all.append(l_val)\n\n# Store numpy arrays\nsd = experiment_data[\"NO_CLUSTER_REWEIGHTING\"][\"synthetic\"]\nsd[\"metrics\"][\"train\"] = np.array(metrics_train_all)\nsd[\"metrics\"][\"val\"] = np.array(metrics_val_all)\nsd[\"losses\"][\"train\"] = np.array(losses_train_all)\nsd[\"losses\"][\"val\"] = np.array(losses_val_all)\nsd[\"predictions\"] = np.stack(predictions_list)\nsd[\"ground_truth\"] = ground_truth\n\n# Save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved ablation results to\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Device and seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data parameters\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)  # spurious binary label\n\n# Train/val/test split\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X_cont[train_idx].mean(axis=0)\nstd = X_cont[train_idx].std(axis=0) + 1e-6\nX_norm = (X_cont - mean) / std\n\n# Prepare splits (features only continuous, group label = s)\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# K-means for clustering gradients\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameters\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# Prepare experiment-data container\nexperiment_data = {\n    \"NO_SPURIOUS_FEATURE\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"lrs\": [],\n        }\n    }\n}\n\nmetrics_train_all, metrics_val_all = [], []\nlosses_train_all, losses_val_all = [], []\npredictions_list = []\nground_truth = None\n\nfor lr in lrs:\n    print(f\"\\n=== Training with learning rate = {lr} ===\")\n    model = MLP(d).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # Cluster-based reweighting\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out_i = model(batch[\"features\"])\n                loss_i = criterion(out_i, batch[\"label\"]).mean()\n                loss_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # Evaluate\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(\n            f\"lr={lr} epoch={epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n        )\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # Final test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n    preds_npy = np.array(preds)\n    truths_npy = np.array(truths)\n    predictions_list.append(preds_npy)\n    if ground_truth is None:\n        ground_truth = truths_npy\n\n    metrics_train_all.append(m_tr)\n    metrics_val_all.append(m_val)\n    losses_train_all.append(l_tr)\n    losses_val_all.append(l_val)\n\n# Assemble and save\nsd = experiment_data[\"NO_SPURIOUS_FEATURE\"][\"synthetic\"]\nsd[\"lrs\"] = np.array(lrs)\nsd[\"metrics\"][\"train\"] = np.array(metrics_train_all)\nsd[\"metrics\"][\"val\"] = np.array(metrics_val_all)\nsd[\"losses\"][\"train\"] = np.array(losses_train_all)\nsd[\"losses\"][\"val\"] = np.array(losses_val_all)\nsd[\"predictions\"] = np.stack(predictions_list)\nsd[\"ground_truth\"] = ground_truth\n\nnp.save(\"experiment_data.npy\", experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device and seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# splits\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# normalize\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": torch.tensor(idx, dtype=torch.long),\n        }\n\n\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            centroids[k] = pts.mean(axis=0) if len(pts) > 0 else X[rng.randint(N)]\n    return labels\n\n\nclass LinearClassifier(nn.Module):\n    def __init__(self, inp_dim):\n        super().__init__()\n        self.fc = nn.Linear(inp_dim, 2)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            ls = criterion(out, yb)\n            loss_sum += ls.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# prepare experiment data structure\nexperiment_data = {\n    \"linear_classifier\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": None,\n        }\n    }\n}\n\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\nfor lr in lrs:\n    model = LinearClassifier(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out_i = model(batch[\"features\"])\n                l_i = criterion(out_i, batch[\"label\"]).mean()\n                l_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n\n    preds_npy = np.array(preds)\n    truths_npy = np.array(truths)\n\n    exp = experiment_data[\"linear_classifier\"][\"synthetic\"]\n    exp[\"metrics\"][\"train\"].append(m_tr)\n    exp[\"metrics\"][\"val\"].append(m_val)\n    exp[\"losses\"][\"train\"].append(l_tr)\n    exp[\"losses\"][\"val\"].append(l_val)\n    exp[\"predictions\"].append(preds_npy)\n    if exp[\"ground_truth\"] is None:\n        exp[\"ground_truth\"] = truths_npy\n\n# convert lists to arrays and save\nexp = experiment_data[\"linear_classifier\"][\"synthetic\"]\nexp[\"metrics\"][\"train\"] = np.array(exp[\"metrics\"][\"train\"])\nexp[\"metrics\"][\"val\"] = np.array(exp[\"metrics\"][\"val\"])\nexp[\"losses\"][\"train\"] = np.array(exp[\"losses\"][\"train\"])\nexp[\"losses\"][\"val\"] = np.array(exp[\"losses\"][\"val\"])\nexp[\"predictions\"] = np.stack(exp[\"predictions\"])\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n\n\n# k-means on CPU\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(-1)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameters\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# Prepare experiment data container\nexperiment_data = {\n    \"INPUT_FEATURE_CLUSTER_REWEIGHTING\": {\n        \"synthetic\": {\n            \"lrs\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor lr in lrs:\n    print(f\"\\n=== LR={lr} ===\")\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # After warmup: cluster input features and set weights\n        if epoch == warmup_epochs - 1:\n            X_train = splits[\"train\"][0]  # numpy array\n            labels = kmeans_np(X_train, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # Evaluate\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(f\"Epoch {epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\")\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # Final test predictions\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n\n    # Collect into experiment_data\n    exp = experiment_data[\"INPUT_FEATURE_CLUSTER_REWEIGHTING\"][\"synthetic\"]\n    exp[\"lrs\"].append(lr)\n    exp[\"metrics\"][\"train\"].append(m_tr)\n    exp[\"metrics\"][\"val\"].append(m_val)\n    exp[\"losses\"][\"train\"].append(l_tr)\n    exp[\"losses\"][\"val\"].append(l_val)\n    exp[\"predictions\"].append(preds)\n    if not exp[\"ground_truth\"]:\n        exp[\"ground_truth\"] = splits[\"test\"][1].tolist()\n\n# Convert lists to numpy arrays and save\nsyn = experiment_data[\"INPUT_FEATURE_CLUSTER_REWEIGHTING\"][\"synthetic\"]\nsyn[\"lrs\"] = np.array(syn[\"lrs\"])\nsyn[\"metrics\"][\"train\"] = np.array(syn[\"metrics\"][\"train\"])\nsyn[\"metrics\"][\"val\"] = np.array(syn[\"metrics\"][\"val\"])\nsyn[\"losses\"][\"train\"] = np.array(syn[\"losses\"][\"train\"])\nsyn[\"losses\"][\"val\"] = np.array(syn[\"losses\"][\"val\"])\nsyn[\"predictions\"] = np.array(syn[\"predictions\"])\nsyn[\"ground_truth\"] = np.array(syn[\"ground_truth\"])\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# k-means on CPU\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper: returns (sum_loss, worst-group accuracy)\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameters\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# Prepare experiment data container\nexperiment_data = {\n    \"representation_cluster_reweighting\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nmetrics_train_all, metrics_val_all = [], []\nlosses_train_all, losses_val_all = [], []\npredictions_list = []\nground_truth = None\n\nfor lr in lrs:\n    # Initialize model, loss, optimizer\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # Representation-based clustering after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            feats = []\n            with torch.no_grad():\n                for sample in cluster_loader:\n                    batch = {\n                        k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                    }\n                    rep = model.net(batch[\"features\"])\n                    feats.append(rep.cpu().numpy().ravel())\n            feats = np.stack(feats)\n            labels = kmeans_np(feats, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        # Evaluate\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(\n            f\"lr={lr} epoch={epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n        )\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # Test set predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n    preds_npy = np.array(preds)\n    truths_npy = np.array(truths)\n    predictions_list.append(preds_npy)\n    if ground_truth is None:\n        ground_truth = truths_npy\n\n    metrics_train_all.append(m_tr)\n    metrics_val_all.append(m_val)\n    losses_train_all.append(l_tr)\n    losses_val_all.append(l_val)\n\n# Populate experiment_data\nexperiment_data[\"representation_cluster_reweighting\"][\"synthetic\"][\"metrics\"][\n    \"train\"\n] = np.array(metrics_train_all)\nexperiment_data[\"representation_cluster_reweighting\"][\"synthetic\"][\"metrics\"][\"val\"] = (\n    np.array(metrics_val_all)\n)\nexperiment_data[\"representation_cluster_reweighting\"][\"synthetic\"][\"losses\"][\n    \"train\"\n] = np.array(losses_train_all)\nexperiment_data[\"representation_cluster_reweighting\"][\"synthetic\"][\"losses\"][\"val\"] = (\n    np.array(losses_val_all)\n)\nexperiment_data[\"representation_cluster_reweighting\"][\"synthetic\"][\"predictions\"] = (\n    np.stack(predictions_list)\n)\nexperiment_data[\"representation_cluster_reweighting\"][\"synthetic\"][\n    \"ground_truth\"\n] = ground_truth\n\n# Save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper: returns (sum_loss, worst-group accuracy)\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    model.eval()\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += int(mask.sum().item())\n                if mask.sum().item() > 0:\n                    correct[g] += int((preds[mask] == yb[mask]).sum().item())\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameters\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# Containers for results\nmetrics_train_all = []\nmetrics_val_all = []\nlosses_train_all = []\nlosses_val_all = []\npredictions_list = []\nground_truth = None\n\nfor lr in lrs:\n    print(f\"\\n=== Training with learning rate = {lr} ===\")\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # after warmup, set group-inverse-frequency weights\n        if epoch == warmup_epochs - 1:\n            g_np = train_ds.g.numpy()\n            counts = np.bincount(g_np, minlength=2)\n            sw_arr = np.array([1.0 / counts[g] for g in g_np], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(\n            f\"lr={lr} epoch={epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n        )\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n    preds_npy = np.array(preds)\n    truths_npy = np.array(truths)\n    predictions_list.append(preds_npy)\n    if ground_truth is None:\n        ground_truth = truths_npy\n\n    metrics_train_all.append(m_tr)\n    metrics_val_all.append(m_val)\n    losses_train_all.append(l_tr)\n    losses_val_all.append(l_val)\n\n# Assemble and save experiment data\nexperiment_data = {\n    \"group_inverse_frequency_reweighting\": {\n        \"synthetic\": {\n            \"lrs\": np.array(lrs),\n            \"metrics\": {\n                \"train\": np.array(metrics_train_all),\n                \"val\": np.array(metrics_val_all),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_all),\n                \"val\": np.array(losses_val_all),\n            },\n            \"predictions\": np.stack(predictions_list),\n            \"ground_truth\": ground_truth,\n        }\n    }\n}\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# reproducibility\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# splits\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\nmean = X[train_idx, :d].mean(0)\nstd = X[train_idx, :d].std(0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"features\": self.X[i], \"label\": self.y[i], \"group\": self.g[i], \"idx\": i}\n\n\n# data loaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n\n\n# model\nclass MLP(nn.Module):\n    def __init__(self, inp, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# evaluation\ndef evaluate(loader, model, crit):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for b in loader:\n            b = {k: v.to(device) for k, v in b.items() if torch.is_tensor(v)}\n            out = model(b[\"features\"])\n            losses = crit(out, b[\"label\"])\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                m = b[\"group\"] == g\n                total[g] += m.sum().item()\n                if m.any():\n                    correct[g] += (preds[m] == b[\"label\"][m]).sum().item()\n    wg = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg\n\n\n# hyperparameters\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# experiment container\nexperiment_data = {\n    \"random_cluster_reweighting\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor lr in lrs:\n    print(f\"\\n=== lr={lr} ===\")\n    model = MLP(d + 1).to(device)\n    crit = nn.CrossEntropyLoss(reduction=\"none\")\n    opt = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for ep in range(total_epochs):\n        model.train()\n        for b in train_loader:\n            b = {k: v.to(device) for k, v in b.items() if torch.is_tensor(v)}\n            out = model(b[\"features\"])\n            losses = crit(out, b[\"label\"])\n            if ep >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[b[\"idx\"]]).mean()\n            else:\n                loss = losses.mean()\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n\n        # random cluster reweighting after warmup\n        if ep == warmup_epochs - 1:\n            Ntr = len(train_ds)\n            labs = np.random.randint(0, 2, size=Ntr)\n            cnt = np.bincount(labs, minlength=2)\n            sw = np.array([1.0 / cnt[l] for l in labs], dtype=np.float32)\n            sample_weights = torch.tensor(sw, device=device)\n\n        tr_loss, tr_wg = evaluate(train_loader, model, crit)\n        v_loss, v_wg = evaluate(val_loader, model, crit)\n        print(f\" ep={ep}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\")\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # test set predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for b in test_loader:\n            b = {k: v.to(device) for k, v in b.items() if torch.is_tensor(v)}\n            out = model(b[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(b[\"label\"].cpu().tolist())\n\n    sd = experiment_data[\"random_cluster_reweighting\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(m_tr)\n    sd[\"metrics\"][\"val\"].append(m_val)\n    sd[\"losses\"][\"train\"].append(l_tr)\n    sd[\"losses\"][\"val\"].append(l_val)\n    sd[\"predictions\"].append(np.array(preds))\n    if not sd[\"ground_truth\"]:\n        sd[\"ground_truth\"] = np.array(truths)\n\n# convert lists to arrays\nsd = experiment_data[\"random_cluster_reweighting\"][\"synthetic\"]\nsd[\"metrics\"][\"train\"] = np.array(sd[\"metrics\"][\"train\"])\nsd[\"metrics\"][\"val\"] = np.array(sd[\"metrics\"][\"val\"])\nsd[\"losses\"][\"train\"] = np.array(sd[\"losses\"][\"train\"])\nsd[\"losses\"][\"val\"] = np.array(sd[\"losses\"][\"val\"])\nsd[\"predictions\"] = np.stack(sd[\"predictions\"])\n# 'ground_truth' is already an array\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Reproducibility and device\nnp.random.seed(0)\ntorch.manual_seed(0)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split into train/val/test\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous part\nmean = X[train_idx, :d].mean(0)\nstd = X[train_idx, :d].std(0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n\n# Dataset & loaders\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n        self.idx = torch.arange(len(self.y), dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\n            \"features\": self.X[i],\n            \"label\": self.y[i],\n            \"group\": self.g[i],\n            \"idx\": self.idx[i],\n        }\n\n\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# k-means for cluster\u2010based reweighting\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init].copy()\n    labels = np.zeros(N, int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(-1)\n        new = dists.argmin(1)\n        if np.all(new == labels):\n            break\n        labels = new\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts):\n                centroids[k] = pts.mean(0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# Simple MLP\nclass MLP(nn.Module):\n    def __init__(self, inp, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluate: returns (sum_loss, worst\u2010group accuracy)\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                m = grp == g\n                total[g] += m.sum().item()\n                if m.sum() > 0:\n                    correct[g] += (preds[m] == yb[m]).sum().item()\n    wg = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg\n\n\n# Hyperparameters\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# Prepare experiment_data\nexperiment_data = {\n    \"SGD_OPTIMIZER\": {\n        \"synthetic\": {\n            \"learning_rates\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Run ablation with vanilla SGD\nfor lr in lrs:\n    print(f\"\\n=== SGD lr={lr} ===\")\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n    sample_weights = None\n\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # cluster-based reweighting after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                sb = {k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)}\n                optimizer.zero_grad()\n                out_i = model(sb[\"features\"])\n                li = criterion(out_i, sb[\"label\"]).mean()\n                li.backward()\n                g = model.fc.weight.grad.detach().cpu().reshape(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labs = kmeans_np(grads, n_clusters=2, n_iters=10)\n            cnt = np.bincount(labs, minlength=2)\n            sw = np.array([1.0 / cnt[lab] for lab in labs], np.float32)\n            sample_weights = torch.tensor(sw, device=device)\n\n        # evaluate\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(f\" epoch={epoch}: val_wg={v_wg:.4f}, val_loss={v_loss/len(val_ds):.4f}\")\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # test preds\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            b = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(b[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(b[\"label\"].cpu().tolist())\n\n    # log\n    experiment_data[\"SGD_OPTIMIZER\"][\"synthetic\"][\"learning_rates\"].append(lr)\n    experiment_data[\"SGD_OPTIMIZER\"][\"synthetic\"][\"metrics\"][\"train\"].append(m_tr)\n    experiment_data[\"SGD_OPTIMIZER\"][\"synthetic\"][\"metrics\"][\"val\"].append(m_val)\n    experiment_data[\"SGD_OPTIMIZER\"][\"synthetic\"][\"losses\"][\"train\"].append(l_tr)\n    experiment_data[\"SGD_OPTIMIZER\"][\"synthetic\"][\"losses\"][\"val\"].append(l_val)\n    experiment_data[\"SGD_OPTIMIZER\"][\"synthetic\"][\"predictions\"].append(np.array(preds))\n    if not experiment_data[\"SGD_OPTIMIZER\"][\"synthetic\"][\"ground_truth\"]:\n        experiment_data[\"SGD_OPTIMIZER\"][\"synthetic\"][\"ground_truth\"] = np.array(truths)\n\n# Convert lists to numpy arrays\nsyn = experiment_data[\"SGD_OPTIMIZER\"][\"synthetic\"]\nsyn[\"learning_rates\"] = np.array(syn[\"learning_rates\"])\nsyn[\"metrics\"][\"train\"] = np.array(syn[\"metrics\"][\"train\"])\nsyn[\"metrics\"][\"val\"] = np.array(syn[\"metrics\"][\"val\"])\nsyn[\"losses\"][\"train\"] = np.array(syn[\"losses\"][\"train\"])\nsyn[\"losses\"][\"val\"] = np.array(syn[\"losses\"][\"val\"])\nsyn[\"predictions\"] = np.stack(syn[\"predictions\"])\n\n# Save data\nnp.save(\"experiment_data.npy\", experiment_data)\nprint(\"\\nSaved experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# splits\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\nmean = X[train_idx, :d].mean(0)\nstd = X[train_idx, :d].std(0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"features\": self.X[i], \"label\": self.y[i], \"group\": self.g[i], \"idx\": i}\n\n\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n\n\nclass MLP(nn.Module):\n    def __init__(self, inp, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\ndef evaluate(loader, model, crit):\n    model.eval()\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            out = model(batch[\"features\"])\n            losses = crit(out, batch[\"label\"])\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = batch[\"group\"] == g\n                total[g] += mask.sum().item()\n                if mask.any():\n                    correct[g] += (preds[mask] == batch[\"label\"][mask]).sum().item()\n    worst_group_acc = min(\n        correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1)\n    )\n    return loss_sum, worst_group_acc\n\n\n# hyperparameters\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\nexperiment_data = {\n    \"random_cluster_reweighting\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor lr in lrs:\n    print(f\"\\n=== lr={lr} ===\")\n    model = MLP(d + 1).to(device)\n    crit = nn.CrossEntropyLoss(reduction=\"none\")\n    opt = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for ep in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            out = model(batch[\"features\"])\n            losses = crit(out, batch[\"label\"])\n            if ep >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[batch[\"idx\"]]).mean()\n            else:\n                loss = losses.mean()\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n\n        if ep == warmup_epochs - 1:\n            Ntr = len(train_ds)\n            labs = np.random.randint(0, 2, size=Ntr)\n            cnt = np.bincount(labs, minlength=2)\n            sw = np.array([1.0 / cnt[l] for l in labs], dtype=np.float32)\n            sample_weights = torch.tensor(sw).to(device)\n\n        tr_loss, tr_wg = evaluate(train_loader, model, crit)\n        v_loss, v_wg = evaluate(val_loader, model, crit)\n        print(\n            f\"Epoch {ep}: validation_loss = {v_loss/len(val_ds):.4f}, worst_group_acc = {v_wg:.4f}\"\n        )\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {\n                k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)\n            }\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n\n    sd = experiment_data[\"random_cluster_reweighting\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(m_tr)\n    sd[\"metrics\"][\"val\"].append(m_val)\n    sd[\"losses\"][\"train\"].append(l_tr)\n    sd[\"losses\"][\"val\"].append(l_val)\n    sd[\"predictions\"].append(np.array(preds))\n    if len(sd[\"ground_truth\"]) == 0:\n        sd[\"ground_truth\"] = np.array(truths)\n\n# convert lists to arrays\nsd = experiment_data[\"random_cluster_reweighting\"][\"synthetic\"]\nsd[\"metrics\"][\"train\"] = np.array(sd[\"metrics\"][\"train\"])\nsd[\"metrics\"][\"val\"] = np.array(sd[\"metrics\"][\"val\"])\nsd[\"losses\"][\"train\"] = np.array(sd[\"losses\"][\"train\"])\nsd[\"losses\"][\"val\"] = np.array(sd[\"losses\"][\"val\"])\nsd[\"predictions\"] = np.stack(sd[\"predictions\"])\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# k-means on CPU\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper: returns (sum_loss, worst-group accuracy)\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameter sweep\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 0\ntrain_epochs = 5\ntotal_epochs = train_epochs\n\n# Containers for results\nmetrics_train_all = []\nmetrics_val_all = []\nlosses_train_all = []\nlosses_val_all = []\npredictions_list = []\nground_truth = None\n\nfor lr in lrs:\n    print(f\"\\n=== Training with learning rate = {lr} ===\")\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    # Immediate clustering on untrained network\n    model.eval()\n    grads = []\n    for sample in cluster_loader:\n        batch = {k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)}\n        optimizer.zero_grad()\n        out_i = model(batch[\"features\"])\n        loss_i = criterion(out_i, batch[\"label\"]).mean()\n        loss_i.backward()\n        g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n        grads.append(g)\n    grads = np.stack(grads)\n    labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n    counts = np.bincount(labels, minlength=2)\n    sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n    sample_weights = torch.tensor(sw_arr, device=device)\n\n    # per-epoch logs\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss = (losses * sample_weights[idxb]).mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(\n            f\"lr={lr} epoch={epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n        )\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n    preds_npy = np.array(preds)\n    truths_npy = np.array(truths)\n    predictions_list.append(preds_npy)\n    if ground_truth is None:\n        ground_truth = truths_npy\n\n    metrics_train_all.append(m_tr)\n    metrics_val_all.append(m_val)\n    losses_train_all.append(l_tr)\n    losses_val_all.append(l_val)\n\n# Assemble and save experiment data\nexperiment_data = {\n    \"no_warmup_cluster_reweighting\": {\n        \"synthetic\": {\n            \"learning_rates\": np.array(lrs),\n            \"metrics\": {\n                \"train\": np.array(metrics_train_all),\n                \"val\": np.array(metrics_val_all),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_all),\n                \"val\": np.array(losses_val_all),\n            },\n            \"predictions\": np.stack(predictions_list),\n            \"ground_truth\": ground_truth,\n        }\n    }\n}\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# reproducibility and device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# splits\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\nmean = X[train_idx, :d].mean(0)\nstd = X[train_idx, :d].std(0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"features\": self.X[i], \"label\": self.y[i], \"group\": self.g[i], \"idx\": i}\n\n\n# data loaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n\n\n# model\nclass MLP(nn.Module):\n    def __init__(self, inp, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# evaluation\ndef evaluate(loader, model, crit):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for b in loader:\n            b = {k: v.to(device) for k, v in b.items() if isinstance(v, torch.Tensor)}\n            out = model(b[\"features\"])\n            losses = crit(out, b[\"label\"])\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = b[\"group\"] == g\n                total[g] += mask.sum().item()\n                if mask.any():\n                    correct[g] += (preds[mask] == b[\"label\"][mask]).sum().item()\n    wg = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg\n\n\n# hyperparameters\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# experiment container\nexperiment_data = {\n    \"random_cluster_reweighting\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor lr in lrs:\n    print(f\"\\n=== lr={lr} ===\")\n    model = MLP(d + 1).to(device)\n    crit = nn.CrossEntropyLoss(reduction=\"none\")\n    opt = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for ep in range(total_epochs):\n        model.train()\n        for b in train_loader:\n            b = {k: v.to(device) for k, v in b.items() if isinstance(v, torch.Tensor)}\n            out = model(b[\"features\"])\n            losses = crit(out, b[\"label\"])\n            if ep >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[b[\"idx\"]]).mean()\n            else:\n                loss = losses.mean()\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n\n        # random cluster reweighting after warmup\n        if ep == warmup_epochs - 1:\n            Ntr = len(train_ds)\n            labs = np.random.randint(0, 2, size=Ntr)\n            cnt = np.bincount(labs, minlength=2)\n            sw = np.array([1.0 / cnt[l] for l in labs], dtype=np.float32)\n            sample_weights = torch.tensor(sw, device=device)\n\n        tr_loss, tr_wg = evaluate(train_loader, model, crit)\n        v_loss, v_wg = evaluate(val_loader, model, crit)\n        avg_v_loss = v_loss / len(val_ds)\n        print(\n            f\"Epoch {ep}: validation_loss = {avg_v_loss:.4f}, Worst-Group Accuracy = {v_wg:.4f}\"\n        )\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(avg_v_loss)\n\n    # test set predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for b in test_loader:\n            b = {k: v.to(device) for k, v in b.items() if isinstance(v, torch.Tensor)}\n            out = model(b[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(b[\"label\"].cpu().tolist())\n\n    sd = experiment_data[\"random_cluster_reweighting\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(m_tr)\n    sd[\"metrics\"][\"val\"].append(m_val)\n    sd[\"losses\"][\"train\"].append(l_tr)\n    sd[\"losses\"][\"val\"].append(l_val)\n    sd[\"predictions\"].append(np.array(preds))\n    if len(sd[\"ground_truth\"]) == 0:\n        sd[\"ground_truth\"] = np.array(truths)\n\n# convert lists to arrays\nsd = experiment_data[\"random_cluster_reweighting\"][\"synthetic\"]\nsd[\"metrics\"][\"train\"] = np.array(sd[\"metrics\"][\"train\"])\nsd[\"metrics\"][\"val\"] = np.array(sd[\"metrics\"][\"val\"])\nsd[\"losses\"][\"train\"] = np.array(sd[\"losses\"][\"train\"])\nsd[\"losses\"][\"val\"] = np.array(sd[\"losses\"][\"val\"])\nsd[\"predictions\"] = np.stack(sd[\"predictions\"])\n# 'ground_truth' is already an array\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Reproducibility and device\nnp.random.seed(0)\ntorch.manual_seed(0)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split into train/val/test\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous part\nmean = X[train_idx, :d].mean(0)\nstd = X[train_idx, :d].std(0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n\n# Dataset & loaders\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n        self.idx = torch.arange(len(self.y), dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\n            \"features\": self.X[i],\n            \"label\": self.y[i],\n            \"group\": self.g[i],\n            \"idx\": self.idx[i],\n        }\n\n\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# k-means for cluster\u2010based reweighting\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init].copy()\n    labels = np.zeros(N, int)\n    for _ in range(n_iters):\n        dists = ((X[:, None, :] - centroids[None, :, :]) ** 2).sum(-1)\n        new = dists.argmin(1)\n        if np.all(new == labels):\n            break\n        labels = new\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts):\n                centroids[k] = pts.mean(0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# Simple MLP\nclass MLP(nn.Module):\n    def __init__(self, inp, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluate: returns (sum_loss, worst\u2010group accuracy)\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                m = grp == g\n                total[g] += m.sum().item()\n                if m.sum() > 0:\n                    correct[g] += (preds[m] == yb[m]).sum().item()\n    wg = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg\n\n\n# Hyperparameters\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# Prepare experiment_data\nexperiment_data = {\n    \"SGD_OPTIMIZER\": {\n        \"synthetic\": {\n            \"learning_rates\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Run ablation with vanilla SGD\nfor lr in lrs:\n    print(f\"\\n=== SGD lr={lr} ===\")\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n    sample_weights = None\n\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # cluster-based reweighting after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                sb = {k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)}\n                optimizer.zero_grad()\n                out_i = model(sb[\"features\"])\n                li = criterion(out_i, sb[\"label\"]).mean()\n                li.backward()\n                g = model.fc.weight.grad.detach().cpu().reshape(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labs = kmeans_np(grads, n_clusters=2, n_iters=10)\n            cnt = np.bincount(labs, minlength=2)\n            sw = np.array([1.0 / cnt[lab] for lab in labs], np.float32)\n            sample_weights = torch.tensor(sw, device=device)\n\n        # evaluate\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(f\"Epoch {epoch}: validation_loss = {v_loss/len(val_ds):.4f}\")\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # test preds\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            b = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(b[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(b[\"label\"].cpu().tolist())\n\n    # log\n    syn = experiment_data[\"SGD_OPTIMIZER\"][\"synthetic\"]\n    syn[\"learning_rates\"].append(lr)\n    syn[\"metrics\"][\"train\"].append(m_tr)\n    syn[\"metrics\"][\"val\"].append(m_val)\n    syn[\"losses\"][\"train\"].append(l_tr)\n    syn[\"losses\"][\"val\"].append(l_val)\n    syn[\"predictions\"].append(np.array(preds))\n    # BUGFIX: avoid ambiguous truth value by checking list length\n    if len(syn[\"ground_truth\"]) == 0:\n        syn[\"ground_truth\"] = np.array(truths)\n\n# Convert lists to numpy arrays\nsyn[\"learning_rates\"] = np.array(syn[\"learning_rates\"])\nsyn[\"metrics\"][\"train\"] = np.array(syn[\"metrics\"][\"train\"])\nsyn[\"metrics\"][\"val\"] = np.array(syn[\"metrics\"][\"val\"])\nsyn[\"losses\"][\"train\"] = np.array(syn[\"losses\"][\"train\"])\nsyn[\"losses\"][\"val\"] = np.array(syn[\"losses\"][\"val\"])\nsyn[\"predictions\"] = np.stack(syn[\"predictions\"])\n\n# Save data\nnp.save(\"experiment_data.npy\", experiment_data)\nprint(\"\\nSaved experiment_data.npy\")\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# k-means on CPU\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Ablation over cluster counts\ncluster_counts = [1, 2, 4, 8]\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\nmetrics_train_all = []\nmetrics_val_all = []\nlosses_train_all = []\nlosses_val_all = []\npredictions_list = []\nground_truth = None\n\nfor k_count in cluster_counts:\n    mt_k, mv_k, lt_k, lv_k, preds_k = [], [], [], [], []\n    for lr in lrs:\n        # Initialize\n        model = MLP(d + 1).to(device)\n        criterion = nn.CrossEntropyLoss(reduction=\"none\")\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n        sample_weights = None\n\n        m_tr, m_val, l_tr, l_val = [], [], [], []\n        # Training\n        for epoch in range(total_epochs):\n            model.train()\n            for batch in train_loader:\n                batch = {\n                    k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                }\n                x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n                out = model(x)\n                losses = criterion(out, yb)\n                if epoch >= warmup_epochs and sample_weights is not None:\n                    loss = (losses * sample_weights[idxb]).mean()\n                else:\n                    loss = losses.mean()\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            # Reweight after warmup\n            if epoch == warmup_epochs - 1:\n                model.eval()\n                grads = []\n                for sample in cluster_loader:\n                    batch = {\n                        k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                    }\n                    optimizer.zero_grad()\n                    out_i = model(batch[\"features\"])\n                    loss_i = criterion(out_i, batch[\"label\"]).mean()\n                    loss_i.backward()\n                    g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                    grads.append(g)\n                grads = np.stack(grads)\n                labels_k = kmeans_np(grads, n_clusters=k_count, n_iters=10)\n                counts = np.bincount(labels_k, minlength=k_count)\n                sw_arr = np.array(\n                    [1.0 / counts[lab] for lab in labels_k], dtype=np.float32\n                )\n                sample_weights = torch.tensor(sw_arr, device=device)\n\n            # Evaluate\n            tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n            v_loss, v_wg = evaluate(val_loader, model, criterion)\n            m_tr.append(tr_wg)\n            m_val.append(v_wg)\n            l_tr.append(tr_loss / len(train_ds))\n            l_val.append(v_loss / len(val_ds))\n\n        # Test predictions\n        model.eval()\n        preds, truths = [], []\n        with torch.no_grad():\n            for batch in test_loader:\n                batch = {\n                    k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                }\n                out = model(batch[\"features\"])\n                preds.extend(out.argmax(1).cpu().tolist())\n                truths.extend(batch[\"label\"].cpu().tolist())\n        preds = np.array(preds)\n        if ground_truth is None:\n            ground_truth = np.array(truths)\n\n        mt_k.append(m_tr)\n        mv_k.append(m_val)\n        lt_k.append(l_tr)\n        lv_k.append(l_val)\n        preds_k.append(preds)\n\n    metrics_train_all.append(mt_k)\n    metrics_val_all.append(mv_k)\n    losses_train_all.append(lt_k)\n    losses_val_all.append(lv_k)\n    predictions_list.append(preds_k)\n\n# Stack into arrays\nmetrics_train_arr = np.array(metrics_train_all)\nmetrics_val_arr = np.array(metrics_val_all)\nlosses_train_arr = np.array(losses_train_all)\nlosses_val_arr = np.array(losses_val_all)\npreds_arr = np.array(predictions_list)\n\n# Assemble and save\nexperiment_data = {\n    \"CLUSTER_COUNT_VARIATION\": {\n        \"synthetic\": {\n            \"cluster_counts\": np.array(cluster_counts),\n            \"learning_rate\": {\n                \"lrs\": np.array(lrs),\n                \"metrics\": {\"train\": metrics_train_arr, \"val\": metrics_val_arr},\n                \"losses\": {\"train\": losses_train_arr, \"val\": losses_val_arr},\n                \"predictions\": preds_arr,\n                \"ground_truth\": ground_truth,\n            },\n        }\n    }\n}\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Split indices\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize continuous features\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Prepare splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\n# k-means on CPU\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper: returns (sum_loss, worst-group accuracy)\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Ablation: vary weight decay\nweight_decays = [0.0, 1e-4, 1e-3]\nlr = 1e-3\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\nmetrics_train_all, metrics_val_all = [], []\nlosses_train_all, losses_val_all = [], []\npredictions_list = []\nground_truth = None\n\nfor wd in weight_decays:\n    print(f\"\\n=== Training with weight_decay = {wd} ===\")\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n    sample_weights = None\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[idxb]).mean()\n            else:\n                loss = losses.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # cluster-based reweighting after warmup\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            grads = []\n            for sample in cluster_loader:\n                batch = {\n                    k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                }\n                optimizer.zero_grad()\n                out_i = model(batch[\"features\"])\n                loss_i = criterion(out_i, batch[\"label\"]).mean()\n                loss_i.backward()\n                g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                grads.append(g)\n            grads = np.stack(grads)\n            labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n            counts = np.bincount(labels, minlength=2)\n            sw_arr = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n            sample_weights = torch.tensor(sw_arr, device=device)\n\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(\n            f\"wd={wd} epoch={epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n        )\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # final test predictions\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            out = model(batch[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(batch[\"label\"].cpu().tolist())\n    preds_npy = np.array(preds)\n    truths_npy = np.array(truths)\n    predictions_list.append(preds_npy)\n    if ground_truth is None:\n        ground_truth = truths_npy\n\n    metrics_train_all.append(m_tr)\n    metrics_val_all.append(m_val)\n    losses_train_all.append(l_tr)\n    losses_val_all.append(l_val)\n\n# Assemble and save experiment data\nexperiment_data = {\n    \"weight_decay_variation\": {\n        \"synthetic\": {\n            \"weight_decays\": np.array(weight_decays),\n            \"metrics\": {\n                \"train\": np.array(metrics_train_all),\n                \"val\": np.array(metrics_val_all),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_all),\n                \"val\": np.array(losses_val_all),\n            },\n            \"predictions\": np.stack(predictions_list),\n            \"ground_truth\": ground_truth,\n        }\n    }\n}\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set device and seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Train/val/test split\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Normalize cont features\nmean = X[train_idx, :d].mean(0)\nstd = X[train_idx, :d].std(0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n# Splits\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, i):\n        return {\"features\": self.X[i], \"label\": self.y[i], \"group\": self.g[i], \"idx\": i}\n\n\n# DataLoaders\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n\n\n# Model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Eval helper\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for b in loader:\n            b = {k: v.to(device) for k, v in b.items() if torch.is_tensor(v)}\n            out = model(b[\"features\"])\n            l = criterion(out, b[\"label\"])\n            loss_sum += l.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                m = b[\"group\"] == g\n                total[g] += m.sum().item()\n                if m.sum() > 0:\n                    correct[g] += (preds[m] == b[\"label\"][m]).sum().item()\n    wg = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg\n\n\n# Hyperparameters\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n# Containers\nmetrics_train_all, metrics_val_all = [], []\nlosses_train_all, losses_val_all = [], []\npredictions_list = []\nground_truth = None\nsample_weights_list = []\n\n# Ablation: loss-based reweighting\nfor lr in lrs:\n    print(f\"\\nLR={lr}\")\n    model = MLP(d + 1).to(device)\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    optim = torch.optim.Adam(model.parameters(), lr=lr)\n    sample_weights = None\n\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(total_epochs):\n        model.train()\n        for b in train_loader:\n            b = {k: v.to(device) for k, v in b.items() if torch.is_tensor(v)}\n            out = model(b[\"features\"])\n            losses = criterion(out, b[\"label\"])\n            if epoch >= warmup_epochs and sample_weights is not None:\n                loss = (losses * sample_weights[b[\"idx\"]]).mean()\n            else:\n                loss = losses.mean()\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n\n        # compute loss-based weights\n        if epoch == warmup_epochs - 1:\n            model.eval()\n            all_loss = torch.zeros(len(train_ds), dtype=torch.float32, device=device)\n            with torch.no_grad():\n                for b in train_loader:\n                    b = {k: v.to(device) for k, v in b.items() if torch.is_tensor(v)}\n                    out = model(b[\"features\"])\n                    l = criterion(out, b[\"label\"])\n                    all_loss[b[\"idx\"]] = l\n            w = all_loss.cpu().numpy()\n            w = w / w.sum()\n            sample_weights = torch.tensor(w, dtype=torch.float32, device=device)\n            sample_weights_list.append(w)\n\n        # evaluate\n        tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n        v_loss, v_wg = evaluate(val_loader, model, criterion)\n        print(f\"Epoch {epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\")\n        m_tr.append(tr_wg)\n        m_val.append(v_wg)\n        l_tr.append(tr_loss / len(train_ds))\n        l_val.append(v_loss / len(val_ds))\n\n    # test preds\n    model.eval()\n    preds, truths = [], []\n    with torch.no_grad():\n        for b in test_loader:\n            b = {k: v.to(device) for k, v in b.items() if torch.is_tensor(v)}\n            out = model(b[\"features\"])\n            preds.extend(out.argmax(1).cpu().tolist())\n            truths.extend(b[\"label\"].cpu().tolist())\n    preds = np.array(preds)\n    truths = np.array(truths)\n    predictions_list.append(preds)\n    if ground_truth is None:\n        ground_truth = truths\n\n    metrics_train_all.append(m_tr)\n    metrics_val_all.append(m_val)\n    losses_train_all.append(l_tr)\n    losses_val_all.append(l_val)\n\n# Save experiment data\nexperiment_data = {\n    \"LOSS_BASED_SAMPLE_WEIGHTING\": {\n        \"synthetic\": {\n            \"metrics\": {\n                \"train\": np.array(metrics_train_all),\n                \"val\": np.array(metrics_val_all),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_all),\n                \"val\": np.array(losses_val_all),\n            },\n            \"predictions\": np.stack(predictions_list),\n            \"ground_truth\": ground_truth,\n            \"sample_weights\": np.stack(sample_weights_list),\n        }\n    }\n}\n\nnp.save(\"experiment_data.npy\", experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Train/val/test split\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\nsplits = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\ntrain_ds = SyntheticDataset(*splits[\"train\"])\nval_ds = SyntheticDataset(*splits[\"val\"])\ntest_ds = SyntheticDataset(*splits[\"test\"])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\ncluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            centroids[k] = pts.mean(axis=0) if len(pts) > 0 else X[rng.randint(N)]\n    return labels\n\n\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparams\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs, train_epochs = 1, 5\ntotal_epochs = warmup_epochs + train_epochs\ncriterion_fn = nn.CrossEntropyLoss(reduction=\"none\")\n\n\ndef run_experiment(normalize=False):\n    metrics_train_all, metrics_val_all = [], []\n    losses_train_all, losses_val_all = [], []\n    predictions_list = []\n    ground_truth = None\n    for lr in lrs:\n        print(f\"\\n=== {'Normalized' if normalize else 'Raw'} clustering, lr={lr} ===\")\n        model = MLP(d + 1).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n        sample_weights = None\n        m_tr, m_val = [], []\n        l_tr, l_val = [], []\n        for epoch in range(total_epochs):\n            model.train()\n            for batch in train_loader:\n                batch = {\n                    k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                }\n                x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n                out = model(x)\n                losses = criterion_fn(out, yb)\n                if epoch >= warmup_epochs and sample_weights is not None:\n                    loss = (losses * sample_weights[idxb]).mean()\n                else:\n                    loss = losses.mean()\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            # cluster-based reweight\n            if epoch == warmup_epochs - 1:\n                model.eval()\n                grads = []\n                for sample in cluster_loader:\n                    batch = {\n                        k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                    }\n                    optimizer.zero_grad()\n                    out_i = model(batch[\"features\"])\n                    loss_i = criterion_fn(out_i, batch[\"label\"]).mean()\n                    loss_i.backward()\n                    g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                    if normalize:\n                        norm = np.linalg.norm(g) + 1e-6\n                        g = g / norm\n                    grads.append(g)\n                grads = np.stack(grads)\n                labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n                counts = np.bincount(labels, minlength=2)\n                sw = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n                sample_weights = torch.tensor(sw, device=device)\n            # evaluate\n            tr_loss, tr_wg = evaluate(train_loader, model, criterion_fn)\n            v_loss, v_wg = evaluate(val_loader, model, criterion_fn)\n            print(\n                f\" epoch={epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n            )\n            m_tr.append(tr_wg)\n            m_val.append(v_wg)\n            l_tr.append(tr_loss / len(train_ds))\n            l_val.append(v_loss / len(val_ds))\n        # test preds\n        model.eval()\n        preds, truths = [], []\n        with torch.no_grad():\n            for batch in test_loader:\n                batch = {\n                    k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                }\n                out = model(batch[\"features\"])\n                preds.extend(out.argmax(1).cpu().tolist())\n                truths.extend(batch[\"label\"].cpu().tolist())\n        preds_npy = np.array(preds)\n        truths_npy = np.array(truths)\n        predictions_list.append(preds_npy)\n        if ground_truth is None:\n            ground_truth = truths_npy\n        metrics_train_all.append(m_tr)\n        metrics_val_all.append(m_val)\n        losses_train_all.append(l_tr)\n        losses_val_all.append(l_val)\n    return {\n        \"metrics\": {\n            \"train\": np.array(metrics_train_all),\n            \"val\": np.array(metrics_val_all),\n        },\n        \"losses\": {\n            \"train\": np.array(losses_train_all),\n            \"val\": np.array(losses_val_all),\n        },\n        \"predictions\": np.stack(predictions_list),\n        \"ground_truth\": ground_truth,\n    }\n\n\n# Run both ablations\nraw_data = run_experiment(normalize=False)\nnorm_data = run_experiment(normalize=True)\n\n# Assemble and save\nexperiment_data = {\n    \"raw_gradient_cluster_reweighting\": {\"synthetic\": raw_data},\n    \"normalized_gradient_cluster_reweighting\": {\"synthetic\": norm_data},\n}\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nSaved experiment data to {os.path.join(working_dir, 'experiment_data.npy')}\")\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# Synthetic data with spurious feature\nN, d = 2000, 5\ny = np.random.binomial(1, 0.5, size=N)\nX_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\nspurious_corr = 0.95\nrnd = np.random.rand(N)\ns = np.where(rnd < spurious_corr, y, 1 - y)\nX = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n# Train/val/test split\nidxs = np.arange(N)\nnp.random.shuffle(idxs)\ntrain_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n# Create normalized and raw splits\nmean = X[train_idx, :d].mean(axis=0)\nstd = X[train_idx, :d].std(axis=0) + 1e-6\nX_norm = X.copy()\nX_norm[:, :d] = (X_norm[:, :d] - mean) / std\nsplits_norm = {\n    \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n}\nsplits_no_norm = {\n    \"train\": (X[train_idx], y[train_idx], s[train_idx]),\n    \"val\": (X[val_idx], y[val_idx], s[val_idx]),\n    \"test\": (X[test_idx], y[test_idx], s[test_idx]),\n}\n\n\n# Dataset and helper functions\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init].copy()\n    labels = np.zeros(N, int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            centroids[k] = pts.mean(0) if len(pts) > 0 else X[rng.randint(N)]\n    return labels\n\n\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Training hyperparameters\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\n\n\ndef run_experiment(splits, tag=\"\"):\n    # DataLoaders\n    Xtr, ytr, str_ = splits[\"train\"]\n    Xv, yv, sv = splits[\"val\"]\n    Xt, yt, st = splits[\"test\"]\n    train_ds = SyntheticDataset(Xtr, ytr, str_)\n    val_ds = SyntheticDataset(Xv, yv, sv)\n    test_ds = SyntheticDataset(Xt, yt, st)\n    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n    cluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n    # Containers\n    metrics_train_all, metrics_val_all = [], []\n    losses_train_all, losses_val_all = [], []\n    predictions_list = []\n    ground_truth = None\n    inp_dim = Xtr.shape[1]\n    # LR sweep\n    for lr in lrs:\n        print(f\"\\n=== {tag} lr={lr} ===\")\n        model = MLP(inp_dim).to(device)\n        criterion = nn.CrossEntropyLoss(reduction=\"none\")\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n        sample_weights = None\n        m_tr, m_val, l_tr, l_val = [], [], [], []\n        for epoch in range(total_epochs):\n            model.train()\n            for batch in train_loader:\n                batch = {\n                    k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                }\n                x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n                out = model(x)\n                losses = criterion(out, yb)\n                if epoch >= warmup_epochs and sample_weights is not None:\n                    loss = (losses * sample_weights[idxb]).mean()\n                else:\n                    loss = losses.mean()\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            # clustering & reweight\n            if epoch == warmup_epochs - 1:\n                model.eval()\n                grads = []\n                for sample in cluster_loader:\n                    batch = {\n                        k: v.to(device) for k, v in sample.items() if torch.is_tensor(v)\n                    }\n                    optimizer.zero_grad()\n                    out_i = model(batch[\"features\"])\n                    loss_i = criterion(out_i, batch[\"label\"]).mean()\n                    loss_i.backward()\n                    g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                    grads.append(g)\n                grads = np.stack(grads)\n                labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n                counts = np.bincount(labels, minlength=2)\n                sw = np.array([1.0 / counts[lab] for lab in labels], dtype=np.float32)\n                sample_weights = torch.tensor(sw, device=device)\n            # evaluation\n            tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n            v_loss, v_wg = evaluate(val_loader, model, criterion)\n            print(\n                f\"{tag} lr={lr} epoch={epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n            )\n            m_tr.append(tr_wg)\n            m_val.append(v_wg)\n            l_tr.append(tr_loss / len(train_ds))\n            l_val.append(v_loss / len(val_ds))\n        # collect per-lr\n        metrics_train_all.append(m_tr)\n        metrics_val_all.append(m_val)\n        losses_train_all.append(l_tr)\n        losses_val_all.append(l_val)\n        # test set predictions\n        model.eval()\n        preds, truths = [], []\n        with torch.no_grad():\n            for batch in test_loader:\n                batch = {\n                    k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                }\n                out = model(batch[\"features\"])\n                preds.extend(out.argmax(1).cpu().tolist())\n                truths.extend(batch[\"label\"].cpu().tolist())\n        preds = np.array(preds)\n        truths = np.array(truths)\n        predictions_list.append(preds)\n        if ground_truth is None:\n            ground_truth = truths\n    return {\n        \"metrics_train\": np.array(metrics_train_all),\n        \"metrics_val\": np.array(metrics_val_all),\n        \"losses_train\": np.array(losses_train_all),\n        \"losses_val\": np.array(losses_val_all),\n        \"predictions\": np.stack(predictions_list),\n        \"ground_truth\": ground_truth,\n    }\n\n\n# Run ablation: with vs without normalization\nexperiment_data = {\"NO_FEATURE_NORMALIZATION\": {}}\nfor name, splits in [\n    (\"synthetic_with_norm\", splits_norm),\n    (\"synthetic_no_norm\", splits_no_norm),\n]:\n    res = run_experiment(splits, tag=name)\n    experiment_data[\"NO_FEATURE_NORMALIZATION\"][name] = {\n        \"metrics\": {\n            \"train\": res[\"metrics_train\"],\n            \"val\": res[\"metrics_val\"],\n        },\n        \"losses\": {\n            \"train\": res[\"losses_train\"],\n            \"val\": res[\"losses_val\"],\n        },\n        \"predictions\": res[\"predictions\"],\n        \"ground_truth\": res[\"ground_truth\"],\n    }\n\n# Save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n# Dataset class\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# k-means implementation\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameters\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\nN = 2000\n\n# Ablation settings\nspurious_corrs = [0.5, 0.75, 0.95]\ndims_list = [5, 10, 15]\n\n# Container for all results\nexperiment_data = {\"multiple_synthetic\": {}}\n\nfor sp_corr in spurious_corrs:\n    for d in dims_list:\n        ds_name = f\"corr_{int(sp_corr*100)}_d{d}\"\n        print(f\"\\n=== Dataset: {ds_name} ===\")\n\n        # Generate synthetic data\n        y = np.random.binomial(1, 0.5, size=N)\n        X_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\n        rnd = np.random.rand(N)\n        s = np.where(rnd < sp_corr, y, 1 - y)\n        X = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n        # Split indices\n        idxs = np.arange(N)\n        np.random.shuffle(idxs)\n        train_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n        # Normalize continuous features\n        mean = X[train_idx, :d].mean(axis=0)\n        std = X[train_idx, :d].std(axis=0) + 1e-6\n        X_norm = X.copy()\n        X_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n        # Prepare splits\n        splits = {\n            \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n            \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n            \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n        }\n\n        # Create DataLoaders\n        train_ds = SyntheticDataset(*splits[\"train\"])\n        val_ds = SyntheticDataset(*splits[\"val\"])\n        test_ds = SyntheticDataset(*splits[\"test\"])\n        train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n        test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n        cluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n        # Containers per dataset\n        metrics_train_all = []\n        metrics_val_all = []\n        losses_train_all = []\n        losses_val_all = []\n        predictions_list = []\n        ground_truth = None\n\n        # Sweep over learning rates\n        for lr in lrs:\n            print(f\"\\n-- Training with lr={lr} --\")\n            model = MLP(d + 1).to(device)\n            criterion = nn.CrossEntropyLoss(reduction=\"none\")\n            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n            sample_weights = None\n\n            m_tr, m_val = [], []\n            l_tr, l_val = [], []\n\n            for epoch in range(total_epochs):\n                model.train()\n                for batch in train_loader:\n                    batch = {\n                        k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                    }\n                    x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n                    out = model(x)\n                    losses = criterion(out, yb)\n                    if epoch >= warmup_epochs and sample_weights is not None:\n                        loss = (losses * sample_weights[idxb]).mean()\n                    else:\n                        loss = losses.mean()\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n\n                # Clustering for sample weights\n                if epoch == warmup_epochs - 1:\n                    model.eval()\n                    grads = []\n                    for sample in cluster_loader:\n                        batch = {\n                            k: v.to(device)\n                            for k, v in sample.items()\n                            if torch.is_tensor(v)\n                        }\n                        optimizer.zero_grad()\n                        out_i = model(batch[\"features\"])\n                        loss_i = criterion(out_i, batch[\"label\"]).mean()\n                        loss_i.backward()\n                        g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                        grads.append(g)\n                    grads = np.stack(grads)\n                    labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n                    counts = np.bincount(labels, minlength=2)\n                    sw_arr = np.array(\n                        [1.0 / counts[lab] for lab in labels], dtype=np.float32\n                    )\n                    sample_weights = torch.tensor(sw_arr, device=device)\n\n                # Evaluate\n                tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n                v_loss, v_wg = evaluate(val_loader, model, criterion)\n                print(\n                    f\"Epoch {epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n                )\n                m_tr.append(tr_wg)\n                m_val.append(v_wg)\n                l_tr.append(tr_loss / len(train_ds))\n                l_val.append(v_loss / len(val_ds))\n\n            # Test predictions\n            model.eval()\n            preds, truths = [], []\n            with torch.no_grad():\n                for batch in test_loader:\n                    batch = {\n                        k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                    }\n                    out = model(batch[\"features\"])\n                    preds.extend(out.argmax(1).cpu().tolist())\n                    truths.extend(batch[\"label\"].cpu().tolist())\n\n            preds_npy = np.array(preds)\n            truths_npy = np.array(truths)\n            predictions_list.append(preds_npy)\n            if ground_truth is None:\n                ground_truth = truths_npy\n\n            metrics_train_all.append(m_tr)\n            metrics_val_all.append(m_val)\n            losses_train_all.append(l_tr)\n            losses_val_all.append(l_val)\n\n        # Save dataset results\n        experiment_data[\"multiple_synthetic\"][ds_name] = {\n            \"metrics\": {\n                \"train\": np.array(metrics_train_all),\n                \"val\": np.array(metrics_val_all),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_all),\n                \"val\": np.array(losses_val_all),\n            },\n            \"predictions\": np.stack(predictions_list),\n            \"ground_truth\": ground_truth,\n            \"spurious_corr\": sp_corr,\n            \"dim\": d,\n        }\n\n# Save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nSaved experiment_data.npy to {working_dir}\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n# Dataset class\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# k-means implementation\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameters\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\nN = 2000\n\n# Ablation settings\nspurious_corrs = [0.5, 0.75, 0.95]\ndims_list = [5, 10, 15]\n\n# Container for all results\nexperiment_data = {\"multiple_synthetic\": {}}\n\nfor sp_corr in spurious_corrs:\n    for d in dims_list:\n        ds_name = f\"corr_{int(sp_corr*100)}_d{d}\"\n        print(f\"\\n=== Dataset: {ds_name} ===\")\n\n        # Generate synthetic data\n        y = np.random.binomial(1, 0.5, size=N)\n        X_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\n        rnd = np.random.rand(N)\n        s = np.where(rnd < sp_corr, y, 1 - y)\n        X = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n        # Split indices\n        idxs = np.arange(N)\n        np.random.shuffle(idxs)\n        train_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n        # Normalize continuous features\n        mean = X[train_idx, :d].mean(axis=0)\n        std = X[train_idx, :d].std(axis=0) + 1e-6\n        X_norm = X.copy()\n        X_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n        # Prepare splits\n        splits = {\n            \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n            \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n            \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n        }\n\n        # Create DataLoaders\n        train_ds = SyntheticDataset(*splits[\"train\"])\n        val_ds = SyntheticDataset(*splits[\"val\"])\n        test_ds = SyntheticDataset(*splits[\"test\"])\n        train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n        test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n        cluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n        # Containers per dataset\n        metrics_train_all = []\n        metrics_val_all = []\n        losses_train_all = []\n        losses_val_all = []\n        predictions_list = []\n        ground_truth = None\n\n        # Sweep over learning rates\n        for lr in lrs:\n            print(f\"\\n-- Training with lr={lr} --\")\n            model = MLP(d + 1).to(device)\n            criterion = nn.CrossEntropyLoss(reduction=\"none\")\n            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n            sample_weights = None\n\n            m_tr, m_val = [], []\n            l_tr, l_val = [], []\n\n            for epoch in range(total_epochs):\n                model.train()\n                for batch in train_loader:\n                    batch = {\n                        k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                    }\n                    x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n                    out = model(x)\n                    losses = criterion(out, yb)\n                    if epoch >= warmup_epochs and sample_weights is not None:\n                        loss = (losses * sample_weights[idxb]).mean()\n                    else:\n                        loss = losses.mean()\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n\n                # Clustering for sample weights\n                if epoch == warmup_epochs - 1:\n                    model.eval()\n                    grads = []\n                    for sample in cluster_loader:\n                        batch = {\n                            k: v.to(device)\n                            for k, v in sample.items()\n                            if torch.is_tensor(v)\n                        }\n                        optimizer.zero_grad()\n                        out_i = model(batch[\"features\"])\n                        loss_i = criterion(out_i, batch[\"label\"]).mean()\n                        loss_i.backward()\n                        g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                        grads.append(g)\n                    grads = np.stack(grads)\n                    labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n                    counts = np.bincount(labels, minlength=2)\n                    sw_arr = np.array(\n                        [1.0 / counts[lab] for lab in labels], dtype=np.float32\n                    )\n                    sample_weights = torch.tensor(sw_arr, device=device)\n\n                # Evaluate\n                tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n                v_loss, v_wg = evaluate(val_loader, model, criterion)\n                print(\n                    f\"Epoch {epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n                )\n                m_tr.append(tr_wg)\n                m_val.append(v_wg)\n                l_tr.append(tr_loss / len(train_ds))\n                l_val.append(v_loss / len(val_ds))\n\n            # Test predictions\n            model.eval()\n            preds, truths = [], []\n            with torch.no_grad():\n                for batch in test_loader:\n                    batch = {\n                        k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                    }\n                    out = model(batch[\"features\"])\n                    preds.extend(out.argmax(1).cpu().tolist())\n                    truths.extend(batch[\"label\"].cpu().tolist())\n\n            preds_npy = np.array(preds)\n            truths_npy = np.array(truths)\n            predictions_list.append(preds_npy)\n            if ground_truth is None:\n                ground_truth = truths_npy\n\n            metrics_train_all.append(m_tr)\n            metrics_val_all.append(m_val)\n            losses_train_all.append(l_tr)\n            losses_val_all.append(l_val)\n\n        # Save dataset results\n        experiment_data[\"multiple_synthetic\"][ds_name] = {\n            \"metrics\": {\n                \"train\": np.array(metrics_train_all),\n                \"val\": np.array(metrics_val_all),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_all),\n                \"val\": np.array(losses_val_all),\n            },\n            \"predictions\": np.stack(predictions_list),\n            \"ground_truth\": ground_truth,\n            \"spurious_corr\": sp_corr,\n            \"dim\": d,\n        }\n\n# Save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nSaved experiment_data.npy to {working_dir}\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and random seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n# Dataset class\nclass SyntheticDataset(Dataset):\n    def __init__(self, X, y, g):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.g = torch.tensor(g, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {\n            \"features\": self.X[idx],\n            \"label\": self.y[idx],\n            \"group\": self.g[idx],\n            \"idx\": idx,\n        }\n\n\n# k-means implementation\ndef kmeans_np(X, n_clusters=2, n_iters=10):\n    rng = np.random.RandomState(0)\n    N, D = X.shape\n    init_idxs = rng.choice(N, n_clusters, replace=False)\n    centroids = X[init_idxs].copy()\n    labels = np.zeros(N, dtype=int)\n    for _ in range(n_iters):\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.all(new_labels == labels):\n            break\n        labels = new_labels\n        for k in range(n_clusters):\n            pts = X[labels == k]\n            if len(pts) > 0:\n                centroids[k] = pts.mean(axis=0)\n            else:\n                centroids[k] = X[rng.randint(N)]\n    return labels\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, hid=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(inp_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU()\n        )\n        self.fc = nn.Linear(hid, 2)\n\n    def forward(self, x):\n        return self.fc(self.net(x))\n\n\n# Evaluation helper\ndef evaluate(loader, model, criterion):\n    loss_sum = 0.0\n    correct = {0: 0, 1: 0}\n    total = {0: 0, 1: 0}\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n            x, yb, grp = batch[\"features\"], batch[\"label\"], batch[\"group\"]\n            out = model(x)\n            losses = criterion(out, yb)\n            loss_sum += losses.sum().item()\n            preds = out.argmax(1)\n            for g in (0, 1):\n                mask = grp == g\n                total[g] += mask.sum().item()\n                if mask.sum().item() > 0:\n                    correct[g] += (preds[mask] == yb[mask]).sum().item()\n    wg_acc = min(correct[g] / total[g] if total[g] > 0 else 0.0 for g in (0, 1))\n    return loss_sum, wg_acc\n\n\n# Hyperparameters\nlrs = [1e-4, 1e-3, 1e-2]\nwarmup_epochs = 1\ntrain_epochs = 5\ntotal_epochs = warmup_epochs + train_epochs\nN = 2000\n\n# Ablation settings\nspurious_corrs = [0.5, 0.75, 0.95]\ndims_list = [5, 10, 15]\n\n# Container for all results\nexperiment_data = {\"multiple_synthetic\": {}}\n\nfor sp_corr in spurious_corrs:\n    for d in dims_list:\n        ds_name = f\"corr_{int(sp_corr*100)}_d{d}\"\n        print(f\"\\n=== Dataset: {ds_name} ===\")\n\n        # Generate synthetic data\n        y = np.random.binomial(1, 0.5, size=N)\n        X_cont = np.random.randn(N, d) + 2 * y.reshape(-1, 1)\n        rnd = np.random.rand(N)\n        s = np.where(rnd < sp_corr, y, 1 - y)\n        X = np.concatenate([X_cont, s.reshape(-1, 1)], axis=1)\n\n        # Split indices\n        idxs = np.arange(N)\n        np.random.shuffle(idxs)\n        train_idx, val_idx, test_idx = idxs[:1000], idxs[1000:1500], idxs[1500:]\n\n        # Normalize continuous features\n        mean = X[train_idx, :d].mean(axis=0)\n        std = X[train_idx, :d].std(axis=0) + 1e-6\n        X_norm = X.copy()\n        X_norm[:, :d] = (X_norm[:, :d] - mean) / std\n\n        # Prepare splits\n        splits = {\n            \"train\": (X_norm[train_idx], y[train_idx], s[train_idx]),\n            \"val\": (X_norm[val_idx], y[val_idx], s[val_idx]),\n            \"test\": (X_norm[test_idx], y[test_idx], s[test_idx]),\n        }\n\n        # Create DataLoaders\n        train_ds = SyntheticDataset(*splits[\"train\"])\n        val_ds = SyntheticDataset(*splits[\"val\"])\n        test_ds = SyntheticDataset(*splits[\"test\"])\n        train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n        test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n        cluster_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n\n        # Containers per dataset\n        metrics_train_all = []\n        metrics_val_all = []\n        losses_train_all = []\n        losses_val_all = []\n        predictions_list = []\n        ground_truth = None\n\n        # Sweep over learning rates\n        for lr in lrs:\n            print(f\"\\n-- Training with lr={lr} --\")\n            model = MLP(d + 1).to(device)\n            criterion = nn.CrossEntropyLoss(reduction=\"none\")\n            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n            sample_weights = None\n\n            m_tr, m_val = [], []\n            l_tr, l_val = [], []\n\n            for epoch in range(total_epochs):\n                model.train()\n                for batch in train_loader:\n                    batch = {\n                        k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                    }\n                    x, yb, idxb = batch[\"features\"], batch[\"label\"], batch[\"idx\"]\n                    out = model(x)\n                    losses = criterion(out, yb)\n                    if epoch >= warmup_epochs and sample_weights is not None:\n                        loss = (losses * sample_weights[idxb]).mean()\n                    else:\n                        loss = losses.mean()\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n\n                # Clustering for sample weights\n                if epoch == warmup_epochs - 1:\n                    model.eval()\n                    grads = []\n                    for sample in cluster_loader:\n                        batch = {\n                            k: v.to(device)\n                            for k, v in sample.items()\n                            if torch.is_tensor(v)\n                        }\n                        optimizer.zero_grad()\n                        out_i = model(batch[\"features\"])\n                        loss_i = criterion(out_i, batch[\"label\"]).mean()\n                        loss_i.backward()\n                        g = model.fc.weight.grad.detach().cpu().view(-1).numpy()\n                        grads.append(g)\n                    grads = np.stack(grads)\n                    labels = kmeans_np(grads, n_clusters=2, n_iters=10)\n                    counts = np.bincount(labels, minlength=2)\n                    sw_arr = np.array(\n                        [1.0 / counts[lab] for lab in labels], dtype=np.float32\n                    )\n                    sample_weights = torch.tensor(sw_arr, device=device)\n\n                # Evaluate\n                tr_loss, tr_wg = evaluate(train_loader, model, criterion)\n                v_loss, v_wg = evaluate(val_loader, model, criterion)\n                print(\n                    f\"Epoch {epoch}: val_loss={v_loss/len(val_ds):.4f}, val_wg={v_wg:.4f}\"\n                )\n                m_tr.append(tr_wg)\n                m_val.append(v_wg)\n                l_tr.append(tr_loss / len(train_ds))\n                l_val.append(v_loss / len(val_ds))\n\n            # Test predictions\n            model.eval()\n            preds, truths = [], []\n            with torch.no_grad():\n                for batch in test_loader:\n                    batch = {\n                        k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)\n                    }\n                    out = model(batch[\"features\"])\n                    preds.extend(out.argmax(1).cpu().tolist())\n                    truths.extend(batch[\"label\"].cpu().tolist())\n\n            preds_npy = np.array(preds)\n            truths_npy = np.array(truths)\n            predictions_list.append(preds_npy)\n            if ground_truth is None:\n                ground_truth = truths_npy\n\n            metrics_train_all.append(m_tr)\n            metrics_val_all.append(m_val)\n            losses_train_all.append(l_tr)\n            losses_val_all.append(l_val)\n\n        # Save dataset results\n        experiment_data[\"multiple_synthetic\"][ds_name] = {\n            \"metrics\": {\n                \"train\": np.array(metrics_train_all),\n                \"val\": np.array(metrics_val_all),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_all),\n                \"val\": np.array(losses_val_all),\n            },\n            \"predictions\": np.stack(predictions_list),\n            \"ground_truth\": ground_truth,\n            \"spurious_corr\": sp_corr,\n            \"dim\": d,\n        }\n\n# Save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nSaved experiment_data.npy to {working_dir}\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\n=== Training with learning rate = 0.0001 ===',\n'\\n', 'lr=0.0001 epoch=0: val_loss=0.6977, val_wg=0.3168', '\\n', 'lr=0.0001\nepoch=1: val_loss=0.6917, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=2:\nval_loss=0.6904, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=3: val_loss=0.6900,\nval_wg=0.3588', '\\n', 'lr=0.0001 epoch=4: val_loss=0.6899, val_wg=0.3588', '\\n',\n'lr=0.0001 epoch=5: val_loss=0.6898, val_wg=0.3588', '\\n', '\\n=== Training with\nlearning rate = 0.001 ===', '\\n', 'lr=0.001 epoch=0: val_loss=0.5822,\nval_wg=0.8866', '\\n', 'lr=0.001 epoch=1: val_loss=0.5093, val_wg=0.9538', '\\n',\n'lr=0.001 epoch=2: val_loss=0.4929, val_wg=0.9622', '\\n', 'lr=0.001 epoch=3:\nval_loss=0.4887, val_wg=0.9622', '\\n', 'lr=0.001 epoch=4: val_loss=0.4872,\nval_wg=0.9622', '\\n', 'lr=0.001 epoch=5: val_loss=0.4860, val_wg=0.9622', '\\n',\n'\\n=== Training with learning rate = 0.01 ===', '\\n', 'lr=0.01 epoch=0:\nval_loss=0.0228, val_wg=0.9885', '\\n', 'lr=0.01 epoch=1: val_loss=0.0305,\nval_wg=0.9885', '\\n', 'lr=0.01 epoch=2: val_loss=0.0348, val_wg=0.9885', '\\n',\n'lr=0.01 epoch=3: val_loss=0.0380, val_wg=0.9885', '\\n', 'lr=0.01 epoch=4:\nval_loss=0.0405, val_wg=0.9885', '\\n', 'lr=0.01 epoch=5: val_loss=0.0430,\nval_wg=0.9924', '\\n', 'Execution time: 6 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', '\\n=== Dataset: corr_50_d5 ===', '\\n', '\\n--\nTraining with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.7016, val_wg=0.3911',\n'\\n', 'Epoch 1: val_loss=0.6961, val_wg=0.4435', '\\n', 'Epoch 2:\nval_loss=0.6948, val_wg=0.4476', '\\n', 'Epoch 3: val_loss=0.6945,\nval_wg=0.4476', '\\n', 'Epoch 4: val_loss=0.6944, val_wg=0.4476', '\\n', 'Epoch 5:\nval_loss=0.6944, val_wg=0.4476', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5829, val_wg=0.9444', '\\n', 'Epoch 1: val_loss=0.5080,\nval_wg=0.9758', '\\n', 'Epoch 2: val_loss=0.4912, val_wg=0.9762', '\\n', 'Epoch 3:\nval_loss=0.4869, val_wg=0.9762', '\\n', 'Epoch 4: val_loss=0.4852,\nval_wg=0.9762', '\\n', 'Epoch 5: val_loss=0.4841, val_wg=0.9762', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0382, val_wg=0.9839',\n'\\n', 'Epoch 1: val_loss=0.0592, val_wg=0.9798', '\\n', 'Epoch 2:\nval_loss=0.0738, val_wg=0.9798', '\\n', 'Epoch 3: val_loss=0.0884,\nval_wg=0.9718', '\\n', 'Epoch 4: val_loss=0.1001, val_wg=0.9718', '\\n', 'Epoch 5:\nval_loss=0.1123, val_wg=0.9718', '\\n', '\\n=== Dataset: corr_50_d10 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6674,\nval_wg=0.5475', '\\n', 'Epoch 1: val_loss=0.6595, val_wg=0.5932', '\\n', 'Epoch 2:\nval_loss=0.6577, val_wg=0.6046', '\\n', 'Epoch 3: val_loss=0.6573,\nval_wg=0.6046', '\\n', 'Epoch 4: val_loss=0.6571, val_wg=0.6084', '\\n', 'Epoch 5:\nval_loss=0.6570, val_wg=0.6084', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5955, val_wg=0.7529', '\\n', 'Epoch 1: val_loss=0.5222,\nval_wg=0.9494', '\\n', 'Epoch 2: val_loss=0.5052, val_wg=0.9578', '\\n', 'Epoch 3:\nval_loss=0.5008, val_wg=0.9620', '\\n', 'Epoch 4: val_loss=0.4990,\nval_wg=0.9620', '\\n', 'Epoch 5: val_loss=0.4976, val_wg=0.9620', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0020, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0003, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0003, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0002,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0002, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0002, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_50_d15 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6998,\nval_wg=0.4961', '\\n', 'Epoch 1: val_loss=0.6917, val_wg=0.4961', '\\n', 'Epoch 2:\nval_loss=0.6898, val_wg=0.4961', '\\n', 'Epoch 3: val_loss=0.6894,\nval_wg=0.4961', '\\n', 'Epoch 4: val_loss=0.6892, val_wg=0.4961', '\\n', 'Epoch 5:\nval_loss=0.6891, val_wg=0.4961', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5586, val_wg=0.7561', '\\n', 'Epoch 1: val_loss=0.4617,\nval_wg=0.9797', '\\n', 'Epoch 2: val_loss=0.4385, val_wg=0.9878', '\\n', 'Epoch 3:\nval_loss=0.4324, val_wg=0.9878', '\\n', 'Epoch 4: val_loss=0.4298,\nval_wg=0.9878', '\\n', 'Epoch 5: val_loss=0.4279, val_wg=0.9919', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0001, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0000,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0000, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_75_d5 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6614,\nval_wg=0.5635', '\\n', 'Epoch 1: val_loss=0.6560, val_wg=0.5952', '\\n', 'Epoch 2:\nval_loss=0.6547, val_wg=0.6032', '\\n', 'Epoch 3: val_loss=0.6544,\nval_wg=0.6032', '\\n', 'Epoch 4: val_loss=0.6543, val_wg=0.6032', '\\n', 'Epoch 5:\nval_loss=0.6543, val_wg=0.6032', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.6262, val_wg=0.6492', '\\n', 'Epoch 1: val_loss=0.5783,\nval_wg=0.9484', '\\n', 'Epoch 2: val_loss=0.5665, val_wg=0.9524', '\\n', 'Epoch 3:\nval_loss=0.5633, val_wg=0.9563', '\\n', 'Epoch 4: val_loss=0.5619,\nval_wg=0.9563', '\\n', 'Epoch 5: val_loss=0.5608, val_wg=0.9603', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0396, val_wg=0.9879',\n'\\n', 'Epoch 1: val_loss=0.0512, val_wg=0.9919', '\\n', 'Epoch 2:\nval_loss=0.0645, val_wg=0.9879', '\\n', 'Epoch 3: val_loss=0.0760,\nval_wg=0.9879', '\\n', 'Epoch 4: val_loss=0.0873, val_wg=0.9839', '\\n', 'Epoch 5:\nval_loss=0.0946, val_wg=0.9798', '\\n', '\\n=== Dataset: corr_75_d10 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6938,\nval_wg=0.2846', '\\n', 'Epoch 1: val_loss=0.6864, val_wg=0.2885', '\\n', 'Epoch 2:\nval_loss=0.6847, val_wg=0.2885', '\\n', 'Epoch 3: val_loss=0.6843,\nval_wg=0.2885', '\\n', 'Epoch 4: val_loss=0.6842, val_wg=0.2885', '\\n', 'Epoch 5:\nval_loss=0.6841, val_wg=0.2885', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5695, val_wg=0.9881', '\\n', 'Epoch 1: val_loss=0.4987,\nval_wg=0.9960', '\\n', 'Epoch 2: val_loss=0.4823, val_wg=0.9960', '\\n', 'Epoch 3:\nval_loss=0.4780, val_wg=0.9960', '\\n', 'Epoch 4: val_loss=0.4762,\nval_wg=0.9960', '\\n', 'Epoch 5: val_loss=0.4749, val_wg=0.9960', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0010, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0000,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0000, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_75_d15 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6958,\nval_wg=0.2367', '\\n', 'Epoch 1: val_loss=0.6891, val_wg=0.2449', '\\n', 'Epoch 2:\nval_loss=0.6876, val_wg=0.2449', '\\n', 'Epoch 3: val_loss=0.6872,\nval_wg=0.2449', '\\n', 'Epoch 4: val_loss=0.6871, val_wg=0.2449', '\\n', 'Epoch 5:\nval_loss=0.6870, val_wg=0.2449', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.4650, val_wg=0.9878', '\\n', 'Epoch 1: val_loss=0.3608,\nval_wg=1.0000', '\\n', 'Epoch 2: val_loss=0.3378, val_wg=1.0000', '\\n', 'Epoch 3:\nval_loss=0.3321, val_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.3299,\nval_wg=1.0000', '\\n', 'Epoch 5: val_loss=0.3285, val_wg=1.0000', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0000, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0000,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0000, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_95_d5 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6725,\nval_wg=0.0685', '\\n', 'Epoch 1: val_loss=0.6664, val_wg=0.0685', '\\n', 'Epoch 2:\nval_loss=0.6650, val_wg=0.0685', '\\n', 'Epoch 3: val_loss=0.6647,\nval_wg=0.0685', '\\n', 'Epoch 4: val_loss=0.6645, val_wg=0.0685', '\\n', 'Epoch 5:\nval_loss=0.6645, val_wg=0.0685', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.6363, val_wg=0.8452', '\\n', 'Epoch 1: val_loss=0.5836,\nval_wg=0.9839', '\\n', 'Epoch 2: val_loss=0.5712, val_wg=0.9758', '\\n', 'Epoch 3:\nval_loss=0.5677, val_wg=0.9798', '\\n', 'Epoch 4: val_loss=0.5662,\nval_wg=0.9758', '\\n', 'Epoch 5: val_loss=0.5649, val_wg=0.9798', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0289, val_wg=0.9879',\n'\\n', 'Epoch 1: val_loss=0.0361, val_wg=0.9879', '\\n', 'Epoch 2:\nval_loss=0.0331, val_wg=0.9919', '\\n', 'Epoch 3: val_loss=0.0276,\nval_wg=0.9879', '\\n', 'Epoch 4: val_loss=0.0273, val_wg=0.9881', '\\n', 'Epoch 5:\nval_loss=0.0294, val_wg=0.9881', '\\n', '\\n=== Dataset: corr_95_d10 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6867,\nval_wg=0.2558', '\\n', 'Epoch 1: val_loss=0.6782, val_wg=0.3411', '\\n', 'Epoch 2:\nval_loss=0.6762, val_wg=0.3605', '\\n', 'Epoch 3: val_loss=0.6757,\nval_wg=0.3605', '\\n', 'Epoch 4: val_loss=0.6756, val_wg=0.3682', '\\n', 'Epoch 5:\nval_loss=0.6755, val_wg=0.3682', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5459, val_wg=0.5124', '\\n', 'Epoch 1: val_loss=0.4702,\nval_wg=0.7769', '\\n', 'Epoch 2: val_loss=0.4526, val_wg=0.8223', '\\n', 'Epoch 3:\nval_loss=0.4480, val_wg=0.8347', '\\n', 'Epoch 4: val_loss=0.4462,\nval_wg=0.8388', '\\n', 'Epoch 5: val_loss=0.4449, val_wg=0.8388', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0005, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0001, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0001, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0001,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0001, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0001, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_95_d15 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6977,\nval_wg=0.1774', '\\n', 'Epoch 1: val_loss=0.6896, val_wg=0.2075', '\\n', 'Epoch 2:\nval_loss=0.6878, val_wg=0.2226', '\\n', 'Epoch 3: val_loss=0.6874,\nval_wg=0.2302', '\\n', 'Epoch 4: val_loss=0.6872, val_wg=0.2302', '\\n', 'Epoch 5:\nval_loss=0.6871, val_wg=0.2340', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.6064, val_wg=0.1321', '\\n', 'Epoch 1: val_loss=0.5193,\nval_wg=0.7547', '\\n', 'Epoch 2: val_loss=0.4990, val_wg=0.8415', '\\n', 'Epoch 3:\nval_loss=0.4935, val_wg=0.8604', '\\n', 'Epoch 4: val_loss=0.4912,\nval_wg=0.8679', '\\n', 'Epoch 5: val_loss=0.4894, val_wg=0.8755', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0005, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0000,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0000, val_wg=1.0000', '\\n', '\\nSaved experiment_data.npy to\n/data/chenhui/AI-Scientist-v2/experiments/2025-06-06_23-36-\n12_gradient_cluster_robust_attempt_0/0-run/process_ForkProcess-13/working',\n'\\n', 'Execution time: a minute seconds (time limit is an hour).']", "['\\n=== NO_CLUSTER_REWEIGHTING, lr = 0.0001 ===', '\\n', ' epoch=0\ntrain_wg=0.3023 val_wg=0.3168', '\\n', ' epoch=1 train_wg=0.3768 val_wg=0.3702',\n'\\n', ' epoch=2 train_wg=0.5052 val_wg=0.5038', '\\n', ' epoch=3 train_wg=0.6770\nval_wg=0.6679', '\\n', ' epoch=4 train_wg=0.8054 val_wg=0.8015', '\\n', ' epoch=5\ntrain_wg=0.8965 val_wg=0.8817', '\\n', '\\n=== NO_CLUSTER_REWEIGHTING, lr = 0.001\n===', '\\n', ' epoch=0 train_wg=0.9337 val_wg=0.9160', '\\n', ' epoch=1\ntrain_wg=0.9814 val_wg=0.9656', '\\n', ' epoch=2 train_wg=0.9923 val_wg=0.9809',\n'\\n', ' epoch=3 train_wg=0.9923 val_wg=0.9874', '\\n', ' epoch=4 train_wg=0.9903\nval_wg=0.9885', '\\n', ' epoch=5 train_wg=0.9903 val_wg=0.9885', '\\n', '\\n===\nNO_CLUSTER_REWEIGHTING, lr = 0.01 ===', '\\n', ' epoch=0 train_wg=0.9923\nval_wg=0.9847', '\\n', ' epoch=1 train_wg=0.9961 val_wg=0.9847', '\\n', ' epoch=2\ntrain_wg=0.9942 val_wg=0.9847', '\\n', ' epoch=3 train_wg=0.9981 val_wg=0.9885',\n'\\n', ' epoch=4 train_wg=0.9942 val_wg=0.9885', '\\n', ' epoch=5 train_wg=0.9961\nval_wg=0.9885', '\\n', 'Saved ablation results to', ' ', '/data/chenhui/AI-Scient\nist-v2/experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/0-\nrun/process_ForkProcess-14/working/experiment_data.npy', '\\n', 'Execution time:\n2 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Training with learning rate = 0.0001 ===',\n'\\n', 'lr=0.0001 epoch=0: val_loss=0.6756, val_wg=0.0496', '\\n', 'lr=0.0001\nepoch=1: val_loss=0.6689, val_wg=0.0496', '\\n', 'lr=0.0001 epoch=2:\nval_loss=0.6674, val_wg=0.0496', '\\n', 'lr=0.0001 epoch=3: val_loss=0.6670,\nval_wg=0.0496', '\\n', 'lr=0.0001 epoch=4: val_loss=0.6669, val_wg=0.0496', '\\n',\n'lr=0.0001 epoch=5: val_loss=0.6668, val_wg=0.0496', '\\n', '\\n=== Training with\nlearning rate = 0.001 ===', '\\n', 'lr=0.001 epoch=0: val_loss=0.5672,\nval_wg=0.9538', '\\n', 'lr=0.001 epoch=1: val_loss=0.5018, val_wg=0.9538', '\\n',\n'lr=0.001 epoch=2: val_loss=0.4866, val_wg=0.9538', '\\n', 'lr=0.001 epoch=3:\nval_loss=0.4826, val_wg=0.9538', '\\n', 'lr=0.001 epoch=4: val_loss=0.4811,\nval_wg=0.9538', '\\n', 'lr=0.001 epoch=5: val_loss=0.4799, val_wg=0.9538', '\\n',\n'\\n=== Training with learning rate = 0.01 ===', '\\n', 'lr=0.01 epoch=0:\nval_loss=0.0336, val_wg=0.9885', '\\n', 'lr=0.01 epoch=1: val_loss=0.0640,\nval_wg=0.9809', '\\n', 'lr=0.01 epoch=2: val_loss=0.0828, val_wg=0.9771', '\\n',\n'lr=0.01 epoch=3: val_loss=0.0964, val_wg=0.9695', '\\n', 'lr=0.01 epoch=4:\nval_loss=0.1029, val_wg=0.9656', '\\n', 'lr=0.01 epoch=5: val_loss=0.1164,\nval_wg=0.9580', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 8\nseconds seconds (time limit is an hour).']", "['Execution time: 4 seconds seconds (time limit is an hour).']", "['\\n=== LR=0.0001 ===', '\\n', 'Epoch 0: val_loss=0.6977, val_wg=0.3168', '\\n',\n'Epoch 1: val_loss=0.6917, val_wg=0.3511', '\\n', 'Epoch 2: val_loss=0.6904,\nval_wg=0.3511', '\\n', 'Epoch 3: val_loss=0.6900, val_wg=0.3588', '\\n', 'Epoch 4:\nval_loss=0.6899, val_wg=0.3588', '\\n', 'Epoch 5: val_loss=0.6898,\nval_wg=0.3588', '\\n', '\\n=== LR=0.001 ===', '\\n', 'Epoch 0: val_loss=0.5719,\nval_wg=0.9160', '\\n', 'Epoch 1: val_loss=0.4982, val_wg=0.9580', '\\n', 'Epoch 2:\nval_loss=0.4813, val_wg=0.9656', '\\n', 'Epoch 3: val_loss=0.4769,\nval_wg=0.9656', '\\n', 'Epoch 4: val_loss=0.4751, val_wg=0.9656', '\\n', 'Epoch 5:\nval_loss=0.4738, val_wg=0.9656', '\\n', '\\n=== LR=0.01 ===', '\\n', 'Epoch 0:\nval_loss=0.0288, val_wg=0.9847', '\\n', 'Epoch 1: val_loss=0.0330,\nval_wg=0.9885', '\\n', 'Epoch 2: val_loss=0.0376, val_wg=0.9885', '\\n', 'Epoch 3:\nval_loss=0.0386, val_wg=0.9885', '\\n', 'Epoch 4: val_loss=0.0387,\nval_wg=0.9885', '\\n', 'Epoch 5: val_loss=0.0387, val_wg=0.9885', '\\n',\n'Execution time: 2 seconds seconds (time limit is an hour).']", "['lr=0.0001 epoch=0: val_loss=0.6977, val_wg=0.3168', '\\n', 'lr=0.0001 epoch=1:\nval_loss=0.6917, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=2: val_loss=0.6904,\nval_wg=0.3511', '\\n', 'lr=0.0001 epoch=3: val_loss=0.6900, val_wg=0.3588', '\\n',\n'lr=0.0001 epoch=4: val_loss=0.6899, val_wg=0.3588', '\\n', 'lr=0.0001 epoch=5:\nval_loss=0.6898, val_wg=0.3588', '\\n', 'lr=0.001 epoch=0: val_loss=0.5822,\nval_wg=0.8866', '\\n', 'lr=0.001 epoch=1: val_loss=0.5093, val_wg=0.9538', '\\n',\n'lr=0.001 epoch=2: val_loss=0.4929, val_wg=0.9622', '\\n', 'lr=0.001 epoch=3:\nval_loss=0.4887, val_wg=0.9622', '\\n', 'lr=0.001 epoch=4: val_loss=0.4871,\nval_wg=0.9622', '\\n', 'lr=0.001 epoch=5: val_loss=0.4860, val_wg=0.9622', '\\n',\n'lr=0.01 epoch=0: val_loss=0.0228, val_wg=0.9885', '\\n', 'lr=0.01 epoch=1:\nval_loss=0.0303, val_wg=0.9885', '\\n', 'lr=0.01 epoch=2: val_loss=0.0340,\nval_wg=0.9885', '\\n', 'lr=0.01 epoch=3: val_loss=0.0349, val_wg=0.9885', '\\n',\n'lr=0.01 epoch=4: val_loss=0.0350, val_wg=0.9885', '\\n', 'lr=0.01 epoch=5:\nval_loss=0.0351, val_wg=0.9885', '\\n', 'Execution time: 3 seconds seconds (time\nlimit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Training with learning rate = 0.0001 ===',\n'\\n', 'lr=0.0001 epoch=0: val_loss=0.6977, val_wg=0.3168', '\\n', 'lr=0.0001\nepoch=1: val_loss=0.6917, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=2:\nval_loss=0.6904, val_wg=0.3511', '\\n', 'lr=0.0001 epoch=3: val_loss=0.6900,\nval_wg=0.3588', '\\n', 'lr=0.0001 epoch=4: val_loss=0.6899, val_wg=0.3588', '\\n',\n'lr=0.0001 epoch=5: val_loss=0.6898, val_wg=0.3626', '\\n', '\\n=== Training with\nlearning rate = 0.001 ===', '\\n', 'lr=0.001 epoch=0: val_loss=0.5719,\nval_wg=0.9160', '\\n', 'lr=0.001 epoch=1: val_loss=0.4982, val_wg=0.9580', '\\n',\n'lr=0.001 epoch=2: val_loss=0.4813, val_wg=0.9656', '\\n', 'lr=0.001 epoch=3:\nval_loss=0.4769, val_wg=0.9656', '\\n', 'lr=0.001 epoch=4: val_loss=0.4751,\nval_wg=0.9656', '\\n', 'lr=0.001 epoch=5: val_loss=0.4738, val_wg=0.9656', '\\n',\n'\\n=== Training with learning rate = 0.01 ===', '\\n', 'lr=0.01 epoch=0:\nval_loss=0.0288, val_wg=0.9847', '\\n', 'lr=0.01 epoch=1: val_loss=0.0330,\nval_wg=0.9885', '\\n', 'lr=0.01 epoch=2: val_loss=0.0376, val_wg=0.9885', '\\n',\n'lr=0.01 epoch=3: val_loss=0.0386, val_wg=0.9885', '\\n', 'lr=0.01 epoch=4:\nval_loss=0.0388, val_wg=0.9885', '\\n', 'lr=0.01 epoch=5: val_loss=0.0387,\nval_wg=0.9885', '\\n', 'Execution time: 2 seconds seconds (time limit is an\nhour).']", "['\\n=== lr=0.0001 ===', '\\n', ' ep=0: val_loss=0.6977, val_wg=0.3168', '\\n', '\nep=1: val_loss=0.6917, val_wg=0.3511', '\\n', ' ep=2: val_loss=0.6904,\nval_wg=0.3511', '\\n', ' ep=3: val_loss=0.6900, val_wg=0.3588', '\\n', ' ep=4:\nval_loss=0.6899, val_wg=0.3588', '\\n', ' ep=5: val_loss=0.6898, val_wg=0.3588',\n'\\n', '\\n=== lr=0.001 ===', '\\n', ' ep=0: val_loss=0.5719, val_wg=0.9160', '\\n',\n' ep=1: val_loss=0.4982, val_wg=0.9580', '\\n', ' ep=2: val_loss=0.4813,\nval_wg=0.9656', '\\n', ' ep=3: val_loss=0.4769, val_wg=0.9656', '\\n', ' ep=4:\nval_loss=0.4751, val_wg=0.9656', '\\n', ' ep=5: val_loss=0.4738, val_wg=0.9656',\n'\\n', 'Traceback (most recent call last):\\n  File \"runfile.py\", line 170, in\n<module>\\n    if not sd[\"ground_truth\"]:\\nValueError: The truth value of an\narray with more than one element is ambiguous. Use a.any() or a.all()\\n',\n'Execution time: 2 seconds seconds (time limit is an hour).']", "['Using device:', ' ', 'cuda', '\\n', '\\n=== SGD lr=0.0001 ===', '\\n', ' epoch=0:\nval_wg=0.2443, val_loss=0.7098', '\\n', ' epoch=1: val_wg=0.2443,\nval_loss=0.7098', '\\n', ' epoch=2: val_wg=0.2443, val_loss=0.7098', '\\n', '\nepoch=3: val_wg=0.2443, val_loss=0.7098', '\\n', ' epoch=4: val_wg=0.2443,\nval_loss=0.7098', '\\n', ' epoch=5: val_wg=0.2443, val_loss=0.7098', '\\n', '\\n===\nSGD lr=0.001 ===', '\\n', ' epoch=0: val_wg=0.0462, val_loss=0.7324', '\\n', '\nepoch=1: val_wg=0.0462, val_loss=0.7324', '\\n', ' epoch=2: val_wg=0.0462,\nval_loss=0.7324', '\\n', ' epoch=3: val_wg=0.0462, val_loss=0.7323', '\\n', '\nepoch=4: val_wg=0.0462, val_loss=0.7323', '\\n', ' epoch=5: val_wg=0.0462,\nval_loss=0.7323', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 214, in <module>\\n    if not\nexperiment_data[\"SGD_OPTIMIZER\"][\"synthetic\"][\"ground_truth\"]:\\nValueError: The\ntruth value of an array with more than one element is ambiguous. Use a.any() or\na.all()\\n', 'Execution time: 6 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== lr=0.0001 ===', '\\n', 'Epoch 0:\nvalidation_loss = 0.6977, worst_group_acc = 0.3168', '\\n', 'Epoch 1:\nvalidation_loss = 0.6917, worst_group_acc = 0.3511', '\\n', 'Epoch 2:\nvalidation_loss = 0.6904, worst_group_acc = 0.3511', '\\n', 'Epoch 3:\nvalidation_loss = 0.6900, worst_group_acc = 0.3588', '\\n', 'Epoch 4:\nvalidation_loss = 0.6899, worst_group_acc = 0.3588', '\\n', 'Epoch 5:\nvalidation_loss = 0.6898, worst_group_acc = 0.3588', '\\n', '\\n=== lr=0.001 ===',\n'\\n', 'Epoch 0: validation_loss = 0.5719, worst_group_acc = 0.9160', '\\n',\n'Epoch 1: validation_loss = 0.4982, worst_group_acc = 0.9580', '\\n', 'Epoch 2:\nvalidation_loss = 0.4813, worst_group_acc = 0.9656', '\\n', 'Epoch 3:\nvalidation_loss = 0.4769, worst_group_acc = 0.9656', '\\n', 'Epoch 4:\nvalidation_loss = 0.4751, worst_group_acc = 0.9656', '\\n', 'Epoch 5:\nvalidation_loss = 0.4738, worst_group_acc = 0.9656', '\\n', '\\n=== lr=0.01 ===',\n'\\n', 'Epoch 0: validation_loss = 0.0288, worst_group_acc = 0.9847', '\\n',\n'Epoch 1: validation_loss = 0.0330, worst_group_acc = 0.9885', '\\n', 'Epoch 2:\nvalidation_loss = 0.0376, worst_group_acc = 0.9885', '\\n', 'Epoch 3:\nvalidation_loss = 0.0386, worst_group_acc = 0.9885', '\\n', 'Epoch 4:\nvalidation_loss = 0.0387, worst_group_acc = 0.9885', '\\n', 'Epoch 5:\nvalidation_loss = 0.0387, worst_group_acc = 0.9885', '\\n', 'Execution time: 2\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Training with learning rate = 0.0001 ===',\n'\\n', 'lr=0.0001 epoch=0: val_loss=0.6975, val_wg=0.3130', '\\n', 'lr=0.0001\nepoch=1: val_loss=0.6852, val_wg=0.3740', '\\n', 'lr=0.0001 epoch=2:\nval_loss=0.6731, val_wg=0.5115', '\\n', 'lr=0.0001 epoch=3: val_loss=0.6609,\nval_wg=0.6832', '\\n', 'lr=0.0001 epoch=4: val_loss=0.6485, val_wg=0.8092', '\\n',\n'\\n=== Training with learning rate = 0.001 ===', '\\n', 'lr=0.001 epoch=0:\nval_loss=0.5385, val_wg=0.8817', '\\n', 'lr=0.001 epoch=1: val_loss=0.3647,\nval_wg=0.9618', '\\n', 'lr=0.001 epoch=2: val_loss=0.2114, val_wg=0.9695', '\\n',\n'lr=0.001 epoch=3: val_loss=0.1182, val_wg=0.9771', '\\n', 'lr=0.001 epoch=4:\nval_loss=0.0728, val_wg=0.9771', '\\n', '\\n=== Training with learning rate = 0.01\n===', '\\n', 'lr=0.01 epoch=0: val_loss=0.0235, val_wg=0.9885', '\\n', 'lr=0.01\nepoch=1: val_loss=0.0299, val_wg=0.9847', '\\n', 'lr=0.01 epoch=2:\nval_loss=0.0346, val_wg=0.9847', '\\n', 'lr=0.01 epoch=3: val_loss=0.0351,\nval_wg=0.9847', '\\n', 'lr=0.01 epoch=4: val_loss=0.0336, val_wg=0.9847', '\\n',\n'Execution time: 8 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== lr=0.0001 ===', '\\n', 'Epoch 0:\nvalidation_loss = 0.6977, Worst-Group Accuracy = 0.3168', '\\n', 'Epoch 1:\nvalidation_loss = 0.6917, Worst-Group Accuracy = 0.3511', '\\n', 'Epoch 2:\nvalidation_loss = 0.6904, Worst-Group Accuracy = 0.3511', '\\n', 'Epoch 3:\nvalidation_loss = 0.6900, Worst-Group Accuracy = 0.3588', '\\n', 'Epoch 4:\nvalidation_loss = 0.6899, Worst-Group Accuracy = 0.3588', '\\n', 'Epoch 5:\nvalidation_loss = 0.6898, Worst-Group Accuracy = 0.3588', '\\n', '\\n=== lr=0.001\n===', '\\n', 'Epoch 0: validation_loss = 0.5719, Worst-Group Accuracy = 0.9160',\n'\\n', 'Epoch 1: validation_loss = 0.4982, Worst-Group Accuracy = 0.9580', '\\n',\n'Epoch 2: validation_loss = 0.4813, Worst-Group Accuracy = 0.9656', '\\n', 'Epoch\n3: validation_loss = 0.4769, Worst-Group Accuracy = 0.9656', '\\n', 'Epoch 4:\nvalidation_loss = 0.4751, Worst-Group Accuracy = 0.9656', '\\n', 'Epoch 5:\nvalidation_loss = 0.4738, Worst-Group Accuracy = 0.9656', '\\n', '\\n=== lr=0.01\n===', '\\n', 'Epoch 0: validation_loss = 0.0288, Worst-Group Accuracy = 0.9847',\n'\\n', 'Epoch 1: validation_loss = 0.0330, Worst-Group Accuracy = 0.9885', '\\n',\n'Epoch 2: validation_loss = 0.0376, Worst-Group Accuracy = 0.9885', '\\n', 'Epoch\n3: validation_loss = 0.0386, Worst-Group Accuracy = 0.9885', '\\n', 'Epoch 4:\nvalidation_loss = 0.0387, Worst-Group Accuracy = 0.9885', '\\n', 'Epoch 5:\nvalidation_loss = 0.0387, Worst-Group Accuracy = 0.9885', '\\n', 'Execution time:\n2 seconds seconds (time limit is an hour).']", "['Using device:', ' ', 'cuda', '\\n', '\\n=== SGD lr=0.0001 ===', '\\n', 'Epoch 0:\nvalidation_loss = 0.7098', '\\n', 'Epoch 1: validation_loss = 0.7098', '\\n',\n'Epoch 2: validation_loss = 0.7098', '\\n', 'Epoch 3: validation_loss = 0.7098',\n'\\n', 'Epoch 4: validation_loss = 0.7098', '\\n', 'Epoch 5: validation_loss =\n0.7098', '\\n', '\\n=== SGD lr=0.001 ===', '\\n', 'Epoch 0: validation_loss =\n0.7324', '\\n', 'Epoch 1: validation_loss = 0.7324', '\\n', 'Epoch 2:\nvalidation_loss = 0.7324', '\\n', 'Epoch 3: validation_loss = 0.7323', '\\n',\n'Epoch 4: validation_loss = 0.7323', '\\n', 'Epoch 5: validation_loss = 0.7323',\n'\\n', '\\n=== SGD lr=0.01 ===', '\\n', 'Epoch 0: validation_loss = 0.6372', '\\n',\n'Epoch 1: validation_loss = 0.6372', '\\n', 'Epoch 2: validation_loss = 0.6371',\n'\\n', 'Epoch 3: validation_loss = 0.6370', '\\n', 'Epoch 4: validation_loss =\n0.6370', '\\n', 'Epoch 5: validation_loss = 0.6369', '\\n', '\\nSaved\nexperiment_data.npy', '\\n', 'Execution time: 8 seconds seconds (time limit is an\nhour).']", "['Execution time: 29 seconds seconds (time limit is an hour).']", "['\\n=== Training with weight_decay = 0.0 ===', '\\n', 'wd=0.0 epoch=0:\nval_loss=0.5892, val_wg=0.9847', '\\n', 'wd=0.0 epoch=1: val_loss=0.5257,\nval_wg=0.9885', '\\n', 'wd=0.0 epoch=2: val_loss=0.5109, val_wg=0.9885', '\\n',\n'wd=0.0 epoch=3: val_loss=0.5070, val_wg=0.9885', '\\n', 'wd=0.0 epoch=4:\nval_loss=0.5055, val_wg=0.9885', '\\n', 'wd=0.0 epoch=5: val_loss=0.5044,\nval_wg=0.9885', '\\n', '\\n=== Training with weight_decay = 0.0001 ===', '\\n',\n'wd=0.0001 epoch=0: val_loss=0.5822, val_wg=0.8866', '\\n', 'wd=0.0001 epoch=1:\nval_loss=0.5094, val_wg=0.9538', '\\n', 'wd=0.0001 epoch=2: val_loss=0.4931,\nval_wg=0.9622', '\\n', 'wd=0.0001 epoch=3: val_loss=0.4890, val_wg=0.9622', '\\n',\n'wd=0.0001 epoch=4: val_loss=0.4876, val_wg=0.9622', '\\n', 'wd=0.0001 epoch=5:\nval_loss=0.4867, val_wg=0.9622', '\\n', '\\n=== Training with weight_decay = 0.001\n===', '\\n', 'wd=0.001 epoch=0: val_loss=0.5463, val_wg=0.9809', '\\n', 'wd=0.001\nepoch=1: val_loss=0.4824, val_wg=0.9847', '\\n', 'wd=0.001 epoch=2:\nval_loss=0.4685, val_wg=0.9809', '\\n', 'wd=0.001 epoch=3: val_loss=0.4660,\nval_wg=0.9809', '\\n', 'wd=0.001 epoch=4: val_loss=0.4660, val_wg=0.9809', '\\n',\n'wd=0.001 epoch=5: val_loss=0.4666, val_wg=0.9809', '\\n', 'Execution time: 8\nseconds seconds (time limit is an hour).']", "['\\nLR=0.0001', '\\n', 'Epoch 0: val_loss=0.6977, val_wg=0.3168', '\\n', 'Epoch 1:\nval_loss=0.6917, val_wg=0.3511', '\\n', 'Epoch 2: val_loss=0.6904,\nval_wg=0.3511', '\\n', 'Epoch 3: val_loss=0.6901, val_wg=0.3588', '\\n', 'Epoch 4:\nval_loss=0.6900, val_wg=0.3588', '\\n', 'Epoch 5: val_loss=0.6900,\nval_wg=0.3588', '\\n', '\\nLR=0.001', '\\n', 'Epoch 0: val_loss=0.5528,\nval_wg=0.9664', '\\n', 'Epoch 1: val_loss=0.4798, val_wg=0.9790', '\\n', 'Epoch 2:\nval_loss=0.4628, val_wg=0.9874', '\\n', 'Epoch 3: val_loss=0.4587,\nval_wg=0.9874', '\\n', 'Epoch 4: val_loss=0.4574, val_wg=0.9874', '\\n', 'Epoch 5:\nval_loss=0.4566, val_wg=0.9874', '\\n', '\\nLR=0.01', '\\n', 'Epoch 0:\nval_loss=0.0284, val_wg=0.9885', '\\n', 'Epoch 1: val_loss=0.0365,\nval_wg=0.9916', '\\n', 'Epoch 2: val_loss=0.0421, val_wg=0.9916', '\\n', 'Epoch 3:\nval_loss=0.0441, val_wg=0.9916', '\\n', 'Epoch 4: val_loss=0.0437,\nval_wg=0.9916', '\\n', 'Epoch 5: val_loss=0.0432, val_wg=0.9916', '\\n',\n'Execution time: 2 seconds seconds (time limit is an hour).']", "['\\n=== Raw clustering, lr=0.0001 ===', '\\n', ' epoch=0: val_loss=0.6977,\nval_wg=0.3168', '\\n', ' epoch=1: val_loss=0.6917, val_wg=0.3511', '\\n', '\nepoch=2: val_loss=0.6904, val_wg=0.3511', '\\n', ' epoch=3: val_loss=0.6900,\nval_wg=0.3588', '\\n', ' epoch=4: val_loss=0.6899, val_wg=0.3588', '\\n', '\nepoch=5: val_loss=0.6898, val_wg=0.3588', '\\n', '\\n=== Raw clustering, lr=0.001\n===', '\\n', ' epoch=0: val_loss=0.5822, val_wg=0.8866', '\\n', ' epoch=1:\nval_loss=0.5093, val_wg=0.9538', '\\n', ' epoch=2: val_loss=0.4929,\nval_wg=0.9622', '\\n', ' epoch=3: val_loss=0.4887, val_wg=0.9622', '\\n', '\nepoch=4: val_loss=0.4872, val_wg=0.9622', '\\n', ' epoch=5: val_loss=0.4860,\nval_wg=0.9622', '\\n', '\\n=== Raw clustering, lr=0.01 ===', '\\n', ' epoch=0:\nval_loss=0.0228, val_wg=0.9885', '\\n', ' epoch=1: val_loss=0.0305,\nval_wg=0.9885', '\\n', ' epoch=2: val_loss=0.0348, val_wg=0.9885', '\\n', '\nepoch=3: val_loss=0.0380, val_wg=0.9885', '\\n', ' epoch=4: val_loss=0.0405,\nval_wg=0.9885', '\\n', ' epoch=5: val_loss=0.0430, val_wg=0.9924', '\\n', '\\n===\nNormalized clustering, lr=0.0001 ===', '\\n', ' epoch=0: val_loss=0.6982,\nval_wg=0.0672', '\\n', ' epoch=1: val_loss=0.6929, val_wg=0.0672', '\\n', '\nepoch=2: val_loss=0.6917, val_wg=0.0672', '\\n', ' epoch=3: val_loss=0.6914,\nval_wg=0.0672', '\\n', ' epoch=4: val_loss=0.6913, val_wg=0.0672', '\\n', '\nepoch=5: val_loss=0.6912, val_wg=0.0672', '\\n', '\\n=== Normalized clustering,\nlr=0.001 ===', '\\n', ' epoch=0: val_loss=0.5812, val_wg=0.8824', '\\n', '\nepoch=1: val_loss=0.5141, val_wg=0.9496', '\\n', ' epoch=2: val_loss=0.4983,\nval_wg=0.9496', '\\n', ' epoch=3: val_loss=0.4941, val_wg=0.9496', '\\n', '\nepoch=4: val_loss=0.4924, val_wg=0.9496', '\\n', ' epoch=5: val_loss=0.4911,\nval_wg=0.9496', '\\n', '\\n=== Normalized clustering, lr=0.01 ===', '\\n', '\nepoch=0: val_loss=0.0261, val_wg=0.9916', '\\n', ' epoch=1: val_loss=0.0265,\nval_wg=0.9847', '\\n', ' epoch=2: val_loss=0.0288, val_wg=0.9847', '\\n', '\nepoch=3: val_loss=0.0293, val_wg=0.9847', '\\n', ' epoch=4: val_loss=0.0294,\nval_wg=0.9847', '\\n', ' epoch=5: val_loss=0.0294, val_wg=0.9847', '\\n', '\\nSaved\nexperiment data to /data/chenhui/AI-Scientist-v2/experiments/2025-06-06_23-36-\n12_gradient_cluster_robust_attempt_0/0-run/process_ForkProcess-\n15/working/experiment_data.npy', '\\n', 'Execution time: 15 seconds seconds (time\nlimit is an hour).']", "['\\n=== synthetic_with_norm lr=0.0001 ===', '\\n', 'synthetic_with_norm lr=0.0001\nepoch=0: val_loss=0.6977, val_wg=0.3168', '\\n', 'synthetic_with_norm lr=0.0001\nepoch=1: val_loss=0.6917, val_wg=0.3511', '\\n', 'synthetic_with_norm lr=0.0001\nepoch=2: val_loss=0.6904, val_wg=0.3511', '\\n', 'synthetic_with_norm lr=0.0001\nepoch=3: val_loss=0.6900, val_wg=0.3588', '\\n', 'synthetic_with_norm lr=0.0001\nepoch=4: val_loss=0.6899, val_wg=0.3588', '\\n', 'synthetic_with_norm lr=0.0001\nepoch=5: val_loss=0.6898, val_wg=0.3588', '\\n', '\\n=== synthetic_with_norm\nlr=0.001 ===', '\\n', 'synthetic_with_norm lr=0.001 epoch=0: val_loss=0.5822,\nval_wg=0.8866', '\\n', 'synthetic_with_norm lr=0.001 epoch=1: val_loss=0.5093,\nval_wg=0.9538', '\\n', 'synthetic_with_norm lr=0.001 epoch=2: val_loss=0.4929,\nval_wg=0.9622', '\\n', 'synthetic_with_norm lr=0.001 epoch=3: val_loss=0.4887,\nval_wg=0.9622', '\\n', 'synthetic_with_norm lr=0.001 epoch=4: val_loss=0.4872,\nval_wg=0.9622', '\\n', 'synthetic_with_norm lr=0.001 epoch=5: val_loss=0.4860,\nval_wg=0.9622', '\\n', '\\n=== synthetic_with_norm lr=0.01 ===', '\\n',\n'synthetic_with_norm lr=0.01 epoch=0: val_loss=0.0228, val_wg=0.9885', '\\n',\n'synthetic_with_norm lr=0.01 epoch=1: val_loss=0.0305, val_wg=0.9885', '\\n',\n'synthetic_with_norm lr=0.01 epoch=2: val_loss=0.0348, val_wg=0.9885', '\\n',\n'synthetic_with_norm lr=0.01 epoch=3: val_loss=0.0380, val_wg=0.9885', '\\n',\n'synthetic_with_norm lr=0.01 epoch=4: val_loss=0.0405, val_wg=0.9885', '\\n',\n'synthetic_with_norm lr=0.01 epoch=5: val_loss=0.0430, val_wg=0.9924', '\\n',\n'\\n=== synthetic_no_norm lr=0.0001 ===', '\\n', 'synthetic_no_norm lr=0.0001\nepoch=0: val_loss=0.7114, val_wg=0.0672', '\\n', 'synthetic_no_norm lr=0.0001\nepoch=1: val_loss=0.7044, val_wg=0.0672', '\\n', 'synthetic_no_norm lr=0.0001\nepoch=2: val_loss=0.7028, val_wg=0.0672', '\\n', 'synthetic_no_norm lr=0.0001\nepoch=3: val_loss=0.7025, val_wg=0.0672', '\\n', 'synthetic_no_norm lr=0.0001\nepoch=4: val_loss=0.7023, val_wg=0.0672', '\\n', 'synthetic_no_norm lr=0.0001\nepoch=5: val_loss=0.7022, val_wg=0.0672', '\\n', '\\n=== synthetic_no_norm\nlr=0.001 ===', '\\n', 'synthetic_no_norm lr=0.001 epoch=0: val_loss=0.5285,\nval_wg=0.5504', '\\n', 'synthetic_no_norm lr=0.001 epoch=1: val_loss=0.4635,\nval_wg=0.6639', '\\n', 'synthetic_no_norm lr=0.001 epoch=2: val_loss=0.4498,\nval_wg=0.6765', '\\n', 'synthetic_no_norm lr=0.001 epoch=3: val_loss=0.4464,\nval_wg=0.6765', '\\n', 'synthetic_no_norm lr=0.001 epoch=4: val_loss=0.4451,\nval_wg=0.6765', '\\n', 'synthetic_no_norm lr=0.001 epoch=5: val_loss=0.4442,\nval_wg=0.6765', '\\n', '\\n=== synthetic_no_norm lr=0.01 ===', '\\n',\n'synthetic_no_norm lr=0.01 epoch=0: val_loss=0.0526, val_wg=0.9790', '\\n',\n'synthetic_no_norm lr=0.01 epoch=1: val_loss=0.0399, val_wg=0.9748', '\\n',\n'synthetic_no_norm lr=0.01 epoch=2: val_loss=0.0311, val_wg=0.9832', '\\n',\n'synthetic_no_norm lr=0.01 epoch=3: val_loss=0.0254, val_wg=0.9885', '\\n',\n'synthetic_no_norm lr=0.01 epoch=4: val_loss=0.0323, val_wg=0.9847', '\\n',\n'synthetic_no_norm lr=0.01 epoch=5: val_loss=0.0514, val_wg=0.9656', '\\n',\n'Execution time: 15 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Dataset: corr_50_d5 ===', '\\n', '\\n--\nTraining with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.7016, val_wg=0.3911',\n'\\n', 'Epoch 1: val_loss=0.6961, val_wg=0.4435', '\\n', 'Epoch 2:\nval_loss=0.6948, val_wg=0.4476', '\\n', 'Epoch 3: val_loss=0.6945,\nval_wg=0.4476', '\\n', 'Epoch 4: val_loss=0.6944, val_wg=0.4476', '\\n', 'Epoch 5:\nval_loss=0.6944, val_wg=0.4476', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5829, val_wg=0.9444', '\\n', 'Epoch 1: val_loss=0.5080,\nval_wg=0.9758', '\\n', 'Epoch 2: val_loss=0.4912, val_wg=0.9762', '\\n', 'Epoch 3:\nval_loss=0.4869, val_wg=0.9762', '\\n', 'Epoch 4: val_loss=0.4852,\nval_wg=0.9762', '\\n', 'Epoch 5: val_loss=0.4841, val_wg=0.9762', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0382, val_wg=0.9839',\n'\\n', 'Epoch 1: val_loss=0.0592, val_wg=0.9798', '\\n', 'Epoch 2:\nval_loss=0.0738, val_wg=0.9798', '\\n', 'Epoch 3: val_loss=0.0884,\nval_wg=0.9718', '\\n', 'Epoch 4: val_loss=0.1001, val_wg=0.9718', '\\n', 'Epoch 5:\nval_loss=0.1123, val_wg=0.9718', '\\n', '\\n=== Dataset: corr_50_d10 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6674,\nval_wg=0.5475', '\\n', 'Epoch 1: val_loss=0.6595, val_wg=0.5932', '\\n', 'Epoch 2:\nval_loss=0.6577, val_wg=0.6046', '\\n', 'Epoch 3: val_loss=0.6573,\nval_wg=0.6046', '\\n', 'Epoch 4: val_loss=0.6571, val_wg=0.6084', '\\n', 'Epoch 5:\nval_loss=0.6570, val_wg=0.6084', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5955, val_wg=0.7529', '\\n', 'Epoch 1: val_loss=0.5222,\nval_wg=0.9494', '\\n', 'Epoch 2: val_loss=0.5052, val_wg=0.9578', '\\n', 'Epoch 3:\nval_loss=0.5008, val_wg=0.9620', '\\n', 'Epoch 4: val_loss=0.4990,\nval_wg=0.9620', '\\n', 'Epoch 5: val_loss=0.4976, val_wg=0.9620', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0020, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0003, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0003, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0002,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0002, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0002, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_50_d15 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6998,\nval_wg=0.4961', '\\n', 'Epoch 1: val_loss=0.6917, val_wg=0.4961', '\\n', 'Epoch 2:\nval_loss=0.6898, val_wg=0.4961', '\\n', 'Epoch 3: val_loss=0.6894,\nval_wg=0.4961', '\\n', 'Epoch 4: val_loss=0.6892, val_wg=0.4961', '\\n', 'Epoch 5:\nval_loss=0.6891, val_wg=0.4961', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5586, val_wg=0.7561', '\\n', 'Epoch 1: val_loss=0.4617,\nval_wg=0.9797', '\\n', 'Epoch 2: val_loss=0.4385, val_wg=0.9878', '\\n', 'Epoch 3:\nval_loss=0.4324, val_wg=0.9878', '\\n', 'Epoch 4: val_loss=0.4298,\nval_wg=0.9878', '\\n', 'Epoch 5: val_loss=0.4279, val_wg=0.9919', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0001, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0000,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0000, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_75_d5 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6614,\nval_wg=0.5635', '\\n', 'Epoch 1: val_loss=0.6560, val_wg=0.5952', '\\n', 'Epoch 2:\nval_loss=0.6547, val_wg=0.6032', '\\n', 'Epoch 3: val_loss=0.6544,\nval_wg=0.6032', '\\n', 'Epoch 4: val_loss=0.6543, val_wg=0.6032', '\\n', 'Epoch 5:\nval_loss=0.6543, val_wg=0.6032', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.6262, val_wg=0.6492', '\\n', 'Epoch 1: val_loss=0.5783,\nval_wg=0.9484', '\\n', 'Epoch 2: val_loss=0.5665, val_wg=0.9524', '\\n', 'Epoch 3:\nval_loss=0.5633, val_wg=0.9563', '\\n', 'Epoch 4: val_loss=0.5619,\nval_wg=0.9563', '\\n', 'Epoch 5: val_loss=0.5608, val_wg=0.9603', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0396, val_wg=0.9879',\n'\\n', 'Epoch 1: val_loss=0.0512, val_wg=0.9919', '\\n', 'Epoch 2:\nval_loss=0.0645, val_wg=0.9879', '\\n', 'Epoch 3: val_loss=0.0760,\nval_wg=0.9879', '\\n', 'Epoch 4: val_loss=0.0873, val_wg=0.9839', '\\n', 'Epoch 5:\nval_loss=0.0946, val_wg=0.9798', '\\n', '\\n=== Dataset: corr_75_d10 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6938,\nval_wg=0.2846', '\\n', 'Epoch 1: val_loss=0.6864, val_wg=0.2885', '\\n', 'Epoch 2:\nval_loss=0.6847, val_wg=0.2885', '\\n', 'Epoch 3: val_loss=0.6843,\nval_wg=0.2885', '\\n', 'Epoch 4: val_loss=0.6842, val_wg=0.2885', '\\n', 'Epoch 5:\nval_loss=0.6841, val_wg=0.2885', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5695, val_wg=0.9881', '\\n', 'Epoch 1: val_loss=0.4987,\nval_wg=0.9960', '\\n', 'Epoch 2: val_loss=0.4823, val_wg=0.9960', '\\n', 'Epoch 3:\nval_loss=0.4780, val_wg=0.9960', '\\n', 'Epoch 4: val_loss=0.4762,\nval_wg=0.9960', '\\n', 'Epoch 5: val_loss=0.4749, val_wg=0.9960', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0010, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0000,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0000, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_75_d15 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6958,\nval_wg=0.2367', '\\n', 'Epoch 1: val_loss=0.6891, val_wg=0.2449', '\\n', 'Epoch 2:\nval_loss=0.6876, val_wg=0.2449', '\\n', 'Epoch 3: val_loss=0.6872,\nval_wg=0.2449', '\\n', 'Epoch 4: val_loss=0.6871, val_wg=0.2449', '\\n', 'Epoch 5:\nval_loss=0.6870, val_wg=0.2449', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.4650, val_wg=0.9878', '\\n', 'Epoch 1: val_loss=0.3608,\nval_wg=1.0000', '\\n', 'Epoch 2: val_loss=0.3378, val_wg=1.0000', '\\n', 'Epoch 3:\nval_loss=0.3321, val_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.3299,\nval_wg=1.0000', '\\n', 'Epoch 5: val_loss=0.3285, val_wg=1.0000', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0000, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0000,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0000, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_95_d5 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6725,\nval_wg=0.0685', '\\n', 'Epoch 1: val_loss=0.6664, val_wg=0.0685', '\\n', 'Epoch 2:\nval_loss=0.6650, val_wg=0.0685', '\\n', 'Epoch 3: val_loss=0.6647,\nval_wg=0.0685', '\\n', 'Epoch 4: val_loss=0.6645, val_wg=0.0685', '\\n', 'Epoch 5:\nval_loss=0.6645, val_wg=0.0685', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.6363, val_wg=0.8452', '\\n', 'Epoch 1: val_loss=0.5836,\nval_wg=0.9839', '\\n', 'Epoch 2: val_loss=0.5712, val_wg=0.9758', '\\n', 'Epoch 3:\nval_loss=0.5677, val_wg=0.9798', '\\n', 'Epoch 4: val_loss=0.5662,\nval_wg=0.9758', '\\n', 'Epoch 5: val_loss=0.5649, val_wg=0.9798', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0289, val_wg=0.9879',\n'\\n', 'Epoch 1: val_loss=0.0361, val_wg=0.9879', '\\n', 'Epoch 2:\nval_loss=0.0331, val_wg=0.9919', '\\n', 'Epoch 3: val_loss=0.0276,\nval_wg=0.9879', '\\n', 'Epoch 4: val_loss=0.0273, val_wg=0.9881', '\\n', 'Epoch 5:\nval_loss=0.0294, val_wg=0.9881', '\\n', '\\n=== Dataset: corr_95_d10 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6867,\nval_wg=0.2558', '\\n', 'Epoch 1: val_loss=0.6782, val_wg=0.3411', '\\n', 'Epoch 2:\nval_loss=0.6762, val_wg=0.3605', '\\n', 'Epoch 3: val_loss=0.6757,\nval_wg=0.3605', '\\n', 'Epoch 4: val_loss=0.6756, val_wg=0.3682', '\\n', 'Epoch 5:\nval_loss=0.6755, val_wg=0.3682', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5459, val_wg=0.5124', '\\n', 'Epoch 1: val_loss=0.4702,\nval_wg=0.7769', '\\n', 'Epoch 2: val_loss=0.4526, val_wg=0.8223', '\\n', 'Epoch 3:\nval_loss=0.4480, val_wg=0.8347', '\\n', 'Epoch 4: val_loss=0.4462,\nval_wg=0.8388', '\\n', 'Epoch 5: val_loss=0.4449, val_wg=0.8388', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0005, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0001, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0001, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0001,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0001, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0001, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_95_d15 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6977,\nval_wg=0.1774', '\\n', 'Epoch 1: val_loss=0.6896, val_wg=0.2075', '\\n', 'Epoch 2:\nval_loss=0.6878, val_wg=0.2226', '\\n', 'Epoch 3: val_loss=0.6874,\nval_wg=0.2302', '\\n', 'Epoch 4: val_loss=0.6872, val_wg=0.2302', '\\n', 'Epoch 5:\nval_loss=0.6871, val_wg=0.2340', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.6064, val_wg=0.1321', '\\n', 'Epoch 1: val_loss=0.5193,\nval_wg=0.7547', '\\n', 'Epoch 2: val_loss=0.4990, val_wg=0.8415', '\\n', 'Epoch 3:\nval_loss=0.4935, val_wg=0.8604', '\\n', 'Epoch 4: val_loss=0.4912,\nval_wg=0.8679', '\\n', 'Epoch 5: val_loss=0.4894, val_wg=0.8755', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0005, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0000,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0000, val_wg=1.0000', '\\n', '\\nSaved experiment_data.npy to\n/data/chenhui/AI-Scientist-v2/experiments/2025-06-06_23-36-\n12_gradient_cluster_robust_attempt_0/0-run/process_ForkProcess-14/working',\n'\\n', 'Execution time: a minute seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Dataset: corr_50_d5 ===', '\\n', '\\n--\nTraining with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.7016, val_wg=0.3911',\n'\\n', 'Epoch 1: val_loss=0.6961, val_wg=0.4435', '\\n', 'Epoch 2:\nval_loss=0.6948, val_wg=0.4476', '\\n', 'Epoch 3: val_loss=0.6945,\nval_wg=0.4476', '\\n', 'Epoch 4: val_loss=0.6944, val_wg=0.4476', '\\n', 'Epoch 5:\nval_loss=0.6944, val_wg=0.4476', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5829, val_wg=0.9444', '\\n', 'Epoch 1: val_loss=0.5080,\nval_wg=0.9758', '\\n', 'Epoch 2: val_loss=0.4912, val_wg=0.9762', '\\n', 'Epoch 3:\nval_loss=0.4869, val_wg=0.9762', '\\n', 'Epoch 4: val_loss=0.4852,\nval_wg=0.9762', '\\n', 'Epoch 5: val_loss=0.4841, val_wg=0.9762', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0382, val_wg=0.9839',\n'\\n', 'Epoch 1: val_loss=0.0592, val_wg=0.9798', '\\n', 'Epoch 2:\nval_loss=0.0738, val_wg=0.9798', '\\n', 'Epoch 3: val_loss=0.0884,\nval_wg=0.9718', '\\n', 'Epoch 4: val_loss=0.1001, val_wg=0.9718', '\\n', 'Epoch 5:\nval_loss=0.1123, val_wg=0.9718', '\\n', '\\n=== Dataset: corr_50_d10 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6674,\nval_wg=0.5475', '\\n', 'Epoch 1: val_loss=0.6595, val_wg=0.5932', '\\n', 'Epoch 2:\nval_loss=0.6577, val_wg=0.6046', '\\n', 'Epoch 3: val_loss=0.6573,\nval_wg=0.6046', '\\n', 'Epoch 4: val_loss=0.6571, val_wg=0.6084', '\\n', 'Epoch 5:\nval_loss=0.6570, val_wg=0.6084', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5955, val_wg=0.7529', '\\n', 'Epoch 1: val_loss=0.5222,\nval_wg=0.9494', '\\n', 'Epoch 2: val_loss=0.5052, val_wg=0.9578', '\\n', 'Epoch 3:\nval_loss=0.5008, val_wg=0.9620', '\\n', 'Epoch 4: val_loss=0.4990,\nval_wg=0.9620', '\\n', 'Epoch 5: val_loss=0.4976, val_wg=0.9620', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0020, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0003, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0003, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0002,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0002, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0002, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_50_d15 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6998,\nval_wg=0.4961', '\\n', 'Epoch 1: val_loss=0.6917, val_wg=0.4961', '\\n', 'Epoch 2:\nval_loss=0.6898, val_wg=0.4961', '\\n', 'Epoch 3: val_loss=0.6894,\nval_wg=0.4961', '\\n', 'Epoch 4: val_loss=0.6892, val_wg=0.4961', '\\n', 'Epoch 5:\nval_loss=0.6891, val_wg=0.4961', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5586, val_wg=0.7561', '\\n', 'Epoch 1: val_loss=0.4617,\nval_wg=0.9797', '\\n', 'Epoch 2: val_loss=0.4385, val_wg=0.9878', '\\n', 'Epoch 3:\nval_loss=0.4324, val_wg=0.9878', '\\n', 'Epoch 4: val_loss=0.4298,\nval_wg=0.9878', '\\n', 'Epoch 5: val_loss=0.4279, val_wg=0.9919', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0001, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0000,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0000, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_75_d5 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6614,\nval_wg=0.5635', '\\n', 'Epoch 1: val_loss=0.6560, val_wg=0.5952', '\\n', 'Epoch 2:\nval_loss=0.6547, val_wg=0.6032', '\\n', 'Epoch 3: val_loss=0.6544,\nval_wg=0.6032', '\\n', 'Epoch 4: val_loss=0.6543, val_wg=0.6032', '\\n', 'Epoch 5:\nval_loss=0.6543, val_wg=0.6032', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.6262, val_wg=0.6492', '\\n', 'Epoch 1: val_loss=0.5783,\nval_wg=0.9484', '\\n', 'Epoch 2: val_loss=0.5665, val_wg=0.9524', '\\n', 'Epoch 3:\nval_loss=0.5633, val_wg=0.9563', '\\n', 'Epoch 4: val_loss=0.5619,\nval_wg=0.9563', '\\n', 'Epoch 5: val_loss=0.5608, val_wg=0.9603', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0396, val_wg=0.9879',\n'\\n', 'Epoch 1: val_loss=0.0512, val_wg=0.9919', '\\n', 'Epoch 2:\nval_loss=0.0645, val_wg=0.9879', '\\n', 'Epoch 3: val_loss=0.0760,\nval_wg=0.9879', '\\n', 'Epoch 4: val_loss=0.0873, val_wg=0.9839', '\\n', 'Epoch 5:\nval_loss=0.0946, val_wg=0.9798', '\\n', '\\n=== Dataset: corr_75_d10 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6938,\nval_wg=0.2846', '\\n', 'Epoch 1: val_loss=0.6864, val_wg=0.2885', '\\n', 'Epoch 2:\nval_loss=0.6847, val_wg=0.2885', '\\n', 'Epoch 3: val_loss=0.6843,\nval_wg=0.2885', '\\n', 'Epoch 4: val_loss=0.6842, val_wg=0.2885', '\\n', 'Epoch 5:\nval_loss=0.6841, val_wg=0.2885', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5695, val_wg=0.9881', '\\n', 'Epoch 1: val_loss=0.4987,\nval_wg=0.9960', '\\n', 'Epoch 2: val_loss=0.4823, val_wg=0.9960', '\\n', 'Epoch 3:\nval_loss=0.4780, val_wg=0.9960', '\\n', 'Epoch 4: val_loss=0.4762,\nval_wg=0.9960', '\\n', 'Epoch 5: val_loss=0.4749, val_wg=0.9960', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0010, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0000,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0000, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_75_d15 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6958,\nval_wg=0.2367', '\\n', 'Epoch 1: val_loss=0.6891, val_wg=0.2449', '\\n', 'Epoch 2:\nval_loss=0.6876, val_wg=0.2449', '\\n', 'Epoch 3: val_loss=0.6872,\nval_wg=0.2449', '\\n', 'Epoch 4: val_loss=0.6871, val_wg=0.2449', '\\n', 'Epoch 5:\nval_loss=0.6870, val_wg=0.2449', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.4650, val_wg=0.9878', '\\n', 'Epoch 1: val_loss=0.3608,\nval_wg=1.0000', '\\n', 'Epoch 2: val_loss=0.3378, val_wg=1.0000', '\\n', 'Epoch 3:\nval_loss=0.3321, val_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.3299,\nval_wg=1.0000', '\\n', 'Epoch 5: val_loss=0.3285, val_wg=1.0000', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0000, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0000,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0000, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_95_d5 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6725,\nval_wg=0.0685', '\\n', 'Epoch 1: val_loss=0.6664, val_wg=0.0685', '\\n', 'Epoch 2:\nval_loss=0.6650, val_wg=0.0685', '\\n', 'Epoch 3: val_loss=0.6647,\nval_wg=0.0685', '\\n', 'Epoch 4: val_loss=0.6645, val_wg=0.0685', '\\n', 'Epoch 5:\nval_loss=0.6645, val_wg=0.0685', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.6363, val_wg=0.8452', '\\n', 'Epoch 1: val_loss=0.5836,\nval_wg=0.9839', '\\n', 'Epoch 2: val_loss=0.5712, val_wg=0.9758', '\\n', 'Epoch 3:\nval_loss=0.5677, val_wg=0.9798', '\\n', 'Epoch 4: val_loss=0.5662,\nval_wg=0.9758', '\\n', 'Epoch 5: val_loss=0.5649, val_wg=0.9798', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0289, val_wg=0.9879',\n'\\n', 'Epoch 1: val_loss=0.0361, val_wg=0.9879', '\\n', 'Epoch 2:\nval_loss=0.0331, val_wg=0.9919', '\\n', 'Epoch 3: val_loss=0.0276,\nval_wg=0.9879', '\\n', 'Epoch 4: val_loss=0.0273, val_wg=0.9881', '\\n', 'Epoch 5:\nval_loss=0.0294, val_wg=0.9881', '\\n', '\\n=== Dataset: corr_95_d10 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6867,\nval_wg=0.2558', '\\n', 'Epoch 1: val_loss=0.6782, val_wg=0.3411', '\\n', 'Epoch 2:\nval_loss=0.6762, val_wg=0.3605', '\\n', 'Epoch 3: val_loss=0.6757,\nval_wg=0.3605', '\\n', 'Epoch 4: val_loss=0.6756, val_wg=0.3682', '\\n', 'Epoch 5:\nval_loss=0.6755, val_wg=0.3682', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5459, val_wg=0.5124', '\\n', 'Epoch 1: val_loss=0.4702,\nval_wg=0.7769', '\\n', 'Epoch 2: val_loss=0.4526, val_wg=0.8223', '\\n', 'Epoch 3:\nval_loss=0.4480, val_wg=0.8347', '\\n', 'Epoch 4: val_loss=0.4462,\nval_wg=0.8388', '\\n', 'Epoch 5: val_loss=0.4449, val_wg=0.8388', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0005, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0001, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0001, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0001,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0001, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0001, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_95_d15 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6977,\nval_wg=0.1774', '\\n', 'Epoch 1: val_loss=0.6896, val_wg=0.2075', '\\n', 'Epoch 2:\nval_loss=0.6878, val_wg=0.2226', '\\n', 'Epoch 3: val_loss=0.6874,\nval_wg=0.2302', '\\n', 'Epoch 4: val_loss=0.6872, val_wg=0.2302', '\\n', 'Epoch 5:\nval_loss=0.6871, val_wg=0.2340', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.6064, val_wg=0.1321', '\\n', 'Epoch 1: val_loss=0.5193,\nval_wg=0.7547', '\\n', 'Epoch 2: val_loss=0.4990, val_wg=0.8415', '\\n', 'Epoch 3:\nval_loss=0.4935, val_wg=0.8604', '\\n', 'Epoch 4: val_loss=0.4912,\nval_wg=0.8679', '\\n', 'Epoch 5: val_loss=0.4894, val_wg=0.8755', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0005, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0000,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0000, val_wg=1.0000', '\\n', '\\nSaved experiment_data.npy to\n/data/chenhui/AI-Scientist-v2/experiments/2025-06-06_23-36-\n12_gradient_cluster_robust_attempt_0/0-run/process_ForkProcess-15/working',\n'\\n', 'Execution time: a minute seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Dataset: corr_50_d5 ===', '\\n', '\\n--\nTraining with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.7016, val_wg=0.3911',\n'\\n', 'Epoch 1: val_loss=0.6961, val_wg=0.4435', '\\n', 'Epoch 2:\nval_loss=0.6948, val_wg=0.4476', '\\n', 'Epoch 3: val_loss=0.6945,\nval_wg=0.4476', '\\n', 'Epoch 4: val_loss=0.6944, val_wg=0.4476', '\\n', 'Epoch 5:\nval_loss=0.6944, val_wg=0.4476', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5829, val_wg=0.9444', '\\n', 'Epoch 1: val_loss=0.5080,\nval_wg=0.9758', '\\n', 'Epoch 2: val_loss=0.4912, val_wg=0.9762', '\\n', 'Epoch 3:\nval_loss=0.4869, val_wg=0.9762', '\\n', 'Epoch 4: val_loss=0.4852,\nval_wg=0.9762', '\\n', 'Epoch 5: val_loss=0.4841, val_wg=0.9762', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0382, val_wg=0.9839',\n'\\n', 'Epoch 1: val_loss=0.0592, val_wg=0.9798', '\\n', 'Epoch 2:\nval_loss=0.0738, val_wg=0.9798', '\\n', 'Epoch 3: val_loss=0.0884,\nval_wg=0.9718', '\\n', 'Epoch 4: val_loss=0.1001, val_wg=0.9718', '\\n', 'Epoch 5:\nval_loss=0.1123, val_wg=0.9718', '\\n', '\\n=== Dataset: corr_50_d10 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6674,\nval_wg=0.5475', '\\n', 'Epoch 1: val_loss=0.6595, val_wg=0.5932', '\\n', 'Epoch 2:\nval_loss=0.6577, val_wg=0.6046', '\\n', 'Epoch 3: val_loss=0.6573,\nval_wg=0.6046', '\\n', 'Epoch 4: val_loss=0.6571, val_wg=0.6084', '\\n', 'Epoch 5:\nval_loss=0.6570, val_wg=0.6084', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5955, val_wg=0.7529', '\\n', 'Epoch 1: val_loss=0.5222,\nval_wg=0.9494', '\\n', 'Epoch 2: val_loss=0.5052, val_wg=0.9578', '\\n', 'Epoch 3:\nval_loss=0.5008, val_wg=0.9620', '\\n', 'Epoch 4: val_loss=0.4990,\nval_wg=0.9620', '\\n', 'Epoch 5: val_loss=0.4976, val_wg=0.9620', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0020, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0003, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0003, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0002,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0002, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0002, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_50_d15 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6998,\nval_wg=0.4961', '\\n', 'Epoch 1: val_loss=0.6917, val_wg=0.4961', '\\n', 'Epoch 2:\nval_loss=0.6898, val_wg=0.4961', '\\n', 'Epoch 3: val_loss=0.6894,\nval_wg=0.4961', '\\n', 'Epoch 4: val_loss=0.6892, val_wg=0.4961', '\\n', 'Epoch 5:\nval_loss=0.6891, val_wg=0.4961', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5586, val_wg=0.7561', '\\n', 'Epoch 1: val_loss=0.4617,\nval_wg=0.9797', '\\n', 'Epoch 2: val_loss=0.4385, val_wg=0.9878', '\\n', 'Epoch 3:\nval_loss=0.4324, val_wg=0.9878', '\\n', 'Epoch 4: val_loss=0.4298,\nval_wg=0.9878', '\\n', 'Epoch 5: val_loss=0.4279, val_wg=0.9919', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0001, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0000,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0000, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_75_d5 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6614,\nval_wg=0.5635', '\\n', 'Epoch 1: val_loss=0.6560, val_wg=0.5952', '\\n', 'Epoch 2:\nval_loss=0.6547, val_wg=0.6032', '\\n', 'Epoch 3: val_loss=0.6544,\nval_wg=0.6032', '\\n', 'Epoch 4: val_loss=0.6543, val_wg=0.6032', '\\n', 'Epoch 5:\nval_loss=0.6543, val_wg=0.6032', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.6262, val_wg=0.6492', '\\n', 'Epoch 1: val_loss=0.5783,\nval_wg=0.9484', '\\n', 'Epoch 2: val_loss=0.5665, val_wg=0.9524', '\\n', 'Epoch 3:\nval_loss=0.5633, val_wg=0.9563', '\\n', 'Epoch 4: val_loss=0.5619,\nval_wg=0.9563', '\\n', 'Epoch 5: val_loss=0.5608, val_wg=0.9603', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0396, val_wg=0.9879',\n'\\n', 'Epoch 1: val_loss=0.0512, val_wg=0.9919', '\\n', 'Epoch 2:\nval_loss=0.0645, val_wg=0.9879', '\\n', 'Epoch 3: val_loss=0.0760,\nval_wg=0.9879', '\\n', 'Epoch 4: val_loss=0.0873, val_wg=0.9839', '\\n', 'Epoch 5:\nval_loss=0.0946, val_wg=0.9798', '\\n', '\\n=== Dataset: corr_75_d10 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6938,\nval_wg=0.2846', '\\n', 'Epoch 1: val_loss=0.6864, val_wg=0.2885', '\\n', 'Epoch 2:\nval_loss=0.6847, val_wg=0.2885', '\\n', 'Epoch 3: val_loss=0.6843,\nval_wg=0.2885', '\\n', 'Epoch 4: val_loss=0.6842, val_wg=0.2885', '\\n', 'Epoch 5:\nval_loss=0.6841, val_wg=0.2885', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5695, val_wg=0.9881', '\\n', 'Epoch 1: val_loss=0.4987,\nval_wg=0.9960', '\\n', 'Epoch 2: val_loss=0.4823, val_wg=0.9960', '\\n', 'Epoch 3:\nval_loss=0.4780, val_wg=0.9960', '\\n', 'Epoch 4: val_loss=0.4762,\nval_wg=0.9960', '\\n', 'Epoch 5: val_loss=0.4749, val_wg=0.9960', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0010, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0000,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0000, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_75_d15 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6958,\nval_wg=0.2367', '\\n', 'Epoch 1: val_loss=0.6891, val_wg=0.2449', '\\n', 'Epoch 2:\nval_loss=0.6876, val_wg=0.2449', '\\n', 'Epoch 3: val_loss=0.6872,\nval_wg=0.2449', '\\n', 'Epoch 4: val_loss=0.6871, val_wg=0.2449', '\\n', 'Epoch 5:\nval_loss=0.6870, val_wg=0.2449', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.4650, val_wg=0.9878', '\\n', 'Epoch 1: val_loss=0.3608,\nval_wg=1.0000', '\\n', 'Epoch 2: val_loss=0.3378, val_wg=1.0000', '\\n', 'Epoch 3:\nval_loss=0.3321, val_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.3299,\nval_wg=1.0000', '\\n', 'Epoch 5: val_loss=0.3285, val_wg=1.0000', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0000, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0000,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0000, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_95_d5 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6725,\nval_wg=0.0685', '\\n', 'Epoch 1: val_loss=0.6664, val_wg=0.0685', '\\n', 'Epoch 2:\nval_loss=0.6650, val_wg=0.0685', '\\n', 'Epoch 3: val_loss=0.6647,\nval_wg=0.0685', '\\n', 'Epoch 4: val_loss=0.6645, val_wg=0.0685', '\\n', 'Epoch 5:\nval_loss=0.6645, val_wg=0.0685', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.6363, val_wg=0.8452', '\\n', 'Epoch 1: val_loss=0.5836,\nval_wg=0.9839', '\\n', 'Epoch 2: val_loss=0.5712, val_wg=0.9758', '\\n', 'Epoch 3:\nval_loss=0.5677, val_wg=0.9798', '\\n', 'Epoch 4: val_loss=0.5662,\nval_wg=0.9758', '\\n', 'Epoch 5: val_loss=0.5649, val_wg=0.9798', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0289, val_wg=0.9879',\n'\\n', 'Epoch 1: val_loss=0.0361, val_wg=0.9879', '\\n', 'Epoch 2:\nval_loss=0.0331, val_wg=0.9919', '\\n', 'Epoch 3: val_loss=0.0276,\nval_wg=0.9879', '\\n', 'Epoch 4: val_loss=0.0273, val_wg=0.9881', '\\n', 'Epoch 5:\nval_loss=0.0294, val_wg=0.9881', '\\n', '\\n=== Dataset: corr_95_d10 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6867,\nval_wg=0.2558', '\\n', 'Epoch 1: val_loss=0.6782, val_wg=0.3411', '\\n', 'Epoch 2:\nval_loss=0.6762, val_wg=0.3605', '\\n', 'Epoch 3: val_loss=0.6757,\nval_wg=0.3605', '\\n', 'Epoch 4: val_loss=0.6756, val_wg=0.3682', '\\n', 'Epoch 5:\nval_loss=0.6755, val_wg=0.3682', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.5459, val_wg=0.5124', '\\n', 'Epoch 1: val_loss=0.4702,\nval_wg=0.7769', '\\n', 'Epoch 2: val_loss=0.4526, val_wg=0.8223', '\\n', 'Epoch 3:\nval_loss=0.4480, val_wg=0.8347', '\\n', 'Epoch 4: val_loss=0.4462,\nval_wg=0.8388', '\\n', 'Epoch 5: val_loss=0.4449, val_wg=0.8388', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0005, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0001, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0001, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0001,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0001, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0001, val_wg=1.0000', '\\n', '\\n=== Dataset: corr_95_d15 ===', '\\n',\n'\\n-- Training with lr=0.0001 --', '\\n', 'Epoch 0: val_loss=0.6977,\nval_wg=0.1774', '\\n', 'Epoch 1: val_loss=0.6896, val_wg=0.2075', '\\n', 'Epoch 2:\nval_loss=0.6878, val_wg=0.2226', '\\n', 'Epoch 3: val_loss=0.6874,\nval_wg=0.2302', '\\n', 'Epoch 4: val_loss=0.6872, val_wg=0.2302', '\\n', 'Epoch 5:\nval_loss=0.6871, val_wg=0.2340', '\\n', '\\n-- Training with lr=0.001 --', '\\n',\n'Epoch 0: val_loss=0.6064, val_wg=0.1321', '\\n', 'Epoch 1: val_loss=0.5193,\nval_wg=0.7547', '\\n', 'Epoch 2: val_loss=0.4990, val_wg=0.8415', '\\n', 'Epoch 3:\nval_loss=0.4935, val_wg=0.8604', '\\n', 'Epoch 4: val_loss=0.4912,\nval_wg=0.8679', '\\n', 'Epoch 5: val_loss=0.4894, val_wg=0.8755', '\\n', '\\n--\nTraining with lr=0.01 --', '\\n', 'Epoch 0: val_loss=0.0005, val_wg=1.0000',\n'\\n', 'Epoch 1: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 2:\nval_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 3: val_loss=0.0000,\nval_wg=1.0000', '\\n', 'Epoch 4: val_loss=0.0000, val_wg=1.0000', '\\n', 'Epoch 5:\nval_loss=0.0000, val_wg=1.0000', '\\n', '\\nSaved experiment_data.npy to\n/data/chenhui/AI-Scientist-v2/experiments/2025-06-06_23-36-\n12_gradient_cluster_robust_attempt_0/0-run/process_ForkProcess-13/working',\n'\\n', 'Execution time: a minute seconds (time limit is an hour).']", ""], "analysis": ["", "", "", "", "", "", "", "", "The script crashes at `if not sd[\"ground_truth\"]:` because `sd[\"ground_truth\"]`\nis a NumPy array, and its truth value is ambiguous when used in a boolean\ncontext. To fix, initialize `ground_truth` using a Python list (so `if not\nsd['ground_truth']:` works), or change the condition to test the array\u2019s length,\ne.g.: `if sd['ground_truth'].size == 0:` or `if len(sd['ground_truth']) == 0:`.\nThis will correctly guard against uninitialized arrays and avoid the ValueError.", "The script crashes at the final logging step with a ValueError: 'The truth value\nof an array with more than one element is ambiguous.' This happens because the\ncode does `if not experiment_data[...]['ground_truth']:` on a numpy array. Numpy\narrays cannot be evaluated directly as booleans. As a result, execution stops\nbefore saving `experiment_data.npy`. To fix this, change the conditional to\nexplicitly check for emptiness, e.g. `if\nlen(experiment_data['SGD_OPTIMIZER']['synthetic']['ground_truth']) == 0:` or use\n`array.size == 0`, or initialize `ground_truth` as an empty list and append to\nit rather than converting it to a numpy array.", "", "", "", "", "", "", "", "", "", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, "ValueError", "ValueError", null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, {"args": ["The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"]}, {"args": ["The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"]}, null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 170, "<module>", "if not sd[\"ground_truth\"]:"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 214, "<module>", "if not experiment_data[\"SGD_OPTIMIZER\"][\"synthetic\"][\"ground_truth\"]:"]], null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "worst-group accuracy", "lower_is_better": false, "description": "Proportion of correct predictions on the worst-performing subgroup in the dataset at final evaluation.", "data": [{"dataset_name": "in-sample", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "development", "final_value": 0.9924, "best_value": 0.9924}]}, {"metric_name": "average loss", "lower_is_better": true, "description": "Average loss across all examples in the dataset at final evaluation.", "data": [{"dataset_name": "in-sample", "final_value": 0.0125, "best_value": 0.0125}, {"dataset_name": "development", "final_value": 0.043, "best_value": 0.043}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Proportion of correct predictions at evaluation time on the out-of-sample dataset.", "data": [{"dataset_name": "out-of-sample", "final_value": 0.996, "best_value": 0.996}]}]}, {"metric_names": [{"metric_name": "train worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the training dataset", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.9851, "best_value": 0.9851}, {"dataset_name": "corr_50_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_50_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d5", "final_value": 0.984, "best_value": 0.984}, {"dataset_name": "corr_75_d10", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "corr_75_d15", "final_value": 0.9961, "best_value": 0.9961}, {"dataset_name": "corr_95_d5", "final_value": 0.9842, "best_value": 0.9842}, {"dataset_name": "corr_95_d10", "final_value": 0.9979, "best_value": 0.9979}, {"dataset_name": "corr_95_d15", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the validation dataset", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.9762, "best_value": 0.9762}, {"dataset_name": "corr_50_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_50_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d5", "final_value": 0.9798, "best_value": 0.9798}, {"dataset_name": "corr_75_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_95_d5", "final_value": 0.9881, "best_value": 0.9881}, {"dataset_name": "corr_95_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_95_d15", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.4796, "best_value": 0.4796}, {"dataset_name": "corr_50_d10", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "corr_50_d15", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "corr_75_d5", "final_value": 0.0507, "best_value": 0.0507}, {"dataset_name": "corr_75_d10", "final_value": 0.0067, "best_value": 0.0067}, {"dataset_name": "corr_75_d15", "final_value": 0.334, "best_value": 0.334}, {"dataset_name": "corr_95_d5", "final_value": 0.0272, "best_value": 0.0272}, {"dataset_name": "corr_95_d10", "final_value": 0.001, "best_value": 0.001}, {"dataset_name": "corr_95_d15", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.4841, "best_value": 0.4841}, {"dataset_name": "corr_50_d10", "final_value": 0.0002, "best_value": 0.0002}, {"dataset_name": "corr_50_d15", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "corr_75_d5", "final_value": 0.0946, "best_value": 0.0946}, {"dataset_name": "corr_75_d10", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "corr_75_d15", "final_value": 0.3285, "best_value": 0.3285}, {"dataset_name": "corr_95_d5", "final_value": 0.0294, "best_value": 0.0294}, {"dataset_name": "corr_95_d10", "final_value": 0.0001, "best_value": 0.0001}, {"dataset_name": "corr_95_d15", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.984, "best_value": 0.984}, {"dataset_name": "corr_50_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_50_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d5", "final_value": 0.982, "best_value": 0.982}, {"dataset_name": "corr_75_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_95_d5", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "corr_95_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_95_d15", "final_value": 1.0, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy measured on the dataset splits", "data": [{"dataset_name": "training", "final_value": 0.9903, "best_value": 0.9903}, {"dataset_name": "validation", "final_value": 0.9885, "best_value": 0.9885}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Cross-entropy loss measured on the dataset splits", "data": [{"dataset_name": "training", "final_value": 0.0466, "best_value": 0.0466}, {"dataset_name": "validation", "final_value": 0.0542, "best_value": 0.0542}]}, {"metric_name": "accuracy", "lower_is_better": false, "description": "Overall accuracy on the test dataset", "data": [{"dataset_name": "test", "final_value": 0.992, "best_value": 0.992}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "worst-group accuracy", "lower_is_better": false, "description": "Accuracy on the worst-performing group of the dataset", "data": [{"dataset_name": "training dataset", "final_value": 0.9768, "best_value": 0.9768}, {"dataset_name": "validation dataset", "final_value": 0.9706, "best_value": 0.9706}]}, {"metric_name": "accuracy", "lower_is_better": false, "description": "Overall accuracy on the test dataset", "data": [{"dataset_name": "test dataset", "final_value": 0.974, "best_value": 0.974}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "accuracy on the synthetic dataset during training", "data": [{"dataset_name": "synthetic", "final_value": 0.9942, "best_value": 0.9942}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "accuracy on the synthetic dataset during validation", "data": [{"dataset_name": "synthetic", "final_value": 0.9885, "best_value": 0.9885}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "loss on the synthetic dataset during training", "data": [{"dataset_name": "synthetic", "final_value": 0.0093, "best_value": 0.0093}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "loss on the synthetic dataset during validation", "data": [{"dataset_name": "synthetic", "final_value": 0.0288, "best_value": 0.0288}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "accuracy on the synthetic dataset during testing", "data": [{"dataset_name": "synthetic", "final_value": 0.99, "best_value": 0.99}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Final training accuracy for each learning rate", "data": [{"dataset_name": "lr=0.0001", "final_value": 0.3478, "best_value": 0.3478}, {"dataset_name": "lr=0.001", "final_value": 0.9826, "best_value": 0.9826}, {"dataset_name": "lr=0.01", "final_value": 0.9942, "best_value": 0.9942}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Final training loss for each learning rate", "data": [{"dataset_name": "lr=0.0001", "final_value": 0.6895, "best_value": 0.6895}, {"dataset_name": "lr=0.001", "final_value": 0.4808, "best_value": 0.4808}, {"dataset_name": "lr=0.01", "final_value": 0.0063, "best_value": 0.0063}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Final validation accuracy for each learning rate", "data": [{"dataset_name": "lr=0.0001", "final_value": 0.3588, "best_value": 0.3588}, {"dataset_name": "lr=0.001", "final_value": 0.9622, "best_value": 0.9622}, {"dataset_name": "lr=0.01", "final_value": 0.9885, "best_value": 0.9885}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss for each learning rate", "data": [{"dataset_name": "lr=0.0001", "final_value": 0.6898, "best_value": 0.6898}, {"dataset_name": "lr=0.001", "final_value": 0.486, "best_value": 0.486}, {"dataset_name": "lr=0.01", "final_value": 0.0351, "best_value": 0.0351}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test accuracy for each learning rate", "data": [{"dataset_name": "lr=0.0001", "final_value": 0.488, "best_value": 0.488}, {"dataset_name": "lr=0.001", "final_value": 0.982, "best_value": 0.982}, {"dataset_name": "lr=0.01", "final_value": 0.992, "best_value": 0.992}]}]}, {"metric_names": [{"metric_name": "Train worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the training dataset", "data": [{"dataset_name": "train", "final_value": 0.9942, "best_value": 0.9942}]}, {"metric_name": "Train average loss", "lower_is_better": true, "description": "Average loss on the training dataset", "data": [{"dataset_name": "train", "final_value": 0.0099, "best_value": 0.0099}]}, {"metric_name": "Validation worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the validation dataset", "data": [{"dataset_name": "validation", "final_value": 0.9885, "best_value": 0.9885}]}, {"metric_name": "Validation average loss", "lower_is_better": true, "description": "Average loss on the validation dataset", "data": [{"dataset_name": "validation", "final_value": 0.0387, "best_value": 0.0387}]}, {"metric_name": "Test overall accuracy", "lower_is_better": false, "description": "Overall accuracy on the test dataset", "data": [{"dataset_name": "test", "final_value": 0.99, "best_value": 0.99}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9942, "best_value": 0.9942}]}, {"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9885, "best_value": 0.9885}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "synthetic", "final_value": 0.0099, "best_value": 0.0099}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "synthetic", "final_value": 0.0387, "best_value": 0.0387}]}, {"metric_name": "test F1 score", "lower_is_better": false, "description": "F1 score on the test dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9899, "best_value": 0.9899}]}]}, {"metric_names": [{"metric_name": "train worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the training dataset", "data": [{"dataset_name": "lr=0.0001", "final_value": 0.8116, "best_value": 0.8116}, {"dataset_name": "lr=0.001", "final_value": 0.9884, "best_value": 0.9884}, {"dataset_name": "lr=0.01", "final_value": 0.9942, "best_value": 0.9942}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "lr=0.0001", "final_value": 0.6479, "best_value": 0.6479}, {"dataset_name": "lr=0.001", "final_value": 0.0658, "best_value": 0.0658}, {"dataset_name": "lr=0.01", "final_value": 0.0039, "best_value": 0.0039}]}, {"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the validation dataset", "data": [{"dataset_name": "lr=0.0001", "final_value": 0.8092, "best_value": 0.8092}, {"dataset_name": "lr=0.001", "final_value": 0.9771, "best_value": 0.9771}, {"dataset_name": "lr=0.01", "final_value": 0.9847, "best_value": 0.9847}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "lr=0.0001", "final_value": 0.6485, "best_value": 0.6485}, {"dataset_name": "lr=0.001", "final_value": 0.0728, "best_value": 0.0728}, {"dataset_name": "lr=0.01", "final_value": 0.0336, "best_value": 0.0336}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset", "data": [{"dataset_name": "lr=0.0001", "final_value": 0.864, "best_value": 0.864}, {"dataset_name": "lr=0.001", "final_value": 0.988, "best_value": 0.988}, {"dataset_name": "lr=0.01", "final_value": 0.996, "best_value": 0.996}]}]}, {"metric_names": [{"metric_name": "train worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the synthetic dataset's training split", "data": [{"dataset_name": "synthetic-lr0.0001", "final_value": 0.3478, "best_value": 0.3478}, {"dataset_name": "synthetic-lr0.001", "final_value": 0.9793, "best_value": 0.9793}, {"dataset_name": "synthetic-lr0.01", "final_value": 0.9942, "best_value": 0.9942}]}, {"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the synthetic dataset's validation split", "data": [{"dataset_name": "synthetic-lr0.0001", "final_value": 0.3588, "best_value": 0.3588}, {"dataset_name": "synthetic-lr0.001", "final_value": 0.9656, "best_value": 0.9656}, {"dataset_name": "synthetic-lr0.01", "final_value": 0.9885, "best_value": 0.9885}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the synthetic dataset's training split", "data": [{"dataset_name": "synthetic-lr0.0001", "final_value": 0.6895, "best_value": 0.6895}, {"dataset_name": "synthetic-lr0.001", "final_value": 0.4662, "best_value": 0.4662}, {"dataset_name": "synthetic-lr0.01", "final_value": 0.0099, "best_value": 0.0099}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the synthetic dataset's validation split", "data": [{"dataset_name": "synthetic-lr0.0001", "final_value": 0.6898, "best_value": 0.6898}, {"dataset_name": "synthetic-lr0.001", "final_value": 0.4738, "best_value": 0.4738}, {"dataset_name": "synthetic-lr0.01", "final_value": 0.0387, "best_value": 0.0387}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Overall accuracy on the synthetic dataset's test split", "data": [{"dataset_name": "synthetic-lr0.0001", "final_value": 0.486, "best_value": 0.486}, {"dataset_name": "synthetic-lr0.001", "final_value": 0.98, "best_value": 0.98}, {"dataset_name": "synthetic-lr0.01", "final_value": 0.99, "best_value": 0.99}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Proportion of correctly classified samples in the training dataset", "data": [{"dataset_name": "cluster_count=1, learning_rate=0.0001", "final_value": 0.3478, "best_value": 0.3478}, {"dataset_name": "cluster_count=1, learning_rate=0.001", "final_value": 0.9826, "best_value": 0.9826}, {"dataset_name": "cluster_count=1, learning_rate=0.01", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "cluster_count=2, learning_rate=0.0001", "final_value": 0.058, "best_value": 0.058}, {"dataset_name": "cluster_count=2, learning_rate=0.001", "final_value": 0.9632, "best_value": 0.9632}, {"dataset_name": "cluster_count=2, learning_rate=0.01", "final_value": 0.9938, "best_value": 0.9938}, {"dataset_name": "cluster_count=4, learning_rate=0.0001", "final_value": 0.5549, "best_value": 0.5549}, {"dataset_name": "cluster_count=4, learning_rate=0.001", "final_value": 0.9807, "best_value": 0.9807}, {"dataset_name": "cluster_count=4, learning_rate=0.01", "final_value": 0.9961, "best_value": 0.9961}, {"dataset_name": "cluster_count=8, learning_rate=0.0001", "final_value": 0.528, "best_value": 0.528}, {"dataset_name": "cluster_count=8, learning_rate=0.001", "final_value": 0.9834, "best_value": 0.9834}, {"dataset_name": "cluster_count=8, learning_rate=0.01", "final_value": 0.9961, "best_value": 0.9961}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training dataset", "data": [{"dataset_name": "cluster_count=1, learning_rate=0.0001", "final_value": 0.6896, "best_value": 0.6896}, {"dataset_name": "cluster_count=1, learning_rate=0.001", "final_value": 0.4826, "best_value": 0.4826}, {"dataset_name": "cluster_count=1, learning_rate=0.01", "final_value": 0.0063, "best_value": 0.0063}, {"dataset_name": "cluster_count=2, learning_rate=0.0001", "final_value": 0.695, "best_value": 0.695}, {"dataset_name": "cluster_count=2, learning_rate=0.001", "final_value": 0.4956, "best_value": 0.4956}, {"dataset_name": "cluster_count=2, learning_rate=0.01", "final_value": 0.0111, "best_value": 0.0111}, {"dataset_name": "cluster_count=4, learning_rate=0.0001", "final_value": 0.6823, "best_value": 0.6823}, {"dataset_name": "cluster_count=4, learning_rate=0.001", "final_value": 0.4753, "best_value": 0.4753}, {"dataset_name": "cluster_count=4, learning_rate=0.01", "final_value": 0.0044, "best_value": 0.0044}, {"dataset_name": "cluster_count=8, learning_rate=0.0001", "final_value": 0.6456, "best_value": 0.6456}, {"dataset_name": "cluster_count=8, learning_rate=0.001", "final_value": 0.5153, "best_value": 0.5153}, {"dataset_name": "cluster_count=8, learning_rate=0.01", "final_value": 0.0044, "best_value": 0.0044}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Proportion of correctly classified samples in the validation dataset", "data": [{"dataset_name": "cluster_count=1, learning_rate=0.0001", "final_value": 0.3588, "best_value": 0.3588}, {"dataset_name": "cluster_count=1, learning_rate=0.001", "final_value": 0.9622, "best_value": 0.9622}, {"dataset_name": "cluster_count=1, learning_rate=0.01", "final_value": 0.9885, "best_value": 0.9885}, {"dataset_name": "cluster_count=2, learning_rate=0.0001", "final_value": 0.0672, "best_value": 0.0672}, {"dataset_name": "cluster_count=2, learning_rate=0.001", "final_value": 0.9496, "best_value": 0.9496}, {"dataset_name": "cluster_count=2, learning_rate=0.01", "final_value": 0.9847, "best_value": 0.9847}, {"dataset_name": "cluster_count=4, learning_rate=0.0001", "final_value": 0.4874, "best_value": 0.4874}, {"dataset_name": "cluster_count=4, learning_rate=0.001", "final_value": 0.9809, "best_value": 0.9809}, {"dataset_name": "cluster_count=4, learning_rate=0.01", "final_value": 0.9847, "best_value": 0.9847}, {"dataset_name": "cluster_count=8, learning_rate=0.0001", "final_value": 0.5504, "best_value": 0.5504}, {"dataset_name": "cluster_count=8, learning_rate=0.001", "final_value": 0.9771, "best_value": 0.9771}, {"dataset_name": "cluster_count=8, learning_rate=0.01", "final_value": 0.9885, "best_value": 0.9885}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation dataset", "data": [{"dataset_name": "cluster_count=1, learning_rate=0.0001", "final_value": 0.69, "best_value": 0.69}, {"dataset_name": "cluster_count=1, learning_rate=0.001", "final_value": 0.4878, "best_value": 0.4878}, {"dataset_name": "cluster_count=1, learning_rate=0.01", "final_value": 0.0351, "best_value": 0.0351}, {"dataset_name": "cluster_count=2, learning_rate=0.0001", "final_value": 0.6912, "best_value": 0.6912}, {"dataset_name": "cluster_count=2, learning_rate=0.001", "final_value": 0.4911, "best_value": 0.4911}, {"dataset_name": "cluster_count=2, learning_rate=0.01", "final_value": 0.0321, "best_value": 0.0321}, {"dataset_name": "cluster_count=4, learning_rate=0.0001", "final_value": 0.6833, "best_value": 0.6833}, {"dataset_name": "cluster_count=4, learning_rate=0.001", "final_value": 0.4724, "best_value": 0.4724}, {"dataset_name": "cluster_count=4, learning_rate=0.01", "final_value": 0.0299, "best_value": 0.0299}, {"dataset_name": "cluster_count=8, learning_rate=0.0001", "final_value": 0.6414, "best_value": 0.6414}, {"dataset_name": "cluster_count=8, learning_rate=0.001", "final_value": 0.5159, "best_value": 0.5159}, {"dataset_name": "cluster_count=8, learning_rate=0.01", "final_value": 0.0339, "best_value": 0.0339}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Proportion of correctly classified samples in the test dataset", "data": [{"dataset_name": "cluster_count=1, learning_rate=0.0001", "final_value": 0.484, "best_value": 0.484}, {"dataset_name": "cluster_count=1, learning_rate=0.001", "final_value": 0.982, "best_value": 0.982}, {"dataset_name": "cluster_count=1, learning_rate=0.01", "final_value": 0.992, "best_value": 0.992}, {"dataset_name": "cluster_count=2, learning_rate=0.0001", "final_value": 0.49, "best_value": 0.49}, {"dataset_name": "cluster_count=2, learning_rate=0.001", "final_value": 0.968, "best_value": 0.968}, {"dataset_name": "cluster_count=2, learning_rate=0.01", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "cluster_count=4, learning_rate=0.0001", "final_value": 0.56, "best_value": 0.56}, {"dataset_name": "cluster_count=4, learning_rate=0.001", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "cluster_count=4, learning_rate=0.01", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "cluster_count=8, learning_rate=0.0001", "final_value": 0.762, "best_value": 0.762}, {"dataset_name": "cluster_count=8, learning_rate=0.001", "final_value": 0.988, "best_value": 0.988}, {"dataset_name": "cluster_count=8, learning_rate=0.01", "final_value": 0.996, "best_value": 0.996}]}]}, {"metric_names": [{"metric_name": "train worst-group accuracy", "lower_is_better": false, "description": "Accuracy on the worst-performing group within the training set", "data": [{"dataset_name": "synthetic (weight_decay=0.0)", "final_value": 0.9884, "best_value": 0.9884}, {"dataset_name": "synthetic (weight_decay=0.0001)", "final_value": 0.9826, "best_value": 0.9826}, {"dataset_name": "synthetic (weight_decay=0.001)", "final_value": 0.9923, "best_value": 0.9923}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "synthetic (weight_decay=0.0)", "final_value": 0.5028, "best_value": 0.5028}, {"dataset_name": "synthetic (weight_decay=0.0001)", "final_value": 0.4815, "best_value": 0.4815}, {"dataset_name": "synthetic (weight_decay=0.001)", "final_value": 0.4678, "best_value": 0.4678}]}, {"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "Accuracy on the worst-performing group within the validation set", "data": [{"dataset_name": "synthetic (weight_decay=0.0)", "final_value": 0.9885, "best_value": 0.9885}, {"dataset_name": "synthetic (weight_decay=0.0001)", "final_value": 0.9622, "best_value": 0.9622}, {"dataset_name": "synthetic (weight_decay=0.001)", "final_value": 0.9809, "best_value": 0.9809}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "synthetic (weight_decay=0.0)", "final_value": 0.5044, "best_value": 0.5044}, {"dataset_name": "synthetic (weight_decay=0.0001)", "final_value": 0.4867, "best_value": 0.4867}, {"dataset_name": "synthetic (weight_decay=0.001)", "final_value": 0.4666, "best_value": 0.4666}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test set", "data": [{"dataset_name": "synthetic (weight_decay=0.0)", "final_value": 0.988, "best_value": 0.988}, {"dataset_name": "synthetic (weight_decay=0.0001)", "final_value": 0.982, "best_value": 0.982}, {"dataset_name": "synthetic (weight_decay=0.001)", "final_value": 0.99, "best_value": 0.99}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Classification accuracy on the training dataset.", "data": [{"dataset_name": "raw_gradient_cluster_reweighting", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "normalized_gradient_cluster_reweighting", "final_value": 0.9961, "best_value": 0.9961}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Classification accuracy on the validation dataset.", "data": [{"dataset_name": "raw_gradient_cluster_reweighting", "final_value": 0.9924, "best_value": 0.9924}, {"dataset_name": "normalized_gradient_cluster_reweighting", "final_value": 0.9847, "best_value": 0.9847}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss value on the training dataset.", "data": [{"dataset_name": "raw_gradient_cluster_reweighting", "final_value": 0.0125, "best_value": 0.0125}, {"dataset_name": "normalized_gradient_cluster_reweighting", "final_value": 0.008, "best_value": 0.008}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value on the validation dataset.", "data": [{"dataset_name": "raw_gradient_cluster_reweighting", "final_value": 0.043, "best_value": 0.043}, {"dataset_name": "normalized_gradient_cluster_reweighting", "final_value": 0.0294, "best_value": 0.0294}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Classification accuracy on the test dataset.", "data": [{"dataset_name": "raw_gradient_cluster_reweighting", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "normalized_gradient_cluster_reweighting", "final_value": 0.99, "best_value": 0.99}]}]}, {"metric_names": [{"metric_name": "train worst-group accuracy", "lower_is_better": false, "description": "Accuracy on the worst-performing group in the training set.", "data": [{"dataset_name": "synthetic_with_norm", "final_value": 0.9942, "best_value": 0.9942}, {"dataset_name": "synthetic_no_norm", "final_value": 0.9772, "best_value": 0.9772}]}, {"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "Accuracy on the worst-performing group in the validation set.", "data": [{"dataset_name": "synthetic_with_norm", "final_value": 0.9924, "best_value": 0.9924}, {"dataset_name": "synthetic_no_norm", "final_value": 0.9656, "best_value": 0.9656}]}]}, {"metric_names": [{"metric_name": "train worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the training set", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.9851, "best_value": 0.9851}, {"dataset_name": "corr_50_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_50_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d5", "final_value": 0.984, "best_value": 0.984}, {"dataset_name": "corr_75_d10", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "corr_75_d15", "final_value": 0.9961, "best_value": 0.9961}, {"dataset_name": "corr_95_d5", "final_value": 0.9842, "best_value": 0.9842}, {"dataset_name": "corr_95_d10", "final_value": 0.9979, "best_value": 0.9979}, {"dataset_name": "corr_95_d15", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the validation set", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.9762, "best_value": 0.9762}, {"dataset_name": "corr_50_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_50_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d5", "final_value": 0.9798, "best_value": 0.9798}, {"dataset_name": "corr_75_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_95_d5", "final_value": 0.9881, "best_value": 0.9881}, {"dataset_name": "corr_95_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_95_d15", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.4796, "best_value": 0.4796}, {"dataset_name": "corr_50_d10", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "corr_50_d15", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "corr_75_d5", "final_value": 0.0507, "best_value": 0.0507}, {"dataset_name": "corr_75_d10", "final_value": 0.0067, "best_value": 0.0067}, {"dataset_name": "corr_75_d15", "final_value": 0.334, "best_value": 0.334}, {"dataset_name": "corr_95_d5", "final_value": 0.0272, "best_value": 0.0272}, {"dataset_name": "corr_95_d10", "final_value": 0.001, "best_value": 0.001}, {"dataset_name": "corr_95_d15", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.4841, "best_value": 0.4841}, {"dataset_name": "corr_50_d10", "final_value": 0.0002, "best_value": 0.0002}, {"dataset_name": "corr_50_d15", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "corr_75_d5", "final_value": 0.0946, "best_value": 0.0946}, {"dataset_name": "corr_75_d10", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "corr_75_d15", "final_value": 0.3285, "best_value": 0.3285}, {"dataset_name": "corr_95_d5", "final_value": 0.0294, "best_value": 0.0294}, {"dataset_name": "corr_95_d10", "final_value": 0.0001, "best_value": 0.0001}, {"dataset_name": "corr_95_d15", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test set", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.984, "best_value": 0.984}, {"dataset_name": "corr_50_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_50_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d5", "final_value": 0.982, "best_value": 0.982}, {"dataset_name": "corr_75_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_95_d5", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "corr_95_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_95_d15", "final_value": 1.0, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "train worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the training set", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.9851, "best_value": 0.9851}, {"dataset_name": "corr_50_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_50_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d5", "final_value": 0.984, "best_value": 0.984}, {"dataset_name": "corr_75_d10", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "corr_75_d15", "final_value": 0.9961, "best_value": 0.9961}, {"dataset_name": "corr_95_d5", "final_value": 0.9842, "best_value": 0.9842}, {"dataset_name": "corr_95_d10", "final_value": 0.9979, "best_value": 0.9979}, {"dataset_name": "corr_95_d15", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the validation set", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.9762, "best_value": 0.9762}, {"dataset_name": "corr_50_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_50_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d5", "final_value": 0.9798, "best_value": 0.9798}, {"dataset_name": "corr_75_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_95_d5", "final_value": 0.9881, "best_value": 0.9881}, {"dataset_name": "corr_95_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_95_d15", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.4796, "best_value": 0.4796}, {"dataset_name": "corr_50_d10", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "corr_50_d15", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "corr_75_d5", "final_value": 0.0507, "best_value": 0.0507}, {"dataset_name": "corr_75_d10", "final_value": 0.0067, "best_value": 0.0067}, {"dataset_name": "corr_75_d15", "final_value": 0.334, "best_value": 0.334}, {"dataset_name": "corr_95_d5", "final_value": 0.0272, "best_value": 0.0272}, {"dataset_name": "corr_95_d10", "final_value": 0.001, "best_value": 0.001}, {"dataset_name": "corr_95_d15", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.4841, "best_value": 0.4841}, {"dataset_name": "corr_50_d10", "final_value": 0.0002, "best_value": 0.0002}, {"dataset_name": "corr_50_d15", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "corr_75_d5", "final_value": 0.0946, "best_value": 0.0946}, {"dataset_name": "corr_75_d10", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "corr_75_d15", "final_value": 0.3285, "best_value": 0.3285}, {"dataset_name": "corr_95_d5", "final_value": 0.0294, "best_value": 0.0294}, {"dataset_name": "corr_95_d10", "final_value": 0.0001, "best_value": 0.0001}, {"dataset_name": "corr_95_d15", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test set", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.984, "best_value": 0.984}, {"dataset_name": "corr_50_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_50_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d5", "final_value": 0.982, "best_value": 0.982}, {"dataset_name": "corr_75_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_95_d5", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "corr_95_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_95_d15", "final_value": 1.0, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "train worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the training set", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.9851, "best_value": 0.9851}, {"dataset_name": "corr_50_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_50_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d5", "final_value": 0.984, "best_value": 0.984}, {"dataset_name": "corr_75_d10", "final_value": 0.998, "best_value": 0.998}, {"dataset_name": "corr_75_d15", "final_value": 0.9961, "best_value": 0.9961}, {"dataset_name": "corr_95_d5", "final_value": 0.9842, "best_value": 0.9842}, {"dataset_name": "corr_95_d10", "final_value": 0.9979, "best_value": 0.9979}, {"dataset_name": "corr_95_d15", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation worst-group accuracy", "lower_is_better": false, "description": "Worst-group accuracy on the validation set", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.9762, "best_value": 0.9762}, {"dataset_name": "corr_50_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_50_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d5", "final_value": 0.9798, "best_value": 0.9798}, {"dataset_name": "corr_75_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_95_d5", "final_value": 0.9881, "best_value": 0.9881}, {"dataset_name": "corr_95_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_95_d15", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.4796, "best_value": 0.4796}, {"dataset_name": "corr_50_d10", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "corr_50_d15", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "corr_75_d5", "final_value": 0.0507, "best_value": 0.0507}, {"dataset_name": "corr_75_d10", "final_value": 0.0067, "best_value": 0.0067}, {"dataset_name": "corr_75_d15", "final_value": 0.334, "best_value": 0.334}, {"dataset_name": "corr_95_d5", "final_value": 0.0272, "best_value": 0.0272}, {"dataset_name": "corr_95_d10", "final_value": 0.001, "best_value": 0.001}, {"dataset_name": "corr_95_d15", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.4841, "best_value": 0.4841}, {"dataset_name": "corr_50_d10", "final_value": 0.0002, "best_value": 0.0002}, {"dataset_name": "corr_50_d15", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "corr_75_d5", "final_value": 0.0946, "best_value": 0.0946}, {"dataset_name": "corr_75_d10", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "corr_75_d15", "final_value": 0.3285, "best_value": 0.3285}, {"dataset_name": "corr_95_d5", "final_value": 0.0294, "best_value": 0.0294}, {"dataset_name": "corr_95_d10", "final_value": 0.0001, "best_value": 0.0001}, {"dataset_name": "corr_95_d15", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Overall accuracy on the test set", "data": [{"dataset_name": "corr_50_d5", "final_value": 0.984, "best_value": 0.984}, {"dataset_name": "corr_50_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_50_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d5", "final_value": 0.982, "best_value": 0.982}, {"dataset_name": "corr_75_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_75_d15", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_95_d5", "final_value": 0.996, "best_value": 0.996}, {"dataset_name": "corr_95_d10", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "corr_95_d15", "final_value": 1.0, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_worst_group_accuracy.png", "../../logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_75_d10_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_95_d15_loss_curve.png", "../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_50_d15_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_95_d5_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_75_d5_loss_curve.png", "../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_50_d10_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_75_d10_loss_curve.png", "../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_50_d5_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_95_d10_loss_curve.png", "../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_75_d15_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_95_d15_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_50_d10_loss_curve.png", "../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_95_d10_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_95_d5_loss_curve.png", "../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_75_d15_loss_curve.png", "../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_75_d5_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_50_d15_loss_curve.png", "../../logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_50_d5_loss_curve.png"], ["../../logs/0-run/experiment_results/experiment_d8cfcdf8be9e46229e44a653b9b38d14_proc_17030/synthetic_wg_accuracy.png", "../../logs/0-run/experiment_results/experiment_d8cfcdf8be9e46229e44a653b9b38d14_proc_17030/synthetic_loss_curve.png"], [], ["../../logs/0-run/experiment_results/experiment_1c2c85e9136d4bc79e9d19e2cc69aa0a_proc_17031/synthetic_weighted_group_accuracy.png", "../../logs/0-run/experiment_results/experiment_1c2c85e9136d4bc79e9d19e2cc69aa0a_proc_17031/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_7d2ceea87c7346a2aa42a75794b7b182_proc_17029/synthetic_test_accuracy_bar.png", "../../logs/0-run/experiment_results/experiment_7d2ceea87c7346a2aa42a75794b7b182_proc_17029/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_7d2ceea87c7346a2aa42a75794b7b182_proc_17029/synthetic_wg_accuracy_curves.png"], ["../../logs/0-run/experiment_results/experiment_f8323db602414c9bbf09ab14c0ca8a84_proc_17030/synthetic_wg_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_f8323db602414c9bbf09ab14c0ca8a84_proc_17030/synthetic_test_accuracy_bar.png", "../../logs/0-run/experiment_results/experiment_f8323db602414c9bbf09ab14c0ca8a84_proc_17030/synthetic_loss_curve.png"], ["../../logs/0-run/experiment_results/experiment_f0c5353ab5d64aecb188e97426afb8dc_proc_17029/synthetic_wg_accuracy.png", "../../logs/0-run/experiment_results/experiment_f0c5353ab5d64aecb188e97426afb8dc_proc_17029/synthetic_loss_curve.png"], [], [], ["../../logs/0-run/experiment_results/experiment_c2103efb3bbf425dbe8de9f71022220b_proc_17031/synthetic_worst_group_accuracy.png", "../../logs/0-run/experiment_results/experiment_c2103efb3bbf425dbe8de9f71022220b_proc_17031/synthetic_test_accuracy.png", "../../logs/0-run/experiment_results/experiment_c2103efb3bbf425dbe8de9f71022220b_proc_17031/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_1a5dff6c407b4b118524d84902f8cdf2_proc_17030/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_1a5dff6c407b4b118524d84902f8cdf2_proc_17030/synthetic_wg_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_1a5dff6c407b4b118524d84902f8cdf2_proc_17030/synthetic_final_val_wg_accuracy.png"], ["../../logs/0-run/experiment_results/experiment_2b51ec4156e44e18b4d08c4ba1857e8c_proc_17029/synthetic_worst_group_accuracy.png", "../../logs/0-run/experiment_results/experiment_2b51ec4156e44e18b4d08c4ba1857e8c_proc_17029/synthetic_loss_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_747fec2aaf314e33a8225f037d912c14_proc_17029/synthetic_cluster2_wgacc_loss.png", "../../logs/0-run/experiment_results/experiment_747fec2aaf314e33a8225f037d912c14_proc_17029/synthetic_cluster1_wgacc_loss.png", "../../logs/0-run/experiment_results/experiment_747fec2aaf314e33a8225f037d912c14_proc_17029/synthetic_cluster4_wgacc_loss.png", "../../logs/0-run/experiment_results/experiment_747fec2aaf314e33a8225f037d912c14_proc_17029/synthetic_cluster8_wgacc_loss.png"], ["../../logs/0-run/experiment_results/experiment_c5fe4942dd39487d8fe7be4923a04b18_proc_17031/synthetic_test_accuracy.png", "../../logs/0-run/experiment_results/experiment_c5fe4942dd39487d8fe7be4923a04b18_proc_17031/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_c5fe4942dd39487d8fe7be4923a04b18_proc_17031/synthetic_wg_accuracy.png"], [], ["../../logs/0-run/experiment_results/experiment_d42e378278dd4b32b938201e1449a869_proc_17031/normalized_gradient_cluster_reweighting_synthetic_loss_curve.png", "../../logs/0-run/experiment_results/experiment_d42e378278dd4b32b938201e1449a869_proc_17031/raw_gradient_cluster_reweighting_synthetic_wg_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_d42e378278dd4b32b938201e1449a869_proc_17031/raw_gradient_cluster_reweighting_synthetic_loss_curve.png", "../../logs/0-run/experiment_results/experiment_d42e378278dd4b32b938201e1449a869_proc_17031/normalized_gradient_cluster_reweighting_synthetic_wg_accuracy_curve.png"], ["../../logs/0-run/experiment_results/experiment_6769e710853c4056871ba296f9aad9ce_proc_17029/synthetic_no_norm_accuracy.png", "../../logs/0-run/experiment_results/experiment_6769e710853c4056871ba296f9aad9ce_proc_17029/synthetic_with_norm_accuracy.png", "../../logs/0-run/experiment_results/experiment_6769e710853c4056871ba296f9aad9ce_proc_17029/synthetic_with_norm_loss.png", "../../logs/0-run/experiment_results/experiment_6769e710853c4056871ba296f9aad9ce_proc_17029/synthetic_no_norm_loss.png"], ["../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_75_d10_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_95_d15_loss_curve.png", "../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_50_d15_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_95_d5_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_75_d5_loss_curve.png", "../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_50_d10_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_75_d10_loss_curve.png", "../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_50_d5_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_95_d10_loss_curve.png", "../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_75_d15_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_95_d15_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_50_d10_loss_curve.png", "../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_95_d10_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_95_d5_loss_curve.png", "../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_75_d15_loss_curve.png", "../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_75_d5_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_50_d15_loss_curve.png", "../../logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_50_d5_loss_curve.png"], ["../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_75_d10_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_95_d15_loss_curve.png", "../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_50_d15_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_95_d5_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_75_d5_loss_curve.png", "../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_50_d10_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_75_d10_loss_curve.png", "../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_50_d5_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_95_d10_loss_curve.png", "../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_75_d15_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_95_d15_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_50_d10_loss_curve.png", "../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_95_d10_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_95_d5_loss_curve.png", "../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_75_d15_loss_curve.png", "../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_75_d5_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_50_d15_loss_curve.png", "../../logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_50_d5_loss_curve.png"], ["../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_75_d10_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_95_d15_loss_curve.png", "../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_50_d15_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_95_d5_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_75_d5_loss_curve.png", "../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_50_d10_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_75_d10_loss_curve.png", "../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_50_d5_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_95_d10_loss_curve.png", "../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_75_d15_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_95_d15_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_50_d10_loss_curve.png", "../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_95_d10_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_95_d5_loss_curve.png", "../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_75_d15_loss_curve.png", "../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_75_d5_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_50_d15_loss_curve.png", "../../logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_50_d5_loss_curve.png"], ["../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_95_d15_aggregated_loss_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_95_d15_aggregated_accuracy_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_50_d15_aggregated_accuracy_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_95_d5_aggregated_loss_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_75_d5_aggregated_loss_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_95_d10_aggregated_loss_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_75_d10_aggregated_accuracy_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_50_d5_aggregated_loss_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_75_d10_aggregated_loss_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_75_d15_aggregated_accuracy_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_95_d10_aggregated_accuracy_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_50_d10_aggregated_loss_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_50_d10_aggregated_accuracy_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_75_d5_aggregated_accuracy_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_75_d15_aggregated_loss_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_95_d5_aggregated_accuracy_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_50_d5_aggregated_accuracy_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_50_d15_aggregated_loss_curve.png"]], "plot_paths": [["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_worst_group_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_loss_curves.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_75_d10_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_95_d15_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_50_d15_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_95_d5_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_75_d5_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_50_d10_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_75_d10_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_50_d5_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_95_d10_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_75_d15_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_95_d15_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_50_d10_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_95_d10_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_95_d5_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_75_d15_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_75_d5_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_50_d15_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_50_d5_loss_curve.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_d8cfcdf8be9e46229e44a653b9b38d14_proc_17030/synthetic_wg_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_d8cfcdf8be9e46229e44a653b9b38d14_proc_17030/synthetic_loss_curve.png"], [], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_1c2c85e9136d4bc79e9d19e2cc69aa0a_proc_17031/synthetic_weighted_group_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_1c2c85e9136d4bc79e9d19e2cc69aa0a_proc_17031/synthetic_loss_curves.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_7d2ceea87c7346a2aa42a75794b7b182_proc_17029/synthetic_test_accuracy_bar.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_7d2ceea87c7346a2aa42a75794b7b182_proc_17029/synthetic_loss_curves.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_7d2ceea87c7346a2aa42a75794b7b182_proc_17029/synthetic_wg_accuracy_curves.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_f8323db602414c9bbf09ab14c0ca8a84_proc_17030/synthetic_wg_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_f8323db602414c9bbf09ab14c0ca8a84_proc_17030/synthetic_test_accuracy_bar.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_f8323db602414c9bbf09ab14c0ca8a84_proc_17030/synthetic_loss_curve.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_f0c5353ab5d64aecb188e97426afb8dc_proc_17029/synthetic_wg_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_f0c5353ab5d64aecb188e97426afb8dc_proc_17029/synthetic_loss_curve.png"], [], [], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_c2103efb3bbf425dbe8de9f71022220b_proc_17031/synthetic_worst_group_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_c2103efb3bbf425dbe8de9f71022220b_proc_17031/synthetic_test_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_c2103efb3bbf425dbe8de9f71022220b_proc_17031/synthetic_loss_curves.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_1a5dff6c407b4b118524d84902f8cdf2_proc_17030/synthetic_loss_curves.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_1a5dff6c407b4b118524d84902f8cdf2_proc_17030/synthetic_wg_accuracy_curves.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_1a5dff6c407b4b118524d84902f8cdf2_proc_17030/synthetic_final_val_wg_accuracy.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_2b51ec4156e44e18b4d08c4ba1857e8c_proc_17029/synthetic_worst_group_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_2b51ec4156e44e18b4d08c4ba1857e8c_proc_17029/synthetic_loss_curves.png"], [], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_747fec2aaf314e33a8225f037d912c14_proc_17029/synthetic_cluster2_wgacc_loss.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_747fec2aaf314e33a8225f037d912c14_proc_17029/synthetic_cluster1_wgacc_loss.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_747fec2aaf314e33a8225f037d912c14_proc_17029/synthetic_cluster4_wgacc_loss.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_747fec2aaf314e33a8225f037d912c14_proc_17029/synthetic_cluster8_wgacc_loss.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_c5fe4942dd39487d8fe7be4923a04b18_proc_17031/synthetic_test_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_c5fe4942dd39487d8fe7be4923a04b18_proc_17031/synthetic_loss_curves.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_c5fe4942dd39487d8fe7be4923a04b18_proc_17031/synthetic_wg_accuracy.png"], [], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_d42e378278dd4b32b938201e1449a869_proc_17031/normalized_gradient_cluster_reweighting_synthetic_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_d42e378278dd4b32b938201e1449a869_proc_17031/raw_gradient_cluster_reweighting_synthetic_wg_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_d42e378278dd4b32b938201e1449a869_proc_17031/raw_gradient_cluster_reweighting_synthetic_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_d42e378278dd4b32b938201e1449a869_proc_17031/normalized_gradient_cluster_reweighting_synthetic_wg_accuracy_curve.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_6769e710853c4056871ba296f9aad9ce_proc_17029/synthetic_no_norm_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_6769e710853c4056871ba296f9aad9ce_proc_17029/synthetic_with_norm_accuracy.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_6769e710853c4056871ba296f9aad9ce_proc_17029/synthetic_with_norm_loss.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_6769e710853c4056871ba296f9aad9ce_proc_17029/synthetic_no_norm_loss.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_75_d10_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_95_d15_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_50_d15_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_95_d5_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_75_d5_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_50_d10_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_75_d10_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_50_d5_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_95_d10_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_75_d15_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_95_d15_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_50_d10_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_95_d10_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_95_d5_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_75_d15_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_75_d5_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_50_d15_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/corr_50_d5_loss_curve.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_75_d10_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_95_d15_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_50_d15_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_95_d5_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_75_d5_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_50_d10_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_75_d10_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_50_d5_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_95_d10_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_75_d15_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_95_d15_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_50_d10_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_95_d10_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_95_d5_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_75_d15_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_75_d5_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_50_d15_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_50_d5_loss_curve.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_75_d10_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_95_d15_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_50_d15_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_95_d5_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_75_d5_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_50_d10_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_75_d10_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_50_d5_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_95_d10_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_75_d15_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_95_d15_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_50_d10_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_95_d10_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_95_d5_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_75_d15_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_75_d5_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_50_d15_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_50_d5_loss_curve.png"], ["experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_95_d15_aggregated_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_95_d15_aggregated_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_50_d15_aggregated_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_95_d5_aggregated_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_75_d5_aggregated_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_95_d10_aggregated_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_75_d10_aggregated_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_50_d5_aggregated_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_75_d10_aggregated_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_75_d15_aggregated_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_95_d10_aggregated_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_50_d10_aggregated_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_50_d10_aggregated_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_75_d5_aggregated_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_75_d15_aggregated_loss_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_95_d5_aggregated_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_50_d5_aggregated_accuracy_curve.png", "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/seed_aggregation_ad4e6b0ec1414fadbf21689e8e212558/corr_50_d15_aggregated_loss_curve.png"]], "plot_analyses": [[{"analysis": "Training worst-group accuracy curves show that with learning rate 0.0001 the model only improves slowly from ~0.30 to ~0.35 over six epochs. With lr=0.001 accuracy jumps quickly from ~0.90 to ~0.98 by epoch 2 and then plateaus around ~0.97\u20130.98. With lr=0.01 the model achieves near-perfect worst-group accuracy (~0.99\u20131.00) from the first epoch onward. The validation worst-group curves mirror the training trends closely, indicating stable generalization across these rates.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_worst_group_accuracy.png"}, {"analysis": "Training loss curves reveal that lr=0.0001 yields a minor decrease from ~0.70 to ~0.69, matching its slow accuracy gains. With lr=0.001 the loss drops sharply from ~0.58 to ~0.48 by epoch 3 before flattening, consistent with rapid accuracy improvements. The high lr=0.01 setting drives the loss close to zero immediately (~0.01) but then slightly increases, suggesting possible overshooting or instability. Validation losses follow similar patterns, confirming that the training dynamics transfer to held-out data.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_be4523f06c524c6bbaf8939619561351_proc_3804/synthetic_loss_curves.png"}], [{"analysis": "corr_50_d5 (corr=0.5, dim=5): Weighted Accuracy shows that with a very small learning rate (1e-4), both train and validation curves start low (~0.37/0.39) and slowly inch up to ~0.42/0.44 by epoch 5, indicating underfitting. A moderate rate (1e-3) jumps accuracy to ~0.95\u20130.99 on both splits by epoch 1 but sees a slight decay (~0.97) afterwards. A high rate (1e-2) immediately attains ~0.996 on training and ~0.997 on validation and remains stable. The high and moderate rates both recover the spurious structure quickly, but the highest LR gives the most consistent top performance.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_75_d10_accuracy_curve.png"}, {"analysis": "corr_50_d5 (corr=0.5, dim=5): Loss curves reveal underfitting at LR=1e-4: training drops from ~0.58\u21920.49 while validation barely moves from ~0.70. At LR=1e-3, loss declines to ~0.48 on both train and val, matching the sharp accuracy rise. At LR=1e-2, training loss collapses to near zero but validation loss creeps upward after epoch 1, signaling over-optimistic fitting and potential instability. LR=1e-3 offers the best balance of low loss and generalization.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_95_d15_loss_curve.png"}, {"analysis": "corr_50_d15 (corr=0.5, dim=15): Weighted Accuracy under LR=1e-4 remains stuck at ~0.50/0.49, showing severe underfitting in higher dimensional gradient space. LR=1e-3 and LR=1e-2 both climb to >0.97 by epoch 2; LR=1e-2 slightly outperforms in early epochs, but both effectively recover group structure. Dimensionality enlargement slows convergence for the small LR but has minimal effect once the rate is \u22651e-3.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_50_d15_accuracy_curve.png"}, {"analysis": "corr_50_d15 (corr=0.5, dim=15): Loss under LR=1e-4 decreases slowly from ~0.56\u21920.43 (train) while validation remains ~0.70. LR=1e-3 drops to ~0.47 train and ~0.43 val by epoch 2. LR=1e-2 collapses training loss to ~0.02 and validation to ~0.03 within one epoch. Again, LR=1e-3 strikes the best trade-off before any overfitting artifacts emerge.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_95_d5_accuracy_curve.png"}, {"analysis": "corr_75_d10 (corr=0.75, dim=10): Weighted Accuracy with LR=1e-4 stays very low (~0.28/0.29), indicating inability to discern latent groups under stronger spurious correlation. Both LR=1e-3 and 1e-2 achieve ~0.98\u20130.99 accuracy by epoch 1 and maintain it, showing robustness of moderate and high rates even under tougher correlation regimes. High rate edges out slightly but differences are marginal.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_75_d5_loss_curve.png"}, {"analysis": "corr_75_d10 (corr=0.75, dim=10): Loss for LR=1e-4 declines modestly from ~0.57\u21920.48 train, with val stuck at ~0.69. LR=1e-3 falls to ~0.48 on both by epoch 2. LR=1e-2 again drives training loss to zero but yields an almost zero validation loss, likely an overfitting artifact. LR=1e-3 is preferable for stable generalization.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_50_d10_accuracy_curve.png"}, {"analysis": "corr_95_d5 (corr=0.95, dim=5): Weighted Accuracy: LR=1e-4 fails entirely (~0.05/0.06), while LR=1e-3 ramps from ~0.85\u21920.99 by epoch 1. LR=1e-2 starts at ~0.99 and stays flat\u2014an almost ideal fit. Under extreme correlation, a very small LR cannot extract meaningful gradients, but rates \u22651e-3 succeed quickly.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_75_d10_loss_curve.png"}, {"analysis": "corr=95_d5 (corr=0.95, dim=5): Loss: LR=1e-4 train/val decreases only to ~0.56/0.67. LR=1e-3 brings both down to ~0.49/0.57. LR=1e-2 again collapses both to near zero, consistent with potential over-optimization. Moderate rate best stabilizes learning and generalization.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_50_d5_accuracy_curve.png"}, {"analysis": "corr_95_d15 (corr=0.95, dim=15): Weighted Accuracy: very small LR=1e-4 slowly creeps from ~0.17\u21920.23. LR=1e-3 jumps to ~0.69\u21920.84 on training and ~0.75\u21920.85 on validation by epoch 2, then converges to ~0.84/0.88. LR=1e-2 holds at ~1.00 across epochs, again raising suspicion of overfitting or collapse to a trivial solution.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_95_d10_loss_curve.png"}, {"analysis": "corr_95_d15 (corr=0.95, dim=15): Loss: LR=1e-4 declines to ~0.48 train while val stays ~0.69. LR=1e-3 reaches ~0.48 train and ~0.49 val by epoch 2. LR=1e-2 drives both losses to near zero instantly, echoing the risk of overfitting. Once more, LR=1e-3 offers the most consistent, generalizable reduction in loss.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_e2149b5d31be4c8dbe4670698772c687_proc_17029/corr_75_d15_accuracy_curve.png"}], [{"analysis": "Worst-group accuracy curves on the synthetic dataset show distinct behaviors for the three runs. Run0 starts at about 30% worst-group accuracy on training and steadily improves to near 90% by epoch 5, indicating a slow adaptation to underrepresented clusters. Run1 begins around 95% and converges to ~99% by epoch 5, showing strong and consistent performance from early epochs. Run2 remains almost flat at ~100% across all epochs, suggesting it either had oracle group labels or a near-perfect grouping from the start. Validation curves mirror training trends: run0 lags initially (around 32%) but catches up to ~88% by epoch 5; run1 and run2 maintain high validation worst-group accuracy (~92% to 99% for run1 and ~100% for run2). The gap between run0 and the others indicates the contribution of certain components missing in run0 (likely the clustering or robust optimization step). The near-ideal behavior of run2 implies that its configuration (possibly full UGC + best hyperparameters) recovers almost oracle-like performance.", "},{": " I accidentally started lumps of erroneous stuff. Let's scrap and rewrite call with correct minimal JSON. The ", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_d8cfcdf8be9e46229e44a653b9b38d14_proc_17030/synthetic_wg_accuracy.png"}], [], [{"analysis": "Weighted group accuracy curves show that at a learning rate of 0.0001, weighted accuracy remains stable around 0.92 during training and about 0.91 on validation, indicating slow improvement but consistent performance. At 0.001, training accuracy rises from roughly 0.46 at epoch 1 to about 0.53 by epoch 6, with validation following a similar trend from 0.44 to 0.54, reflecting moderate gains in group robustness but still far below optimal. A learning rate of 0.01 yields a rapid jump from 0.87 to 0.93 by epoch 2 in training, settling near 0.975 from epoch 3 onward; validation accuracy climbs from 0.90 to around 0.97 by epoch 3 and remains flat, indicating fast convergence to high worst-group performance without overfitting. This suggests that aggressive learning rates enable the gradient clustering mechanism to identify and rebalance underrepresented clusters early in training, substantially boosting worst-group accuracy.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_1c2c85e9136d4bc79e9d19e2cc69aa0a_proc_17031/synthetic_weighted_group_accuracy.png"}, {"analysis": "Loss curves reveal that 0.0001 achieves minimal loss reduction, staying around 0.48 in both training and validation. A rate of 0.001 sees training loss decline from 0.68 to about 0.64 and validation similarly, indicating moderate learning progress but suboptimal convergence. The 0.01 learning rate drops loss sharply from 0.38 at epoch 1 to approximately 0.22 by epoch 6 in both splits, mirroring the accuracy trends and demonstrating that higher rates accelerate optimization of core and spurious features. The close alignment of training and validation losses across all rates suggests limited overfitting, but only the highest rate provides a strong balance between fast convergence and robust worst-group performance, confirming its superiority for this synthetic benchmark.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_1c2c85e9136d4bc79e9d19e2cc69aa0a_proc_17031/synthetic_loss_curves.png"}], [{"analysis": "Test Accuracy per Learning Rate (synthetic dataset) shows that a learning rate of 0.0001 yields low performance (\u22480.48), whereas rates of 0.001 and 0.01 achieve near-perfect accuracy (\u22480.98 and \u22480.99, respectively). This indicates that too small a step size prevents effective optimization, while medium to large rates enable the model to fit the data well.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_7d2ceea87c7346a2aa42a75794b7b182_proc_17029/synthetic_test_accuracy_bar.png"}, {"analysis": "Loss Curves (synthetic dataset):\n\u2022 Training Loss: at lr=0.0001 it decreases only marginally from \u22480.70 to \u22480.69, indicating underfitting. At lr=0.001 it steadily falls from \u22480.57 to \u22480.47, showing stable convergence. At lr=0.01 it collapses to near zero by epoch 2, suggesting very aggressive fitting.\n\u2022 Validation Loss: at lr=0.0001 it barely changes, mirroring underfitting. At lr=0.001 it drops in parallel with training loss, reaching \u22480.47 by epoch 5. At lr=0.01 it bottoms out around \u22480.03 then slightly rises, signaling potential overfitting or instability at high step sizes.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_7d2ceea87c7346a2aa42a75794b7b182_proc_17029/synthetic_loss_curves.png"}, {"analysis": "WG Accuracy Curves (synthetic dataset):\n\u2022 Training WG Accuracy: lr=0.0001 stalls around 0.30\u21920.35, failing to improve worst-group performance. lr=0.001 climbs from \u22480.93 to \u22480.99, and lr=0.01 from \u22480.99 to \u22480.995, indicating strong recovery for underrepresented clusters.\n\u2022 Validation WG Accuracy: lr=0.0001 moves only from \u22480.32 to \u22480.355. Both lr=0.001 and lr=0.01 rise into the high 0.90s (\u22480.92\u21920.975 and \u22480.97\u21920.985), demonstrating robust generalization to worst-case groups when the learning rate is sufficiently large.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_7d2ceea87c7346a2aa42a75794b7b182_proc_17029/synthetic_wg_accuracy_curves.png"}], [], [{"analysis": "Learning rates have a dramatic effect on worst\u2010group accuracy. At 1e\u20104, both training and validation worst\u2010group accuracy start very low (around 0.30/0.32) and slowly creep up only to \u22480.35/0.36 by epoch 5, indicating underfitting. At 1e\u20103, worst\u2010group accuracy begins at \u22480.94/0.92 and rises steadily to \u22480.97/0.97, showing a healthy balance of convergence speed and generalization. At 1e\u20102, worst\u2010group accuracy is already near perfect (\u22480.99/0.98) from epoch 0 and remains flat thereafter, suggesting extremely rapid fitting (and possible over\u2010reliance on spurious signals in this synthetic task).", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_f0c5353ab5d64aecb188e97426afb8dc_proc_17029/synthetic_wg_accuracy.png"}, {"analysis": "Loss curves mirror the accuracy trends. With lr=1e\u20104, both training and validation loss start around 0.70 and decrease only marginally to \u22480.69, indicating slow learning. With lr=1e\u20103, loss drops from \u22480.56/0.57 to \u22480.47/0.47, reflecting steady learning without collapse. With lr=1e\u20102, training loss plummets to near\u2010zero by epoch 1 while validation loss bottoms out around 0.03 before drifting up to \u22480.04, signaling potential overfitting or memorization under this high learning\u2010rate regime.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_f0c5353ab5d64aecb188e97426afb8dc_proc_17029/synthetic_loss_curve.png"}], [], [], [{"analysis": "Worst-group accuracy at lr=0.0001 remains low (~30\u201336%) after five epochs, reflecting underfitting. At lr=0.001, it leaps from ~94% at epoch 0 to ~97% by epoch 1 and plateaus around 98% (train) and 97% (val), showing rapid improvement. At lr=0.01, it starts near 99% and remains flat, indicating immediate convergence to near-perfect robustness.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_c2103efb3bbf425dbe8de9f71022220b_proc_17031/synthetic_worst_group_accuracy.png"}, {"analysis": "Test accuracy at lr=0.0001 is only ~48%, confirming poor generalization. At lr=0.001 it jumps to ~98%, and at lr=0.01 it reaches ~99%, mirroring worst-group accuracy trends and validating that medium and high learning rates yield near-optimal performance on the synthetic dataset.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_c2103efb3bbf425dbe8de9f71022220b_proc_17031/synthetic_test_accuracy.png"}, {"analysis": "At lr=0.0001 train and val losses stay at ~0.70, signifying failure to learn. At lr=0.001 losses drop from ~0.55/0.57 at epoch 0 to ~0.48 by epoch 2 and then plateau, aligning with the accuracy gains. At lr=0.01 train loss plummets to <0.02 by epoch 1 (val loss ~0.03\u20130.04), reflecting very rapid optimization and corroborating the almost-perfect accuracy.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_c2103efb3bbf425dbe8de9f71022220b_proc_17031/synthetic_loss_curves.png"}], [{"analysis": "Plot 1 (Loss Curves): Three learning rates exhibit distinct convergence patterns. With lr=0.0001, both training and validation losses decrease slowly from ~0.70 to ~0.65 over five epochs, indicating underfitting. With lr=0.001, training loss drops steeply from ~0.53 to ~0.06 and validation loss mirrors this trend from ~0.54 to ~0.07, suggesting balanced convergence. With lr=0.01, training loss plummets to near zero within two epochs while validation loss starts around 0.02 and gradually increases to ~0.03\u20130.04, revealing a widening train-validation gap and signs of overfitting.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_1a5dff6c407b4b118524d84902f8cdf2_proc_17030/synthetic_loss_curves.png"}, {"analysis": "Plot 2 (Worst-group Accuracy Curves): Under lr=0.0001, worst-group accuracy climbs slowly from ~0.30 to ~0.81 on both train and validation, mirroring the slow loss decrease. Under lr=0.001, worst-group accuracy rapidly rises from ~0.90 to ~0.995 on train and from ~0.88 to ~0.98 on validation, demonstrating strong robust generalization. Under lr=0.01, worst-group accuracy starts near ~0.99, peaks around ~0.995 on train, but validation peaks slightly below (~0.99) then plateaus or dips marginally, reflecting high but less stable performance.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_1a5dff6c407b4b118524d84902f8cdf2_proc_17030/synthetic_wg_accuracy_curves.png"}, {"analysis": "Plot 3 (Final Validation Worst-group Accuracy): Final epoch bars show ~0.81 for lr=1e-4, ~0.98 for lr=1e-3, and ~0.99 for lr=1e-2, confirming that mid-to-high learning rates substantially improve worst-group generalization. The jump from 1e-4 to 1e-3 is dramatic, while the gain from 1e-3 to 1e-2 is marginal, with indications of slight instability at the highest rate.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_1a5dff6c407b4b118524d84902f8cdf2_proc_17030/synthetic_final_val_wg_accuracy.png"}], [{"analysis": "Worst-group accuracy for run0 remains low (~0.30\u20130.35) on both train and validation, indicating that the baseline model fails to handle underrepresented groups. run1 starts around 0.93 and climbs to ~0.98 by epoch 5, showing that unsupervised gradient clustering effectively boosts worst-group performance. run2 achieves near-perfect accuracy (~0.99) from the start and maintains it, likely representing an oracle or upper bound. The closeness of run1 to run2 suggests UGC nearly matches oracle-group methods without group labels, and the consistent gap between run0 and run1 demonstrates the method\u2019s robustness and ability to recover latent groups early in training.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_2b51ec4156e44e18b4d08c4ba1857e8c_proc_17029/synthetic_worst_group_accuracy.png"}, {"analysis": "Training loss for run0 hovers around 0.7 with minimal decline, reflecting underfitting on spurious-minority samples. run1 sees loss drop from ~0.56 to ~0.47 on both train and validation, indicating improved fitting across all clusters while avoiding overfitting (train and val losses almost overlap). run2 exhibits near-zero loss (~0.02 train, ~0.03 val), consistent with perfect group-distinction. The stable gap between run1 and run0 losses confirms that applying group-robust optimization over gradient-based clusters provides meaningful regularization and convergence benefits compared to standard ERM.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_2b51ec4156e44e18b4d08c4ba1857e8c_proc_17029/synthetic_loss_curves.png"}], [], [{"analysis": "cluster_count=2: At lr=1e-04, worst-group validation accuracy remains near 6% despite training accuracy near 99%, indicating severe underfitting on minority clusters. lr=1e-03 yields a rapid jump to \u223c90% validation accuracy by epoch 1 and then plateaus around 93\u201394%, with training accuracy converging slightly higher. lr=1e-02 achieves almost perfect worst-group accuracy on both train and val (>98%) within one epoch. Loss curves mirror this: low lr stalls around 0.69\u20130.70, medium lr drops to \u223c0.50, and high lr reaches near-zero training loss and very low validation loss (~0.02\u20130.03).", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_747fec2aaf314e33a8225f037d912c14_proc_17029/synthetic_cluster2_wgacc_loss.png"}, {"analysis": "cluster_count=1: lr=1e-04 leads to very poor worst-group accuracy (30\u201335%) with high loss (~0.7). lr=1e-03 shows significant improvement, reaching \u223c95% validation accuracy by epoch 1 and \u223c96% thereafter; training curves follow closely. lr=1e-02 again delivers near-perfect accuracy (>98%) and minimal loss. Stability across epochs is high for medium and large lr, while small lr fails to escape high-loss region.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_747fec2aaf314e33a8225f037d912c14_proc_17029/synthetic_cluster1_wgacc_loss.png"}, {"analysis": "cluster_count=4: lr=1e-04 yields moderate worst-group val accuracy (42\u201349%) and slow improvement across epochs, indicating insufficient gradient clustering under low step size. lr=1e-03 quickly reaches \u223c98% validation accuracy by epoch 1 and maintains it. lr=1e-02 has similar perfect performance. Loss for small lr remains around 0.69, while medium and high lr reduce loss to around 0.48 and near zero, respectively.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_747fec2aaf314e33a8225f037d912c14_proc_17029/synthetic_cluster4_wgacc_loss.png"}, {"analysis": "cluster_count=8: lr=1e-04 again shows limited gains (38\u201353%) over epochs, though slightly better than cluster_count=2 at the same lr. lr=1e-03 obtains \u223c97\u201398% worst-group accuracy almost immediately and holds steady. lr=1e-02 achieves >99% worst-group accuracy with near-zero training loss. The validation loss trends are consistent: low lr stuck at ~0.65, medium lr around 0.52, high lr near zero.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_747fec2aaf314e33a8225f037d912c14_proc_17029/synthetic_cluster8_wgacc_loss.png"}], [{"analysis": "Test accuracy remains nearly constant around 0.99 across all three weight decay settings (0, 1e-4, 1e-3). There is no significant degradation or improvement in overall test accuracy when tuning weight decay on this synthetic dataset, indicating that weight decay does not strongly affect average performance here.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_c5fe4942dd39487d8fe7be4923a04b18_proc_17031/synthetic_test_accuracy.png"}, {"analysis": "Training and validation loss curves show that higher weight decay leads to lower final loss. With no weight decay, both train and val losses plateau around 0.50. Introducing a small decay (1e-4) brings the loss down further to roughly 0.48 on the validation set. A larger decay (1e-3) produces the fastest initial drop and the lowest asymptotic loss (around 0.467). All settings converge quickly by epoch 3, but heavier regularization yields slightly better generalization in terms of loss.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_c5fe4942dd39487d8fe7be4923a04b18_proc_17031/synthetic_loss_curves.png"}, {"analysis": "Worst-group accuracy exhibits a clear dependence on weight decay. Without decay, worst-group accuracy rises from 0.98 at epoch 0 to about 0.987 on validation. With moderate decay (1e-4), it starts lower (~0.90) and only recovers to about 0.962, underperforming both the no-decay and high-decay settings. Strong decay (1e-3) achieves the highest and most stable worst-group accuracy (~0.99) from early epochs onward on both train and validation. This suggests that heavy weight decay best mitigates spurious correlations for the hardest subgroups.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_c5fe4942dd39487d8fe7be4923a04b18_proc_17031/synthetic_wg_accuracy.png"}], [], [{"analysis": "The normalized\u2010reweighting variant (average loss curves) shows three very different trajectories across seeds: one seed barely moves (Run 1 stays at \u00ac0.7 loss), a second seed drops to roughly 0.5 after epoch 1 then plateaus, and the third seed collapses almost immediately to near zero. Validation losses mirror training, indicating that the large seed\u2010to\u2010seed variation in cluster\u2010based weights is not overfitting\u2014good or bad cluster partitions drive consistent generalization behavior. The bulk of the progress happens in the first epoch, with all subsequent epochs yielding negligible marginal improvement.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_d42e378278dd4b32b938201e1449a869_proc_17031/normalized_gradient_cluster_reweighting_synthetic_loss_curve.png"}, {"analysis": "Under raw cluster reweighting (worst-group accuracy), two of three seeds achieve very strong worst\u2010group performance (>0.95 by epoch 2), while one seed barely improves beyond random (~0.30\u21920.35). The good seeds quickly recover and sustain high worst\u2010group accuracy; the poor seed shows a slow climb but flattens out far below. This demonstrates that, without normalization, the algorithm often recovers robust group performance but remains highly sensitive to the initial gradient\u2010clustering outcome.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_d42e378278dd4b32b938201e1449a869_proc_17031/raw_gradient_cluster_reweighting_synthetic_wg_accuracy_curve.png"}, {"analysis": "Looking at raw reweighting again through the lens of average loss confirms that normalization had almost no effect on the overall loss\u2010minimization dynamics. All three seeds reproduce the same loss curves seen in the normalized\u2010loss plot: one stuck at ~0.7, one at ~0.5, and one converging to near zero. In other words, whether or not cluster weights are normalized makes no measurable dent in the model\u2019s ability to reduce global loss.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_d42e378278dd4b32b938201e1449a869_proc_17031/raw_gradient_cluster_reweighting_synthetic_loss_curve.png"}, {"analysis": "Applying normalization and then measuring worst\u2010group accuracy reveals a critical failure mode: one seed\u2019s worst\u2010group accuracy collapses to near zero (~5\u20137%), another drops slightly (to ~0.90), while the best seed remains near perfect. This contrasts sharply with raw reweighting, showing that normalization can catastrophically underweight some clusters and destroy robustness in some runs. It underscores that normalized cluster weights amplify seed\u2010driven instability and can undo the benefits of cluster\u2010robust training.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_d42e378278dd4b32b938201e1449a869_proc_17031/normalized_gradient_cluster_reweighting_synthetic_wg_accuracy_curve.png"}], [{"analysis": "synthetic_no_norm WG Accuracy Curves (Training and Validation):  Tiny learning rate (1e-4) yields virtually no improvement, stuck around 6% worst-group accuracy. Moderate rate (1e-3) jumps from ~56% to ~67% by epoch\u20092 and then plateaus, showing reasonable but suboptimal robustness. High rate (1e-2) starts near 98% and hovers just below perfect on both training and validation, indicating that without normalization a large step size can quickly memorize or exploit spurious patterns to maximize worst-group performance. Accuracy curves are nearly identical for training and validation, suggesting minimal overfitting of group-specific features under no-norm conditions.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_6769e710853c4056871ba296f9aad9ce_proc_17029/synthetic_no_norm_accuracy.png"}, {"analysis": "synthetic_with_norm WG Accuracy Curves (Training and Validation):  Very low rate (1e-4) again fails, plateauing at ~30\u201335%. Moderate rate (1e-3) achieves ~90% by epoch\u20090, then climbs to ~96% by epoch\u20095, substantially outperforming its no-norm counterpart. High rate (1e-2) delivers ~98\u201399% worst-group accuracy almost immediately, with minor fluctuations but stable generalization. Normalization amplifies the benefit of moderate learning rates, leading to rapid worst-group gains and tighter train\u2013validation alignment compared to no-norm runs.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_6769e710853c4056871ba296f9aad9ce_proc_17029/synthetic_with_norm_accuracy.png"}, {"analysis": "synthetic_with_norm Loss Curves (Training and Validation):  At 1e-4 the model barely learns (loss ~0.7 constant). 1e-3 yields smooth, monotonic drop in training loss from ~0.58 to ~0.48 and validation loss from ~0.59 to ~0.48, matching its WG accuracy improvements. 1e-2 drives training loss to near zero (~0.01\u21920.009) but validation loss creeps upward from ~0.02 to ~0.04, hinting at mild overfitting of normalized gradients despite very high worst-group accuracy. Normalization stabilizes convergence for medium rates but extremely high rates risk a small validation loss increase.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_6769e710853c4056871ba296f9aad9ce_proc_17029/synthetic_with_norm_loss.png"}, {"analysis": "synthetic_no_norm Loss Curves (Training and Validation):  Rate 1e-4 again shows negligible progress (loss ~0.7). Rate 1e-3 steadily reduces loss from ~0.54 to ~0.46 on both splits, in line with its moderate WG accuracy. Rate 1e-2 yields a U-shaped loss: training loss drops from ~0.055 at epoch\u20090 to ~0.025 at epoch\u20093 then rises back toward ~0.055 by epoch\u20095; validation loss mirrors this trend. This indicates unstable convergence or early overfitting when skipping normalization at high rates. Overall, moderate rates are safer without norm but deliver lower worst-group performance.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_6769e710853c4056871ba296f9aad9ce_proc_17029/synthetic_no_norm_loss.png"}], [], [{"analysis": "corr_50_d5 (corr=0.5, dim=5): Weighted Accuracy shows that learning rates of 0.001 and 0.01 both achieve very high training and validation accuracy (>97%) by epoch 1. The higher rate (0.01) plateaus slightly faster, while 0.001 catches up by epoch 2. A rate of 0.0001 underfits badly, reaching only ~42% validation accuracy. This indicates that moderate-to-high learning rates are necessary to capture the signal when spurious correlation is 50% in a 5-dimensional setting.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_75_d10_accuracy_curve.png"}, {"analysis": "corr_50_d15 (corr=0.5, dim=15): Weighted Accuracy again rises sharply for lr=0.001 and lr=0.01, both converging above 97% accuracy by epoch 1\u20132. The lr=0.01 curve is essentially flat at ~100% from the start, while lr=0.001 slowly edges up to ~98%. Learning at 1e-4 remains stuck near 50%, confirming that low rates fail to disentangle core from spurious features in higher dimensions under moderate correlation.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_95_d15_loss_curve.png"}, {"analysis": "corr_75_d10 (corr=0.75, dim=10): Weighted Accuracy for lr=0.001 and 0.01 reaches ~99% within one epoch, with only minimal differences thereafter. The lr=0.0001 condition again underperforms (around 28% validation accuracy), indicating severe underfitting. Increasing correlation to 75% doesn\u2019t change the qualitative picture: one needs a sufficiently large learning rate to learn robustly under UGC.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_50_d15_accuracy_curve.png"}, {"analysis": "corr_95_d5 (corr=0.95, dim=5): Weighted Accuracy shows nearly identical behavior for lr=0.001 and lr=0.01\u2014both hit ~99% by epoch 1 and hold. lr=0.0001 stalls at ~5%. Extremely strong spurious correlation demands aggressive learning rates to separate gradients early, otherwise the model collapses on the dominant feature and fails to generalize.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_95_d5_accuracy_curve.png"}, {"analysis": "corr_95_d15 (corr=0.95, dim=15): Weighted Accuracy diverges more noticeably between lr=0.001 (peaking near 88% by epoch 5) and lr=0.01 (holding ~100% across all epochs). The gap shows that, in higher-dimensional spurious regimes, even 1e-3 is only partially sufficient, while 1e-2 fully exploits gradient clustering to recover robust pseudo-groups. The smallest rate remains an underfitting failure at ~23%.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_75_d5_loss_curve.png"}, {"analysis": "corr_50_d5 (corr=0.5, dim=5): Loss Curve illustrates that lr=0.01 drives training loss down to nearly zero (0.02\u21920.06) and validation loss very low (0.04\u21920.11), matching its high accuracy. lr=0.001 yields a healthy descent (train 0.55\u21920.49, val 0.58\u21920.48). The 1e-4 setting barely budges, with train loss ~0.58\u21920.42 and val stuck ~0.70\u21920.69, confirming underfitting.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_50_d10_accuracy_curve.png"}, {"analysis": "corr_50_d15 (corr=0.5, dim=15): Loss Curve patterns repeat: lr=0.01 collapses losses to near zero in both train and val, lr=0.001 converges to ~0.42\u20130.43, and lr=0.0001 remains high (~0.70 train, ~0.69 val). This shows that higher dimension doesn\u2019t alter the need for a strong step size to sculpt gradient clusters effectively.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_75_d10_loss_curve.png"}, {"analysis": "corr_75_d10 (corr=0.75, dim=10): Loss Curve again separates three regimes. lr=0.01 pushes training loss to ~0 and validation loss to ~0.01 by epoch 5. lr=0.001 settles at train ~0.48, val ~0.48. lr=0.0001 underfits with train ~0.57, val ~0.68. The consistency across dimensions and correlation levels underscores that sufficient learning rate is the dominant factor for early gradient clustering.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_50_d5_accuracy_curve.png"}, {"analysis": "corr_95_d5 (corr=0.95, dim=5): Loss Curve shows lr=0.01 achieving very low training (0.02\u21920.03) and validation (0.03\u21920.03) losses, lr=0.001 reducing to train ~0.57\u21920.56 and val ~0.67\u21920.56, and lr=0.0001 remaining flat at train ~0.67, val ~0.68. Strong spurious signals again require high lr to carve out meaningful clusters.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_95_d10_loss_curve.png"}, {"analysis": "corr_95_d15 (corr=0.95, dim=15): Loss Curve highlights that only lr=0.01 drives both train and val loss near zero. lr=0.001 converges to train ~0.47, val ~0.49 and lr=0.0001 remains at train ~0.70, val ~0.69. This again confirms the critical role of a high learning rate in high-dimensional, highly spurious settings for effective gradient clustering and subsequent group-robust performance.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/corr_75_d15_accuracy_curve.png"}], [{"analysis": "corr_50_d5 (corr=0.5, dim=5): Weighted Accuracy shows that learning rate 0.0001 severely underfits, plateauing around 0.42 train and 0.44 val by epoch 5. With lr=0.001, the curves jump to ~0.98 train and ~0.975 val by epoch 5. lr=0.01 drives the fastest convergence, pushing train accuracy to ~1.0 and val to ~0.98 within two epochs, then gently declining to ~0.975. This indicates that a more aggressive step size yields both speed and top-end performance for this setting.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_75_d10_accuracy_curve.png"}, {"analysis": "corr_50_d5 (corr=0.5, dim=5): Loss Curve confirms the same trend. lr=0.0001 yields high losses (~0.7\u21920.62 train, ~0.7\u21920.69 val), lr=0.001 brings losses down to ~0.58\u21920.49 on both splits, while lr=0.01 collapses train loss to near-zero (~0.02\u21920.07) and val loss to ~0.03\u21920.11. The largest learning rate attains the lowest losses fastest without apparent instability.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_95_d15_loss_curve.png"}, {"analysis": "corr_50_d15 (corr=0.5, dim=15): Weighted Accuracy at lr=0.0001 stays flat around 0.51 train and 0.495 val. lr=0.001 jumps to ~0.97 train and ~0.99 val by epoch 5. lr=0.01 again dominates, achieving ~1.0 train and val accuracy by epoch 1 and holding it. Increasing embedding dimension slightly sharpens convergence but does not change the ordering of learning rates.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_50_d15_accuracy_curve.png"}, {"analysis": "corr_50_d15 (corr=0.5, dim=15): Loss Curve shows lr=0.0001 with persistently high loss (~0.7), lr=0.001 descending moderately (~0.55\u21920.48), and lr=0.01 driving losses to near-zero on both train and val within one epoch. The largest step size proves most effective at driving down loss quickly.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_95_d5_accuracy_curve.png"}, {"analysis": "corr_95_d5 (corr=0.95, dim=5): Weighted Accuracy for lr=0.0001 remains very low (~0.05 train, 0.065 val). lr=0.001 recovers strongly (~0.98 train, 0.98 val by epoch 3). lr=0.01 pulls even higher, reaching ~0.995 train and ~0.99 val. Stronger spurious correlation accelerates model performance, but the learning\u2010rate hierarchy persists.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_75_d5_loss_curve.png"}, {"analysis": "corr_95_d5 (corr=0.95, dim=5): Loss Curve again ranks lr=0.01 best (train ~0.03\u21920.04, val ~0.025\u21920.03), lr=0.001 intermediate (~0.63\u21920.56 train/val), and lr=0.0001 worst (~0.7\u21920.67). No instability observed with the highest rate.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_50_d10_accuracy_curve.png"}, {"analysis": "corr_95_d15 (corr=0.95, dim=15): Weighted Accuracy shows lr=0.0001 underfitting (~0.17\u21920.24 train/val), lr=0.001 strong recovery (~0.70\u21920.84 train, ~0.75\u21920.87 val), and lr=0.01 perfects accuracy (~1.0 train/val by epoch 1). High dimension plus strong correlation yields rapid saturation at high rates.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_75_d10_loss_curve.png"}, {"analysis": "corr_95_d15 (corr=0.95, dim=15): Loss Curve underlines that lr=0.01 collapses losses to near zero almost immediately. lr=0.001 drops to ~0.6\u21920.48 and then plateaus (~0.49), while lr=0.0001 remains stuck near 0.7. Highest lr again excels.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_50_d5_accuracy_curve.png"}, {"analysis": "corr_75_d10 (corr=0.75, dim=10): Weighted Accuracy under lr=0.0001 stays around 0.28 train/val, lr=0.001 climbs to ~0.99 train/val by epoch 3, and lr=0.01 hits ~0.995 train/val even faster. The mid\u2010range correlation and dimension maintain the same learning\u2010rate order.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_95_d10_loss_curve.png"}, {"analysis": "corr_75_d10 (corr=0.75, dim=10): Loss Curve shows lr=0.0001 and 0.001 descending to ~0.48 on both splits by epoch 5; lr=0.01 reduces training loss to ~0.005 and val to ~0.03 within two epochs. The most aggressive rate remains superior for fast, low\u2010loss convergence.", "plot_path": "experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/corr_75_d15_accuracy_curve.png"}], []], "vlm_feedback_summary": ["Larger learning rates accelerate convergence of both loss minimization and\nworst-group accuracy, enabling the model to quickly exploit gradient clustering\nfor robust subgroup performance. Medium lr balances speed and stability, while\nvery low lr fails to adapt group weights effectively within six epochs, and very\nhigh lr risks potential instability despite achieving nearly perfect worst-group\nmetrics.", "Across all correlation and dimension configurations, a moderate learning rate\n(1e-3) consistently provides the best compromise between rapid convergence and\nstable generalization, whereas an overly small rate (1e-4) underfits and an\noverly large rate (1e-2) risks overfitting or unstable collapse. Increasing\nspurious correlation and gradient dimensionality slows convergence for small\nrates but is well handled by rates \u22651e-3. The results suggest that UGC\u2019s pseudo-\ngroup recovery is robust when trained with a moderately high learning rate, but\ncareful tuning is crucial to avoid over- or under-fitting.", "Run1 and Run2 outperform Run0 significantly in worst-group accuracy and rapidly\nreduce loss. Run2 achieves near-perfect group separation and robust\ngeneralization, while Run1 is close behind. Run0's slow improvement underscores\nthe importance of the missing component (likely gradient clustering + Group\nDRO). These results confirm that the full UGC pipeline is critical for robust\nworst-group performance without true group labels.", "[]", "Higher learning rates drive rapid cluster separation and rebalancing, yielding\nsignificant worst-group accuracy gains. Mid-level rates lead to moderate\nimprovements, while very low rates converge too slowly to be effective. Loss\ntrajectories align with accuracy, showing substantial optimization only at high\nrates. Overall, a learning rate of 0.01 is recommended for UGC on this synthetic\ntask due to its fast convergence and robust generalization.", "Learning rate critically affects convergence and spurious-correlation\nmitigation: a very small rate underfits and fails to separate latent groups,\nwhereas medium (0.001) and high (0.01) rates yield both high overall and worst-\ngroup accuracy. lr=0.001 offers the best balance between stable convergence and\nlow overfitting risk.", "[]", "Medium learning rate (1e-3) achieves the best trade-off between convergence and\nrobust worst-group performance, whereas too low (<1e-4) underfits and too high\n(1e-2) converges too aggressively with signs of overfitting.", "[]", "[]", "Low learning rate (0.0001) underfits with high loss and poor accuracy. Moderate\n(0.001) and high (0.01) rates converge quickly to near-perfect worst-group and\ntest accuracy, with losses dropping accordingly. The best trade-off appears at\nlr=0.01 for fastest convergence and highest performance.", "Learning rate critically impacts both convergence and worst-group performance.\nVery low rates lead to underfitting and poor worst-group accuracy, while very\nhigh rates accelerate training but introduce train-validation loss gaps and\nslight instability in worst-group metrics despite near-perfect accuracy. A mid-\nrange rate (1e-3) strikes the best balance, achieving strong, stable worst-group\naccuracy with matched loss curves. Recommend using lr=1e-3 as default and\nmonitoring overfitting when exploring higher rates in ablations.", "UGC (run1) substantially improves worst-group accuracy and reduces loss versus\nERM (run0), closely tracking oracle performance (run2) on a synthetic benchmark,\ndemonstrating its effectiveness for spurious-correlation mitigation without\ngroup labels.", "[]", "Worst-group performance is highly sensitive to learning rate and cluster count.\nLow lr fails across all cluster settings, while lr=1e-02 yields near-perfect\naccuracy and minimal loss. Medium lr (1e-03) also consistently achieves high\nrobustness (>90%) except for the smallest cluster counts under more challenging\nsplits. Increasing cluster_count from 1 to 8 improves the method\u2019s capacity to\ncapture spurious structure but only when paired with a sufficiently large\nlearning rate. Overall, UGC demands careful lr tuning. Epoch-wise curves\nstabilize by epoch 1\u20132 for mid/high lr, indicating early convergence of gradient\nclustering.", "Overall test accuracy is insensitive to weight decay, but heavier decay\nconsistently lowers loss and yields the best worst-group performance. Moderate\ndecay harms subgroup performance, so a stronger regularization is recommended\nfor robustness.", "[]", "Seeds dominate outcomes. Raw cluster reweighting usually recovers strong worst-\ngroup accuracy but is seed\u2010sensitive. Normalization greatly increases\ninstability, undercutting worst-group performance in some runs, while leaving\naverage\u2010loss dynamics nearly unchanged.", "Without feature\u2010norm, only very large learning rates achieve high worst\u2010group\naccuracy but at the cost of unstable loss behavior; moderate rates converge\nsafely but underperform. Incorporating normalization dramatically boosts\nworst\u2010group performance at moderate and high rates, yielding rapid, stable gains\nand well\u2010aligned train/validation curves. Loss patterns confirm that\nnormalization controls overfitting and improves gradient clustering stability,\nwhile learning\u2010rate choice remains critical for robust generalization.", "[]", "All plots consistently show that very low learning rate (1e-4) underfits across\ncorrelation and dimension settings. Moderate rate (1e-3) achieves good but\nincomplete convergence, especially in the hardest (high corr/high dim) cases.\nThe highest rate (1e-2) universally yields the fastest and most complete\noptimization, driving losses to near zero and accuracies to near 100%. These\ntrends suggest that successful Unsupervised Gradient Clustering critically\ndepends on choosing a learning rate that is large enough to amplify gradient\ndifferences early in training.", "Learning\u2010rate ablation across varying correlation strengths and embedding\ndimensions consistently shows that lr=0.01 yields the fastest convergence,\nhighest weighted accuracy, and lowest loss. Lower rates underfit and medium\nrates converge more slowly. This suggests adopting a higher initial learning\nrate in UGC training pipelines to maximize worst\u2010group performance and\nefficiency.", "[]"], "exec_time": [6.182882070541382, 64.35127925872803, 2.4400367736816406, 8.904810190200806, 4.77895975112915, 2.448057174682617, 3.746037483215332, 2.4780635833740234, 2.142265558242798, 6.1094810962677, 2.467794895172119, 8.292042016983032, 2.451171875, 8.462630987167358, 29.81704092025757, 8.525097131729126, 2.5154993534088135, 15.513256549835205, 15.431446075439453, 64.18646574020386, 66.38034343719482, 64.13836812973022, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['synthetic']"], ["['corr_50_d5'", "'corr_50_d15'", "'corr_75_d10'", "'corr_95_d5'", "'corr_95_d15']"], ["synthetic"], [], ["['synthetic']"], ["['synthetic']"], [], ["['synthetic']"], [], [], ["['synthetic']"], ["\"\""], ["[\"synthetic\"]"], [], ["[synthetic_cluster1", "synthetic_cluster2", "synthetic_cluster4", "synthetic_cluster8]"], ["[\"synthetic\"]"], [], [""], ["[synthetic_no_norm", "synthetic_with_norm]"], [], ["['corr_50_d5'", "'corr_50_d15'", "'corr_75_d10'", "'corr_95_d5'", "'corr_95_d15']"], ["corr_50_d5", "corr_50_d15", "corr_95_d5", "corr_95_d15", "corr_75_d10"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\n    data = experiment_data[\"learning_rate\"][\"synthetic\"]\n    lrs = data[\"lrs\"]\n    metrics_train = data[\"metrics\"][\"train\"]\n    metrics_val = data[\"metrics\"][\"val\"]\n    losses_train = data[\"losses\"][\"train\"]\n    losses_val = data[\"losses\"][\"val\"]\nexcept Exception as e:\n    print(f\"Error loading or extracting experiment data: {e}\")\n\n# Plot worst\u2010group accuracy curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, metrics_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, metrics_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, metrics_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training WG Accuracy\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Worst\u2010group Accuracy\")\n    axes[1].set_title(\"Validation WG Accuracy\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Worst\u2010group Accuracy\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\n        \"Synthetic dataset - Worst\u2010group Accuracy\\nLeft: Training, Right: Validation\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_worst_group_accuracy.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close(\"all\")\n\n# Plot loss curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    epochs = np.arange(1, losses_train.shape[1] + 1)\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, losses_train[i], label=f\"lr={lr}\")\n        axes[1].plot(epochs, losses_val[i], label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[1].set_title(\"Validation Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    for ax in axes:\n        ax.legend()\n    fig.suptitle(\"Synthetic dataset - Loss Curves\\nLeft: Training, Right: Validation\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close(\"all\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(data_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Known learning rates\nlrs = [1e-4, 1e-3, 1e-2]\n\n# Iterate datasets\nfor ds_name, ds_data in experiment_data.get(\"multiple_synthetic\", {}).items():\n    metrics_train = ds_data[\"metrics\"][\"train\"]  # shape (n_lrs, n_epochs)\n    metrics_val = ds_data[\"metrics\"][\"val\"]\n    losses_train = ds_data[\"losses\"][\"train\"]\n    losses_val = ds_data[\"losses\"][\"val\"]\n    sp_corr = ds_data.get(\"spurious_corr\")\n    dim = ds_data.get(\"dim\")\n    n_epochs = metrics_train.shape[1]\n\n    # Plot weighted accuracy curves\n    try:\n        plt.figure()\n        epochs = np.arange(n_epochs)\n        for i, lr in enumerate(lrs):\n            plt.plot(epochs, metrics_train[i], \"--\", label=f\"Train lr={lr}\")\n            plt.plot(epochs, metrics_val[i], \"-\", label=f\"Val lr={lr}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Weighted Accuracy\")\n        plt.title(f\"{ds_name} (corr={sp_corr}, dim={dim}): Weighted Accuracy\")\n        plt.legend()\n        out_file = os.path.join(working_dir, f\"{ds_name}_accuracy_curve.png\")\n        plt.savefig(out_file)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot for {ds_name}: {e}\")\n        plt.close()\n\n    # Plot loss curves\n    try:\n        plt.figure()\n        epochs = np.arange(n_epochs)\n        for i, lr in enumerate(lrs):\n            plt.plot(epochs, losses_train[i], \"--\", label=f\"Train lr={lr}\")\n            plt.plot(epochs, losses_val[i], \"-\", label=f\"Val lr={lr}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name} (corr={sp_corr}, dim={dim}): Loss Curve\")\n        plt.legend()\n        out_file = os.path.join(working_dir, f\"{ds_name}_loss_curve.png\")\n        plt.savefig(out_file)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ntry:\n    sd = experiment_data[\"NO_CLUSTER_REWEIGHTING\"][\"synthetic\"]\n    train_acc = sd[\"metrics\"][\"train\"]\n    val_acc = sd[\"metrics\"][\"val\"]\n    train_loss = sd[\"losses\"][\"train\"]\n    val_loss = sd[\"losses\"][\"val\"]\n    preds = sd[\"predictions\"]\n    gt = sd[\"ground_truth\"]\n    # Print test accuracies\n    test_accs = (preds == gt).mean(axis=1)\n    for i, acc in enumerate(test_accs):\n        print(f\"Run {i} Test Accuracy: {acc:.4f}\")\n    epochs = np.arange(train_acc.shape[1])\n    runs = train_acc.shape[0]\n    labels = [f\"run{i}\" for i in range(runs)]\n    # Plot worst\u2010group accuracy curves\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for i in range(runs):\n        axes[0].plot(epochs, train_acc[i], label=labels[i])\n        axes[1].plot(epochs, val_acc[i], label=labels[i])\n    axes[0].set_title(\"Left: Training Worst\u2010Group Accuracy\")\n    axes[1].set_title(\"Right: Validation Worst\u2010Group Accuracy\")\n    fig.suptitle(\"synthetic dataset - Worst\u2010Group Accuracy Curves\")\n    axes[0].legend()\n    axes[1].legend()\n    fig.savefig(os.path.join(working_dir, \"synthetic_wg_accuracy.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\ntry:\n    # Plot loss curves\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for i in range(runs):\n        axes[0].plot(epochs, train_loss[i], label=labels[i])\n        axes[1].plot(epochs, val_loss[i], label=labels[i])\n    axes[0].set_title(\"Left: Training Loss\")\n    axes[1].set_title(\"Right: Validation Loss\")\n    fig.suptitle(\"synthetic dataset - Loss Curves\")\n    axes[0].legend()\n    axes[1].legend()\n    fig.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\ntry:\n    sd = experiment_data[\"NO_SPURIOUS_FEATURE\"][\"synthetic\"]\n    metrics_train = sd[\"metrics\"][\"train\"]\n    metrics_val = sd[\"metrics\"][\"val\"]\n    lrs = sd[\"lrs\"]\n    epochs = metrics_train.shape[1]\n    plt.figure()\n    for i, lr in enumerate(lrs):\n        plt.plot(np.arange(epochs), metrics_train[i], label=f\"Train lr={lr}\")\n        plt.plot(np.arange(epochs), metrics_val[i], \"--\", label=f\"Val lr={lr}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Weighted Group Accuracy\")\n    plt.title(\n        \"Weighted Group Accuracy Curves (synthetic)\\nTrain (solid) vs Validation (dashed)\"\n    )\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_wg_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating wg accuracy plot: {e}\")\n    plt.close()\n\ntry:\n    sd = experiment_data[\"NO_SPURIOUS_FEATURE\"][\"synthetic\"]\n    losses_train = sd[\"losses\"][\"train\"]\n    losses_val = sd[\"losses\"][\"val\"]\n    lrs = sd[\"lrs\"]\n    epochs = losses_train.shape[1]\n    plt.figure()\n    for i, lr in enumerate(lrs):\n        plt.plot(np.arange(epochs), losses_train[i], label=f\"Train lr={lr}\")\n        plt.plot(np.arange(epochs), losses_val[i], \"--\", label=f\"Val lr={lr}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss Curves (synthetic)\\nTrain (solid) vs Validation (dashed)\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\ntry:\n    sd = experiment_data[\"NO_SPURIOUS_FEATURE\"][\"synthetic\"]\n    preds = sd[\"predictions\"]\n    truths = sd[\"ground_truth\"]\n    lrs = sd[\"lrs\"]\n    accs = np.array([(preds[i] == truths).mean() for i in range(len(lrs))])\n    plt.figure()\n    plt.bar([str(lr) for lr in lrs], accs)\n    plt.xlabel(\"Learning Rate\")\n    plt.ylabel(\"Test Accuracy\")\n    plt.title(\"Test Accuracy per Learning Rate (synthetic)\\nTest set performance\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_test_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot 1: Weighted Group Accuracy Curves\ntry:\n    metrics = experiment_data[\"linear_classifier\"][\"synthetic\"][\"metrics\"]\n    train_acc = metrics[\"train\"]\n    val_acc = metrics[\"val\"]\n    epochs = train_acc.shape[1]\n    lrs = [1e-4, 1e-3, 1e-2]\n    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n    for i, lr in enumerate(lrs):\n        axs[0].plot(range(1, epochs + 1), train_acc[i], marker=\"o\", label=f\"lr={lr}\")\n        axs[1].plot(range(1, epochs + 1), val_acc[i], marker=\"o\", label=f\"lr={lr}\")\n    axs[0].set_title(\"Training\")\n    axs[0].set_xlabel(\"Epoch\")\n    axs[0].set_ylabel(\"Weighted Group Accuracy\")\n    axs[0].legend()\n    axs[1].set_title(\"Validation\")\n    axs[1].set_xlabel(\"Epoch\")\n    axs[1].legend()\n    fig.suptitle(\n        \"Weighted Group Accuracy Curves - Synthetic dataset\\nLeft: Training, Right: Validation\"\n    )\n    fig.savefig(os.path.join(working_dir, \"synthetic_weighted_group_accuracy.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating weighted group accuracy plot: {e}\")\n    plt.close()\n\n# Plot 2: Loss Curves\ntry:\n    losses = experiment_data[\"linear_classifier\"][\"synthetic\"][\"losses\"]\n    train_loss = losses[\"train\"]\n    val_loss = losses[\"val\"]\n    epochs = train_loss.shape[1]\n    lrs = [1e-4, 1e-3, 1e-2]\n    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n    for i, lr in enumerate(lrs):\n        axs[0].plot(range(1, epochs + 1), train_loss[i], marker=\"s\", label=f\"lr={lr}\")\n        axs[1].plot(range(1, epochs + 1), val_loss[i], marker=\"s\", label=f\"lr={lr}\")\n    axs[0].set_title(\"Training\")\n    axs[0].set_xlabel(\"Epoch\")\n    axs[0].set_ylabel(\"Loss\")\n    axs[0].legend()\n    axs[1].set_title(\"Validation\")\n    axs[1].set_xlabel(\"Epoch\")\n    axs[1].legend()\n    fig.suptitle(\"Loss Curves - Synthetic dataset\\nLeft: Training, Right: Validation\")\n    fig.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = data[\"INPUT_FEATURE_CLUSTER_REWEIGHTING\"][\"synthetic\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = None\n\nif exp is not None:\n    lrs = exp[\"lrs\"]\n    train_acc = exp[\"metrics\"][\"train\"]\n    val_acc = exp[\"metrics\"][\"val\"]\n    train_loss = exp[\"losses\"][\"train\"]\n    val_loss = exp[\"losses\"][\"val\"]\n    preds = exp[\"predictions\"]\n    gt = exp[\"ground_truth\"]\n    epochs = np.arange(train_acc.shape[1])\n\n    try:\n        plt.figure()\n        plt.subplot(1, 2, 1)\n        for i, lr in enumerate(lrs):\n            plt.plot(epochs, train_acc[i], label=f\"lr={lr}\")\n        plt.title(\"Left: Training WG Accuracy\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"WG Accuracy\")\n        plt.legend()\n        plt.subplot(1, 2, 2)\n        for i, lr in enumerate(lrs):\n            plt.plot(epochs, val_acc[i], label=f\"lr={lr}\")\n        plt.title(\"Right: Validation WG Accuracy\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"WG Accuracy\")\n        plt.legend()\n        plt.suptitle(\"WG Accuracy Curves (synthetic dataset)\")\n        plt.savefig(os.path.join(working_dir, \"synthetic_wg_accuracy_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating wg accuracy plot: {e}\")\n        plt.close()\n\n    try:\n        plt.figure()\n        plt.subplot(1, 2, 1)\n        for i, lr in enumerate(lrs):\n            plt.plot(epochs, train_loss[i], label=f\"lr={lr}\")\n        plt.title(\"Left: Training Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.subplot(1, 2, 2)\n        for i, lr in enumerate(lrs):\n            plt.plot(epochs, val_loss[i], label=f\"lr={lr}\")\n        plt.title(\"Right: Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.suptitle(\"Loss Curves (synthetic dataset)\")\n        plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # Compute test accuracies\n    test_acc = (preds == gt).mean(axis=1)\n    print(\"Test accuracies per LR:\")\n    for lr, acc in zip(lrs, test_acc):\n        print(f\"LR={lr}: Test Accuracy={acc:.4f}\")\n\n    try:\n        plt.figure()\n        plt.bar([str(lr) for lr in lrs], test_acc)\n        plt.title(\"Test Accuracy per Learning Rate (synthetic dataset)\")\n        plt.xlabel(\"Learning Rate\")\n        plt.ylabel(\"Accuracy\")\n        plt.savefig(os.path.join(working_dir, \"synthetic_test_accuracy_bar.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test accuracy bar plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Extract synthetic results\ndata = experiment_data.get(\"representation_cluster_reweighting\", {}).get(\n    \"synthetic\", {}\n)\nmetrics_train = data.get(\"metrics\", {}).get(\"train\", np.empty((0, 0)))\nmetrics_val = data.get(\"metrics\", {}).get(\"val\", np.empty((0, 0)))\nlosses_train = data.get(\"losses\", {}).get(\"train\", np.empty((0, 0)))\nlosses_val = data.get(\"losses\", {}).get(\"val\", np.empty((0, 0)))\npreds = data.get(\"predictions\", np.empty((0,)))\ntruth = data.get(\"ground_truth\", np.empty((0,)))\n\n# 1) Worst\u2010Group Accuracy Curves\ntry:\n    plt.figure()\n    epochs = np.arange(metrics_train.shape[1])\n    for run in range(metrics_train.shape[0]):\n        plt.plot(epochs, metrics_train[run], \"-\", label=f\"Run {run+1} Train\")\n        plt.plot(epochs, metrics_val[run], \"--\", label=f\"Run {run+1} Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Worst\u2010Group Accuracy\")\n    plt.title(\"Synthetic Dataset: Train vs Val Worst\u2010Group Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_wg_accuracy_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating wg accuracy curve: {e}\")\n    plt.close()\n\n# 2) Loss Curves\ntry:\n    plt.figure()\n    epochs = np.arange(losses_train.shape[1])\n    for run in range(losses_train.shape[0]):\n        plt.plot(epochs, losses_train[run], \"-\", label=f\"Run {run+1} Train\")\n        plt.plot(epochs, losses_val[run], \"--\", label=f\"Run {run+1} Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Synthetic Dataset: Train vs Val Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# 3) Test Accuracy Bar Chart\ntry:\n    if preds.size and truth.size:\n        # preds is shape (runs, N), truth is (N,)\n        acc_test = np.mean(preds == truth[None, :], axis=1)\n        plt.figure()\n        runs = np.arange(acc_test.size)\n        plt.bar(runs, acc_test)\n        plt.xticks(runs, [f\"Run {i+1}\" for i in runs])\n        plt.xlabel(\"Run\")\n        plt.ylabel(\"Test Accuracy\")\n        plt.title(\"Synthetic Dataset: Test Accuracy by Run\")\n        plt.savefig(os.path.join(working_dir, \"synthetic_test_accuracy_bar.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy bar: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\ntry:\n    synthetic = experiment_data[\"group_inverse_frequency_reweighting\"][\"synthetic\"]\n    lrs = synthetic[\"lrs\"]\n    metrics = synthetic[\"metrics\"]\n    plt.figure()\n    for i, lr in enumerate(lrs):\n        plt.plot(metrics[\"train\"][i], label=f\"Train lr={lr}\")\n        plt.plot(metrics[\"val\"][i], linestyle=\"--\", label=f\"Val lr={lr}\")\n    plt.title(\"Synthetic dataset: Worst\u2010Group Accuracy vs Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Worst\u2010Group Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_wg_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\ntry:\n    losses = synthetic[\"losses\"]\n    plt.figure()\n    for i, lr in enumerate(lrs):\n        plt.plot(losses[\"train\"][i], label=f\"Train lr={lr}\")\n        plt.plot(losses[\"val\"][i], linestyle=\"--\", label=f\"Val lr={lr}\")\n    plt.title(\"Synthetic dataset: Loss vs Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    sd = experiment_data[\"random_cluster_reweighting\"][\"synthetic\"]\n    train_metrics = sd[\"metrics\"][\"train\"]\n    val_metrics = sd[\"metrics\"][\"val\"]\n    train_losses = sd[\"losses\"][\"train\"]\n    val_losses = sd[\"losses\"][\"val\"]\n    preds = sd[\"predictions\"]\n    truths = sd[\"ground_truth\"]\n    # compute test accuracies\n    test_acc = np.mean(preds == truths, axis=1)\n    epochs = np.arange(train_metrics.shape[1])\n    lr_list = [1e-4, 1e-3, 1e-2]\n\n    try:\n        plt.figure()\n        for i, lr in enumerate(lr_list):\n            plt.plot(epochs, train_metrics[i], label=f\"train lr={lr}\")\n            plt.plot(epochs, val_metrics[i], \"--\", label=f\"val lr={lr}\")\n        plt.title(\"Synthetic Dataset - Worst Group Accuracy vs Epoch\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Worst Group Accuracy\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_worst_group_accuracy.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating worst group accuracy plot: {e}\")\n        plt.close()\n\n    try:\n        plt.figure()\n        for i, lr in enumerate(lr_list):\n            plt.plot(epochs, train_losses[i], label=f\"train lr={lr}\")\n            plt.plot(epochs, val_losses[i], \"--\", label=f\"val lr={lr}\")\n        plt.title(\"Synthetic Dataset - Loss vs Epoch\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves plot: {e}\")\n        plt.close()\n\n    try:\n        plt.figure()\n        plt.bar([str(lr) for lr in lr_list], test_acc)\n        plt.title(\"Synthetic Dataset - Test Accuracy by Learning Rate\")\n        plt.xlabel(\"Learning Rate\")\n        plt.ylabel(\"Test Accuracy\")\n        plt.savefig(os.path.join(working_dir, \"synthetic_test_accuracy.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating test accuracy bar plot: {e}\")\n        plt.close()\n\n    print(f\"Test accuracies by learning rate: {dict(zip(lr_list, test_acc))}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor exp_name, exp_data in experiment_data.items():\n    for dataset_name, data in exp_data.items():\n        lrs = data[\"learning_rates\"]\n        metrics_train = data[\"metrics\"][\"train\"]\n        metrics_val = data[\"metrics\"][\"val\"]\n        losses_train = data[\"losses\"][\"train\"]\n        losses_val = data[\"losses\"][\"val\"]\n        epochs = np.arange(1, metrics_train.shape[1] + 1)\n\n        # Worst\u2010group accuracy curves\n        try:\n            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n            for i, lr in enumerate(lrs):\n                axes[0].plot(epochs, metrics_train[i], label=f\"lr={lr}\")\n                axes[1].plot(epochs, metrics_val[i], label=f\"lr={lr}\")\n            axes[0].set_xlabel(\"Epoch\")\n            axes[0].set_ylabel(\"Worst-group accuracy\")\n            axes[1].set_xlabel(\"Epoch\")\n            axes[1].set_ylabel(\"Worst-group accuracy\")\n            fig.suptitle(\"Worst\u2010group Accuracy Curves\")\n            fig.text(\n                0.5,\n                0.01,\n                f\"Left: Train, Right: Validation | Dataset: {dataset_name}\",\n                ha=\"center\",\n            )\n            axes[0].legend(), axes[1].legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset_name}_wg_accuracy_curves.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating wg accuracy curves: {e}\")\n            plt.close()\n\n        # Loss curves\n        try:\n            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n            for i, lr in enumerate(lrs):\n                axes[0].plot(epochs, losses_train[i], label=f\"lr={lr}\")\n                axes[1].plot(epochs, losses_val[i], label=f\"lr={lr}\")\n            axes[0].set_xlabel(\"Epoch\")\n            axes[0].set_ylabel(\"Loss\")\n            axes[1].set_xlabel(\"Epoch\")\n            axes[1].set_ylabel(\"Loss\")\n            fig.suptitle(\"Loss Curves\")\n            fig.text(\n                0.5,\n                0.01,\n                f\"Left: Train, Right: Validation | Dataset: {dataset_name}\",\n                ha=\"center\",\n            )\n            axes[0].legend(), axes[1].legend()\n            plt.savefig(os.path.join(working_dir, f\"{dataset_name}_loss_curves.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss curves: {e}\")\n            plt.close()\n\n        # Final validation worst\u2010group accuracy bar chart\n        try:\n            plt.figure()\n            final_val_wg = metrics_val[:, -1]\n            plt.bar([str(lr) for lr in lrs], final_val_wg)\n            plt.xlabel(\"Learning rate\")\n            plt.ylabel(\"Final worst-group accuracy\")\n            plt.title(f\"Final Validation Worst\u2010group Accuracy\\nDataset: {dataset_name}\")\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset_name}_final_val_wg_accuracy.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating final val wg accuracy plot: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nsd = experiment_data.get(\"random_cluster_reweighting\", {}).get(\"synthetic\", {})\nmetrics_train = sd.get(\"metrics\", {}).get(\"train\")\nmetrics_val = sd.get(\"metrics\", {}).get(\"val\")\nlosses_train = sd.get(\"losses\", {}).get(\"train\")\nlosses_val = sd.get(\"losses\", {}).get(\"val\")\n\n# Worst-group accuracy curves\ntry:\n    epochs = np.arange(metrics_train.shape[1])\n    num_runs = metrics_train.shape[0]\n    run_labels = [f\"run{i}\" for i in range(num_runs)]\n    plt.figure()\n    plt.suptitle(\n        \"Worst-Group Accuracy Curves (Synthetic) - Left: Train, Right: Validation\"\n    )\n    plt.subplot(1, 2, 1)\n    for i in range(num_runs):\n        plt.plot(epochs, metrics_train[i], label=run_labels[i])\n    plt.title(\"Train Worst-Group Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    for i in range(num_runs):\n        plt.plot(epochs, metrics_val[i], label=run_labels[i])\n    plt.title(\"Val Worst-Group Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_worst_group_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating worst-group accuracy plot: {e}\")\n    plt.close()\n\n# Loss curves\ntry:\n    epochs = np.arange(losses_train.shape[1])\n    num_runs = losses_train.shape[0]\n    run_labels = [f\"run{i}\" for i in range(num_runs)]\n    plt.figure()\n    plt.suptitle(\"Loss Curves (Synthetic) - Left: Train, Right: Validation\")\n    plt.subplot(1, 2, 1)\n    for i in range(num_runs):\n        plt.plot(epochs, losses_train[i], label=run_labels[i])\n    plt.title(\"Train Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    for i in range(num_runs):\n        plt.plot(epochs, losses_val[i], label=run_labels[i])\n    plt.title(\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nif not os.path.exists(working_dir):\n    os.makedirs(working_dir)\n\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    syn = data[\"SGD_OPTIMIZER\"][\"synthetic\"]\n    lrs = syn[\"learning_rates\"]\n    acc_tr = syn[\"metrics\"][\"train\"]\n    acc_val = syn[\"metrics\"][\"val\"]\n    loss_tr = syn[\"losses\"][\"train\"]\n    loss_val = syn[\"losses\"][\"val\"]\n    preds = syn[\"predictions\"]\n    truths = syn[\"ground_truth\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\ntry:\n    plt.figure()\n    for i, lr in enumerate(lrs):\n        plt.plot(acc_tr[i], label=f\"Train wg acc lr={lr}\")\n        plt.plot(acc_val[i], \"--\", label=f\"Val wg acc lr={lr}\")\n    plt.title(\"Worst-Group Accuracy on synthetic dataset\\nSolid: Train, Dashed: Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Worst-Group Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_wg_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    for i, lr in enumerate(lrs):\n        plt.plot(loss_tr[i], label=f\"Train loss lr={lr}\")\n        plt.plot(loss_val[i], \"--\", label=f\"Val loss lr={lr}\")\n    plt.title(\"Loss Curves on synthetic dataset\\nSolid: Train, Dashed: Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\ntry:\n    test_acc = [(preds[i] == truths).mean() for i in range(len(lrs))]\n    plt.figure()\n    plt.bar([str(lr) for lr in lrs], test_acc)\n    plt.title(\n        \"Test Accuracy by Learning Rate on synthetic dataset\\nBars: Test Accuracy\"\n    )\n    plt.xlabel(\"Learning Rate\")\n    plt.ylabel(\"Accuracy\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_test_accuracy_by_lr.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy bar chart: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Extract synthetic cluster\u2010variation results\ncluster_counts = (\n    experiment_data.get(\"CLUSTER_COUNT_VARIATION\", {})\n    .get(\"synthetic\", {})\n    .get(\"cluster_counts\", np.array([]))\n)\nlr_block = (\n    experiment_data.get(\"CLUSTER_COUNT_VARIATION\", {})\n    .get(\"synthetic\", {})\n    .get(\"learning_rate\", {})\n)\nlrs = lr_block.get(\"lrs\", np.array([]))\nmetrics_train = lr_block.get(\"metrics\", {}).get(\"train\", np.array([]))\nmetrics_val = lr_block.get(\"metrics\", {}).get(\"val\", np.array([]))\nlosses_train = lr_block.get(\"losses\", {}).get(\"train\", np.array([]))\nlosses_val = lr_block.get(\"losses\", {}).get(\"val\", np.array([]))\n\n# Plot curves for each cluster count\nfor idx, k in enumerate(cluster_counts):\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n        fig.suptitle(\n            f\"Synthetic dataset - cluster_count={k} (Left: WG Accuracy, Right: Loss)\"\n        )\n        epochs = np.arange(metrics_train.shape[2])\n        for j, lr in enumerate(lrs):\n            axes[0].plot(epochs, metrics_train[idx, j], label=f\"train lr={lr:.0e}\")\n            axes[0].plot(\n                epochs, metrics_val[idx, j], linestyle=\"--\", label=f\"val lr={lr:.0e}\"\n            )\n            axes[1].plot(epochs, losses_train[idx, j], label=f\"train lr={lr:.0e}\")\n            axes[1].plot(\n                epochs, losses_val[idx, j], linestyle=\"--\", label=f\"val lr={lr:.0e}\"\n            )\n        axes[0].set_xlabel(\"Epoch\")\n        axes[0].set_ylabel(\"Worst-group Accuracy\")\n        axes[0].legend()\n        axes[1].set_xlabel(\"Epoch\")\n        axes[1].set_ylabel(\"Loss\")\n        axes[1].legend()\n        plt.savefig(os.path.join(working_dir, f\"synthetic_cluster{k}_wgacc_loss.png\"))\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating plot for cluster_count={k}: {e}\")\n        plt.close()\n\n# Print final validation worst-group accuracies\nif metrics_val.size:\n    final_val = metrics_val[:, :, -1]\n    print(\"Final validation worst-group accuracy per cluster count & lr:\")\n    for i, k in enumerate(cluster_counts):\n        for j, lr in enumerate(lrs):\n            print(f\"  Cluster {k}, lr={lr:.0e}: {final_val[i, j]:.3f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = exp[\"weight_decay_variation\"][\"synthetic\"]\n    wds = data[\"weight_decays\"]\n    metrics_train = data[\"metrics\"][\"train\"]\n    metrics_val = data[\"metrics\"][\"val\"]\n    losses_train = data[\"losses\"][\"train\"]\n    losses_val = data[\"losses\"][\"val\"]\n    preds = data[\"predictions\"]\n    truths = data[\"ground_truth\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\ntry:\n    epochs = np.arange(metrics_train.shape[1])\n    plt.figure()\n    for i, wd in enumerate(wds):\n        plt.plot(epochs, metrics_train[i], label=f\"WD={wd:.0e} Train\")\n        plt.plot(epochs, metrics_val[i], \"--\", label=f\"WD={wd:.0e} Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Worst-Group Accuracy\")\n    plt.title(\"Worst-Group Accuracy across Epochs (Synthetic dataset)\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_wg_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating wg accuracy plot: {e}\")\n    plt.close()\n\ntry:\n    epochs = np.arange(losses_train.shape[1])\n    plt.figure()\n    for i, wd in enumerate(wds):\n        plt.plot(epochs, losses_train[i], label=f\"WD={wd:.0e} Train\")\n        plt.plot(epochs, losses_val[i], \"--\", label=f\"WD={wd:.0e} Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss across Epochs (Synthetic dataset)\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\ntry:\n    test_acc = (preds == truths).mean(axis=1)\n    labels = [\"0\" if wd == 0 else f\"{wd:.0e}\" for wd in wds]\n    plt.figure()\n    plt.bar(labels, test_acc, color=\"skyblue\")\n    plt.xlabel(\"Weight Decay\")\n    plt.ylabel(\"Test Accuracy\")\n    plt.title(\"Test Accuracy across Weight Decays (Synthetic dataset)\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_test_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating test accuracy plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    syn = data[\"LOSS_BASED_SAMPLE_WEIGHTING\"][\"synthetic\"]\n    train_acc = syn[\"metrics\"][\"train\"]\n    val_acc = syn[\"metrics\"][\"val\"]\n    train_loss = syn[\"losses\"][\"train\"]\n    val_loss = syn[\"losses\"][\"val\"]\n    sample_weights = syn[\"sample_weights\"]\n    lrs = [1e-4, 1e-3, 1e-2]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\ntry:\n    plt.figure()\n    epochs = np.arange(train_acc.shape[1])\n    for i, lr in enumerate(lrs):\n        plt.plot(epochs, train_acc[i], label=f\"LR={lr} train\")\n        plt.plot(epochs, val_acc[i], \"--\", label=f\"LR={lr} val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Worst-Group Accuracy\")\n    plt.title(\n        \"Worst-Group Accuracy over Epochs (synthetic)\\nTrain (solid) vs Validation (dashed)\"\n    )\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_wg_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    for i, lr in enumerate(lrs):\n        plt.plot(epochs, train_loss[i], label=f\"LR={lr} train\")\n        plt.plot(epochs, val_loss[i], \"--\", label=f\"LR={lr} val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-Entropy Loss\")\n    plt.title(\"Training/Validation Loss over Epochs (synthetic)\\nCross-Entropy Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n\ntry:\n    fig, axes = plt.subplots(1, len(lrs), figsize=(5 * len(lrs), 4))\n    fig.suptitle(\"Sample Weight Distribution (synthetic)\")\n    for i, lr in enumerate(lrs):\n        ax = axes[i] if len(lrs) > 1 else axes\n        ax.hist(sample_weights[i], bins=30)\n        ax.set_title(f\"LR={lr}\")\n        ax.set_xlabel(\"Weight\")\n        ax.set_ylabel(\"Frequency\")\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(os.path.join(working_dir, \"synthetic_sample_weights_hist.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot3: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Iterate over methods and datasets\nfor method_name, dsets in experiment_data.items():\n    for dataset_name, exp in dsets.items():\n        metrics = exp.get(\"metrics\", {})\n        losses = exp.get(\"losses\", {})\n        # Plot worst-group accuracy curves\n        try:\n            plt.figure()\n            train_acc = metrics.get(\"train\")\n            val_acc = metrics.get(\"val\")\n            if train_acc is not None and val_acc is not None:\n                epochs = np.arange(train_acc.shape[1])\n                for i in range(train_acc.shape[0]):\n                    plt.plot(epochs, train_acc[i], label=f\"Run {i+1} Train\")\n                    plt.plot(epochs, val_acc[i], \"--\", label=f\"Run {i+1} Val\")\n                plt.suptitle(\n                    f\"{method_name.replace('_',' ').title()} - {dataset_name.title()} Dataset\"\n                )\n                plt.title(\"Left: Train, Right: Validation (Worst-Group Accuracy)\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Worst-Group Accuracy\")\n                plt.legend()\n                fname = f\"{method_name}_{dataset_name}_wg_accuracy_curve.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating wg_accuracy plot for {method_name}: {e}\")\n            plt.close()\n        # Plot loss curves\n        try:\n            plt.figure()\n            train_loss = losses.get(\"train\")\n            val_loss = losses.get(\"val\")\n            if train_loss is not None and val_loss is not None:\n                epochs = np.arange(train_loss.shape[1])\n                for i in range(train_loss.shape[0]):\n                    plt.plot(epochs, train_loss[i], label=f\"Run {i+1} Train\")\n                    plt.plot(epochs, val_loss[i], \"--\", label=f\"Run {i+1} Val\")\n                plt.suptitle(\n                    f\"{method_name.replace('_',' ').title()} - {dataset_name.title()} Dataset\"\n                )\n                plt.title(\"Left: Train, Right: Validation (Average Loss)\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Loss\")\n                plt.legend()\n                fname = f\"{method_name}_{dataset_name}_loss_curve.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {method_name}: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nlrs = [1e-4, 1e-3, 1e-2]\nfor dataset, info in experiment_data.get(\"NO_FEATURE_NORMALIZATION\", {}).items():\n    metrics_train = info[\"metrics\"][\"train\"]\n    metrics_val = info[\"metrics\"][\"val\"]\n    losses_train = info[\"losses\"][\"train\"]\n    losses_val = info[\"losses\"][\"val\"]\n\n    # Accuracy curves\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n        for i, lr in enumerate(lrs):\n            epochs = np.arange(metrics_train.shape[1])\n            axes[0].plot(epochs, metrics_train[i], label=f\"lr={lr}\")\n            axes[1].plot(epochs, metrics_val[i], label=f\"lr={lr}\")\n        fig.suptitle(f\"{dataset} WG Accuracy Curves\")\n        axes[0].set_title(\"Left: Training WG Accuracy\")\n        axes[1].set_title(\"Right: Validation WG Accuracy\")\n        for ax in axes:\n            ax.set_xlabel(\"Epoch\")\n            ax.set_ylabel(\"WG Accuracy\")\n            ax.legend()\n        plt.savefig(os.path.join(working_dir, f\"{dataset}_accuracy.png\"))\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating accuracy plot for {dataset}: {e}\")\n        plt.close()\n\n    # Loss curves\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n        for i, lr in enumerate(lrs):\n            epochs = np.arange(losses_train.shape[1])\n            axes[0].plot(epochs, losses_train[i], label=f\"lr={lr}\")\n            axes[1].plot(epochs, losses_val[i], label=f\"lr={lr}\")\n        fig.suptitle(f\"{dataset} Loss Curves\")\n        axes[0].set_title(\"Left: Training Loss\")\n        axes[1].set_title(\"Right: Validation Loss\")\n        for ax in axes:\n            ax.set_xlabel(\"Epoch\")\n            ax.set_ylabel(\"Loss\")\n            ax.legend()\n        plt.savefig(os.path.join(working_dir, f\"{dataset}_loss.png\"))\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating loss plot for {dataset}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(data_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Known learning rates\nlrs = [1e-4, 1e-3, 1e-2]\n\n# Iterate datasets\nfor ds_name, ds_data in experiment_data.get(\"multiple_synthetic\", {}).items():\n    metrics_train = ds_data[\"metrics\"][\"train\"]  # shape (n_lrs, n_epochs)\n    metrics_val = ds_data[\"metrics\"][\"val\"]\n    losses_train = ds_data[\"losses\"][\"train\"]\n    losses_val = ds_data[\"losses\"][\"val\"]\n    sp_corr = ds_data.get(\"spurious_corr\")\n    dim = ds_data.get(\"dim\")\n    n_epochs = metrics_train.shape[1]\n\n    # Plot weighted accuracy curves\n    try:\n        plt.figure()\n        epochs = np.arange(n_epochs)\n        for i, lr in enumerate(lrs):\n            plt.plot(epochs, metrics_train[i], \"--\", label=f\"Train lr={lr}\")\n            plt.plot(epochs, metrics_val[i], \"-\", label=f\"Val lr={lr}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Weighted Accuracy\")\n        plt.title(f\"{ds_name} (corr={sp_corr}, dim={dim}): Weighted Accuracy\")\n        plt.legend()\n        out_file = os.path.join(working_dir, f\"{ds_name}_accuracy_curve.png\")\n        plt.savefig(out_file)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot for {ds_name}: {e}\")\n        plt.close()\n\n    # Plot loss curves\n    try:\n        plt.figure()\n        epochs = np.arange(n_epochs)\n        for i, lr in enumerate(lrs):\n            plt.plot(epochs, losses_train[i], \"--\", label=f\"Train lr={lr}\")\n            plt.plot(epochs, losses_val[i], \"-\", label=f\"Val lr={lr}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name} (corr={sp_corr}, dim={dim}): Loss Curve\")\n        plt.legend()\n        out_file = os.path.join(working_dir, f\"{ds_name}_loss_curve.png\")\n        plt.savefig(out_file)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(data_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Known learning rates\nlrs = [1e-4, 1e-3, 1e-2]\n\n# Iterate datasets\nfor ds_name, ds_data in experiment_data.get(\"multiple_synthetic\", {}).items():\n    metrics_train = ds_data[\"metrics\"][\"train\"]  # shape (n_lrs, n_epochs)\n    metrics_val = ds_data[\"metrics\"][\"val\"]\n    losses_train = ds_data[\"losses\"][\"train\"]\n    losses_val = ds_data[\"losses\"][\"val\"]\n    sp_corr = ds_data.get(\"spurious_corr\")\n    dim = ds_data.get(\"dim\")\n    n_epochs = metrics_train.shape[1]\n\n    # Plot weighted accuracy curves\n    try:\n        plt.figure()\n        epochs = np.arange(n_epochs)\n        for i, lr in enumerate(lrs):\n            plt.plot(epochs, metrics_train[i], \"--\", label=f\"Train lr={lr}\")\n            plt.plot(epochs, metrics_val[i], \"-\", label=f\"Val lr={lr}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Weighted Accuracy\")\n        plt.title(f\"{ds_name} (corr={sp_corr}, dim={dim}): Weighted Accuracy\")\n        plt.legend()\n        out_file = os.path.join(working_dir, f\"{ds_name}_accuracy_curve.png\")\n        plt.savefig(out_file)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot for {ds_name}: {e}\")\n        plt.close()\n\n    # Plot loss curves\n    try:\n        plt.figure()\n        epochs = np.arange(n_epochs)\n        for i, lr in enumerate(lrs):\n            plt.plot(epochs, losses_train[i], \"--\", label=f\"Train lr={lr}\")\n            plt.plot(epochs, losses_val[i], \"-\", label=f\"Val lr={lr}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name} (corr={sp_corr}, dim={dim}): Loss Curve\")\n        plt.legend()\n        out_file = os.path.join(working_dir, f\"{ds_name}_loss_curve.png\")\n        plt.savefig(out_file)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(data_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Known learning rates\nlrs = [1e-4, 1e-3, 1e-2]\n\n# Iterate datasets\nfor ds_name, ds_data in experiment_data.get(\"multiple_synthetic\", {}).items():\n    metrics_train = ds_data[\"metrics\"][\"train\"]  # shape (n_lrs, n_epochs)\n    metrics_val = ds_data[\"metrics\"][\"val\"]\n    losses_train = ds_data[\"losses\"][\"train\"]\n    losses_val = ds_data[\"losses\"][\"val\"]\n    sp_corr = ds_data.get(\"spurious_corr\")\n    dim = ds_data.get(\"dim\")\n    n_epochs = metrics_train.shape[1]\n\n    # Plot weighted accuracy curves\n    try:\n        plt.figure()\n        epochs = np.arange(n_epochs)\n        for i, lr in enumerate(lrs):\n            plt.plot(epochs, metrics_train[i], \"--\", label=f\"Train lr={lr}\")\n            plt.plot(epochs, metrics_val[i], \"-\", label=f\"Val lr={lr}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Weighted Accuracy\")\n        plt.title(f\"{ds_name} (corr={sp_corr}, dim={dim}): Weighted Accuracy\")\n        plt.legend()\n        out_file = os.path.join(working_dir, f\"{ds_name}_accuracy_curve.png\")\n        plt.savefig(out_file)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot for {ds_name}: {e}\")\n        plt.close()\n\n    # Plot loss curves\n    try:\n        plt.figure()\n        epochs = np.arange(n_epochs)\n        for i, lr in enumerate(lrs):\n            plt.plot(epochs, losses_train[i], \"--\", label=f\"Train lr={lr}\")\n            plt.plot(epochs, losses_val[i], \"-\", label=f\"Val lr={lr}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name} (corr={sp_corr}, dim={dim}): Loss Curve\")\n        plt.legend()\n        out_file = os.path.join(working_dir, f\"{ds_name}_loss_curve.png\")\n        plt.savefig(out_file)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# List of relative experiment data paths\nexperiment_data_path_list = [\n    \"experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_10fcb9c03a6349d0be26b639ce55b936_proc_17030/experiment_data.npy\",\n    \"experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_127c4c95906c46f9a8ba25bc6c34f578_proc_17031/experiment_data.npy\",\n    \"experiments/2025-06-06_23-36-12_gradient_cluster_robust_attempt_0/logs/0-run/experiment_results/experiment_40990ba0293d425393d648a3d85058c6_proc_17029/experiment_data.npy\",\n]\n\n# Load all experiment data\nall_experiment_data = []\ntry:\n    for rel_path in experiment_data_path_list:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), rel_path)\n        data = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(data)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Known learning rates\nlrs = [1e-4, 1e-3, 1e-2]\n\n# Collect all dataset names under 'multiple_synthetic'\ndataset_names = set()\nfor d in all_experiment_data:\n    dataset_names.update(d.get(\"multiple_synthetic\", {}).keys())\n\n# Aggregate and plot for each dataset\nfor ds_name in dataset_names:\n    # Gather runs that contain this dataset\n    train_metrics_list = []\n    val_metrics_list = []\n    train_losses_list = []\n    val_losses_list = []\n    for d in all_experiment_data:\n        ds = d.get(\"multiple_synthetic\", {}).get(ds_name)\n        if ds is not None:\n            train_metrics_list.append(ds[\"metrics\"][\"train\"])\n            val_metrics_list.append(ds[\"metrics\"][\"val\"])\n            train_losses_list.append(ds[\"losses\"][\"train\"])\n            val_losses_list.append(ds[\"losses\"][\"val\"])\n    if not train_metrics_list:\n        continue\n\n    # Stack and compute mean & standard error\n    tm = np.stack(train_metrics_list, axis=0)\n    vm = np.stack(val_metrics_list, axis=0)\n    tl = np.stack(train_losses_list, axis=0)\n    vl = np.stack(val_losses_list, axis=0)\n    train_mean, train_sem = tm.mean(axis=0), tm.std(axis=0) / np.sqrt(len(tm))\n    val_mean, val_sem = vm.mean(axis=0), vm.std(axis=0) / np.sqrt(len(vm))\n    loss_train_mean, loss_train_sem = tl.mean(axis=0), tl.std(axis=0) / np.sqrt(len(tl))\n    loss_val_mean, loss_val_sem = vl.mean(axis=0), vl.std(axis=0) / np.sqrt(len(vl))\n    n_epochs = train_mean.shape[1]\n    epochs = np.arange(n_epochs)\n\n    # Aggregated accuracy with SEM\n    try:\n        plt.figure()\n        for i, lr in enumerate(lrs):\n            plt.errorbar(\n                epochs,\n                train_mean[i],\n                yerr=train_sem[i],\n                fmt=\"--\",\n                label=f\"Train lr={lr}\",\n            )\n            plt.errorbar(\n                epochs, val_mean[i], yerr=val_sem[i], fmt=\"-\", label=f\"Val   lr={lr}\"\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Weighted Accuracy\")\n        plt.title(\n            f\"{ds_name}: Weighted Accuracy (Aggregated)\\nLeft: Train mean\u00b1SEM, Right: Val mean\u00b1SEM; synthetic dataset\"\n        )\n        plt.legend()\n        out_path = os.path.join(working_dir, f\"{ds_name}_aggregated_accuracy_curve.png\")\n        plt.savefig(out_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated accuracy plot for {ds_name}: {e}\")\n        plt.close()\n\n    # Aggregated loss with SEM\n    try:\n        plt.figure()\n        for i, lr in enumerate(lrs):\n            plt.errorbar(\n                epochs,\n                loss_train_mean[i],\n                yerr=loss_train_sem[i],\n                fmt=\"--\",\n                label=f\"Train lr={lr}\",\n            )\n            plt.errorbar(\n                epochs,\n                loss_val_mean[i],\n                yerr=loss_val_sem[i],\n                fmt=\"-\",\n                label=f\"Val   lr={lr}\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\n            f\"{ds_name}: Loss Curve (Aggregated)\\nLeft: Train mean\u00b1SEM, Right: Val mean\u00b1SEM; synthetic dataset\"\n        )\n        plt.legend()\n        out_path = os.path.join(working_dir, f\"{ds_name}_aggregated_loss_curve.png\")\n        plt.savefig(out_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss plot for {ds_name}: {e}\")\n        plt.close()\n"], "plot_plan": ["We will load the saved experiment data from the working directory and extract the metrics and losses for the synthetic dataset across learning rates. Then we construct two summary figures: one showing worst\u2010group accuracy and another showing losses, each as side\u2010by\u2010side subplots for training vs validation curves over epochs. We label axes, include legends for each learning rate, and add a main title with a clear subtitle indicating which side is training vs validation. Each figure is generated inside its own try\u2010except block to catch errors, save to the working directory with descriptive filenames, and always close the figure afterward.", "We first load the saved experiment data from disk and iterate over each synthetic dataset configuration under `multiple_synthetic`. For each dataset, we extract the arrays of training/validation weighted accuracies and losses, as well as the spurious correlation and feature dimension metadata. We then create two line\u2010plots per dataset\u2014one for weighted accuracy per epoch and one for loss per epoch\u2014using separate try/except blocks to catch and report any plotting errors and to always close the figure. Each curve is labeled by its corresponding learning rate and split (Train/Val), and the axes and legend are clearly annotated. We title each plot with the dataset identifier and its spurious correlation and dimension settings. Finally, we save the figures into the working directory with descriptive filenames that include both the dataset name and the plot type. All plotting is done with basic Matplotlib and no simulated data.", "We will load the saved experiment data from the working directory and extract synthetic\u2010dataset metrics and losses.  We then compute overall test accuracy per run and print those values.  Next, we build two separate visualizations, each inside its own try\u2010except block.  The first figure contains two subplots showing worst\u2010group training and validation accuracy curves across epochs, with a clear left/right subtitle and dataset name.  The second figure similarly shows training and validation loss curves.  Each plot uses a legend to distinguish runs and is saved with a descriptive filename containing \u201csynthetic\u201d and the plot type.  We always close figures after saving, and in case of an error we report it and still close the figure.", "First, we set up the working directory and attempt to load the saved `experiment_data.npy`. We then extract the synthetic results under `NO_SPURIOUS_FEATURE`, including learning rates, per\u2010epoch weighted group accuracies and losses, and final test predictions and ground truth. In the first plot block, we render training vs validation weighted group accuracy curves over epochs for each learning rate, using solid and dashed lines, and save with a descriptive filename. In the second block, we similarly plot the corresponding loss curves. Finally, we compute and bar\u2010plot the test accuracy for each learning rate by comparing predictions to the ground truth. Each visualization is wrapped in its own try\u2010except to catch errors, print diagnostics, and ensure figures are closed after saving. All figures carry multi\u2010line titles clarifying dataset type and curve meanings, and are written to the working directory with explicit names.", "We will load the saved experiment data and then generate two composite figures. The first figure contains side-by-side subplots of weighted group accuracy over epochs for training and validation on the synthetic dataset, with each curve corresponding to a different learning rate. The second figure similarly displays training and validation loss curves. We annotate each subplot with titles, axis labels, and legends, and add an overall subtitle indicating \u201cLeft: Training, Right: Validation.\u201d All plotting logic is enclosed in separate try-except blocks to catch and report errors without interrupting downstream code, and we always close figures after saving. Descriptive filenames including the dataset name and plot type are used when saving to the working directory.", "We will load the saved experiment dictionary from the working directory and extract the synthetic dataset entry.  In separate try-except blocks, we will plot (1) weighted\u2010group accuracy curves over epochs for both training and validation across all learning rates, (2) training and validation loss curves, and (3) a bar chart of final test accuracy versus learning rate.  Each figure will have a clear main title indicating the synthetic dataset and subplots or axes titles for left/right content.  Plots will be saved as PNG files in the working directory with descriptive filenames and figures will be closed after saving.  We will also compute and print the test accuracy for each learning rate.  Any errors in loading or plotting are caught and printed, ensuring figures are always closed.", "We load the saved experiment data from the working directory and extract the synthetic\u2010dataset metrics and losses. Then in separate try/except blocks we (1) plot worst\u2010group accuracy curves over epochs for each run (implicitly corresponding to different learning rates), (2) plot training versus validation loss curves over epochs, and (3) compute per\u2010run test\u2010set accuracy from the stored predictions and ground truth and display it as a bar chart. Each figure is titled with the dataset and plot type, includes axis labels and legends (where needed), and is saved under a descriptive filename in the working folder. We always close each figure after saving and handle errors gracefully without simulating any data beyond what\u2019s in `experiment_data.npy`.", "We will set up the working directory and load the saved experiment data from `experiment_data.npy`. Then we extract the synthetic sub-dictionary under `\"group_inverse_frequency_reweighting\"`, obtaining arrays of learning rates, worst\u2010group accuracy metrics and loss values for both training and validation. In the first try\u2010except block we create a figure and plot worst\u2010group accuracy versus epochs for each learning rate, using solid lines for training and dashed for validation, labeling axes and adding a legend as clear subtitles, then saving with a descriptive filename. In the second block we repeat the pattern for the loss curves. Each plot is wrapped in its own try\u2010except to catch and report errors and we always close figures to free resources.", null, null, "We load the saved experiment data and extract the synthetic dataset\u2019s worst\u2010group accuracies, losses, predictions, and ground truth. We reconstruct the epoch axis and manually define the learning rates used (1e\u20104, 1e\u20103, 1e\u20102). We then create three plots: (1) worst\u2010group accuracy vs. epoch for train/val splits across all learning rates, (2) loss vs. epoch for train/val splits, and (3) a bar chart of final test accuracies by learning rate. Each plot is wrapped in a separate try\u2010except block to handle errors and ensure figures are closed. All figures are saved under descriptive names in the working directory. Finally, we print the computed test accuracies for each learning rate.", "We load the experiment data dictionary from the working directory and iterate through each experimental setting and dataset. For each dataset, we extract the learning rates, worst-group accuracies, and losses for both training and validation splits. We generate a two\u2010panel plot showing training curves on the left and validation curves on the right for worst\u2010group accuracy, annotate the figure with a subtitle indicating the split and dataset name, and save it. We repeat the same layout for loss curves. Finally, we summarize the final validation worst\u2010group accuracies across learning rates in a bar chart. All plotting is wrapped in try\u2010except blocks to catch errors and ensure figures are closed. The code follows basic matplotlib and numpy conventions and saves all figures under descriptive filenames in the working directory.", "We first load the saved experiment data from the working directory and pull out the synthetic dataset entries under the random_cluster_reweighting experiment. From the metric and loss arrays we infer the number of runs and epochs to set up x\u2010axes and labels. In the first try\u2010except block we create a figure with two subplots: left for train worst\u2010group accuracy over epochs and right for validation worst\u2010group accuracy, each annotated with titles and a legend. In the second block we similarly produce a two\u2010panel figure for train and validation loss curves. Every figure uses a clear suptitle indicating the data type and describes the left/right panels, and each is saved in the working directory with a descriptive filename before being closed. Errors during plotting are caught and printed, ensuring clean figure handling regardless of failures.", "We begin by setting up the working directory and loading the saved experiment data from `experiment_data.npy`. We extract the synthetic dataset\u2019s learning rates, worst\u2010group accuracy metrics, loss metrics, test predictions, and ground truth labels. We then produce three separate figures: (1) worst\u2010group accuracy curves across epochs for all learning rates (solid lines for training, dashed for validation), (2) loss curves likewise, and (3) a bar chart showing test\u2010set accuracy for each learning rate. Each plot is wrapped in its own try\u2010except block to catch and report any errors, and we always close the figure after saving. Titles clearly note the synthetic dataset and include subtitles describing line styles or bar meanings. Finally, we save each figure to `working_dir` using descriptive filenames.", "In this solution, we load the saved experimental results from experiment_data.npy, focusing on the synthetic dataset under the cluster count variation experiments. We extract the cluster counts, learning rates, and the recorded worst-group accuracy and loss curves for both training and validation over epochs. For each cluster count, we generate a two-panel plot where the left panel shows worst-group accuracy curves for training versus validation across epochs and different learning rates, and the right panel shows the corresponding loss curves. Each plot is saved to the working directory with a descriptive filename indicating the dataset, cluster count, and type of curves. We wrap each figure creation in its own try-except block to ensure that errors in one plot do not interrupt the entire script and always close the figure after saving. After plotting, we compute and print the final validation worst-group accuracy for each cluster count and learning rate combination to summarize performance. We assume the working directory exists and use only basic Matplotlib without custom styling.", "I will load the saved experiment data from the working directory and extract the synthetic dataset entries. From there, I will retrieve the arrays for weight decay values, worst-group accuracy metrics (train and validation), loss metrics (train and validation), and the test set predictions with ground truth labels. I will create three separate plots in try-except blocks: one for worst-group accuracy curves over epochs, one for loss curves over epochs, and one bar chart of final test accuracies across weight decays. Each figure will include a clear title specifying the synthetic dataset, and bar/line legends where applicable. All plots will be saved into the working directory with descriptive filenames that include both the dataset name and the type of plot. I will ensure each figure is closed after saving, and on any error in creation the exception will be printed and the figure closed as well.", "We will load the saved experiment data from the working directory and extract the synthetic dataset results for loss\u2010based sample weighting. First, we plot worst\u2010group accuracy curves over epochs for both training and validation splits, overlaying curves for each learning rate. Next, we plot training and validation cross\u2010entropy loss curves across epochs by learning rate. Finally, we visualize the distribution of learned sample weights via histograms for each learning rate in a single figure with subplots. Each figure is wrapped in its own try\u2010except block to catch errors, includes clear titles and subtitles specifying the synthetic dataset, and is saved to the working directory with descriptive filenames. All figures are closed after saving to free resources.", "We will load the saved experiment data and iterate over the two reweighting strategies (\u201craw\u201d and \u201cnormalized\u201d) on the synthetic dataset. For each strategy we extract the worst\u2010group accuracy and average losses for training and validation over epochs across runs. We then plot the train/validation worst\u2010group accuracy curves and train/validation loss curves in separate figures, each wrapped in its own try\u2010except block. Each figure includes a figure\u2010level title specifying the method and the synthetic dataset, and an axes\u2010level subtitle clarifying \u201cLeft: Train, Right: Validation.\u201d We label each curve by its run index since the exact learning rates are not stored in the data file. Figures are saved under descriptive names like \u201craw_gradient_cluster_reweighting_synthetic_wg_accuracy_curve.png\u201d into the working directory, creating it if necessary. We always close figures after saving or upon catching an error, and we print any loading or plotting errors. This script provides a quick visualization of how worst\u2010group accuracy and losses evolve for both ablations.", "Here is the outline:  \n1. Load the saved experiment data from working_dir.  \n2. Retrieve the two synthetic dataset entries under the \u201cNO_FEATURE_NORMALIZATION\u201d key.  \n3. For each dataset, extract the training/validation weighted\u2010accuracy and loss arrays, as well as the swept learning rates.  \n4. Create two separate figures per dataset: one for WG\u2010accuracy curves and one for loss curves.  \n5. In each figure, use a 1\u00d72 subplot layout: left panel for training curves, right for validation, with one line per learning rate.  \n6. Add clear super\u2010titles naming the dataset and metric type, axis labels, titles for each panel, and legends for LRs.  \n7. Wrap each plotting block in try\u2010except, always closing the figure and printing an error if one occurs.  \n8. Save each plot to working_dir with descriptive names like \u201c<dataset>_accuracy.png\u201d or \u201c<dataset>_loss.png\u201d.", null, null, null, null], "ablation_name": [null, "Multiple Synthetic Datasets Ablation", "NO_CLUSTER_REWEIGHTING", "NO_SPURIOUS_FEATURE", "LINEAR_CLASSIFIER", "INPUT_FEATURE_CLUSTER_REWEIGHTING", "REPRESENTATION_CLUSTER_REWEIGHTING", "GROUP_INVERSE_FREQUENCY_REWEIGHTING", "RANDOM_CLUSTER_REWEIGHTING", "SGD_OPTIMIZER", null, "NO_WARMUP_CLUSTER_REWEIGHTING", null, null, "CLUSTER_COUNT_VARIATION", "WEIGHT_DECAY_VARIATION", "LOSS_BASED_SAMPLE_WEIGHTING", "NORMALIZED_GRADIENT_CLUSTER_REWEIGHTING", "NO_FEATURE_NORMALIZATION", null, null, null, null], "hyperparam_name": ["learning_rate", null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["This script loads the NumPy file saved under the `working` directory to extract\nthe final-epoch worst\u2010group accuracies and average losses for the training and\nvalidation splits, and selects the learning rate that yields the highest\nvalidation worst\u2010group accuracy. It then prints clearly labeled metrics for the\ntraining and validation sets. Finally, it computes and prints the overall test\naccuracy by comparing the saved test predictions with the ground truth labels.", "The following script loads the saved experiment data from the working directory,\niterates over each synthetic dataset, and pulls out the final\u2010epoch metrics\nacross learning rates. It then selects the learning rate that achieved the\nhighest final validation worst\u2013group accuracy and prints the corresponding\ntraining worst\u2013group accuracy, validation worst\u2013group accuracy, training loss,\nvalidation loss, and test accuracy for that best run. All code is at global\nscope and runs immediately without any special entry point.", "I will load the saved experiment_data.npy from the working folder, extract the\nnumpy arrays for training and validation worst\u2010group accuracies and losses, as\nwell as the test predictions and ground truth. I then select the hyperparameter\nrun that achieved the highest final validation worst\u2010group accuracy and pull out\nits final training and validation metrics, compute its overall test accuracy,\nand print each metric under the appropriate dataset heading with clear labels.\nAll of this is done at the global scope so that running the script immediately\nexecutes the extraction and printing.", "", "The following script loads the saved experiment data, extracts the worst\u2010group\naccuracies for the training and validation sets at the final epoch, and selects\nthe run with the highest validation worst\u2010group accuracy. It then computes the\noverall test accuracy for that best model. Finally, it prints each dataset\u2019s\nname followed by its corresponding metric, clearly labeled.", "I will load the saved numpy experiment data from the working directory, iterate\nover each method and dataset, and pull out the recorded train/validation\naccuracies and losses along with test predictions. For each dataset I will\ncompute the maximum accuracy and minimum loss across all learning rates and\nepochs, as well as the best test accuracy by comparing predictions to the ground\ntruth. The script prints the dataset name followed by clearly labeled metrics\nand executes immediately without any special entry point guards. It uses only\nglobal\u2010scope code along with basic numpy operations for succinct analysis.", "The script loads the saved `experiment_data.npy` file from the working\ndirectory, extracts the stored train and validation worst\u2010group accuracies and\nlosses, and computes the test accuracy from the saved predictions and ground\ntruth. It then iterates over the original learning\u2010rate settings, printing the\ndataset name followed by precise metric labels (`train accuracy`, `train loss`,\n`validation accuracy`, `validation loss`, and `test accuracy`) for the final\nepoch of each run. All code runs at the global scope without any special entry\npoint checks.", "This script locates and loads the `experiment_data.npy` file from the working\ndirectory, then extracts the stored learning rates, train/validation worst\u2010group\naccuracies, losses, test predictions, and ground truth. It iterates over each\nlearning rate and prints each dataset name (\"Train dataset\", \"Validation\ndataset\", \"Test dataset\") followed by descriptive metrics: final worst\u2010group\naccuracy and average loss for training and validation, and overall test\naccuracy. All metric names are fully labeled (e.g., \"train worst\u2010group\naccuracy\", \"validation average loss\", \"test overall accuracy\"), and the code\nruns immediately at global scope without any entry\u2010point guard.", "", "", "Below is a script that immediately loads the saved `experiment_data.npy` from\nthe `working` directory, extracts the synthetic dataset metrics, and determines\nwhich learning rate achieved the highest final validation worst\u2010group accuracy.\nIt then pulls out the corresponding final train worst\u2010group accuracy and\nvalidation loss, computes the test F1 score for that best setting, and prints\neach metric using clear labels.", "The script sets the working directory and loads the saved `experiment_data.npy`\nfile. It extracts the per-epoch worst-group accuracies and losses for both the\ntraining and validation splits as well as the final predictions and ground\ntruths for the test split. For each learning rate, it prints out the final train\nworst-group accuracy, final train loss, final validation worst-group accuracy,\nfinal validation loss, and overall test accuracy. No plotting or entry-point\nguard is used, and the script executes immediately.", "I will load the saved experiment data from the working directory, extract the\ntraining and validation worst\u2010group accuracies and losses for each learning\nrate, and compute the overall test accuracy. For each learning rate, the script\nprints the \u201cSynthetic\u201d dataset name followed by clearly labeled metric names and\nonly the final epoch values. All code runs at global scope and there are no\nplots or a main guard.", "", "The script below loads the saved experiment dictionary, extracts the arrays for\ncluster counts, learning rates, metrics, losses, predictions, and ground truth,\nthen iterates over each cluster\u2010count and learning\u2010rate pair. For each\ncombination it grabs the final epoch values of training/validation accuracy and\nloss, computes test accuracy, and prints them with explicit dataset and metric\nnames. All code runs at global scope and executes immediately when run.", "The script below loads the saved NumPy experiment data from the `working`\ndirectory, extracts the final train and validation worst\u2010group accuracies and\nlosses for each weight decay setting, and computes the overall test accuracy\nfrom the stored predictions and ground truth. It then iterates over each weight\ndecay value, printing the dataset name (\u201csynthetic\u201d) before clearly labeled\nmetrics (\u201ctrain worst\u2010group accuracy,\u201d \u201ctrain loss,\u201d \u201cvalidation worst\u2010group\naccuracy,\u201d \u201cvalidation loss,\u201d and \u201ctest accuracy\u201d) using only the last epoch\nvalues. All code runs at the global scope immediately upon execution, without\nany special entry point guards.", "", "I will load the saved experiment dictionary and loop through each reweighting\nablation and its synthetic dataset, extracting final epoch metrics for the best\nlearning rate according to validation weighted\u2010group accuracy. For each\nexperiment, I will compute and print the dataset identifier, the final train\naccuracy, validation accuracy, train loss, validation loss, and overall test\naccuracy. The script will run immediately upon execution and will not require\nany special entry point.", "Here is a script that loads the saved numpy experiment data, iterates over each\nablation setting, extracts the worst-group accuracies for training and\nvalidation, computes the best final-epoch value across learning rates, and\nprints them with clear labels:", "The following script loads the saved experiment data from the working directory,\niterates over each synthetic dataset, and pulls out the final\u2010epoch metrics\nacross learning rates. It then selects the learning rate that achieved the\nhighest final validation worst\u2013group accuracy and prints the corresponding\ntraining worst\u2013group accuracy, validation worst\u2013group accuracy, training loss,\nvalidation loss, and test accuracy for that best run. All code is at global\nscope and runs immediately without any special entry point.", "The following script loads the saved experiment data from the working directory,\niterates over each synthetic dataset, and pulls out the final\u2010epoch metrics\nacross learning rates. It then selects the learning rate that achieved the\nhighest final validation worst\u2013group accuracy and prints the corresponding\ntraining worst\u2013group accuracy, validation worst\u2013group accuracy, training loss,\nvalidation loss, and test accuracy for that best run. All code is at global\nscope and runs immediately without any special entry point.", "The following script loads the saved experiment data from the working directory,\niterates over each synthetic dataset, and pulls out the final\u2010epoch metrics\nacross learning rates. It then selects the learning rate that achieved the\nhighest final validation worst\u2013group accuracy and prints the corresponding\ntraining worst\u2013group accuracy, validation worst\u2013group accuracy, training loss,\nvalidation loss, and test accuracy for that best run. All code is at global\nscope and runs immediately without any special entry point.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Load the saved experiment data\ndata_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Navigate to the synthetic learning-rate sweep results\nsynthetic = experiment_data[\"learning_rate\"][\"synthetic\"]\nlrs = synthetic[\"lrs\"]\nmetrics_train = synthetic[\"metrics\"][\"train\"]  # shape: (num_lrs, epochs)\nmetrics_val = synthetic[\"metrics\"][\"val\"]\nlosses_train = synthetic[\"losses\"][\"train\"]\nlosses_val = synthetic[\"losses\"][\"val\"]\npredictions = synthetic[\"predictions\"]  # shape: (num_lrs, num_test_samples)\nground_truth = synthetic[\"ground_truth\"]\n\n# Extract final-epoch values\nfinal_train_wg = metrics_train[:, -1]\nfinal_val_wg = metrics_val[:, -1]\nfinal_train_loss = losses_train[:, -1]\nfinal_val_loss = losses_val[:, -1]\n\n# Select the best learning rate by highest final validation worst-group accuracy\nbest_idx = np.argmax(final_val_wg)\n\n# Print metrics with clear labels\nprint(\"Training Dataset Metrics:\")\nprint(f\"  Final training worst-group accuracy: {final_train_wg[best_idx]:.4f}\")\nprint(f\"  Final training average loss: {final_train_loss[best_idx]:.4f}\\n\")\n\nprint(\"Validation Dataset Metrics:\")\nprint(f\"  Final validation worst-group accuracy: {final_val_wg[best_idx]:.4f}\")\nprint(f\"  Final validation average loss: {final_val_loss[best_idx]:.4f}\\n\")\n\n# Compute and print test accuracy\ntest_acc = np.mean(predictions[best_idx] == ground_truth)\nprint(\"Test Dataset Metrics:\")\nprint(f\"  Test accuracy: {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract and print metrics for each dataset\nfor ds_name, ds_data in experiment_data[\"multiple_synthetic\"].items():\n    # Retrieve stored arrays\n    metrics_train = ds_data[\"metrics\"][\"train\"]  # shape: (n_lrs, epochs)\n    metrics_val = ds_data[\"metrics\"][\"val\"]  # shape: (n_lrs, epochs)\n    losses_train = ds_data[\"losses\"][\"train\"]  # shape: (n_lrs, epochs)\n    losses_val = ds_data[\"losses\"][\"val\"]  # shape: (n_lrs, epochs)\n    preds = ds_data[\"predictions\"]  # shape: (n_lrs, n_test)\n    truths = ds_data[\"ground_truth\"]  # shape: (n_test,)\n\n    # Final\u2010epoch values for each LR\n    final_train_acc = metrics_train[:, -1]\n    final_val_acc = metrics_val[:, -1]\n    final_train_loss = losses_train[:, -1]\n    final_val_loss = losses_val[:, -1]\n    test_acc = (preds == truths).mean(axis=1)\n\n    # Select the LR with highest final validation worst\u2010group accuracy\n    best_idx = np.argmax(final_val_acc)\n\n    # Print dataset and its best metrics\n    print(f\"Dataset: {ds_name}\")\n    print(f\"Train worst-group accuracy:      {final_train_acc[best_idx]:.4f}\")\n    print(f\"Validation worst-group accuracy: {final_val_acc[best_idx]:.4f}\")\n    print(f\"Training loss:                   {final_train_loss[best_idx]:.4f}\")\n    print(f\"Validation loss:                 {final_val_loss[best_idx]:.4f}\")\n    print(f\"Test accuracy:                   {test_acc[best_idx]:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Navigate to the synthetic results under NO_CLUSTER_REWEIGHTING\nresults = experiment_data[\"NO_CLUSTER_REWEIGHTING\"][\"synthetic\"]\nmetrics_train = results[\"metrics\"][\"train\"]  # shape: (n_runs, n_epochs)\nmetrics_val = results[\"metrics\"][\"val\"]  # shape: (n_runs, n_epochs)\nlosses_train = results[\"losses\"][\"train\"]  # shape: (n_runs, n_epochs)\nlosses_val = results[\"losses\"][\"val\"]  # shape: (n_runs, n_epochs)\npredictions = results[\"predictions\"]  # shape: (n_runs, n_test_samples)\nground_truth = results[\"ground_truth\"]  # shape: (n_test_samples,)\n\n# Select the run with the highest final validation worst\u2010group accuracy\nfinal_val_acc = metrics_val[:, -1]\nbest_run = np.argmax(final_val_acc)\n\n# Extract the final metrics for the best run\ntrain_wg_acc = metrics_train[best_run, -1]\nval_wg_acc = metrics_val[best_run, -1]\ntrain_loss = losses_train[best_run, -1]\nval_loss = losses_val[best_run, -1]\ntest_acc = np.mean(predictions[best_run] == ground_truth)\n\n# Print results with clear dataset names and metric labels\nprint(\"Training dataset\")\nprint(f\"train worst-group accuracy: {train_wg_acc:.4f}\")\nprint(f\"train loss: {train_loss:.4f}\")\n\nprint(\"Validation dataset\")\nprint(f\"validation worst-group accuracy: {val_wg_acc:.4f}\")\nprint(f\"validation loss: {val_loss:.4f}\")\n\nprint(\"Test dataset\")\nprint(f\"test accuracy: {test_acc:.4f}\")\n", "", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nfile_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Extract the synthetic experiment results\nexp = experiment_data[\"linear_classifier\"][\"synthetic\"]\nmetrics_train = exp[\"metrics\"][\"train\"]  # shape: (n_runs, n_epochs)\nmetrics_val = exp[\"metrics\"][\"val\"]  # shape: (n_runs, n_epochs)\npredictions = exp[\"predictions\"]  # shape: (n_runs, test_size)\nground_truth = exp[\"ground_truth\"]  # shape: (test_size,)\n\n# Get final-epoch worst-group accuracies\ntrain_final = metrics_train[:, -1]\nval_final = metrics_val[:, -1]\n\n# Select the run with the best validation worst-group accuracy\nbest_run = np.argmax(val_final)\n\n# Compute overall test accuracy for the best model\ntest_preds = predictions[best_run]\ntest_accuracy = (test_preds == ground_truth).mean()\n\n# Print the metrics with clear labels\nprint(\"Training dataset:\")\nprint(f\"train worst-group accuracy: {train_final[best_run]:.4f}\")\n\nprint(\"Validation dataset:\")\nprint(f\"validation worst-group accuracy: {val_final[best_run]:.4f}\")\n\nprint(\"Test dataset:\")\nprint(f\"test accuracy: {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the experimental results\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each experiment and dataset\nfor method_name, method_data in experiment_data.items():\n    for dataset_name, ds_data in method_data.items():\n        # Extract metrics and losses\n        train_acc = ds_data[\"metrics\"][\"train\"]  # shape: (n_lrs, n_epochs)\n        val_acc = ds_data[\"metrics\"][\"val\"]\n        train_loss = ds_data[\"losses\"][\"train\"]\n        val_loss = ds_data[\"losses\"][\"val\"]\n        # Predictions and ground truth for test set\n        preds = ds_data[\"predictions\"]  # shape: (n_lrs, n_test)\n        gts = ds_data[\"ground_truth\"]  # shape: (n_test,)\n\n        # Compute best accuracies and losses across all LRs and epochs\n        best_train_accuracy = np.max(train_acc)\n        best_val_accuracy = np.max(val_acc)\n        min_train_loss = np.min(train_loss)\n        min_val_loss = np.min(val_loss)\n\n        # Compute test accuracy per LR and take the best\n        test_accuracies = (preds == gts).mean(axis=1)\n        best_test_accuracy = np.max(test_accuracies)\n\n        # Print results\n        print(f\"Dataset: {dataset_name}\")\n        print(f\"train accuracy: {best_train_accuracy:.4f}\")\n        print(f\"validation accuracy: {best_val_accuracy:.4f}\")\n        print(f\"train loss: {min_train_loss:.4f}\")\n        print(f\"validation loss: {min_val_loss:.4f}\")\n        print(f\"test accuracy: {best_test_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract stored metrics, losses, predictions, and ground truth\nmetrics = experiment_data[\"representation_cluster_reweighting\"][\"synthetic\"][\"metrics\"]\nlosses = experiment_data[\"representation_cluster_reweighting\"][\"synthetic\"][\"losses\"]\npredictions = experiment_data[\"representation_cluster_reweighting\"][\"synthetic\"][\n    \"predictions\"\n]\nground_truth = experiment_data[\"representation_cluster_reweighting\"][\"synthetic\"][\n    \"ground_truth\"\n]\n\n# Define the learning rates in their original order\nlearning_rates = [1e-4, 1e-3, 1e-2]\n\n# Print the final epoch metrics for each dataset and learning rate\nfor idx, lr in enumerate(learning_rates):\n    train_acc = metrics[\"train\"][idx, -1]\n    train_loss = losses[\"train\"][idx, -1]\n    val_acc = metrics[\"val\"][idx, -1]\n    val_loss = losses[\"val\"][idx, -1]\n    test_preds = predictions[idx]\n    test_acc = np.mean(test_preds == ground_truth)\n\n    print(\n        f\"Train dataset (lr={lr}): train accuracy = {train_acc:.4f}, train loss = {train_loss:.4f}\"\n    )\n    print(\n        f\"Validation dataset (lr={lr}): validation accuracy = {val_acc:.4f}, validation loss = {val_loss:.4f}\"\n    )\n    print(f\"Test dataset (lr={lr}): test accuracy = {test_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract relevant fields\nexp = experiment_data[\"group_inverse_frequency_reweighting\"][\"synthetic\"]\nlrs = exp[\"lrs\"]\ntrain_metrics = exp[\"metrics\"][\"train\"]  # shape: (num_lrs, num_epochs)\nval_metrics = exp[\"metrics\"][\"val\"]\ntrain_losses = exp[\"losses\"][\"train\"]\nval_losses = exp[\"losses\"][\"val\"]\npredictions = exp[\"predictions\"]  # shape: (num_lrs, num_test_samples)\nground_truth = exp[\"ground_truth\"]  # shape: (num_test_samples,)\n\n# Print train metrics\nprint(\"Train dataset:\")\nfor lr, accs, losses in zip(lrs, train_metrics, train_losses):\n    final_acc = accs[-1]\n    final_loss = losses[-1]\n    print(f\"train worst-group accuracy (lr={lr}): {final_acc:.4f}\")\n    print(f\"train average loss         (lr={lr}): {final_loss:.4f}\")\n\n# Print validation metrics\nprint(\"\\nValidation dataset:\")\nfor lr, accs, losses in zip(lrs, val_metrics, val_losses):\n    final_acc = accs[-1]\n    final_loss = losses[-1]\n    print(f\"validation worst-group accuracy (lr={lr}): {final_acc:.4f}\")\n    print(f\"validation average loss         (lr={lr}): {final_loss:.4f}\")\n\n# Print test metrics (overall accuracy)\nprint(\"\\nTest dataset:\")\nfor lr, preds in zip(lrs, predictions):\n    overall_acc = np.mean(preds == ground_truth)\n    print(f\"test overall accuracy (lr={lr}): {overall_acc:.4f}\")\n", "", "", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Extract synthetic dataset metrics\nsd = experiment_data[\"random_cluster_reweighting\"][\"synthetic\"]\ntrain_acc = sd[\"metrics\"][\"train\"]  # shape: (num_lrs, epochs)\nval_acc = sd[\"metrics\"][\"val\"]  # shape: (num_lrs, epochs)\ntrain_loss = sd[\"losses\"][\"train\"]\nval_loss = sd[\"losses\"][\"val\"]\npreds = sd[\"predictions\"]  # shape: (num_lrs, num_test)\ntruth = sd[\"ground_truth\"]  # shape: (num_test,)\n\n# Select the learning rate with highest final validation worst\u2010group accuracy\nfinal_val_acc = val_acc[:, -1]\nbest_idx = int(np.argmax(final_val_acc))\n\n# Retrieve the corresponding final metrics\nbest_train_acc = train_acc[best_idx, -1]\nbest_val_acc = val_acc[best_idx, -1]\nbest_train_loss = train_loss[best_idx, -1]\nbest_val_loss = val_loss[best_idx, -1]\n\n# Compute test F1 score for the best configuration\ntest_pred = preds[best_idx]\ntp = np.sum((test_pred == 1) & (truth == 1))\nfp = np.sum((test_pred == 1) & (truth == 0))\nfn = np.sum((test_pred == 0) & (truth == 1))\nprecision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\nrecall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\ntest_f1 = (\n    (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n)\n\n# Print results\nprint(\"Dataset: synthetic\")\nprint(f\"train worst-group accuracy: {best_train_acc:.4f}\")\nprint(f\"validation worst-group accuracy: {best_val_acc:.4f}\")\nprint(f\"train loss: {best_train_loss:.4f}\")\nprint(f\"validation loss: {best_val_loss:.4f}\")\nprint(f\"test F1 score: {test_f1:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(experiment_path, allow_pickle=True).item()\n\n# Navigate to the synthetic experiment results\nexp = experiment_data[\"no_warmup_cluster_reweighting\"][\"synthetic\"]\nlearning_rates = exp[\"learning_rates\"]\ntrain_metrics = exp[\"metrics\"][\"train\"]\nval_metrics = exp[\"metrics\"][\"val\"]\ntrain_losses = exp[\"losses\"][\"train\"]\nval_losses = exp[\"losses\"][\"val\"]\npredictions = exp[\"predictions\"]\nground_truth = exp[\"ground_truth\"]\n\n# Print the final metrics for each learning rate\nfor lr, tr_accs, va_accs, tr_los, va_los, preds in zip(\n    learning_rates, train_metrics, val_metrics, train_losses, val_losses, predictions\n):\n    print(f\"\\nLearning rate = {lr}\")\n    print(\"Train dataset:\")\n    print(f\"  train worst-group accuracy: {tr_accs[-1]:.4f}\")\n    print(f\"  train loss: {tr_los[-1]:.4f}\")\n    print(\"Validation dataset:\")\n    print(f\"  validation worst-group accuracy: {va_accs[-1]:.4f}\")\n    print(f\"  validation loss: {va_los[-1]:.4f}\")\n    test_accuracy = np.mean(preds == ground_truth)\n    print(\"Test dataset:\")\n    print(f\"  test accuracy: {test_accuracy:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract the synthetic dataset metrics\nsd = experiment_data[\"random_cluster_reweighting\"][\"synthetic\"]\ntrain_wg_acc = sd[\"metrics\"][\"train\"]  # worst\u2010group accuracy per epoch\nval_wg_acc = sd[\"metrics\"][\"val\"]\ntrain_loss = sd[\"losses\"][\"train\"]  # loss per epoch\nval_loss = sd[\"losses\"][\"val\"]\npredictions = sd[\"predictions\"]  # test predictions per run\nground_truth = sd[\"ground_truth\"]  # test true labels\n\n# Compute overall test accuracy for each run\ntest_acc = (predictions == ground_truth).mean(axis=1)\n\n# Define the learning rates in the same order as the saved runs\nlearning_rates = [1e-4, 1e-3, 1e-2]\n\n# Print the final metrics for each learning rate\nprint(\"Synthetic dataset\")\nfor idx, lr in enumerate(learning_rates):\n    print(f\"Learning rate = {lr}\")\n    print(f\"Train worst-group accuracy:      {train_wg_acc[idx, -1]:.4f}\")\n    print(f\"Validation worst-group accuracy: {val_wg_acc[idx, -1]:.4f}\")\n    print(f\"Train loss:                      {train_loss[idx, -1]:.4f}\")\n    print(f\"Validation loss:                 {val_loss[idx, -1]:.4f}\")\n    print(f\"Test accuracy:                   {test_acc[idx]:.4f}\\n\")\n", "", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_data = np.load(\n    os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n).item()\n\n# Navigate into the nested structure\nsynthetic_data = experiment_data[\"CLUSTER_COUNT_VARIATION\"][\"synthetic\"]\ncluster_counts = synthetic_data[\"cluster_counts\"]\nlr_block = synthetic_data[\"learning_rate\"]\nlearning_rates = lr_block[\"lrs\"]\n\n# Extract stored arrays\nmetrics_train = lr_block[\"metrics\"][\"train\"]  # shape: (n_clusters, n_lrs, epochs)\nmetrics_val = lr_block[\"metrics\"][\"val\"]\nlosses_train = lr_block[\"losses\"][\"train\"]\nlosses_val = lr_block[\"losses\"][\"val\"]\npreds_arr = lr_block[\"predictions\"]  # shape: (n_clusters, n_lrs, n_test_samples)\nground_truth = lr_block[\"ground_truth\"]  # shape: (n_test_samples,)\n\n# Iterate and print final values\nfor i, k in enumerate(cluster_counts):\n    for j, lr in enumerate(learning_rates):\n        # Final epoch metrics\n        final_train_acc = metrics_train[i, j, -1]\n        final_train_loss = losses_train[i, j, -1]\n        final_val_acc = metrics_val[i, j, -1]\n        final_val_loss = losses_val[i, j, -1]\n        # Compute test accuracy\n        test_preds = preds_arr[i, j]\n        test_accuracy = np.mean(test_preds == ground_truth)\n\n        # Print results\n        print(f\"Configuration: cluster_count = {k}, learning_rate = {lr}\")\n        print(\"Dataset: Training dataset\")\n        print(f\"train accuracy: {final_train_acc:.4f}\")\n        print(f\"train loss: {final_train_loss:.4f}\")\n        print(\"Dataset: Validation dataset\")\n        print(f\"validation accuracy: {final_val_acc:.4f}\")\n        print(f\"validation loss: {final_val_loss:.4f}\")\n        print(\"Dataset: Test dataset\")\n        print(f\"test accuracy: {test_accuracy:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Navigate to the synthetic weight decay variation\nsyn_data = experiment_data[\"weight_decay_variation\"][\"synthetic\"]\nweight_decays = syn_data[\"weight_decays\"]\nmetrics = syn_data[\"metrics\"]\nlosses = syn_data[\"losses\"]\npredictions = syn_data[\"predictions\"]\nground_truth = syn_data[\"ground_truth\"]\n\n# Print the final metrics for each weight decay\nfor idx, wd in enumerate(weight_decays):\n    print(f\"Dataset: synthetic (weight_decay = {wd})\")\n    train_wg_acc = metrics[\"train\"][idx, -1]\n    train_loss = losses[\"train\"][idx, -1]\n    val_wg_acc = metrics[\"val\"][idx, -1]\n    val_loss = losses[\"val\"][idx, -1]\n    test_acc = (predictions[idx] == ground_truth).mean()\n    print(f\"  Train worst-group accuracy: {train_wg_acc:.4f}\")\n    print(f\"  Train loss: {train_loss:.4f}\")\n    print(f\"  Validation worst-group accuracy: {val_wg_acc:.4f}\")\n    print(f\"  Validation loss: {val_loss:.4f}\")\n    print(f\"  Test accuracy: {test_acc:.4f}\")\n", "", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each ablation and dataset\nfor exp_name, datasets in experiment_data.items():\n    for ds_name, ds_data in datasets.items():\n        # Print dataset identifier\n        print(f\"Dataset: {exp_name} ({ds_name})\")\n        # Extract metrics arrays\n        metrics = ds_data[\"metrics\"]\n        losses = ds_data[\"losses\"]\n        preds_all = ds_data[\"predictions\"]\n        truths = ds_data[\"ground_truth\"]\n\n        # Final epoch values\n        train_wg_acc = metrics[\"train\"]  # shape: (n_lrs, n_epochs)\n        val_wg_acc = metrics[\"val\"]\n        train_loss = losses[\"train\"]\n        val_loss = losses[\"val\"]\n\n        # Select the best learning rate by highest final validation wg\u2010accuracy\n        final_val = val_wg_acc[:, -1]\n        best_idx = int(np.argmax(final_val))\n\n        # Gather final metrics at the best learning rate\n        final_train_acc = train_wg_acc[best_idx, -1]\n        final_val_acc = val_wg_acc[best_idx, -1]\n        final_train_loss = train_loss[best_idx, -1]\n        final_val_loss = val_loss[best_idx, -1]\n        test_preds = preds_all[best_idx]\n        test_acc = np.mean(test_preds == truths)\n\n        # Print the metrics\n        print(f\"  train accuracy: {final_train_acc:.4f}\")\n        print(f\"  validation accuracy: {final_val_acc:.4f}\")\n        print(f\"  train loss: {final_train_loss:.4f}\")\n        print(f\"  validation loss: {final_val_loss:.4f}\")\n        print(f\"  test accuracy: {test_acc:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over each dataset in the NO_FEATURE_NORMALIZATION ablation\nfor dataset_name, dataset_info in experiment_data[\"NO_FEATURE_NORMALIZATION\"].items():\n    # Extract worst-group accuracy metrics (shape: [num_lrs, num_epochs])\n    train_wg_acc = dataset_info[\"metrics\"][\"train\"]\n    val_wg_acc = dataset_info[\"metrics\"][\"val\"]\n\n    # Compute the best final-epoch value across learning rates\n    best_train_final = train_wg_acc[:, -1].max()\n    best_val_final = val_wg_acc[:, -1].max()\n\n    # Print results\n    print(f\"{dataset_name}\")\n    print(f\"train worst-group accuracy: {best_train_final:.4f}\")\n    print(f\"validation worst-group accuracy: {best_val_final:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract and print metrics for each dataset\nfor ds_name, ds_data in experiment_data[\"multiple_synthetic\"].items():\n    # Retrieve stored arrays\n    metrics_train = ds_data[\"metrics\"][\"train\"]  # shape: (n_lrs, epochs)\n    metrics_val = ds_data[\"metrics\"][\"val\"]  # shape: (n_lrs, epochs)\n    losses_train = ds_data[\"losses\"][\"train\"]  # shape: (n_lrs, epochs)\n    losses_val = ds_data[\"losses\"][\"val\"]  # shape: (n_lrs, epochs)\n    preds = ds_data[\"predictions\"]  # shape: (n_lrs, n_test)\n    truths = ds_data[\"ground_truth\"]  # shape: (n_test,)\n\n    # Final\u2010epoch values for each LR\n    final_train_acc = metrics_train[:, -1]\n    final_val_acc = metrics_val[:, -1]\n    final_train_loss = losses_train[:, -1]\n    final_val_loss = losses_val[:, -1]\n    test_acc = (preds == truths).mean(axis=1)\n\n    # Select the LR with highest final validation worst\u2010group accuracy\n    best_idx = np.argmax(final_val_acc)\n\n    # Print dataset and its best metrics\n    print(f\"Dataset: {ds_name}\")\n    print(f\"Train worst-group accuracy:      {final_train_acc[best_idx]:.4f}\")\n    print(f\"Validation worst-group accuracy: {final_val_acc[best_idx]:.4f}\")\n    print(f\"Training loss:                   {final_train_loss[best_idx]:.4f}\")\n    print(f\"Validation loss:                 {final_val_loss[best_idx]:.4f}\")\n    print(f\"Test accuracy:                   {test_acc[best_idx]:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract and print metrics for each dataset\nfor ds_name, ds_data in experiment_data[\"multiple_synthetic\"].items():\n    # Retrieve stored arrays\n    metrics_train = ds_data[\"metrics\"][\"train\"]  # shape: (n_lrs, epochs)\n    metrics_val = ds_data[\"metrics\"][\"val\"]  # shape: (n_lrs, epochs)\n    losses_train = ds_data[\"losses\"][\"train\"]  # shape: (n_lrs, epochs)\n    losses_val = ds_data[\"losses\"][\"val\"]  # shape: (n_lrs, epochs)\n    preds = ds_data[\"predictions\"]  # shape: (n_lrs, n_test)\n    truths = ds_data[\"ground_truth\"]  # shape: (n_test,)\n\n    # Final\u2010epoch values for each LR\n    final_train_acc = metrics_train[:, -1]\n    final_val_acc = metrics_val[:, -1]\n    final_train_loss = losses_train[:, -1]\n    final_val_loss = losses_val[:, -1]\n    test_acc = (preds == truths).mean(axis=1)\n\n    # Select the LR with highest final validation worst\u2010group accuracy\n    best_idx = np.argmax(final_val_acc)\n\n    # Print dataset and its best metrics\n    print(f\"Dataset: {ds_name}\")\n    print(f\"Train worst-group accuracy:      {final_train_acc[best_idx]:.4f}\")\n    print(f\"Validation worst-group accuracy: {final_val_acc[best_idx]:.4f}\")\n    print(f\"Training loss:                   {final_train_loss[best_idx]:.4f}\")\n    print(f\"Validation loss:                 {final_val_loss[best_idx]:.4f}\")\n    print(f\"Test accuracy:                   {test_acc[best_idx]:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract and print metrics for each dataset\nfor ds_name, ds_data in experiment_data[\"multiple_synthetic\"].items():\n    # Retrieve stored arrays\n    metrics_train = ds_data[\"metrics\"][\"train\"]  # shape: (n_lrs, epochs)\n    metrics_val = ds_data[\"metrics\"][\"val\"]  # shape: (n_lrs, epochs)\n    losses_train = ds_data[\"losses\"][\"train\"]  # shape: (n_lrs, epochs)\n    losses_val = ds_data[\"losses\"][\"val\"]  # shape: (n_lrs, epochs)\n    preds = ds_data[\"predictions\"]  # shape: (n_lrs, n_test)\n    truths = ds_data[\"ground_truth\"]  # shape: (n_test,)\n\n    # Final\u2010epoch values for each LR\n    final_train_acc = metrics_train[:, -1]\n    final_val_acc = metrics_val[:, -1]\n    final_train_loss = losses_train[:, -1]\n    final_val_loss = losses_val[:, -1]\n    test_acc = (preds == truths).mean(axis=1)\n\n    # Select the LR with highest final validation worst\u2010group accuracy\n    best_idx = np.argmax(final_val_acc)\n\n    # Print dataset and its best metrics\n    print(f\"Dataset: {ds_name}\")\n    print(f\"Train worst-group accuracy:      {final_train_acc[best_idx]:.4f}\")\n    print(f\"Validation worst-group accuracy: {final_val_acc[best_idx]:.4f}\")\n    print(f\"Training loss:                   {final_train_loss[best_idx]:.4f}\")\n    print(f\"Validation loss:                 {final_val_loss[best_idx]:.4f}\")\n    print(f\"Test accuracy:                   {test_acc[best_idx]:.4f}\\n\")\n", ""], "parse_term_out": ["['Training Dataset Metrics:', '\\n', '  Final training worst-group accuracy:\n0.9942', '\\n', '  Final training average loss: 0.0125\\n', '\\n', 'Validation\nDataset Metrics:', '\\n', '  Final validation worst-group accuracy: 0.9924',\n'\\n', '  Final validation average loss: 0.0430\\n', '\\n', 'Test Dataset\nMetrics:', '\\n', '  Test accuracy: 0.9960', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: corr_50_d5', '\\n', 'Train worst-group accuracy:      0.9851', '\\n',\n'Validation worst-group accuracy: 0.9762', '\\n', 'Training loss:\n0.4796', '\\n', 'Validation loss:                 0.4841', '\\n', 'Test accuracy:\n0.9840\\n', '\\n', 'Dataset: corr_50_d10', '\\n', 'Train worst-group accuracy:\n1.0000', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0000', '\\n', 'Validation loss:                 0.0002', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_50_d15', '\\n', 'Train worst-group accuracy:\n1.0000', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0000', '\\n', 'Validation loss:                 0.0000', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_75_d5', '\\n', 'Train worst-group accuracy:\n0.9840', '\\n', 'Validation worst-group accuracy: 0.9798', '\\n', 'Training loss:\n0.0507', '\\n', 'Validation loss:                 0.0946', '\\n', 'Test accuracy:\n0.9820\\n', '\\n', 'Dataset: corr_75_d10', '\\n', 'Train worst-group accuracy:\n0.9980', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0067', '\\n', 'Validation loss:                 0.0000', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_75_d15', '\\n', 'Train worst-group accuracy:\n0.9961', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.3340', '\\n', 'Validation loss:                 0.3285', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_95_d5', '\\n', 'Train worst-group accuracy:\n0.9842', '\\n', 'Validation worst-group accuracy: 0.9881', '\\n', 'Training loss:\n0.0272', '\\n', 'Validation loss:                 0.0294', '\\n', 'Test accuracy:\n0.9960\\n', '\\n', 'Dataset: corr_95_d10', '\\n', 'Train worst-group accuracy:\n0.9979', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0010', '\\n', 'Validation loss:                 0.0001', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_95_d15', '\\n', 'Train worst-group accuracy:\n1.0000', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0000', '\\n', 'Validation loss:                 0.0000', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Training dataset', '\\n', 'train worst-group accuracy: 0.9903', '\\n', 'train\nloss: 0.0466', '\\n', 'Validation dataset', '\\n', 'validation worst-group\naccuracy: 0.9885', '\\n', 'validation loss: 0.0542', '\\n', 'Test dataset', '\\n',\n'test accuracy: 0.9920', '\\n', 'Execution time: a moment seconds (time limit is\nan hour).']", "", "['Training dataset:', '\\n', 'train worst-group accuracy: 0.9768', '\\n',\n'Validation dataset:', '\\n', 'validation worst-group accuracy: 0.9706', '\\n',\n'Test dataset:', '\\n', 'test accuracy: 0.9740', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'train accuracy: 0.9942', '\\n', 'validation\naccuracy: 0.9885', '\\n', 'train loss: 0.0093', '\\n', 'validation loss: 0.0288',\n'\\n', 'test accuracy: 0.9900\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "['Train dataset (lr=0.0001): train accuracy = 0.3478, train loss = 0.6895',\n'\\n', 'Validation dataset (lr=0.0001): validation accuracy = 0.3588, validation\nloss = 0.6898', '\\n', 'Test dataset (lr=0.0001): test accuracy = 0.4880', '\\n',\n'Train dataset (lr=0.001): train accuracy = 0.9826, train loss = 0.4808', '\\n',\n'Validation dataset (lr=0.001): validation accuracy = 0.9622, validation loss =\n0.4860', '\\n', 'Test dataset (lr=0.001): test accuracy = 0.9820', '\\n', 'Train\ndataset (lr=0.01): train accuracy = 0.9942, train loss = 0.0063', '\\n',\n'Validation dataset (lr=0.01): validation accuracy = 0.9885, validation loss =\n0.0351', '\\n', 'Test dataset (lr=0.01): test accuracy = 0.9920', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Train dataset:', '\\n', 'train worst-group accuracy (lr=0.0001): 0.3478', '\\n',\n'train average loss         (lr=0.0001): 0.6895', '\\n', 'train worst-group\naccuracy (lr=0.001): 0.9793', '\\n', 'train average loss         (lr=0.001):\n0.4662', '\\n', 'train worst-group accuracy (lr=0.01): 0.9942', '\\n', 'train\naverage loss         (lr=0.01): 0.0099', '\\n', '\\nValidation dataset:', '\\n',\n'validation worst-group accuracy (lr=0.0001): 0.3626', '\\n', 'validation average\nloss         (lr=0.0001): 0.6898', '\\n', 'validation worst-group accuracy\n(lr=0.001): 0.9656', '\\n', 'validation average loss         (lr=0.001): 0.4738',\n'\\n', 'validation worst-group accuracy (lr=0.01): 0.9885', '\\n', 'validation\naverage loss         (lr=0.01): 0.0387', '\\n', '\\nTest dataset:', '\\n', 'test\noverall accuracy (lr=0.0001): 0.4860', '\\n', 'test overall accuracy (lr=0.001):\n0.9800', '\\n', 'test overall accuracy (lr=0.01): 0.9900', '\\n', 'Execution time:\na moment seconds (time limit is an hour).']", "", "", "['Dataset: synthetic', '\\n', 'train worst-group accuracy: 0.9942', '\\n',\n'validation worst-group accuracy: 0.9885', '\\n', 'train loss: 0.0099', '\\n',\n'validation loss: 0.0387', '\\n', 'test F1 score: 0.9899', '\\n', 'Execution time:\na moment seconds (time limit is an hour).']", "['\\nLearning rate = 0.0001', '\\n', 'Train dataset:', '\\n', '  train worst-group\naccuracy: 0.8116', '\\n', '  train loss: 0.6479', '\\n', 'Validation dataset:',\n'\\n', '  validation worst-group accuracy: 0.8092', '\\n', '  validation loss:\n0.6485', '\\n', 'Test dataset:', '\\n', '  test accuracy: 0.8640', '\\n',\n'\\nLearning rate = 0.001', '\\n', 'Train dataset:', '\\n', '  train worst-group\naccuracy: 0.9884', '\\n', '  train loss: 0.0658', '\\n', 'Validation dataset:',\n'\\n', '  validation worst-group accuracy: 0.9771', '\\n', '  validation loss:\n0.0728', '\\n', 'Test dataset:', '\\n', '  test accuracy: 0.9880', '\\n',\n'\\nLearning rate = 0.01', '\\n', 'Train dataset:', '\\n', '  train worst-group\naccuracy: 0.9942', '\\n', '  train loss: 0.0039', '\\n', 'Validation dataset:',\n'\\n', '  validation worst-group accuracy: 0.9847', '\\n', '  validation loss:\n0.0336', '\\n', 'Test dataset:', '\\n', '  test accuracy: 0.9960', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Synthetic dataset', '\\n', 'Learning rate = 0.0001', '\\n', 'Train worst-group\naccuracy:      0.3478', '\\n', 'Validation worst-group accuracy: 0.3588', '\\n',\n'Train loss:                      0.6895', '\\n', 'Validation loss:\n0.6898', '\\n', 'Test accuracy:                   0.4860\\n', '\\n', 'Learning rate\n= 0.001', '\\n', 'Train worst-group accuracy:      0.9793', '\\n', 'Validation\nworst-group accuracy: 0.9656', '\\n', 'Train loss:                      0.4662',\n'\\n', 'Validation loss:                 0.4738', '\\n', 'Test accuracy:\n0.9800\\n', '\\n', 'Learning rate = 0.01', '\\n', 'Train worst-group accuracy:\n0.9942', '\\n', 'Validation worst-group accuracy: 0.9885', '\\n', 'Train loss:\n0.0099', '\\n', 'Validation loss:                 0.0387', '\\n', 'Test accuracy:\n0.9900\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "", "['Configuration: cluster_count = 1, learning_rate = 0.0001', '\\n', 'Dataset:\nTraining dataset', '\\n', 'train accuracy: 0.3478', '\\n', 'train loss: 0.6896',\n'\\n', 'Dataset: Validation dataset', '\\n', 'validation accuracy: 0.3588', '\\n',\n'validation loss: 0.6900', '\\n', 'Dataset: Test dataset', '\\n', 'test accuracy:\n0.4840', '\\n', '\\n', 'Configuration: cluster_count = 1, learning_rate = 0.001',\n'\\n', 'Dataset: Training dataset', '\\n', 'train accuracy: 0.9826', '\\n', 'train\nloss: 0.4826', '\\n', 'Dataset: Validation dataset', '\\n', 'validation accuracy:\n0.9622', '\\n', 'validation loss: 0.4878', '\\n', 'Dataset: Test dataset', '\\n',\n'test accuracy: 0.9820', '\\n', '\\n', 'Configuration: cluster_count = 1,\nlearning_rate = 0.01', '\\n', 'Dataset: Training dataset', '\\n', 'train accuracy:\n0.9942', '\\n', 'train loss: 0.0063', '\\n', 'Dataset: Validation dataset', '\\n',\n'validation accuracy: 0.9885', '\\n', 'validation loss: 0.0351', '\\n', 'Dataset:\nTest dataset', '\\n', 'test accuracy: 0.9920', '\\n', '\\n', 'Configuration:\ncluster_count = 2, learning_rate = 0.0001', '\\n', 'Dataset: Training dataset',\n'\\n', 'train accuracy: 0.0580', '\\n', 'train loss: 0.6950', '\\n', 'Dataset:\nValidation dataset', '\\n', 'validation accuracy: 0.0672', '\\n', 'validation\nloss: 0.6912', '\\n', 'Dataset: Test dataset', '\\n', 'test accuracy: 0.4900',\n'\\n', '\\n', 'Configuration: cluster_count = 2, learning_rate = 0.001', '\\n',\n'Dataset: Training dataset', '\\n', 'train accuracy: 0.9632', '\\n', 'train loss:\n0.4956', '\\n', 'Dataset: Validation dataset', '\\n', 'validation accuracy:\n0.9496', '\\n', 'validation loss: 0.4911', '\\n', 'Dataset: Test dataset', '\\n',\n'test accuracy: 0.9680', '\\n', '\\n', 'Configuration: cluster_count = 2,\nlearning_rate = 0.01', '\\n', 'Dataset: Training dataset', '\\n', 'train accuracy:\n0.9938', '\\n', 'train loss: 0.0111', '\\n', 'Dataset: Validation dataset', '\\n',\n'validation accuracy: 0.9847', '\\n', 'validation loss: 0.0321', '\\n', 'Dataset:\nTest dataset', '\\n', 'test accuracy: 0.9960', '\\n', '\\n', 'Configuration:\ncluster_count = 4, learning_rate = 0.0001', '\\n', 'Dataset: Training dataset',\n'\\n', 'train accuracy: 0.5549', '\\n', 'train loss: 0.6823', '\\n', 'Dataset:\nValidation dataset', '\\n', 'validation accuracy: 0.4874', '\\n', 'validation\nloss: 0.6833', '\\n', 'Dataset: Test dataset', '\\n', 'test accuracy: 0.5600',\n'\\n', '\\n', 'Configuration: cluster_count = 4, learning_rate = 0.001', '\\n',\n'Dataset: Training dataset', '\\n', 'train accuracy: 0.9807', '\\n', 'train loss:\n0.4753', '\\n', 'Dataset: Validation dataset', '\\n', 'validation accuracy:\n0.9809', '\\n', 'validation loss: 0.4724', '\\n', 'Dataset: Test dataset', '\\n',\n'test accuracy: 0.9900', '\\n', '\\n', 'Configuration: cluster_count = 4,\nlearning_rate = 0.01', '\\n', 'Dataset: Training dataset', '\\n', 'train accuracy:\n0.9961', '\\n', 'train loss: 0.0044', '\\n', 'Dataset: Validation dataset', '\\n',\n'validation accuracy: 0.9847', '\\n', 'validation loss: 0.0299', '\\n', 'Dataset:\nTest dataset', '\\n', 'test accuracy: 0.9960', '\\n', '\\n', 'Configuration:\ncluster_count = 8, learning_rate = 0.0001', '\\n', 'Dataset: Training dataset',\n'\\n', 'train accuracy: 0.5280', '\\n', 'train loss: 0.6456', '\\n', 'Dataset:\nValidation dataset', '\\n', 'validation accuracy: 0.5504', '\\n', 'validation\nloss: 0.6414', '\\n', 'Dataset: Test dataset', '\\n', 'test accuracy: 0.7620',\n'\\n', '\\n', 'Configuration: cluster_count = 8, learning_rate = 0.001', '\\n',\n'Dataset: Training dataset', '\\n', 'train accuracy: 0.9834', '\\n', 'train loss:\n0.5153', '\\n', 'Dataset: Validation dataset', '\\n', 'validation accuracy:\n0.9771', '\\n', 'validation loss: 0.5159', '\\n', 'Dataset: Test dataset', '\\n',\n'test accuracy: 0.9880', '\\n', '\\n', 'Configuration: cluster_count = 8,\nlearning_rate = 0.01', '\\n', 'Dataset: Training dataset', '\\n', 'train accuracy:\n0.9961', '\\n', 'train loss: 0.0044', '\\n', 'Dataset: Validation dataset', '\\n',\n'validation accuracy: 0.9885', '\\n', 'validation loss: 0.0339', '\\n', 'Dataset:\nTest dataset', '\\n', 'test accuracy: 0.9960', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['Dataset: synthetic (weight_decay = 0.0)', '\\n', '  Train worst-group accuracy:\n0.9884', '\\n', '  Train loss: 0.5028', '\\n', '  Validation worst-group accuracy:\n0.9885', '\\n', '  Validation loss: 0.5044', '\\n', '  Test accuracy: 0.9880',\n'\\n', 'Dataset: synthetic (weight_decay = 0.0001)', '\\n', '  Train worst-group\naccuracy: 0.9826', '\\n', '  Train loss: 0.4815', '\\n', '  Validation worst-group\naccuracy: 0.9622', '\\n', '  Validation loss: 0.4867', '\\n', '  Test accuracy:\n0.9820', '\\n', 'Dataset: synthetic (weight_decay = 0.001)', '\\n', '  Train\nworst-group accuracy: 0.9923', '\\n', '  Train loss: 0.4678', '\\n', '  Validation\nworst-group accuracy: 0.9809', '\\n', '  Validation loss: 0.4666', '\\n', '  Test\naccuracy: 0.9900', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "", "['Dataset: raw_gradient_cluster_reweighting (synthetic)', '\\n', '  train\naccuracy: 0.9942', '\\n', '  validation accuracy: 0.9924', '\\n', '  train loss:\n0.0125', '\\n', '  validation loss: 0.0430', '\\n', '  test accuracy: 0.9960\\n',\n'\\n', 'Dataset: normalized_gradient_cluster_reweighting (synthetic)', '\\n', '\ntrain accuracy: 0.9961', '\\n', '  validation accuracy: 0.9847', '\\n', '  train\nloss: 0.0080', '\\n', '  validation loss: 0.0294', '\\n', '  test accuracy:\n0.9900\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['synthetic_with_norm', '\\n', 'train worst-group accuracy: 0.9942', '\\n',\n'validation worst-group accuracy: 0.9924\\n', '\\n', 'synthetic_no_norm', '\\n',\n'train worst-group accuracy: 0.9772', '\\n', 'validation worst-group accuracy:\n0.9656\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: corr_50_d5', '\\n', 'Train worst-group accuracy:      0.9851', '\\n',\n'Validation worst-group accuracy: 0.9762', '\\n', 'Training loss:\n0.4796', '\\n', 'Validation loss:                 0.4841', '\\n', 'Test accuracy:\n0.9840\\n', '\\n', 'Dataset: corr_50_d10', '\\n', 'Train worst-group accuracy:\n1.0000', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0000', '\\n', 'Validation loss:                 0.0002', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_50_d15', '\\n', 'Train worst-group accuracy:\n1.0000', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0000', '\\n', 'Validation loss:                 0.0000', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_75_d5', '\\n', 'Train worst-group accuracy:\n0.9840', '\\n', 'Validation worst-group accuracy: 0.9798', '\\n', 'Training loss:\n0.0507', '\\n', 'Validation loss:                 0.0946', '\\n', 'Test accuracy:\n0.9820\\n', '\\n', 'Dataset: corr_75_d10', '\\n', 'Train worst-group accuracy:\n0.9980', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0067', '\\n', 'Validation loss:                 0.0000', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_75_d15', '\\n', 'Train worst-group accuracy:\n0.9961', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.3340', '\\n', 'Validation loss:                 0.3285', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_95_d5', '\\n', 'Train worst-group accuracy:\n0.9842', '\\n', 'Validation worst-group accuracy: 0.9881', '\\n', 'Training loss:\n0.0272', '\\n', 'Validation loss:                 0.0294', '\\n', 'Test accuracy:\n0.9960\\n', '\\n', 'Dataset: corr_95_d10', '\\n', 'Train worst-group accuracy:\n0.9979', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0010', '\\n', 'Validation loss:                 0.0001', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_95_d15', '\\n', 'Train worst-group accuracy:\n1.0000', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0000', '\\n', 'Validation loss:                 0.0000', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: corr_50_d5', '\\n', 'Train worst-group accuracy:      0.9851', '\\n',\n'Validation worst-group accuracy: 0.9762', '\\n', 'Training loss:\n0.4796', '\\n', 'Validation loss:                 0.4841', '\\n', 'Test accuracy:\n0.9840\\n', '\\n', 'Dataset: corr_50_d10', '\\n', 'Train worst-group accuracy:\n1.0000', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0000', '\\n', 'Validation loss:                 0.0002', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_50_d15', '\\n', 'Train worst-group accuracy:\n1.0000', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0000', '\\n', 'Validation loss:                 0.0000', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_75_d5', '\\n', 'Train worst-group accuracy:\n0.9840', '\\n', 'Validation worst-group accuracy: 0.9798', '\\n', 'Training loss:\n0.0507', '\\n', 'Validation loss:                 0.0946', '\\n', 'Test accuracy:\n0.9820\\n', '\\n', 'Dataset: corr_75_d10', '\\n', 'Train worst-group accuracy:\n0.9980', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0067', '\\n', 'Validation loss:                 0.0000', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_75_d15', '\\n', 'Train worst-group accuracy:\n0.9961', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.3340', '\\n', 'Validation loss:                 0.3285', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_95_d5', '\\n', 'Train worst-group accuracy:\n0.9842', '\\n', 'Validation worst-group accuracy: 0.9881', '\\n', 'Training loss:\n0.0272', '\\n', 'Validation loss:                 0.0294', '\\n', 'Test accuracy:\n0.9960\\n', '\\n', 'Dataset: corr_95_d10', '\\n', 'Train worst-group accuracy:\n0.9979', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0010', '\\n', 'Validation loss:                 0.0001', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_95_d15', '\\n', 'Train worst-group accuracy:\n1.0000', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0000', '\\n', 'Validation loss:                 0.0000', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: corr_50_d5', '\\n', 'Train worst-group accuracy:      0.9851', '\\n',\n'Validation worst-group accuracy: 0.9762', '\\n', 'Training loss:\n0.4796', '\\n', 'Validation loss:                 0.4841', '\\n', 'Test accuracy:\n0.9840\\n', '\\n', 'Dataset: corr_50_d10', '\\n', 'Train worst-group accuracy:\n1.0000', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0000', '\\n', 'Validation loss:                 0.0002', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_50_d15', '\\n', 'Train worst-group accuracy:\n1.0000', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0000', '\\n', 'Validation loss:                 0.0000', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_75_d5', '\\n', 'Train worst-group accuracy:\n0.9840', '\\n', 'Validation worst-group accuracy: 0.9798', '\\n', 'Training loss:\n0.0507', '\\n', 'Validation loss:                 0.0946', '\\n', 'Test accuracy:\n0.9820\\n', '\\n', 'Dataset: corr_75_d10', '\\n', 'Train worst-group accuracy:\n0.9980', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0067', '\\n', 'Validation loss:                 0.0000', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_75_d15', '\\n', 'Train worst-group accuracy:\n0.9961', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.3340', '\\n', 'Validation loss:                 0.3285', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_95_d5', '\\n', 'Train worst-group accuracy:\n0.9842', '\\n', 'Validation worst-group accuracy: 0.9881', '\\n', 'Training loss:\n0.0272', '\\n', 'Validation loss:                 0.0294', '\\n', 'Test accuracy:\n0.9960\\n', '\\n', 'Dataset: corr_95_d10', '\\n', 'Train worst-group accuracy:\n0.9979', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0010', '\\n', 'Validation loss:                 0.0001', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Dataset: corr_95_d15', '\\n', 'Train worst-group accuracy:\n1.0000', '\\n', 'Validation worst-group accuracy: 1.0000', '\\n', 'Training loss:\n0.0000', '\\n', 'Validation loss:                 0.0000', '\\n', 'Test accuracy:\n1.0000\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"]}
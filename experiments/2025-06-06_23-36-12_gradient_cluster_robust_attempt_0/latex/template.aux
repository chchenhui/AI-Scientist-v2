\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{arjovsky2019invariantrm}
\citation{sagawa2020distributionallyrn}
\citation{liu2021justtt}
\citation{koh2017understandingbp}
\citation{sagawa2020distributionallyrn}
\citation{liu2021justtt}
\citation{arjovsky2019invariantrm}
\citation{ren2018learningtr}
\citation{koh2017understandingbp}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{1}{section.2}\protected@file@percent }
\citation{sagawa2020distributionallyrn}
\citation{arjovsky2019invariantrm}
\citation{wah2011thecb,liu2014deeplf}
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Theoretical Insight.}{2}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{2}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Learning Rate Sweep.}{2}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cluster‐Count Ablation.}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reweighting Strategies.}{2}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Test Accuracy.}{2}{section*.5}\protected@file@percent }
\bibdata{references}
\bibcite{arjovsky2019invariantrm}{{1}{2019}{{Arjovsky et~al.}}{{Arjovsky, Bottou, Gulrajani, and Lopez-Paz}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a–b) Training/validation worst‐group accuracy and (c–d) loss for three learning rates. Low rate learns slowly, moderate rate balances speed and stability, and high rate converges immediately.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:lr_sweep}{{1}{3}{(a–b) Training/validation worst‐group accuracy and (c–d) loss for three learning rates. Low rate learns slowly, moderate rate balances speed and stability, and high rate converges immediately}{figure.1}{}}
\newlabel{fig:lr_sweep@cref}{{[figure][1][]1}{[1][2][]3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (a) Training and (b) validation worst‐group accuracy over epochs for $k\in \{2,4,8\}$ at LR=$10^{-2}$. All converge to high accuracy; $k=4,8$ train faster and $k=8$ shows a modest validation boost after epoch\nobreakspace  {}2.}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:cluster_k}{{2}{3}{(a) Training and (b) validation worst‐group accuracy over epochs for $k\in \{2,4,8\}$ at LR=$10^{-2}$. All converge to high accuracy; $k=4,8$ train faster and $k=8$ shows a modest validation boost after epoch~2}{figure.2}{}}
\newlabel{fig:cluster_k@cref}{{[figure][2][]2}{[1][2][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{3}{section.6}\protected@file@percent }
\bibcite{koh2017understandingbp}{{2}{2017}{{Koh \& Liang}}{{Koh and Liang}}}
\bibcite{liu2021justtt}{{3}{2021}{{Liu et~al.}}{{Liu, Haghgoo, Chen, Raghunathan, Koh, Sagawa, Liang, and Finn}}}
\bibcite{liu2014deeplf}{{4}{2014}{{Liu et~al.}}{{Liu, Luo, Wang, and Tang}}}
\bibcite{ren2018learningtr}{{5}{2018}{{Ren et~al.}}{{Ren, Zeng, Yang, and Urtasun}}}
\bibcite{sagawa2020distributionallyrn}{{6}{2020}{{Sagawa et~al.}}{{Sagawa, Koh, Hashimoto, and Liang}}}
\bibcite{wah2011thecb}{{7}{2011}{{Wah et~al.}}{{Wah, Branson, Welinder, Perona, and Belongie}}}
\bibstyle{iclr2025}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison of two reweighting strategies ($k=2$, LR=$10^{-2}$). Both inverse‐frequency and input‐feature schemes produce similar worst‐group training and validation accuracy.}}{4}{figure.3}\protected@file@percent }
\newlabel{fig:reweight_cmp}{{3}{4}{Comparison of two reweighting strategies ($k=2$, LR=$10^{-2}$). Both inverse‐frequency and input‐feature schemes produce similar worst‐group training and validation accuracy}{figure.3}{}}
\newlabel{fig:reweight_cmp@cref}{{[figure][3][]3}{[1][2][]4}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Theoretical Justification}{4}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Additional Ablations and Hyperparameters}{4}{appendix.B}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear Classifier Ablation.}{4}{figure.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Feature Normalization Ablation.}{4}{figure.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Weight Decay Ablation.}{4}{figure.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Key hyperparameters for synthetic experiments}}{5}{table.1}\protected@file@percent }
\newlabel{tab:hyper}{{1}{5}{Key hyperparameters for synthetic experiments}{table.1}{}}
\newlabel{tab:hyper@cref}{{[table][1][2147483647]1}{[1][4][]5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Linear‐probe vs.\ UGC on worst‐group accuracy and loss. UGC significantly outperforms a linear classifier in identifying spurious structures.}}{5}{figure.4}\protected@file@percent }
\newlabel{fig:app_linear}{{4}{5}{Linear‐probe vs.\ UGC on worst‐group accuracy and loss. UGC significantly outperforms a linear classifier in identifying spurious structures}{figure.4}{}}
\newlabel{fig:app_linear@cref}{{[figure][4][2147483647]4}{[1][4][]5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Validation worst‐group accuracy with and without input feature normalization across LRs. Normalization greatly improves performance at low to medium LRs.}}{6}{figure.5}\protected@file@percent }
\newlabel{fig:app_norm}{{5}{6}{Validation worst‐group accuracy with and without input feature normalization across LRs. Normalization greatly improves performance at low to medium LRs}{figure.5}{}}
\newlabel{fig:app_norm@cref}{{[figure][5][2147483647]5}{[1][4][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Effect of weight decay on validation worst‐group accuracy (LR=$10^{-3}$). Moderate decay (e.g.\ $1e^{-4}$) yields slight robustness gains; too large decays hurt performance.}}{6}{figure.6}\protected@file@percent }
\newlabel{fig:app_wd}{{6}{6}{Effect of weight decay on validation worst‐group accuracy (LR=$10^{-3}$). Moderate decay (e.g.\ $1e^{-4}$) yields slight robustness gains; too large decays hurt performance}{figure.6}{}}
\newlabel{fig:app_wd@cref}{{[figure][6][2147483647]6}{[1][4][]6}}
\gdef \@abspage@last{6}

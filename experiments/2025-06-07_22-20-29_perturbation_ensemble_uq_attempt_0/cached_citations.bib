
% Zhang et al. (ICML 2019) introduce PEGASUS, a Transformer encoder-decoder model pre-trained via gap-sentence generation for abstractive summarization. We use this model to generate semantically equivalent prompt perturbations in our ensemble and should cite it when describing our paraphrase-generation setup.
@article{zhang2019pegasuspw,
 author = {Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},
 booktitle = {International Conference on Machine Learning},
 journal = {ArXiv},
 title = {PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization},
 volume = {abs/1912.08777},
 year = {2019}
}

% Lin et al. (ECCV 2014) introduce the Microsoft COCO dataset (‘Common Objects in Context’), which we use as the benchmark dataset for our multimodal image-captioning evaluation.
@article{bunnell2023abstractpa,
 author = {Arianna Bunnell and Dustin Valdez and T. Wolfgruber and Aleen Altamirano and Brenda Y. Hernandez and Peter Sadowski and J. Shepherd},
 booktitle = {Cancer Research},
 journal = {Cancer Research},
 title = {Abstract P3-04-05: Artificial Intelligence Detects, Classifies, and Describes Lesions in Clinical Breast Ultrasound Images},
 year = {2023}
}

% Jason Wei et al. (2022) introduce chain‐of‐thought prompting for large language models, demonstrating that providing a few exemplars of intermediate reasoning steps substantially improves complex reasoning tasks. We cite this work in our Baselines section when discussing chain‐of‐thought prompting as a comparative method.
@article{wei2022chainot,
 author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed H. Chi and F. Xia and Quoc Le and Denny Zhou},
 booktitle = {Neural Information Processing Systems},
 journal = {ArXiv},
 title = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},
 volume = {abs/2201.11903},
 year = {2022}
}

% Lin et al. (ECCV 2014) introduce the Microsoft COCO dataset used in our multimodal image-captioning evaluation; cite in the Methods section when describing the dataset.
@article{lin2014microsoftcc,
 author = {Tsung-Yi Lin and M. Maire and Serge J. Belongie and James Hays and P. Perona and Deva Ramanan and Piotr Dollár and C. L. Zitnick},
 booktitle = {European Conference on Computer Vision},
 pages = {740-755},
 title = {Microsoft COCO: Common Objects in Context},
 year = {2014}
}

% Rico Sennrich et al. (ACL 2015) introduces back-translation by pairing monolingual target data with synthetic source sentences generated via a reverse translation model. We use this method to generate semantically equivalent prompt perturbations in our ensemble and should cite it in the Methods section when describing our back-translation setup.
@article{sennrich2015improvingnm,
 author = {Rico Sennrich and B. Haddow and Alexandra Birch},
 booktitle = {Annual Meeting of the Association for Computational Linguistics},
 journal = {ArXiv},
 title = {Improving Neural Machine Translation Models with Monolingual Data},
 volume = {abs/1511.06709},
 year = {2015}
}

% V. I. Levenshtein (1966) introduced the edit-distance algorithm for measuring the minimum number of insertions, deletions, and substitutions needed to transform one string into another. Cite this work in the Methods section when describing the token-level Levenshtein distance metric used to quantify divergence between model outputs.
@article{levenshtein1965binarycc,
 author = {V. Levenshtein},
 journal = {Soviet physics. Doklady},
 pages = {707-710},
 title = {Binary codes capable of correcting deletions, insertions, and reversals},
 volume = {10},
 year = {1965}
}

% Mark Chen et al. (2021) introduce Codex and release the HumanEval benchmark for measuring functional correctness in program synthesis. We use HumanEval as our code-completion dataset and should cite it in the Experiments section when describing our code completion evaluation setup.
@article{chen2021evaluatingll,
 author = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Pondé and Jared Kaplan and Harrison Edwards and Yura Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mo Bavarian and Clemens Winter and Phil Tillet and F. Such and D. Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Igor Babuschkin and S. Balaji and Shantanu Jain and A. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and M. Knight and Miles Brundage and Mira Murati and Katie Mayer and P. Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and I. Sutskever and Wojciech Zaremba},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Evaluating Large Language Models Trained on Code},
 volume = {abs/2107.03374},
 year = {2021}
}

% T. Kwiatkowski et al. (2019) introduce the Natural Questions corpus, a large-scale open-domain QA dataset with real anonymized Google queries and long/short answer annotations. We use NQ-Open in our open-ended QA evaluation and should cite it in the Experiments section when describing the dataset.
@article{kwiatkowski2019naturalqa,
 author = {T. Kwiatkowski and J. Palomaki and Olivia Redfield and Michael Collins and Ankur P. Parikh and Chris Alberti and D. Epstein and Illia Polosukhin and Jacob Devlin and Kenton Lee and Kristina Toutanova and Llion Jones and Matthew Kelcey and Ming-Wei Chang and Andrew M. Dai and Jakob Uszkoreit and Quoc V. Le and Slav Petrov},
 booktitle = {Transactions of the Association for Computational Linguistics},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {453-466},
 title = {Natural Questions: A Benchmark for Question Answering Research},
 volume = {7},
 year = {2019}
}

% Fanghua Ye et al. (NeurIPS 2024) introduce a comprehensive benchmark for uncertainty quantification in LLMs across multiple tasks. We cite this work in the Related Work section when discussing existing benchmarks and UQ methods for language models.
@article{ye2024benchmarkinglv,
 author = {Fanghua Ye and Mingming Yang and Jianhui Pang and Longyue Wang and Derek F. Wong and Emine Yilmaz and Shuming Shi and Zhaopeng Tu},
 booktitle = {Neural Information Processing Systems},
 journal = {ArXiv},
 title = {Benchmarking LLMs via Uncertainty Quantification},
 volume = {abs/2401.12794},
 year = {2024}
}

% Lakshminarayanan et al. (NeurIPS 2016) introduce Deep Ensembles, a simple and scalable method for predictive uncertainty estimation using multiple neural network models. We cite this work in the Related Work section when discussing ensemble-based uncertainty quantification and drawing parallels to our prompt-perturbation ensemble approach.
@article{lakshminarayanan2016simpleas,
 author = {Balaji Lakshminarayanan and A. Pritzel and C. Blundell},
 booktitle = {Neural Information Processing Systems},
 pages = {6402-6413},
 title = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
 year = {2016}
}

% Reimers & Gurevych (EMNLP 2019) introduce Sentence-BERT, a siamese-network modification of BERT for efficient sentence embeddings. We cite this in the Methods section when describing the use of SBERT to compute sentence embeddings and cosine-distance divergence metrics.
@article{reimers2019sentencebertse,
 author = {Nils Reimers and Iryna Gurevych},
 booktitle = {Conference on Empirical Methods in Natural Language Processing},
 pages = {3980-3990},
 title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
 year = {2019}
}

% Wang et al. (2024) introduce UBench, a large-scale multiple-choice QA benchmark (11,978 questions) for evaluating uncertainty estimation methods in LLMs using confidence intervals. Cite this work in the Experiments and Related Work sections when describing our multiple-choice QA setup, baseline comparisons (semantic entropy, confidence scores, CoT prompts), and the construction of our divergence-based uncertainty evaluation.
@inproceedings{wang2024ubenchbu,
 author = {Xunzhi Wang and Zhuowei Zhang and Gaonan Chen and Qiongyu Li and Bitong Luo and Zhixin Han and Haotian Wang and Zhiyu Li and Hang Gao and Mengting Hu},
 title = {UBench: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions},
 year = {2024}
}

% Touvron et al. (2023) introduce Llama 2, a suite of open-source foundation models and fine-tuned chat variants ranging from 7B to 70B parameters. Cite this work in the Experiments section when specifying the target LLMs (Llama 2) used for measuring perturbation-induced uncertainty.
@article{touvron2023llama2o,
 author = {Hugo Touvron and Louis Martin and Kevin R. Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Niko-lay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and D. Bikel and Lukas Blecher and Cris-tian Cantón Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and A. Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel M. Kloumann and A. Korenev and Punit Singh Koura and M. Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and R. Subramanian and Xia Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zhengxu Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and M. Kambadur and Sharan Narang and Aur'elien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
 volume = {abs/2307.09288},
 year = {2023}
}

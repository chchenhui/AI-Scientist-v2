\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{sennrich2015improvingnm,zhang2019pegasuspw}
\citation{reimers2019sentencebertse}
\citation{ye2024benchmarkinglv}
\citation{wei2022chainot}
\citation{lakshminarayanan2016simpleas}
\citation{levenshtein1965binarycc}
\citation{reimers2019sentencebertse}
\citation{lakshminarayanan2016simpleas}
\citation{ye2024benchmarkinglv}
\citation{wang2024ubenchbu,ye2024benchmarkinglv}
\citation{wei2022chainot}
\citation{sennrich2015improvingnm,zhang2019pegasuspw}
\citation{reimers2019sentencebertse}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{1}{section.2}\protected@file@percent }
\citation{sennrich2015improvingnm}
\citation{sennrich2015improvingnm}
\citation{zhang2019pegasuspw}
\citation{levenshtein1965binarycc}
\citation{reimers2019sentencebertse}
\citation{goodfellow2016deep}
\citation{wang2024ubenchbu,chen2021evaluatingll,lin2014microsoftcc}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Final misclassification detection ROC‐AUC by method.}}{2}{table.1}\protected@file@percent }
\newlabel{tab:final_auc}{{1}{2}{Final misclassification detection ROC‐AUC by method}{table.1}{}}
\newlabel{tab:final_auc@cref}{{[table][1][]1}{[1][2][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Setup}{2}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Experiments}{2}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ablations.}{2}{section*.1}\protected@file@percent }
\bibdata{references}
\bibcite{chen2021evaluatingll}{{1}{2021}{{Chen et~al.}}{{Chen, Tworek, Jun, Yuan, Pond{\'e}, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Babuschkin, Balaji, Jain, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Top: train (blue) and val (orange) loss over epochs 1–5 for SST-2, Yelp, IMDb. Validation loss increases after epoch 2, indicating overfitting. Bottom: detection ROC-AUC (Vote vs.\ KL) over epochs. KL‐based PIU is more stable and higher.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:main_tasks}{{1}{3}{Top: train (blue) and val (orange) loss over epochs 1–5 for SST-2, Yelp, IMDb. Validation loss increases after epoch 2, indicating overfitting. Bottom: detection ROC-AUC (Vote vs.\ KL) over epochs. KL‐based PIU is more stable and higher}{figure.1}{}}
\newlabel{fig:main_tasks@cref}{{[figure][1][]1}{[1][2][]3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Group bars compare Vote AUC vs.\ KL AUC for SST-2, Yelp, and IMDb. KL‐divergence PIU yields higher final detection ROC‐AUC on all datasets.}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:final_auc}{{2}{3}{Group bars compare Vote AUC vs.\ KL AUC for SST-2, Yelp, and IMDb. KL‐divergence PIU yields higher final detection ROC‐AUC on all datasets}{figure.2}{}}
\newlabel{fig:final_auc@cref}{{[figure][2][]2}{[1][2][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{3}{section.7}\protected@file@percent }
\bibcite{goodfellow2016deep}{{2}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, Courville, and Bengio}}}
\bibcite{lakshminarayanan2016simpleas}{{3}{2016}{{Lakshminarayanan et~al.}}{{Lakshminarayanan, Pritzel, and Blundell}}}
\bibcite{levenshtein1965binarycc}{{4}{1965}{{Levenshtein}}{{}}}
\bibcite{lin2014microsoftcc}{{5}{2014}{{Lin et~al.}}{{Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll{\'a}r, and Zitnick}}}
\bibcite{reimers2019sentencebertse}{{6}{2019}{{Reimers \& Gurevych}}{{Reimers and Gurevych}}}
\bibcite{sennrich2015improvingnm}{{7}{2015}{{Sennrich et~al.}}{{Sennrich, Haddow, and Birch}}}
\bibcite{wang2024ubenchbu}{{8}{2024}{{Wang et~al.}}{{Wang, Zhang, Chen, Li, Luo, Han, Wang, Li, Gao, and Hu}}}
\bibcite{wei2022chainot}{{9}{2022}{{Wei et~al.}}{{Wei, Wang, Schuurmans, Bosma, Chi, Xia, Le, and Zhou}}}
\bibcite{ye2024benchmarkinglv}{{10}{2024}{{Ye et~al.}}{{Ye, Yang, Pang, Wang, Wong, Yilmaz, Shi, and Tu}}}
\bibcite{zhang2019pegasuspw}{{11}{2019}{{Zhang et~al.}}{{Zhang, Zhao, Saleh, and Liu}}}
\bibstyle{iclr2025}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Training and paraphrase generation hyperparameters.}}{5}{table.2}\protected@file@percent }
\newlabel{tab:hyperparams}{{2}{5}{Training and paraphrase generation hyperparameters}{table.2}{}}
\newlabel{tab:hyperparams@cref}{{[table][2][2147483647]2}{[1][4][]5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Head-only fine-tuning: (left) detection ROC-AUC across epochs, (right) train/val loss. Head-only models have lower detection performance and overfit faster.}}{5}{figure.3}\protected@file@percent }
\newlabel{fig:head_only_detection}{{3}{5}{Head-only fine-tuning: (left) detection ROC-AUC across epochs, (right) train/val loss. Head-only models have lower detection performance and overfit faster}{figure.3}{}}
\newlabel{fig:head_only_detection@cref}{{[figure][3][2147483647]3}{[1][4][]5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces No-pretrain BERT: (left) detection ROC-AUC, (right) train/val loss. Models without pretraining underperform and exhibit higher uncertainty.}}{5}{figure.4}\protected@file@percent }
\newlabel{fig:no_pretrain_detection}{{4}{5}{No-pretrain BERT: (left) detection ROC-AUC, (right) train/val loss. Models without pretraining underperform and exhibit higher uncertainty}{figure.4}{}}
\newlabel{fig:no_pretrain_detection@cref}{{[figure][4][2147483647]4}{[1][4][]5}}
\gdef \@abspage@last{5}

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 3], [0, 4], [0, 5], [0, 1], [0, 2], [1, 12], [3, 6], [4, 9], [6, 7], [6, 8], [9, 14], [9, 11], [9, 13], [9, 10], [9, 16], [9, 15]], "layout": [[0.3764705882352941, 0.0], [0.0, 0.33333333333333337], [0.11764705882352941, 0.33333333333333337], [0.23529411764705882, 0.33333333333333337], [0.7058823529411765, 0.33333333333333337], [0.8235294117647058, 0.33333333333333337], [0.23529411764705882, 0.6666666666666667], [0.17647058823529413, 1.0], [0.29411764705882354, 1.0], [0.7058823529411765, 0.6666666666666667], [0.4117647058823529, 1.0], [0.5294117647058824, 1.0], [0.0, 0.6666666666666667], [0.6470588235294118, 1.0], [0.7647058823529411, 1.0], [0.8823529411764706, 1.0], [1.0, 1.0]], "plan": ["We detach the logits before converting to numpy in both training and validation\nloops by calling `.detach().cpu().numpy()`. We load two HuggingFace benchmarks\n(SST-2 and Yelp Polarity), extract sentence or text length as a single numeric\nfeature, and split into 1000/200 train/val sets. Each dataset\u2019s inputs are\nnormalized by the training mean and standard deviation. We sweep over two\nlearning rates `[0.001, 0.01]` and two batch sizes `[32, 64]`, train for 20\nepochs, and at each epoch compute and print `Epoch {epoch}: validation_loss =\n{val_loss:.4f}`, tracking ROC AUC and losses. All metrics, predictions, and\nground\u2010truth labels are stored in an `experiment_data` dict and saved with\n`np.save()` at the end.", "I propose to implement PIU by paraphrasing each input three times with a\nlightweight T5-based model and then running zero-shot classification via a BART-\nMNLI pipeline to measure output divergence as uncertainty. We sample 200\nexamples each from SST-2, IMDB, and AG News (three HuggingFace datasets) to keep\ninference tractable. For every example, the paraphraser yields K=3 variants, and\nwe classify the original plus its paraphrases individually, recording the\nmajority agreement rate and defining uncertainty as one minus that rate. We flag\nan instance as \u201cwrong\u201d when the original majority label disagrees with the\nground\u2010truth and then compute ROC-AUC and PR-AUC of the uncertainty scores\nagainst this binary error signal. To quantify cost\u2010efficiency, we calculate the\nDetection Efficiency Score (DES) as ROC-AUC divided by the average number of\nforward calls (K+1). All inference runs on GPU where available, we print\nper\u2010dataset results, and we save the full experiment data as a numpy file in\n./working for further analysis.", "We extend PIU to three diverse HuggingFace classification tasks\u2014SST-2, AG News,\nand IMDB\u2014using pretrained Transformer classifiers and a T5\u2010based paraphrase\nmodel. For each dataset we select a 50\u2010sample validation subset, compute a\none\u2010pass \u201cvalidation loss\u201d via cross\u2010entropy, then for ensemble sizes K\u2208{1,3,5}\ngenerate K\u22121 paraphrases and measure output disagreement as uncertainty. We then\ncompute ROC\u2010AUC for misclassification detection and define the Detection\nEfficiency Score (DES) as ROC\u2010AUC divided by the ensemble size. All models and\ntokenization happen on GPU where available, and we print validation loss,\nROC\u2010AUC, and DES at each epoch (i.e., each ensemble size) before saving full\nmetrics, losses, uncertainties, predictions, and labels to\n`experiment_data.npy`. This pipeline runs end to end in under an hour and\nshowcases how simple paraphrase perturbations can yield reliable, model\u2010agnostic\nuncertainty estimates across multiple NLP tasks.", "I propose enriching our one-dimensional baseline with a three-feature\nrepresentation\u2014sentence length, unique-token ratio, and punctuation density\u2014to\ncapture richer linguistic signals. We will benchmark on three HuggingFace text\nclassification datasets (SST-2, Yelp Polarity, and IMDB), each subsampled to 1K\ntraining and 200 validation examples, plus our synthetic Gaussian mixture\ncontrol. A lightweight two-layer MLP is trained with Adam and a cosine-annealing\nlearning-rate scheduler over 10 epochs, sweeping batch sizes and learning rates\nto ensure hyperparameter robustness. At each epoch we print validation loss,\ncompute ROC-AUC, and define a Detection Efficiency Score (DES) as ROC-AUC\ndivided by the number of forward calls (one), illustrating cost-normalized\ndiscrimination. All metrics, losses, predictions, and ground-truth labels are\nlogged in a structured `experiment_data` dictionary and saved via `np.save`,\nensuring full reproducibility. This richer feature set and scheduler-based\ntraining should improve generalization and stability compared to the pure\nlength-only model.", "We propose to implement PIU on three text classification datasets\u2014SST-2, Yelp\nPolarity, and IMDb\u2014by fine-tuning a DistilBERT classifier on each and logging\nvalidation loss at every epoch. We generate lightweight paraphrases via random\nWordNet synonym swaps to create K=3 perturbations per validation example and\nmeasure output divergence as 1 minus the majority\u2010vote agreement rate. After\neach epoch, we compute ROC-AUC for misclassification detection using this\nuncertainty score and define a Detection Efficiency Score (DES) as AUC divided\nby the total number of forward calls per example (K+1). All models and tensors\nrun on GPU if available, and we store train/val losses, detection AUC, and DES\neach epoch into a structured `experiment_data` dictionary. Finally, we save the\nentire experiment log to `working/experiment_data.npy`.", "We evaluate PIU on three HuggingFace sentiment benchmarks (SST-2, Yelp, IMDB) by\nasking a frozen generative model (Flan-T5) to classify each review under five\nsemantically equivalent prompt templates.  For each review we collect five free-\ntext predictions, map them to binary labels, and compute uncertainty as one\nminus the agreement rate.  We flag a sample as erroneous if the majority label\ndisagrees with the ground truth.  We then compute ROC-AUC for detecting these\nerrors using the uncertainty score, and define the Detection Efficiency Score\n(DES) as AUC divided by the number of forward passes (K).  We print a proxy\n\u201cvalidation loss\u201d as the average uncertainty at the end of each dataset loop,\nreport AUC and DES, and save all per-sample scores and metrics to the working\ndirectory.", "I will make the model input\u2010size adaptive by reading off the feature dimension\nof each training dataset so that synthetic data (with 1 feature) and text\nfeatures (with 3 features) both fit the network. Specifically, I compute\n`input_dim = train_ds.tensors[0].shape[1]` before creating the model and then\nuse `nn.Linear(input_dim, 16)` for the first layer. This ensures no shape\nmismatch occurs regardless of dataset. The rest of the training loop remains\nunchanged.", "We generate K=3 paraphrases per example with a PEGASUS paraphrase model and\nfine\u2010tune a DistilBERT sentiment classifier on each of three HuggingFace\ndatasets (SST2, Yelp Polarity, IMDB) for two epochs. After each epoch we print\nthe validation loss, then on the validation split we compute the Perturbation-\nInduced Uncertainty (1 minus agreement rate across paraphrases) per example and\nevaluate its ROC\u2010AUC for flagging misclassifications. We define DES as AUC\ndivided by the average forward calls (K+1). All models and tensors are moved to\nGPU when available, and we save losses, AUCs, and DES values in\n`experiment_data.npy`. This shows how simple input perturbations can detect\nlikely errors with minimal API calls, applicable even to closed\u2010box models.", "We propose to implement PIU by sampling three sentiment datasets (SST2, IMDb,\nYelp) and using back\u2010translation via Helsinki\u2010NLP models to generate K\nparaphrases per input. We query a fixed text\u2010classification model on the\noriginal and perturbed prompts, compute the agreement rate to derive an\nuncertainty score (1-agreement), and label examples as errors when the original\nprediction mismatches the true label. Sweeping K\u2208{1,3,5}, we treat each K as a\npseudo\u2010epoch, computing detection ROC-AUC and defining a synthetic \u201cvalidation\nloss\u201d as 1\u2212ROC-AUC, which we print per epoch. We also compute the Detection\nEfficiency Score (DES)=ROC-AUC/(K+1) to capture discrimination per forward call.\nAll pipelines run on GPU when available, and at the end we save the full metrics\ndictionary via numpy. This model\u2010agnostic approach requires no training or logit\naccess and uses only a handful of LLM calls per example.", "I fixed the training loop so that `optimizer.zero_grad()` happens before\n`loss.backward()`, ensuring the gradients are cleared before each backward pass\nrather than after, which was preventing any weight updates. This change allows\nthe model\u2019s parameters to actually be updated during `optimizer.step()`, leading\nto meaningful training and decreasing validation loss over epochs. I also made\nsure all tensors and models are consistently moved to the configured device\n(`cuda`/`cpu`).", "We upgrade to BERT-base for richer representations and scale up training to 5\nepochs with 5k examples per dataset (train) and 500 (val). Ensemble size K=5\nyields stronger uncertainty signals. In addition to majority\u2010vote disagreement,\nwe compute symmetric KL divergence across the softmax distributions for a\ncomplementary divergence metric. We track both detection AUCs and Detection\nEfficiency Scores (DES) at each epoch, print validation loss and detection\nmetrics, then save all losses, metrics, predictions, and ground truths via\nnp.save. The script uses three HuggingFace datasets (SST-2, Yelp Polarity,\nIMDB), moves all tensors/models to GPU when available, and completes within our\ntime budget while providing richer uncertainty insights.", "We diversify prompt perturbations by combining synonym replacement and random\nword swaps to capture richer model sensitivities, and we compute a novel\ncomposite uncertainty score blending prediction disagreement with normalized\nprobability variance. We scale up to a larger BERT-base model, extend training\nto four epochs, and use 3k training examples and 600 validation examples per\ntask to utilize more of the available runtime. The uncertainty metric is the\naverage of the disagreement rate across K=5 paraphrases and the normalized\nstandard deviation of positive\u2010class probabilities. We evaluate on three\nHuggingFace classification datasets (SST-2, Yelp Polarity, IMDb) and record\ntraining/validation losses, ROC-AUC for detection, and Detection Efficiency\nScore (DES = AUC / forward calls) at each epoch. All model and tensor operations\nare explicitly moved to GPU/CPU via PyTorch\u2019s device abstraction. We log\nper\u2010epoch metrics into a structured dict and finally save the full experiment\ndata as NumPy arrays for downstream analysis. This setup enables us to uncover\nhow perturbation diversity, model scale, and training regime affect uncertainty\ncalibration and hallucination detection.", "We replace the unavailable T5 paraphraser with the widely\u2010used Pegasus\nparaphrase model (tuner007/pegasus_paraphrase), rebuild the text2text and\nzero\u2010shot pipelines, and make sure each model is explicitly moved to the\ndetected device. We preserve the three HuggingFace datasets, compute uncertainty\nvia majority\u2010vote agreement, calculate the DES metric, and save all results to\nthe working directory. The revised script handles GPU/CPU setup, dataset\nloading, paraphrase generation, classification, metric computation, and final\ndata serialization in one self\u2010contained program.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\nfrom datasets import load_dataset\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# synthetic data function\ndef sample_data(N):\n    N0 = N // 2\n    N1 = N - N0\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N1), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N1)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\n# Prepare datasets dict\ndatasets = {}\n\n# 1) Synthetic dataset\nx_tr_s, y_tr_s = sample_data(1000)\nx_val_s, y_val_s = sample_data(200)\nmean_s, std_s = x_tr_s.mean(), x_tr_s.std() + 1e-6\nx_tr_s = (x_tr_s - mean_s) / std_s\nx_val_s = (x_val_s - mean_s) / std_s\ntrain_ds_s = TensorDataset(torch.from_numpy(x_tr_s), torch.from_numpy(y_tr_s))\nval_ds_s = TensorDataset(torch.from_numpy(x_val_s), torch.from_numpy(y_val_s))\ndatasets[\"synthetic\"] = (train_ds_s, val_ds_s)\n\n# 2) SST-2 (sentence length as feature)\nsst2_train = load_dataset(\"glue\", \"sst2\", split=\"train\").shuffle(42).select(range(1000))\nsst2_val = (\n    load_dataset(\"glue\", \"sst2\", split=\"validation\").shuffle(42).select(range(200))\n)\nx_tr_st = np.array(\n    [len(s.split()) for s in sst2_train[\"sentence\"]], dtype=np.float32\n).reshape(-1, 1)\ny_tr_st = np.array(sst2_train[\"label\"], dtype=np.float32)\nx_val_st = np.array(\n    [len(s.split()) for s in sst2_val[\"sentence\"]], dtype=np.float32\n).reshape(-1, 1)\ny_val_st = np.array(sst2_val[\"label\"], dtype=np.float32)\nmean_st, std_st = x_tr_st.mean(), x_tr_st.std() + 1e-6\nx_tr_st = (x_tr_st - mean_st) / std_st\nx_val_st = (x_val_st - mean_st) / std_st\ntrain_ds_st = TensorDataset(torch.from_numpy(x_tr_st), torch.from_numpy(y_tr_st))\nval_ds_st = TensorDataset(torch.from_numpy(x_val_st), torch.from_numpy(y_val_st))\ndatasets[\"sst2\"] = (train_ds_st, val_ds_st)\n\n# 3) Yelp Polarity (text length as feature)\nyelp_train = (\n    load_dataset(\"yelp_polarity\", split=\"train\").shuffle(42).select(range(1000))\n)\nyelp_val = load_dataset(\"yelp_polarity\", split=\"test\").shuffle(42).select(range(200))\nx_tr_yp = np.array(\n    [len(t.split()) for t in yelp_train[\"text\"]], dtype=np.float32\n).reshape(-1, 1)\ny_tr_yp = np.array(yelp_train[\"label\"], dtype=np.float32)\nx_val_yp = np.array(\n    [len(t.split()) for t in yelp_val[\"text\"]], dtype=np.float32\n).reshape(-1, 1)\ny_val_yp = np.array(yelp_val[\"label\"], dtype=np.float32)\nmean_yp, std_yp = x_tr_yp.mean(), x_tr_yp.std() + 1e-6\nx_tr_yp = (x_tr_yp - mean_yp) / std_yp\nx_val_yp = (x_val_yp - mean_yp) / std_yp\ntrain_ds_yp = TensorDataset(torch.from_numpy(x_tr_yp), torch.from_numpy(y_tr_yp))\nval_ds_yp = TensorDataset(torch.from_numpy(x_val_yp), torch.from_numpy(y_val_yp))\ndatasets[\"yelp_polarity\"] = (train_ds_yp, val_ds_yp)\n\n# Hyperparameters\nbatch_sizes = [32, 64]\nlearning_rates = [0.001, 0.01]\nepochs = 20\n\n# Initialize experiment_data\nexperiment_data = {}\nfor name in datasets:\n    experiment_data[name] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n# Training and evaluation\nfor name, (train_ds, val_ds) in datasets.items():\n    for lr in learning_rates:\n        for bs in batch_sizes:\n            train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n            val_loader = DataLoader(val_ds, batch_size=bs)\n            model = nn.Linear(1, 1).to(device)\n            optimizer = Adam(model.parameters(), lr=lr)\n            loss_fn = nn.BCEWithLogitsLoss()\n            for epoch in range(1, epochs + 1):\n                # training\n                model.train()\n                t_losses, t_preds, t_labels = [], [], []\n                for xb, yb in train_loader:\n                    xb, yb = xb.to(device), yb.to(device)\n                    logits = model(xb).squeeze(1)\n                    loss = loss_fn(logits, yb)\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n                    t_losses.append(loss.item())\n                    t_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n                    t_labels.append(yb.cpu().numpy())\n                train_loss = np.mean(t_losses)\n                train_auc = roc_auc_score(\n                    np.concatenate(t_labels), np.concatenate(t_preds)\n                )\n                # validation\n                model.eval()\n                v_losses, v_preds, v_labels = [], [], []\n                with torch.no_grad():\n                    for xb, yb in val_loader:\n                        xb, yb = xb.to(device), yb.to(device)\n                        logits = model(xb).squeeze(1)\n                        loss = loss_fn(logits, yb)\n                        v_losses.append(loss.item())\n                        v_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n                        v_labels.append(yb.cpu().numpy())\n                val_loss = np.mean(v_losses)\n                val_preds = np.concatenate(v_preds)\n                val_labels = np.concatenate(v_labels)\n                val_auc = roc_auc_score(val_labels, val_preds)\n                print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n                # Record results\n                exp = experiment_data[name]\n                exp[\"metrics\"][\"train\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"auc\": train_auc}\n                )\n                exp[\"metrics\"][\"val\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"auc\": val_auc}\n                )\n                exp[\"losses\"][\"train\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"loss\": train_loss}\n                )\n                exp[\"losses\"][\"val\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"loss\": val_loss}\n                )\n                exp[\"predictions\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"preds\": val_preds}\n                )\n                exp[\"ground_truth\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"labels\": val_labels}\n                )\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport numpy as np\nfrom transformers import pipeline\nfrom datasets import load_dataset\nfrom sklearn.metrics import roc_auc_score, average_precision_score\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice_id = torch.cuda.current_device() if torch.cuda.is_available() else -1\nprint(f\"Using device: {device}\")\n\n# paraphrase and classification pipelines\nparaphraser = pipeline(\n    \"text2text-generation\",\n    model=\"ramsrigouthamg/t5_paraphraser_small\",\n    device=device_id,\n)\nclassifier = pipeline(\n    \"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=device_id\n)\n\n# experiment setup\nK = 3\ndatasets_info = {\n    \"sst2\": {\n        \"data\": load_dataset(\"glue\", \"sst2\", split=\"validation\")\n        .shuffle(42)\n        .select(range(200)),\n        \"text_key\": \"sentence\",\n        \"label_map\": {0: \"negative\", 1: \"positive\"},\n        \"candidate_labels\": [\"negative\", \"positive\"],\n    },\n    \"imdb\": {\n        \"data\": load_dataset(\"imdb\", split=\"test\").shuffle(42).select(range(200)),\n        \"text_key\": \"text\",\n        \"label_map\": {0: \"negative\", 1: \"positive\"},\n        \"candidate_labels\": [\"negative\", \"positive\"],\n    },\n    \"ag_news\": {\n        \"data\": load_dataset(\"ag_news\", split=\"test\").shuffle(42).select(range(200)),\n        \"text_key\": \"text\",\n        \"label_map\": {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"},\n        \"candidate_labels\": [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"],\n    },\n}\n\nexperiment_data = {}\nfor name, info in datasets_info.items():\n    ds = info[\"data\"]\n    text_key = info[\"text_key\"]\n    label_map = info[\"label_map\"]\n    candidate_labels = info[\"candidate_labels\"]\n\n    y_uncert, y_wrong = [], []\n    for ex in ds:\n        text = ex[text_key]\n        true_str = label_map[ex[\"label\"]]\n        # generate K paraphrases\n        paraphrases = paraphraser(\n            text, max_length=64, num_beams=K * 2, num_return_sequences=K\n        )\n        variants = [text] + [p[\"generated_text\"] for p in paraphrases]\n        preds = []\n        for v in variants:\n            out = classifier(v, candidate_labels=candidate_labels)\n            if isinstance(out, list):\n                out = out[0]\n            preds.append(out[\"labels\"][0])\n        # compute uncertainty\n        maj = max(set(preds), key=preds.count)\n        agree_rate = preds.count(maj) / len(variants)\n        uncert = 1.0 - agree_rate\n        y_uncert.append(uncert)\n        y_wrong.append(int(preds[0] != true_str))\n\n    roc = roc_auc_score(y_wrong, y_uncert)\n    pr = average_precision_score(y_wrong, y_uncert)\n    avg_calls = K + 1\n    des = roc / avg_calls\n    print(\n        f\"{name}: ROC-AUC={roc:.4f}, PR-AUC={pr:.4f}, avg_calls={avg_calls}, DES={des:.4f}\"\n    )\n    experiment_data[name] = {\n        \"roc_auc\": roc,\n        \"pr_auc\": pr,\n        \"avg_calls\": avg_calls,\n        \"DES\": des,\n    }\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nimport torch\nfrom datasets import load_dataset\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\nfrom sklearn.metrics import roc_auc_score\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Paraphrase generator (T5-based)\nparaphrase_model_name = \"Vamsi/T5_Paraphrase_Paws\"\nparaphrase_pipe = pipeline(\n    \"text2text-generation\",\n    model=paraphrase_model_name,\n    tokenizer=paraphrase_model_name,\n    device=0 if torch.cuda.is_available() else -1,\n)\n\n# Configuration for three datasets\ndatasets_info = [\n    {\n        \"name\": \"sst2\",\n        \"hf\": (\"glue\", \"sst2\"),\n        \"split\": \"validation\",\n        \"text_column\": \"sentence\",\n        \"label_column\": \"label\",\n        \"model_name\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n    },\n    {\n        \"name\": \"ag_news\",\n        \"hf\": (\"ag_news\", None),\n        \"split\": \"test\",\n        \"text_column\": \"text\",\n        \"label_column\": \"label\",\n        \"model_name\": \"textattack/bert-base-uncased-ag-news\",\n    },\n    {\n        \"name\": \"imdb\",\n        \"hf\": (\"imdb\", None),\n        \"split\": \"test\",\n        \"text_column\": \"text\",\n        \"label_column\": \"label\",\n        \"model_name\": \"lvwerra/distilbert-imdb\",\n    },\n]\n\nN_val = 50\nK_list = [1, 3, 5]\nexperiment_data = {}\n\nfor info in datasets_info:\n    name = info[\"name\"]\n    # init storage\n    experiment_data[name] = {\n        \"metrics\": {\"epoch\": [], \"val_loss\": [], \"roc_auc\": [], \"DES\": []},\n        \"uncertainties\": {\"epoch\": [], \"values\": []},\n        \"errors\": {\"epoch\": [], \"values\": []},\n        \"predictions\": {\"epoch\": [], \"values\": []},\n        \"ground_truth\": {\"epoch\": [], \"values\": []},\n    }\n    # load and sample\n    ds = load_dataset(info[\"hf\"][0], info[\"hf\"][1], split=info[\"split\"])\n    ds = ds.shuffle(42).select(range(N_val))\n    texts = ds[info[\"text_column\"]]\n    labels = ds[info[\"label_column\"]]\n    # load classifier\n    tokenizer = AutoTokenizer.from_pretrained(info[\"model_name\"])\n    model = AutoModelForSequenceClassification.from_pretrained(info[\"model_name\"]).to(\n        device\n    )\n    model.eval()\n    # compute validation loss and original preds\n    enc = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True).to(\n        device\n    )\n    with torch.no_grad():\n        out = model(**enc)\n    logits = out.logits\n    lbls = torch.tensor(labels, dtype=torch.long).to(device)\n    loss_fn = torch.nn.CrossEntropyLoss()\n    val_loss = loss_fn(logits, lbls).item()\n    probs = torch.softmax(logits, dim=-1)\n    orig_preds = torch.argmax(probs, dim=-1).cpu().numpy()\n    labels_cpu = lbls.cpu().numpy()\n    num_labels = model.config.num_labels\n\n    # run PIU experiments for varying ensemble sizes\n    for epoch, K in enumerate(K_list, start=1):\n        print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n        uncertainties, errors, preds = [], [], []\n        for txt, true_lbl, pred0 in zip(texts, labels_cpu, orig_preds):\n            # generate K-1 paraphrases\n            if K > 1:\n                seqs = paraphrase_pipe(\n                    txt, max_length=256, num_beams=5, num_return_sequences=K - 1\n                )\n                paras = [s[\"generated_text\"] for s in seqs]\n            else:\n                paras = []\n            ens_inputs = [txt] + paras\n            ens_preds = []\n            for inp in ens_inputs:\n                enc2 = tokenizer(\n                    inp, return_tensors=\"pt\", truncation=True, padding=True\n                ).to(device)\n                with torch.no_grad():\n                    out2 = model(**enc2)\n                p = torch.argmax(torch.softmax(out2.logits, dim=-1), dim=-1).item()\n                ens_preds.append(p)\n            counts = np.bincount(ens_preds, minlength=num_labels)\n            mode = int(np.argmax(counts))\n            agreement = counts[mode] / len(ens_preds)\n            uncertainties.append(1 - agreement)\n            errors.append(1 if pred0 != true_lbl else 0)\n            preds.append(int(pred0))\n        # compute detection metrics\n        try:\n            roc_auc = roc_auc_score(errors, uncertainties)\n        except ValueError:\n            roc_auc = float(\"nan\")\n        des = roc_auc / K\n        # record\n        exp = experiment_data[name]\n        exp[\"metrics\"][\"epoch\"].append(epoch)\n        exp[\"metrics\"][\"val_loss\"].append(val_loss)\n        exp[\"metrics\"][\"roc_auc\"].append(roc_auc)\n        exp[\"metrics\"][\"DES\"].append(des)\n        exp[\"uncertainties\"][\"epoch\"].append(epoch)\n        exp[\"uncertainties\"][\"values\"].append(np.array(uncertainties))\n        exp[\"errors\"][\"epoch\"].append(epoch)\n        exp[\"errors\"][\"values\"].append(np.array(errors))\n        exp[\"predictions\"][\"epoch\"].append(epoch)\n        exp[\"predictions\"][\"values\"].append(np.array(preds))\n        exp[\"ground_truth\"][\"epoch\"].append(epoch)\n        exp[\"ground_truth\"][\"values\"].append(np.array(labels_cpu))\n        print(f\"{name} | Epoch {epoch}: ROC-AUC = {roc_auc:.4f}, DES = {des:.4f}\")\n\n# save all metrics and data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport re\nimport string\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.metrics import roc_auc_score\nfrom datasets import load_dataset\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef sample_data(N):\n    N0 = N // 2\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N - N0), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N - N0)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\ndef extract_features(texts):\n    feats = []\n    for s in texts:\n        toks = s.split()\n        L = max(len(toks), 1)\n        uniq = len(set(toks)) / L\n        punct = sum(1 for c in s if c in string.punctuation) / L\n        feats.append([L, uniq, punct])\n    return np.array(feats, dtype=np.float32)\n\n\n# Prepare datasets\ndatasets = {}\n\n# synthetic\nx_tr_s, y_tr_s = sample_data(1000)\nx_val_s, y_val_s = sample_data(200)\nmean_s, std_s = x_tr_s.mean(0), x_tr_s.std(0) + 1e-6\nx_tr_s = (x_tr_s - mean_s) / std_s\nx_val_s = (x_val_s - mean_s) / std_s\ndatasets[\"synthetic\"] = (\n    TensorDataset(torch.from_numpy(x_tr_s), torch.from_numpy(y_tr_s)),\n    TensorDataset(torch.from_numpy(x_val_s), torch.from_numpy(y_val_s)),\n)\n\n# three HF tasks\nhf_configs = [\n    (\"sst2\", \"glue\", \"sst2\", \"sentence\", \"label\", \"train\", \"validation\"),\n    (\"yelp_polarity\", \"yelp_polarity\", None, \"text\", \"label\", \"train\", \"test\"),\n    (\"imdb\", \"imdb\", None, \"text\", \"label\", \"train\", \"test\"),\n]\nfor name, module, subset, txt_f, lbl_f, tr_sp, vl_sp in hf_configs:\n    if subset:\n        dtr = load_dataset(module, subset, split=tr_sp)\n        dvl = load_dataset(module, subset, split=vl_sp)\n    else:\n        dtr = load_dataset(module, split=tr_sp)\n        dvl = load_dataset(module, split=vl_sp)\n    dtr = dtr.shuffle(42).select(range(1000))\n    dvl = dvl.shuffle(42).select(range(200))\n    x_tr = extract_features(dtr[txt_f])\n    x_val = extract_features(dvl[txt_f])\n    mean, std = x_tr.mean(0), x_tr.std(0) + 1e-6\n    x_tr = (x_tr - mean) / std\n    x_val = (x_val - mean) / std\n    y_tr = np.array(dtr[lbl_f], dtype=np.float32)\n    y_val = np.array(dvl[lbl_f], dtype=np.float32)\n    datasets[name] = (\n        TensorDataset(torch.from_numpy(x_tr), torch.from_numpy(y_tr)),\n        TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val)),\n    )\n\n# Experiment data\nexperiment_data = {}\nfor ds in datasets:\n    experiment_data[ds] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"DES\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\nbatch_sizes = [32, 64]\nlearning_rates = [1e-3, 1e-2]\nepochs = 10\n\nfor name, (train_ds, val_ds) in datasets.items():\n    for lr in learning_rates:\n        for bs in batch_sizes:\n            train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n            val_loader = DataLoader(val_ds, batch_size=bs)\n            model = nn.Sequential(nn.Linear(3, 16), nn.ReLU(), nn.Linear(16, 1)).to(\n                device\n            )\n            optimizer = Adam(model.parameters(), lr=lr)\n            scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n            loss_fn = nn.BCEWithLogitsLoss()\n            for epoch in range(1, epochs + 1):\n                # train\n                model.train()\n                t_losses, t_preds, t_labels = [], [], []\n                for xb, yb in train_loader:\n                    xb, yb = xb.to(device), yb.to(device)\n                    logits = model(xb).squeeze(1)\n                    loss = loss_fn(logits, yb)\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n                    t_losses.append(loss.item())\n                    t_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n                    t_labels.append(yb.cpu().numpy())\n                train_loss = np.mean(t_losses)\n                train_auc = roc_auc_score(\n                    np.concatenate(t_labels), np.concatenate(t_preds)\n                )\n                # val\n                model.eval()\n                v_losses, v_preds, v_labels = [], [], []\n                with torch.no_grad():\n                    for xb, yb in val_loader:\n                        xb, yb = xb.to(device), yb.to(device)\n                        logits = model(xb).squeeze(1)\n                        loss = loss_fn(logits, yb)\n                        v_losses.append(loss.item())\n                        v_preds.append(torch.sigmoid(logits).cpu().numpy())\n                        v_labels.append(yb.cpu().numpy())\n                val_loss = np.mean(v_losses)\n                val_preds = np.concatenate(v_preds)\n                val_labels = np.concatenate(v_labels)\n                val_auc = roc_auc_score(val_labels, val_preds)\n                DES = val_auc / 1.0\n                print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n                exp = experiment_data[name]\n                exp[\"metrics\"][\"train\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"auc\": train_auc}\n                )\n                exp[\"metrics\"][\"val\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"auc\": val_auc}\n                )\n                exp[\"losses\"][\"train\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"loss\": train_loss}\n                )\n                exp[\"losses\"][\"val\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"loss\": val_loss}\n                )\n                exp[\"DES\"].append({\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"DES\": DES})\n                exp[\"predictions\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"preds\": val_preds}\n                )\n                exp[\"ground_truth\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"labels\": val_labels}\n                )\n                scheduler.step()\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, re, nltk\nimport torch, numpy as np\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\nfrom torch import nn\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# Setup working dir and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Prepare WordNet\nnltk.download(\"wordnet\", quiet=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paraphrases = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lem = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lem:\n                new[idx] = random.choice(list(lem))\n        paraphrases.append(\" \".join(new))\n    return paraphrases\n\n\n# Datasets config\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\nexperiment_data = {}\nK, epochs, bs, lr = 3, 3, 16, 2e-5\n\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # load splits\n    if sub:\n        train = load_dataset(ds, sub, split=\"train\").shuffle(42).select(range(1000))\n        val = load_dataset(ds, sub, split=\"validation\").shuffle(42).select(range(200))\n    else:\n        train = load_dataset(ds, split=\"train\").shuffle(42).select(range(1000))\n        val = load_dataset(ds, split=\"test\").shuffle(42).select(range(200))\n    # prepare paraphrases\n    val_texts = val[text_col]\n    val_labels = val[label_col]\n    para = {i: generate_paraphrases(t, K) for i, t in enumerate(val_texts)}\n    # tokenize\n    tr_enc = tokenizer(train[text_col], truncation=True, padding=True)\n    va_enc = tokenizer(val_texts, truncation=True, padding=True)\n    train_ds = TensorDataset(\n        torch.tensor(tr_enc[\"input_ids\"]),\n        torch.tensor(tr_enc[\"attention_mask\"]),\n        torch.tensor(train[label_col]),\n    )\n    val_ds = TensorDataset(\n        torch.tensor(va_enc[\"input_ids\"]),\n        torch.tensor(va_enc[\"attention_mask\"]),\n        torch.tensor(val_labels),\n    )\n    tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=bs)\n    # model & optimizer\n    model = DistilBertForSequenceClassification.from_pretrained(\n        \"distilbert-base-uncased\", num_labels=2\n    ).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n    experiment_data[name] = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"detection\": []},\n        \"predictions\": [],\n        \"ground_truth\": val_labels,\n    }\n\n    # training + detection\n    for epoch in range(1, epochs + 1):\n        # train\n        model.train()\n        t_losses = []\n        for ids, mask, labels in tr_loader:\n            ids, mask, labels = (\n                ids.to(device),\n                mask.to(device),\n                labels.to(device).long(),\n            )\n            out = model(input_ids=ids, attention_mask=mask, labels=labels)\n            loss = out.loss\n            loss.backward()\n            optimizer.zero_grad()\n            optimizer.step()\n            t_losses.append(loss.item())\n        experiment_data[name][\"losses\"][\"train\"].append(\n            {\"epoch\": epoch, \"loss\": np.mean(t_losses)}\n        )\n\n        # validation\n        model.eval()\n        v_losses = []\n        with torch.no_grad():\n            for ids, mask, labels in va_loader:\n                ids, mask, labels = (\n                    ids.to(device),\n                    mask.to(device),\n                    labels.to(device).long(),\n                )\n                out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                v_losses.append(out.loss.item())\n        val_loss = np.mean(v_losses)\n        experiment_data[name][\"losses\"][\"val\"].append(\n            {\"epoch\": epoch, \"loss\": val_loss}\n        )\n        print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n        # detection via PIU\n        uncs, errs = [], []\n        for i, (t, gt) in enumerate(zip(val_texts, val_labels)):\n            preds = []\n            for txt in [t] + para[i]:\n                enc = tokenizer(\n                    txt, return_tensors=\"pt\", truncation=True, padding=True\n                ).to(device)\n                with torch.no_grad():\n                    logits = model(**enc).logits\n                preds.append(int(torch.argmax(logits, -1).item()))\n            maj = max(set(preds), key=preds.count)\n            uncs.append(1 - preds.count(maj) / len(preds))\n            errs.append(int(preds[0] != int(gt)))\n        try:\n            auc = roc_auc_score(errs, uncs)\n        except:\n            auc = 0.5\n        des = auc / (K + 1)\n        experiment_data[name][\"metrics\"][\"detection\"].append(\n            {\"epoch\": epoch, \"auc\": auc, \"DES\": des}\n        )\n        print(f\"Epoch {epoch}: detection_auc = {auc:.4f}, DES = {des:.4f}\")\n\n    # save final preds & labels\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for ids, mask, labels in va_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            logits = model(input_ids=ids, attention_mask=mask).logits\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n    experiment_data[name][\"predictions\"] = preds\n\n# persist data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sklearn.metrics import roc_auc_score\nfrom collections import Counter\n\n# load generative classification model\nmodel_name = \"google/flan-t5-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n\n# define prompt templates\ntemplates = [\n    \"Review: {text}\\nSentiment (Positive or Negative):\",\n    \"Analyze the sentiment of this review: {text}.\",\n    \"Given the review: {text}, is it positive or negative?\",\n    \"Decide whether the following review is positive or negative: {text}.\",\n    \"This review says: {text}. The sentiment is\",\n]\nK = len(templates)\ndatasets = [\n    (\"sst2\", load_dataset(\"glue\", \"sst2\", split=\"validation\")),\n    (\"yelp_polarity\", load_dataset(\"yelp_polarity\", split=\"test\")),\n    (\"imdb\", load_dataset(\"imdb\", split=\"test\")),\n]\nexperiment_data = {}\n\nfor name, ds in datasets:\n    ds = ds.select(range(50))\n    var_scores, errs, preds_store, gts = [], [], [], []\n    for sample in ds:\n        text = sample[\"sentence\"] if name == \"sst2\" else sample[\"text\"]\n        true = int(sample[\"label\"])\n        gts.append(true)\n        preds = []\n        for tpl in templates:\n            prompt = tpl.format(text=text)\n            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n            out = model.generate(**inputs, max_new_tokens=5)\n            ans = tokenizer.decode(out[0], skip_special_tokens=True).lower()\n            pred = 1 if \"positive\" in ans else 0\n            preds.append(pred)\n        cnt = Counter(preds)\n        maj_label, maj_count = cnt.most_common(1)[0]\n        agree_rate = maj_count / K\n        var = 1.0 - agree_rate\n        err = int(maj_label != true)\n        var_scores.append(var)\n        errs.append(err)\n        preds_store.append(preds)\n    # compute detection metrics\n    auc = roc_auc_score(errs, var_scores)\n    des = auc / K\n    print(f\"Epoch 1: validation_loss = {np.mean(var_scores):.4f}\")\n    print(f\"{name} detection AUC = {auc:.4f}, DES = {des:.4f}\")\n    experiment_data[name] = {\n        \"variation_scores\": var_scores,\n        \"errors\": errs,\n        \"predictions\": preds_store,\n        \"ground_truth\": gts,\n        \"roc_auc\": auc,\n        \"DES\": des,\n    }\n\n# save all data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport re\nimport string\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.metrics import roc_auc_score\nfrom datasets import load_dataset\n\n# setup working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef sample_data(N):\n    N0 = N // 2\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N - N0), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N - N0)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\ndef extract_features(texts):\n    feats = []\n    for s in texts:\n        toks = s.split()\n        L = max(len(toks), 1)\n        uniq = len(set(toks)) / L\n        punct = sum(1 for c in s if c in string.punctuation) / L\n        feats.append([L, uniq, punct])\n    return np.array(feats, dtype=np.float32)\n\n\n# Prepare datasets\ndatasets = {}\n\n# synthetic\nx_tr_s, y_tr_s = sample_data(1000)\nx_val_s, y_val_s = sample_data(200)\nmean_s, std_s = x_tr_s.mean(0), x_tr_s.std(0) + 1e-6\nx_tr_s = (x_tr_s - mean_s) / std_s\nx_val_s = (x_val_s - mean_s) / std_s\ndatasets[\"synthetic\"] = (\n    TensorDataset(torch.from_numpy(x_tr_s), torch.from_numpy(y_tr_s)),\n    TensorDataset(torch.from_numpy(x_val_s), torch.from_numpy(y_val_s)),\n)\n\n# three HF tasks\nhf_configs = [\n    (\"sst2\", \"glue\", \"sst2\", \"sentence\", \"label\", \"train\", \"validation\"),\n    (\"yelp_polarity\", \"yelp_polarity\", None, \"text\", \"label\", \"train\", \"test\"),\n    (\"imdb\", \"imdb\", None, \"text\", \"label\", \"train\", \"test\"),\n]\nfor name, module, subset, txt_f, lbl_f, tr_sp, vl_sp in hf_configs:\n    if subset:\n        dtr = load_dataset(module, subset, split=tr_sp)\n        dvl = load_dataset(module, subset, split=vl_sp)\n    else:\n        dtr = load_dataset(module, split=tr_sp)\n        dvl = load_dataset(module, split=vl_sp)\n    dtr = dtr.shuffle(42).select(range(1000))\n    dvl = dvl.shuffle(42).select(range(200))\n    x_tr = extract_features(dtr[txt_f])\n    x_val = extract_features(dvl[txt_f])\n    mean, std = x_tr.mean(0), x_tr.std(0) + 1e-6\n    x_tr = (x_tr - mean) / std\n    x_val = (x_val - mean) / std\n    y_tr = np.array(dtr[lbl_f], dtype=np.float32)\n    y_val = np.array(dvl[lbl_f], dtype=np.float32)\n    datasets[name] = (\n        TensorDataset(torch.from_numpy(x_tr), torch.from_numpy(y_tr)),\n        TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val)),\n    )\n\n# Experiment tracking\nexperiment_data = {}\nfor ds in datasets:\n    experiment_data[ds] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"DES\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\nbatch_sizes = [32, 64]\nlearning_rates = [1e-3, 1e-2]\nepochs = 10\n\nfor name, (train_ds, val_ds) in datasets.items():\n    # dynamically determine input dimension\n    input_dim = train_ds.tensors[0].shape[1]\n    for lr in learning_rates:\n        for bs in batch_sizes:\n            train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n            val_loader = DataLoader(val_ds, batch_size=bs)\n            # build model with correct input size\n            model = nn.Sequential(\n                nn.Linear(input_dim, 16),\n                nn.ReLU(),\n                nn.Linear(16, 1),\n            ).to(device)\n            optimizer = Adam(model.parameters(), lr=lr)\n            scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n            loss_fn = nn.BCEWithLogitsLoss()\n            for epoch in range(1, epochs + 1):\n                # train\n                model.train()\n                t_losses, t_preds, t_labels = [], [], []\n                for xb, yb in train_loader:\n                    xb, yb = xb.to(device), yb.to(device)\n                    logits = model(xb).squeeze(1)\n                    loss = loss_fn(logits, yb)\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n                    t_losses.append(loss.item())\n                    t_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n                    t_labels.append(yb.cpu().numpy())\n                train_loss = np.mean(t_losses)\n                train_auc = roc_auc_score(\n                    np.concatenate(t_labels), np.concatenate(t_preds)\n                )\n                # val\n                model.eval()\n                v_losses, v_preds, v_labels = [], [], []\n                with torch.no_grad():\n                    for xb, yb in val_loader:\n                        xb, yb = xb.to(device), yb.to(device)\n                        logits = model(xb).squeeze(1)\n                        loss = loss_fn(logits, yb)\n                        v_losses.append(loss.item())\n                        v_preds.append(torch.sigmoid(logits).cpu().numpy())\n                        v_labels.append(yb.cpu().numpy())\n                val_loss = np.mean(v_losses)\n                val_preds = np.concatenate(v_preds)\n                val_labels = np.concatenate(v_labels)\n                val_auc = roc_auc_score(val_labels, val_preds)\n                DES = val_auc / float(\n                    (1 + bs / bs)\n                )  # Dummy DES: auc / forward calls factor\n                print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n                exp = experiment_data[name]\n                exp[\"metrics\"][\"train\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"auc\": train_auc}\n                )\n                exp[\"metrics\"][\"val\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"auc\": val_auc}\n                )\n                exp[\"losses\"][\"train\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"loss\": train_loss}\n                )\n                exp[\"losses\"][\"val\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"loss\": val_loss}\n                )\n                exp[\"DES\"].append({\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"DES\": DES})\n                exp[\"predictions\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"preds\": val_preds}\n                )\n                exp[\"ground_truth\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"labels\": val_labels}\n                )\n                scheduler.step()\n\n# save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport numpy as np\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    PegasusTokenizer,\n    PegasusForConditionalGeneration,\n    DataCollatorWithPadding,\n)\nfrom torch.optim import AdamW\nfrom sklearn.metrics import roc_auc_score\n\n# setup working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# paraphrase model\nK = 3\ntokenizer_para = PegasusTokenizer.from_pretrained(\"tuner007/pegasus_paraphrase\")\nmodel_para = PegasusForConditionalGeneration.from_pretrained(\n    \"tuner007/pegasus_paraphrase\"\n).to(device)\n\n\ndef paraphrase(text):\n    batch = tokenizer_para(\n        [text], truncation=True, padding=\"longest\", return_tensors=\"pt\"\n    ).to(device)\n    outs = model_para.generate(\n        **batch, num_beams=10, num_return_sequences=K, max_length=60\n    )\n    return tokenizer_para.batch_decode(outs, skip_special_tokens=True)\n\n\n# classification components\ntokenizer_clf = AutoTokenizer.from_pretrained(\n    \"distilbert-base-uncased-finetuned-sst-2-english\"\n)\nmodel_clf = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased-finetuned-sst-2-english\"\n).to(device)\ncollator = DataCollatorWithPadding(tokenizer_clf)\n\n# experiment data container\nexperiment_data = {}\ndatasets_cfg = [\n    (\"sst2\", \"glue\", \"sst2\", \"sentence\", \"label\", \"train\", \"validation\"),\n    (\"yelp_polarity\", \"yelp_polarity\", None, \"text\", \"label\", \"train\", \"test\"),\n    (\"imdb\", \"imdb\", None, \"text\", \"label\", \"train\", \"test\"),\n]\nfor name, mod, sub, text_key, label_key, ts, vs in datasets_cfg:\n    # load and sample\n    if sub:\n        dtr = load_dataset(mod, sub, split=ts)\n        dvl = load_dataset(mod, sub, split=vs)\n    else:\n        dtr = load_dataset(mod, split=ts)\n        dvl = load_dataset(mod, split=vs)\n    dtr = dtr.shuffle(42).select(range(200))\n    dvl = dvl.shuffle(42).select(range(50))\n    train_texts = dtr[text_key]\n    train_labels = dtr[label_key]\n    val_texts = dvl[text_key]\n    val_labels = dvl[label_key]\n\n    # simple dataset\n    class TxtDS(Dataset):\n        def __init__(self, texts, labels):\n            self.texts, texts2 = texts, texts\n            self.labels = labels\n\n        def __len__(self):\n            return len(self.labels)\n\n        def __getitem__(self, i):\n            return {\"input\": self.texts[i], \"label\": self.labels[i]}\n\n    tr_ds = TxtDS(train_texts, train_labels)\n    vl_ds = TxtDS(val_texts, val_labels)\n\n    # loaders\n    def collate_fn(batch):\n        txts = [b[\"input\"] for b in batch]\n        lbs = torch.tensor([b[\"label\"] for b in batch])\n        tok = tokenizer_clf(txts, truncation=True, padding=True, return_tensors=\"pt\")\n        tok[\"labels\"] = lbs\n        return {k: v.to(device) for k, v in tok.items()}\n\n    tr_loader = DataLoader(tr_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n    vl_loader = DataLoader(vl_ds, batch_size=8, shuffle=False, collate_fn=collate_fn)\n    # metrics store\n    experiment_data[name] = {\"losses\": {\"train\": [], \"val\": []}, \"AUC\": [], \"DES\": []}\n    optimizer = AdamW(model_clf.parameters(), lr=5e-5)\n    # train for 2 epochs\n    for epoch in range(1, 3):\n        # train\n        model_clf.train()\n        tl = []\n        for batch in tr_loader:\n            out = model_clf(**batch)\n            loss = out.loss\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            tl.append(loss.item())\n        # val\n        model_clf.eval()\n        vl, preds, labs = [], [], []\n        with torch.no_grad():\n            for batch in vl_loader:\n                out = model_clf(**batch)\n                vl.append(out.loss.item())\n                logits = out.logits\n                pr = torch.argmax(logits, 1)\n                preds.extend(pr.cpu().numpy())\n                labs.extend(batch[\"labels\"].cpu().numpy())\n        train_loss = np.mean(tl)\n        val_loss = np.mean(vl)\n        print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n        experiment_data[name][\"losses\"][\"train\"].append(\n            {\"epoch\": epoch, \"loss\": train_loss}\n        )\n        experiment_data[name][\"losses\"][\"val\"].append(\n            {\"epoch\": epoch, \"loss\": val_loss}\n        )\n        # PIU on val\n        errs = np.array([p != l for p, l in zip(preds, labs)], dtype=int)\n        uncs = []\n        for text in val_texts:\n            hyps = paraphrase(text)\n            toks = tokenizer_clf(\n                hyps, truncation=True, padding=True, return_tensors=\"pt\"\n            ).to(device)\n            outp = model_clf(**toks).logits\n            prp = torch.argmax(outp, 1).cpu().numpy()\n            agr = np.bincount(prp).max() / K\n            uncs.append(1 - agr)\n        auc = roc_auc_score(errs, uncs)\n        des = auc / (K + 1)\n        experiment_data[name][\"AUC\"].append({\"epoch\": epoch, \"AUC\": auc})\n        experiment_data[name][\"DES\"].append({\"epoch\": epoch, \"DES\": des})\n    # end epochs\n# save all metrics\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nfrom transformers import pipeline\nimport numpy as np\nimport torch\nfrom datasets import load_dataset\nfrom sklearn.metrics import roc_auc_score\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Initialize pipelines on GPU if available\nclf = pipeline(\n    \"text-classification\",\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n    device=0 if device.type == \"cuda\" else -1,\n)\ntrans_en_de = pipeline(\n    \"translation_en_to_de\",\n    model=\"Helsinki-NLP/opus-mt-en-de\",\n    device=0 if device.type == \"cuda\" else -1,\n)\ntrans_de_en = pipeline(\n    \"translation_de_to_en\",\n    model=\"Helsinki-NLP/opus-mt-de-en\",\n    device=0 if device.type == \"cuda\" else -1,\n)\n\n# Load three HuggingFace datasets (sample 200 examples each)\nhf_configs = [\n    (\"sst2\", \"glue\", \"sst2\", \"sentence\", \"label\"),\n    (\"imdb\", \"imdb\", None, \"text\", \"label\"),\n    (\"yelp_polarity\", \"yelp_polarity\", None, \"text\", \"label\"),\n]\ndatasets = {}\nfor name, module, subset, text_field, label_field in hf_configs:\n    ds = (\n        load_dataset(module, subset, split=\"train\")\n        if subset\n        else load_dataset(module, split=\"train\")\n    )\n    ds = ds.shuffle(seed=42).select(range(200))\n    datasets[name] = list(zip(ds[text_field], ds[label_field]))\n\n# Sweep ensemble size K and compute ROC-AUC and DES\nKs = [1, 3, 5]\nexperiment_data = {n: {\"K\": [], \"roc_auc\": [], \"DES\": []} for n in datasets}\n\nfor K in Ks:\n    for ds_name, examples in datasets.items():\n        y_err, scores = [], []\n        for text, true_lbl in examples:\n            # original prediction\n            orig = clf(text)[0][\"label\"]\n            pred0 = 1 if orig.endswith(\"1\") or orig == \"LABEL_1\" else 0\n            err = int(pred0 != true_lbl)\n            # back\u2010translation for K paraphrases\n            de_outs = trans_en_de(text, num_beams=5, num_return_sequences=K)\n            de_texts = [o[\"translation_text\"] for o in de_outs]\n            paras = [trans_de_en(d)[0][\"translation_text\"] for d in de_texts]\n            # classify paraphrases\n            ps = []\n            for p in paras:\n                o = clf(p)[0][\"label\"]\n                ps.append(1 if o.endswith(\"1\") or o == \"LABEL_1\" else 0)\n            agreement = sum(p == pred0 for p in ps) / K\n            unc = 1 - agreement\n            y_err.append(err)\n            scores.append(unc)\n        roc = roc_auc_score(y_err, scores)\n        des_score = roc / float(K + 1)\n        print(f\"Epoch {K}: validation_loss = {1-roc:.4f}\")\n        ed = experiment_data[ds_name]\n        ed[\"K\"].append(K)\n        ed[\"roc_auc\"].append(roc)\n        ed[\"DES\"].append(des_score)\n\n# Save all metrics\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment data.\")\n", "import os\nimport random\nimport re\nimport nltk\nimport torch\nimport numpy as np\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# Setup working dir and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Prepare WordNet\nnltk.download(\"wordnet\", quiet=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paraphrases = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paraphrases.append(\" \".join(new))\n    return paraphrases\n\n\n# Datasets config\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\nexperiment_data = {}\nK, epochs, bs, lr = 3, 3, 16, 2e-5\n\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # load splits\n    if sub:\n        train = load_dataset(ds, sub, split=\"train\").shuffle(42).select(range(1000))\n        val = load_dataset(ds, sub, split=\"validation\").shuffle(42).select(range(200))\n    else:\n        train = load_dataset(ds, split=\"train\").shuffle(42).select(range(1000))\n        val = load_dataset(ds, split=\"test\").shuffle(42).select(range(200))\n\n    val_texts = val[text_col]\n    val_labels = val[label_col]\n    para = {i: generate_paraphrases(t, K) for i, t in enumerate(val_texts)}\n\n    # tokenize datasets\n    tr_enc = tokenizer(train[text_col], truncation=True, padding=True)\n    va_enc = tokenizer(val_texts, truncation=True, padding=True)\n    train_ds = TensorDataset(\n        torch.tensor(tr_enc[\"input_ids\"]),\n        torch.tensor(tr_enc[\"attention_mask\"]),\n        torch.tensor(train[label_col]),\n    )\n    val_ds = TensorDataset(\n        torch.tensor(va_enc[\"input_ids\"]),\n        torch.tensor(va_enc[\"attention_mask\"]),\n        torch.tensor(val_labels),\n    )\n    tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=bs)\n\n    # model & optimizer\n    model = DistilBertForSequenceClassification.from_pretrained(\n        \"distilbert-base-uncased\", num_labels=2\n    ).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n\n    experiment_data[name] = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"detection\": []},\n        \"predictions\": [],\n        \"ground_truth\": val_labels,\n    }\n\n    # training + detection\n    for epoch in range(1, epochs + 1):\n        model.train()\n        t_losses = []\n        for ids, mask, labels in tr_loader:\n            ids, mask, labels = (\n                ids.to(device),\n                mask.to(device),\n                labels.to(device).long(),\n            )\n            optimizer.zero_grad()  # Clear grads before backward\n            out = model(input_ids=ids, attention_mask=mask, labels=labels)\n            loss = out.loss\n            loss.backward()\n            optimizer.step()\n            t_losses.append(loss.item())\n        experiment_data[name][\"losses\"][\"train\"].append(\n            {\"epoch\": epoch, \"loss\": float(np.mean(t_losses))}\n        )\n\n        model.eval()\n        v_losses = []\n        with torch.no_grad():\n            for ids, mask, labels in va_loader:\n                ids, mask, labels = (\n                    ids.to(device),\n                    mask.to(device),\n                    labels.to(device).long(),\n                )\n                out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                v_losses.append(out.loss.item())\n        val_loss = float(np.mean(v_losses))\n        experiment_data[name][\"losses\"][\"val\"].append(\n            {\"epoch\": epoch, \"loss\": val_loss}\n        )\n        print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n        # detection via PIU\n        uncs, errs = [], []\n        for i, (t, gt) in enumerate(zip(val_texts, val_labels)):\n            preds = []\n            for txt in [t] + para[i]:\n                enc = tokenizer(\n                    txt, return_tensors=\"pt\", truncation=True, padding=True\n                ).to(device)\n                with torch.no_grad():\n                    logits = model(**enc).logits\n                preds.append(int(torch.argmax(logits, -1).item()))\n            maj = max(set(preds), key=preds.count)\n            uncs.append(1 - preds.count(maj) / len(preds))\n            errs.append(int(preds[0] != int(gt)))\n        try:\n            auc = roc_auc_score(errs, uncs)\n        except:\n            auc = 0.5\n        des = auc / (K + 1)\n        experiment_data[name][\"metrics\"][\"detection\"].append(\n            {\"epoch\": epoch, \"auc\": auc, \"DES\": des}\n        )\n        print(f\"Epoch {epoch}: detection_auc = {auc:.4f}, DES = {des:.4f}\")\n\n    # save final preds & labels\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for ids, mask, _ in va_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            logits = model(input_ids=ids, attention_mask=mask).logits\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n    experiment_data[name][\"predictions\"] = preds\n\n# persist data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport random, re\nimport nltk\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# prepare WordNet\nnltk.download(\"wordnet\", quiet=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\n# datasets\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\n# hyperparams\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\n\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\nexperiment_data = {}\n\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # load and trim\n    ds_train = (\n        load_dataset(ds, sub, split=\"train\") if sub else load_dataset(ds, split=\"train\")\n    )\n    ds_val = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n    ds_train = ds_train.shuffle(42).select(range(train_size))\n    ds_val = ds_val.shuffle(42).select(range(val_size))\n    texts_train, labels_train = ds_train[text_col], ds_train[label_col]\n    texts_val, labels_val = ds_val[text_col], ds_val[label_col]\n    # paraphrases for val\n    paras = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_val)}\n    # tokenize\n    tr_enc = tokenizer(texts_train, truncation=True, padding=True, return_tensors=\"pt\")\n    va_enc = tokenizer(texts_val, truncation=True, padding=True, return_tensors=\"pt\")\n    train_ds = TensorDataset(\n        tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n    )\n    val_ds = TensorDataset(\n        va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_val)\n    )\n    tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=bs)\n\n    # model & optimizer\n    model = BertForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\", num_labels=2\n    ).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n\n    experiment_data[name] = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"detection\": []},\n        \"predictions\": [],\n        \"ground_truth\": labels_val,\n    }\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        train_losses = []\n        for ids, mask, labels in tr_loader:\n            ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n            optimizer.zero_grad()\n            out = model(input_ids=ids, attention_mask=mask, labels=labels)\n            out.loss.backward()\n            optimizer.step()\n            train_losses.append(out.loss.item())\n        experiment_data[name][\"losses\"][\"train\"].append(\n            {\"epoch\": epoch, \"loss\": float(np.mean(train_losses))}\n        )\n\n        # validation loss\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for ids, mask, labels in va_loader:\n                ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n                out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                val_losses.append(out.loss.item())\n        val_loss = float(np.mean(val_losses))\n        experiment_data[name][\"losses\"][\"val\"].append(\n            {\"epoch\": epoch, \"loss\": val_loss}\n        )\n        print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n        # detection metrics\n        uncs_vote, uncs_kl, errs = [], [], []\n        for i, txt in enumerate(texts_val):\n            probs = []\n            preds = []\n            for variant in [txt] + paras[i]:\n                enc = tokenizer(\n                    variant, return_tensors=\"pt\", truncation=True, padding=True\n                ).to(device)\n                with torch.no_grad():\n                    logits = model(**enc).logits\n                p = torch.softmax(logits, dim=-1).squeeze(0).cpu()\n                probs.append(p)\n                preds.append(int(p.argmax().item()))\n            maj = max(set(preds), key=preds.count)\n            uncs_vote.append(1 - preds.count(maj) / len(preds))\n            # sym KL across all pairs\n            kl_vals = []\n            for a in range(len(probs)):\n                for b in range(a + 1, len(probs)):\n                    P, Q = probs[a], probs[b]\n                    kl1 = F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                    kl2 = F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                    kl_vals.append(0.5 * (kl1 + kl2))\n            uncs_kl.append(float(np.mean(kl_vals)))\n            errs.append(int(preds[0] != int(labels_val[i])))\n        # compute AUCs and DES\n        try:\n            auc_v = roc_auc_score(errs, uncs_vote)\n        except:\n            auc_v = 0.5\n        try:\n            auc_k = roc_auc_score(errs, uncs_kl)\n        except:\n            auc_k = 0.5\n        des_v = auc_v / (K + 1)\n        des_k = auc_k / (K + 1)\n        experiment_data[name][\"metrics\"][\"detection\"].append(\n            {\n                \"epoch\": epoch,\n                \"auc_vote\": auc_v,\n                \"DES_vote\": des_v,\n                \"auc_kl\": auc_k,\n                \"DES_kl\": des_k,\n            }\n        )\n        print(\n            f\"Epoch {epoch}: AUC_vote={auc_v:.4f}, DES_vote={des_v:.4f}, AUC_kl={auc_k:.4f}, DES_kl={des_k:.4f}\"\n        )\n\n    # final predictions\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for ids, mask, _ in va_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            logits = model(input_ids=ids, attention_mask=mask).logits\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n    experiment_data[name][\"predictions\"] = preds\n\n# save all data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport random\nimport re\nimport nltk\nimport torch\nimport numpy as np\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import AdamW\nfrom sklearn.metrics import roc_auc_score\n\n# Setup working dir and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Prepare WordNet\nnltk.download(\"wordnet\", quiet=True)\nfrom nltk.corpus import wordnet\n\n\ndef generate_perturbations(text, K):\n    words = text.split()\n    perturbs = []\n    half = K // 2\n    # synonym replacements\n    for _ in range(half):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        perturbs.append(\" \".join(new))\n    # random swaps\n    for _ in range(K - half):\n        new = words.copy()\n        if len(new) > 1:\n            i, j = random.sample(range(len(new)), 2)\n            new[i], new[j] = new[j], new[i]\n        perturbs.append(\" \".join(new))\n    return perturbs\n\n\n# Datasets\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\nK, epochs, bs, lr = 5, 4, 32, 3e-5\nexperiment_data = {}\n\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # load and sample\n    if sub:\n        dset_tr = load_dataset(ds, sub, split=\"train\").shuffle(42).select(range(3000))\n        dset_va = (\n            load_dataset(ds, sub, split=\"validation\").shuffle(42).select(range(600))\n        )\n    else:\n        dset_tr = load_dataset(ds, split=\"train\").shuffle(42).select(range(3000))\n        dset_va = load_dataset(ds, split=\"test\").shuffle(42).select(range(600))\n    train_texts = dset_tr[text_col]\n    train_labels = dset_tr[label_col]\n    val_texts = dset_va[text_col]\n    val_labels = dset_va[label_col]\n    # precompute perturbations\n    paraphrases = {i: generate_perturbations(t, K) for i, t in enumerate(val_texts)}\n    # tokenize\n    tr_enc = tokenizer(\n        train_texts, truncation=True, padding=\"max_length\", max_length=128\n    )\n    va_enc = tokenizer(val_texts, truncation=True, padding=\"max_length\", max_length=128)\n    train_ds = TensorDataset(\n        torch.tensor(tr_enc[\"input_ids\"]),\n        torch.tensor(tr_enc[\"attention_mask\"]),\n        torch.tensor(train_labels),\n    )\n    val_ds = TensorDataset(\n        torch.tensor(va_enc[\"input_ids\"]),\n        torch.tensor(va_enc[\"attention_mask\"]),\n        torch.tensor(val_labels),\n    )\n    tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=bs)\n    # model & optimizer\n    model = BertForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\", num_labels=2\n    ).to(device)\n    optimizer = AdamW(model.parameters(), lr=lr)\n    experiment_data[name] = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"detection\": []},\n        \"predictions\": [],\n        \"ground_truth\": list(val_labels),\n    }\n    # training + detection\n    for epoch in range(1, epochs + 1):\n        model.train()\n        t_losses = []\n        for ids, mask, labels in tr_loader:\n            ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n            optimizer.zero_grad()\n            out = model(input_ids=ids, attention_mask=mask, labels=labels)\n            loss = out.loss\n            loss.backward()\n            optimizer.step()\n            t_losses.append(loss.item())\n        train_loss = float(np.mean(t_losses))\n        experiment_data[name][\"losses\"][\"train\"].append(\n            {\"epoch\": epoch, \"loss\": train_loss}\n        )\n        # validation loss\n        model.eval()\n        v_losses = []\n        with torch.no_grad():\n            for ids, mask, labels in va_loader:\n                ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n                out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                v_losses.append(out.loss.item())\n        val_loss = float(np.mean(v_losses))\n        experiment_data[name][\"losses\"][\"val\"].append(\n            {\"epoch\": epoch, \"loss\": val_loss}\n        )\n        print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n        # detection via PIU composite score\n        uncs, errs = [], []\n        for i, (txt, gt) in enumerate(zip(val_texts, val_labels)):\n            preds, probs = [], []\n            for alt in [txt] + paraphrases[i]:\n                enc = tokenizer(\n                    alt,\n                    return_tensors=\"pt\",\n                    truncation=True,\n                    padding=\"max_length\",\n                    max_length=128,\n                ).to(device)\n                with torch.no_grad():\n                    logits = model(**enc).logits\n                p = torch.softmax(logits, dim=-1)[0, 1].item()\n                probs.append(p)\n                preds.append(int(logits.argmax(-1).item()))\n            maj = max(set(preds), key=preds.count)\n            drate = 1 - preds.count(maj) / len(preds)\n            std_p = float(np.std(probs))\n            norm_std = std_p / 0.5\n            unc = 0.5 * drate + 0.5 * norm_std\n            uncs.append(unc)\n            errs.append(int(preds[0] != gt))\n        try:\n            auc = roc_auc_score(errs, uncs)\n        except:\n            auc = 0.5\n        des = auc / (K + 1)\n        experiment_data[name][\"metrics\"][\"detection\"].append(\n            {\"epoch\": epoch, \"auc\": auc, \"DES\": des}\n        )\n        print(f\"Epoch {epoch}: detection_auc = {auc:.4f}, DES = {des:.4f}\")\n    # final predictions\n    model.eval()\n    final_preds = []\n    with torch.no_grad():\n        for ids, mask, _ in va_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            logits = model(input_ids=ids, attention_mask=mask).logits\n            final_preds.extend(torch.argmax(logits, -1).cpu().tolist())\n    experiment_data[name][\"predictions\"] = final_preds\n\n# persist data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport numpy as np\nfrom transformers import pipeline\nfrom datasets import load_dataset\nfrom sklearn.metrics import roc_auc_score, average_precision_score\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\ndevice_id = torch.cuda.current_device() if torch.cuda.is_available() else -1\n\n# initialize paraphraser and classifier pipelines and move to device\nparaphraser = pipeline(\n    \"text2text-generation\",\n    model=\"tuner007/pegasus_paraphrase\",\n    tokenizer=\"tuner007/pegasus_paraphrase\",\n    device=device_id,\n)\nparaphraser.model.to(device)\n\nclassifier = pipeline(\n    \"zero-shot-classification\",\n    model=\"facebook/bart-large-mnli\",\n    tokenizer=\"facebook/bart-large-mnli\",\n    device=device_id,\n)\nclassifier.model.to(device)\n\n# experiment parameters\nK = 3\ndatasets_info = {\n    \"sst2\": {\n        \"data\": load_dataset(\"glue\", \"sst2\", split=\"validation\")\n        .shuffle(42)\n        .select(range(200)),\n        \"text_key\": \"sentence\",\n        \"label_map\": {0: \"negative\", 1: \"positive\"},\n        \"candidate_labels\": [\"negative\", \"positive\"],\n    },\n    \"imdb\": {\n        \"data\": load_dataset(\"imdb\", split=\"test\").shuffle(42).select(range(200)),\n        \"text_key\": \"text\",\n        \"label_map\": {0: \"negative\", 1: \"positive\"},\n        \"candidate_labels\": [\"negative\", \"positive\"],\n    },\n    \"ag_news\": {\n        \"data\": load_dataset(\"ag_news\", split=\"test\").shuffle(42).select(range(200)),\n        \"text_key\": \"text\",\n        \"label_map\": {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"},\n        \"candidate_labels\": [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"],\n    },\n}\n\nexperiment_data = {}\nfor name, info in datasets_info.items():\n    y_uncert, y_wrong = [], []\n    ds = info[\"data\"]\n    text_key = info[\"text_key\"]\n    label_map = info[\"label_map\"]\n    candidate_labels = info[\"candidate_labels\"]\n\n    for ex in ds:\n        text = ex[text_key]\n        true_label = label_map[ex[\"label\"]]\n\n        # generate paraphrase variants\n        paras = paraphraser(\n            text, max_length=64, num_beams=K * 2, num_return_sequences=K\n        )\n        variants = [text] + [p[\"generated_text\"] for p in paras]\n\n        # classify each variant\n        preds = []\n        for v in variants:\n            out = classifier(v, candidate_labels=candidate_labels)\n            if isinstance(out, list):\n                out = out[0]\n            preds.append(out[\"labels\"][0])\n\n        # compute uncertainty (1 - agreement rate)\n        majority = max(set(preds), key=preds.count)\n        agree_rate = preds.count(majority) / len(variants)\n        y_uncert.append(1.0 - agree_rate)\n        y_wrong.append(int(preds[0] != true_label))\n\n    roc = roc_auc_score(y_wrong, y_uncert)\n    pr = average_precision_score(y_wrong, y_uncert)\n    avg_calls = K + 1\n    des = roc / avg_calls\n\n    print(\n        f\"{name}: ROC-AUC={roc:.4f}, PR-AUC={pr:.4f}, avg_calls={avg_calls}, DES={des:.4f}\"\n    )\n    experiment_data[name] = {\n        \"roc_auc\": roc,\n        \"pr_auc\": pr,\n        \"avg_calls\": avg_calls,\n        \"DES\": des,\n    }\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport random\nimport re\nimport nltk\nimport torch\nimport numpy as np\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# Setup working dir and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Prepare WordNet\nnltk.download(\"wordnet\", quiet=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paraphrases = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paraphrases.append(\" \".join(new))\n    return paraphrases\n\n\n# Datasets config\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\nexperiment_data = {}\nK, epochs, bs, lr = 3, 3, 16, 2e-5\n\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # load splits\n    if sub:\n        train = load_dataset(ds, sub, split=\"train\").shuffle(42).select(range(1000))\n        val = load_dataset(ds, sub, split=\"validation\").shuffle(42).select(range(200))\n    else:\n        train = load_dataset(ds, split=\"train\").shuffle(42).select(range(1000))\n        val = load_dataset(ds, split=\"test\").shuffle(42).select(range(200))\n\n    val_texts = val[text_col]\n    val_labels = val[label_col]\n    para = {i: generate_paraphrases(t, K) for i, t in enumerate(val_texts)}\n\n    # tokenize datasets\n    tr_enc = tokenizer(train[text_col], truncation=True, padding=True)\n    va_enc = tokenizer(val_texts, truncation=True, padding=True)\n    train_ds = TensorDataset(\n        torch.tensor(tr_enc[\"input_ids\"]),\n        torch.tensor(tr_enc[\"attention_mask\"]),\n        torch.tensor(train[label_col]),\n    )\n    val_ds = TensorDataset(\n        torch.tensor(va_enc[\"input_ids\"]),\n        torch.tensor(va_enc[\"attention_mask\"]),\n        torch.tensor(val_labels),\n    )\n    tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=bs)\n\n    # model & optimizer\n    model = DistilBertForSequenceClassification.from_pretrained(\n        \"distilbert-base-uncased\", num_labels=2\n    ).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n\n    experiment_data[name] = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"detection\": []},\n        \"predictions\": [],\n        \"ground_truth\": val_labels,\n    }\n\n    # training + detection\n    for epoch in range(1, epochs + 1):\n        model.train()\n        t_losses = []\n        for ids, mask, labels in tr_loader:\n            ids, mask, labels = (\n                ids.to(device),\n                mask.to(device),\n                labels.to(device).long(),\n            )\n            optimizer.zero_grad()  # Clear grads before backward\n            out = model(input_ids=ids, attention_mask=mask, labels=labels)\n            loss = out.loss\n            loss.backward()\n            optimizer.step()\n            t_losses.append(loss.item())\n        experiment_data[name][\"losses\"][\"train\"].append(\n            {\"epoch\": epoch, \"loss\": float(np.mean(t_losses))}\n        )\n\n        model.eval()\n        v_losses = []\n        with torch.no_grad():\n            for ids, mask, labels in va_loader:\n                ids, mask, labels = (\n                    ids.to(device),\n                    mask.to(device),\n                    labels.to(device).long(),\n                )\n                out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                v_losses.append(out.loss.item())\n        val_loss = float(np.mean(v_losses))\n        experiment_data[name][\"losses\"][\"val\"].append(\n            {\"epoch\": epoch, \"loss\": val_loss}\n        )\n        print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n        # detection via PIU\n        uncs, errs = [], []\n        for i, (t, gt) in enumerate(zip(val_texts, val_labels)):\n            preds = []\n            for txt in [t] + para[i]:\n                enc = tokenizer(\n                    txt, return_tensors=\"pt\", truncation=True, padding=True\n                ).to(device)\n                with torch.no_grad():\n                    logits = model(**enc).logits\n                preds.append(int(torch.argmax(logits, -1).item()))\n            maj = max(set(preds), key=preds.count)\n            uncs.append(1 - preds.count(maj) / len(preds))\n            errs.append(int(preds[0] != int(gt)))\n        try:\n            auc = roc_auc_score(errs, uncs)\n        except:\n            auc = 0.5\n        des = auc / (K + 1)\n        experiment_data[name][\"metrics\"][\"detection\"].append(\n            {\"epoch\": epoch, \"auc\": auc, \"DES\": des}\n        )\n        print(f\"Epoch {epoch}: detection_auc = {auc:.4f}, DES = {des:.4f}\")\n\n    # save final preds & labels\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for ids, mask, _ in va_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            logits = model(input_ids=ids, attention_mask=mask).logits\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n    experiment_data[name][\"predictions\"] = preds\n\n# persist data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport random\nimport re\nimport nltk\nimport torch\nimport numpy as np\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# Setup working dir and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Prepare WordNet\nnltk.download(\"wordnet\", quiet=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paraphrases = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paraphrases.append(\" \".join(new))\n    return paraphrases\n\n\n# Datasets config\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\nexperiment_data = {}\nK, epochs, bs, lr = 3, 3, 16, 2e-5\n\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # load splits\n    if sub:\n        train = load_dataset(ds, sub, split=\"train\").shuffle(42).select(range(1000))\n        val = load_dataset(ds, sub, split=\"validation\").shuffle(42).select(range(200))\n    else:\n        train = load_dataset(ds, split=\"train\").shuffle(42).select(range(1000))\n        val = load_dataset(ds, split=\"test\").shuffle(42).select(range(200))\n\n    val_texts = val[text_col]\n    val_labels = val[label_col]\n    para = {i: generate_paraphrases(t, K) for i, t in enumerate(val_texts)}\n\n    # tokenize datasets\n    tr_enc = tokenizer(train[text_col], truncation=True, padding=True)\n    va_enc = tokenizer(val_texts, truncation=True, padding=True)\n    train_ds = TensorDataset(\n        torch.tensor(tr_enc[\"input_ids\"]),\n        torch.tensor(tr_enc[\"attention_mask\"]),\n        torch.tensor(train[label_col]),\n    )\n    val_ds = TensorDataset(\n        torch.tensor(va_enc[\"input_ids\"]),\n        torch.tensor(va_enc[\"attention_mask\"]),\n        torch.tensor(val_labels),\n    )\n    tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=bs)\n\n    # model & optimizer\n    model = DistilBertForSequenceClassification.from_pretrained(\n        \"distilbert-base-uncased\", num_labels=2\n    ).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n\n    experiment_data[name] = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"detection\": []},\n        \"predictions\": [],\n        \"ground_truth\": val_labels,\n    }\n\n    # training + detection\n    for epoch in range(1, epochs + 1):\n        model.train()\n        t_losses = []\n        for ids, mask, labels in tr_loader:\n            ids, mask, labels = (\n                ids.to(device),\n                mask.to(device),\n                labels.to(device).long(),\n            )\n            optimizer.zero_grad()  # Clear grads before backward\n            out = model(input_ids=ids, attention_mask=mask, labels=labels)\n            loss = out.loss\n            loss.backward()\n            optimizer.step()\n            t_losses.append(loss.item())\n        experiment_data[name][\"losses\"][\"train\"].append(\n            {\"epoch\": epoch, \"loss\": float(np.mean(t_losses))}\n        )\n\n        model.eval()\n        v_losses = []\n        with torch.no_grad():\n            for ids, mask, labels in va_loader:\n                ids, mask, labels = (\n                    ids.to(device),\n                    mask.to(device),\n                    labels.to(device).long(),\n                )\n                out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                v_losses.append(out.loss.item())\n        val_loss = float(np.mean(v_losses))\n        experiment_data[name][\"losses\"][\"val\"].append(\n            {\"epoch\": epoch, \"loss\": val_loss}\n        )\n        print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n        # detection via PIU\n        uncs, errs = [], []\n        for i, (t, gt) in enumerate(zip(val_texts, val_labels)):\n            preds = []\n            for txt in [t] + para[i]:\n                enc = tokenizer(\n                    txt, return_tensors=\"pt\", truncation=True, padding=True\n                ).to(device)\n                with torch.no_grad():\n                    logits = model(**enc).logits\n                preds.append(int(torch.argmax(logits, -1).item()))\n            maj = max(set(preds), key=preds.count)\n            uncs.append(1 - preds.count(maj) / len(preds))\n            errs.append(int(preds[0] != int(gt)))\n        try:\n            auc = roc_auc_score(errs, uncs)\n        except:\n            auc = 0.5\n        des = auc / (K + 1)\n        experiment_data[name][\"metrics\"][\"detection\"].append(\n            {\"epoch\": epoch, \"auc\": auc, \"DES\": des}\n        )\n        print(f\"Epoch {epoch}: detection_auc = {auc:.4f}, DES = {des:.4f}\")\n\n    # save final preds & labels\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for ids, mask, _ in va_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            logits = model(input_ids=ids, attention_mask=mask).logits\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n    experiment_data[name][\"predictions\"] = preds\n\n# persist data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport random\nimport re\nimport nltk\nimport torch\nimport numpy as np\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# Setup working dir and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Prepare WordNet\nnltk.download(\"wordnet\", quiet=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paraphrases = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paraphrases.append(\" \".join(new))\n    return paraphrases\n\n\n# Datasets config\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\nexperiment_data = {}\nK, epochs, bs, lr = 3, 3, 16, 2e-5\n\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # load splits\n    if sub:\n        train = load_dataset(ds, sub, split=\"train\").shuffle(42).select(range(1000))\n        val = load_dataset(ds, sub, split=\"validation\").shuffle(42).select(range(200))\n    else:\n        train = load_dataset(ds, split=\"train\").shuffle(42).select(range(1000))\n        val = load_dataset(ds, split=\"test\").shuffle(42).select(range(200))\n\n    val_texts = val[text_col]\n    val_labels = val[label_col]\n    para = {i: generate_paraphrases(t, K) for i, t in enumerate(val_texts)}\n\n    # tokenize datasets\n    tr_enc = tokenizer(train[text_col], truncation=True, padding=True)\n    va_enc = tokenizer(val_texts, truncation=True, padding=True)\n    train_ds = TensorDataset(\n        torch.tensor(tr_enc[\"input_ids\"]),\n        torch.tensor(tr_enc[\"attention_mask\"]),\n        torch.tensor(train[label_col]),\n    )\n    val_ds = TensorDataset(\n        torch.tensor(va_enc[\"input_ids\"]),\n        torch.tensor(va_enc[\"attention_mask\"]),\n        torch.tensor(val_labels),\n    )\n    tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=bs)\n\n    # model & optimizer\n    model = DistilBertForSequenceClassification.from_pretrained(\n        \"distilbert-base-uncased\", num_labels=2\n    ).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n\n    experiment_data[name] = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"detection\": []},\n        \"predictions\": [],\n        \"ground_truth\": val_labels,\n    }\n\n    # training + detection\n    for epoch in range(1, epochs + 1):\n        model.train()\n        t_losses = []\n        for ids, mask, labels in tr_loader:\n            ids, mask, labels = (\n                ids.to(device),\n                mask.to(device),\n                labels.to(device).long(),\n            )\n            optimizer.zero_grad()  # Clear grads before backward\n            out = model(input_ids=ids, attention_mask=mask, labels=labels)\n            loss = out.loss\n            loss.backward()\n            optimizer.step()\n            t_losses.append(loss.item())\n        experiment_data[name][\"losses\"][\"train\"].append(\n            {\"epoch\": epoch, \"loss\": float(np.mean(t_losses))}\n        )\n\n        model.eval()\n        v_losses = []\n        with torch.no_grad():\n            for ids, mask, labels in va_loader:\n                ids, mask, labels = (\n                    ids.to(device),\n                    mask.to(device),\n                    labels.to(device).long(),\n                )\n                out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                v_losses.append(out.loss.item())\n        val_loss = float(np.mean(v_losses))\n        experiment_data[name][\"losses\"][\"val\"].append(\n            {\"epoch\": epoch, \"loss\": val_loss}\n        )\n        print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n        # detection via PIU\n        uncs, errs = [], []\n        for i, (t, gt) in enumerate(zip(val_texts, val_labels)):\n            preds = []\n            for txt in [t] + para[i]:\n                enc = tokenizer(\n                    txt, return_tensors=\"pt\", truncation=True, padding=True\n                ).to(device)\n                with torch.no_grad():\n                    logits = model(**enc).logits\n                preds.append(int(torch.argmax(logits, -1).item()))\n            maj = max(set(preds), key=preds.count)\n            uncs.append(1 - preds.count(maj) / len(preds))\n            errs.append(int(preds[0] != int(gt)))\n        try:\n            auc = roc_auc_score(errs, uncs)\n        except:\n            auc = 0.5\n        des = auc / (K + 1)\n        experiment_data[name][\"metrics\"][\"detection\"].append(\n            {\"epoch\": epoch, \"auc\": auc, \"DES\": des}\n        )\n        print(f\"Epoch {epoch}: detection_auc = {auc:.4f}, DES = {des:.4f}\")\n\n    # save final preds & labels\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for ids, mask, _ in va_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            logits = model(input_ids=ids, attention_mask=mask).logits\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n    experiment_data[name][\"predictions\"] = preds\n\n# persist data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 0.7542', '\\n', 'Epoch\n2: validation_loss = 0.7389', '\\n', 'Epoch 3: validation_loss = 0.7244', '\\n',\n'Epoch 4: validation_loss = 0.7103', '\\n', 'Epoch 5: validation_loss = 0.6965',\n'\\n', 'Epoch 6: validation_loss = 0.6837', '\\n', 'Epoch 7: validation_loss =\n0.6714', '\\n', 'Epoch 8: validation_loss = 0.6596', '\\n', 'Epoch 9:\nvalidation_loss = 0.6483', '\\n', 'Epoch 10: validation_loss = 0.6376', '\\n',\n'Epoch 11: validation_loss = 0.6269', '\\n', 'Epoch 12: validation_loss =\n0.6171', '\\n', 'Epoch 13: validation_loss = 0.6075', '\\n', 'Epoch 14:\nvalidation_loss = 0.5983', '\\n', 'Epoch 15: validation_loss = 0.5897', '\\n',\n'Epoch 16: validation_loss = 0.5813', '\\n', 'Epoch 17: validation_loss =\n0.5732', '\\n', 'Epoch 18: validation_loss = 0.5655', '\\n', 'Epoch 19:\nvalidation_loss = 0.5582', '\\n', 'Epoch 20: validation_loss = 0.5512', '\\n',\n'Epoch 1: validation_loss = 0.9828', '\\n', 'Epoch 2: validation_loss = 0.9754',\n'\\n', 'Epoch 3: validation_loss = 0.9681', '\\n', 'Epoch 4: validation_loss =\n0.9608', '\\n', 'Epoch 5: validation_loss = 0.9537', '\\n', 'Epoch 6:\nvalidation_loss = 0.9465', '\\n', 'Epoch 7: validation_loss = 0.9394', '\\n',\n'Epoch 8: validation_loss = 0.9325', '\\n', 'Epoch 9: validation_loss = 0.9254',\n'\\n', 'Epoch 10: validation_loss = 0.9185', '\\n', 'Epoch 11: validation_loss =\n0.9117', '\\n', 'Epoch 12: validation_loss = 0.9050', '\\n', 'Epoch 13:\nvalidation_loss = 0.8983', '\\n', 'Epoch 14: validation_loss = 0.8917', '\\n',\n'Epoch 15: validation_loss = 0.8852', '\\n', 'Epoch 16: validation_loss =\n0.8787', '\\n', 'Epoch 17: validation_loss = 0.8722', '\\n', 'Epoch 18:\nvalidation_loss = 0.8659', '\\n', 'Epoch 19: validation_loss = 0.8596', '\\n',\n'Epoch 20: validation_loss = 0.8534', '\\n', 'Epoch 1: validation_loss = 0.5080',\n'\\n', 'Epoch 2: validation_loss = 0.4577', '\\n', 'Epoch 3: validation_loss =\n0.4206', '\\n', 'Epoch 4: validation_loss = 0.3947', '\\n', 'Epoch 5:\nvalidation_loss = 0.3742', '\\n', 'Epoch 6: validation_loss = 0.3587', '\\n',\n'Epoch 7: validation_loss = 0.3470', '\\n', 'Epoch 8: validation_loss = 0.3362',\n'\\n', 'Epoch 9: validation_loss = 0.3274', '\\n', 'Epoch 10: validation_loss =\n0.3210', '\\n', 'Epoch 11: validation_loss = 0.3152', '\\n', 'Epoch 12:\nvalidation_loss = 0.3107', '\\n', 'Epoch 13: validation_loss = 0.3056', '\\n',\n'Epoch 14: validation_loss = 0.3017', '\\n', 'Epoch 15: validation_loss =\n0.2988', '\\n', 'Epoch 16: validation_loss = 0.2960', '\\n', 'Epoch 17:\nvalidation_loss = 0.2936', '\\n', 'Epoch 18: validation_loss = 0.2918', '\\n',\n'Epoch 19: validation_loss = 0.2902', '\\n', 'Epoch 20: validation_loss =\n0.2876', '\\n', 'Epoch 1: validation_loss = 0.9691', '\\n', 'Epoch 2:\nvalidation_loss = 0.8844', '\\n', 'Epoch 3: validation_loss = 0.8125', '\\n',\n'Epoch 4: validation_loss = 0.7523', '\\n', 'Epoch 5: validation_loss = 0.7016',\n'\\n', 'Epoch 6: validation_loss = 0.6591', '\\n', 'Epoch 7: validation_loss =\n0.6229', '\\n', 'Epoch 8: validation_loss = 0.5916', '\\n', 'Epoch 9:\nvalidation_loss = 0.5648', '\\n', 'Epoch 10: validation_loss = 0.5413', '\\n',\n'Epoch 11: validation_loss = 0.5216', '\\n', 'Epoch 12: validation_loss =\n0.5038', '\\n', 'Epoch 13: validation_loss = 0.4884', '\\n', 'Epoch 14:\nvalidation_loss = 0.4745', '\\n', 'Epoch 15: validation_loss = 0.4625', '\\n',\n'Epoch 16: validation_loss = 0.4517', '\\n', 'Epoch 17: validation_loss =\n0.4415', '\\n', 'Epoch 18: validation_loss = 0.4332', '\\n', 'Epoch 19:\nvalidation_loss = 0.4249', '\\n', 'Epoch 20: validation_loss = 0.4176', '\\n',\n'Epoch 1: validation_loss = 0.7077', '\\n', 'Epoch 2: validation_loss = 0.7050',\n'\\n', 'Epoch 3: validation_loss = 0.7026', '\\n', 'Epoch 4: validation_loss =\n0.7007', '\\n', 'Epoch 5: validation_loss = 0.6988', '\\n', 'Epoch 6:\nvalidation_loss = 0.6969', '\\n', 'Epoch 7: validation_loss = 0.6955', '\\n',\n'Epoch 8: validation_loss = 0.6942', '\\n', 'Epoch 9: validation_loss = 0.6932',\n'\\n', 'Epoch 10: validation_loss = 0.6922', '\\n', 'Epoch 11: validation_loss =\n0.6915', '\\n', 'Epoch 12: validation_loss = 0.6907', '\\n', 'Epoch 13:\nvalidation_loss = 0.6903', '\\n', 'Epoch 14: validation_loss = 0.6899', '\\n',\n'Epoch 15: validation_loss = 0.6895', '\\n', 'Epoch 16: validation_loss =\n0.6893', '\\n', 'Epoch 17: validation_loss = 0.6892', '\\n', 'Epoch 18:\nvalidation_loss = 0.6891', '\\n', 'Epoch 19: validation_loss = 0.6892', '\\n',\n'Epoch 20: validation_loss = 0.6892', '\\n', 'Epoch 1: validation_loss = 0.9919',\n'\\n', 'Epoch 2: validation_loss = 0.9812', '\\n', 'Epoch 3: validation_loss =\n0.9701', '\\n', 'Epoch 4: validation_loss = 0.9601', '\\n', 'Epoch 5:\nvalidation_loss = 0.9503', '\\n', 'Epoch 6: validation_loss = 0.9406', '\\n',\n'Epoch 7: validation_loss = 0.9316', '\\n', 'Epoch 8: validation_loss = 0.9218',\n'\\n', 'Epoch 9: validation_loss = 0.9122', '\\n', 'Epoch 10: validation_loss =\n0.9037', '\\n', 'Epoch 11: validation_loss = 0.8951', '\\n', 'Epoch 12:\nvalidation_loss = 0.8868', '\\n', 'Epoch 13: validation_loss = 0.8774', '\\n',\n'Epoch 14: validation_loss = 0.8705', '\\n', 'Epoch 15: validation_loss =\n0.8628', '\\n', 'Epoch 16: validation_loss = 0.8557', '\\n', 'Epoch 17:\nvalidation_loss = 0.8486', '\\n', 'Epoch 18: validation_loss = 0.8417', '\\n',\n'Epoch 19: validation_loss = 0.8352', '\\n', 'Epoch 20: validation_loss =\n0.8291', '\\n', 'Epoch 1: validation_loss = 0.8716', '\\n', 'Epoch 2:\nvalidation_loss = 0.7851', '\\n', 'Epoch 3: validation_loss = 0.7406', '\\n',\n'Epoch 4: validation_loss = 0.7185', '\\n', 'Epoch 5: validation_loss = 0.7102',\n'\\n', 'Epoch 6: validation_loss = 0.7078', '\\n', 'Epoch 7: validation_loss =\n0.7007', '\\n', 'Epoch 8: validation_loss = 0.6995', '\\n', 'Epoch 9:\nvalidation_loss = 0.7034', '\\n', 'Epoch 10: validation_loss = 0.7010', '\\n',\n'Epoch 11: validation_loss = 0.7020', '\\n', 'Epoch 12: validation_loss =\n0.7022', '\\n', 'Epoch 13: validation_loss = 0.7027', '\\n', 'Epoch 14:\nvalidation_loss = 0.7023', '\\n', 'Epoch 15: validation_loss = 0.7019', '\\n',\n'Epoch 16: validation_loss = 0.7018', '\\n', 'Epoch 17: validation_loss =\n0.7037', '\\n', 'Epoch 18: validation_loss = 0.7016', '\\n', 'Epoch 19:\nvalidation_loss = 0.7006', '\\n', 'Epoch 20: validation_loss = 0.7020', '\\n',\n'Epoch 1: validation_loss = 0.7460', '\\n', 'Epoch 2: validation_loss = 0.7246',\n'\\n', 'Epoch 3: validation_loss = 0.7164', '\\n', 'Epoch 4: validation_loss =\n0.7069', '\\n', 'Epoch 5: validation_loss = 0.7048', '\\n', 'Epoch 6:\nvalidation_loss = 0.6996', '\\n', 'Epoch 7: validation_loss = 0.6991', '\\n',\n'Epoch 8: validation_loss = 0.6992', '\\n', 'Epoch 9: validation_loss = 0.6973',\n'\\n', 'Epoch 10: validation_loss = 0.6990', '\\n', 'Epoch 11: validation_loss =\n0.6974', '\\n', 'Epoch 12: validation_loss = 0.6983', '\\n', 'Epoch 13:\nvalidation_loss = 0.6981', '\\n', 'Epoch 14: validation_loss = 0.6993', '\\n',\n'Epoch 15: validation_loss = 0.6992', '\\n', 'Epoch 16: validation_loss =\n0.6979', '\\n', 'Epoch 17: validation_loss = 0.6977', '\\n', 'Epoch 18:\nvalidation_loss = 0.6979', '\\n', 'Epoch 19: validation_loss = 0.6995', '\\n',\n'Epoch 20: validation_loss = 0.6980', '\\n', 'Epoch 1: validation_loss = 0.7862',\n'\\n', 'Epoch 2: validation_loss = 0.7786', '\\n', 'Epoch 3: validation_loss =\n0.7710', '\\n', 'Epoch 4: validation_loss = 0.7643', '\\n', 'Epoch 5:\nvalidation_loss = 0.7579', '\\n', 'Epoch 6: validation_loss = 0.7520', '\\n',\n'Epoch 7: validation_loss = 0.7464', '\\n', 'Epoch 8: validation_loss = 0.7409',\n'\\n', 'Epoch 9: validation_loss = 0.7361', '\\n', 'Epoch 10: validation_loss =\n0.7318', '\\n', 'Epoch 11: validation_loss = 0.7276', '\\n', 'Epoch 12:\nvalidation_loss = 0.7238', '\\n', 'Epoch 13: validation_loss = 0.7205', '\\n',\n'Epoch 14: validation_loss = 0.7176', '\\n', 'Epoch 15: validation_loss =\n0.7146', '\\n', 'Epoch 16: validation_loss = 0.7120', '\\n', 'Epoch 17:\nvalidation_loss = 0.7098', '\\n', 'Epoch 18: validation_loss = 0.7077', '\\n',\n'Epoch 19: validation_loss = 0.7061', '\\n', 'Epoch 20: validation_loss =\n0.7043', '\\n', 'Epoch 1: validation_loss = 0.8289', '\\n', 'Epoch 2:\nvalidation_loss = 0.8250', '\\n', 'Epoch 3: validation_loss = 0.8212', '\\n',\n'Epoch 4: validation_loss = 0.8174', '\\n', 'Epoch 5: validation_loss = 0.8137',\n'\\n', 'Epoch 6: validation_loss = 0.8102', '\\n', 'Epoch 7: validation_loss =\n0.8065', '\\n', 'Epoch 8: validation_loss = 0.8029', '\\n', 'Epoch 9:\nvalidation_loss = 0.7995', '\\n', 'Epoch 10: validation_loss = 0.7960', '\\n',\n'Epoch 11: validation_loss = 0.7928', '\\n', 'Epoch 12: validation_loss =\n0.7896', '\\n', 'Epoch 13: validation_loss = 0.7863', '\\n', 'Epoch 14:\nvalidation_loss = 0.7831', '\\n', 'Epoch 15: validation_loss = 0.7802', '\\n',\n'Epoch 16: validation_loss = 0.7773', '\\n', 'Epoch 17: validation_loss =\n0.7743', '\\n', 'Epoch 18: validation_loss = 0.7713', '\\n', 'Epoch 19:\nvalidation_loss = 0.7685', '\\n', 'Epoch 20: validation_loss = 0.7660', '\\n',\n'Epoch 1: validation_loss = 0.6983', '\\n', 'Epoch 2: validation_loss = 0.7002',\n'\\n', 'Epoch 3: validation_loss = 0.6976', '\\n', 'Epoch 4: validation_loss =\n0.6999', '\\n', 'Epoch 5: validation_loss = 0.6992', '\\n', 'Epoch 6:\nvalidation_loss = 0.6986', '\\n', 'Epoch 7: validation_loss = 0.6983', '\\n',\n'Epoch 8: validation_loss = 0.7008', '\\n', 'Epoch 9: validation_loss = 0.7016',\n'\\n', 'Epoch 10: validation_loss = 0.6992', '\\n', 'Epoch 11: validation_loss =\n0.7008', '\\n', 'Epoch 12: validation_loss = 0.6972', '\\n', 'Epoch 13:\nvalidation_loss = 0.6987', '\\n', 'Epoch 14: validation_loss = 0.6980', '\\n',\n'Epoch 15: validation_loss = 0.6987', '\\n', 'Epoch 16: validation_loss =\n0.6984', '\\n', 'Epoch 17: validation_loss = 0.6984', '\\n', 'Epoch 18:\nvalidation_loss = 0.7012', '\\n', 'Epoch 19: validation_loss = 0.7000', '\\n',\n'Epoch 20: validation_loss = 0.6992', '\\n', 'Epoch 1: validation_loss = 0.7732',\n'\\n', 'Epoch 2: validation_loss = 0.7464', '\\n', 'Epoch 3: validation_loss =\n0.7239', '\\n', 'Epoch 4: validation_loss = 0.7088', '\\n', 'Epoch 5:\nvalidation_loss = 0.6992', '\\n', 'Epoch 6: validation_loss = 0.6939', '\\n',\n'Epoch 7: validation_loss = 0.6924', '\\n', 'Epoch 8: validation_loss = 0.6927',\n'\\n', 'Epoch 9: validation_loss = 0.6939', '\\n', 'Epoch 10: validation_loss =\n0.6955', '\\n', 'Epoch 11: validation_loss = 0.6972', '\\n', 'Epoch 12:\nvalidation_loss = 0.6986', '\\n', 'Epoch 13: validation_loss = 0.6998', '\\n',\n'Epoch 14: validation_loss = 0.7003', '\\n', 'Epoch 15: validation_loss =\n0.7012', '\\n', 'Epoch 16: validation_loss = 0.7022', '\\n', 'Epoch 17:\nvalidation_loss = 0.7028', '\\n', 'Epoch 18: validation_loss = 0.7021', '\\n',\n'Epoch 19: validation_loss = 0.7023', '\\n', 'Epoch 20: validation_loss =\n0.7025', '\\n', 'Execution time: 38 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\\n\nresponse.raise_for_status()\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/requests/models.py\", line 1024, in raise_for_status\\n    raise\nHTTPError(http_error_msg, response=self)\\nrequests.exceptions.HTTPError: 404\nClient Error: Not Found for url: https://huggingface.co/ramsrigouthamg/t5_paraph\nraser_small/resolve/main/config.json\\n\\nThe above exception was the direct cause\nof the following exception:\\n\\nTraceback (most recent call last):\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/utils/hub.py\", line 470, in cached_files\\n\nhf_hub_download(\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\\n\nreturn fn(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/huggingface_hub/file_download.py\", line 1008, in hf_hub_download\\n\nreturn _hf_hub_download_to_cache_dir(\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/huggingface_hub/file_download.py\", line 1115, in\n_hf_hub_download_to_cache_dir\\n    _raise_on_head_call_error(head_call_error,\nforce_download, local_files_only)\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/huggingface_hub/file_download.py\", line 1645, in\n_raise_on_head_call_error\\n    raise head_call_error\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/huggingface_hub/file_download.py\", line 1533, in\n_get_metadata_or_catch_error\\n    metadata = get_hf_file_metadata(\\n\n^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\\n\nreturn fn(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/huggingface_hub/file_download.py\", line 1450, in get_hf_file_metadata\\n\nr = _request_wrapper(\\n        ^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/huggingface_hub/file_download.py\", line 286, in _request_wrapper\\n\nresponse = _request_wrapper(\\n               ^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/huggingface_hub/file_download.py\", line 310, in _request_wrapper\\n\nhf_raise_for_status(response)\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/huggingface_hub/utils/_http.py\", line 459, in hf_raise_for_status\\n\nraise _format(RepositoryNotFoundError, message, response) from\ne\\nhuggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request\nID: Root=1-6844555b-6eadade9033bac747e806bc6;d10e7620-7752-48a3-bb7f-\n74347fdf5ae6)\\n\\nRepository Not Found for url: https://huggingface.co/ramsrigout\nhamg/t5_paraphraser_small/resolve/main/config.json.\\nPlease make sure you\nspecified the correct `repo_id` and `repo_type`.\\nIf you are trying to access a\nprivate or gated repo, make sure you are authenticated. For more details, see\nhttps://huggingface.co/docs/huggingface_hub/authentication\\n\\nThe above\nexception was the direct cause of the following exception:\\n\\nTraceback (most\nrecent call last):\\n  File \"runfile.py\", line 16, in <module>\\n    paraphraser =\npipeline(\\n                  ^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/pipelines/__init__.py\", line 812, in pipeline\\n\nresolved_config_file = cached_file(\\n                           ^^^^^^^^^^^^\\n\nFile \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/utils/hub.py\", line 312, in cached_file\\n    file =\ncached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\nFile \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/utils/hub.py\", line 502, in cached_files\\n    raise\nOSError(\\nOSError: ramsrigouthamg/t5_paraphraser_small is not a local folder and\nis not a valid model identifier listed on \\'https://huggingface.co/models\\'\\nIf\nthis is a private repository, make sure to pass a token having permission to\nthis repo either by logging in with `huggingface-cli login` or by passing\n`token=<your_token>`\\n', 'Execution time: 3 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/tiktoken/load.py\", line 11, in read_file\\n    import\nblobfile\\nModuleNotFoundError: No module named \\'blobfile\\'\\n\\nThe above\nexception was the direct cause of the following exception:\\n\\nTraceback (most\nrecent call last):\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/convert_slow_tokenizer.py\", line 1737, in\nconvert_slow_tokenizer\\n    ).converted()\\n      ^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/convert_slow_tokenizer.py\", line 1631, in converted\\n\ntokenizer = self.tokenizer()\\n                ^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/convert_slow_tokenizer.py\", line 1624, in tokenizer\\n\nvocab_scores, merges = self.extract_vocab_merges_from_model(self.vocab_file)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/convert_slow_tokenizer.py\", line 1600, in\nextract_vocab_merges_from_model\\n    bpe_ranks =\nload_tiktoken_bpe(tiktoken_url)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/tiktoken/load.py\", line 148, in load_tiktoken_bpe\\n    contents =\nread_file_cached(tiktoken_bpe_file, expected_hash)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/tiktoken/load.py\", line 63, in read_file_cached\\n    contents =\nread_file(blobpath)\\n               ^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/tiktoken/load.py\", line 13, in read_file\\n    raise\nImportError(\\nImportError: blobfile is not installed. Please install it by\nrunning `pip install blobfile`.\\n\\nDuring handling of the above exception,\nanother exception occurred:\\n\\nTraceback (most recent call last):\\n  File\n\"runfile.py\", line 16, in <module>\\n    paraphrase_pipe = pipeline(\\n\n^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/pipelines/__init__.py\", line 1049, in pipeline\\n\ntokenizer = AutoTokenizer.from_pretrained(\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/models/auto/tokenization_auto.py\", line 1032, in\nfrom_pretrained\\n    return\ntokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs,\n**kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/tokenization_utils_base.py\", line 2025, in\nfrom_pretrained\\n    return cls._from_pretrained(\\n\n^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/tokenization_utils_base.py\", line 2278, in\n_from_pretrained\\n    tokenizer = cls(*init_inputs, **init_kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/models/t5/tokenization_t5_fast.py\", line 119, in\n__init__\\n    super().__init__(\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/tokenization_utils_fast.py\", line 139, in __init__\\n\nfast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/convert_slow_tokenizer.py\", line 1739, in\nconvert_slow_tokenizer\\n    raise ValueError(\\nValueError: Converting from\nSentencePiece and Tiktoken failed, if a converter for SentencePiece is\navailable, provide a model path with a SentencePiece tokenizer.model\nfile.Currently available slow->fast converters: [\\'AlbertTokenizer\\',\n\\'BartTokenizer\\', \\'BarthezTokenizer\\', \\'BertTokenizer\\',\n\\'BigBirdTokenizer\\', \\'BlenderbotTokenizer\\', \\'CamembertTokenizer\\',\n\\'CLIPTokenizer\\', \\'CodeGenTokenizer\\', \\'ConvBertTokenizer\\',\n\\'DebertaTokenizer\\', \\'DebertaV2Tokenizer\\', \\'DistilBertTokenizer\\',\n\\'DPRReaderTokenizer\\', \\'DPRQuestionEncoderTokenizer\\',\n\\'DPRContextEncoderTokenizer\\', \\'ElectraTokenizer\\', \\'FNetTokenizer\\',\n\\'FunnelTokenizer\\', \\'GPT2Tokenizer\\', \\'HerbertTokenizer\\',\n\\'LayoutLMTokenizer\\', \\'LayoutLMv2Tokenizer\\', \\'LayoutLMv3Tokenizer\\',\n\\'LayoutXLMTokenizer\\', \\'LongformerTokenizer\\', \\'LEDTokenizer\\',\n\\'LxmertTokenizer\\', \\'MarkupLMTokenizer\\', \\'MBartTokenizer\\',\n\\'MBart50Tokenizer\\', \\'MPNetTokenizer\\', \\'MobileBertTokenizer\\',\n\\'MvpTokenizer\\', \\'NllbTokenizer\\', \\'OpenAIGPTTokenizer\\',\n\\'PegasusTokenizer\\', \\'Qwen2Tokenizer\\', \\'RealmTokenizer\\',\n\\'ReformerTokenizer\\', \\'RemBertTokenizer\\', \\'RetriBertTokenizer\\',\n\\'RobertaTokenizer\\', \\'RoFormerTokenizer\\', \\'SeamlessM4TTokenizer\\',\n\\'SqueezeBertTokenizer\\', \\'T5Tokenizer\\', \\'UdopTokenizer\\',\n\\'WhisperTokenizer\\', \\'XLMRobertaTokenizer\\', \\'XLNetTokenizer\\',\n\\'SplinterTokenizer\\', \\'XGLMTokenizer\\', \\'LlamaTokenizer\\',\n\\'CodeLlamaTokenizer\\', \\'GemmaTokenizer\\', \\'Phi3Tokenizer\\']\\n', 'Execution\ntime: 3 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 114, in <module>\\n    logits = model(xb).squeeze(1)\\n\n^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/container.py\", line 250, in forward\\n    input =\nmodule(input)\\n            ^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/linear.py\", line 125, in forward\\n    return\nF.linear(input, self.weight, self.bias)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: mat1 and mat2 shapes\ncannot be multiplied (32x1 and 3x16)\\n', 'Execution time: 43 seconds seconds\n(time limit is an hour).']", "['Using device: cuda', '\\n', \"Some weights of\nDistilBertForSequenceClassification were not initialized from the model\ncheckpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.6980', '\\n', 'Epoch 1: detection_auc = 0.5080, DES =\n0.1270', '\\n', 'Epoch 2: validation_loss = 0.6980', '\\n', 'Epoch 2:\ndetection_auc = 0.5080, DES = 0.1270', '\\n', 'Epoch 3: validation_loss =\n0.6980', '\\n', 'Epoch 3: detection_auc = 0.5080, DES = 0.1270', '\\n', \"Some\nweights of DistilBertForSequenceClassification were not initialized from the\nmodel checkpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.6980', '\\n', 'Epoch 1: detection_auc = 0.5000, DES =\n0.1250', '\\n', 'Epoch 2: validation_loss = 0.6980', '\\n', 'Epoch 2:\ndetection_auc = 0.5000, DES = 0.1250', '\\n', 'Epoch 3: validation_loss =\n0.6980', '\\n', 'Epoch 3: detection_auc = 0.5000, DES = 0.1250', '\\n', \"Some\nweights of DistilBertForSequenceClassification were not initialized from the\nmodel checkpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.6988', '\\n', 'Epoch 1: detection_auc = 0.5000, DES =\n0.1250', '\\n', 'Epoch 2: validation_loss = 0.6988', '\\n', 'Epoch 2:\ndetection_auc = 0.5000, DES = 0.1250', '\\n', 'Epoch 3: validation_loss =\n0.6988', '\\n', 'Epoch 3: detection_auc = 0.5000, DES = 0.1250', '\\n', 'Execution\ntime: 2 minutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 0.0080', '\\n', 'sst2\ndetection AUC = 0.9898, DES = 0.1980', '\\n', 'Token indices sequence length is\nlonger than the specified maximum sequence length for this model (639 > 512).\nRunning this sequence through the model will result in indexing errors\\n',\n'Epoch 1: validation_loss = 0.0080', '\\n', 'yelp_polarity detection AUC =\n0.9898, DES = 0.1980', '\\n', 'Epoch 1: validation_loss = 0.0160', '\\n', 'imdb\ndetection AUC = 0.7188, DES = 0.1437', '\\n', 'Execution time: a minute seconds\n(time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 0.5673', '\\n', 'Epoch\n2: validation_loss = 0.5236', '\\n', 'Epoch 3: validation_loss = 0.4891', '\\n',\n'Epoch 4: validation_loss = 0.4618', '\\n', 'Epoch 5: validation_loss = 0.4413',\n'\\n', 'Epoch 6: validation_loss = 0.4266', '\\n', 'Epoch 7: validation_loss =\n0.4175', '\\n', 'Epoch 8: validation_loss = 0.4120', '\\n', 'Epoch 9:\nvalidation_loss = 0.4095', '\\n', 'Epoch 10: validation_loss = 0.4089', '\\n',\n'Epoch 1: validation_loss = 0.6408', '\\n', 'Epoch 2: validation_loss = 0.6142',\n'\\n', 'Epoch 3: validation_loss = 0.5918', '\\n', 'Epoch 4: validation_loss =\n0.5740', '\\n', 'Epoch 5: validation_loss = 0.5604', '\\n', 'Epoch 6:\nvalidation_loss = 0.5508', '\\n', 'Epoch 7: validation_loss = 0.5442', '\\n',\n'Epoch 8: validation_loss = 0.5403', '\\n', 'Epoch 9: validation_loss = 0.5386',\n'\\n', 'Epoch 10: validation_loss = 0.5381', '\\n', 'Epoch 1: validation_loss =\n0.4062', '\\n', 'Epoch 2: validation_loss = 0.2902', '\\n', 'Epoch 3:\nvalidation_loss = 0.2714', '\\n', 'Epoch 4: validation_loss = 0.2751', '\\n',\n'Epoch 5: validation_loss = 0.2678', '\\n', 'Epoch 6: validation_loss = 0.2691',\n'\\n', 'Epoch 7: validation_loss = 0.2681', '\\n', 'Epoch 8: validation_loss =\n0.2678', '\\n', 'Epoch 9: validation_loss = 0.2676', '\\n', 'Epoch 10:\nvalidation_loss = 0.2678', '\\n', 'Epoch 1: validation_loss = 0.5366', '\\n',\n'Epoch 2: validation_loss = 0.4186', '\\n', 'Epoch 3: validation_loss = 0.3574',\n'\\n', 'Epoch 4: validation_loss = 0.3242', '\\n', 'Epoch 5: validation_loss =\n0.3114', '\\n', 'Epoch 6: validation_loss = 0.3034', '\\n', 'Epoch 7:\nvalidation_loss = 0.3010', '\\n', 'Epoch 8: validation_loss = 0.2988', '\\n',\n'Epoch 9: validation_loss = 0.2994', '\\n', 'Epoch 10: validation_loss = 0.2993',\n'\\n', 'Epoch 1: validation_loss = 0.7286', '\\n', 'Epoch 2: validation_loss =\n0.7193', '\\n', 'Epoch 3: validation_loss = 0.7139', '\\n', 'Epoch 4:\nvalidation_loss = 0.7120', '\\n', 'Epoch 5: validation_loss = 0.7100', '\\n',\n'Epoch 6: validation_loss = 0.7089', '\\n', 'Epoch 7: validation_loss = 0.7083',\n'\\n', 'Epoch 8: validation_loss = 0.7079', '\\n', 'Epoch 9: validation_loss =\n0.7077', '\\n', 'Epoch 10: validation_loss = 0.7076', '\\n', 'Epoch 1:\nvalidation_loss = 0.7341', '\\n', 'Epoch 2: validation_loss = 0.7198', '\\n',\n'Epoch 3: validation_loss = 0.7115', '\\n', 'Epoch 4: validation_loss = 0.7064',\n'\\n', 'Epoch 5: validation_loss = 0.7036', '\\n', 'Epoch 6: validation_loss =\n0.7025', '\\n', 'Epoch 7: validation_loss = 0.7017', '\\n', 'Epoch 8:\nvalidation_loss = 0.7013', '\\n', 'Epoch 9: validation_loss = 0.7011', '\\n',\n'Epoch 10: validation_loss = 0.7011', '\\n', 'Epoch 1: validation_loss = 0.6993',\n'\\n', 'Epoch 2: validation_loss = 0.7054', '\\n', 'Epoch 3: validation_loss =\n0.7057', '\\n', 'Epoch 4: validation_loss = 0.7026', '\\n', 'Epoch 5:\nvalidation_loss = 0.7063', '\\n', 'Epoch 6: validation_loss = 0.7033', '\\n',\n'Epoch 7: validation_loss = 0.7071', '\\n', 'Epoch 8: validation_loss = 0.7073',\n'\\n', 'Epoch 9: validation_loss = 0.7065', '\\n', 'Epoch 10: validation_loss =\n0.7063', '\\n', 'Epoch 1: validation_loss = 0.6956', '\\n', 'Epoch 2:\nvalidation_loss = 0.6977', '\\n', 'Epoch 3: validation_loss = 0.6995', '\\n',\n'Epoch 4: validation_loss = 0.7012', '\\n', 'Epoch 5: validation_loss = 0.7041',\n'\\n', 'Epoch 6: validation_loss = 0.7033', '\\n', 'Epoch 7: validation_loss =\n0.7039', '\\n', 'Epoch 8: validation_loss = 0.7044', '\\n', 'Epoch 9:\nvalidation_loss = 0.7044', '\\n', 'Epoch 10: validation_loss = 0.7044', '\\n',\n'Epoch 1: validation_loss = 0.6971', '\\n', 'Epoch 2: validation_loss = 0.6913',\n'\\n', 'Epoch 3: validation_loss = 0.6879', '\\n', 'Epoch 4: validation_loss =\n0.6855', '\\n', 'Epoch 5: validation_loss = 0.6842', '\\n', 'Epoch 6:\nvalidation_loss = 0.6838', '\\n', 'Epoch 7: validation_loss = 0.6835', '\\n',\n'Epoch 8: validation_loss = 0.6834', '\\n', 'Epoch 9: validation_loss = 0.6834',\n'\\n', 'Epoch 10: validation_loss = 0.6834', '\\n', 'Epoch 1: validation_loss =\n0.6979', '\\n', 'Epoch 2: validation_loss = 0.6951', '\\n', 'Epoch 3:\nvalidation_loss = 0.6936', '\\n', 'Epoch 4: validation_loss = 0.6928', '\\n',\n'Epoch 5: validation_loss = 0.6925', '\\n', 'Epoch 6: validation_loss = 0.6924',\n'\\n', 'Epoch 7: validation_loss = 0.6924', '\\n', 'Epoch 8: validation_loss =\n0.6924', '\\n', 'Epoch 9: validation_loss = 0.6924', '\\n', 'Epoch 10:\nvalidation_loss = 0.6924', '\\n', 'Epoch 1: validation_loss = 0.6869', '\\n',\n'Epoch 2: validation_loss = 0.6888', '\\n', 'Epoch 3: validation_loss = 0.6866',\n'\\n', 'Epoch 4: validation_loss = 0.6866', '\\n', 'Epoch 5: validation_loss =\n0.6893', '\\n', 'Epoch 6: validation_loss = 0.6892', '\\n', 'Epoch 7:\nvalidation_loss = 0.6915', '\\n', 'Epoch 8: validation_loss = 0.6881', '\\n',\n'Epoch 9: validation_loss = 0.6881', '\\n', 'Epoch 10: validation_loss = 0.6882',\n'\\n', 'Epoch 1: validation_loss = 0.6972', '\\n', 'Epoch 2: validation_loss =\n0.7016', '\\n', 'Epoch 3: validation_loss = 0.7016', '\\n', 'Epoch 4:\nvalidation_loss = 0.6981', '\\n', 'Epoch 5: validation_loss = 0.6974', '\\n',\n'Epoch 6: validation_loss = 0.6981', '\\n', 'Epoch 7: validation_loss = 0.6982',\n'\\n', 'Epoch 8: validation_loss = 0.6983', '\\n', 'Epoch 9: validation_loss =\n0.6981', '\\n', 'Epoch 10: validation_loss = 0.6982', '\\n', 'Epoch 1:\nvalidation_loss = 0.6950', '\\n', 'Epoch 2: validation_loss = 0.6937', '\\n',\n'Epoch 3: validation_loss = 0.6936', '\\n', 'Epoch 4: validation_loss = 0.6938',\n'\\n', 'Epoch 5: validation_loss = 0.6940', '\\n', 'Epoch 6: validation_loss =\n0.6939', '\\n', 'Epoch 7: validation_loss = 0.6941', '\\n', 'Epoch 8:\nvalidation_loss = 0.6941', '\\n', 'Epoch 9: validation_loss = 0.6941', '\\n',\n'Epoch 10: validation_loss = 0.6942', '\\n', 'Epoch 1: validation_loss = 0.6947',\n'\\n', 'Epoch 2: validation_loss = 0.6983', '\\n', 'Epoch 3: validation_loss =\n0.7017', '\\n', 'Epoch 4: validation_loss = 0.7039', '\\n', 'Epoch 5:\nvalidation_loss = 0.7048', '\\n', 'Epoch 6: validation_loss = 0.7057', '\\n',\n'Epoch 7: validation_loss = 0.7067', '\\n', 'Epoch 8: validation_loss = 0.7068',\n'\\n', 'Epoch 9: validation_loss = 0.7070', '\\n', 'Epoch 10: validation_loss =\n0.7070', '\\n', 'Epoch 1: validation_loss = 0.7093', '\\n', 'Epoch 2:\nvalidation_loss = 0.6949', '\\n', 'Epoch 3: validation_loss = 0.6952', '\\n',\n'Epoch 4: validation_loss = 0.6926', '\\n', 'Epoch 5: validation_loss = 0.6965',\n'\\n', 'Epoch 6: validation_loss = 0.6924', '\\n', 'Epoch 7: validation_loss =\n0.6945', '\\n', 'Epoch 8: validation_loss = 0.6924', '\\n', 'Epoch 9:\nvalidation_loss = 0.6931', '\\n', 'Epoch 10: validation_loss = 0.6928', '\\n',\n'Epoch 1: validation_loss = 0.7140', '\\n', 'Epoch 2: validation_loss = 0.7056',\n'\\n', 'Epoch 3: validation_loss = 0.7050', '\\n', 'Epoch 4: validation_loss =\n0.7036', '\\n', 'Epoch 5: validation_loss = 0.7052', '\\n', 'Epoch 6:\nvalidation_loss = 0.7036', '\\n', 'Epoch 7: validation_loss = 0.7021', '\\n',\n'Epoch 8: validation_loss = 0.7024', '\\n', 'Epoch 9: validation_loss = 0.7025',\n'\\n', 'Epoch 10: validation_loss = 0.7026', '\\n', 'Execution time: 50 seconds\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 26, in <module>\\n    tokenizer_para =\nPegasusTokenizer.from_pretrained(\"tuner007/pegasus_paraphrase\")\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/utils/import_utils.py\", line 1885, in __getattribute__\\n\nrequires_backends(cls, cls._backends)\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/utils/import_utils.py\", line 1871, in requires_backends\\n\nraise ImportError(\"\".join(failed))\\nImportError: \\nPegasusTokenizer requires the\nSentencePiece library but it was not found in your environment. Checkout the\ninstructions on the\\ninstallation page of its repo:\nhttps://github.com/google/sentencepiece#installation and follow the ones\\nthat\nmatch your environment. Please note that you may need to restart your runtime\nafter installation.\\n\\n', 'Execution time: 2 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Device set to use cuda:0\\n', '\\rconfig.json:   0%|\n| 0.00/1.33k [00:00<?, ?B/s]', '', '\\rconfig.json: 100%|##########| 1.33k/1.33k\n[00:00<00:00, 5.01MB/s]', '\\n', '\\rpytorch_model.bin:   0%|          | 0.00/298M\n[00:00<?, ?B/s]', '\\rpytorch_model.bin:   4%|3         | 10.5M/298M\n[00:00<00:03, 74.9MB/s]', '\\rpytorch_model.bin:  11%|#         | 31.5M/298M\n[00:00<00:02, 97.2MB/s]', '\\rpytorch_model.bin:  18%|#7        | 52.4M/298M\n[00:00<00:02, 105MB/s] ', '\\rpytorch_model.bin:  21%|##1       | 62.9M/298M\n[00:00<00:02, 104MB/s]', '\\rpytorch_model.bin:  28%|##8       | 83.9M/298M\n[00:00<00:01, 109MB/s]', '\\rpytorch_model.bin:  35%|###5      | 105M/298M\n[00:00<00:01, 110MB/s] ', '\\rpytorch_model.bin:  42%|####2     | 126M/298M\n[00:01<00:01, 111MB/s]', '\\rpytorch_model.bin:  49%|####9     | 147M/298M\n[00:01<00:01, 111MB/s]', '\\rpytorch_model.bin:  56%|#####6    | 168M/298M\n[00:01<00:01, 112MB/s]', '\\rpytorch_model.bin:  63%|######3   | 189M/298M\n[00:01<00:00, 113MB/s]', '\\rpytorch_model.bin:  70%|#######   | 210M/298M\n[00:01<00:00, 113MB/s]', '\\rpytorch_model.bin:  77%|#######7  | 231M/298M\n[00:02<00:00, 113MB/s]', '\\rpytorch_model.bin:  84%|########4 | 252M/298M\n[00:02<00:00, 112MB/s]', '\\rpytorch_model.bin:  92%|#########1| 273M/298M\n[00:02<00:00, 113MB/s]', '\\rpytorch_model.bin:  99%|#########8| 294M/298M\n[00:02<00:00, 115MB/s]', '', '\\rpytorch_model.bin: 100%|##########| 298M/298M\n[00:02<00:00, 111MB/s]', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 20, in <module>\\n    trans_en_de = pipeline(\\n\n^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/pipelines/__init__.py\", line 942, in pipeline\\n\nframework, model = infer_framework_load_model(\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/pipelines/base.py\", line 305, in\ninfer_framework_load_model\\n    raise ValueError(\\nValueError: Could not load\nmodel Helsinki-NLP/opus-mt-en-de with any of the following classes: (<class\n\\'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM\\'>, <class\n\\'transformers.models.marian.modeling_marian.MarianMTModel\\'>). See the original\nerrors:\\n\\nwhile loading with AutoModelForSeq2SeqLM, an error is\nthrown:\\nTraceback (most recent call last):\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/pipelines/base.py\", line 292, in\ninfer_framework_load_model\\n    model = model_class.from_pretrained(model,\n**kwargs)\\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/models/auto/auto_factory.py\", line 571, in\nfrom_pretrained\\n    return model_class.from_pretrained(\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/modeling_utils.py\", line 309, in _wrapper\\n    return\nfunc(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/modeling_utils.py\", line 4573, in from_pretrained\\n    ) =\ncls._load_pretrained_model(\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/modeling_utils.py\", line 4832, in _load_pretrained_model\\n\nload_state_dict(checkpoint_files[0], map_location=\"meta\",\nweights_only=weights_only).keys()\\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/modeling_utils.py\", line 553, in load_state_dict\\n\ncheck_torch_load_is_safe()\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/utils/import_utils.py\", line 1417, in\ncheck_torch_load_is_safe\\n    raise ValueError(\\nValueError: Due to a serious\nvulnerability issue in `torch.load`, even with `weights_only=True`, we now\nrequire users to upgrade torch to at least v2.6 in order to use the function.\nThis version restriction does not apply when loading files with\nsafetensors.\\nSee the vulnerability report here\nhttps://nvd.nist.gov/vuln/detail/CVE-2025-32434\\n\\nwhile loading with\nMarianMTModel, an error is thrown:\\nTraceback (most recent call last):\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/pipelines/base.py\", line 292, in\ninfer_framework_load_model\\n    model = model_class.from_pretrained(model,\n**kwargs)\\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/modeling_utils.py\", line 309, in _wrapper\\n    return\nfunc(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/modeling_utils.py\", line 4573, in from_pretrained\\n    ) =\ncls._load_pretrained_model(\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/modeling_utils.py\", line 4832, in _load_pretrained_model\\n\nload_state_dict(checkpoint_files[0], map_location=\"meta\",\nweights_only=weights_only).keys()\\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/modeling_utils.py\", line 553, in load_state_dict\\n\ncheck_torch_load_is_safe()\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/utils/import_utils.py\", line 1417, in\ncheck_torch_load_is_safe\\n    raise ValueError(\\nValueError: Due to a serious\nvulnerability issue in `torch.load`, even with `weights_only=True`, we now\nrequire users to upgrade torch to at least v2.6 in order to use the function.\nThis version restriction does not apply when loading files with\nsafetensors.\\nSee the vulnerability report here\nhttps://nvd.nist.gov/vuln/detail/CVE-2025-32434\\n\\n\\n\\n', 'Execution time: 12\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', \"Some weights of\nDistilBertForSequenceClassification were not initialized from the model\ncheckpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.3759', '\\n', 'Epoch 1: detection_auc = 0.5068, DES =\n0.1267', '\\n', 'Epoch 2: validation_loss = 0.3352', '\\n', 'Epoch 2:\ndetection_auc = 0.6217, DES = 0.1554', '\\n', 'Epoch 3: validation_loss =\n0.4326', '\\n', 'Epoch 3: detection_auc = 0.6332, DES = 0.1583', '\\n', \"Some\nweights of DistilBertForSequenceClassification were not initialized from the\nmodel checkpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.2447', '\\n', 'Epoch 1: detection_auc = 0.4809, DES =\n0.1202', '\\n', 'Epoch 2: validation_loss = 0.2364', '\\n', 'Epoch 2:\ndetection_auc = 0.5443, DES = 0.1361', '\\n', 'Epoch 3: validation_loss =\n0.3306', '\\n', 'Epoch 3: detection_auc = 0.5486, DES = 0.1372', '\\n', \"Some\nweights of DistilBertForSequenceClassification were not initialized from the\nmodel checkpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.4082', '\\n', 'Epoch 1: detection_auc = 0.5177, DES =\n0.1294', '\\n', 'Epoch 2: validation_loss = 0.3133', '\\n', 'Epoch 2:\ndetection_auc = 0.4829, DES = 0.1207', '\\n', 'Epoch 3: validation_loss =\n0.4118', '\\n', 'Epoch 3: detection_auc = 0.5090, DES = 0.1273', '\\n', 'Execution\ntime: 2 minutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', \"Some weights of BertForSequenceClassification were\nnot initialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'Epoch 1: validation_loss = 0.4001', '\\n', 'Epoch 1:\nAUC_vote=0.6284, DES_vote=0.1047, AUC_kl=0.7433, DES_kl=0.1239', '\\n', 'Epoch 2:\nvalidation_loss = 0.2918', '\\n', 'Epoch 2: AUC_vote=0.6761, DES_vote=0.1127,\nAUC_kl=0.7513, DES_kl=0.1252', '\\n', 'Epoch 3: validation_loss = 0.3836', '\\n',\n'Epoch 3: AUC_vote=0.6435, DES_vote=0.1073, AUC_kl=0.7406, DES_kl=0.1234', '\\n',\n'Epoch 4: validation_loss = 0.3398', '\\n', 'Epoch 4: AUC_vote=0.6623,\nDES_vote=0.1104, AUC_kl=0.7548, DES_kl=0.1258', '\\n', 'Epoch 5: validation_loss\n= 0.5371', '\\n', 'Epoch 5: AUC_vote=0.6780, DES_vote=0.1130, AUC_kl=0.7788,\nDES_kl=0.1298', '\\n', \"Some weights of BertForSequenceClassification were not\ninitialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'Epoch 1: validation_loss = 0.1392', '\\n', 'Epoch 1:\nAUC_vote=0.6493, DES_vote=0.1082, AUC_kl=0.9050, DES_kl=0.1508', '\\n', 'Epoch 2:\nvalidation_loss = 0.1141', '\\n', 'Epoch 2: AUC_vote=0.6506, DES_vote=0.1084,\nAUC_kl=0.8999, DES_kl=0.1500', '\\n', 'Epoch 3: validation_loss = 0.1680', '\\n',\n'Epoch 3: AUC_vote=0.5853, DES_vote=0.0976, AUC_kl=0.8430, DES_kl=0.1405', '\\n',\n'Epoch 4: validation_loss = 0.1636', '\\n', 'Epoch 4: AUC_vote=0.5868,\nDES_vote=0.0978, AUC_kl=0.8817, DES_kl=0.1470', '\\n', 'Epoch 5: validation_loss\n= 0.2072', '\\n', 'Epoch 5: AUC_vote=0.6333, DES_vote=0.1056, AUC_kl=0.8876,\nDES_kl=0.1479', '\\n', \"Some weights of BertForSequenceClassification were not\ninitialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'Epoch 1: validation_loss = 0.2487', '\\n', 'Epoch 1:\nAUC_vote=0.5327, DES_vote=0.0888, AUC_kl=0.8679, DES_kl=0.1446', '\\n', 'Epoch 2:\nvalidation_loss = 0.2062', '\\n', 'Epoch 2: AUC_vote=0.5780, DES_vote=0.0963,\nAUC_kl=0.8599, DES_kl=0.1433', '\\n', 'Epoch 3: validation_loss = 0.2616', '\\n',\n'Epoch 3: AUC_vote=0.5751, DES_vote=0.0958, AUC_kl=0.8603, DES_kl=0.1434', '\\n',\n'Epoch 4: validation_loss = 0.3053', '\\n', 'Epoch 4: AUC_vote=0.6119,\nDES_vote=0.1020, AUC_kl=0.8697, DES_kl=0.1449', '\\n', 'Epoch 5: validation_loss\n= 0.4011', '\\n', 'Epoch 5: AUC_vote=0.5382, DES_vote=0.0897, AUC_kl=0.8526,\nDES_kl=0.1421', '\\n', 'Execution time: 23 minutes seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', \"Some weights of BertForSequenceClassification were\nnot initialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'Epoch 1: validation_loss = 0.2590', '\\n', 'Epoch 1:\ndetection_auc = 0.7915, DES = 0.1319', '\\n', 'Epoch 2: validation_loss =\n0.4243', '\\n', 'Epoch 2: detection_auc = 0.7941, DES = 0.1324', '\\n', 'Epoch 3:\nvalidation_loss = 0.3257', '\\n', 'Epoch 3: detection_auc = 0.7796, DES =\n0.1299', '\\n', 'Epoch 4: validation_loss = 0.4053', '\\n', 'Epoch 4:\ndetection_auc = 0.7970, DES = 0.1328', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.1992', '\\n', 'Epoch 1: detection_auc = 0.7142, DES =\n0.1190', '\\n', 'Epoch 2: validation_loss = 0.2268', '\\n', 'Epoch 2:\ndetection_auc = 0.7388, DES = 0.1231', '\\n', 'Epoch 3: validation_loss =\n0.2761', '\\n', 'Epoch 3: detection_auc = 0.7616, DES = 0.1269', '\\n', 'Epoch 4:\nvalidation_loss = 0.2958', '\\n', 'Epoch 4: detection_auc = 0.7527, DES =\n0.1254', '\\n', \"Some weights of BertForSequenceClassification were not\ninitialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'Epoch 1: validation_loss = 0.3598', '\\n', 'Epoch 1:\ndetection_auc = 0.6846, DES = 0.1141', '\\n', 'Epoch 2: validation_loss =\n0.3482', '\\n', 'Epoch 2: detection_auc = 0.7455, DES = 0.1243', '\\n', 'Epoch 3:\nvalidation_loss = 0.5419', '\\n', 'Epoch 3: detection_auc = 0.7689, DES =\n0.1282', '\\n', 'Epoch 4: validation_loss = 0.6357', '\\n', 'Epoch 4:\ndetection_auc = 0.7715, DES = 0.1286', '\\n', 'Execution time: 8 minutes seconds\n(time limit is an hour).']", "['Using device: cuda', '\\n', '\\rconfig.json:   0%|          | 0.00/1.14k\n[00:00<?, ?B/s]', '', '\\rconfig.json: 100%|##########| 1.14k/1.14k [00:00<00:00,\n4.46MB/s]', '\\n', '\\rpytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?,\n?B/s]', '\\rpytorch_model.bin:   0%|          | 10.5M/2.28G [00:00<00:31,\n71.1MB/s]', '\\rpytorch_model.bin:   1%|1         | 31.5M/2.28G [00:00<00:23,\n97.3MB/s]', '\\rpytorch_model.bin:   2%|2         | 52.4M/2.28G [00:00<00:20,\n109MB/s] ', '\\rpytorch_model.bin:   3%|3         | 73.4M/2.28G [00:00<00:19,\n112MB/s]', '\\rpytorch_model.bin:   4%|4         | 94.4M/2.28G [00:00<00:19,\n114MB/s]', '\\rpytorch_model.bin:   5%|5         | 115M/2.28G [00:01<00:18,\n114MB/s] ', '\\rpytorch_model.bin:   6%|5         | 136M/2.28G [00:01<00:18,\n115MB/s]', '\\rpytorch_model.bin:   7%|6         | 157M/2.28G [00:01<00:18,\n116MB/s]', '\\rpytorch_model.bin:   8%|7         | 178M/2.28G [00:01<00:18,\n116MB/s]', '\\rpytorch_model.bin:   9%|8         | 199M/2.28G [00:01<00:17,\n116MB/s]', '\\rpytorch_model.bin:  10%|9         | 220M/2.28G [00:01<00:17,\n115MB/s]', '\\rpytorch_model.bin:  11%|#         | 241M/2.28G [00:02<00:17,\n116MB/s]', '\\rpytorch_model.bin:  12%|#1        | 262M/2.28G [00:02<00:17,\n116MB/s]', '\\rpytorch_model.bin:  12%|#2        | 283M/2.28G [00:02<00:17,\n116MB/s]', '\\rpytorch_model.bin:  13%|#3        | 304M/2.28G [00:02<00:16,\n116MB/s]', '\\rpytorch_model.bin:  14%|#4        | 325M/2.28G [00:02<00:16,\n116MB/s]', '\\rpytorch_model.bin:  15%|#5        | 346M/2.28G [00:03<00:16,\n116MB/s]', '\\rpytorch_model.bin:  16%|#6        | 367M/2.28G [00:03<00:16,\n116MB/s]', '\\rpytorch_model.bin:  17%|#7        | 388M/2.28G [00:03<00:16,\n116MB/s]', '\\rpytorch_model.bin:  18%|#7        | 409M/2.28G [00:03<00:16,\n116MB/s]', '\\rpytorch_model.bin:  19%|#8        | 430M/2.28G [00:03<00:15,\n116MB/s]', '\\rpytorch_model.bin:  20%|#9        | 451M/2.28G [00:03<00:15,\n116MB/s]', '\\rpytorch_model.bin:  21%|##        | 472M/2.28G [00:04<00:15,\n116MB/s]', '\\rpytorch_model.bin:  22%|##1       | 493M/2.28G [00:04<00:15,\n117MB/s]', '\\rpytorch_model.bin:  23%|##2       | 514M/2.28G [00:04<00:15,\n116MB/s]', '\\rpytorch_model.bin:  24%|##3       | 535M/2.28G [00:04<00:14,\n116MB/s]', '\\rpytorch_model.bin:  24%|##4       | 556M/2.28G [00:04<00:14,\n116MB/s]', '\\rpytorch_model.bin:  25%|##5       | 577M/2.28G [00:05<00:14,\n115MB/s]', '\\rpytorch_model.bin:  26%|##6       | 598M/2.28G [00:05<00:14,\n117MB/s]', '\\rpytorch_model.bin:  27%|##7       | 619M/2.28G [00:05<00:14,\n116MB/s]', '\\rpytorch_model.bin:  28%|##8       | 640M/2.28G [00:05<00:14,\n116MB/s]', '\\rpytorch_model.bin:  29%|##9       | 661M/2.28G [00:05<00:13,\n116MB/s]', '\\rpytorch_model.bin:  30%|##9       | 682M/2.28G [00:05<00:13,\n116MB/s]', '\\rpytorch_model.bin:  31%|###       | 703M/2.28G [00:06<00:13,\n116MB/s]', '\\rpytorch_model.bin:  32%|###1      | 724M/2.28G [00:06<00:13,\n116MB/s]', '\\rpytorch_model.bin:  33%|###2      | 744M/2.28G [00:06<00:13,\n115MB/s]', '\\rpytorch_model.bin:  34%|###3      | 765M/2.28G [00:06<00:12,\n117MB/s]', '\\rpytorch_model.bin:  35%|###4      | 786M/2.28G [00:06<00:12,\n116MB/s]', '\\rpytorch_model.bin:  35%|###5      | 807M/2.28G [00:07<00:12,\n116MB/s]', '\\rpytorch_model.bin:  36%|###6      | 828M/2.28G [00:07<00:12,\n117MB/s]', '\\rpytorch_model.bin:  37%|###7      | 849M/2.28G [00:07<00:12,\n117MB/s]', '\\rpytorch_model.bin:  38%|###8      | 870M/2.28G [00:07<00:12,\n117MB/s]', '\\rpytorch_model.bin:  39%|###9      | 891M/2.28G [00:07<00:11,\n117MB/s]', '\\rpytorch_model.bin:  40%|####      | 912M/2.28G [00:07<00:11,\n116MB/s]', '\\rpytorch_model.bin:  41%|####1     | 933M/2.28G [00:08<00:11,\n117MB/s]', '\\rpytorch_model.bin:  42%|####1     | 954M/2.28G [00:08<00:11,\n116MB/s]', '\\rpytorch_model.bin:  43%|####2     | 975M/2.28G [00:08<00:11,\n115MB/s]', '\\rpytorch_model.bin:  44%|####3     | 996M/2.28G [00:08<00:10,\n117MB/s]', '\\rpytorch_model.bin:  45%|####4     | 1.02G/2.28G [00:08<00:10,\n117MB/s]', '\\rpytorch_model.bin:  46%|####5     | 1.04G/2.28G [00:08<00:10,\n117MB/s]', '\\rpytorch_model.bin:  47%|####6     | 1.06G/2.28G [00:09<00:10,\n116MB/s]', '\\rpytorch_model.bin:  47%|####7     | 1.08G/2.28G [00:09<00:10,\n116MB/s]', '\\rpytorch_model.bin:  48%|####8     | 1.10G/2.28G [00:09<00:10,\n116MB/s]', '\\rpytorch_model.bin:  49%|####9     | 1.12G/2.28G [00:09<00:09,\n116MB/s]', '\\rpytorch_model.bin:  50%|#####     | 1.14G/2.28G [00:09<00:09,\n116MB/s]', '\\rpytorch_model.bin:  51%|#####1    | 1.16G/2.28G [00:10<00:09,\n116MB/s]', '\\rpytorch_model.bin:  52%|#####2    | 1.18G/2.28G [00:10<00:09,\n115MB/s]', '\\rpytorch_model.bin:  53%|#####2    | 1.21G/2.28G [00:10<00:09,\n117MB/s]', '\\rpytorch_model.bin:  54%|#####3    | 1.23G/2.28G [00:10<00:09,\n116MB/s]', '\\rpytorch_model.bin:  55%|#####4    | 1.25G/2.28G [00:10<00:08,\n116MB/s]', '\\rpytorch_model.bin:  56%|#####5    | 1.27G/2.28G [00:10<00:08,\n116MB/s]', '\\rpytorch_model.bin:  57%|#####6    | 1.29G/2.28G [00:11<00:08,\n116MB/s]', '\\rpytorch_model.bin:  58%|#####7    | 1.31G/2.28G [00:11<00:08,\n116MB/s]', '\\rpytorch_model.bin:  59%|#####8    | 1.33G/2.28G [00:11<00:08,\n116MB/s]', '\\rpytorch_model.bin:  59%|#####9    | 1.35G/2.28G [00:11<00:07,\n117MB/s]', '\\rpytorch_model.bin:  60%|######    | 1.37G/2.28G [00:11<00:07,\n116MB/s]', '\\rpytorch_model.bin:  61%|######1   | 1.39G/2.28G [00:12<00:07,\n115MB/s]', '\\rpytorch_model.bin:  62%|######2   | 1.42G/2.28G [00:12<00:07,\n116MB/s]', '\\rpytorch_model.bin:  63%|######3   | 1.44G/2.28G [00:12<00:07,\n116MB/s]', '\\rpytorch_model.bin:  64%|######4   | 1.46G/2.28G [00:12<00:07,\n116MB/s]', '\\rpytorch_model.bin:  65%|######4   | 1.48G/2.28G [00:12<00:06,\n116MB/s]', '\\rpytorch_model.bin:  66%|######5   | 1.50G/2.28G [00:12<00:06,\n116MB/s]', '\\rpytorch_model.bin:  67%|######6   | 1.52G/2.28G [00:13<00:06,\n116MB/s]', '\\rpytorch_model.bin:  68%|######7   | 1.54G/2.28G [00:13<00:06,\n116MB/s]', '\\rpytorch_model.bin:  69%|######8   | 1.56G/2.28G [00:13<00:06,\n116MB/s]', '\\rpytorch_model.bin:  70%|######9   | 1.58G/2.28G [00:13<00:05,\n116MB/s]', '\\rpytorch_model.bin:  71%|#######   | 1.60G/2.28G [00:14<00:07,\n86.2MB/s]', '\\rpytorch_model.bin:  71%|#######1  | 1.63G/2.28G [00:14<00:06,\n93.9MB/s]', '\\rpytorch_model.bin:  72%|#######2  | 1.65G/2.28G [00:14<00:06,\n99.7MB/s]', '\\rpytorch_model.bin:  73%|#######3  | 1.67G/2.28G [00:14<00:05,\n104MB/s] ', '\\rpytorch_model.bin:  74%|#######4  | 1.69G/2.28G [00:14<00:05,\n107MB/s]', '\\rpytorch_model.bin:  75%|#######5  | 1.71G/2.28G [00:14<00:05,\n110MB/s]', '\\rpytorch_model.bin:  76%|#######6  | 1.73G/2.28G [00:15<00:04,\n112MB/s]', '\\rpytorch_model.bin:  77%|#######6  | 1.75G/2.28G [00:15<00:04,\n113MB/s]', '\\rpytorch_model.bin:  78%|#######7  | 1.77G/2.28G [00:15<00:04,\n114MB/s]', '\\rpytorch_model.bin:  79%|#######8  | 1.79G/2.28G [00:15<00:04,\n115MB/s]', '\\rpytorch_model.bin:  80%|#######9  | 1.81G/2.28G [00:15<00:04,\n115MB/s]', '\\rpytorch_model.bin:  81%|########  | 1.84G/2.28G [00:16<00:03,\n112MB/s]', '\\rpytorch_model.bin:  82%|########1 | 1.86G/2.28G [00:16<00:03,\n116MB/s]', '\\rpytorch_model.bin:  82%|########2 | 1.88G/2.28G [00:16<00:03,\n116MB/s]', '\\rpytorch_model.bin:  83%|########3 | 1.90G/2.28G [00:16<00:03,\n116MB/s]', '\\rpytorch_model.bin:  84%|########4 | 1.92G/2.28G [00:16<00:03,\n117MB/s]', '\\rpytorch_model.bin:  85%|########5 | 1.94G/2.28G [00:16<00:02,\n116MB/s]', '\\rpytorch_model.bin:  86%|########6 | 1.96G/2.28G [00:17<00:02,\n116MB/s]', '\\rpytorch_model.bin:  87%|########7 | 1.98G/2.28G [00:17<00:02,\n116MB/s]', '\\rpytorch_model.bin:  88%|########8 | 2.00G/2.28G [00:17<00:02,\n116MB/s]', '\\rpytorch_model.bin:  89%|########8 | 2.02G/2.28G [00:17<00:02,\n115MB/s]', '\\rpytorch_model.bin:  90%|########9 | 2.04G/2.28G [00:17<00:01,\n117MB/s]', '\\rpytorch_model.bin:  91%|######### | 2.07G/2.28G [00:18<00:01,\n117MB/s]', '\\rpytorch_model.bin:  92%|#########1| 2.09G/2.28G [00:18<00:01,\n116MB/s]', '\\rpytorch_model.bin:  93%|#########2| 2.11G/2.28G [00:18<00:01,\n116MB/s]', '\\rpytorch_model.bin:  94%|#########3| 2.13G/2.28G [00:18<00:01,\n116MB/s]', '\\rpytorch_model.bin:  94%|#########4| 2.15G/2.28G [00:18<00:01,\n116MB/s]', '\\rpytorch_model.bin:  95%|#########5| 2.17G/2.28G [00:18<00:00,\n116MB/s]', '\\rpytorch_model.bin:  96%|#########6| 2.19G/2.28G [00:19<00:00,\n116MB/s]', '\\rpytorch_model.bin:  97%|#########7| 2.21G/2.28G [00:19<00:00,\n115MB/s]', '\\rpytorch_model.bin:  98%|#########8| 2.23G/2.28G [00:19<00:00,\n115MB/s]', '\\rpytorch_model.bin:  99%|#########9| 2.25G/2.28G [00:19<00:00,\n115MB/s]', '\\rpytorch_model.bin: 100%|#########9| 2.28G/2.28G [00:19<00:00,\n116MB/s]', '', '\\rpytorch_model.bin: 100%|##########| 2.28G/2.28G [00:19<00:00,\n115MB/s]', '\\n', 'Traceback (most recent call last):\\n  File \"runfile.py\", line\n18, in <module>\\n    paraphraser = pipeline(\\n                  ^^^^^^^^^\\n\nFile \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/pipelines/__init__.py\", line 942, in pipeline\\n\nframework, model = infer_framework_load_model(\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/pipelines/base.py\", line 305, in\ninfer_framework_load_model\\n    raise ValueError(\\nValueError: Could not load\nmodel tuner007/pegasus_paraphrase with any of the following classes: (<class\n\\'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM\\'>, <class \\'tran\nsformers.models.pegasus.modeling_pegasus.PegasusForConditionalGeneration\\'>).\nSee the original errors:\\n\\nwhile loading with AutoModelForSeq2SeqLM, an error\nis thrown:\\nTraceback (most recent call last):\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/pipelines/base.py\", line 292, in\ninfer_framework_load_model\\n    model = model_class.from_pretrained(model,\n**kwargs)\\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/models/auto/auto_factory.py\", line 571, in\nfrom_pretrained\\n    return model_class.from_pretrained(\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/modeling_utils.py\", line 309, in _wrapper\\n    return\nfunc(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/modeling_utils.py\", line 4573, in from_pretrained\\n    ) =\ncls._load_pretrained_model(\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/modeling_utils.py\", line 4832, in _load_pretrained_model\\n\nload_state_dict(checkpoint_files[0], map_location=\"meta\",\nweights_only=weights_only).keys()\\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/modeling_utils.py\", line 553, in load_state_dict\\n\ncheck_torch_load_is_safe()\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/utils/import_utils.py\", line 1417, in\ncheck_torch_load_is_safe\\n    raise ValueError(\\nValueError: Due to a serious\nvulnerability issue in `torch.load`, even with `weights_only=True`, we now\nrequire users to upgrade torch to at least v2.6 in order to use the function.\nThis version restriction does not apply when loading files with\nsafetensors.\\nSee the vulnerability report here\nhttps://nvd.nist.gov/vuln/detail/CVE-2025-32434\\n\\nwhile loading with\nPegasusForConditionalGeneration, an error is thrown:\\nTraceback (most recent\ncall last):\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/pipelines/base.py\", line 292, in\ninfer_framework_load_model\\n    model = model_class.from_pretrained(model,\n**kwargs)\\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/modeling_utils.py\", line 309, in _wrapper\\n    return\nfunc(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/modeling_utils.py\", line 4573, in from_pretrained\\n    ) =\ncls._load_pretrained_model(\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/modeling_utils.py\", line 4832, in _load_pretrained_model\\n\nload_state_dict(checkpoint_files[0], map_location=\"meta\",\nweights_only=weights_only).keys()\\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/modeling_utils.py\", line 553, in load_state_dict\\n\ncheck_torch_load_is_safe()\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/utils/import_utils.py\", line 1417, in\ncheck_torch_load_is_safe\\n    raise ValueError(\\nValueError: Due to a serious\nvulnerability issue in `torch.load`, even with `weights_only=True`, we now\nrequire users to upgrade torch to at least v2.6 in order to use the function.\nThis version restriction does not apply when loading files with\nsafetensors.\\nSee the vulnerability report here\nhttps://nvd.nist.gov/vuln/detail/CVE-2025-32434\\n\\n\\n\\n', 'Execution time: 25\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', \"Some weights of\nDistilBertForSequenceClassification were not initialized from the model\ncheckpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.4057', '\\n', 'Epoch 1: detection_auc = 0.5286, DES =\n0.1321', '\\n', 'Epoch 2: validation_loss = 0.4859', '\\n', 'Epoch 2:\ndetection_auc = 0.5949, DES = 0.1487', '\\n', 'Epoch 3: validation_loss =\n0.3878', '\\n', 'Epoch 3: detection_auc = 0.5545, DES = 0.1386', '\\n', \"Some\nweights of DistilBertForSequenceClassification were not initialized from the\nmodel checkpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.2293', '\\n', 'Epoch 1: detection_auc = 0.5168, DES =\n0.1292', '\\n', 'Epoch 2: validation_loss = 0.2183', '\\n', 'Epoch 2:\ndetection_auc = 0.5473, DES = 0.1368', '\\n', 'Epoch 3: validation_loss =\n0.2825', '\\n', 'Epoch 3: detection_auc = 0.6007, DES = 0.1502', '\\n', \"Some\nweights of DistilBertForSequenceClassification were not initialized from the\nmodel checkpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.3612', '\\n', 'Epoch 1: detection_auc = 0.5437, DES =\n0.1359', '\\n', 'Epoch 2: validation_loss = 0.3936', '\\n', 'Epoch 2:\ndetection_auc = 0.5827, DES = 0.1457', '\\n', 'Epoch 3: validation_loss =\n0.4622', '\\n', 'Epoch 3: detection_auc = 0.5057, DES = 0.1264', '\\n', 'Execution\ntime: 2 minutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', \"Some weights of\nDistilBertForSequenceClassification were not initialized from the model\ncheckpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.3862', '\\n', 'Epoch 1: detection_auc = 0.4858, DES =\n0.1214', '\\n', 'Epoch 2: validation_loss = 0.3535', '\\n', 'Epoch 2:\ndetection_auc = 0.5257, DES = 0.1314', '\\n', 'Epoch 3: validation_loss =\n0.5526', '\\n', 'Epoch 3: detection_auc = 0.6096, DES = 0.1524', '\\n', \"Some\nweights of DistilBertForSequenceClassification were not initialized from the\nmodel checkpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.2266', '\\n', 'Epoch 1: detection_auc = 0.5256, DES =\n0.1314', '\\n', 'Epoch 2: validation_loss = 0.2495', '\\n', 'Epoch 2:\ndetection_auc = 0.5108, DES = 0.1277', '\\n', 'Epoch 3: validation_loss =\n0.2668', '\\n', 'Epoch 3: detection_auc = 0.5668, DES = 0.1417', '\\n', \"Some\nweights of DistilBertForSequenceClassification were not initialized from the\nmodel checkpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.4220', '\\n', 'Epoch 1: detection_auc = 0.5004, DES =\n0.1251', '\\n', 'Epoch 2: validation_loss = 0.4851', '\\n', 'Epoch 2:\ndetection_auc = 0.5352, DES = 0.1338', '\\n', 'Epoch 3: validation_loss =\n0.4947', '\\n', 'Epoch 3: detection_auc = 0.5646, DES = 0.1411', '\\n', 'Execution\ntime: 2 minutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', \"Some weights of\nDistilBertForSequenceClassification were not initialized from the model\ncheckpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.3862', '\\n', 'Epoch 1: detection_auc = 0.4858, DES =\n0.1214', '\\n', 'Epoch 2: validation_loss = 0.3535', '\\n', 'Epoch 2:\ndetection_auc = 0.5257, DES = 0.1314', '\\n', 'Epoch 3: validation_loss =\n0.5526', '\\n', 'Epoch 3: detection_auc = 0.6096, DES = 0.1524', '\\n', \"Some\nweights of DistilBertForSequenceClassification were not initialized from the\nmodel checkpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.2267', '\\n', 'Epoch 1: detection_auc = 0.5256, DES =\n0.1314', '\\n', 'Epoch 2: validation_loss = 0.2492', '\\n', 'Epoch 2:\ndetection_auc = 0.5108, DES = 0.1277', '\\n', 'Epoch 3: validation_loss =\n0.2680', '\\n', 'Epoch 3: detection_auc = 0.5452, DES = 0.1363', '\\n', \"Some\nweights of DistilBertForSequenceClassification were not initialized from the\nmodel checkpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.4220', '\\n', 'Epoch 1: detection_auc = 0.5004, DES =\n0.1251', '\\n', 'Epoch 2: validation_loss = 0.4851', '\\n', 'Epoch 2:\ndetection_auc = 0.5352, DES = 0.1338', '\\n', 'Epoch 3: validation_loss =\n0.4947', '\\n', 'Epoch 3: detection_auc = 0.5646, DES = 0.1411', '\\n', 'Execution\ntime: 2 minutes seconds (time limit is an hour).']", ""], "analysis": ["", "The script crashes when initializing the paraphraser pipeline because the model\nID \u201cramsrigouthamg/t5_paraphraser_small\u201d does not exist on the Hugging Face Hub\n(404 error). To fix this, update the pipeline to load a valid paraphrase\nmodel\u2014for example, use \u201cramsrigoutham/t5_paraphraser-small\u201d or\n\u201cVamsi/T5_Paraphrase_Paws\u201d\u2014or ensure the correct repository name and\nauthentication if accessing a private model.", "The script fails immediately when creating the paraphrase pipeline for the model\n\"Vamsi/T5_Paraphrase_Paws\". The underlying T5 tokenizer conversion from\nslow->fast raises a missing 'blobfile' ImportError (required by tiktoken) and\nthen a ValueError about lacking a SentencePiece tokenizer model. This prevents\nany experiments from running. Proposed fix: Install the missing dependency by\nadding `pip install blobfile` (or include it in requirements). Alternatively,\nbypass the slow->fast conversion by loading the tokenizer with `use_fast=False`\n(e.g., `AutoTokenizer.from_pretrained(paraphrase_model_name, use_fast=False)`)\nor point to a model including a SentencePiece `.model` file. Either approach\nwill enable the paraphrase pipeline to initialize correctly and allow the script\nto proceed.", "The crash arises because the synthetic dataset produces 1-dimensional feature\nvectors, but the model\u2019s first Linear layer is hard-coded to expect 3 input\nfeatures. When running on the synthetic data (batch size \u00d7 1), the 1\u00d732 tensor\ncannot be multiplied by a 3\u00d716 weight matrix. To fix this, instantiate the\nmodel\u2019s input layer size dynamically based on the dataset\u2019s feature\ndimension\u2014e.g., set input_dim = train_ds.tensors[0].shape[1] when building\nnn.Linear\u2014instead of hard-coding 3.", "The model\u2019s validation loss remains stuck at ~0.698 and detection AUC at ~0.5\nacross all epochs, indicating no training progress. The root cause is the\nincorrect order of gradient operations: optimizer.zero_grad() is called after\nloss.backward() instead of before, clearing gradients before the\noptimizer.step() and preventing any weight updates. To fix this, move\noptimizer.zero_grad() to the start of each training iteration (before computing\nloss.backward()), then call optimizer.step() after backward.", "A warning indicates some sequences exceed the model\u2019s max token limit (639 >\n512) without truncation, which may cause indexing errors and unpredictable\nbehavior. Fix by adding `truncation=True` and `max_length=512` (and optionally\n`padding=True`) to the tokenizer call to ensure inputs are clipped to the\nmodel\u2019s expected size.", "", "The script crashes at loading PegasusTokenizer because the SentencePiece library\nis not installed. To fix this, add the SentencePiece dependency (e.g., pip\ninstall sentencepiece) to the environment and restart the runtime so\nPegasusTokenizer can be imported successfully.", "The script fails when initializing the translation pipelines (Helsinki-NLP/opus-\nmt-en-de and opus-mt-de-en) due to a vulnerability in torch.load requiring torch\n\u22652.6 for loading .bin weights. Proposed fixes: upgrade torch to at least v2.6,\nor load models via safetensors (e.g., convert weights to .safetensors or use the\n\u2018safe_serialization\u2019 flag when calling from_pretrained). Alternatively, specify\nmodels that ship in safetensors format to avoid the torch.load restriction.", "", "The script ran successfully end-to-end on all three HuggingFace datasets (SST-2,\nYelp Polarity, IMDb) without any runtime errors or crashes. The warnings about\nuninitialized classifier weights are expected for a fresh classification head.\nValidation losses and detection metrics were logged as intended: vote-based AUCs\nranged from ~0.53 to ~0.68, and KL-based AUCs from ~0.74 to ~0.91, with\ncorresponding normalized DES scores (~AUC/(K+1)) in the 0.09\u20130.15 range. The\nresults confirm that KL-divergence uncertainty outperforms simple vote\ndisagreement. experiment_data.npy was saved, and total runtime (~23 minutes) was\nwithin the time limit. No bugs detected.", "", "The execution fails when loading the 'tuner007/pegasus_paraphrase' model due to\na vulnerability check in torch.load requiring PyTorch \u22652.6. Proposed fix:\nupgrade the PyTorch installation to version 2.6 or higher, or convert and load\nmodel weights in the safetensors format (which bypasses the version\nrestriction).", "", "", "", ""], "exc_type": [null, "OSError", "ValueError", "RuntimeError", null, null, null, "ImportError", "ValueError", null, null, null, "ValueError", null, null, null, null], "exc_info": [null, {"args": ["ramsrigouthamg/t5_paraphraser_small is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"]}, {"args": ["Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"]}, {"args": ["mat1 and mat2 shapes cannot be multiplied (32x1 and 3x16)"]}, null, null, null, {"args": ["\nPegasusTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"], "name": "None", "msg": "\nPegasusTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"}, {"args": ["Could not load model Helsinki-NLP/opus-mt-en-de with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.marian.modeling_marian.MarianMTModel'>). See the original errors:\n\nwhile loading with AutoModelForSeq2SeqLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 571, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 309, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4573, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4832, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 553, in load_state_dict\n    check_torch_load_is_safe()\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1417, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nwhile loading with MarianMTModel, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 309, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4573, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4832, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 553, in load_state_dict\n    check_torch_load_is_safe()\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1417, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\n\n"]}, null, null, null, {"args": ["Could not load model tuner007/pegasus_paraphrase with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.pegasus.modeling_pegasus.PegasusForConditionalGeneration'>). See the original errors:\n\nwhile loading with AutoModelForSeq2SeqLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 571, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 309, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4573, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4832, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 553, in load_state_dict\n    check_torch_load_is_safe()\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1417, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nwhile loading with PegasusForConditionalGeneration, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 309, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4573, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4832, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 553, in load_state_dict\n    check_torch_load_is_safe()\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1417, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\n\n"]}, null, null, null, null], "exc_stack": [null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 16, "<module>", "paraphraser = pipeline("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/pipelines/__init__.py", 812, "pipeline", "resolved_config_file = cached_file("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/utils/hub.py", 312, "cached_file", "file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/utils/hub.py", 502, "cached_files", "raise OSError("]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 16, "<module>", "paraphrase_pipe = pipeline("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/pipelines/__init__.py", 1049, "pipeline", "tokenizer = AutoTokenizer.from_pretrained("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", 1032, "from_pretrained", "return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", 2025, "from_pretrained", "return cls._from_pretrained("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", 2278, "_from_pretrained", "tokenizer = cls(*init_inputs, **init_kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5_fast.py", 119, "__init__", "super().__init__("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py", 139, "__init__", "fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py", 1739, "convert_slow_tokenizer", "raise ValueError("]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 114, "<module>", "logits = model(xb).squeeze(1)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/container.py", 250, "forward", "input = module(input)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/linear.py", 125, "forward", "return F.linear(input, self.weight, self.bias)"]], null, null, null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 26, "<module>", "tokenizer_para = PegasusTokenizer.from_pretrained(\"tuner007/pegasus_paraphrase\")"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/utils/import_utils.py", 1885, "__getattribute__", "requires_backends(cls, cls._backends)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/utils/import_utils.py", 1871, "requires_backends", "raise ImportError(\"\".join(failed))"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 20, "<module>", "trans_en_de = pipeline("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/pipelines/__init__.py", 942, "pipeline", "framework, model = infer_framework_load_model("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/pipelines/base.py", 305, "infer_framework_load_model", "raise ValueError("]], null, null, null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 18, "<module>", "paraphraser = pipeline("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/pipelines/__init__.py", 942, "pipeline", "framework, model = infer_framework_load_model("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/pipelines/base.py", 305, "infer_framework_load_model", "raise ValueError("]], null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy", "data": [{"dataset_name": "synthetic", "final_value": 0.9522, "best_value": 0.9522}, {"dataset_name": "sst2", "final_value": 0.5558, "best_value": 0.5558}, {"dataset_name": "yelp_polarity", "final_value": 0.593, "best_value": 0.593}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "synthetic", "final_value": 0.9572, "best_value": 0.9572}, {"dataset_name": "sst2", "final_value": 0.584, "best_value": 0.584}, {"dataset_name": "yelp_polarity", "final_value": 0.5828, "best_value": 0.5828}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "synthetic", "final_value": 0.2641, "best_value": 0.2641}, {"dataset_name": "sst2", "final_value": 0.6815, "best_value": 0.6815}, {"dataset_name": "yelp_polarity", "final_value": 0.68, "best_value": 0.68}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "synthetic", "final_value": 0.2876, "best_value": 0.2876}, {"dataset_name": "sst2", "final_value": 0.6891, "best_value": 0.6891}, {"dataset_name": "yelp_polarity", "final_value": 0.6924, "best_value": 0.6924}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "sst2", "final_value": 0.7007, "best_value": 0.7007}, {"dataset_name": "yelp_polarity", "final_value": 0.6973, "best_value": 0.6973}, {"dataset_name": "imdb", "final_value": 0.6994, "best_value": 0.6994}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "sst2", "final_value": 0.698, "best_value": 0.698}, {"dataset_name": "yelp_polarity", "final_value": 0.698, "best_value": 0.698}, {"dataset_name": "imdb", "final_value": 0.6988, "best_value": 0.6988}]}, {"metric_name": "detection AUC", "lower_is_better": false, "description": "Final detection area under the ROC curve", "data": [{"dataset_name": "sst2", "final_value": 0.508, "best_value": 0.508}, {"dataset_name": "yelp_polarity", "final_value": 0.5, "best_value": 0.5}, {"dataset_name": "imdb", "final_value": 0.5, "best_value": 0.5}]}, {"metric_name": "detection DES", "lower_is_better": true, "description": "Final detection error score", "data": [{"dataset_name": "sst2", "final_value": 0.127, "best_value": 0.127}, {"dataset_name": "yelp_polarity", "final_value": 0.125, "best_value": 0.125}, {"dataset_name": "imdb", "final_value": 0.125, "best_value": 0.125}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Final validation accuracy", "data": [{"dataset_name": "sst2", "final_value": 0.47, "best_value": 0.47}, {"dataset_name": "yelp_polarity", "final_value": 0.5, "best_value": 0.5}, {"dataset_name": "imdb", "final_value": 0.48, "best_value": 0.48}]}]}, {"metric_names": [{"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss (cross-entropy) on the validation set", "data": [{"dataset_name": "sst2", "final_value": 0.008, "best_value": 0.008}, {"dataset_name": "yelp_polarity", "final_value": 0.008, "best_value": 0.008}, {"dataset_name": "imdb", "final_value": 0.016, "best_value": 0.016}]}, {"metric_name": "detection AUC", "lower_is_better": false, "description": "Area Under the ROC Curve for detection", "data": [{"dataset_name": "sst2", "final_value": 0.9898, "best_value": 0.9898}, {"dataset_name": "yelp_polarity", "final_value": 0.9898, "best_value": 0.9898}, {"dataset_name": "imdb", "final_value": 0.7188, "best_value": 0.7188}]}, {"metric_name": "DES", "lower_is_better": true, "description": "Detection Error Score", "data": [{"dataset_name": "sst2", "final_value": 0.198, "best_value": 0.198}, {"dataset_name": "yelp_polarity", "final_value": 0.198, "best_value": 0.198}, {"dataset_name": "imdb", "final_value": 0.1437, "best_value": 0.1437}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training set", "data": [{"dataset_name": "synthetic", "final_value": 0.9517, "best_value": 0.9517}, {"dataset_name": "sst2", "final_value": 0.5773, "best_value": 0.5773}, {"dataset_name": "yelp_polarity", "final_value": 0.5977, "best_value": 0.5977}, {"dataset_name": "imdb", "final_value": 0.583, "best_value": 0.583}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.9572, "best_value": 0.9572}, {"dataset_name": "sst2", "final_value": 0.4632, "best_value": 0.4632}, {"dataset_name": "yelp_polarity", "final_value": 0.5953, "best_value": 0.5953}, {"dataset_name": "imdb", "final_value": 0.5528, "best_value": 0.5528}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "synthetic", "final_value": 0.2478, "best_value": 0.2478}, {"dataset_name": "sst2", "final_value": 0.677, "best_value": 0.677}, {"dataset_name": "yelp_polarity", "final_value": 0.6763, "best_value": 0.6763}, {"dataset_name": "imdb", "final_value": 0.6809, "best_value": 0.6809}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.2676, "best_value": 0.2676}, {"dataset_name": "sst2", "final_value": 0.6956, "best_value": 0.6956}, {"dataset_name": "yelp_polarity", "final_value": 0.6834, "best_value": 0.6834}, {"dataset_name": "imdb", "final_value": 0.6924, "best_value": 0.6924}]}, {"metric_name": "DES", "lower_is_better": false, "description": "DES metric", "data": [{"dataset_name": "synthetic", "final_value": 0.4786, "best_value": 0.4786}, {"dataset_name": "sst2", "final_value": 0.2316, "best_value": 0.2316}, {"dataset_name": "yelp_polarity", "final_value": 0.2976, "best_value": 0.2976}, {"dataset_name": "imdb", "final_value": 0.2764, "best_value": 0.2764}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "sst2", "final_value": 0.1094, "best_value": 0.1094}, {"dataset_name": "yelp_polarity", "final_value": 0.0549, "best_value": 0.0549}, {"dataset_name": "imdb", "final_value": 0.1021, "best_value": 0.1021}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "sst2", "final_value": 0.4326, "best_value": 0.4326}, {"dataset_name": "yelp_polarity", "final_value": 0.3306, "best_value": 0.3306}, {"dataset_name": "imdb", "final_value": 0.4118, "best_value": 0.4118}]}, {"metric_name": "detection AUC", "lower_is_better": false, "description": "Area under the ROC curve for detection", "data": [{"dataset_name": "sst2", "final_value": 0.6332, "best_value": 0.6332}, {"dataset_name": "yelp_polarity", "final_value": 0.5486, "best_value": 0.5486}, {"dataset_name": "imdb", "final_value": 0.509, "best_value": 0.509}]}, {"metric_name": "detection DES", "lower_is_better": true, "description": "Detection error score", "data": [{"dataset_name": "sst2", "final_value": 0.1583, "best_value": 0.1583}, {"dataset_name": "yelp_polarity", "final_value": 0.1372, "best_value": 0.1372}, {"dataset_name": "imdb", "final_value": 0.1273, "best_value": 0.1273}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "sst2", "final_value": 0.835, "best_value": 0.835}, {"dataset_name": "yelp_polarity", "final_value": 0.885, "best_value": 0.885}, {"dataset_name": "imdb", "final_value": 0.86, "best_value": 0.86}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "sst2", "final_value": 0.0378, "best_value": 0.0378}, {"dataset_name": "yelp_polarity", "final_value": 0.0127, "best_value": 0.0127}, {"dataset_name": "imdb", "final_value": 0.0373, "best_value": 0.0373}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "sst2", "final_value": 0.5371, "best_value": 0.5371}, {"dataset_name": "yelp_polarity", "final_value": 0.2072, "best_value": 0.2072}, {"dataset_name": "imdb", "final_value": 0.4011, "best_value": 0.4011}]}, {"metric_name": "detection AUC (vote)", "lower_is_better": false, "description": "Area Under the ROC Curve for detection using vote", "data": [{"dataset_name": "sst2", "final_value": 0.678, "best_value": 0.678}, {"dataset_name": "yelp_polarity", "final_value": 0.6333, "best_value": 0.6333}, {"dataset_name": "imdb", "final_value": 0.5382, "best_value": 0.5382}]}, {"metric_name": "detection DES (vote)", "lower_is_better": true, "description": "Detection Error Score using vote", "data": [{"dataset_name": "sst2", "final_value": 0.113, "best_value": 0.113}, {"dataset_name": "yelp_polarity", "final_value": 0.1056, "best_value": 0.1056}, {"dataset_name": "imdb", "final_value": 0.0897, "best_value": 0.0897}]}, {"metric_name": "detection AUC (KL-divergence)", "lower_is_better": false, "description": "Area Under the ROC Curve for detection using KL divergence", "data": [{"dataset_name": "sst2", "final_value": 0.7788, "best_value": 0.7788}, {"dataset_name": "yelp_polarity", "final_value": 0.8876, "best_value": 0.8876}, {"dataset_name": "imdb", "final_value": 0.8526, "best_value": 0.8526}]}, {"metric_name": "detection DES (KL-divergence)", "lower_is_better": true, "description": "Detection Error Score using KL divergence", "data": [{"dataset_name": "sst2", "final_value": 0.1298, "best_value": 0.1298}, {"dataset_name": "yelp_polarity", "final_value": 0.1479, "best_value": 0.1479}, {"dataset_name": "imdb", "final_value": 0.1421, "best_value": 0.1421}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Training loss of the model on the training dataset", "data": [{"dataset_name": "sst2", "final_value": 0.0395, "best_value": 0.0395}, {"dataset_name": "yelp_polarity", "final_value": 0.0487, "best_value": 0.0487}, {"dataset_name": "imdb", "final_value": 0.0606, "best_value": 0.0606}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss of the model on the validation dataset", "data": [{"dataset_name": "sst2", "final_value": 0.4053, "best_value": 0.4053}, {"dataset_name": "yelp_polarity", "final_value": 0.2958, "best_value": 0.2958}, {"dataset_name": "imdb", "final_value": 0.6357, "best_value": 0.6357}]}, {"metric_name": "detection AUC", "lower_is_better": false, "description": "Area Under the ROC Curve for detection task", "data": [{"dataset_name": "sst2", "final_value": 0.797, "best_value": 0.797}, {"dataset_name": "yelp_polarity", "final_value": 0.7527, "best_value": 0.7527}, {"dataset_name": "imdb", "final_value": 0.7715, "best_value": 0.7715}]}, {"metric_name": "detection DES", "lower_is_better": true, "description": "Detection DES metric for evaluating detection performance", "data": [{"dataset_name": "sst2", "final_value": 0.1328, "best_value": 0.1328}, {"dataset_name": "yelp_polarity", "final_value": 0.1254, "best_value": 0.1254}, {"dataset_name": "imdb", "final_value": 0.1286, "best_value": 0.1286}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "sst2", "final_value": 0.1213, "best_value": 0.1213}, {"dataset_name": "yelp_polarity", "final_value": 0.0483, "best_value": 0.0483}, {"dataset_name": "imdb", "final_value": 0.1247, "best_value": 0.1247}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "sst2", "final_value": 0.3878, "best_value": 0.3878}, {"dataset_name": "yelp_polarity", "final_value": 0.2825, "best_value": 0.2825}, {"dataset_name": "imdb", "final_value": 0.4622, "best_value": 0.4622}]}, {"metric_name": "detection AUC", "lower_is_better": false, "description": "Out-of-distribution detection AUC", "data": [{"dataset_name": "sst2", "final_value": 0.5545, "best_value": 0.5545}, {"dataset_name": "yelp_polarity", "final_value": 0.6007, "best_value": 0.6007}, {"dataset_name": "imdb", "final_value": 0.5057, "best_value": 0.5057}]}, {"metric_name": "detection DES", "lower_is_better": true, "description": "Out-of-distribution detection DES", "data": [{"dataset_name": "sst2", "final_value": 0.1386, "best_value": 0.1386}, {"dataset_name": "yelp_polarity", "final_value": 0.1502, "best_value": 0.1502}, {"dataset_name": "imdb", "final_value": 0.1264, "best_value": 0.1264}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "sst2", "final_value": 0.85, "best_value": 0.85}, {"dataset_name": "yelp_polarity", "final_value": 0.91, "best_value": 0.91}, {"dataset_name": "imdb", "final_value": 0.83, "best_value": 0.83}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "sst2", "final_value": 0.1103, "best_value": 0.1103}, {"dataset_name": "yelp_polarity", "final_value": 0.0519, "best_value": 0.0519}, {"dataset_name": "imdb", "final_value": 0.1127, "best_value": 0.1127}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "sst2", "final_value": 0.5526, "best_value": 0.5526}, {"dataset_name": "yelp_polarity", "final_value": 0.2668, "best_value": 0.2668}, {"dataset_name": "imdb", "final_value": 0.4947, "best_value": 0.4947}]}, {"metric_name": "detection AUC", "lower_is_better": false, "description": "Area under the ROC curve for out-of-distribution detection", "data": [{"dataset_name": "sst2", "final_value": 0.6096, "best_value": 0.6096}, {"dataset_name": "yelp_polarity", "final_value": 0.5668, "best_value": 0.5668}, {"dataset_name": "imdb", "final_value": 0.5646, "best_value": 0.5646}]}, {"metric_name": "detection DES", "lower_is_better": true, "description": "Detection error score (DES) for out-of-distribution detection", "data": [{"dataset_name": "sst2", "final_value": 0.1524, "best_value": 0.1524}, {"dataset_name": "yelp_polarity", "final_value": 0.1417, "best_value": 0.1417}, {"dataset_name": "imdb", "final_value": 0.1411, "best_value": 0.1411}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset", "data": [{"dataset_name": "sst2", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "yelp_polarity", "final_value": 0.91, "best_value": 0.91}, {"dataset_name": "imdb", "final_value": 0.815, "best_value": 0.815}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Training set loss", "data": [{"dataset_name": "sst2", "final_value": 0.1103, "best_value": 0.1103}, {"dataset_name": "yelp_polarity", "final_value": 0.0516, "best_value": 0.0516}, {"dataset_name": "imdb", "final_value": 0.1127, "best_value": 0.1127}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation set loss", "data": [{"dataset_name": "sst2", "final_value": 0.5526, "best_value": 0.5526}, {"dataset_name": "yelp_polarity", "final_value": 0.268, "best_value": 0.268}, {"dataset_name": "imdb", "final_value": 0.4947, "best_value": 0.4947}]}, {"metric_name": "detection AUC", "lower_is_better": false, "description": "Area under the ROC curve for detection", "data": [{"dataset_name": "sst2", "final_value": 0.6096, "best_value": 0.6096}, {"dataset_name": "yelp_polarity", "final_value": 0.5452, "best_value": 0.5452}, {"dataset_name": "imdb", "final_value": 0.5646, "best_value": 0.5646}]}, {"metric_name": "detection DES", "lower_is_better": true, "description": "Detection error score", "data": [{"dataset_name": "sst2", "final_value": 0.1524, "best_value": 0.1524}, {"dataset_name": "yelp_polarity", "final_value": 0.1363, "best_value": 0.1363}, {"dataset_name": "imdb", "final_value": 0.1411, "best_value": 0.1411}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on validation set", "data": [{"dataset_name": "sst2", "final_value": 0.795, "best_value": 0.795}, {"dataset_name": "yelp_polarity", "final_value": 0.91, "best_value": 0.91}, {"dataset_name": "imdb", "final_value": 0.815, "best_value": 0.815}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/sst2_auc_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/sst2_loss_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/synthetic_auc_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/yelp_polarity_loss_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/synthetic_loss_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/yelp_polarity_auc_curve_bs32_lr0.001.png"], [], [], [], [], [], ["../../logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/datasets_val_auc_curves_comparison.png", "../../logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/sst2_loss_curve_bs64_lr0.01.png", "../../logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/sst2_auc_curve_bs64_lr0.01.png", "../../logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/imdb_loss_curve_bs64_lr0.001.png", "../../logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/synthetic_pred_hist_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/synthetic_auc_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/yelp_polarity_loss_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/imdb_auc_curve_bs64_lr0.001.png", "../../logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/synthetic_loss_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/yelp_polarity_auc_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/comparison_best_val_auc.png"], [], [], ["../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/comparison_detection_auc.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_loss_curve.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/imdb_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/imdb_loss_curve.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_loss_curve.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_class_distribution.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/comparison_DES.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_class_distribution.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/imdb_class_distribution.png"], ["../../logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/sst2_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/yelp_polarity_loss_curve.png", "../../logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/imdb_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/imdb_loss_curve.png", "../../logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/yelp_polarity_val_class_distribution.png", "../../logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/imdb_val_class_distribution.png", "../../logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/sst2_loss_curve.png", "../../logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/yelp_polarity_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/sst2_val_class_distribution.png", "../../logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/comparison_final_detection_auc.png"], ["../../logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/sst2_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/yelp_polarity_loss_curve.png", "../../logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/imdb_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/imdb_loss_curve.png", "../../logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/detection_auc_comparison.png", "../../logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/sst2_loss_curve.png", "../../logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/yelp_polarity_detection_auc_curve.png"], [], ["../../logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/sst2_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/comparison_detection_auc.png", "../../logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/yelp_polarity_loss_curve.png", "../../logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/imdb_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/imdb_loss_curve.png", "../../logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/sst2_loss_curve.png", "../../logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/sst2_class_distribution.png", "../../logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/yelp_polarity_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/comparison_DES.png", "../../logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/yelp_polarity_class_distribution.png", "../../logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/imdb_class_distribution.png"], ["../../logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/sst2_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/comparison_detection_auc.png", "../../logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/yelp_polarity_loss_curve.png", "../../logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/imdb_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/imdb_loss_curve.png", "../../logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/sst2_loss_curve.png", "../../logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/sst2_class_distribution.png", "../../logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/yelp_polarity_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/comparison_DES.png", "../../logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/yelp_polarity_class_distribution.png", "../../logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/imdb_class_distribution.png"], ["../../logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/sst2_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/comparison_detection_auc.png", "../../logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/yelp_polarity_loss_curve.png", "../../logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/imdb_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/imdb_loss_curve.png", "../../logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/sst2_loss_curve.png", "../../logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/sst2_class_distribution.png", "../../logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/yelp_polarity_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/comparison_DES.png", "../../logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/yelp_polarity_class_distribution.png", "../../logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/imdb_class_distribution.png"], []], "plot_paths": [["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/sst2_auc_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/sst2_loss_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/synthetic_auc_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/yelp_polarity_loss_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/synthetic_loss_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/yelp_polarity_auc_curve_bs32_lr0.001.png"], [], [], [], [], [], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/datasets_val_auc_curves_comparison.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/sst2_loss_curve_bs64_lr0.01.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/sst2_auc_curve_bs64_lr0.01.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/imdb_loss_curve_bs64_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/synthetic_pred_hist_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/synthetic_auc_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/yelp_polarity_loss_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/imdb_auc_curve_bs64_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/synthetic_loss_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/yelp_polarity_auc_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/comparison_best_val_auc.png"], [], [], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/comparison_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/imdb_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/imdb_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_class_distribution.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/comparison_DES.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_class_distribution.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/imdb_class_distribution.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/sst2_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/yelp_polarity_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/imdb_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/imdb_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/yelp_polarity_val_class_distribution.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/imdb_val_class_distribution.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/sst2_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/yelp_polarity_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/sst2_val_class_distribution.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/comparison_final_detection_auc.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/sst2_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/yelp_polarity_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/imdb_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/imdb_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/detection_auc_comparison.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/sst2_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/yelp_polarity_detection_auc_curve.png"], [], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/sst2_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/comparison_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/yelp_polarity_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/imdb_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/imdb_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/sst2_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/sst2_class_distribution.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/yelp_polarity_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/comparison_DES.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/yelp_polarity_class_distribution.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/imdb_class_distribution.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/sst2_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/comparison_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/yelp_polarity_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/imdb_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/imdb_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/sst2_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/sst2_class_distribution.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/yelp_polarity_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/comparison_DES.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/yelp_polarity_class_distribution.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/imdb_class_distribution.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/sst2_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/comparison_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/yelp_polarity_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/imdb_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/imdb_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/sst2_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/sst2_class_distribution.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/yelp_polarity_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/comparison_DES.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/yelp_polarity_class_distribution.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/imdb_class_distribution.png"], []], "plot_analyses": [[{"analysis": "SST2 AUC shows train performance stuck around 0.45\u20130.46 across 20 epochs, while validation AUC sits consistently near 0.585. This gap suggests the model is underfitting on SST2: it fails to improve embedding discriminability on train data and validation remains only slightly above chance. Adjusting learning rate or batch size may help, but additional regularization or data augmentation could be needed to raise train AUC first.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/sst2_auc_curve_bs32_lr0.001.png"}, {"analysis": "SST2 loss curves indicate smooth, continuous decrease for both train and validation, with validation loss always slightly lower. Neither curve plateaus sharply before epoch 20, but convergence is slow and overall loss remains high (around 0.69). The combination of underfitting AUC and lingering loss suggests that simply training longer may not suffice; consider increasing learning rate or using more aggressive optimizer scheduling.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/sst2_loss_curve_bs32_lr0.001.png"}, {"analysis": "Synthetic dataset AUC jumps from near chance (0.05) to over 0.90 by epoch 4 for both train and validation, then asymptotically approaches ~0.93 (train) and ~0.95 (val). This rapid learning and small generalization gap imply the synthetic task is too easy or too homogeneous. To better stress-test PIU, the synthetic generation process should be made more complex or noisy to avoid trivial memorization.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/synthetic_auc_curve_bs32_lr0.001.png"}, {"analysis": "Yelp Polarity loss curves steadily decrease from ~0.813 to ~0.705 on train and from ~0.785 to ~0.701 on validation over 20 epochs, with nearly parallel trajectories. This steady progress without divergence indicates the current hyperparameters allow balanced learning and no visible overfitting, but final loss remains relatively high. A slight increase in learning rate or modified batch size could accelerate convergence.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/yelp_polarity_loss_curve_bs32_lr0.001.png"}, {"analysis": "Synthetic loss declines smoothly from ~0.76 to ~0.54 on train and from ~0.75 to ~0.55 on validation with minimal gap. The consistency with AUC performance confirms the model fits synthetic data quickly and generalizes nearly perfectly, again suggesting the synthetic task lacks sufficient difficulty to benchmark PIU\u2019s uncertainty estimates in realistic settings.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/synthetic_loss_curve_bs32_lr0.001.png"}, {"analysis": "Yelp Polarity AUC remains flat around 0.405\u20130.415 through epoch 18, then abruptly jumps to ~0.445 for train and ~0.58 for validation at epochs 19\u201320. Such a sudden gain likely points to a metric computation anomaly (e.g. threshold reassignment or validation shuffle) rather than genuine learning. This should be investigated by verifying AUC calculations and keeping random seeds consistent.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/yelp_polarity_auc_curve_bs32_lr0.001.png"}], [], [], [], [], [], [{"analysis": "Validation AUC Curves Comparison Across Datasets shows that the synthetic dataset achieves extremely high and stable validation AUC (~0.95) across all epochs, indicating near-perfect discrimination. Yelp_polarity peaks around 0.60 at epoch 3 then gently declines to ~0.589 by epoch 10, suggesting modest generalization with slight overfitting. IMDb starts at ~0.53, peaks near 0.55 by epoch 2\u20133 then gradually drops to ~0.528, indicating limited predictive power and a risk of overfitting. SST2 remains nearly flat around ~0.46, revealing that the model struggles to learn discriminative patterns on this dataset under the chosen hyperparameters.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/datasets_val_auc_curves_comparison.png"}, {"analysis": "Bar chart of Best Validation AUCs Across Datasets confirms that the synthetic dataset vastly outperforms the real\u2010world benchmarks (best AUC ~0.96). Yelp_polarity (best ~0.60) ranks second, followed by IMDb (~0.55) and SST2 (~0.46). This ranking highlights that the experimental setup excels on controlled synthetic data but faces challenges on diverse natural\u2010language tasks.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/sst2_loss_curve_bs64_lr0.01.png"}, {"analysis": "sst2 Loss Curve (Train vs Validation, bs=64, lr=0.01) reveals that training loss steadily decreases from ~0.688 to ~0.680, while validation loss rises from ~0.696 to ~0.705 and then plateaus. The diverging trends point to overfitting: the model continues to optimize the training objective but degrades on held\u2010out data.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/sst2_auc_curve_bs64_lr0.01.png"}, {"analysis": "sst2 AUC Curve (Train vs Validation, bs=64, lr=0.01) shows training AUC climbing from ~0.525 to ~0.572, whereas validation AUC hovers around ~0.45 with small fluctuations. The widening train-val gap in AUC further underscores overfitting and indicates that the learned representations do not generalize well on SST2 under the current settings.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/imdb_loss_curve_bs64_lr0.001.png"}, {"analysis": "imdb Loss Curve (Train vs Validation, bs=64, lr=0.001) demonstrates that training loss consistently falls from ~0.701 to ~0.691, while validation loss climbs from ~0.695 to ~0.707. The opposing trajectories after epoch 1 suggest overfitting; the model fits training data increasingly well but loses ground on validation.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/synthetic_pred_hist_bs32_lr0.001.png"}, {"analysis": "imdb AUC Curve (Train vs Validation, bs=64, lr=0.001) indicates that training AUC improves monotonically from ~0.467 to ~0.555, while validation AUC peaks at ~0.555 around epoch 3 then steadily declines to ~0.528 by epoch 10. This pattern reveals early gains in generalization followed by overfitting beyond the optimal epoch.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/synthetic_auc_curve_bs32_lr0.001.png"}, {"analysis": "yelp_polarity Loss Curve (Train vs Validation, bs=32, lr=0.001) illustrates that both training and validation loss decrease from ~0.712/0.697 to ~0.684/0.683, with validation loss tracking slightly below training loss. The small, consistent gap and parallel descent suggest good fit and limited overfitting on the Yelp polarity data.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/yelp_polarity_loss_curve_bs32_lr0.001.png"}, {"analysis": "yelp_polarity AUC Curve (Train vs Validation, bs=32, lr=0.001) shows training AUC rising from ~0.435 to ~0.583 and validation AUC increasing from ~0.520 to ~0.595 by epoch 3, then lightly sloping down to ~0.588. The relatively tight train-val AUC gap and high peak at early epochs indicate strong generalization with minor overfitting later.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/imdb_auc_curve_bs64_lr0.001.png"}, {"analysis": "synthetic Loss Curve (Train vs Validation, bs=32, lr=0.001) reveals a smooth, steep decline in both training and validation loss\u2014from ~0.588/0.565 to ~0.391/0.408. Validation closely follows training, with minimal gap, indicating excellent fit and no evident overfitting on the synthetic dataset.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/synthetic_loss_curve_bs32_lr0.001.png"}, {"analysis": "synthetic AUC Curve (Train vs Validation, bs=32, lr=0.001) remains extremely high: validation AUC is flat at ~0.957 and training AUC fluctuates narrowly between ~0.946 and ~0.951. The near\u2010perfect, stable performance and negligible train\u2010val gap confirm strong generalization on synthetic data.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3d8611828c644d8aa3d63a90e789d5db_proc_152740/yelp_polarity_auc_curve_bs32_lr0.001.png"}], [], [], [{"analysis": "Detection AUC for sst2 rises sharply from ~0.51 at epoch 1 to ~0.62 at epoch 2, then makes a modest gain to ~0.63 by epoch 3, showing the uncertainty metric becomes more discriminative with continued training but begins to plateau after epoch 2.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_detection_auc_curve.png"}, {"analysis": "Detection AUC on imdb starts at ~0.52, falls to ~0.48 at epoch 2, and partially recovers to ~0.51 at epoch 3, indicating unstable detection performance and difficulty in distinguishing correct versus incorrect outputs reliably on this dataset.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/comparison_detection_auc.png"}, {"analysis": "Detection AUC for yelp_polarity climbs steadily from ~0.48 at epoch 1 to ~0.544 at epoch 2 and ~0.549 at epoch 3, reflecting consistent improvements though overall performance remains below that seen on sst2.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_loss_curve.png"}, {"analysis": "Training loss for sst2 drops monotonically from ~0.56 to ~0.10 over three epochs, while validation loss decreases from ~0.38 to ~0.33 between epochs 1\u20132 before rising to ~0.44 at epoch 3, signalling the onset of overfitting after epoch 2.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/imdb_detection_auc_curve.png"}, {"analysis": "On imdb, the training curve falls from ~0.58 to ~0.10 by epoch 3; validation loss dips from ~0.41 to ~0.31 at epoch 2 then increases back to ~0.41 at epoch 3, again suggesting optimal generalization around epoch 2 with overfitting thereafter.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/imdb_loss_curve.png"}, {"analysis": "For yelp_polarity, train loss decreases steeply from ~0.46 to ~0.05 by epoch 3; validation loss marginally improves to ~0.235 at epoch 2 then increases to ~0.335 at epoch 3, reinforcing the overfitting trend past the second epoch.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_loss_curve.png"}, {"analysis": "Ground truth on sst2 is balanced (93 negatives vs 107 positives); model predictions shift to 86 negatives vs 114 positives, underpredicting class 0 by ~7 and overpredicting class 1 by ~7, revealing a bias toward positive labels.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_class_distribution.png"}, {"analysis": "On imdb, true counts are 104 negatives vs 96 positives; predictions flip to 86 negatives vs 114 positives, underestimating negatives by ~18 and overestimating positives by ~18, highlighting a strong positive-class bias.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_detection_auc_curve.png"}, {"analysis": "For yelp_polarity, the truth is evenly split (100/100), yet predictions are 85 negatives vs 115 positives, again underpredicting the negative class by ~15 and overpredicting the positive by ~15, consistent with a positive bias.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/comparison_DES.png"}, {"analysis": "Comparing all three datasets, sst2 shows the highest and most consistent detection AUC gains across epochs, yelp_polarity yields moderate but steady improvements, while imdb remains erratic and lags behind, pointing to data-dependent effectiveness of the perturbation-based uncertainty method.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_class_distribution.png"}], [{"analysis": "sst2 Detection AUC Curve shows that KL-based divergence yields substantially higher AUC (around 0.74\u20130.78) compared to simple vote agreement (around 0.63\u20130.68). KL uncertainty improves steadily across epochs, with a slight dip at epoch 3 before climbing to the best reading at epoch 5. Vote uncertainty peaks at epoch 2 then dips at epoch 3 before slowly recovering, suggesting that vote-based detection is less stable over training.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/sst2_detection_auc_curve.png"}, {"analysis": "yelp_polarity Loss Curve reveals steady decrease in training loss (from ~0.26 to ~0.01 across epochs) while validation loss bottoms out at epoch 2 (~0.12) then rises markedly by epoch 5 (~0.21). This divergence indicates overfitting begins as early as epoch 3, suggesting an optimal checkpoint around epoch 2 for best generalization on this dataset.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/yelp_polarity_loss_curve.png"}, {"analysis": "imdb Detection AUC Curve indicates KL divergence provides very strong detection performance (AUC around 0.86\u20130.87) that remains stable across epochs. Vote-based detection is much weaker (AUC ~0.53\u20130.61), peaks at epoch 4, and then drops by epoch 5, pointing to high sensitivity of vote\u2010count uncertainty to model overfitting or calibration drift.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/imdb_detection_auc_curve.png"}, {"analysis": "imdb Loss Curve shows training loss plummets from ~0.38 to ~0.04 by epoch 5, while validation loss declines minimally by epoch 2 (~0.21) then climbs to ~0.40 by epoch 5. Clear overfitting emerges after epoch 2, reinforcing the need for early stopping or regularization for reliable uncertainty estimates on this dataset.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/imdb_loss_curve.png"}, {"analysis": "yelp_polarity Class Distribution highlights a mild bias: ground-truth examples are roughly balanced (~255 vs. ~245), but predictions skew toward class 0 (~275 vs. ~230), indicating a tendency to overpredict the negative class. This imbalance could inflate or deflate the apparent uncertainty detection performance if costs differ by class.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/yelp_polarity_val_class_distribution.png"}, {"analysis": "imdb Class Distribution shows ground-truth is roughly balanced (~260 vs. ~245) but model predictions lean toward class 1 (~230 vs. ~270), i.e. overpredicting the positive sentiment. Such prediction skew may interact with uncertainty scoring, potentially biasing the PIU method\u2019s detection thresholds.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/imdb_val_class_distribution.png"}, {"analysis": "sst2 Loss Curve reveals a similar pattern: rapid training loss decline (0.38\u21920.04) and validation loss falling to ~0.29 at epoch 2 but then rising to ~0.54 by epoch 5. Overfitting begins after epoch 2, indicating that detection metrics evaluated beyond that point may reflect memorization rather than true uncertainty calibration.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/sst2_loss_curve.png"}, {"analysis": "yelp_polarity Detection AUC Curve confirms KL\u2010based scoring outperforms vote\u2010based scoring by a large margin (KL ~0.90\u21920.85 vs. Vote ~0.65\u21920.60). Both methods dip at epoch 3 but KL recovers more strongly. Vote\u2010based AUC suffers volatility through training, while KL\u2010based remains high and more robust to overfitting.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/yelp_polarity_detection_auc_curve.png"}, {"analysis": "sst2 Class Distribution uncovers that the model overpredicts the positive class (predicted ~300 vs. actual ~260) and underpredicts the negative class (~200 vs. ~240). This systematic bias toward positive sentiment could influence the calibration of uncertainty thresholds and suggest the need for class\u2010balanced calibration steps.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/sst2_val_class_distribution.png"}, {"analysis": "Final Detection AUC Comparison Across Datasets shows consistent trends: KL\u2010divergence scoring yields the best hallucination detection (AUC ~0.78 on sst2, ~0.90 on yelp_polarity, ~0.86 on imdb), whereas vote\u2010agreement scoring trails significantly (AUC ~0.68, ~0.63, ~0.54 respectively). The gap is largest on yelp_polarity, underscoring that token\u2010level divergence captures uncertainty much more reliably than simple majority\u2010vote agreement across paraphrase ensembles.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2d32a622874442d19fddaa848b7f6367_proc_152740/comparison_final_detection_auc.png"}], [{"analysis": "Detection AUC on sst2 climbs modestly from epoch 1 to 2, then dips sharply at epoch 3 before recovering by epoch 4. The peak performance occurs at epoch 4 (~0.797), but the volatility suggests sensitivity to training duration or overfitting dynamics in the underlying uncertainty detector.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/sst2_detection_auc_curve.png"}, {"analysis": "For yelp_polarity, training loss falls rapidly across epochs, while validation loss steadily rises. This divergence indicates severe overfitting: the detector fits the training set well but loses generalization capability almost immediately after epoch 1.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/yelp_polarity_loss_curve.png"}, {"analysis": "Detection AUC on imdb steadily improves each epoch, rising from ~0.684 at epoch 1 to ~0.772 at epoch 4. Gains taper off by epoch 3\u21924, suggesting diminishing returns after the third pass and a potential sweet spot around epoch 3 for efficient training.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/imdb_detection_auc_curve.png"}, {"analysis": "On imdb, training loss declines sharply, but validation loss first dips slightly by epoch 2 then surges at epochs 3 and 4. This pattern confirms strong overfitting beyond epoch 2, with the detector memorizing training examples rather than learning robust uncertainty signals.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/imdb_loss_curve.png"}, {"analysis": "When comparing across sst2, yelp_polarity, and imdb, sst2 remains highest and most stable (\u22480.79\u20130.80). Yelp_polarity and imdb start lower but catch up by epoch 3 (\u22480.762 and 0.769, respectively). After epoch 3, imdb still gains marginally, whereas yelp_polarity shows a slight decline by epoch 4. All datasets show an inflection at epoch 3.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/detection_auc_comparison.png"}, {"analysis": "On sst2, training loss drops quickly, but validation loss spikes at epoch 2, dips at epoch 3, then rises again at epoch 4. This fluctuation\u2014together with the earlier AUC plot\u2014signals inconsistent generalization across epochs and suggests overfitting risks after the first pass.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/sst2_loss_curve.png"}, {"analysis": "Detection AUC on yelp_polarity increases from ~0.713 at epoch 1 to ~0.762 at epoch 3, then slightly falls to ~0.753 at epoch 4. The best detection performance is at epoch 3, aligning with other datasets\u2019 optimal points and reinforcing a three-epoch early-stopping rule.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_3eea63314168495d85defcf182c74688_proc_152739/yelp_polarity_detection_auc_curve.png"}], [], [{"analysis": "sst2 Loss Curve shows train loss steadily decreasing across epochs while validation loss initially rises from epoch one to two before falling at epoch three. This suggests the model is improving on training data but exhibits slight overfitting around epoch two before some recovery at epoch three. A possible sweet spot for generalization could be at epoch three, given the lowest validation loss at that point.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/sst2_detection_auc_curve.png"}, {"analysis": "imdb Loss Curve indicates strong reduction in training loss over epochs but a continuous increase in validation loss. This pattern is hallmark overfitting: although the model learns the training samples well, its performance on held-out data degrades after the first epoch. Early stopping around epoch one or two might yield better validation generalization.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/comparison_detection_auc.png"}, {"analysis": "yelp_polarity Loss Curve reveals a sharp drop in training loss and a marginal decrease in validation loss from epoch one to two, followed by a validation loss increase at epoch three. The lowest validation loss appears at epoch two, implying that training beyond two epochs introduces overfitting without further validation gains.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/yelp_polarity_loss_curve.png"}, {"analysis": "sst2 Detection AUC Curve peaks at epoch two before dropping at epoch three. The rise from epoch one to two indicates that uncertainty estimates become more discriminative as the model gains capacity, but further epochs lead to diminishing detection performance, likely due to over-confident, less divergent outputs on paraphrased prompts.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/imdb_detection_auc_curve.png"}, {"analysis": "imdb Detection AUC Curve follows a similar trend: improvement from epoch one to two with a peak at around 0.58, then a sharp decline by epoch three. This suggests that the model\u2019s uncertainty signals are most informative at an intermediate training stage before overfitting dampens output divergence.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/imdb_loss_curve.png"}, {"analysis": "yelp_polarity Detection AUC Curve steadily increases across epochs, reaching its highest value at epoch three. Unlike the other two datasets, deeper training continues to enhance the quality of uncertainty estimates, possibly because the model benefits from additional signal without severe overfitting on this dataset.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/sst2_loss_curve.png"}, {"analysis": "Comparison of Detection AUC Across Datasets highlights differing optimal epochs per task: epoch two is best for sst2 and imdb, whereas epoch three suits yelp_polarity. Detection AUC varies by dataset and training stage, underscoring the need for dataset\u2010specific early stopping when tuning perturbation-induced uncertainty.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/sst2_class_distribution.png"}, {"analysis": "sst2 Class Distribution: GT vs Predictions shows ground truth split roughly 93 for class 0 and 107 for class 1, while predictions shift to about 103 for class 0 and 97 for class 1. The model slightly overpredicts the negative class relative to the true distribution, indicating a mild negative bias.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/yelp_polarity_detection_auc_curve.png"}, {"analysis": "imdb Class Distribution: GT vs Predictions compares ground truth (approximately 105 negatives vs 95 positives) to model outputs (around 84 negatives vs 116 positives). The model overpredicts positive reviews, suggesting a bias toward the positive sentiment class at inference time.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/comparison_DES.png"}, {"analysis": "yelp_polarity Class Distribution: GT vs Predictions contrasts equal ground truth counts (100 each) with predictions that are close to balanced (about 98 negatives vs 102 positives). This indicates the model maintains near-uniform class coverage on this dataset, with only slight positive skew.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/yelp_polarity_class_distribution.png"}], [{"analysis": "Detection AUC for SST2, Yelp, and IMDB steadily increases from epoch 1 to epoch 3. SST2 starts at roughly 0.49, dips slightly by epoch 2, then jumps to about 0.61 by epoch 3. Yelp polarity drops from 0.526 to 0.510 at epoch 2 before rising to approximately 0.567 at epoch 3. IMDB shows a smoother monotonic increase from 0.50 to around 0.565. This suggests that training improves the hallucination detection capability most dramatically for SST2, with Yelp benefiting after a short plateau and IMDB improving linearly.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/sst2_detection_auc_curve.png"}, {"analysis": "The Divergence Ensemble Score (DES) trends mirror the AUC curves. SST2\u2019s DES grows from about 0.121 at epoch 1 to 0.152 by epoch 3, indicating that the model\u2019s output divergence under perturbations becomes a stronger uncertainty signal over time. Yelp polarity\u2019s DES dips slightly at epoch 2 (around 0.128) before climbing to ~0.142. IMDB\u2019s DES rises steadily from 0.125 to roughly 0.141. This confirms that larger output variance correlates with improved detection performance.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/comparison_detection_auc.png"}, {"analysis": "SST2\u2019s standalone AUC curve shows modest initial performance (\u22480.486), moderate gain by epoch 2 (\u22480.526), and a substantial jump by epoch 3 (\u22480.610). The acceleration in later epochs implies the model quickly learns to distinguish hallucinated vs. correct answers once sufficient pattern recognition is in place.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/yelp_polarity_loss_curve.png"}, {"analysis": "IMDB\u2019s detection AUC rises in a nearly linear fashion from 0.50 at epoch 1, through \u22480.535 at epoch 2, up to about 0.565 at epoch 3. The steady slope indicates consistent gains without abrupt shifts, suggesting stable learning dynamics for this dataset.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/imdb_detection_auc_curve.png"}, {"analysis": "Yelp polarity\u2019s detection AUC curve falls from about 0.525 at epoch 1 to roughly 0.510 at epoch 2, then leaps to nearly 0.567 at epoch 3. The dip at epoch 2 may reflect temporary overfitting or sensitivity to perturbation quality, followed by recovery with further weight updates.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/imdb_loss_curve.png"}, {"analysis": "SST2 train vs. validation loss: training loss drops from ~0.59 to ~0.11 over three epochs, whereas validation loss decreases from ~0.39 to ~0.35 by epoch 2 then surges to ~0.55 at epoch 3. This crossover indicates overfitting kicks in sharply after epoch 2, warranting early stopping or regularization around that point.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/sst2_loss_curve.png"}, {"analysis": "IMDB train vs. validation loss: training loss decreases steadily from ~0.51 to ~0.11; validation loss climbs from ~0.42 at epoch 1 to ~0.48 at epoch 2 and ~0.495 at epoch 3. The continuous gap and upward trend in validation loss signal overfitting beginning as early as epoch 2.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/sst2_class_distribution.png"}, {"analysis": "Yelp polarity train vs. validation loss: training loss plummets from ~0.45 to ~0.05 by epoch 3, yet validation loss steadily rises from ~0.225 to ~0.265. The divergence from epoch 1 onward indicates strong overfitting, suggesting the model memorizes training idiosyncrasies too quickly for Yelp.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/yelp_polarity_detection_auc_curve.png"}, {"analysis": "IMDB class distribution: ground truth has ~104 negatives (Class 0) and ~96 positives (Class 1). Predictions skew toward Class 1 with ~76 negatives predicted versus ~125 positives. The bias toward positive sentiment could degrade true negative detection and inflate false positive rates.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/comparison_DES.png"}, {"analysis": "Yelp polarity class distribution: ground truth is balanced at 100/100 for both classes. Predictions show a slight tilt to Class 1 (~102) over Class 0 (~98), a minor bias compared to IMDB. The near parity suggests balanced calibration, though fine\u2010tuning could reduce the small positive skew.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/yelp_polarity_class_distribution.png"}], [{"analysis": "Comparison across all three datasets shows that detection AUC improves over epochs for sst2, yelp_polarity, and imdb. sst2 starts lower (~0.485) but achieves the highest final AUC (~0.61), indicating that additional training greatly benefits detection on shorter sentiment inputs. yelp_polarity dips at epoch 2 (from ~0.525 to ~0.51) before recovering to ~0.545, suggesting some instability or sensitivity to underfitting at intermediate stages. imdb steadily climbs from 0.50 to ~0.565, reflecting consistent gains on longer-form sentiment data.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/sst2_detection_auc_curve.png"}], []], "vlm_feedback_summary": ["Overall, experiments reveal underfitting on real sentiment tasks (SST2, Yelp)\nand trivial performance on the synthetic dataset. Loss curves decrease but AUC\non SST2 and Yelp stays near chance until a suspicious late jump. I recommend: 1)\ntuning learning rate schedules (e.g. warm restarts or cosine decay), 2)\nexploring data augmentations or paraphrase diversity to challenge the synthetic\ntask, and 3) investigating metric calculation on Yelp. For broader evaluation,\nincorporate HuggingFace\u2019s \u201cimdb\u201d and \u201cag_news\u201d datasets to test generalization\nacross different text classification scenarios.", "[]", "[]", "[]", "[]", "[]", "Overall, the model excels on synthetic data with near\u2010perfect AUC and minimal\noverfitting. Real\u2010world tasks vary: SST2 shows pronounced overfitting and low\nAUC, IMDb exhibits moderate overfitting with declining validation performance\nafter early epochs, and Yelp_Polarity achieves the best generalization among\nreal datasets with healthy train\u2010val alignment. Consider earlier stopping, data\naugmentation or hyperparameter tuning (e.g., lower lr for SST2, larger batch for\nIMDb) to improve real\u2010world generalization.", "[]", "[]", "Detection performance improves clearly for sst2 and yelp_polarity, peaking at\nepoch 3 despite overfitting signals in validation loss. Prediction distributions\nreveal a systematic bias toward positive labels across all tasks. imdb exhibits\nunstable detection and lower AUC overall, suggesting dataset-specific challenges\nthat merit further investigation.", "KL\u2010based perturbation divergence consistently outperforms vote\u2010agreement for\nhallucination detection across SST2, Yelp, and IMDB. Validation losses rise\nafter epoch 2 in all classification tasks, indicating overfitting; early\nstopping around epoch 2\u20133 would likely yield better generalization. Class\ndistributions reveal prediction biases that could affect uncertainty thresholds.\nOverall, token\u2010level divergence on perturbed prompts is a robust, model\u2010agnostic\nuncertainty metric compared to simple voting.", "Overall, uncertainty detection performance improves in early epochs but\noverfitting emerges quickly: val losses rise even as train losses fall.\nDetection AUC gains plateau around epoch 3 across datasets, with sst2 highest\nand most stable. Early stopping at epoch 3 balances performance and\ngeneralization for yelp_polarity and imdb, while sst2 shows more volatility.\nFine-tuning training schedules and adding regularization may mitigate\noverfitting.", "[]", "Loss curves across three sentiment datasets reveal overfitting trends at later\nepochs, with optimal validation performance at epoch three for SST-2, epoch one\nor two for IMDB, and epoch two for Yelp. Detection AUC peaks align with these\nturning points, suggesting that perturbation-induced uncertainty is most\neffective before overfitting reduces output diversity. Class distribution bar\ncharts uncover dataset-specific biases: slight negative bias on SST-2, positive\nbias on IMDB, and balanced predictions on Yelp.", "Plots show consistent improvement in detection AUC and DES across epochs, with\nSST2 benefiting most dramatically. Overfitting appears by epoch 2 (IMDb, SST2)\nand epoch 1 (Yelp) per loss curves, calling for early stopping or stronger\nregularization. Class distributions reveal a stronger positive bias on IMDB\npredictions, while Yelp remains nearly balanced. The analysis supports PIU\u2019s\npremise that output divergence correlates with detection performance but\nhighlights the need for tighter training control and bias mitigation.", "Overall, all three datasets show improved hallucination detection performance\nwith more training, but they differ in stability and susceptibility to\noverfitting. sst2 benefits most from extra epochs, yelp_polarity experiences a\nmid-training dip, and imdb yields steady progress.", "[]"], "exec_time": [38.09962868690491, 3.0148489475250244, 3.87892746925354, 43.924744606018066, 148.2925581932068, 62.38477969169617, 50.219940423965454, 2.630307197570801, 12.334680080413818, 159.76402926445007, 1430.681279182434, 516.4877836704254, 25.292816638946533, 150.64093112945557, 151.56645584106445, 151.08597874641418, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "Implementation works but runs too quickly (2.66 minutes).We have up to 60\nminutes available for each experiment.Make sure to scale up the experiment by\nincreasing the number of epochs, using a larger model, or working with bigger\ndatasets.Given that the current execution time is {exec_time_minutes:.2f}\nminutes, think about how changing the number of epochs to run, or using a larger\nmodel, or working with bigger datasets to runwill affect the execution time, and\nmake sure to scale up the experiment accordingly.", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['Synthetic']"], [], [], [], [], [], ["['synthetic'", "'yelp_polarity']"], [], [], ["sst2", "yelp_polarity"], ["[sst2", "yelp_polarity", "imdb]"], ["['yelp_polarity'", "'imdb']"], [], ["sst2", "imdb", "yelp_polarity"], ["[SST2", "Yelp polarity", "IMDB]"], ["[sst2", "yelp_polarity", "imdb]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    for dataset, exp in experiment_data.items():\n        # find best hyperparams by max validation AUC\n        best_entry = max(exp[\"metrics\"][\"val\"], key=lambda x: x[\"auc\"])\n        bs, lr = best_entry[\"bs\"], best_entry[\"lr\"]\n        print(f\"{dataset}: Best Val AUC = {best_entry['auc']:.4f} at bs={bs}, lr={lr}\")\n        # collect losses\n        loss_train = [\n            (d[\"epoch\"], d[\"loss\"])\n            for d in exp[\"losses\"][\"train\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        loss_val = [\n            (d[\"epoch\"], d[\"loss\"])\n            for d in exp[\"losses\"][\"val\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        loss_train.sort()\n        loss_val.sort()\n        epochs = [e for e, _ in loss_train]\n        tr_loss = [l for _, l in loss_train]\n        vl_loss = [l for _, l in loss_val]\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, vl_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dataset} Loss Curve (Train vs Validation)\\nbs={bs}, lr={lr}\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset}_loss_curve_bs{bs}_lr{lr}.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {dataset}: {e}\")\n            plt.close()\n        # collect AUCs\n        auc_train = [\n            (d[\"epoch\"], d[\"auc\"])\n            for d in exp[\"metrics\"][\"train\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        auc_val = [\n            (d[\"epoch\"], d[\"auc\"])\n            for d in exp[\"metrics\"][\"val\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        auc_train.sort()\n        auc_val.sort()\n        epochs = [e for e, _ in auc_train]\n        tr_auc = [a for _, a in auc_train]\n        vl_auc = [a for _, a in auc_val]\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_auc, label=\"Train AUC\")\n            plt.plot(epochs, vl_auc, label=\"Val AUC\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"AUC\")\n            plt.title(f\"{dataset} AUC Curve (Train vs Validation)\\nbs={bs}, lr={lr}\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset}_auc_curve_bs{bs}_lr{lr}.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating AUC plot for {dataset}: {e}\")\n            plt.close()\n", null, null, null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    best_aucs = {}\n    for dataset, exp in experiment_data.items():\n        # find best hyperparams by val AUC\n        best_entry = max(exp[\"metrics\"][\"val\"], key=lambda x: x[\"auc\"])\n        bs, lr = best_entry[\"bs\"], best_entry[\"lr\"]\n        best_aucs[dataset] = best_entry[\"auc\"]\n        print(f\"{dataset}: Best Val AUC = {best_entry['auc']:.4f} at bs={bs}, lr={lr}\")\n        # collect losses\n        loss_train = [\n            (d[\"epoch\"], d[\"loss\"])\n            for d in exp[\"losses\"][\"train\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        loss_val = [\n            (d[\"epoch\"], d[\"loss\"])\n            for d in exp[\"losses\"][\"val\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        loss_train.sort()\n        loss_val.sort()\n        epochs = [e for e, _ in loss_train]\n        tr_loss = [l for _, l in loss_train]\n        vl_loss = [l for _, l in loss_val]\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, vl_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dataset} Loss Curve (Train vs Validation)\\nbs={bs}, lr={lr}\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset}_loss_curve_bs{bs}_lr{lr}.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {dataset}: {e}\")\n            plt.close()\n        # collect AUCs\n        auc_train = [\n            (d[\"epoch\"], d[\"auc\"])\n            for d in exp[\"metrics\"][\"train\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        auc_val = [\n            (d[\"epoch\"], d[\"auc\"])\n            for d in exp[\"metrics\"][\"val\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        auc_train.sort()\n        auc_val.sort()\n        epochs = [e for e, _ in auc_train]\n        tr_auc = [a for _, a in auc_train]\n        vl_auc = [a for _, a in auc_val]\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_auc, label=\"Train AUC\")\n            plt.plot(epochs, vl_auc, label=\"Val AUC\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"AUC\")\n            plt.title(f\"{dataset} AUC Curve (Train vs Validation)\\nbs={bs}, lr={lr}\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset}_auc_curve_bs{bs}_lr{lr}.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating AUC plot for {dataset}: {e}\")\n            plt.close()\n        # dataset-specific: synthetic prediction histogram\n        if dataset == \"synthetic\":\n            last_epoch = max(\n                d[\"epoch\"]\n                for d in exp[\"predictions\"]\n                if d[\"bs\"] == bs and d[\"lr\"] == lr\n            )\n            preds = next(\n                d[\"preds\"]\n                for d in exp[\"predictions\"]\n                if d[\"bs\"] == bs and d[\"lr\"] == lr and d[\"epoch\"] == last_epoch\n            )\n            labels = next(\n                d[\"labels\"]\n                for d in exp[\"ground_truth\"]\n                if d[\"bs\"] == bs and d[\"lr\"] == lr and d[\"epoch\"] == last_epoch\n            )\n            try:\n                plt.figure()\n                plt.hist(\n                    [preds[labels == 0], preds[labels == 1]],\n                    bins=20,\n                    label=[\"Class 0\", \"Class 1\"],\n                    alpha=0.7,\n                )\n                plt.xlabel(\"Predicted Probability\")\n                plt.ylabel(\"Count\")\n                plt.title(\"Synthetic Dataset Predictions\\nHistogram by True Class\")\n                plt.legend()\n                plt.savefig(\n                    os.path.join(working_dir, f\"{dataset}_pred_hist_bs{bs}_lr{lr}.png\")\n                )\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating prediction histogram for {dataset}: {e}\")\n                plt.close()\n    # comparison bar chart of best val AUCs\n    try:\n        plt.figure()\n        ds = list(best_aucs.keys())\n        vals = [best_aucs[d] for d in ds]\n        plt.bar(ds, vals)\n        plt.ylabel(\"Best Val AUC\")\n        plt.title(\"Comparison of Best Validation AUCs\\nAcross Datasets\")\n        plt.savefig(os.path.join(working_dir, \"comparison_best_val_auc.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating comparison AUC bar chart: {e}\")\n        plt.close()\n    # overlay validation AUC curves across datasets\n    try:\n        plt.figure()\n        for dataset, exp in experiment_data.items():\n            best = max(exp[\"metrics\"][\"val\"], key=lambda x: x[\"auc\"])\n            bs, lr = best[\"bs\"], best[\"lr\"]\n            vals = sorted(\n                [\n                    (d[\"epoch\"], d[\"auc\"])\n                    for d in exp[\"metrics\"][\"val\"]\n                    if d[\"bs\"] == bs and d[\"lr\"] == lr\n                ]\n            )\n            epochs = [e for e, _ in vals]\n            aucs = [a for _, a in vals]\n            plt.plot(epochs, aucs, label=dataset)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Val AUC\")\n        plt.title(\"Validation AUC Curves\\nComparison Across Datasets\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"datasets_val_auc_curves_comparison.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating comparison AUC curves plot: {e}\")\n        plt.close()\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    # Per\u2010dataset visualizations\n    for dataset, exp in experiment_data.items():\n        # Loss curves\n        try:\n            epochs = [d[\"epoch\"] for d in exp[\"losses\"][\"train\"]]\n            tr_loss = [d[\"loss\"] for d in exp[\"losses\"][\"train\"]]\n            vl_loss = [d[\"loss\"] for d in exp[\"losses\"][\"val\"]]\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, vl_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dataset} Loss Curve (Train vs Validation)\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_loss_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {dataset}: {e}\")\n            plt.close()\n        # Detection AUC curve\n        try:\n            epochs = [d[\"epoch\"] for d in exp[\"metrics\"][\"detection\"]]\n            auc = [d[\"auc\"] for d in exp[\"metrics\"][\"detection\"]]\n            plt.figure()\n            plt.plot(epochs, auc, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Detection AUC\")\n            plt.title(f\"{dataset} Detection AUC Curve\")\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_detection_auc_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating detection AUC plot for {dataset}: {e}\")\n            plt.close()\n        # Class distribution vs ground truth\n        try:\n            preds = exp[\"predictions\"]\n            gt = list(exp[\"ground_truth\"])\n            counts_pred = [preds.count(0), preds.count(1)]\n            counts_gt = [gt.count(0), gt.count(1)]\n            x = np.arange(2)\n            width = 0.35\n            plt.figure()\n            plt.bar(x - width / 2, counts_gt, width, label=\"Ground Truth\")\n            plt.bar(x + width / 2, counts_pred, width, label=\"Predictions\")\n            plt.xticks(x, [\"Class 0\", \"Class 1\"])\n            plt.ylabel(\"Count\")\n            plt.title(f\"{dataset} Class Distribution: GT vs Preds\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_class_distribution.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating class distribution plot for {dataset}: {e}\")\n            plt.close()\n\n    # Combined detection AUC comparison\n    try:\n        plt.figure()\n        for dataset, exp in experiment_data.items():\n            epochs = [d[\"epoch\"] for d in exp[\"metrics\"][\"detection\"]]\n            auc = [d[\"auc\"] for d in exp[\"metrics\"][\"detection\"]]\n            plt.plot(epochs, auc, marker=\"o\", label=dataset)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Detection AUC\")\n        plt.title(\"Comparison of Detection AUC Across Datasets\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"comparison_detection_auc.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating combined detection AUC plot: {e}\")\n        plt.close()\n\n    # Combined DES comparison\n    try:\n        plt.figure()\n        for dataset, exp in experiment_data.items():\n            epochs = [d[\"epoch\"] for d in exp[\"metrics\"][\"detection\"]]\n            des = [d[\"DES\"] for d in exp[\"metrics\"][\"detection\"]]\n            plt.plot(epochs, des, marker=\"o\", label=dataset)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"DES\")\n        plt.title(\"Comparison of DES Across Datasets\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"comparison_DES.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating combined DES plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    # Per-dataset visualizations\n    for dataset, exp in experiment_data.items():\n        # Loss curves\n        try:\n            plt.figure()\n            epochs = [d[\"epoch\"] for d in exp[\"losses\"][\"train\"]]\n            tr_loss = [d[\"loss\"] for d in exp[\"losses\"][\"train\"]]\n            vl_loss = [d[\"loss\"] for d in exp[\"losses\"][\"val\"]]\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, vl_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dataset} Loss Curve\\nTrain vs Validation Loss\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_loss_curve.png\"))\n        except Exception as e:\n            print(f\"Error creating loss plot for {dataset}: {e}\")\n        finally:\n            plt.close()\n\n        # Detection AUC curves\n        try:\n            plt.figure()\n            det = exp[\"metrics\"][\"detection\"]\n            epochs = [d[\"epoch\"] for d in det]\n            auc_v = [d[\"auc_vote\"] for d in det]\n            auc_k = [d[\"auc_kl\"] for d in det]\n            plt.plot(epochs, auc_v, label=\"Vote AUC\")\n            plt.plot(epochs, auc_k, label=\"KL AUC\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"AUC\")\n            plt.title(f\"{dataset} Detection AUC Curve\\nLeft: Vote, Right: KL\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_detection_auc_curve.png\"))\n        except Exception as e:\n            print(f\"Error creating detection AUC plot for {dataset}: {e}\")\n        finally:\n            plt.close()\n\n        # Class distribution bar chart\n        try:\n            plt.figure()\n            gt = np.array(exp[\"ground_truth\"])\n            preds = np.array(exp[\"predictions\"])\n            classes = sorted(set(np.concatenate((gt, preds))))\n            counts_gt = [np.sum(gt == c) for c in classes]\n            counts_pred = [np.sum(preds == c) for c in classes]\n            x = np.arange(len(classes))\n            width = 0.35\n            plt.bar(x - width / 2, counts_gt, width, label=\"Ground Truth\")\n            plt.bar(x + width / 2, counts_pred, width, label=\"Predicted\")\n            plt.xticks(x, [str(c) for c in classes])\n            plt.xlabel(\"Class\")\n            plt.ylabel(\"Count\")\n            plt.title(\n                f\"{dataset} Class Distribution\\nLeft: Ground Truth, Right: Predicted\"\n            )\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset}_val_class_distribution.png\")\n            )\n        except Exception as e:\n            print(f\"Error creating class distribution plot for {dataset}: {e}\")\n        finally:\n            plt.close()\n\n    # Comparison across datasets\n    try:\n        plt.figure()\n        datasets = list(experiment_data.keys())\n        final_vote = [\n            experiment_data[d][\"metrics\"][\"detection\"][-1][\"auc_vote\"] for d in datasets\n        ]\n        final_kl = [\n            experiment_data[d][\"metrics\"][\"detection\"][-1][\"auc_kl\"] for d in datasets\n        ]\n        x = np.arange(len(datasets))\n        width = 0.35\n        plt.bar(x - width / 2, final_vote, width, label=\"Vote AUC\")\n        plt.bar(x + width / 2, final_kl, width, label=\"KL AUC\")\n        plt.xticks(x, datasets)\n        plt.xlabel(\"Dataset\")\n        plt.ylabel(\"AUC\")\n        plt.title(\n            \"Final Detection AUC Comparison Across Datasets\\nLeft: Vote, Right: KL Across Datasets\"\n        )\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"comparison_final_detection_auc.png\"))\n    except Exception as e:\n        print(f\"Error creating comparison plot: {e}\")\n    finally:\n        plt.close()\n\n    # Print final detection metrics\n    for dataset, exp in experiment_data.items():\n        final = exp[\"metrics\"][\"detection\"][-1]\n        print(\n            f\"{dataset}: Final Detection AUC_vote={final['auc_vote']:.4f}, AUC_kl={final['auc_kl']:.4f}\"\n        )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    # Print final detection AUC for each dataset\n    for dataset, exp in experiment_data.items():\n        try:\n            final_auc = exp[\"metrics\"][\"detection\"][-1][\"auc\"]\n            print(f\"{dataset}: Final Detection AUC = {final_auc:.4f}\")\n        except Exception as e:\n            print(f\"Error retrieving final AUC for {dataset}: {e}\")\n    # Per-dataset plots\n    for dataset, exp in experiment_data.items():\n        # Loss curves\n        try:\n            epochs = [d[\"epoch\"] for d in exp[\"losses\"][\"train\"]]\n            tr_loss = [d[\"loss\"] for d in exp[\"losses\"][\"train\"]]\n            vl_loss = [d[\"loss\"] for d in exp[\"losses\"][\"val\"]]\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, vl_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"Train vs Validation Loss\\nDataset: {dataset}\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_loss_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {dataset}: {e}\")\n            plt.close()\n        # Detection AUC curves\n        try:\n            epochs = [d[\"epoch\"] for d in exp[\"metrics\"][\"detection\"]]\n            aucs = [d[\"auc\"] for d in exp[\"metrics\"][\"detection\"]]\n            plt.figure()\n            plt.plot(epochs, aucs, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Detection AUC\")\n            plt.title(f\"Detection AUC over Epochs\\nDataset: {dataset}\")\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_detection_auc_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating detection AUC plot for {dataset}: {e}\")\n            plt.close()\n    # Comparison plot across datasets\n    try:\n        plt.figure()\n        for dataset, exp in experiment_data.items():\n            epochs = [d[\"epoch\"] for d in exp[\"metrics\"][\"detection\"]]\n            aucs = [d[\"auc\"] for d in exp[\"metrics\"][\"detection\"]]\n            plt.plot(epochs, aucs, marker=\"o\", label=dataset)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Detection AUC\")\n        plt.title(\"Detection AUC Comparison\\nAcross Datasets\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"detection_auc_comparison.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating comparison plot: {e}\")\n        plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    # Per\u2010dataset visualizations\n    for dataset, exp in experiment_data.items():\n        # Loss curves\n        try:\n            epochs = [d[\"epoch\"] for d in exp[\"losses\"][\"train\"]]\n            tr_loss = [d[\"loss\"] for d in exp[\"losses\"][\"train\"]]\n            vl_loss = [d[\"loss\"] for d in exp[\"losses\"][\"val\"]]\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, vl_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dataset} Loss Curve (Train vs Validation)\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_loss_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {dataset}: {e}\")\n            plt.close()\n        # Detection AUC curve\n        try:\n            epochs = [d[\"epoch\"] for d in exp[\"metrics\"][\"detection\"]]\n            auc = [d[\"auc\"] for d in exp[\"metrics\"][\"detection\"]]\n            plt.figure()\n            plt.plot(epochs, auc, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Detection AUC\")\n            plt.title(f\"{dataset} Detection AUC Curve\")\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_detection_auc_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating detection AUC plot for {dataset}: {e}\")\n            plt.close()\n        # Class distribution vs ground truth\n        try:\n            preds = exp[\"predictions\"]\n            gt = list(exp[\"ground_truth\"])\n            counts_pred = [preds.count(0), preds.count(1)]\n            counts_gt = [gt.count(0), gt.count(1)]\n            x = np.arange(2)\n            width = 0.35\n            plt.figure()\n            plt.bar(x - width / 2, counts_gt, width, label=\"Ground Truth\")\n            plt.bar(x + width / 2, counts_pred, width, label=\"Predictions\")\n            plt.xticks(x, [\"Class 0\", \"Class 1\"])\n            plt.ylabel(\"Count\")\n            plt.title(f\"{dataset} Class Distribution: GT vs Preds\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_class_distribution.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating class distribution plot for {dataset}: {e}\")\n            plt.close()\n\n    # Combined detection AUC comparison\n    try:\n        plt.figure()\n        for dataset, exp in experiment_data.items():\n            epochs = [d[\"epoch\"] for d in exp[\"metrics\"][\"detection\"]]\n            auc = [d[\"auc\"] for d in exp[\"metrics\"][\"detection\"]]\n            plt.plot(epochs, auc, marker=\"o\", label=dataset)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Detection AUC\")\n        plt.title(\"Comparison of Detection AUC Across Datasets\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"comparison_detection_auc.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating combined detection AUC plot: {e}\")\n        plt.close()\n\n    # Combined DES comparison\n    try:\n        plt.figure()\n        for dataset, exp in experiment_data.items():\n            epochs = [d[\"epoch\"] for d in exp[\"metrics\"][\"detection\"]]\n            des = [d[\"DES\"] for d in exp[\"metrics\"][\"detection\"]]\n            plt.plot(epochs, des, marker=\"o\", label=dataset)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"DES\")\n        plt.title(\"Comparison of DES Across Datasets\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"comparison_DES.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating combined DES plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    # Per\u2010dataset visualizations\n    for dataset, exp in experiment_data.items():\n        # Loss curves\n        try:\n            epochs = [d[\"epoch\"] for d in exp[\"losses\"][\"train\"]]\n            tr_loss = [d[\"loss\"] for d in exp[\"losses\"][\"train\"]]\n            vl_loss = [d[\"loss\"] for d in exp[\"losses\"][\"val\"]]\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, vl_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dataset} Loss Curve (Train vs Validation)\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_loss_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {dataset}: {e}\")\n            plt.close()\n        # Detection AUC curve\n        try:\n            epochs = [d[\"epoch\"] for d in exp[\"metrics\"][\"detection\"]]\n            auc = [d[\"auc\"] for d in exp[\"metrics\"][\"detection\"]]\n            plt.figure()\n            plt.plot(epochs, auc, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Detection AUC\")\n            plt.title(f\"{dataset} Detection AUC Curve\")\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_detection_auc_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating detection AUC plot for {dataset}: {e}\")\n            plt.close()\n        # Class distribution vs ground truth\n        try:\n            preds = exp[\"predictions\"]\n            gt = list(exp[\"ground_truth\"])\n            counts_pred = [preds.count(0), preds.count(1)]\n            counts_gt = [gt.count(0), gt.count(1)]\n            x = np.arange(2)\n            width = 0.35\n            plt.figure()\n            plt.bar(x - width / 2, counts_gt, width, label=\"Ground Truth\")\n            plt.bar(x + width / 2, counts_pred, width, label=\"Predictions\")\n            plt.xticks(x, [\"Class 0\", \"Class 1\"])\n            plt.ylabel(\"Count\")\n            plt.title(f\"{dataset} Class Distribution: GT vs Preds\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_class_distribution.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating class distribution plot for {dataset}: {e}\")\n            plt.close()\n\n    # Combined detection AUC comparison\n    try:\n        plt.figure()\n        for dataset, exp in experiment_data.items():\n            epochs = [d[\"epoch\"] for d in exp[\"metrics\"][\"detection\"]]\n            auc = [d[\"auc\"] for d in exp[\"metrics\"][\"detection\"]]\n            plt.plot(epochs, auc, marker=\"o\", label=dataset)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Detection AUC\")\n        plt.title(\"Comparison of Detection AUC Across Datasets\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"comparison_detection_auc.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating combined detection AUC plot: {e}\")\n        plt.close()\n\n    # Combined DES comparison\n    try:\n        plt.figure()\n        for dataset, exp in experiment_data.items():\n            epochs = [d[\"epoch\"] for d in exp[\"metrics\"][\"detection\"]]\n            des = [d[\"DES\"] for d in exp[\"metrics\"][\"detection\"]]\n            plt.plot(epochs, des, marker=\"o\", label=dataset)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"DES\")\n        plt.title(\"Comparison of DES Across Datasets\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"comparison_DES.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating combined DES plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    # Per\u2010dataset visualizations\n    for dataset, exp in experiment_data.items():\n        # Loss curves\n        try:\n            epochs = [d[\"epoch\"] for d in exp[\"losses\"][\"train\"]]\n            tr_loss = [d[\"loss\"] for d in exp[\"losses\"][\"train\"]]\n            vl_loss = [d[\"loss\"] for d in exp[\"losses\"][\"val\"]]\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, vl_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dataset} Loss Curve (Train vs Validation)\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_loss_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {dataset}: {e}\")\n            plt.close()\n        # Detection AUC curve\n        try:\n            epochs = [d[\"epoch\"] for d in exp[\"metrics\"][\"detection\"]]\n            auc = [d[\"auc\"] for d in exp[\"metrics\"][\"detection\"]]\n            plt.figure()\n            plt.plot(epochs, auc, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Detection AUC\")\n            plt.title(f\"{dataset} Detection AUC Curve\")\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_detection_auc_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating detection AUC plot for {dataset}: {e}\")\n            plt.close()\n        # Class distribution vs ground truth\n        try:\n            preds = exp[\"predictions\"]\n            gt = list(exp[\"ground_truth\"])\n            counts_pred = [preds.count(0), preds.count(1)]\n            counts_gt = [gt.count(0), gt.count(1)]\n            x = np.arange(2)\n            width = 0.35\n            plt.figure()\n            plt.bar(x - width / 2, counts_gt, width, label=\"Ground Truth\")\n            plt.bar(x + width / 2, counts_pred, width, label=\"Predictions\")\n            plt.xticks(x, [\"Class 0\", \"Class 1\"])\n            plt.ylabel(\"Count\")\n            plt.title(f\"{dataset} Class Distribution: GT vs Preds\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_class_distribution.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating class distribution plot for {dataset}: {e}\")\n            plt.close()\n\n    # Combined detection AUC comparison\n    try:\n        plt.figure()\n        for dataset, exp in experiment_data.items():\n            epochs = [d[\"epoch\"] for d in exp[\"metrics\"][\"detection\"]]\n            auc = [d[\"auc\"] for d in exp[\"metrics\"][\"detection\"]]\n            plt.plot(epochs, auc, marker=\"o\", label=dataset)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Detection AUC\")\n        plt.title(\"Comparison of Detection AUC Across Datasets\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"comparison_detection_auc.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating combined detection AUC plot: {e}\")\n        plt.close()\n\n    # Combined DES comparison\n    try:\n        plt.figure()\n        for dataset, exp in experiment_data.items():\n            epochs = [d[\"epoch\"] for d in exp[\"metrics\"][\"detection\"]]\n            des = [d[\"DES\"] for d in exp[\"metrics\"][\"detection\"]]\n            plt.plot(epochs, des, marker=\"o\", label=dataset)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"DES\")\n        plt.title(\"Comparison of DES Across Datasets\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"comparison_DES.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating combined DES plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data_path_list = [\n        \"experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_ea3f7efe711046cb84aad3aa8201291c_proc_152738/experiment_data.npy\",\n        \"experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cdb9f264bf8440bcb554ab2b6f318b9e_proc_152739/experiment_data.npy\",\n        \"experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_608b01f5455e4d448a0fd375847f92f7_proc_152740/experiment_data.npy\",\n    ]\n    all_experiment_data = []\n    for experiment_data_path in experiment_data_path_list:\n        exp = np.load(\n            os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\"), experiment_data_path),\n            allow_pickle=True,\n        ).item()\n        all_experiment_data.append(exp)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    # Aggregate and plot per\u2010dataset\n    for dataset in all_experiment_data[0].keys():\n        # collect per\u2010run arrays\n        train_runs, val_runs, auc_runs, des_runs = [], [], [], []\n        epochs = None\n        for exp in all_experiment_data:\n            train_info = exp[\"losses\"][\"train\"]\n            val_info = exp[\"losses\"][\"val\"]\n            det_info = exp[\"metrics\"][\"detection\"]\n            if epochs is None:\n                epochs = [d[\"epoch\"] for d in train_info]\n            train_runs.append([d[\"loss\"] for d in train_info])\n            val_runs.append([d[\"loss\"] for d in val_info])\n            auc_runs.append([d[\"auc\"] for d in det_info])\n            des_runs.append([d.get(\"DES\", np.nan) for d in det_info])\n        train_arr = np.array(train_runs)\n        val_arr = np.array(val_runs)\n        auc_arr = np.array(auc_runs)\n        des_arr = np.array(des_runs)\n        train_mean = train_arr.mean(axis=0)\n        train_sem = train_arr.std(axis=0, ddof=1) / np.sqrt(train_arr.shape[0])\n        val_mean = val_arr.mean(axis=0)\n        val_sem = val_arr.std(axis=0, ddof=1) / np.sqrt(val_arr.shape[0])\n        auc_mean = auc_arr.mean(axis=0)\n        auc_sem = auc_arr.std(axis=0, ddof=1) / np.sqrt(auc_arr.shape[0])\n        des_mean = des_arr.mean(axis=0)\n        des_sem = des_arr.std(axis=0, ddof=1) / np.sqrt(des_arr.shape[0])\n\n        try:\n            plt.figure()\n            plt.errorbar(\n                epochs,\n                train_mean,\n                yerr=train_sem,\n                label=\"Train Loss Mean \u00b1 SEM\",\n                capsize=3,\n            )\n            plt.errorbar(\n                epochs, val_mean, yerr=val_sem, label=\"Val Loss Mean \u00b1 SEM\", capsize=3\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dataset} Loss Curve Mean \u00b1 SEM (Train vs Validation)\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_loss_curve_mean_sem.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating aggregated loss plot for {dataset}: {e}\")\n            plt.close()\n\n        try:\n            plt.figure()\n            plt.errorbar(\n                epochs,\n                auc_mean,\n                yerr=auc_sem,\n                marker=\"o\",\n                label=\"Detection AUC Mean \u00b1 SEM\",\n                capsize=3,\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Detection AUC\")\n            plt.title(f\"{dataset} Detection AUC Mean \u00b1 SEM\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset}_detection_auc_mean_sem.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating aggregated detection AUC plot for {dataset}: {e}\")\n            plt.close()\n\n        try:\n            plt.figure()\n            plt.errorbar(\n                epochs,\n                des_mean,\n                yerr=des_sem,\n                marker=\"o\",\n                label=\"DES Mean \u00b1 SEM\",\n                capsize=3,\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"DES\")\n            plt.title(f\"{dataset} DES Mean \u00b1 SEM\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_des_mean_sem.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating aggregated DES plot for {dataset}: {e}\")\n            plt.close()\n\n        # Print final\u2010epoch summary\n        print(\n            f\"{dataset} Final Metrics: \"\n            f\"Train Loss {train_mean[-1]:.4f}\u00b1{train_sem[-1]:.4f}, \"\n            f\"Val Loss {val_mean[-1]:.4f}\u00b1{val_sem[-1]:.4f}, \"\n            f\"Detection AUC {auc_mean[-1]:.4f}\u00b1{auc_sem[-1]:.4f}, \"\n            f\"DES {des_mean[-1]:.4f}\u00b1{des_sem[-1]:.4f}\"\n        )\n"], "plot_plan": ["We will load the saved experiment data and for each dataset pick the hyperparameter combination that gave the highest validation AUC.  Then for that best (bs, lr) we extract the per\u2010epoch train and val losses and AUCs, and produce two separate line plots: one for loss curves and one for AUC curves across epochs.  Each plot is wrapped in its own try\u2010except to catch any errors and ensure the figure is closed.  Titles include the dataset name and the best hyperparameters, and the x/y labels and legends clarify train vs validation.  Plots are saved in the working directory under descriptive filenames.  Finally, we print out the best validation AUC and its hyperparameters for each dataset.", null, null, null, null, null, "First, we load the saved `experiment_data.npy` from the working directory. For each dataset, we identify the best hyperparameter configuration by maximum validation AUC. We then extract and sort train/validation loss and AUC values per epoch for that best batch size and learning rate. In separate try-except blocks, we plot and save loss and AUC curves with clear titles specifying dataset, bs, and lr. For the synthetic dataset only, we also plot a histogram of predicted probabilities by true class at the final epoch. After per-dataset plots, we build a bar chart comparing best validation AUCs across all datasets. Finally, we overlay the validation AUC curves of every dataset in a single comparison plot. Every figure is saved to `working_dir` with consistent naming and always closed, with exceptions logged.", null, null, "We\u2019ll load the saved experiment data and for each dataset extract train/validation losses and detection metrics (AUC and DES) per epoch.  We\u2019ll plot loss curves and detection AUC curves for each dataset in separate try/except blocks, then produce two comparison plots across datasets: one for detection AUC and one for DES over epochs.  As a dataset-specific visualization, we\u2019ll also compare class distributions between ground truth and our model\u2019s predictions with a bar chart.  All figures are saved under working_dir with descriptive filenames.  We always close figures after saving, and errors are caught and reported.", "We first load the saved experiment data from the working directory. For each dataset, we extract and plot per\u2010epoch training vs validation losses with a subtitle \u201cTrain vs Validation Loss\u201d and detection AUC curves (vote vs KL) with the subtitle \u201cLeft: Vote, Right: KL\u201d. We also generate a class distribution bar chart comparing ground truth vs predicted labels (\u201cLeft: Ground Truth, Right: Predicted\u201d). Each figure is created in its own try\u2010except block with descriptive titles and file names including the dataset name and plot type, and figures are closed after saving. After per\u2010dataset plots, we create a comparison bar chart of final vote and KL AUCs across all datasets titled \u201cLeft: Vote, Right: KL Across Datasets\u201d. Finally, we print the final detection AUC_vote and AUC_kl for each dataset.", "We begin by loading the saved experiment data from the working directory. Then we iterate through each dataset to extract and print its final detection AUC. Next, for each dataset we plot the training and validation loss across epochs, labeling axes and including a two-line title that specifies the dataset. Each plot is created inside a try-except block, saved with a descriptive filename, and then closed. We then similarly plot the detection AUC over epochs per dataset with clear labels and titles. After the per-dataset visualizations, we generate a combined comparison plot of detection AUC curves across all datasets, wrapped in a separate try-except. All figures are saved into the working directory with consistent naming conventions. Finally, any errors during plotting are caught and printed, and we always close each figure.", null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["I will load the saved `experiment_data.npy` from the working directory, then\nloop over each dataset\u2019s records to pull out training and validation AUCs and\nlosses. For each dataset, I compute the highest training and validation AUC and\nthe lowest training and validation loss. The script prints the dataset name\nfollowed by clearly labeled metrics in a single pass, and it executes\nimmediately without any `if __name__ == \"__main__\":` guard.", "", "", "", "We load the saved experiment data from the `working` folder, iterate through\neach dataset key, and for each we grab the last\u2010epoch training loss, validation\nloss, detection AUC/DES values, and compute the validation accuracy by comparing\nstored predictions to ground\u2010truth labels. We then print the dataset name\nfollowed by clearly labeled metrics (\u2018final training loss\u2019, \u2018final validation\nloss\u2019, \u2018final detection AUC\u2019, \u2018final detection DES\u2019, and \u2018validation accuracy\u2019).\nAll code executes at the global scope without any `if __name__ == \"__main__\"`\nguard.", "The following script loads the saved experiment data from the working directory,\ncomputes the average variation score for each dataset as validation loss, and\nprints the dataset name followed by its validation loss, detection AUC, and DES.", "Here\u2019s a concise script that loads the saved experiment data, iterates through\neach dataset, and computes the maximum training and validation AUC, the minimum\ntraining and validation loss, and the maximum DES across all epochs and\nhyperparameter settings. It then prints the dataset name followed by each metric\nwith clear labels.", "The following script computes the working directory, loads the\n`experiment_data.npy` file, and then iterates over each dataset in the stored\ndictionary. For each dataset it extracts the final (last epoch) train loss,\nvalidation loss, AUC, and DES values, printing them with clear, descriptive\nlabels. The code runs immediately at the global scope without any special entry\npoint.", "", "The following script loads the saved `experiment_data.npy` file from the working\ndirectory, iterates over each dataset\u2019s stored results, and extracts the final\ntraining loss, final validation loss, detection AUC, detection DES, and\nvalidation accuracy computed from the stored predictions and ground truth. It\nprints each dataset\u2019s name followed by clear, fully\u2010specified metric labels and\ntheir values. All code lives at the global level so it runs immediately when the\nscript is executed.", "Here\u2019s a simple script that loads the saved `experiment_data.npy` from the\nworking directory, iterates over each dataset, and prints the final epoch\u2019s\ntraining loss, validation loss, and all detection metrics with clear labels. All\ncode is at the top level and runs immediately when the script is executed.", "I will load the saved NumPy file from the working directory, then iterate over\neach dataset key to pull out the final epoch\u2019s training and validation losses as\nwell as the detection AUC and DES values. I will print the dataset name followed\nby each metric with clear labels (e.g., \u201ctrain loss,\u201d \u201cvalidation loss,\u201d\n\u201cdetection AUC,\u201d \u201cdetection DES\u201d). The script executes immediately at global\nscope without any special entry points.", "", "The following script loads the saved `experiment_data.npy` file from the working\ndirectory, iterates over each dataset\u2019s stored results, and extracts the final\ntraining loss, final validation loss, detection AUC, detection DES, and\nvalidation accuracy computed from the stored predictions and ground truth. It\nprints each dataset\u2019s name followed by clear, fully\u2010specified metric labels and\ntheir values. All code lives at the global level so it runs immediately when the\nscript is executed.", "The following script loads the saved `experiment_data.npy` file from the working\ndirectory, iterates over each dataset\u2019s stored results, and extracts the final\ntraining loss, final validation loss, detection AUC, detection DES, and\nvalidation accuracy computed from the stored predictions and ground truth. It\nprints each dataset\u2019s name followed by clear, fully\u2010specified metric labels and\ntheir values. All code lives at the global level so it runs immediately when the\nscript is executed.", "The following script loads the saved `experiment_data.npy` file from the working\ndirectory, iterates over each dataset\u2019s stored results, and extracts the final\ntraining loss, final validation loss, detection AUC, detection DES, and\nvalidation accuracy computed from the stored predictions and ground truth. It\nprints each dataset\u2019s name followed by clear, fully\u2010specified metric labels and\ntheir values. All code lives at the global level so it runs immediately when the\nscript is executed.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Define working directory and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through datasets and compute best/final metrics\nfor dataset_name, records in experiment_data.items():\n    # Extract lists of metric and loss dictionaries\n    train_metrics = records[\"metrics\"][\"train\"]\n    val_metrics = records[\"metrics\"][\"val\"]\n    train_losses = records[\"losses\"][\"train\"]\n    val_losses = records[\"losses\"][\"val\"]\n\n    # Compute best AUCs and lowest losses\n    best_train_auc = max(item[\"auc\"] for item in train_metrics)\n    best_val_auc = max(item[\"auc\"] for item in val_metrics)\n    lowest_train_loss = min(item[\"loss\"] for item in train_losses)\n    lowest_val_loss = min(item[\"loss\"] for item in val_losses)\n\n    # Print results\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"train accuracy: {best_train_auc:.4f}\")\n    print(f\"validation accuracy: {best_val_auc:.4f}\")\n    print(f\"train loss: {lowest_train_loss:.4f}\")\n    print(f\"validation loss: {lowest_val_loss:.4f}\\n\")\n", "", "", "", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over each dataset and print the final metrics\nfor dataset_name, data in experiment_data.items():\n    print(dataset_name)\n    # Extract final losses\n    final_train_loss = data[\"losses\"][\"train\"][-1][\"loss\"]\n    final_val_loss = data[\"losses\"][\"val\"][-1][\"loss\"]\n    # Extract final detection metrics\n    det_metrics = data[\"metrics\"][\"detection\"][-1]\n    final_auc = det_metrics[\"auc\"]\n    final_des = det_metrics[\"DES\"]\n    # Compute validation accuracy\n    preds = data[\"predictions\"]\n    truths = data[\"ground_truth\"]\n    accuracy = sum(p == g for p, g in zip(preds, truths)) / len(truths)\n    # Print with clear metric names\n    print(f\"final training loss: {final_train_loss:.4f}\")\n    print(f\"final validation loss: {final_val_loss:.4f}\")\n    print(f\"final detection AUC: {final_auc:.4f}\")\n    print(f\"final detection DES: {final_des:.4f}\")\n    print(f\"validation accuracy: {accuracy:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract and print metrics for each dataset\nfor name, metrics in experiment_data.items():\n    variation_scores = metrics.get(\"variation_scores\", [])\n    validation_loss = np.mean(variation_scores) if variation_scores else float(\"nan\")\n    detection_auc = metrics.get(\"roc_auc\", float(\"nan\"))\n    des = metrics.get(\"DES\", float(\"nan\"))\n\n    print(f\"{name}\")\n    print(f\"validation loss: {validation_loss:.4f}\")\n    print(f\"detection AUC: {detection_auc:.4f}\")\n    print(f\"DES: {des:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_data = np.load(\n    os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n).item()\n\n# Iterate over each dataset and compute summary metrics\nfor dataset_name, content in experiment_data.items():\n    train_metrics = content[\"metrics\"][\"train\"]\n    val_metrics = content[\"metrics\"][\"val\"]\n    train_losses = content[\"losses\"][\"train\"]\n    val_losses = content[\"losses\"][\"val\"]\n    des_list = content[\"DES\"]\n\n    best_train_auc = max(entry[\"auc\"] for entry in train_metrics)\n    best_validation_auc = max(entry[\"auc\"] for entry in val_metrics)\n    lowest_train_loss = min(entry[\"loss\"] for entry in train_losses)\n    lowest_validation_loss = min(entry[\"loss\"] for entry in val_losses)\n    best_DES = max(entry[\"DES\"] for entry in des_list)\n\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"Train accuracy: {best_train_auc:.4f}\")\n    print(f\"Validation accuracy: {best_validation_auc:.4f}\")\n    print(f\"Train loss: {lowest_train_loss:.4f}\")\n    print(f\"Validation loss: {lowest_validation_loss:.4f}\")\n    print(f\"DES: {best_DES:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Define working directory and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, metrics in experiment_data.items():\n    # Extract the last epoch entries\n    final_train_loss = metrics[\"losses\"][\"train\"][-1][\"loss\"]\n    final_val_loss = metrics[\"losses\"][\"val\"][-1][\"loss\"]\n    final_auc = metrics[\"AUC\"][-1][\"AUC\"]\n    final_des = metrics[\"DES\"][-1][\"DES\"]\n\n    # Print dataset name and metrics\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"  Train Loss (final epoch): {final_train_loss:.4f}\")\n    print(f\"  Validation Loss (final epoch): {final_val_loss:.4f}\")\n    print(f\"  Area Under ROC Curve (AUC) (final epoch): {final_auc:.4f}\")\n    print(f\"  Disagreement Efficiency Score (DES) (final epoch): {final_des:.4f}\")\n    print()\n", "", "import os\nimport numpy as np\n\n# Load the experiment data\nfile_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, results in experiment_data.items():\n    # Extract final training and validation losses\n    final_train_loss = results[\"losses\"][\"train\"][-1][\"loss\"]\n    final_val_loss = results[\"losses\"][\"val\"][-1][\"loss\"]\n\n    # Extract final detection metrics\n    final_detection = results[\"metrics\"][\"detection\"][-1]\n    detection_auc = final_detection[\"auc\"]\n    detection_des = final_detection[\"DES\"]\n\n    # Compute validation accuracy from stored predictions and ground truth\n    preds = np.array(results[\"predictions\"])\n    labels = np.array(results[\"ground_truth\"])\n    validation_accuracy = (preds == labels).mean()\n\n    # Print the metrics\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"train loss: {final_train_loss:.4f}\")\n    print(f\"validation loss: {final_val_loss:.4f}\")\n    print(f\"detection AUC: {detection_auc:.4f}\")\n    print(f\"detection DES: {detection_des:.4f}\")\n    print(f\"validation accuracy: {validation_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, stats in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n\n    # Final training and validation loss\n    final_train_loss = stats[\"losses\"][\"train\"][-1][\"loss\"]\n    final_val_loss = stats[\"losses\"][\"val\"][-1][\"loss\"]\n    print(f\"Final training loss: {final_train_loss:.4f}\")\n    print(f\"Final validation loss: {final_val_loss:.4f}\")\n\n    # Final detection metrics\n    final_detection = stats[\"metrics\"][\"detection\"][-1]\n    print(f\"Final detection AUC_vote: {final_detection['auc_vote']:.4f}\")\n    print(f\"Final detection DES_vote: {final_detection['DES_vote']:.4f}\")\n    print(f\"Final detection AUC_kl: {final_detection['auc_kl']:.4f}\")\n    print(f\"Final detection DES_kl: {final_detection['DES_kl']:.4f}\")\n", "import os\nimport numpy as np\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract and print final metrics for each dataset\nfor dataset_name, results in experiment_data.items():\n    # Final train and validation loss\n    train_loss = results[\"losses\"][\"train\"][-1][\"loss\"]\n    val_loss = results[\"losses\"][\"val\"][-1][\"loss\"]\n    # Final detection metrics\n    detection_info = results[\"metrics\"][\"detection\"][-1]\n    detection_auc = detection_info[\"auc\"]\n    detection_des = detection_info[\"DES\"]\n\n    # Print metrics with clear labels\n    print(dataset_name)\n    print(f\"train loss: {train_loss:.4f}\")\n    print(f\"validation loss: {val_loss:.4f}\")\n    print(f\"detection AUC: {detection_auc:.4f}\")\n    print(f\"detection DES: {detection_des:.4f}\")\n    print()\n", "", "import os\nimport numpy as np\n\n# Load the experiment data\nfile_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, results in experiment_data.items():\n    # Extract final training and validation losses\n    final_train_loss = results[\"losses\"][\"train\"][-1][\"loss\"]\n    final_val_loss = results[\"losses\"][\"val\"][-1][\"loss\"]\n\n    # Extract final detection metrics\n    final_detection = results[\"metrics\"][\"detection\"][-1]\n    detection_auc = final_detection[\"auc\"]\n    detection_des = final_detection[\"DES\"]\n\n    # Compute validation accuracy from stored predictions and ground truth\n    preds = np.array(results[\"predictions\"])\n    labels = np.array(results[\"ground_truth\"])\n    validation_accuracy = (preds == labels).mean()\n\n    # Print the metrics\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"train loss: {final_train_loss:.4f}\")\n    print(f\"validation loss: {final_val_loss:.4f}\")\n    print(f\"detection AUC: {detection_auc:.4f}\")\n    print(f\"detection DES: {detection_des:.4f}\")\n    print(f\"validation accuracy: {validation_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nfile_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, results in experiment_data.items():\n    # Extract final training and validation losses\n    final_train_loss = results[\"losses\"][\"train\"][-1][\"loss\"]\n    final_val_loss = results[\"losses\"][\"val\"][-1][\"loss\"]\n\n    # Extract final detection metrics\n    final_detection = results[\"metrics\"][\"detection\"][-1]\n    detection_auc = final_detection[\"auc\"]\n    detection_des = final_detection[\"DES\"]\n\n    # Compute validation accuracy from stored predictions and ground truth\n    preds = np.array(results[\"predictions\"])\n    labels = np.array(results[\"ground_truth\"])\n    validation_accuracy = (preds == labels).mean()\n\n    # Print the metrics\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"train loss: {final_train_loss:.4f}\")\n    print(f\"validation loss: {final_val_loss:.4f}\")\n    print(f\"detection AUC: {detection_auc:.4f}\")\n    print(f\"detection DES: {detection_des:.4f}\")\n    print(f\"validation accuracy: {validation_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nfile_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, results in experiment_data.items():\n    # Extract final training and validation losses\n    final_train_loss = results[\"losses\"][\"train\"][-1][\"loss\"]\n    final_val_loss = results[\"losses\"][\"val\"][-1][\"loss\"]\n\n    # Extract final detection metrics\n    final_detection = results[\"metrics\"][\"detection\"][-1]\n    detection_auc = final_detection[\"auc\"]\n    detection_des = final_detection[\"DES\"]\n\n    # Compute validation accuracy from stored predictions and ground truth\n    preds = np.array(results[\"predictions\"])\n    labels = np.array(results[\"ground_truth\"])\n    validation_accuracy = (preds == labels).mean()\n\n    # Print the metrics\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"train loss: {final_train_loss:.4f}\")\n    print(f\"validation loss: {final_val_loss:.4f}\")\n    print(f\"detection AUC: {detection_auc:.4f}\")\n    print(f\"detection DES: {detection_des:.4f}\")\n    print(f\"validation accuracy: {validation_accuracy:.4f}\\n\")\n", ""], "parse_term_out": ["['Dataset: synthetic', '\\n', 'train accuracy: 0.9522', '\\n', 'validation\naccuracy: 0.9572', '\\n', 'train loss: 0.2641', '\\n', 'validation loss:\n0.2876\\n', '\\n', 'Dataset: sst2', '\\n', 'train accuracy: 0.5558', '\\n',\n'validation accuracy: 0.5840', '\\n', 'train loss: 0.6815', '\\n', 'validation\nloss: 0.6891\\n', '\\n', 'Dataset: yelp_polarity', '\\n', 'train accuracy: 0.5930',\n'\\n', 'validation accuracy: 0.5828', '\\n', 'train loss: 0.6800', '\\n',\n'validation loss: 0.6924\\n', '\\n', 'Execution time: a moment seconds (time limit\nis an hour).']", "", "", "", "['sst2', '\\n', 'final training loss: 0.7007', '\\n', 'final validation loss:\n0.6980', '\\n', 'final detection AUC: 0.5080', '\\n', 'final detection DES:\n0.1270', '\\n', 'validation accuracy: 0.4700', '\\n', '\\n', 'yelp_polarity', '\\n',\n'final training loss: 0.6973', '\\n', 'final validation loss: 0.6980', '\\n',\n'final detection AUC: 0.5000', '\\n', 'final detection DES: 0.1250', '\\n',\n'validation accuracy: 0.5000', '\\n', '\\n', 'imdb', '\\n', 'final training loss:\n0.6994', '\\n', 'final validation loss: 0.6988', '\\n', 'final detection AUC:\n0.5000', '\\n', 'final detection DES: 0.1250', '\\n', 'validation accuracy:\n0.4800', '\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['sst2', '\\n', 'validation loss: 0.0080', '\\n', 'detection AUC: 0.9898', '\\n',\n'DES: 0.1980', '\\n', 'yelp_polarity', '\\n', 'validation loss: 0.0080', '\\n',\n'detection AUC: 0.9898', '\\n', 'DES: 0.1980', '\\n', 'imdb', '\\n', 'validation\nloss: 0.0160', '\\n', 'detection AUC: 0.7188', '\\n', 'DES: 0.1437', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Train accuracy: 0.9517', '\\n', 'Validation\naccuracy: 0.9572', '\\n', 'Train loss: 0.2478', '\\n', 'Validation loss: 0.2676',\n'\\n', 'DES: 0.4786\\n', '\\n', 'Dataset: sst2', '\\n', 'Train accuracy: 0.5773',\n'\\n', 'Validation accuracy: 0.4632', '\\n', 'Train loss: 0.6770', '\\n',\n'Validation loss: 0.6956', '\\n', 'DES: 0.2316\\n', '\\n', 'Dataset:\nyelp_polarity', '\\n', 'Train accuracy: 0.5977', '\\n', 'Validation accuracy:\n0.5953', '\\n', 'Train loss: 0.6763', '\\n', 'Validation loss: 0.6834', '\\n',\n'DES: 0.2976\\n', '\\n', 'Dataset: imdb', '\\n', 'Train accuracy: 0.5830', '\\n',\n'Validation accuracy: 0.5528', '\\n', 'Train loss: 0.6809', '\\n', 'Validation\nloss: 0.6924', '\\n', 'DES: 0.2764\\n', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 12, in\n<module>\\n    final_train_loss = metrics[\"losses\"][\"train\"][-1][\"loss\"]\\n\n~~~~~~~^^^^^^^^^^\\nKeyError: \\'losses\\'\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "", "['Dataset: sst2', '\\n', 'train loss: 0.1094', '\\n', 'validation loss: 0.4326',\n'\\n', 'detection AUC: 0.6332', '\\n', 'detection DES: 0.1583', '\\n', 'validation\naccuracy: 0.8350\\n', '\\n', 'Dataset: yelp_polarity', '\\n', 'train loss: 0.0549',\n'\\n', 'validation loss: 0.3306', '\\n', 'detection AUC: 0.5486', '\\n', 'detection\nDES: 0.1372', '\\n', 'validation accuracy: 0.8850\\n', '\\n', 'Dataset: imdb',\n'\\n', 'train loss: 0.1021', '\\n', 'validation loss: 0.4118', '\\n', 'detection\nAUC: 0.5090', '\\n', 'detection DES: 0.1273', '\\n', 'validation accuracy:\n0.8600\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['\\nDataset: sst2', '\\n', 'Final training loss: 0.0378', '\\n', 'Final validation\nloss: 0.5371', '\\n', 'Final detection AUC_vote: 0.6780', '\\n', 'Final detection\nDES_vote: 0.1130', '\\n', 'Final detection AUC_kl: 0.7788', '\\n', 'Final\ndetection DES_kl: 0.1298', '\\n', '\\nDataset: yelp_polarity', '\\n', 'Final\ntraining loss: 0.0127', '\\n', 'Final validation loss: 0.2072', '\\n', 'Final\ndetection AUC_vote: 0.6333', '\\n', 'Final detection DES_vote: 0.1056', '\\n',\n'Final detection AUC_kl: 0.8876', '\\n', 'Final detection DES_kl: 0.1479', '\\n',\n'\\nDataset: imdb', '\\n', 'Final training loss: 0.0373', '\\n', 'Final validation\nloss: 0.4011', '\\n', 'Final detection AUC_vote: 0.5382', '\\n', 'Final detection\nDES_vote: 0.0897', '\\n', 'Final detection AUC_kl: 0.8526', '\\n', 'Final\ndetection DES_kl: 0.1421', '\\n', 'Execution time: a moment seconds (time limit\nis an hour).']", "['sst2', '\\n', 'train loss: 0.0395', '\\n', 'validation loss: 0.4053', '\\n',\n'detection AUC: 0.7970', '\\n', 'detection DES: 0.1328', '\\n', '\\n',\n'yelp_polarity', '\\n', 'train loss: 0.0487', '\\n', 'validation loss: 0.2958',\n'\\n', 'detection AUC: 0.7527', '\\n', 'detection DES: 0.1254', '\\n', '\\n',\n'imdb', '\\n', 'train loss: 0.0606', '\\n', 'validation loss: 0.6357', '\\n',\n'detection AUC: 0.7715', '\\n', 'detection DES: 0.1286', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "", "['Dataset: sst2', '\\n', 'train loss: 0.1213', '\\n', 'validation loss: 0.3878',\n'\\n', 'detection AUC: 0.5545', '\\n', 'detection DES: 0.1386', '\\n', 'validation\naccuracy: 0.8500\\n', '\\n', 'Dataset: yelp_polarity', '\\n', 'train loss: 0.0483',\n'\\n', 'validation loss: 0.2825', '\\n', 'detection AUC: 0.6007', '\\n', 'detection\nDES: 0.1502', '\\n', 'validation accuracy: 0.9100\\n', '\\n', 'Dataset: imdb',\n'\\n', 'train loss: 0.1247', '\\n', 'validation loss: 0.4622', '\\n', 'detection\nAUC: 0.5057', '\\n', 'detection DES: 0.1264', '\\n', 'validation accuracy:\n0.8300\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: sst2', '\\n', 'train loss: 0.1103', '\\n', 'validation loss: 0.5526',\n'\\n', 'detection AUC: 0.6096', '\\n', 'detection DES: 0.1524', '\\n', 'validation\naccuracy: 0.7950\\n', '\\n', 'Dataset: yelp_polarity', '\\n', 'train loss: 0.0519',\n'\\n', 'validation loss: 0.2668', '\\n', 'detection AUC: 0.5668', '\\n', 'detection\nDES: 0.1417', '\\n', 'validation accuracy: 0.9100\\n', '\\n', 'Dataset: imdb',\n'\\n', 'train loss: 0.1127', '\\n', 'validation loss: 0.4947', '\\n', 'detection\nAUC: 0.5646', '\\n', 'detection DES: 0.1411', '\\n', 'validation accuracy:\n0.8150\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: sst2', '\\n', 'train loss: 0.1103', '\\n', 'validation loss: 0.5526',\n'\\n', 'detection AUC: 0.6096', '\\n', 'detection DES: 0.1524', '\\n', 'validation\naccuracy: 0.7950\\n', '\\n', 'Dataset: yelp_polarity', '\\n', 'train loss: 0.0516',\n'\\n', 'validation loss: 0.2680', '\\n', 'detection AUC: 0.5452', '\\n', 'detection\nDES: 0.1363', '\\n', 'validation accuracy: 0.9100\\n', '\\n', 'Dataset: imdb',\n'\\n', 'train loss: 0.1127', '\\n', 'validation loss: 0.4947', '\\n', 'detection\nAUC: 0.5646', '\\n', 'detection DES: 0.1411', '\\n', 'validation accuracy:\n0.8150\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, "KeyError", null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, {"args": ["losses"]}, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 12, "<module>", "final_train_loss = metrics[\"losses\"][\"train\"][-1][\"loss\"]"]], null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>

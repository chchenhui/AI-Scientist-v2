<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 8], [0, 3], [0, 6], [0, 7], [0, 10], [0, 11], [0, 1], [0, 2], [1, 4], [2, 12], [3, 5], [4, 9], [12, 13], [12, 14], [12, 15], [12, 16]], "layout": [[0.5661764705882353, 0.0], [0.0, 0.33333333333333337], [0.29411764705882354, 0.33333333333333337], [0.4117647058823529, 0.33333333333333337], [0.0, 0.6666666666666667], [0.4117647058823529, 0.6666666666666667], [0.5294117647058824, 0.33333333333333337], [0.6470588235294118, 0.33333333333333337], [0.7647058823529411, 0.33333333333333337], [0.0, 1.0], [0.8823529411764706, 0.33333333333333337], [1.0, 0.33333333333333337], [0.29411764705882354, 0.6666666666666667], [0.11764705882352941, 1.0], [0.23529411764705882, 1.0], [0.35294117647058826, 1.0], [0.47058823529411764, 1.0]], "plan": ["We simulate Perturbation-Induced Uncertainty by sampling synthetic divergence\nscores for \u201ccorrect\u201d and \u201challucinated\u201d examples, then train a simple logistic\nregression in PyTorch to map divergence to hallucination risk. We draw\ndivergence values from different distributions for each class to mimic higher\noutput variability when the model is wrong. We assemble synthetic data into\ntrain and validation splits and wrap them in DataLoaders. Our detector is a one-\ndimensional linear model with sigmoid output, optimized with BCEWithLogitsLoss\non GPU if available. We train for a fixed number of epochs, computing train and\nval losses as well as AUC-ROC for hallucination detection at each epoch. We\nprint the validation loss per epoch and record all metrics, losses, predictions,\nand ground truths in an experiment_data dictionary. Finally, we save the\nexperiment_data as a numpy file in the working directory. This minimal pipeline\nestablishes a solid baseline that can later be extended to real LLM outputs and\nparaphrase-based divergences.", "Hyperparam tuning name: learning_rate. We iterate over a set of candidate\nlearning rates and, for each, train a fresh logistic regression model for a\nfixed number of epochs. During training, we record per-epoch training/validation\nlosses and AUCs, as well as validation predictions and ground-truth labels. All\nresults are organized under the \"learning_rate\" tuning key in an\n`experiment_data` dictionary, with separate entries per hyperparameter value.\nFinally, we save the full dictionary to `experiment_data.npy` in the working\ndirectory.", "Hyperparam tuning name: batch_size. Below is a self-contained script that loops\nover batch sizes [16, 32, 64, 128], reinitializes the data loaders, model, and\noptimizer for each setting, and records per\u2010epoch train/validation losses and\nAUCs as well as validation predictions and labels. All results are stored in a\nnested `experiment_data` dictionary under `'batch_size' -> 'synthetic'` and\nsaved with `np.save` at the end.", "Hyperparam tuning name: weight_decay. We extend the script by looping over a\nlist of weight_decay values, re\u2010initializing the model and Adam optimizer with\neach value. For each run we collect per\u2010epoch train/val losses and AUCs, as well\nas final validation predictions and ground truth labels, and store them under\n`experiment_data['weight_decay']['synthetic']`. We also record the list of\ntested hyperparameters to keep track of which entries correspond to which value.\nAt the end we save the entire `experiment_data` dict via `np.save()`.", "I fixed the gradient-to-NumPy bug by detaching tensors before conversion, and I\nintroduced two real\u2010world tabular datasets from HuggingFace\n(ieee_fraud_detection and bank_marketing). Each is loaded, split (if needed),\nnumeric features extracted, normalized, and wrapped in DataLoaders. The\nhyperparameter grid sweep now trains a linear model on synthetic, fraud, and\nmarketing data without altering its architecture, logging per-epoch validation\nloss and ROC AUC, and saving all results to working/experiment_data.npy.", "We can resolve the numpy conversion bug by detaching the logits tensor before\ncalling `.cpu().numpy()`.  I also load two numeric binary\u2010classification tasks\n(Pima Indians Diabetes and Blood Transfusion) via Huggingface\u2019s OpenML loader,\nstandardize their features, and split into train/validation sets.  The script\nthen loops over a list of weight\u2010decay hyperparameters for each dataset, trains\na one\u2010layer linear model entirely on GPU, and records epoch\u2010wise losses and\nROC\u2010AUC scores.  Finally, all experiment data are saved as a single numpy\ndictionary in the working directory.", "Hyperparam tuning name: adam_beta1. We will sweep the Adam optimizer\u2019s beta1\nover a set of candidate values and for each value reinitialize the linear model\nand optimizer, then train for a fixed number of epochs. During each run we\nrecord per\u2010epoch training and validation losses and AUCs, and capture final\nvalidation predictions and ground\u2010truth labels. All results are stored in a\nnested dictionary under the key \u201cadam_beta1\u201d \u2192 \u201csynthetic\u201d and saved as a single\nnumpy file.", "Hyperparam tuning name: adam_beta2. Here\u2019s a sketch of the solution: we generate\nsynthetic training and validation sets once, define a list of beta2 values to\ntry, and then loop over each beta2 setting; for each one we re\u2010initialize the\nmodel and Adam optimizer with betas=(0.9, beta2), train for a fixed number of\nepochs, compute and store losses, AUC metrics, predictions and ground\u2010truth\nlabels per epoch in a nested dictionary under `'adam_beta2'\u2192'synthetic'`, and\nfinally save the entire `experiment_data` dict as `\"experiment_data.npy\"`.", "Hyperparam tuning name: lr_scheduler_gamma. I will loop over the set of \u03b3 values\n{0.9, 0.95, 0.99}, and for each, reinitialize the model, optimizer, and\nExponentialLR scheduler. During 20 epochs of training on the same synthetic\ndataset, I will decay the learning rate by \u03b3 each epoch via scheduler.step() and\nrecord train/val losses, AUC metrics, predictions, and ground truth. All results\nfor each \u03b3 are stored under\nexperiment_data['lr_scheduler_gamma']['synthetic'][str(\u03b3)]. Finally, I save the\nfull experiment_data dictionary to \u201cexperiment_data.npy\u201d in the working\ndirectory.", "We encountered a `DatasetNotFoundError` because \u201cieee_fraud_detection\u201d is not a\nvalid HuggingFace dataset.  We replace it with the correct \u201ccreditcard\u201d (Kaggle\ncredit card fraud) dataset and specify its V1\u2013V28, Time, Amount features and\n\u201cClass\u201d label.  We then introduce two new HF tabular classification tests: the\nUCI Adult Income dataset (\u201cadult\u201d) using its numeric features and binary `label`\ncolumn, alongside the existing bank_marketing and synthetic baselines.  No model\narchitecture changes are made, and the same training loop, hyperparameters, and\nsaving logic are preserved.", "Hyperparam tuning name: adam_eps. I will loop over the desired epsilon values,\nreinitializing the model and Adam optimizer for each, and collect per\u2010epoch\nlosses, AUC metrics, predictions, and ground truths into a nested\n`experiment_data` dict under `\"adam_eps\"`\u2192`\"synthetic\"`. After running all\nconfigurations, the entire dictionary is saved via `np.save` as\n`experiment_data.npy` in the working directory.", "Hyperparam tuning name: num_epochs. We can wrap the training loop in an outer\nloop over epoch settings, reinitializing the model and optimizer each time and\nusing constant synthetic train/val splits. For each `num_epochs` value we\ncollect per-epoch losses, AUC metrics, predictions and labels, then append them\nto structured lists under `experiment_data['num_epochs']['synthetic']`. Finally\nwe save the entire nested dictionary in one file `experiment_data.npy` for\ndownstream plotting and analysis.", "We detach the logits before converting to numpy in both training and validation\nloops by calling `.detach().cpu().numpy()`. We load two HuggingFace benchmarks\n(SST-2 and Yelp Polarity), extract sentence or text length as a single numeric\nfeature, and split into 1000/200 train/val sets. Each dataset\u2019s inputs are\nnormalized by the training mean and standard deviation. We sweep over two\nlearning rates `[0.001, 0.01]` and two batch sizes `[32, 64]`, train for 20\nepochs, and at each epoch compute and print `Epoch {epoch}: validation_loss =\n{val_loss:.4f}`, tracking ROC AUC and losses. All metrics, predictions, and\nground\u2010truth labels are stored in an `experiment_data` dict and saved with\n`np.save()` at the end.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import roc_auc_score\n\n\n# Synthetic data generation\ndef sample_data(N):\n    N0 = N // 2\n    N1 = N - N0\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N1), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N1)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\n# Prepare datasets\nx_train, y_train = sample_data(1000)\nx_val, y_val = sample_data(200)\ntrain_ds = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\nval_ds = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=32)\n\n# Model and optimizer\nmodel = nn.Linear(1, 1).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nloss_fn = nn.BCEWithLogitsLoss()\n\n# Experiment tracking\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# Training loop\nepochs = 20\nfor epoch in range(1, epochs + 1):\n    # Train\n    model.train()\n    train_losses, all_preds, all_labels = [], [], []\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        logits = model(xb).squeeze(1)\n        loss = loss_fn(logits, yb)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n        all_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n        all_labels.append(yb.cpu().numpy())\n    train_loss = np.mean(train_losses)\n    train_preds = np.concatenate(all_preds)\n    train_labels = np.concatenate(all_labels)\n    train_auc = roc_auc_score(train_labels, train_preds)\n\n    # Validate\n    model.eval()\n    val_losses, v_preds, v_labels = [], [], []\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits = model(xb).squeeze(1)\n            loss = loss_fn(logits, yb)\n            val_losses.append(loss.item())\n            v_preds.append(torch.sigmoid(logits).cpu().numpy())\n            v_labels.append(yb.cpu().numpy())\n    val_loss = np.mean(val_losses)\n    val_preds = np.concatenate(v_preds)\n    val_labels = np.concatenate(v_labels)\n    val_auc = roc_auc_score(val_labels, val_preds)\n\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n    # Record metrics\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(train_auc)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(val_auc)\n    experiment_data[\"synthetic\"][\"predictions\"].append(val_preds)\n    experiment_data[\"synthetic\"][\"ground_truth\"].append(val_labels)\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import roc_auc_score\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic data generation\ndef sample_data(N):\n    N0 = N // 2\n    N1 = N - N0\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N1), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N1)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\n# Prepare datasets\nx_train, y_train = sample_data(1000)\nx_val, y_val = sample_data(200)\ntrain_ds = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\nval_ds = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=32)\n\n# Hyperparameter grid\nlearning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2]\n\n# Experiment data structure\nexperiment_data = {\n    \"learning_rate\": {\n        \"synthetic\": {\n            \"lrs\": learning_rates,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Training settings\nepochs = 20\nloss_fn = nn.BCEWithLogitsLoss()\n\n# Hyperparameter tuning loop\nfor lr in learning_rates:\n    # Initialize model and optimizer\n    model = nn.Linear(1, 1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    # Containers for this learning rate\n    train_loss_hist, val_loss_hist = [], []\n    train_auc_hist, val_auc_hist = [], []\n    preds_hist, truths_hist = [], []\n\n    for epoch in range(1, epochs + 1):\n        # Training\n        model.train()\n        batch_losses, all_preds, all_labels = [], [], []\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits = model(xb).squeeze(1)\n            loss = loss_fn(logits, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            batch_losses.append(loss.item())\n            all_preds.append(torch.sigmoid(logits).cpu().numpy())\n            all_labels.append(yb.cpu().numpy())\n        train_loss = np.mean(batch_losses)\n        train_preds = np.concatenate(all_preds)\n        train_labels = np.concatenate(all_labels)\n        train_auc = roc_auc_score(train_labels, train_preds)\n\n        # Validation\n        model.eval()\n        val_batch_losses, v_preds, v_labels = [], [], []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits = model(xb).squeeze(1)\n                loss = loss_fn(logits, yb)\n                val_batch_losses.append(loss.item())\n                v_preds.append(torch.sigmoid(logits).cpu().numpy())\n                v_labels.append(yb.cpu().numpy())\n        val_loss = np.mean(val_batch_losses)\n        val_preds = np.concatenate(v_preds)\n        val_labels = np.concatenate(v_labels)\n        val_auc = roc_auc_score(val_labels, val_preds)\n\n        print(\n            f\"LR {lr:.1e} Epoch {epoch}: val_loss={val_loss:.4f}, val_auc={val_auc:.4f}\"\n        )\n\n        # Record epoch data\n        train_loss_hist.append(train_loss)\n        val_loss_hist.append(val_loss)\n        train_auc_hist.append(train_auc)\n        val_auc_hist.append(val_auc)\n        preds_hist.append(val_preds)\n        truths_hist.append(val_labels)\n\n    # Append results for this learning rate\n    sd = experiment_data[\"learning_rate\"][\"synthetic\"]\n    sd[\"losses\"][\"train\"].append(train_loss_hist)\n    sd[\"losses\"][\"val\"].append(val_loss_hist)\n    sd[\"metrics\"][\"train\"].append(train_auc_hist)\n    sd[\"metrics\"][\"val\"].append(val_auc_hist)\n    sd[\"predictions\"].append(preds_hist)\n    sd[\"ground_truth\"].append(truths_hist)\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import roc_auc_score\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# synthetic data function\ndef sample_data(N):\n    N0 = N // 2\n    N1 = N - N0\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N1), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N1)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\n# prepare datasets once\nx_train, y_train = sample_data(1000)\nx_val, y_val = sample_data(200)\ntrain_ds = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\nval_ds = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))\n\n# hyperparameter tuning setup\nbatch_sizes = [16, 32, 64, 128]\nexperiment_data = {\n    \"batch_size\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": {}, \"val\": {}},\n            \"losses\": {\"train\": {}, \"val\": {}},\n            \"predictions\": {},\n            \"ground_truth\": {},\n        }\n    }\n}\n\nepochs = 20\nfor bs in batch_sizes:\n    # data loaders\n    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=bs)\n    # initialize storage for this batch size\n    exp = experiment_data[\"batch_size\"][\"synthetic\"]\n    exp[\"metrics\"][\"train\"][bs] = []\n    exp[\"metrics\"][\"val\"][bs] = []\n    exp[\"losses\"][\"train\"][bs] = []\n    exp[\"losses\"][\"val\"][bs] = []\n    exp[\"predictions\"][bs] = []\n    exp[\"ground_truth\"][bs] = []\n\n    # model and optimizer\n    model = nn.Linear(1, 1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    loss_fn = nn.BCEWithLogitsLoss()\n\n    # training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        train_losses, all_preds, all_labels = [], [], []\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits = model(xb).squeeze(1)\n            loss = loss_fn(logits, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_losses.append(loss.item())\n            all_preds.append(torch.sigmoid(logits).cpu().numpy())\n            all_labels.append(yb.cpu().numpy())\n        train_loss = np.mean(train_losses)\n        train_preds = np.concatenate(all_preds)\n        train_labels = np.concatenate(all_labels)\n        train_auc = roc_auc_score(train_labels, train_preds)\n\n        # validation\n        model.eval()\n        val_losses, v_preds, v_labels = [], [], []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits = model(xb).squeeze(1)\n                val_losses.append(loss_fn(logits, yb).item())\n                v_preds.append(torch.sigmoid(logits).cpu().numpy())\n                v_labels.append(yb.cpu().numpy())\n        val_loss = np.mean(val_losses)\n        val_preds = np.concatenate(v_preds)\n        val_labels = np.concatenate(v_labels)\n        val_auc = roc_auc_score(val_labels, val_preds)\n\n        print(\n            f\"Batch {bs} Epoch {epoch}: val_loss={val_loss:.4f}, val_auc={val_auc:.4f}\"\n        )\n\n        # record\n        exp[\"metrics\"][\"train\"][bs].append(train_auc)\n        exp[\"metrics\"][\"val\"][bs].append(val_auc)\n        exp[\"losses\"][\"train\"][bs].append(train_loss)\n        exp[\"losses\"][\"val\"][bs].append(val_loss)\n        exp[\"predictions\"][bs].append(val_preds)\n        exp[\"ground_truth\"][bs].append(val_labels)\n\n# save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import roc_auc_score\n\n# Setup working directory and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic data generation\ndef sample_data(N):\n    N0 = N // 2\n    N1 = N - N0\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N1), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N1)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\n# Prepare datasets\nx_train, y_train = sample_data(1000)\nx_val, y_val = sample_data(200)\ntrain_ds = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\nval_ds = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=32)\n\n# Hyperparameter tuning setup\nweight_decays = [0, 1e-5, 1e-4, 1e-3]\nexperiment_data = {\n    \"weight_decay\": {\n        \"synthetic\": {\n            \"hyperparams\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Training loop for each weight_decay\nepochs = 20\nfor wd in weight_decays:\n    experiment_data[\"weight_decay\"][\"synthetic\"][\"hyperparams\"].append(wd)\n    model = nn.Linear(1, 1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=wd)\n    loss_fn = nn.BCEWithLogitsLoss()\n    train_losses, val_losses = [], []\n    train_aucs, val_aucs = [], []\n    final_preds, final_labels = None, None\n\n    for epoch in range(1, epochs + 1):\n        # Train\n        model.train()\n        t_losses, t_preds, t_labels = [], [], []\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits = model(xb).squeeze(1)\n            loss = loss_fn(logits, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            t_losses.append(loss.item())\n            t_preds.append(torch.sigmoid(logits).cpu().numpy())\n            t_labels.append(yb.cpu().numpy())\n        train_loss = np.mean(t_losses)\n        train_auc = roc_auc_score(np.concatenate(t_labels), np.concatenate(t_preds))\n\n        # Validate\n        model.eval()\n        v_losses, v_preds, v_labels = [], [], []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits = model(xb).squeeze(1)\n                loss = loss_fn(logits, yb)\n                v_losses.append(loss.item())\n                v_preds.append(torch.sigmoid(logits).cpu().numpy())\n                v_labels.append(yb.cpu().numpy())\n        val_loss = np.mean(v_losses)\n        val_auc = roc_auc_score(np.concatenate(v_labels), np.concatenate(v_preds))\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        train_aucs.append(train_auc)\n        val_aucs.append(val_auc)\n        final_preds = np.concatenate(v_preds)\n        final_labels = np.concatenate(v_labels)\n\n        print(f\"wd={wd} epoch={epoch}: val_loss={val_loss:.4f}, val_auc={val_auc:.4f}\")\n\n    # Record run data\n    sd = experiment_data[\"weight_decay\"][\"synthetic\"]\n    sd[\"losses\"][\"train\"].append(np.array(train_losses))\n    sd[\"losses\"][\"val\"].append(np.array(val_losses))\n    sd[\"metrics\"][\"train\"].append(np.array(train_aucs))\n    sd[\"metrics\"][\"val\"].append(np.array(val_aucs))\n    sd[\"predictions\"].append(final_preds)\n    sd[\"ground_truth\"].append(final_labels)\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import roc_auc_score\nfrom datasets import load_dataset\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef sample_data(N):\n    N0 = N // 2\n    N1 = N - N0\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N1), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N1)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\ndef prepare_synthetic(batch_size=32):\n    x_train, y_train = sample_data(1000)\n    x_val, y_val = sample_data(200)\n    train_ds = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n    val_ds = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))\n    return (\n        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n        DataLoader(val_ds, batch_size=batch_size),\n        1,\n    )\n\n\ndef prepare_tabular(dataset_name, num_feats, label_key, positive_label, batch_size=32):\n    raw = load_dataset(dataset_name)\n    if isinstance(raw, dict):\n        if \"train\" in raw and \"test\" in raw:\n            src_train, src_test = raw[\"train\"], raw[\"test\"]\n        elif \"train\" in raw:\n            split = raw[\"train\"].train_test_split(test_size=0.2, seed=42)\n            src_train, src_test = split[\"train\"], split[\"test\"]\n        else:\n            split = raw.train_test_split(test_size=0.2, seed=42)\n            src_train, src_test = split[\"train\"], split[\"test\"]\n    else:\n        split = raw.train_test_split(test_size=0.2, seed=42)\n        src_train, src_test = split[\"train\"], split[\"test\"]\n    X_train = np.stack([src_train[f] for f in num_feats], axis=1).astype(np.float32)\n    y_train = np.array(\n        [1 if v == positive_label else 0 for v in src_train[label_key]],\n        dtype=np.float32,\n    )\n    X_val = np.stack([src_test[f] for f in num_feats], axis=1).astype(np.float32)\n    y_val = np.array(\n        [1 if v == positive_label else 0 for v in src_test[label_key]], dtype=np.float32\n    )\n    mean, std = X_train.mean(0), X_train.std(0) + 1e-6\n    X_train = (X_train - mean) / std\n    X_val = (X_val - mean) / std\n    train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n    val_ds = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n    return (\n        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n        DataLoader(val_ds, batch_size=batch_size),\n        X_train.shape[1],\n    )\n\n\ndata_preps = {\n    \"synthetic\": prepare_synthetic,\n    \"ieee_fraud_detection\": lambda: prepare_tabular(\n        \"ieee_fraud_detection\",\n        [f\"V{i}\" for i in range(1, 29)] + [\"Amount\", \"Time\"],\n        \"Class\",\n        1,\n    ),\n    \"bank_marketing\": lambda: prepare_tabular(\n        \"bank_marketing\",\n        [\n            \"duration\",\n            \"campaign\",\n            \"pdays\",\n            \"previous\",\n            \"emp.var.rate\",\n            \"cons.price.idx\",\n            \"cons.conf.idx\",\n            \"euribor3m\",\n            \"nr.employed\",\n        ],\n        \"y\",\n        \"yes\",\n    ),\n}\n\nlearning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2]\nepochs = 20\nloss_fn = nn.BCEWithLogitsLoss()\n\nexperiment_data = {}\nfor name, prep in data_preps.items():\n    train_loader, val_loader, input_dim = prep()\n    experiment_data[name] = {\n        \"lrs\": learning_rates,\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    for lr in learning_rates:\n        model = nn.Linear(input_dim, 1).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n        train_loss_hist, val_loss_hist = [], []\n        train_auc_hist, val_auc_hist = [], []\n        preds_hist, truths_hist = [], []\n        for epoch in range(1, epochs + 1):\n            model.train()\n            batch_losses, all_preds, all_labels = [], [], []\n            for xb, yb in train_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits = model(xb).squeeze(1)\n                loss = loss_fn(logits, yb)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                batch_losses.append(loss.item())\n                all_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n                all_labels.append(yb.detach().cpu().numpy())\n            train_loss = np.mean(batch_losses)\n            train_preds = np.concatenate(all_preds)\n            train_labels = np.concatenate(all_labels)\n            train_auc = roc_auc_score(train_labels, train_preds)\n\n            model.eval()\n            v_losses, v_preds, v_labels = [], [], []\n            with torch.no_grad():\n                for xb, yb in val_loader:\n                    xb, yb = xb.to(device), yb.to(device)\n                    logits = model(xb).squeeze(1)\n                    loss = loss_fn(logits, yb)\n                    v_losses.append(loss.item())\n                    v_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n                    v_labels.append(yb.detach().cpu().numpy())\n            val_loss = np.mean(v_losses)\n            val_preds = np.concatenate(v_preds)\n            val_labels = np.concatenate(v_labels)\n            val_auc = roc_auc_score(val_labels, val_preds)\n\n            print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n            train_loss_hist.append(train_loss)\n            val_loss_hist.append(val_loss)\n            train_auc_hist.append(train_auc)\n            val_auc_hist.append(val_auc)\n            preds_hist.append(val_preds)\n            truths_hist.append(val_labels)\n\n        experiment_data[name][\"losses\"][\"train\"].append(train_loss_hist)\n        experiment_data[name][\"losses\"][\"val\"].append(val_loss_hist)\n        experiment_data[name][\"metrics\"][\"train\"].append(train_auc_hist)\n        experiment_data[name][\"metrics\"][\"val\"].append(val_auc_hist)\n        experiment_data[name][\"predictions\"].append(preds_hist)\n        experiment_data[name][\"ground_truth\"].append(truths_hist)\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nfrom datasets import load_dataset\n\n# GPU/CPU setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic data generator\ndef sample_data(N, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    N0 = N // 2\n    N1 = N - N0\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N1), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N1)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\n# Prepare synthetic dataset\nx_train_syn, y_train_syn = sample_data(1000, seed=42)\nx_val_syn, y_val_syn = sample_data(200, seed=123)\n# standardize\nmean_syn = x_train_syn.mean(axis=0)\nstd_syn = x_train_syn.std(axis=0) + 1e-8\nx_train_syn = (x_train_syn - mean_syn) / std_syn\nx_val_syn = (x_val_syn - mean_syn) / std_syn\ntrain_ds_syn = TensorDataset(\n    torch.from_numpy(x_train_syn), torch.from_numpy(y_train_syn)\n)\nval_ds_syn = TensorDataset(torch.from_numpy(x_val_syn), torch.from_numpy(y_val_syn))\ntrain_loader_syn = DataLoader(train_ds_syn, batch_size=32, shuffle=True)\nval_loader_syn = DataLoader(val_ds_syn, batch_size=32)\n\n\n# Helper to load and preprocess OpenML numeric datasets\ndef prepare_openml(dataset_id, test_size=0.2, seed=42):\n    ds = load_dataset(\"openml\", str(dataset_id))\n    split = ds[\"train\"].train_test_split(test_size=test_size, seed=seed)\n    out = {}\n    for part in [\"train\", \"test\"]:\n        dd = split[part]\n        cols = dd.column_names\n        feats = cols[:-1]\n        X = np.array([[row[c] for c in feats] for row in dd]).astype(np.float32)\n        y = np.array(dd[cols[-1]]).astype(np.float32)\n        # standardize by train stats\n        if part == \"train\":\n            mu, sig = X.mean(axis=0), X.std(axis=0) + 1e-8\n        X = (X - mu) / sig\n        out[part] = (X, y)\n    return out[\"train\"], out[\"test\"]\n\n\n# Load two new HF datasets\n(train_X_dia, train_y_dia), (val_X_dia, val_y_dia) = prepare_openml(37)  # Pima Diabetes\n(train_X_bld, train_y_bld), (val_X_bld, val_y_bld) = prepare_openml(\n    1461\n)  # Blood Transfusion\n\n# Wrap into loaders\ntrain_loader_dia = DataLoader(\n    TensorDataset(torch.from_numpy(train_X_dia), torch.from_numpy(train_y_dia)),\n    batch_size=32,\n    shuffle=True,\n)\nval_loader_dia = DataLoader(\n    TensorDataset(torch.from_numpy(val_X_dia), torch.from_numpy(val_y_dia)),\n    batch_size=32,\n)\ntrain_loader_bld = DataLoader(\n    TensorDataset(torch.from_numpy(train_X_bld), torch.from_numpy(train_y_bld)),\n    batch_size=32,\n    shuffle=True,\n)\nval_loader_bld = DataLoader(\n    TensorDataset(torch.from_numpy(val_X_bld), torch.from_numpy(val_y_bld)),\n    batch_size=32,\n)\n\n# Collect all datasets\ndatasets = {\n    \"synthetic\": {\"train\": train_loader_syn, \"val\": val_loader_syn},\n    \"pima_diabetes\": {\"train\": train_loader_dia, \"val\": val_loader_dia},\n    \"blood_transfusion\": {\"train\": train_loader_bld, \"val\": val_loader_bld},\n}\n\n# Experiment bookkeeping\nweight_decays = [0, 1e-5, 1e-4, 1e-3]\nexperiment_data = {}\nfor name in datasets:\n    experiment_data[name] = {\n        \"hyperparams\": list(weight_decays),\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n# Training & evaluation\nepochs = 20\nfor ds_name, loaders in datasets.items():\n    train_loader, val_loader = loaders[\"train\"], loaders[\"val\"]\n    # infer input dim\n    sample_x, _ = next(iter(train_loader))\n    input_dim = sample_x.shape[1]\n    for wd in weight_decays:\n        model = nn.Linear(input_dim, 1).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=wd)\n        loss_fn = nn.BCEWithLogitsLoss()\n        train_losses, val_losses = [], []\n        train_aucs, val_aucs = [], []\n        final_preds, final_labels = None, None\n\n        for epoch in range(1, epochs + 1):\n            # train\n            model.train()\n            tlosses, tps, tls = [], [], []\n            for xb, yb in train_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits = model(xb).squeeze(1)\n                loss = loss_fn(logits, yb)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                tlosses.append(loss.item())\n                tps.append(torch.sigmoid(logits).detach().cpu().numpy())\n                tls.append(yb.detach().cpu().numpy())\n            tr_loss = np.mean(tlosses)\n            tr_auc = roc_auc_score(np.concatenate(tls), np.concatenate(tps))\n\n            # validate\n            model.eval()\n            vlosses, vps, vls = [], [], []\n            with torch.no_grad():\n                for xb, yb in val_loader:\n                    xb, yb = xb.to(device), yb.to(device)\n                    logits = model(xb).squeeze(1)\n                    loss = loss_fn(logits, yb)\n                    vlosses.append(loss.item())\n                    vps.append(torch.sigmoid(logits).detach().cpu().numpy())\n                    vls.append(yb.detach().cpu().numpy())\n            va_loss = np.mean(vlosses)\n            va_auc = roc_auc_score(np.concatenate(vls), np.concatenate(vps))\n\n            train_losses.append(tr_loss)\n            val_losses.append(va_loss)\n            train_aucs.append(tr_auc)\n            val_aucs.append(va_auc)\n            final_preds = np.concatenate(vps)\n            final_labels = np.concatenate(vls)\n            print(\n                f\"{ds_name} wd={wd} epoch={epoch}: val_loss={va_loss:.4f}, val_auc={va_auc:.4f}\"\n            )\n\n        sd = experiment_data[ds_name]\n        sd[\"losses\"][\"train\"].append(np.array(train_losses))\n        sd[\"losses\"][\"val\"].append(np.array(val_losses))\n        sd[\"metrics\"][\"train\"].append(np.array(train_aucs))\n        sd[\"metrics\"][\"val\"].append(np.array(val_aucs))\n        sd[\"predictions\"].append(final_preds)\n        sd[\"ground_truth\"].append(final_labels)\n\n# Save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import roc_auc_score\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic data generation\ndef sample_data(N):\n    N0 = N // 2\n    N1 = N - N0\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N1), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N1)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\n# Prepare datasets\nx_train, y_train = sample_data(1000)\nx_val, y_val = sample_data(200)\ntrain_ds = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\nval_ds = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=32)\n\n# Experiment data structure\nexperiment_data = {\n    \"adam_beta1\": {\n        \"synthetic\": {\n            \"beta1_list\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Hyperparameter sweep\nbetas = [0.5, 0.8, 0.9, 0.99]\nepochs = 20\n\nfor beta in betas:\n    print(f\"Running Adam beta1={beta}\")\n    model = nn.Linear(1, 1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(beta, 0.999))\n    loss_fn = nn.BCEWithLogitsLoss()\n\n    train_auc_list, val_auc_list = [], []\n    train_loss_list, val_loss_list = [], []\n    final_preds, final_labels = None, None\n\n    for epoch in range(1, epochs + 1):\n        # Training\n        model.train()\n        t_losses, t_preds, t_labels = [], [], []\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits = model(xb).squeeze(1)\n            loss = loss_fn(logits, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            t_losses.append(loss.item())\n            t_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n            t_labels.append(yb.detach().cpu().numpy())\n        train_loss = np.mean(t_losses)\n        train_auc = roc_auc_score(np.concatenate(t_labels), np.concatenate(t_preds))\n        train_loss_list.append(train_loss)\n        train_auc_list.append(train_auc)\n\n        # Validation\n        model.eval()\n        v_losses, v_preds, v_labels = [], [], []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits = model(xb).squeeze(1)\n                loss = loss_fn(logits, yb)\n                v_losses.append(loss.item())\n                v_preds.append(torch.sigmoid(logits).cpu().numpy())\n                v_labels.append(yb.cpu().numpy())\n        val_loss = np.mean(v_losses)\n        val_preds_full = np.concatenate(v_preds)\n        val_labels_full = np.concatenate(v_labels)\n        val_auc = roc_auc_score(val_labels_full, val_preds_full)\n        val_loss_list.append(val_loss)\n        val_auc_list.append(val_auc)\n\n        print(f\"  Epoch {epoch}: val_loss={val_loss:.4f}, val_auc={val_auc:.4f}\")\n        if epoch == epochs:\n            final_preds, final_labels = val_preds_full, val_labels_full\n\n    # Record results\n    sd = experiment_data[\"adam_beta1\"][\"synthetic\"]\n    sd[\"beta1_list\"].append(beta)\n    sd[\"metrics\"][\"train\"].append(train_auc_list)\n    sd[\"metrics\"][\"val\"].append(val_auc_list)\n    sd[\"losses\"][\"train\"].append(train_loss_list)\n    sd[\"losses\"][\"val\"].append(val_loss_list)\n    sd[\"predictions\"].append(final_preds)\n    sd[\"ground_truth\"].append(final_labels)\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import roc_auc_score\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic data generation\ndef sample_data(N):\n    N0 = N // 2\n    N1 = N - N0\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N1), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N1)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\n# Prepare datasets once\nx_train, y_train = sample_data(1000)\nx_val, y_val = sample_data(200)\ntrain_ds = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\nval_ds = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=32)\n\n# Hyperparameter sweep for beta2 values\nbeta2_list = [0.9, 0.95, 0.99, 0.999, 0.9999]\nexperiment_data = {\n    \"adam_beta2\": {\n        \"synthetic\": {\n            \"beta2_values\": beta2_list,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Training settings\nepochs = 20\nlr = 0.01\nbeta1 = 0.9\n\n# Sweep\nfor beta2 in beta2_list:\n    # New model & optimizer for each beta2\n    model = nn.Linear(1, 1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(beta1, beta2))\n    loss_fn = nn.BCEWithLogitsLoss()\n\n    # Per-epoch records\n    train_auc_ep, val_auc_ep = [], []\n    train_loss_ep, val_loss_ep = [], []\n    preds_ep, gt_ep = [], []\n\n    for epoch in range(1, epochs + 1):\n        # Train\n        model.train()\n        losses_tr, preds_tr, labels_tr = [], [], []\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits = model(xb).squeeze(1)\n            loss = loss_fn(logits, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            losses_tr.append(loss.item())\n            preds_tr.append(torch.sigmoid(logits).detach().cpu().numpy())\n            labels_tr.append(yb.cpu().numpy())\n        train_loss = np.mean(losses_tr)\n        train_auc = roc_auc_score(np.concatenate(labels_tr), np.concatenate(preds_tr))\n        train_loss_ep.append(train_loss)\n        train_auc_ep.append(train_auc)\n\n        # Validate\n        model.eval()\n        losses_val, preds_val, labels_val = [], [], []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits = model(xb).squeeze(1)\n                loss = loss_fn(logits, yb)\n                losses_val.append(loss.item())\n                preds_val.append(torch.sigmoid(logits).cpu().numpy())\n                labels_val.append(yb.cpu().numpy())\n        val_loss = np.mean(losses_val)\n        val_auc = roc_auc_score(np.concatenate(labels_val), np.concatenate(preds_val))\n        val_loss_ep.append(val_loss)\n        val_auc_ep.append(val_auc)\n\n        preds_ep.append(np.concatenate(preds_val))\n        gt_ep.append(np.concatenate(labels_val))\n\n        print(\n            f\"beta2={beta2} epoch={epoch} val_loss={val_loss:.4f} val_auc={val_auc:.4f}\"\n        )\n\n    # Store results for this beta2\n    sd = experiment_data[\"adam_beta2\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(train_auc_ep)\n    sd[\"metrics\"][\"val\"].append(val_auc_ep)\n    sd[\"losses\"][\"train\"].append(train_loss_ep)\n    sd[\"losses\"][\"val\"].append(val_loss_ep)\n    sd[\"predictions\"].append(preds_ep)\n    sd[\"ground_truth\"].append(gt_ep)\n\n# Save all data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import roc_auc_score\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic data generation\ndef sample_data(N):\n    N0 = N // 2\n    N1 = N - N0\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N1), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N1)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\n# Prepare datasets\nx_train, y_train = sample_data(1000)\nx_val, y_val = sample_data(200)\ntrain_ds = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\nval_ds = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=32)\n\n# Hyperparameter tuning setup\ngamma_list = [0.9, 0.95, 0.99]\nepochs = 20\nexperiment_data = {\"lr_scheduler_gamma\": {\"synthetic\": {}}}\n\nfor gamma in gamma_list:\n    key = str(gamma)\n    # Initialize storage for this gamma\n    experiment_data[\"lr_scheduler_gamma\"][\"synthetic\"][key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # Model, optimizer, scheduler, loss\n    model = nn.Linear(1, 1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n    loss_fn = nn.BCEWithLogitsLoss()\n\n    # Training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        train_losses, all_preds, all_labels = [], [], []\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits = model(xb).squeeze(1)\n            loss = loss_fn(logits, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_losses.append(loss.item())\n            all_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n            all_labels.append(yb.cpu().numpy())\n        train_loss = np.mean(train_losses)\n        train_preds = np.concatenate(all_preds)\n        train_labels = np.concatenate(all_labels)\n        train_auc = roc_auc_score(train_labels, train_preds)\n\n        # Validation\n        model.eval()\n        val_losses, v_preds, v_labels = [], [], []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits = model(xb).squeeze(1)\n                loss = loss_fn(logits, yb)\n                val_losses.append(loss.item())\n                v_preds.append(torch.sigmoid(logits).cpu().numpy())\n                v_labels.append(yb.cpu().numpy())\n        val_loss = np.mean(val_losses)\n        val_preds = np.concatenate(v_preds)\n        val_labels = np.concatenate(v_labels)\n        val_auc = roc_auc_score(val_labels, val_preds)\n\n        print(\n            f\"Gamma {gamma} Epoch {epoch}: val_loss = {val_loss:.4f}, val_auc = {val_auc:.4f}\"\n        )\n\n        # Record metrics\n        exp = experiment_data[\"lr_scheduler_gamma\"][\"synthetic\"][key]\n        exp[\"losses\"][\"train\"].append(train_loss)\n        exp[\"losses\"][\"val\"].append(val_loss)\n        exp[\"metrics\"][\"train\"].append(train_auc)\n        exp[\"metrics\"][\"val\"].append(val_auc)\n        exp[\"predictions\"].append(val_preds)\n        exp[\"ground_truth\"].append(val_labels)\n\n        # Step scheduler\n        scheduler.step()\n\n# Save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import roc_auc_score\nfrom datasets import load_dataset\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef sample_data(N):\n    N0 = N // 2\n    N1 = N - N0\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N1), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N1)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\ndef prepare_synthetic(batch_size=32):\n    x_train, y_train = sample_data(1000)\n    x_val, y_val = sample_data(200)\n    train_ds = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n    val_ds = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))\n    return (\n        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n        DataLoader(val_ds, batch_size=batch_size),\n        1,\n    )\n\n\ndef prepare_tabular(dataset_name, num_feats, label_key, positive_label, batch_size=32):\n    raw = load_dataset(dataset_name)\n    # handle splits\n    if isinstance(raw, dict):\n        if \"train\" in raw and \"test\" in raw:\n            src_train, src_test = raw[\"train\"], raw[\"test\"]\n        elif \"train\" in raw:\n            split = raw[\"train\"].train_test_split(test_size=0.2, seed=42)\n            src_train, src_test = split[\"train\"], split[\"test\"]\n        else:\n            split = raw.train_test_split(test_size=0.2, seed=42)\n            src_train, src_test = split[\"train\"], split[\"test\"]\n    else:\n        split = raw.train_test_split(test_size=0.2, seed=42)\n        src_train, src_test = split[\"train\"], split[\"test\"]\n    # extract features and labels\n    X_train = np.stack([src_train[f] for f in num_feats], axis=1).astype(np.float32)\n    y_train = np.array(\n        [1 if v == positive_label else 0 for v in src_train[label_key]],\n        dtype=np.float32,\n    )\n    X_val = np.stack([src_test[f] for f in num_feats], axis=1).astype(np.float32)\n    y_val = np.array(\n        [1 if v == positive_label else 0 for v in src_test[label_key]],\n        dtype=np.float32,\n    )\n    # normalize\n    mean, std = X_train.mean(0), X_train.std(0) + 1e-6\n    X_train = (X_train - mean) / std\n    X_val = (X_val - mean) / std\n    train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n    val_ds = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n    return (\n        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n        DataLoader(val_ds, batch_size=batch_size),\n        X_train.shape[1],\n    )\n\n\n# define datasets\ndata_preps = {\n    \"synthetic\": prepare_synthetic,\n    \"bank_marketing\": lambda: prepare_tabular(\n        \"bank_marketing\",\n        [\n            \"duration\",\n            \"campaign\",\n            \"pdays\",\n            \"previous\",\n            \"emp.var.rate\",\n            \"cons.price.idx\",\n            \"cons.conf.idx\",\n            \"euribor3m\",\n            \"nr.employed\",\n        ],\n        \"y\",\n        \"yes\",\n    ),\n    \"creditcard\": lambda: prepare_tabular(\n        \"creditcard\",\n        [f\"V{i}\" for i in range(1, 29)] + [\"Time\", \"Amount\"],\n        \"Class\",\n        1,\n    ),\n    \"adult\": lambda: prepare_tabular(\n        \"adult\",\n        [\n            \"age\",\n            \"fnlwgt\",\n            \"education-num\",\n            \"capital-gain\",\n            \"capital-loss\",\n            \"hours-per-week\",\n        ],\n        \"label\",\n        1,\n    ),\n}\n\nlearning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2]\nepochs = 20\nloss_fn = nn.BCEWithLogitsLoss()\n\nexperiment_data = {}\nfor name, prep in data_preps.items():\n    train_loader, val_loader, input_dim = prep()\n    experiment_data[name] = {\n        \"lrs\": learning_rates,\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    for lr in learning_rates:\n        model = nn.Linear(input_dim, 1).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n        train_loss_hist, val_loss_hist = [], []\n        train_auc_hist, val_auc_hist = [], []\n        preds_hist, truths_hist = [], []\n        for epoch in range(1, epochs + 1):\n            model.train()\n            batch_losses, all_preds, all_labels = [], [], []\n            for xb, yb in train_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits = model(xb).squeeze(1)\n                loss = loss_fn(logits, yb)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                batch_losses.append(loss.item())\n                all_preds.append(torch.sigmoid(logits).cpu().numpy())\n                all_labels.append(yb.cpu().numpy())\n            train_loss = np.mean(batch_losses)\n            train_preds = np.concatenate(all_preds)\n            train_labels = np.concatenate(all_labels)\n            train_auc = roc_auc_score(train_labels, train_preds)\n\n            model.eval()\n            v_losses, v_preds, v_labels = [], [], []\n            with torch.no_grad():\n                for xb, yb in val_loader:\n                    xb, yb = xb.to(device), yb.to(device)\n                    logits = model(xb).squeeze(1)\n                    loss = loss_fn(logits, yb)\n                    v_losses.append(loss.item())\n                    v_preds.append(torch.sigmoid(logits).cpu().numpy())\n                    v_labels.append(yb.cpu().numpy())\n            val_loss = np.mean(v_losses)\n            val_preds = np.concatenate(v_preds)\n            val_labels = np.concatenate(v_labels)\n            val_auc = roc_auc_score(val_labels, val_preds)\n\n            print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n            train_loss_hist.append(train_loss)\n            val_loss_hist.append(val_loss)\n            train_auc_hist.append(train_auc)\n            val_auc_hist.append(val_auc)\n            preds_hist.append(val_preds)\n            truths_hist.append(val_labels)\n\n        experiment_data[name][\"losses\"][\"train\"].append(train_loss_hist)\n        experiment_data[name][\"losses\"][\"val\"].append(val_loss_hist)\n        experiment_data[name][\"metrics\"][\"train\"].append(train_auc_hist)\n        experiment_data[name][\"metrics\"][\"val\"].append(val_auc_hist)\n        experiment_data[name][\"predictions\"].append(preds_hist)\n        experiment_data[name][\"ground_truth\"].append(truths_hist)\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import roc_auc_score\n\n# Working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic data\ndef sample_data(N):\n    N0, N1 = N // 2, N - N // 2\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N1), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N1)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\nx_train, y_train = sample_data(1000)\nx_val, y_val = sample_data(200)\ntrain_ds = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\nval_ds = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=32)\n\n# Experiment data structure\nexperiment_data = {\n    \"adam_eps\": {\n        \"synthetic\": {\n            \"eps_values\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Hyperparameter sweep\nepochs = 20\neps_list = [1e-8, 1e-7, 1e-6, 1e-5]\nfor eps in eps_list:\n    # Reinitialize model & optimizer\n    model = nn.Linear(1, 1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, eps=eps)\n    loss_fn = nn.BCEWithLogitsLoss()\n    # Containers per eps\n    train_loss_hist, val_loss_hist = [], []\n    train_auc_hist, val_auc_hist = [], []\n    preds_hist, gts_hist = [], []\n    # Training loop\n    for epoch in range(1, epochs + 1):\n        # Train\n        model.train()\n        tlosses, tp, tl = [], [], []\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits = model(xb).squeeze(1)\n            loss = loss_fn(logits, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            tlosses.append(loss.item())\n            tp.append(torch.sigmoid(logits).cpu().numpy())\n            tl.append(yb.cpu().numpy())\n        train_loss = np.mean(tlosses)\n        train_preds = np.concatenate(tp)\n        train_labels = np.concatenate(tl)\n        train_auc = roc_auc_score(train_labels, train_preds)\n        # Validate\n        model.eval()\n        vlosses, vp, vl = [], [], []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits = model(xb).squeeze(1)\n                vlosses.append(loss_fn(logits, yb).item())\n                vp.append(torch.sigmoid(logits).cpu().numpy())\n                vl.append(yb.cpu().numpy())\n        val_loss = np.mean(vlosses)\n        val_preds = np.concatenate(vp)\n        val_labels = np.concatenate(vl)\n        val_auc = roc_auc_score(val_labels, val_preds)\n        print(f\"eps={eps} Epoch {epoch}: val_loss={val_loss:.4f}\")\n        # Record epoch data\n        train_loss_hist.append(train_loss)\n        val_loss_hist.append(val_loss)\n        train_auc_hist.append(train_auc)\n        val_auc_hist.append(val_auc)\n        preds_hist.append(val_preds)\n        gts_hist.append(val_labels)\n    # Save per-eps results\n    exp = experiment_data[\"adam_eps\"][\"synthetic\"]\n    exp[\"eps_values\"].append(eps)\n    exp[\"losses\"][\"train\"].append(train_loss_hist)\n    exp[\"losses\"][\"val\"].append(val_loss_hist)\n    exp[\"metrics\"][\"train\"].append(train_auc_hist)\n    exp[\"metrics\"][\"val\"].append(val_auc_hist)\n    exp[\"predictions\"].append(preds_hist)\n    exp[\"ground_truth\"].append(gts_hist)\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import roc_auc_score\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic data generator\ndef sample_data(N):\n    N0 = N // 2\n    N1 = N - N0\n    d0 = np.clip(np.random.normal(0, 0.5, N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, N1), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N1)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\n# Prepare datasets once\nx_train, y_train = sample_data(1000)\nx_val, y_val = sample_data(200)\ntrain_ds = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\nval_ds = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=32)\n\n# Hyperparameter settings\nepoch_settings = [50, 100, 200]\n\n# Initialize experiment data structure\nexperiment_data = {\n    \"num_epochs\": {\n        \"synthetic\": {\n            \"epoch_values\": epoch_settings,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Hyperparam tuning loop\nfor epochs in epoch_settings:\n    print(f\"\\nRunning with num_epochs = {epochs}\")\n    # Reinitialize model & optimizer\n    model = nn.Linear(1, 1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    loss_fn = nn.BCEWithLogitsLoss()\n    # Per-epoch records\n    train_aucs, val_aucs = [], []\n    train_losses, val_losses = [], []\n    preds_per_epoch, labels_per_epoch = [], []\n\n    for epoch in range(1, epochs + 1):\n        # Train\n        model.train()\n        t_losses, t_preds, t_labels = [], [], []\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits = model(xb).squeeze(1)\n            loss = loss_fn(logits, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            t_losses.append(loss.item())\n            t_preds.append(torch.sigmoid(logits).cpu().numpy())\n            t_labels.append(yb.cpu().numpy())\n        train_loss = np.mean(t_losses)\n        train_auc = roc_auc_score(np.concatenate(t_labels), np.concatenate(t_preds))\n\n        # Validate\n        model.eval()\n        v_losses, v_preds, v_labels = [], [], []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits = model(xb).squeeze(1)\n                v_losses.append(loss_fn(logits, yb).item())\n                v_preds.append(torch.sigmoid(logits).cpu().numpy())\n                v_labels.append(yb.cpu().numpy())\n        val_loss = np.mean(v_losses)\n        val_auc = roc_auc_score(np.concatenate(v_labels), np.concatenate(v_preds))\n\n        # Record epoch metrics\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        train_aucs.append(train_auc)\n        val_aucs.append(val_auc)\n        preds_per_epoch.append(np.concatenate(v_preds))\n        labels_per_epoch.append(np.concatenate(v_labels))\n\n        print(\n            f\" Epoch {epoch}/{epochs} - train_loss {train_loss:.4f}, val_loss {val_loss:.4f}, val_auc {val_auc:.4f}\"\n        )\n\n    # Append this setting's results\n    sd = experiment_data[\"num_epochs\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(train_aucs)\n    sd[\"metrics\"][\"val\"].append(val_aucs)\n    sd[\"losses\"][\"train\"].append(train_losses)\n    sd[\"losses\"][\"val\"].append(val_losses)\n    sd[\"predictions\"].append(preds_per_epoch)\n    sd[\"ground_truth\"].append(labels_per_epoch)\n\n# Save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nSaved experiment data to {os.path.join(working_dir, 'experiment_data.npy')}\")\n", "import os\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\nfrom datasets import load_dataset\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# synthetic data function\ndef sample_data(N):\n    N0 = N // 2\n    N1 = N - N0\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N1), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N1)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\n# Prepare datasets dict\ndatasets = {}\n\n# 1) Synthetic dataset\nx_tr_s, y_tr_s = sample_data(1000)\nx_val_s, y_val_s = sample_data(200)\nmean_s, std_s = x_tr_s.mean(), x_tr_s.std() + 1e-6\nx_tr_s = (x_tr_s - mean_s) / std_s\nx_val_s = (x_val_s - mean_s) / std_s\ntrain_ds_s = TensorDataset(torch.from_numpy(x_tr_s), torch.from_numpy(y_tr_s))\nval_ds_s = TensorDataset(torch.from_numpy(x_val_s), torch.from_numpy(y_val_s))\ndatasets[\"synthetic\"] = (train_ds_s, val_ds_s)\n\n# 2) SST-2 (sentence length as feature)\nsst2_train = load_dataset(\"glue\", \"sst2\", split=\"train\").shuffle(42).select(range(1000))\nsst2_val = (\n    load_dataset(\"glue\", \"sst2\", split=\"validation\").shuffle(42).select(range(200))\n)\nx_tr_st = np.array(\n    [len(s.split()) for s in sst2_train[\"sentence\"]], dtype=np.float32\n).reshape(-1, 1)\ny_tr_st = np.array(sst2_train[\"label\"], dtype=np.float32)\nx_val_st = np.array(\n    [len(s.split()) for s in sst2_val[\"sentence\"]], dtype=np.float32\n).reshape(-1, 1)\ny_val_st = np.array(sst2_val[\"label\"], dtype=np.float32)\nmean_st, std_st = x_tr_st.mean(), x_tr_st.std() + 1e-6\nx_tr_st = (x_tr_st - mean_st) / std_st\nx_val_st = (x_val_st - mean_st) / std_st\ntrain_ds_st = TensorDataset(torch.from_numpy(x_tr_st), torch.from_numpy(y_tr_st))\nval_ds_st = TensorDataset(torch.from_numpy(x_val_st), torch.from_numpy(y_val_st))\ndatasets[\"sst2\"] = (train_ds_st, val_ds_st)\n\n# 3) Yelp Polarity (text length as feature)\nyelp_train = (\n    load_dataset(\"yelp_polarity\", split=\"train\").shuffle(42).select(range(1000))\n)\nyelp_val = load_dataset(\"yelp_polarity\", split=\"test\").shuffle(42).select(range(200))\nx_tr_yp = np.array(\n    [len(t.split()) for t in yelp_train[\"text\"]], dtype=np.float32\n).reshape(-1, 1)\ny_tr_yp = np.array(yelp_train[\"label\"], dtype=np.float32)\nx_val_yp = np.array(\n    [len(t.split()) for t in yelp_val[\"text\"]], dtype=np.float32\n).reshape(-1, 1)\ny_val_yp = np.array(yelp_val[\"label\"], dtype=np.float32)\nmean_yp, std_yp = x_tr_yp.mean(), x_tr_yp.std() + 1e-6\nx_tr_yp = (x_tr_yp - mean_yp) / std_yp\nx_val_yp = (x_val_yp - mean_yp) / std_yp\ntrain_ds_yp = TensorDataset(torch.from_numpy(x_tr_yp), torch.from_numpy(y_tr_yp))\nval_ds_yp = TensorDataset(torch.from_numpy(x_val_yp), torch.from_numpy(y_val_yp))\ndatasets[\"yelp_polarity\"] = (train_ds_yp, val_ds_yp)\n\n# Hyperparameters\nbatch_sizes = [32, 64]\nlearning_rates = [0.001, 0.01]\nepochs = 20\n\n# Initialize experiment_data\nexperiment_data = {}\nfor name in datasets:\n    experiment_data[name] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n# Training and evaluation\nfor name, (train_ds, val_ds) in datasets.items():\n    for lr in learning_rates:\n        for bs in batch_sizes:\n            train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n            val_loader = DataLoader(val_ds, batch_size=bs)\n            model = nn.Linear(1, 1).to(device)\n            optimizer = Adam(model.parameters(), lr=lr)\n            loss_fn = nn.BCEWithLogitsLoss()\n            for epoch in range(1, epochs + 1):\n                # training\n                model.train()\n                t_losses, t_preds, t_labels = [], [], []\n                for xb, yb in train_loader:\n                    xb, yb = xb.to(device), yb.to(device)\n                    logits = model(xb).squeeze(1)\n                    loss = loss_fn(logits, yb)\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n                    t_losses.append(loss.item())\n                    t_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n                    t_labels.append(yb.cpu().numpy())\n                train_loss = np.mean(t_losses)\n                train_auc = roc_auc_score(\n                    np.concatenate(t_labels), np.concatenate(t_preds)\n                )\n                # validation\n                model.eval()\n                v_losses, v_preds, v_labels = [], [], []\n                with torch.no_grad():\n                    for xb, yb in val_loader:\n                        xb, yb = xb.to(device), yb.to(device)\n                        logits = model(xb).squeeze(1)\n                        loss = loss_fn(logits, yb)\n                        v_losses.append(loss.item())\n                        v_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n                        v_labels.append(yb.cpu().numpy())\n                val_loss = np.mean(v_losses)\n                val_preds = np.concatenate(v_preds)\n                val_labels = np.concatenate(v_labels)\n                val_auc = roc_auc_score(val_labels, val_preds)\n                print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n                # Record results\n                exp = experiment_data[name]\n                exp[\"metrics\"][\"train\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"auc\": train_auc}\n                )\n                exp[\"metrics\"][\"val\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"auc\": val_auc}\n                )\n                exp[\"losses\"][\"train\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"loss\": train_loss}\n                )\n                exp[\"losses\"][\"val\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"loss\": val_loss}\n                )\n                exp[\"predictions\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"preds\": val_preds}\n                )\n                exp[\"ground_truth\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"labels\": val_labels}\n                )\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\nfrom datasets import load_dataset\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# synthetic data function\ndef sample_data(N):\n    N0 = N // 2\n    N1 = N - N0\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N1), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N1)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\n# Prepare datasets dict\ndatasets = {}\n\n# 1) Synthetic dataset\nx_tr_s, y_tr_s = sample_data(1000)\nx_val_s, y_val_s = sample_data(200)\nmean_s, std_s = x_tr_s.mean(), x_tr_s.std() + 1e-6\nx_tr_s = (x_tr_s - mean_s) / std_s\nx_val_s = (x_val_s - mean_s) / std_s\ntrain_ds_s = TensorDataset(torch.from_numpy(x_tr_s), torch.from_numpy(y_tr_s))\nval_ds_s = TensorDataset(torch.from_numpy(x_val_s), torch.from_numpy(y_val_s))\ndatasets[\"synthetic\"] = (train_ds_s, val_ds_s)\n\n# 2) SST-2 (sentence length as feature)\nsst2_train = load_dataset(\"glue\", \"sst2\", split=\"train\").shuffle(42).select(range(1000))\nsst2_val = (\n    load_dataset(\"glue\", \"sst2\", split=\"validation\").shuffle(42).select(range(200))\n)\nx_tr_st = np.array(\n    [len(s.split()) for s in sst2_train[\"sentence\"]], dtype=np.float32\n).reshape(-1, 1)\ny_tr_st = np.array(sst2_train[\"label\"], dtype=np.float32)\nx_val_st = np.array(\n    [len(s.split()) for s in sst2_val[\"sentence\"]], dtype=np.float32\n).reshape(-1, 1)\ny_val_st = np.array(sst2_val[\"label\"], dtype=np.float32)\nmean_st, std_st = x_tr_st.mean(), x_tr_st.std() + 1e-6\nx_tr_st = (x_tr_st - mean_st) / std_st\nx_val_st = (x_val_st - mean_st) / std_st\ntrain_ds_st = TensorDataset(torch.from_numpy(x_tr_st), torch.from_numpy(y_tr_st))\nval_ds_st = TensorDataset(torch.from_numpy(x_val_st), torch.from_numpy(y_val_st))\ndatasets[\"sst2\"] = (train_ds_st, val_ds_st)\n\n# 3) Yelp Polarity (text length as feature)\nyelp_train = (\n    load_dataset(\"yelp_polarity\", split=\"train\").shuffle(42).select(range(1000))\n)\nyelp_val = load_dataset(\"yelp_polarity\", split=\"test\").shuffle(42).select(range(200))\nx_tr_yp = np.array(\n    [len(t.split()) for t in yelp_train[\"text\"]], dtype=np.float32\n).reshape(-1, 1)\ny_tr_yp = np.array(yelp_train[\"label\"], dtype=np.float32)\nx_val_yp = np.array(\n    [len(t.split()) for t in yelp_val[\"text\"]], dtype=np.float32\n).reshape(-1, 1)\ny_val_yp = np.array(yelp_val[\"label\"], dtype=np.float32)\nmean_yp, std_yp = x_tr_yp.mean(), x_tr_yp.std() + 1e-6\nx_tr_yp = (x_tr_yp - mean_yp) / std_yp\nx_val_yp = (x_val_yp - mean_yp) / std_yp\ntrain_ds_yp = TensorDataset(torch.from_numpy(x_tr_yp), torch.from_numpy(y_tr_yp))\nval_ds_yp = TensorDataset(torch.from_numpy(x_val_yp), torch.from_numpy(y_val_yp))\ndatasets[\"yelp_polarity\"] = (train_ds_yp, val_ds_yp)\n\n# Hyperparameters\nbatch_sizes = [32, 64]\nlearning_rates = [0.001, 0.01]\nepochs = 20\n\n# Initialize experiment_data\nexperiment_data = {}\nfor name in datasets:\n    experiment_data[name] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n# Training and evaluation\nfor name, (train_ds, val_ds) in datasets.items():\n    for lr in learning_rates:\n        for bs in batch_sizes:\n            train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n            val_loader = DataLoader(val_ds, batch_size=bs)\n            model = nn.Linear(1, 1).to(device)\n            optimizer = Adam(model.parameters(), lr=lr)\n            loss_fn = nn.BCEWithLogitsLoss()\n            for epoch in range(1, epochs + 1):\n                # training\n                model.train()\n                t_losses, t_preds, t_labels = [], [], []\n                for xb, yb in train_loader:\n                    xb, yb = xb.to(device), yb.to(device)\n                    logits = model(xb).squeeze(1)\n                    loss = loss_fn(logits, yb)\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n                    t_losses.append(loss.item())\n                    t_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n                    t_labels.append(yb.cpu().numpy())\n                train_loss = np.mean(t_losses)\n                train_auc = roc_auc_score(\n                    np.concatenate(t_labels), np.concatenate(t_preds)\n                )\n                # validation\n                model.eval()\n                v_losses, v_preds, v_labels = [], [], []\n                with torch.no_grad():\n                    for xb, yb in val_loader:\n                        xb, yb = xb.to(device), yb.to(device)\n                        logits = model(xb).squeeze(1)\n                        loss = loss_fn(logits, yb)\n                        v_losses.append(loss.item())\n                        v_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n                        v_labels.append(yb.cpu().numpy())\n                val_loss = np.mean(v_losses)\n                val_preds = np.concatenate(v_preds)\n                val_labels = np.concatenate(v_labels)\n                val_auc = roc_auc_score(val_labels, val_preds)\n                print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n                # Record results\n                exp = experiment_data[name]\n                exp[\"metrics\"][\"train\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"auc\": train_auc}\n                )\n                exp[\"metrics\"][\"val\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"auc\": val_auc}\n                )\n                exp[\"losses\"][\"train\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"loss\": train_loss}\n                )\n                exp[\"losses\"][\"val\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"loss\": val_loss}\n                )\n                exp[\"predictions\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"preds\": val_preds}\n                )\n                exp[\"ground_truth\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"labels\": val_labels}\n                )\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\nfrom datasets import load_dataset\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# synthetic data function\ndef sample_data(N):\n    N0 = N // 2\n    N1 = N - N0\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N1), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N1)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\n# Prepare datasets dict\ndatasets = {}\n\n# 1) Synthetic dataset\nx_tr_s, y_tr_s = sample_data(1000)\nx_val_s, y_val_s = sample_data(200)\nmean_s, std_s = x_tr_s.mean(), x_tr_s.std() + 1e-6\nx_tr_s = (x_tr_s - mean_s) / std_s\nx_val_s = (x_val_s - mean_s) / std_s\ntrain_ds_s = TensorDataset(torch.from_numpy(x_tr_s), torch.from_numpy(y_tr_s))\nval_ds_s = TensorDataset(torch.from_numpy(x_val_s), torch.from_numpy(y_val_s))\ndatasets[\"synthetic\"] = (train_ds_s, val_ds_s)\n\n# 2) SST-2 (sentence length as feature)\nsst2_train = load_dataset(\"glue\", \"sst2\", split=\"train\").shuffle(42).select(range(1000))\nsst2_val = (\n    load_dataset(\"glue\", \"sst2\", split=\"validation\").shuffle(42).select(range(200))\n)\nx_tr_st = np.array(\n    [len(s.split()) for s in sst2_train[\"sentence\"]], dtype=np.float32\n).reshape(-1, 1)\ny_tr_st = np.array(sst2_train[\"label\"], dtype=np.float32)\nx_val_st = np.array(\n    [len(s.split()) for s in sst2_val[\"sentence\"]], dtype=np.float32\n).reshape(-1, 1)\ny_val_st = np.array(sst2_val[\"label\"], dtype=np.float32)\nmean_st, std_st = x_tr_st.mean(), x_tr_st.std() + 1e-6\nx_tr_st = (x_tr_st - mean_st) / std_st\nx_val_st = (x_val_st - mean_st) / std_st\ntrain_ds_st = TensorDataset(torch.from_numpy(x_tr_st), torch.from_numpy(y_tr_st))\nval_ds_st = TensorDataset(torch.from_numpy(x_val_st), torch.from_numpy(y_val_st))\ndatasets[\"sst2\"] = (train_ds_st, val_ds_st)\n\n# 3) Yelp Polarity (text length as feature)\nyelp_train = (\n    load_dataset(\"yelp_polarity\", split=\"train\").shuffle(42).select(range(1000))\n)\nyelp_val = load_dataset(\"yelp_polarity\", split=\"test\").shuffle(42).select(range(200))\nx_tr_yp = np.array(\n    [len(t.split()) for t in yelp_train[\"text\"]], dtype=np.float32\n).reshape(-1, 1)\ny_tr_yp = np.array(yelp_train[\"label\"], dtype=np.float32)\nx_val_yp = np.array(\n    [len(t.split()) for t in yelp_val[\"text\"]], dtype=np.float32\n).reshape(-1, 1)\ny_val_yp = np.array(yelp_val[\"label\"], dtype=np.float32)\nmean_yp, std_yp = x_tr_yp.mean(), x_tr_yp.std() + 1e-6\nx_tr_yp = (x_tr_yp - mean_yp) / std_yp\nx_val_yp = (x_val_yp - mean_yp) / std_yp\ntrain_ds_yp = TensorDataset(torch.from_numpy(x_tr_yp), torch.from_numpy(y_tr_yp))\nval_ds_yp = TensorDataset(torch.from_numpy(x_val_yp), torch.from_numpy(y_val_yp))\ndatasets[\"yelp_polarity\"] = (train_ds_yp, val_ds_yp)\n\n# Hyperparameters\nbatch_sizes = [32, 64]\nlearning_rates = [0.001, 0.01]\nepochs = 20\n\n# Initialize experiment_data\nexperiment_data = {}\nfor name in datasets:\n    experiment_data[name] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n# Training and evaluation\nfor name, (train_ds, val_ds) in datasets.items():\n    for lr in learning_rates:\n        for bs in batch_sizes:\n            train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n            val_loader = DataLoader(val_ds, batch_size=bs)\n            model = nn.Linear(1, 1).to(device)\n            optimizer = Adam(model.parameters(), lr=lr)\n            loss_fn = nn.BCEWithLogitsLoss()\n            for epoch in range(1, epochs + 1):\n                # training\n                model.train()\n                t_losses, t_preds, t_labels = [], [], []\n                for xb, yb in train_loader:\n                    xb, yb = xb.to(device), yb.to(device)\n                    logits = model(xb).squeeze(1)\n                    loss = loss_fn(logits, yb)\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n                    t_losses.append(loss.item())\n                    t_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n                    t_labels.append(yb.cpu().numpy())\n                train_loss = np.mean(t_losses)\n                train_auc = roc_auc_score(\n                    np.concatenate(t_labels), np.concatenate(t_preds)\n                )\n                # validation\n                model.eval()\n                v_losses, v_preds, v_labels = [], [], []\n                with torch.no_grad():\n                    for xb, yb in val_loader:\n                        xb, yb = xb.to(device), yb.to(device)\n                        logits = model(xb).squeeze(1)\n                        loss = loss_fn(logits, yb)\n                        v_losses.append(loss.item())\n                        v_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n                        v_labels.append(yb.cpu().numpy())\n                val_loss = np.mean(v_losses)\n                val_preds = np.concatenate(v_preds)\n                val_labels = np.concatenate(v_labels)\n                val_auc = roc_auc_score(val_labels, val_preds)\n                print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n                # Record results\n                exp = experiment_data[name]\n                exp[\"metrics\"][\"train\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"auc\": train_auc}\n                )\n                exp[\"metrics\"][\"val\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"auc\": val_auc}\n                )\n                exp[\"losses\"][\"train\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"loss\": train_loss}\n                )\n                exp[\"losses\"][\"val\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"loss\": val_loss}\n                )\n                exp[\"predictions\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"preds\": val_preds}\n                )\n                exp[\"ground_truth\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"labels\": val_labels}\n                )\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\nfrom datasets import load_dataset\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# synthetic data function\ndef sample_data(N):\n    N0 = N // 2\n    N1 = N - N0\n    d0 = np.clip(np.random.normal(0, 0.5, size=N0), 0, None)\n    d1 = np.clip(np.random.normal(2, 1.0, size=N1), 0, None)\n    xs = np.concatenate([d0, d1]).astype(np.float32).reshape(-1, 1)\n    ys = np.concatenate([np.zeros(N0), np.ones(N1)]).astype(np.float32)\n    idx = np.random.permutation(N)\n    return xs[idx], ys[idx]\n\n\n# Prepare datasets dict\ndatasets = {}\n\n# 1) Synthetic dataset\nx_tr_s, y_tr_s = sample_data(1000)\nx_val_s, y_val_s = sample_data(200)\nmean_s, std_s = x_tr_s.mean(), x_tr_s.std() + 1e-6\nx_tr_s = (x_tr_s - mean_s) / std_s\nx_val_s = (x_val_s - mean_s) / std_s\ntrain_ds_s = TensorDataset(torch.from_numpy(x_tr_s), torch.from_numpy(y_tr_s))\nval_ds_s = TensorDataset(torch.from_numpy(x_val_s), torch.from_numpy(y_val_s))\ndatasets[\"synthetic\"] = (train_ds_s, val_ds_s)\n\n# 2) SST-2 (sentence length as feature)\nsst2_train = load_dataset(\"glue\", \"sst2\", split=\"train\").shuffle(42).select(range(1000))\nsst2_val = (\n    load_dataset(\"glue\", \"sst2\", split=\"validation\").shuffle(42).select(range(200))\n)\nx_tr_st = np.array(\n    [len(s.split()) for s in sst2_train[\"sentence\"]], dtype=np.float32\n).reshape(-1, 1)\ny_tr_st = np.array(sst2_train[\"label\"], dtype=np.float32)\nx_val_st = np.array(\n    [len(s.split()) for s in sst2_val[\"sentence\"]], dtype=np.float32\n).reshape(-1, 1)\ny_val_st = np.array(sst2_val[\"label\"], dtype=np.float32)\nmean_st, std_st = x_tr_st.mean(), x_tr_st.std() + 1e-6\nx_tr_st = (x_tr_st - mean_st) / std_st\nx_val_st = (x_val_st - mean_st) / std_st\ntrain_ds_st = TensorDataset(torch.from_numpy(x_tr_st), torch.from_numpy(y_tr_st))\nval_ds_st = TensorDataset(torch.from_numpy(x_val_st), torch.from_numpy(y_val_st))\ndatasets[\"sst2\"] = (train_ds_st, val_ds_st)\n\n# 3) Yelp Polarity (text length as feature)\nyelp_train = (\n    load_dataset(\"yelp_polarity\", split=\"train\").shuffle(42).select(range(1000))\n)\nyelp_val = load_dataset(\"yelp_polarity\", split=\"test\").shuffle(42).select(range(200))\nx_tr_yp = np.array(\n    [len(t.split()) for t in yelp_train[\"text\"]], dtype=np.float32\n).reshape(-1, 1)\ny_tr_yp = np.array(yelp_train[\"label\"], dtype=np.float32)\nx_val_yp = np.array(\n    [len(t.split()) for t in yelp_val[\"text\"]], dtype=np.float32\n).reshape(-1, 1)\ny_val_yp = np.array(yelp_val[\"label\"], dtype=np.float32)\nmean_yp, std_yp = x_tr_yp.mean(), x_tr_yp.std() + 1e-6\nx_tr_yp = (x_tr_yp - mean_yp) / std_yp\nx_val_yp = (x_val_yp - mean_yp) / std_yp\ntrain_ds_yp = TensorDataset(torch.from_numpy(x_tr_yp), torch.from_numpy(y_tr_yp))\nval_ds_yp = TensorDataset(torch.from_numpy(x_val_yp), torch.from_numpy(y_val_yp))\ndatasets[\"yelp_polarity\"] = (train_ds_yp, val_ds_yp)\n\n# Hyperparameters\nbatch_sizes = [32, 64]\nlearning_rates = [0.001, 0.01]\nepochs = 20\n\n# Initialize experiment_data\nexperiment_data = {}\nfor name in datasets:\n    experiment_data[name] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n# Training and evaluation\nfor name, (train_ds, val_ds) in datasets.items():\n    for lr in learning_rates:\n        for bs in batch_sizes:\n            train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n            val_loader = DataLoader(val_ds, batch_size=bs)\n            model = nn.Linear(1, 1).to(device)\n            optimizer = Adam(model.parameters(), lr=lr)\n            loss_fn = nn.BCEWithLogitsLoss()\n            for epoch in range(1, epochs + 1):\n                # training\n                model.train()\n                t_losses, t_preds, t_labels = [], [], []\n                for xb, yb in train_loader:\n                    xb, yb = xb.to(device), yb.to(device)\n                    logits = model(xb).squeeze(1)\n                    loss = loss_fn(logits, yb)\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n                    t_losses.append(loss.item())\n                    t_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n                    t_labels.append(yb.cpu().numpy())\n                train_loss = np.mean(t_losses)\n                train_auc = roc_auc_score(\n                    np.concatenate(t_labels), np.concatenate(t_preds)\n                )\n                # validation\n                model.eval()\n                v_losses, v_preds, v_labels = [], [], []\n                with torch.no_grad():\n                    for xb, yb in val_loader:\n                        xb, yb = xb.to(device), yb.to(device)\n                        logits = model(xb).squeeze(1)\n                        loss = loss_fn(logits, yb)\n                        v_losses.append(loss.item())\n                        v_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n                        v_labels.append(yb.cpu().numpy())\n                val_loss = np.mean(v_losses)\n                val_preds = np.concatenate(v_preds)\n                val_labels = np.concatenate(v_labels)\n                val_auc = roc_auc_score(val_labels, val_preds)\n                print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n                # Record results\n                exp = experiment_data[name]\n                exp[\"metrics\"][\"train\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"auc\": train_auc}\n                )\n                exp[\"metrics\"][\"val\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"auc\": val_auc}\n                )\n                exp[\"losses\"][\"train\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"loss\": train_loss}\n                )\n                exp[\"losses\"][\"val\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"loss\": val_loss}\n                )\n                exp[\"predictions\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"preds\": val_preds}\n                )\n                exp[\"ground_truth\"].append(\n                    {\"bs\": bs, \"lr\": lr, \"epoch\": epoch, \"labels\": val_labels}\n                )\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 0.6511', '\\n', 'Epoch\n2: validation_loss = 0.5682', '\\n', 'Epoch 3: validation_loss = 0.5067', '\\n',\n'Epoch 4: validation_loss = 0.4590', '\\n', 'Epoch 5: validation_loss = 0.4226',\n'\\n', 'Epoch 6: validation_loss = 0.3953', '\\n', 'Epoch 7: validation_loss =\n0.3739', '\\n', 'Epoch 8: validation_loss = 0.3571', '\\n', 'Epoch 9:\nvalidation_loss = 0.3436', '\\n', 'Epoch 10: validation_loss = 0.3326', '\\n',\n'Epoch 11: validation_loss = 0.3239', '\\n', 'Epoch 12: validation_loss =\n0.3166', '\\n', 'Epoch 13: validation_loss = 0.3109', '\\n', 'Epoch 14:\nvalidation_loss = 0.3053', '\\n', 'Epoch 15: validation_loss = 0.3009', '\\n',\n'Epoch 16: validation_loss = 0.2971', '\\n', 'Epoch 17: validation_loss =\n0.2943', '\\n', 'Epoch 18: validation_loss = 0.2915', '\\n', 'Epoch 19:\nvalidation_loss = 0.2886', '\\n', 'Epoch 20: validation_loss = 0.2861', '\\n',\n'Execution time: 2 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 80, in <module>\\n\nall_preds.append(torch.sigmoid(logits).cpu().numpy())\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: Can\\'t call numpy() on Tensor\nthat requires grad. Use tensor.detach().numpy() instead.\\n', 'Execution time: a\nsecond seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 78, in <module>\\n\nall_preds.append(torch.sigmoid(logits).cpu().numpy())\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: Can\\'t call numpy() on Tensor\nthat requires grad. Use tensor.detach().numpy() instead.\\n', 'Execution time: a\nsecond seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 72, in <module>\\n\nt_preds.append(torch.sigmoid(logits).cpu().numpy())\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: Can\\'t call numpy() on Tensor\nthat requires grad. Use tensor.detach().numpy() instead.\\n', 'Execution time: a\nsecond seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 0.7617', '\\n', 'Epoch\n2: validation_loss = 0.7604', '\\n', 'Epoch 3: validation_loss = 0.7591', '\\n',\n'Epoch 4: validation_loss = 0.7579', '\\n', 'Epoch 5: validation_loss = 0.7566',\n'\\n', 'Epoch 6: validation_loss = 0.7553', '\\n', 'Epoch 7: validation_loss =\n0.7541', '\\n', 'Epoch 8: validation_loss = 0.7529', '\\n', 'Epoch 9:\nvalidation_loss = 0.7516', '\\n', 'Epoch 10: validation_loss = 0.7504', '\\n',\n'Epoch 11: validation_loss = 0.7492', '\\n', 'Epoch 12: validation_loss =\n0.7479', '\\n', 'Epoch 13: validation_loss = 0.7467', '\\n', 'Epoch 14:\nvalidation_loss = 0.7455', '\\n', 'Epoch 15: validation_loss = 0.7443', '\\n',\n'Epoch 16: validation_loss = 0.7431', '\\n', 'Epoch 17: validation_loss =\n0.7419', '\\n', 'Epoch 18: validation_loss = 0.7407', '\\n', 'Epoch 19:\nvalidation_loss = 0.7394', '\\n', 'Epoch 20: validation_loss = 0.7383', '\\n',\n'Epoch 1: validation_loss = 1.1689', '\\n', 'Epoch 2: validation_loss = 1.1544',\n'\\n', 'Epoch 3: validation_loss = 1.1399', '\\n', 'Epoch 4: validation_loss =\n1.1255', '\\n', 'Epoch 5: validation_loss = 1.1118', '\\n', 'Epoch 6:\nvalidation_loss = 1.0979', '\\n', 'Epoch 7: validation_loss = 1.0846', '\\n',\n'Epoch 8: validation_loss = 1.0714', '\\n', 'Epoch 9: validation_loss = 1.0583',\n'\\n', 'Epoch 10: validation_loss = 1.0458', '\\n', 'Epoch 11: validation_loss =\n1.0333', '\\n', 'Epoch 12: validation_loss = 1.0211', '\\n', 'Epoch 13:\nvalidation_loss = 1.0092', '\\n', 'Epoch 14: validation_loss = 0.9979', '\\n',\n'Epoch 15: validation_loss = 0.9865', '\\n', 'Epoch 16: validation_loss =\n0.9754', '\\n', 'Epoch 17: validation_loss = 0.9645', '\\n', 'Epoch 18:\nvalidation_loss = 0.9539', '\\n', 'Epoch 19: validation_loss = 0.9437', '\\n',\n'Epoch 20: validation_loss = 0.9332', '\\n', 'Epoch 1: validation_loss = 0.5689',\n'\\n', 'Epoch 2: validation_loss = 0.5604', '\\n', 'Epoch 3: validation_loss =\n0.5522', '\\n', 'Epoch 4: validation_loss = 0.5442', '\\n', 'Epoch 5:\nvalidation_loss = 0.5368', '\\n', 'Epoch 6: validation_loss = 0.5296', '\\n',\n'Epoch 7: validation_loss = 0.5227', '\\n', 'Epoch 8: validation_loss = 0.5160',\n'\\n', 'Epoch 9: validation_loss = 0.5095', '\\n', 'Epoch 10: validation_loss =\n0.5032', '\\n', 'Epoch 11: validation_loss = 0.4971', '\\n', 'Epoch 12:\nvalidation_loss = 0.4911', '\\n', 'Epoch 13: validation_loss = 0.4854', '\\n',\n'Epoch 14: validation_loss = 0.4799', '\\n', 'Epoch 15: validation_loss =\n0.4745', '\\n', 'Epoch 16: validation_loss = 0.4693', '\\n', 'Epoch 17:\nvalidation_loss = 0.4642', '\\n', 'Epoch 18: validation_loss = 0.4593', '\\n',\n'Epoch 19: validation_loss = 0.4546', '\\n', 'Epoch 20: validation_loss =\n0.4500', '\\n', 'Epoch 1: validation_loss = 1.0137', '\\n', 'Epoch 2:\nvalidation_loss = 0.9340', '\\n', 'Epoch 3: validation_loss = 0.8639', '\\n',\n'Epoch 4: validation_loss = 0.8023', '\\n', 'Epoch 5: validation_loss = 0.7484',\n'\\n', 'Epoch 6: validation_loss = 0.7009', '\\n', 'Epoch 7: validation_loss =\n0.6596', '\\n', 'Epoch 8: validation_loss = 0.6228', '\\n', 'Epoch 9:\nvalidation_loss = 0.5905', '\\n', 'Epoch 10: validation_loss = 0.5616', '\\n',\n'Epoch 11: validation_loss = 0.5367', '\\n', 'Epoch 12: validation_loss =\n0.5144', '\\n', 'Epoch 13: validation_loss = 0.4946', '\\n', 'Epoch 14:\nvalidation_loss = 0.4770', '\\n', 'Epoch 15: validation_loss = 0.4611', '\\n',\n'Epoch 16: validation_loss = 0.4467', '\\n', 'Epoch 17: validation_loss =\n0.4335', '\\n', 'Epoch 18: validation_loss = 0.4222', '\\n', 'Epoch 19:\nvalidation_loss = 0.4117', '\\n', 'Epoch 20: validation_loss = 0.4021', '\\n',\n'Epoch 1: validation_loss = 0.4717', '\\n', 'Epoch 2: validation_loss = 0.4243',\n'\\n', 'Epoch 3: validation_loss = 0.3901', '\\n', 'Epoch 4: validation_loss =\n0.3652', '\\n', 'Epoch 5: validation_loss = 0.3473', '\\n', 'Epoch 6:\nvalidation_loss = 0.3333', '\\n', 'Epoch 7: validation_loss = 0.3224', '\\n',\n'Epoch 8: validation_loss = 0.3137', '\\n', 'Epoch 9: validation_loss = 0.3069',\n'\\n', 'Epoch 10: validation_loss = 0.3013', '\\n', 'Epoch 11: validation_loss =\n0.2966', '\\n', 'Epoch 12: validation_loss = 0.2927', '\\n', 'Epoch 13:\nvalidation_loss = 0.2897', '\\n', 'Epoch 14: validation_loss = 0.2868', '\\n',\n'Epoch 15: validation_loss = 0.2858', '\\n', 'Epoch 16: validation_loss =\n0.2833', '\\n', 'Epoch 17: validation_loss = 0.2815', '\\n', 'Epoch 18:\nvalidation_loss = 0.2799', '\\n', 'Epoch 19: validation_loss = 0.2782', '\\n',\n'Epoch 20: validation_loss = 0.2778', '\\n', 'Epoch 1: validation_loss = 0.5050',\n'\\n', 'Epoch 2: validation_loss = 0.3800', '\\n', 'Epoch 3: validation_loss =\n0.3322', '\\n', 'Epoch 4: validation_loss = 0.3087', '\\n', 'Epoch 5:\nvalidation_loss = 0.2967', '\\n', 'Epoch 6: validation_loss = 0.2873', '\\n',\n'Epoch 7: validation_loss = 0.2829', '\\n', 'Epoch 8: validation_loss = 0.2817',\n'\\n', 'Epoch 9: validation_loss = 0.2768', '\\n', 'Epoch 10: validation_loss =\n0.2749', '\\n', 'Epoch 11: validation_loss = 0.2773', '\\n', 'Epoch 12:\nvalidation_loss = 0.2724', '\\n', 'Epoch 13: validation_loss = 0.2733', '\\n',\n'Epoch 14: validation_loss = 0.2752', '\\n', 'Epoch 15: validation_loss =\n0.2717', '\\n', 'Epoch 16: validation_loss = 0.2726', '\\n', 'Epoch 17:\nvalidation_loss = 0.2744', '\\n', 'Epoch 18: validation_loss = 0.2718', '\\n',\n'Epoch 19: validation_loss = 0.2717', '\\n', 'Epoch 20: validation_loss =\n0.2721', '\\n', 'Traceback (most recent call last):\\n  File \"runfile.py\", line\n107, in <module>\\n    train_loader, val_loader, input_dim = prep()\\n\n^^^^^^\\n  File \"runfile.py\", line 77, in <lambda>\\n    \"ieee_fraud_detection\":\nlambda: prepare_tabular(\\n                                    ^^^^^^^^^^^^^^^^\\n\nFile \"runfile.py\", line 41, in prepare_tabular\\n    raw =\nload_dataset(dataset_name)\\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1652, in dataset_module_factory\\n    raise e1\nfrom None\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1578, in dataset_module_factory\\n    raise\nDatasetNotFoundError(f\"Dataset \\'{path}\\' doesn\\'t exist on the Hub or cannot be\naccessed.\") from e\\ndatasets.exceptions.DatasetNotFoundError: Dataset\n\\'ieee_fraud_detection\\' doesn\\'t exist on the Hub or cannot be accessed.\\n',\n'Execution time: 7 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 68, in <module>\\n    (train_X_dia, train_y_dia), (val_X_dia,\nval_y_dia) = prepare_openml(37)  # Pima Diabetes\\n\n^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line 50, in prepare_openml\\n    ds =\nload_dataset(\"openml\", str(dataset_id))\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1652, in dataset_module_factory\\n    raise e1\nfrom None\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1578, in dataset_module_factory\\n    raise\nDatasetNotFoundError(f\"Dataset \\'{path}\\' doesn\\'t exist on the Hub or cannot be\naccessed.\") from e\\ndatasets.exceptions.DatasetNotFoundError: Dataset \\'openml\\'\ndoesn\\'t exist on the Hub or cannot be accessed.\\n', 'Execution time: a second\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Running Adam beta1=0.5', '\\n', '  Epoch 1:\nval_loss=0.6548, val_auc=0.9572', '\\n', '  Epoch 2: val_loss=0.5742,\nval_auc=0.9572', '\\n', '  Epoch 3: val_loss=0.5124, val_auc=0.9572', '\\n', '\nEpoch 4: val_loss=0.4652, val_auc=0.9572', '\\n', '  Epoch 5: val_loss=0.4291,\nval_auc=0.9572', '\\n', '  Epoch 6: val_loss=0.4013, val_auc=0.9572', '\\n', '\nEpoch 7: val_loss=0.3794, val_auc=0.9572', '\\n', '  Epoch 8: val_loss=0.3618,\nval_auc=0.9572', '\\n', '  Epoch 9: val_loss=0.3481, val_auc=0.9572', '\\n', '\nEpoch 10: val_loss=0.3369, val_auc=0.9572', '\\n', '  Epoch 11: val_loss=0.3275,\nval_auc=0.9572', '\\n', '  Epoch 12: val_loss=0.3199, val_auc=0.9572', '\\n', '\nEpoch 13: val_loss=0.3134, val_auc=0.9572', '\\n', '  Epoch 14: val_loss=0.3079,\nval_auc=0.9572', '\\n', '  Epoch 15: val_loss=0.3034, val_auc=0.9572', '\\n', '\nEpoch 16: val_loss=0.2994, val_auc=0.9572', '\\n', '  Epoch 17: val_loss=0.2958,\nval_auc=0.9572', '\\n', '  Epoch 18: val_loss=0.2927, val_auc=0.9572', '\\n', '\nEpoch 19: val_loss=0.2902, val_auc=0.9572', '\\n', '  Epoch 20: val_loss=0.2881,\nval_auc=0.9572', '\\n', 'Running Adam beta1=0.8', '\\n', '  Epoch 1:\nval_loss=0.9284, val_auc=0.0428', '\\n', '  Epoch 2: val_loss=0.7710,\nval_auc=0.0428', '\\n', '  Epoch 3: val_loss=0.6773, val_auc=0.9572', '\\n', '\nEpoch 4: val_loss=0.6124, val_auc=0.9572', '\\n', '  Epoch 5: val_loss=0.5609,\nval_auc=0.9572', '\\n', '  Epoch 6: val_loss=0.5191, val_auc=0.9572', '\\n', '\nEpoch 7: val_loss=0.4847, val_auc=0.9572', '\\n', '  Epoch 8: val_loss=0.4562,\nval_auc=0.9572', '\\n', '  Epoch 9: val_loss=0.4327, val_auc=0.9572', '\\n', '\nEpoch 10: val_loss=0.4133, val_auc=0.9572', '\\n', '  Epoch 11: val_loss=0.3968,\nval_auc=0.9572', '\\n', '  Epoch 12: val_loss=0.3827, val_auc=0.9572', '\\n', '\nEpoch 13: val_loss=0.3711, val_auc=0.9572', '\\n', '  Epoch 14: val_loss=0.3612,\nval_auc=0.9572', '\\n', '  Epoch 15: val_loss=0.3523, val_auc=0.9572', '\\n', '\nEpoch 16: val_loss=0.3446, val_auc=0.9572', '\\n', '  Epoch 17: val_loss=0.3380,\nval_auc=0.9572', '\\n', '  Epoch 18: val_loss=0.3317, val_auc=0.9572', '\\n', '\nEpoch 19: val_loss=0.3262, val_auc=0.9572', '\\n', '  Epoch 20: val_loss=0.3211,\nval_auc=0.9572', '\\n', 'Running Adam beta1=0.9', '\\n', '  Epoch 1:\nval_loss=0.4998, val_auc=0.9572', '\\n', '  Epoch 2: val_loss=0.4453,\nval_auc=0.9572', '\\n', '  Epoch 3: val_loss=0.4061, val_auc=0.9572', '\\n', '\nEpoch 4: val_loss=0.3778, val_auc=0.9572', '\\n', '  Epoch 5: val_loss=0.3566,\nval_auc=0.9572', '\\n', '  Epoch 6: val_loss=0.3409, val_auc=0.9572', '\\n', '\nEpoch 7: val_loss=0.3293, val_auc=0.9572', '\\n', '  Epoch 8: val_loss=0.3187,\nval_auc=0.9572', '\\n', '  Epoch 9: val_loss=0.3106, val_auc=0.9572', '\\n', '\nEpoch 10: val_loss=0.3046, val_auc=0.9572', '\\n', '  Epoch 11: val_loss=0.2996,\nval_auc=0.9572', '\\n', '  Epoch 12: val_loss=0.2956, val_auc=0.9572', '\\n', '\nEpoch 13: val_loss=0.2915, val_auc=0.9572', '\\n', '  Epoch 14: val_loss=0.2885,\nval_auc=0.9572', '\\n', '  Epoch 15: val_loss=0.2861, val_auc=0.9572', '\\n', '\nEpoch 16: val_loss=0.2839, val_auc=0.9572', '\\n', '  Epoch 17: val_loss=0.2822,\nval_auc=0.9572', '\\n', '  Epoch 18: val_loss=0.2812, val_auc=0.9572', '\\n', '\nEpoch 19: val_loss=0.2802, val_auc=0.9572', '\\n', '  Epoch 20: val_loss=0.2782,\nval_auc=0.9572', '\\n', 'Running Adam beta1=0.99', '\\n', '  Epoch 1:\nval_loss=0.9291, val_auc=0.0428', '\\n', '  Epoch 2: val_loss=0.7970,\nval_auc=0.0428', '\\n', '  Epoch 3: val_loss=0.6936, val_auc=0.9572', '\\n', '\nEpoch 4: val_loss=0.6094, val_auc=0.9572', '\\n', '  Epoch 5: val_loss=0.5396,\nval_auc=0.9572', '\\n', '  Epoch 6: val_loss=0.4826, val_auc=0.9572', '\\n', '\nEpoch 7: val_loss=0.4377, val_auc=0.9572', '\\n', '  Epoch 8: val_loss=0.4039,\nval_auc=0.9572', '\\n', '  Epoch 9: val_loss=0.3793, val_auc=0.9572', '\\n', '\nEpoch 10: val_loss=0.3614, val_auc=0.9572', '\\n', '  Epoch 11: val_loss=0.3481,\nval_auc=0.9572', '\\n', '  Epoch 12: val_loss=0.3376, val_auc=0.9572', '\\n', '\nEpoch 13: val_loss=0.3289, val_auc=0.9572', '\\n', '  Epoch 14: val_loss=0.3210,\nval_auc=0.9572', '\\n', '  Epoch 15: val_loss=0.3142, val_auc=0.9572', '\\n', '\nEpoch 16: val_loss=0.3082, val_auc=0.9572', '\\n', '  Epoch 17: val_loss=0.3027,\nval_auc=0.9572', '\\n', '  Epoch 18: val_loss=0.2984, val_auc=0.9572', '\\n', '\nEpoch 19: val_loss=0.2948, val_auc=0.9572', '\\n', '  Epoch 20: val_loss=0.2917,\nval_auc=0.9572', '\\n', 'Execution time: 5 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'beta2=0.9 epoch=1 val_loss=0.6491 val_auc=0.9572',\n'\\n', 'beta2=0.9 epoch=2 val_loss=0.5580 val_auc=0.9572', '\\n', 'beta2=0.9\nepoch=3 val_loss=0.4848 val_auc=0.9572', '\\n', 'beta2=0.9 epoch=4\nval_loss=0.4283 val_auc=0.9572', '\\n', 'beta2=0.9 epoch=5 val_loss=0.3842\nval_auc=0.9572', '\\n', 'beta2=0.9 epoch=6 val_loss=0.3522 val_auc=0.9572', '\\n',\n'beta2=0.9 epoch=7 val_loss=0.3288 val_auc=0.9572', '\\n', 'beta2=0.9 epoch=8\nval_loss=0.3138 val_auc=0.9572', '\\n', 'beta2=0.9 epoch=9 val_loss=0.3024\nval_auc=0.9572', '\\n', 'beta2=0.9 epoch=10 val_loss=0.2941 val_auc=0.9572',\n'\\n', 'beta2=0.9 epoch=11 val_loss=0.2891 val_auc=0.9572', '\\n', 'beta2=0.9\nepoch=12 val_loss=0.2847 val_auc=0.9572', '\\n', 'beta2=0.9 epoch=13\nval_loss=0.2822 val_auc=0.9572', '\\n', 'beta2=0.9 epoch=14 val_loss=0.2790\nval_auc=0.9572', '\\n', 'beta2=0.9 epoch=15 val_loss=0.2776 val_auc=0.9572',\n'\\n', 'beta2=0.9 epoch=16 val_loss=0.2762 val_auc=0.9572', '\\n', 'beta2=0.9\nepoch=17 val_loss=0.2755 val_auc=0.9572', '\\n', 'beta2=0.9 epoch=18\nval_loss=0.2741 val_auc=0.9572', '\\n', 'beta2=0.9 epoch=19 val_loss=0.2729\nval_auc=0.9572', '\\n', 'beta2=0.9 epoch=20 val_loss=0.2720 val_auc=0.9572',\n'\\n', 'beta2=0.95 epoch=1 val_loss=0.9227 val_auc=0.0428', '\\n', 'beta2=0.95\nepoch=2 val_loss=0.7556 val_auc=0.0428', '\\n', 'beta2=0.95 epoch=3\nval_loss=0.6484 val_auc=0.9572', '\\n', 'beta2=0.95 epoch=4 val_loss=0.5656\nval_auc=0.9572', '\\n', 'beta2=0.95 epoch=5 val_loss=0.4966 val_auc=0.9572',\n'\\n', 'beta2=0.95 epoch=6 val_loss=0.4433 val_auc=0.9572', '\\n', 'beta2=0.95\nepoch=7 val_loss=0.4004 val_auc=0.9572', '\\n', 'beta2=0.95 epoch=8\nval_loss=0.3663 val_auc=0.9572', '\\n', 'beta2=0.95 epoch=9 val_loss=0.3412\nval_auc=0.9572', '\\n', 'beta2=0.95 epoch=10 val_loss=0.3219 val_auc=0.9572',\n'\\n', 'beta2=0.95 epoch=11 val_loss=0.3078 val_auc=0.9572', '\\n', 'beta2=0.95\nepoch=12 val_loss=0.2968 val_auc=0.9572', '\\n', 'beta2=0.95 epoch=13\nval_loss=0.2899 val_auc=0.9572', '\\n', 'beta2=0.95 epoch=14 val_loss=0.2859\nval_auc=0.9572', '\\n', 'beta2=0.95 epoch=15 val_loss=0.2828 val_auc=0.9572',\n'\\n', 'beta2=0.95 epoch=16 val_loss=0.2800 val_auc=0.9572', '\\n', 'beta2=0.95\nepoch=17 val_loss=0.2791 val_auc=0.9572', '\\n', 'beta2=0.95 epoch=18\nval_loss=0.2766 val_auc=0.9572', '\\n', 'beta2=0.95 epoch=19 val_loss=0.2754\nval_auc=0.9572', '\\n', 'beta2=0.95 epoch=20 val_loss=0.2745 val_auc=0.9572',\n'\\n', 'beta2=0.99 epoch=1 val_loss=0.4997 val_auc=0.9572', '\\n', 'beta2=0.99\nepoch=2 val_loss=0.4446 val_auc=0.9572', '\\n', 'beta2=0.99 epoch=3\nval_loss=0.4044 val_auc=0.9572', '\\n', 'beta2=0.99 epoch=4 val_loss=0.3750\nval_auc=0.9572', '\\n', 'beta2=0.99 epoch=5 val_loss=0.3529 val_auc=0.9572',\n'\\n', 'beta2=0.99 epoch=6 val_loss=0.3365 val_auc=0.9572', '\\n', 'beta2=0.99\nepoch=7 val_loss=0.3241 val_auc=0.9572', '\\n', 'beta2=0.99 epoch=8\nval_loss=0.3129 val_auc=0.9572', '\\n', 'beta2=0.99 epoch=9 val_loss=0.3043\nval_auc=0.9572', '\\n', 'beta2=0.99 epoch=10 val_loss=0.2980 val_auc=0.9572',\n'\\n', 'beta2=0.99 epoch=11 val_loss=0.2927 val_auc=0.9572', '\\n', 'beta2=0.99\nepoch=12 val_loss=0.2885 val_auc=0.9572', '\\n', 'beta2=0.99 epoch=13\nval_loss=0.2843 val_auc=0.9572', '\\n', 'beta2=0.99 epoch=14 val_loss=0.2816\nval_auc=0.9572', '\\n', 'beta2=0.99 epoch=15 val_loss=0.2795 val_auc=0.9572',\n'\\n', 'beta2=0.99 epoch=16 val_loss=0.2775 val_auc=0.9572', '\\n', 'beta2=0.99\nepoch=17 val_loss=0.2763 val_auc=0.9572', '\\n', 'beta2=0.99 epoch=18\nval_loss=0.2760 val_auc=0.9572', '\\n', 'beta2=0.99 epoch=19 val_loss=0.2754\nval_auc=0.9572', '\\n', 'beta2=0.99 epoch=20 val_loss=0.2735 val_auc=0.9572',\n'\\n', 'beta2=0.999 epoch=1 val_loss=0.9312 val_auc=0.0428', '\\n', 'beta2=0.999\nepoch=2 val_loss=0.8018 val_auc=0.0428', '\\n', 'beta2=0.999 epoch=3\nval_loss=0.6999 val_auc=0.9572', '\\n', 'beta2=0.999 epoch=4 val_loss=0.6207\nval_auc=0.9572', '\\n', 'beta2=0.999 epoch=5 val_loss=0.5594 val_auc=0.9572',\n'\\n', 'beta2=0.999 epoch=6 val_loss=0.5119 val_auc=0.9572', '\\n', 'beta2=0.999\nepoch=7 val_loss=0.4748 val_auc=0.9572', '\\n', 'beta2=0.999 epoch=8\nval_loss=0.4449 val_auc=0.9572', '\\n', 'beta2=0.999 epoch=9 val_loss=0.4212\nval_auc=0.9572', '\\n', 'beta2=0.999 epoch=10 val_loss=0.4012 val_auc=0.9572',\n'\\n', 'beta2=0.999 epoch=11 val_loss=0.3855 val_auc=0.9572', '\\n', 'beta2=0.999\nepoch=12 val_loss=0.3719 val_auc=0.9572', '\\n', 'beta2=0.999 epoch=13\nval_loss=0.3607 val_auc=0.9572', '\\n', 'beta2=0.999 epoch=14 val_loss=0.3506\nval_auc=0.9572', '\\n', 'beta2=0.999 epoch=15 val_loss=0.3426 val_auc=0.9572',\n'\\n', 'beta2=0.999 epoch=16 val_loss=0.3350 val_auc=0.9572', '\\n', 'beta2=0.999\nepoch=17 val_loss=0.3280 val_auc=0.9572', '\\n', 'beta2=0.999 epoch=18\nval_loss=0.3233 val_auc=0.9572', '\\n', 'beta2=0.999 epoch=19 val_loss=0.3179\nval_auc=0.9572', '\\n', 'beta2=0.999 epoch=20 val_loss=0.3135 val_auc=0.9572',\n'\\n', 'beta2=0.9999 epoch=1 val_loss=0.4717 val_auc=0.9572', '\\n', 'beta2=0.9999\nepoch=2 val_loss=0.4244 val_auc=0.9572', '\\n', 'beta2=0.9999 epoch=3\nval_loss=0.3903 val_auc=0.9572', '\\n', 'beta2=0.9999 epoch=4 val_loss=0.3654\nval_auc=0.9572', '\\n', 'beta2=0.9999 epoch=5 val_loss=0.3477 val_auc=0.9572',\n'\\n', 'beta2=0.9999 epoch=6 val_loss=0.3337 val_auc=0.9572', '\\n', 'beta2=0.9999\nepoch=7 val_loss=0.3229 val_auc=0.9572', '\\n', 'beta2=0.9999 epoch=8\nval_loss=0.3143 val_auc=0.9572', '\\n', 'beta2=0.9999 epoch=9 val_loss=0.3075\nval_auc=0.9572', '\\n', 'beta2=0.9999 epoch=10 val_loss=0.3020 val_auc=0.9572',\n'\\n', 'beta2=0.9999 epoch=11 val_loss=0.2973 val_auc=0.9572', '\\n',\n'beta2=0.9999 epoch=12 val_loss=0.2934 val_auc=0.9572', '\\n', 'beta2=0.9999\nepoch=13 val_loss=0.2904 val_auc=0.9572', '\\n', 'beta2=0.9999 epoch=14\nval_loss=0.2875 val_auc=0.9572', '\\n', 'beta2=0.9999 epoch=15 val_loss=0.2865\nval_auc=0.9572', '\\n', 'beta2=0.9999 epoch=16 val_loss=0.2840 val_auc=0.9572',\n'\\n', 'beta2=0.9999 epoch=17 val_loss=0.2822 val_auc=0.9572', '\\n',\n'beta2=0.9999 epoch=18 val_loss=0.2806 val_auc=0.9572', '\\n', 'beta2=0.9999\nepoch=19 val_loss=0.2789 val_auc=0.9572', '\\n', 'beta2=0.9999 epoch=20\nval_loss=0.2784 val_auc=0.9572', '\\n', 'Execution time: 6 seconds seconds (time\nlimit is an hour).']", "['Using device: cuda', '\\n', 'Gamma 0.9 Epoch 1: val_loss = 0.6511, val_auc =\n0.9572', '\\n', 'Gamma 0.9 Epoch 2: val_loss = 0.5755, val_auc = 0.9572', '\\n',\n'Gamma 0.9 Epoch 3: val_loss = 0.5229, val_auc = 0.9572', '\\n', 'Gamma 0.9 Epoch\n4: val_loss = 0.4837, val_auc = 0.9572', '\\n', 'Gamma 0.9 Epoch 5: val_loss =\n0.4544, val_auc = 0.9572', '\\n', 'Gamma 0.9 Epoch 6: val_loss = 0.4324, val_auc\n= 0.9572', '\\n', 'Gamma 0.9 Epoch 7: val_loss = 0.4152, val_auc = 0.9572', '\\n',\n'Gamma 0.9 Epoch 8: val_loss = 0.4016, val_auc = 0.9572', '\\n', 'Gamma 0.9 Epoch\n9: val_loss = 0.3906, val_auc = 0.9572', '\\n', 'Gamma 0.9 Epoch 10: val_loss =\n0.3817, val_auc = 0.9572', '\\n', 'Gamma 0.9 Epoch 11: val_loss = 0.3741, val_auc\n= 0.9572', '\\n', 'Gamma 0.9 Epoch 12: val_loss = 0.3681, val_auc = 0.9572',\n'\\n', 'Gamma 0.9 Epoch 13: val_loss = 0.3630, val_auc = 0.9572', '\\n', 'Gamma\n0.9 Epoch 14: val_loss = 0.3584, val_auc = 0.9572', '\\n', 'Gamma 0.9 Epoch 15:\nval_loss = 0.3545, val_auc = 0.9572', '\\n', 'Gamma 0.9 Epoch 16: val_loss =\n0.3512, val_auc = 0.9572', '\\n', 'Gamma 0.9 Epoch 17: val_loss = 0.3484, val_auc\n= 0.9572', '\\n', 'Gamma 0.9 Epoch 18: val_loss = 0.3459, val_auc = 0.9572',\n'\\n', 'Gamma 0.9 Epoch 19: val_loss = 0.3436, val_auc = 0.9572', '\\n', 'Gamma\n0.9 Epoch 20: val_loss = 0.3416, val_auc = 0.9572', '\\n', 'Gamma 0.95 Epoch 1:\nval_loss = 0.9251, val_auc = 0.0428', '\\n', 'Gamma 0.95 Epoch 2: val_loss =\n0.7711, val_auc = 0.0428', '\\n', 'Gamma 0.95 Epoch 3: val_loss = 0.6840, val_auc\n= 0.9572', '\\n', 'Gamma 0.95 Epoch 4: val_loss = 0.6245, val_auc = 0.9572',\n'\\n', 'Gamma 0.95 Epoch 5: val_loss = 0.5783, val_auc = 0.9572', '\\n', 'Gamma\n0.95 Epoch 6: val_loss = 0.5414, val_auc = 0.9572', '\\n', 'Gamma 0.95 Epoch 7:\nval_loss = 0.5111, val_auc = 0.9572', '\\n', 'Gamma 0.95 Epoch 8: val_loss =\n0.4860, val_auc = 0.9572', '\\n', 'Gamma 0.95 Epoch 9: val_loss = 0.4651, val_auc\n= 0.9572', '\\n', 'Gamma 0.95 Epoch 10: val_loss = 0.4478, val_auc = 0.9572',\n'\\n', 'Gamma 0.95 Epoch 11: val_loss = 0.4329, val_auc = 0.9572', '\\n', 'Gamma\n0.95 Epoch 12: val_loss = 0.4204, val_auc = 0.9572', '\\n', 'Gamma 0.95 Epoch 13:\nval_loss = 0.4097, val_auc = 0.9572', '\\n', 'Gamma 0.95 Epoch 14: val_loss =\n0.4005, val_auc = 0.9572', '\\n', 'Gamma 0.95 Epoch 15: val_loss = 0.3924,\nval_auc = 0.9572', '\\n', 'Gamma 0.95 Epoch 16: val_loss = 0.3853, val_auc =\n0.9572', '\\n', 'Gamma 0.95 Epoch 17: val_loss = 0.3791, val_auc = 0.9572', '\\n',\n'Gamma 0.95 Epoch 18: val_loss = 0.3732, val_auc = 0.9572', '\\n', 'Gamma 0.95\nEpoch 19: val_loss = 0.3682, val_auc = 0.9572', '\\n', 'Gamma 0.95 Epoch 20:\nval_loss = 0.3635, val_auc = 0.9572', '\\n', 'Gamma 0.99 Epoch 1: val_loss =\n0.4998, val_auc = 0.9572', '\\n', 'Gamma 0.99 Epoch 2: val_loss = 0.4457, val_auc\n= 0.9572', '\\n', 'Gamma 0.99 Epoch 3: val_loss = 0.4071, val_auc = 0.9572',\n'\\n', 'Gamma 0.99 Epoch 4: val_loss = 0.3792, val_auc = 0.9572', '\\n', 'Gamma\n0.99 Epoch 5: val_loss = 0.3584, val_auc = 0.9572', '\\n', 'Gamma 0.99 Epoch 6:\nval_loss = 0.3430, val_auc = 0.9572', '\\n', 'Gamma 0.99 Epoch 7: val_loss =\n0.3315, val_auc = 0.9572', '\\n', 'Gamma 0.99 Epoch 8: val_loss = 0.3211, val_auc\n= 0.9572', '\\n', 'Gamma 0.99 Epoch 9: val_loss = 0.3131, val_auc = 0.9572',\n'\\n', 'Gamma 0.99 Epoch 10: val_loss = 0.3071, val_auc = 0.9572', '\\n', 'Gamma\n0.99 Epoch 11: val_loss = 0.3021, val_auc = 0.9572', '\\n', 'Gamma 0.99 Epoch 12:\nval_loss = 0.2980, val_auc = 0.9572', '\\n', 'Gamma 0.99 Epoch 13: val_loss =\n0.2939, val_auc = 0.9572', '\\n', 'Gamma 0.99 Epoch 14: val_loss = 0.2909,\nval_auc = 0.9572', '\\n', 'Gamma 0.99 Epoch 15: val_loss = 0.2884, val_auc =\n0.9572', '\\n', 'Gamma 0.99 Epoch 16: val_loss = 0.2861, val_auc = 0.9572', '\\n',\n'Gamma 0.99 Epoch 17: val_loss = 0.2843, val_auc = 0.9572', '\\n', 'Gamma 0.99\nEpoch 18: val_loss = 0.2832, val_auc = 0.9572', '\\n', 'Gamma 0.99 Epoch 19:\nval_loss = 0.2820, val_auc = 0.9572', '\\n', 'Gamma 0.99 Epoch 20: val_loss =\n0.2801, val_auc = 0.9572', '\\n', 'Execution time: 4 seconds seconds (time limit\nis an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 150, in <module>\\n\nall_preds.append(torch.sigmoid(logits).cpu().numpy())\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: Can\\'t call numpy() on Tensor\nthat requires grad. Use tensor.detach().numpy() instead.\\n', 'Execution time: 2\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 73, in <module>\\n\ntp.append(torch.sigmoid(logits).cpu().numpy())\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: Can\\'t call numpy() on Tensor\nthat requires grad. Use tensor.detach().numpy() instead.\\n', 'Execution time: a\nsecond seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\nRunning with num_epochs = 50', '\\n', 'Traceback\n(most recent call last):\\n  File \"runfile.py\", line 77, in <module>\\n\nt_preds.append(torch.sigmoid(logits).cpu().numpy())\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: Can\\'t call numpy() on Tensor\nthat requires grad. Use tensor.detach().numpy() instead.\\n', 'Execution time: a\nsecond seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 0.7542', '\\n', 'Epoch\n2: validation_loss = 0.7389', '\\n', 'Epoch 3: validation_loss = 0.7244', '\\n',\n'Epoch 4: validation_loss = 0.7103', '\\n', 'Epoch 5: validation_loss = 0.6965',\n'\\n', 'Epoch 6: validation_loss = 0.6837', '\\n', 'Epoch 7: validation_loss =\n0.6714', '\\n', 'Epoch 8: validation_loss = 0.6596', '\\n', 'Epoch 9:\nvalidation_loss = 0.6483', '\\n', 'Epoch 10: validation_loss = 0.6376', '\\n',\n'Epoch 11: validation_loss = 0.6269', '\\n', 'Epoch 12: validation_loss =\n0.6171', '\\n', 'Epoch 13: validation_loss = 0.6075', '\\n', 'Epoch 14:\nvalidation_loss = 0.5983', '\\n', 'Epoch 15: validation_loss = 0.5897', '\\n',\n'Epoch 16: validation_loss = 0.5813', '\\n', 'Epoch 17: validation_loss =\n0.5732', '\\n', 'Epoch 18: validation_loss = 0.5655', '\\n', 'Epoch 19:\nvalidation_loss = 0.5582', '\\n', 'Epoch 20: validation_loss = 0.5512', '\\n',\n'Epoch 1: validation_loss = 0.9828', '\\n', 'Epoch 2: validation_loss = 0.9754',\n'\\n', 'Epoch 3: validation_loss = 0.9681', '\\n', 'Epoch 4: validation_loss =\n0.9608', '\\n', 'Epoch 5: validation_loss = 0.9537', '\\n', 'Epoch 6:\nvalidation_loss = 0.9465', '\\n', 'Epoch 7: validation_loss = 0.9394', '\\n',\n'Epoch 8: validation_loss = 0.9325', '\\n', 'Epoch 9: validation_loss = 0.9254',\n'\\n', 'Epoch 10: validation_loss = 0.9185', '\\n', 'Epoch 11: validation_loss =\n0.9117', '\\n', 'Epoch 12: validation_loss = 0.9050', '\\n', 'Epoch 13:\nvalidation_loss = 0.8983', '\\n', 'Epoch 14: validation_loss = 0.8917', '\\n',\n'Epoch 15: validation_loss = 0.8852', '\\n', 'Epoch 16: validation_loss =\n0.8787', '\\n', 'Epoch 17: validation_loss = 0.8722', '\\n', 'Epoch 18:\nvalidation_loss = 0.8659', '\\n', 'Epoch 19: validation_loss = 0.8596', '\\n',\n'Epoch 20: validation_loss = 0.8534', '\\n', 'Epoch 1: validation_loss = 0.5080',\n'\\n', 'Epoch 2: validation_loss = 0.4577', '\\n', 'Epoch 3: validation_loss =\n0.4206', '\\n', 'Epoch 4: validation_loss = 0.3947', '\\n', 'Epoch 5:\nvalidation_loss = 0.3742', '\\n', 'Epoch 6: validation_loss = 0.3587', '\\n',\n'Epoch 7: validation_loss = 0.3470', '\\n', 'Epoch 8: validation_loss = 0.3362',\n'\\n', 'Epoch 9: validation_loss = 0.3274', '\\n', 'Epoch 10: validation_loss =\n0.3210', '\\n', 'Epoch 11: validation_loss = 0.3152', '\\n', 'Epoch 12:\nvalidation_loss = 0.3107', '\\n', 'Epoch 13: validation_loss = 0.3056', '\\n',\n'Epoch 14: validation_loss = 0.3017', '\\n', 'Epoch 15: validation_loss =\n0.2988', '\\n', 'Epoch 16: validation_loss = 0.2960', '\\n', 'Epoch 17:\nvalidation_loss = 0.2936', '\\n', 'Epoch 18: validation_loss = 0.2918', '\\n',\n'Epoch 19: validation_loss = 0.2902', '\\n', 'Epoch 20: validation_loss =\n0.2876', '\\n', 'Epoch 1: validation_loss = 0.9691', '\\n', 'Epoch 2:\nvalidation_loss = 0.8844', '\\n', 'Epoch 3: validation_loss = 0.8125', '\\n',\n'Epoch 4: validation_loss = 0.7523', '\\n', 'Epoch 5: validation_loss = 0.7016',\n'\\n', 'Epoch 6: validation_loss = 0.6591', '\\n', 'Epoch 7: validation_loss =\n0.6229', '\\n', 'Epoch 8: validation_loss = 0.5916', '\\n', 'Epoch 9:\nvalidation_loss = 0.5648', '\\n', 'Epoch 10: validation_loss = 0.5413', '\\n',\n'Epoch 11: validation_loss = 0.5216', '\\n', 'Epoch 12: validation_loss =\n0.5038', '\\n', 'Epoch 13: validation_loss = 0.4884', '\\n', 'Epoch 14:\nvalidation_loss = 0.4745', '\\n', 'Epoch 15: validation_loss = 0.4625', '\\n',\n'Epoch 16: validation_loss = 0.4517', '\\n', 'Epoch 17: validation_loss =\n0.4415', '\\n', 'Epoch 18: validation_loss = 0.4332', '\\n', 'Epoch 19:\nvalidation_loss = 0.4249', '\\n', 'Epoch 20: validation_loss = 0.4176', '\\n',\n'Epoch 1: validation_loss = 0.7077', '\\n', 'Epoch 2: validation_loss = 0.7050',\n'\\n', 'Epoch 3: validation_loss = 0.7026', '\\n', 'Epoch 4: validation_loss =\n0.7007', '\\n', 'Epoch 5: validation_loss = 0.6988', '\\n', 'Epoch 6:\nvalidation_loss = 0.6969', '\\n', 'Epoch 7: validation_loss = 0.6955', '\\n',\n'Epoch 8: validation_loss = 0.6942', '\\n', 'Epoch 9: validation_loss = 0.6932',\n'\\n', 'Epoch 10: validation_loss = 0.6922', '\\n', 'Epoch 11: validation_loss =\n0.6915', '\\n', 'Epoch 12: validation_loss = 0.6907', '\\n', 'Epoch 13:\nvalidation_loss = 0.6903', '\\n', 'Epoch 14: validation_loss = 0.6899', '\\n',\n'Epoch 15: validation_loss = 0.6895', '\\n', 'Epoch 16: validation_loss =\n0.6893', '\\n', 'Epoch 17: validation_loss = 0.6892', '\\n', 'Epoch 18:\nvalidation_loss = 0.6891', '\\n', 'Epoch 19: validation_loss = 0.6892', '\\n',\n'Epoch 20: validation_loss = 0.6892', '\\n', 'Epoch 1: validation_loss = 0.9919',\n'\\n', 'Epoch 2: validation_loss = 0.9812', '\\n', 'Epoch 3: validation_loss =\n0.9701', '\\n', 'Epoch 4: validation_loss = 0.9601', '\\n', 'Epoch 5:\nvalidation_loss = 0.9503', '\\n', 'Epoch 6: validation_loss = 0.9406', '\\n',\n'Epoch 7: validation_loss = 0.9316', '\\n', 'Epoch 8: validation_loss = 0.9218',\n'\\n', 'Epoch 9: validation_loss = 0.9122', '\\n', 'Epoch 10: validation_loss =\n0.9037', '\\n', 'Epoch 11: validation_loss = 0.8951', '\\n', 'Epoch 12:\nvalidation_loss = 0.8868', '\\n', 'Epoch 13: validation_loss = 0.8774', '\\n',\n'Epoch 14: validation_loss = 0.8705', '\\n', 'Epoch 15: validation_loss =\n0.8628', '\\n', 'Epoch 16: validation_loss = 0.8557', '\\n', 'Epoch 17:\nvalidation_loss = 0.8486', '\\n', 'Epoch 18: validation_loss = 0.8417', '\\n',\n'Epoch 19: validation_loss = 0.8352', '\\n', 'Epoch 20: validation_loss =\n0.8291', '\\n', 'Epoch 1: validation_loss = 0.8716', '\\n', 'Epoch 2:\nvalidation_loss = 0.7851', '\\n', 'Epoch 3: validation_loss = 0.7406', '\\n',\n'Epoch 4: validation_loss = 0.7185', '\\n', 'Epoch 5: validation_loss = 0.7102',\n'\\n', 'Epoch 6: validation_loss = 0.7078', '\\n', 'Epoch 7: validation_loss =\n0.7007', '\\n', 'Epoch 8: validation_loss = 0.6995', '\\n', 'Epoch 9:\nvalidation_loss = 0.7034', '\\n', 'Epoch 10: validation_loss = 0.7010', '\\n',\n'Epoch 11: validation_loss = 0.7020', '\\n', 'Epoch 12: validation_loss =\n0.7022', '\\n', 'Epoch 13: validation_loss = 0.7027', '\\n', 'Epoch 14:\nvalidation_loss = 0.7023', '\\n', 'Epoch 15: validation_loss = 0.7019', '\\n',\n'Epoch 16: validation_loss = 0.7018', '\\n', 'Epoch 17: validation_loss =\n0.7037', '\\n', 'Epoch 18: validation_loss = 0.7016', '\\n', 'Epoch 19:\nvalidation_loss = 0.7006', '\\n', 'Epoch 20: validation_loss = 0.7020', '\\n',\n'Epoch 1: validation_loss = 0.7460', '\\n', 'Epoch 2: validation_loss = 0.7246',\n'\\n', 'Epoch 3: validation_loss = 0.7164', '\\n', 'Epoch 4: validation_loss =\n0.7069', '\\n', 'Epoch 5: validation_loss = 0.7048', '\\n', 'Epoch 6:\nvalidation_loss = 0.6996', '\\n', 'Epoch 7: validation_loss = 0.6991', '\\n',\n'Epoch 8: validation_loss = 0.6992', '\\n', 'Epoch 9: validation_loss = 0.6973',\n'\\n', 'Epoch 10: validation_loss = 0.6990', '\\n', 'Epoch 11: validation_loss =\n0.6974', '\\n', 'Epoch 12: validation_loss = 0.6983', '\\n', 'Epoch 13:\nvalidation_loss = 0.6981', '\\n', 'Epoch 14: validation_loss = 0.6993', '\\n',\n'Epoch 15: validation_loss = 0.6992', '\\n', 'Epoch 16: validation_loss =\n0.6979', '\\n', 'Epoch 17: validation_loss = 0.6977', '\\n', 'Epoch 18:\nvalidation_loss = 0.6979', '\\n', 'Epoch 19: validation_loss = 0.6995', '\\n',\n'Epoch 20: validation_loss = 0.6980', '\\n', 'Epoch 1: validation_loss = 0.7862',\n'\\n', 'Epoch 2: validation_loss = 0.7786', '\\n', 'Epoch 3: validation_loss =\n0.7710', '\\n', 'Epoch 4: validation_loss = 0.7643', '\\n', 'Epoch 5:\nvalidation_loss = 0.7579', '\\n', 'Epoch 6: validation_loss = 0.7520', '\\n',\n'Epoch 7: validation_loss = 0.7464', '\\n', 'Epoch 8: validation_loss = 0.7409',\n'\\n', 'Epoch 9: validation_loss = 0.7361', '\\n', 'Epoch 10: validation_loss =\n0.7318', '\\n', 'Epoch 11: validation_loss = 0.7276', '\\n', 'Epoch 12:\nvalidation_loss = 0.7238', '\\n', 'Epoch 13: validation_loss = 0.7205', '\\n',\n'Epoch 14: validation_loss = 0.7176', '\\n', 'Epoch 15: validation_loss =\n0.7146', '\\n', 'Epoch 16: validation_loss = 0.7120', '\\n', 'Epoch 17:\nvalidation_loss = 0.7098', '\\n', 'Epoch 18: validation_loss = 0.7077', '\\n',\n'Epoch 19: validation_loss = 0.7061', '\\n', 'Epoch 20: validation_loss =\n0.7043', '\\n', 'Epoch 1: validation_loss = 0.8289', '\\n', 'Epoch 2:\nvalidation_loss = 0.8250', '\\n', 'Epoch 3: validation_loss = 0.8212', '\\n',\n'Epoch 4: validation_loss = 0.8174', '\\n', 'Epoch 5: validation_loss = 0.8137',\n'\\n', 'Epoch 6: validation_loss = 0.8102', '\\n', 'Epoch 7: validation_loss =\n0.8065', '\\n', 'Epoch 8: validation_loss = 0.8029', '\\n', 'Epoch 9:\nvalidation_loss = 0.7995', '\\n', 'Epoch 10: validation_loss = 0.7960', '\\n',\n'Epoch 11: validation_loss = 0.7928', '\\n', 'Epoch 12: validation_loss =\n0.7896', '\\n', 'Epoch 13: validation_loss = 0.7863', '\\n', 'Epoch 14:\nvalidation_loss = 0.7831', '\\n', 'Epoch 15: validation_loss = 0.7802', '\\n',\n'Epoch 16: validation_loss = 0.7773', '\\n', 'Epoch 17: validation_loss =\n0.7743', '\\n', 'Epoch 18: validation_loss = 0.7713', '\\n', 'Epoch 19:\nvalidation_loss = 0.7685', '\\n', 'Epoch 20: validation_loss = 0.7660', '\\n',\n'Epoch 1: validation_loss = 0.6983', '\\n', 'Epoch 2: validation_loss = 0.7002',\n'\\n', 'Epoch 3: validation_loss = 0.6976', '\\n', 'Epoch 4: validation_loss =\n0.6999', '\\n', 'Epoch 5: validation_loss = 0.6992', '\\n', 'Epoch 6:\nvalidation_loss = 0.6986', '\\n', 'Epoch 7: validation_loss = 0.6983', '\\n',\n'Epoch 8: validation_loss = 0.7008', '\\n', 'Epoch 9: validation_loss = 0.7016',\n'\\n', 'Epoch 10: validation_loss = 0.6992', '\\n', 'Epoch 11: validation_loss =\n0.7008', '\\n', 'Epoch 12: validation_loss = 0.6972', '\\n', 'Epoch 13:\nvalidation_loss = 0.6987', '\\n', 'Epoch 14: validation_loss = 0.6980', '\\n',\n'Epoch 15: validation_loss = 0.6987', '\\n', 'Epoch 16: validation_loss =\n0.6984', '\\n', 'Epoch 17: validation_loss = 0.6984', '\\n', 'Epoch 18:\nvalidation_loss = 0.7012', '\\n', 'Epoch 19: validation_loss = 0.7000', '\\n',\n'Epoch 20: validation_loss = 0.6992', '\\n', 'Epoch 1: validation_loss = 0.7732',\n'\\n', 'Epoch 2: validation_loss = 0.7464', '\\n', 'Epoch 3: validation_loss =\n0.7239', '\\n', 'Epoch 4: validation_loss = 0.7088', '\\n', 'Epoch 5:\nvalidation_loss = 0.6992', '\\n', 'Epoch 6: validation_loss = 0.6939', '\\n',\n'Epoch 7: validation_loss = 0.6924', '\\n', 'Epoch 8: validation_loss = 0.6927',\n'\\n', 'Epoch 9: validation_loss = 0.6939', '\\n', 'Epoch 10: validation_loss =\n0.6955', '\\n', 'Epoch 11: validation_loss = 0.6972', '\\n', 'Epoch 12:\nvalidation_loss = 0.6986', '\\n', 'Epoch 13: validation_loss = 0.6998', '\\n',\n'Epoch 14: validation_loss = 0.7003', '\\n', 'Epoch 15: validation_loss =\n0.7012', '\\n', 'Epoch 16: validation_loss = 0.7022', '\\n', 'Epoch 17:\nvalidation_loss = 0.7028', '\\n', 'Epoch 18: validation_loss = 0.7021', '\\n',\n'Epoch 19: validation_loss = 0.7023', '\\n', 'Epoch 20: validation_loss =\n0.7025', '\\n', 'Execution time: 38 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 0.7083', '\\n', 'Epoch\n2: validation_loss = 0.6926', '\\n', 'Epoch 3: validation_loss = 0.6772', '\\n',\n'Epoch 4: validation_loss = 0.6627', '\\n', 'Epoch 5: validation_loss = 0.6488',\n'\\n', 'Epoch 6: validation_loss = 0.6354', '\\n', 'Epoch 7: validation_loss =\n0.6224', '\\n', 'Epoch 8: validation_loss = 0.6104', '\\n', 'Epoch 9:\nvalidation_loss = 0.5987', '\\n', 'Epoch 10: validation_loss = 0.5873', '\\n',\n'Epoch 11: validation_loss = 0.5764', '\\n', 'Epoch 12: validation_loss =\n0.5658', '\\n', 'Epoch 13: validation_loss = 0.5561', '\\n', 'Epoch 14:\nvalidation_loss = 0.5464', '\\n', 'Epoch 15: validation_loss = 0.5373', '\\n',\n'Epoch 16: validation_loss = 0.5287', '\\n', 'Epoch 17: validation_loss =\n0.5204', '\\n', 'Epoch 18: validation_loss = 0.5123', '\\n', 'Epoch 19:\nvalidation_loss = 0.5045', '\\n', 'Epoch 20: validation_loss = 0.4972', '\\n',\n'Epoch 1: validation_loss = 0.9442', '\\n', 'Epoch 2: validation_loss = 0.9341',\n'\\n', 'Epoch 3: validation_loss = 0.9243', '\\n', 'Epoch 4: validation_loss =\n0.9147', '\\n', 'Epoch 5: validation_loss = 0.9050', '\\n', 'Epoch 6:\nvalidation_loss = 0.8955', '\\n', 'Epoch 7: validation_loss = 0.8861', '\\n',\n'Epoch 8: validation_loss = 0.8770', '\\n', 'Epoch 9: validation_loss = 0.8681',\n'\\n', 'Epoch 10: validation_loss = 0.8592', '\\n', 'Epoch 11: validation_loss =\n0.8504', '\\n', 'Epoch 12: validation_loss = 0.8418', '\\n', 'Epoch 13:\nvalidation_loss = 0.8332', '\\n', 'Epoch 14: validation_loss = 0.8248', '\\n',\n'Epoch 15: validation_loss = 0.8166', '\\n', 'Epoch 16: validation_loss =\n0.8084', '\\n', 'Epoch 17: validation_loss = 0.8004', '\\n', 'Epoch 18:\nvalidation_loss = 0.7926', '\\n', 'Epoch 19: validation_loss = 0.7848', '\\n',\n'Epoch 20: validation_loss = 0.7772', '\\n', 'Epoch 1: validation_loss = 0.4835',\n'\\n', 'Epoch 2: validation_loss = 0.4051', '\\n', 'Epoch 3: validation_loss =\n0.3589', '\\n', 'Epoch 4: validation_loss = 0.3290', '\\n', 'Epoch 5:\nvalidation_loss = 0.3078', '\\n', 'Epoch 6: validation_loss = 0.2921', '\\n',\n'Epoch 7: validation_loss = 0.2800', '\\n', 'Epoch 8: validation_loss = 0.2704',\n'\\n', 'Epoch 9: validation_loss = 0.2623', '\\n', 'Epoch 10: validation_loss =\n0.2556', '\\n', 'Epoch 11: validation_loss = 0.2503', '\\n', 'Epoch 12:\nvalidation_loss = 0.2457', '\\n', 'Epoch 13: validation_loss = 0.2417', '\\n',\n'Epoch 14: validation_loss = 0.2382', '\\n', 'Epoch 15: validation_loss =\n0.2353', '\\n', 'Epoch 16: validation_loss = 0.2329', '\\n', 'Epoch 17:\nvalidation_loss = 0.2305', '\\n', 'Epoch 18: validation_loss = 0.2285', '\\n',\n'Epoch 19: validation_loss = 0.2270', '\\n', 'Epoch 20: validation_loss =\n0.2255', '\\n', 'Epoch 1: validation_loss = 0.4736', '\\n', 'Epoch 2:\nvalidation_loss = 0.4271', '\\n', 'Epoch 3: validation_loss = 0.3912', '\\n',\n'Epoch 4: validation_loss = 0.3640', '\\n', 'Epoch 5: validation_loss = 0.3437',\n'\\n', 'Epoch 6: validation_loss = 0.3272', '\\n', 'Epoch 7: validation_loss =\n0.3140', '\\n', 'Epoch 8: validation_loss = 0.3032', '\\n', 'Epoch 9:\nvalidation_loss = 0.2938', '\\n', 'Epoch 10: validation_loss = 0.2858', '\\n',\n'Epoch 11: validation_loss = 0.2790', '\\n', 'Epoch 12: validation_loss =\n0.2730', '\\n', 'Epoch 13: validation_loss = 0.2677', '\\n', 'Epoch 14:\nvalidation_loss = 0.2631', '\\n', 'Epoch 15: validation_loss = 0.2590', '\\n',\n'Epoch 16: validation_loss = 0.2551', '\\n', 'Epoch 17: validation_loss =\n0.2518', '\\n', 'Epoch 18: validation_loss = 0.2488', '\\n', 'Epoch 19:\nvalidation_loss = 0.2460', '\\n', 'Epoch 20: validation_loss = 0.2435', '\\n',\n'Epoch 1: validation_loss = 0.7088', '\\n', 'Epoch 2: validation_loss = 0.7082',\n'\\n', 'Epoch 3: validation_loss = 0.7076', '\\n', 'Epoch 4: validation_loss =\n0.7075', '\\n', 'Epoch 5: validation_loss = 0.7067', '\\n', 'Epoch 6:\nvalidation_loss = 0.7061', '\\n', 'Epoch 7: validation_loss = 0.7058', '\\n',\n'Epoch 8: validation_loss = 0.7055', '\\n', 'Epoch 9: validation_loss = 0.7051',\n'\\n', 'Epoch 10: validation_loss = 0.7057', '\\n', 'Epoch 11: validation_loss =\n0.7045', '\\n', 'Epoch 12: validation_loss = 0.7043', '\\n', 'Epoch 13:\nvalidation_loss = 0.7041', '\\n', 'Epoch 14: validation_loss = 0.7045', '\\n',\n'Epoch 15: validation_loss = 0.7040', '\\n', 'Epoch 16: validation_loss =\n0.7035', '\\n', 'Epoch 17: validation_loss = 0.7034', '\\n', 'Epoch 18:\nvalidation_loss = 0.7031', '\\n', 'Epoch 19: validation_loss = 0.7035', '\\n',\n'Epoch 20: validation_loss = 0.7026', '\\n', 'Epoch 1: validation_loss = 0.8190',\n'\\n', 'Epoch 2: validation_loss = 0.8163', '\\n', 'Epoch 3: validation_loss =\n0.8145', '\\n', 'Epoch 4: validation_loss = 0.8117', '\\n', 'Epoch 5:\nvalidation_loss = 0.8097', '\\n', 'Epoch 6: validation_loss = 0.8068', '\\n',\n'Epoch 7: validation_loss = 0.8040', '\\n', 'Epoch 8: validation_loss = 0.8015',\n'\\n', 'Epoch 9: validation_loss = 0.7995', '\\n', 'Epoch 10: validation_loss =\n0.7970', '\\n', 'Epoch 11: validation_loss = 0.7947', '\\n', 'Epoch 12:\nvalidation_loss = 0.7918', '\\n', 'Epoch 13: validation_loss = 0.7896', '\\n',\n'Epoch 14: validation_loss = 0.7870', '\\n', 'Epoch 15: validation_loss =\n0.7844', '\\n', 'Epoch 16: validation_loss = 0.7822', '\\n', 'Epoch 17:\nvalidation_loss = 0.7794', '\\n', 'Epoch 18: validation_loss = 0.7775', '\\n',\n'Epoch 19: validation_loss = 0.7751', '\\n', 'Epoch 20: validation_loss =\n0.7726', '\\n', 'Epoch 1: validation_loss = 0.7050', '\\n', 'Epoch 2:\nvalidation_loss = 0.7027', '\\n', 'Epoch 3: validation_loss = 0.7014', '\\n',\n'Epoch 4: validation_loss = 0.7053', '\\n', 'Epoch 5: validation_loss = 0.7002',\n'\\n', 'Epoch 6: validation_loss = 0.6999', '\\n', 'Epoch 7: validation_loss =\n0.7010', '\\n', 'Epoch 8: validation_loss = 0.7048', '\\n', 'Epoch 9:\nvalidation_loss = 0.7038', '\\n', 'Epoch 10: validation_loss = 0.6992', '\\n',\n'Epoch 11: validation_loss = 0.7009', '\\n', 'Epoch 12: validation_loss =\n0.6987', '\\n', 'Epoch 13: validation_loss = 0.7036', '\\n', 'Epoch 14:\nvalidation_loss = 0.7028', '\\n', 'Epoch 15: validation_loss = 0.7019', '\\n',\n'Epoch 16: validation_loss = 0.6990', '\\n', 'Epoch 17: validation_loss =\n0.7032', '\\n', 'Epoch 18: validation_loss = 0.7047', '\\n', 'Epoch 19:\nvalidation_loss = 0.7030', '\\n', 'Epoch 20: validation_loss = 0.7021', '\\n',\n'Epoch 1: validation_loss = 0.7196', '\\n', 'Epoch 2: validation_loss = 0.7176',\n'\\n', 'Epoch 3: validation_loss = 0.7153', '\\n', 'Epoch 4: validation_loss =\n0.7119', '\\n', 'Epoch 5: validation_loss = 0.7125', '\\n', 'Epoch 6:\nvalidation_loss = 0.7092', '\\n', 'Epoch 7: validation_loss = 0.7075', '\\n',\n'Epoch 8: validation_loss = 0.7041', '\\n', 'Epoch 9: validation_loss = 0.7031',\n'\\n', 'Epoch 10: validation_loss = 0.7018', '\\n', 'Epoch 11: validation_loss =\n0.7005', '\\n', 'Epoch 12: validation_loss = 0.6996', '\\n', 'Epoch 13:\nvalidation_loss = 0.6990', '\\n', 'Epoch 14: validation_loss = 0.6988', '\\n',\n'Epoch 15: validation_loss = 0.6998', '\\n', 'Epoch 16: validation_loss =\n0.6985', '\\n', 'Epoch 17: validation_loss = 0.6991', '\\n', 'Epoch 18:\nvalidation_loss = 0.6990', '\\n', 'Epoch 19: validation_loss = 0.6982', '\\n',\n'Epoch 20: validation_loss = 0.6976', '\\n', 'Epoch 1: validation_loss = 0.8400',\n'\\n', 'Epoch 2: validation_loss = 0.8301', '\\n', 'Epoch 3: validation_loss =\n0.8208', '\\n', 'Epoch 4: validation_loss = 0.8117', '\\n', 'Epoch 5:\nvalidation_loss = 0.8031', '\\n', 'Epoch 6: validation_loss = 0.7951', '\\n',\n'Epoch 7: validation_loss = 0.7868', '\\n', 'Epoch 8: validation_loss = 0.7793',\n'\\n', 'Epoch 9: validation_loss = 0.7724', '\\n', 'Epoch 10: validation_loss =\n0.7656', '\\n', 'Epoch 11: validation_loss = 0.7592', '\\n', 'Epoch 12:\nvalidation_loss = 0.7530', '\\n', 'Epoch 13: validation_loss = 0.7476', '\\n',\n'Epoch 14: validation_loss = 0.7420', '\\n', 'Epoch 15: validation_loss =\n0.7370', '\\n', 'Epoch 16: validation_loss = 0.7325', '\\n', 'Epoch 17:\nvalidation_loss = 0.7284', '\\n', 'Epoch 18: validation_loss = 0.7248', '\\n',\n'Epoch 19: validation_loss = 0.7212', '\\n', 'Epoch 20: validation_loss =\n0.7178', '\\n', 'Epoch 1: validation_loss = 0.6939', '\\n', 'Epoch 2:\nvalidation_loss = 0.6937', '\\n', 'Epoch 3: validation_loss = 0.6935', '\\n',\n'Epoch 4: validation_loss = 0.6933', '\\n', 'Epoch 5: validation_loss = 0.6932',\n'\\n', 'Epoch 6: validation_loss = 0.6931', '\\n', 'Epoch 7: validation_loss =\n0.6931', '\\n', 'Epoch 8: validation_loss = 0.6932', '\\n', 'Epoch 9:\nvalidation_loss = 0.6932', '\\n', 'Epoch 10: validation_loss = 0.6933', '\\n',\n'Epoch 11: validation_loss = 0.6935', '\\n', 'Epoch 12: validation_loss =\n0.6936', '\\n', 'Epoch 13: validation_loss = 0.6938', '\\n', 'Epoch 14:\nvalidation_loss = 0.6940', '\\n', 'Epoch 15: validation_loss = 0.6942', '\\n',\n'Epoch 16: validation_loss = 0.6944', '\\n', 'Epoch 17: validation_loss =\n0.6947', '\\n', 'Epoch 18: validation_loss = 0.6949', '\\n', 'Epoch 19:\nvalidation_loss = 0.6952', '\\n', 'Epoch 20: validation_loss = 0.6954', '\\n',\n'Epoch 1: validation_loss = 0.7430', '\\n', 'Epoch 2: validation_loss = 0.7092',\n'\\n', 'Epoch 3: validation_loss = 0.6940', '\\n', 'Epoch 4: validation_loss =\n0.6915', '\\n', 'Epoch 5: validation_loss = 0.6935', '\\n', 'Epoch 6:\nvalidation_loss = 0.6947', '\\n', 'Epoch 7: validation_loss = 0.6963', '\\n',\n'Epoch 8: validation_loss = 0.6981', '\\n', 'Epoch 9: validation_loss = 0.6995',\n'\\n', 'Epoch 10: validation_loss = 0.6980', '\\n', 'Epoch 11: validation_loss =\n0.6989', '\\n', 'Epoch 12: validation_loss = 0.7010', '\\n', 'Epoch 13:\nvalidation_loss = 0.7003', '\\n', 'Epoch 14: validation_loss = 0.6997', '\\n',\n'Epoch 15: validation_loss = 0.7001', '\\n', 'Epoch 16: validation_loss =\n0.7009', '\\n', 'Epoch 17: validation_loss = 0.7004', '\\n', 'Epoch 18:\nvalidation_loss = 0.6986', '\\n', 'Epoch 19: validation_loss = 0.6990', '\\n',\n'Epoch 20: validation_loss = 0.6987', '\\n', 'Epoch 1: validation_loss = 0.7541',\n'\\n', 'Epoch 2: validation_loss = 0.7306', '\\n', 'Epoch 3: validation_loss =\n0.7176', '\\n', 'Epoch 4: validation_loss = 0.7096', '\\n', 'Epoch 5:\nvalidation_loss = 0.7055', '\\n', 'Epoch 6: validation_loss = 0.7042', '\\n',\n'Epoch 7: validation_loss = 0.7027', '\\n', 'Epoch 8: validation_loss = 0.7031',\n'\\n', 'Epoch 9: validation_loss = 0.7018', '\\n', 'Epoch 10: validation_loss =\n0.7024', '\\n', 'Epoch 11: validation_loss = 0.7030', '\\n', 'Epoch 12:\nvalidation_loss = 0.7026', '\\n', 'Epoch 13: validation_loss = 0.7029', '\\n',\n'Epoch 14: validation_loss = 0.7033', '\\n', 'Epoch 15: validation_loss =\n0.7040', '\\n', 'Epoch 16: validation_loss = 0.7030', '\\n', 'Epoch 17:\nvalidation_loss = 0.7051', '\\n', 'Epoch 18: validation_loss = 0.7029', '\\n',\n'Epoch 19: validation_loss = 0.7043', '\\n', 'Epoch 20: validation_loss =\n0.7035', '\\n', 'Execution time: 37 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 0.5510', '\\n', 'Epoch\n2: validation_loss = 0.5409', '\\n', 'Epoch 3: validation_loss = 0.5309', '\\n',\n'Epoch 4: validation_loss = 0.5218', '\\n', 'Epoch 5: validation_loss = 0.5129',\n'\\n', 'Epoch 6: validation_loss = 0.5044', '\\n', 'Epoch 7: validation_loss =\n0.4964', '\\n', 'Epoch 8: validation_loss = 0.4890', '\\n', 'Epoch 9:\nvalidation_loss = 0.4817', '\\n', 'Epoch 10: validation_loss = 0.4748', '\\n',\n'Epoch 11: validation_loss = 0.4682', '\\n', 'Epoch 12: validation_loss =\n0.4617', '\\n', 'Epoch 13: validation_loss = 0.4557', '\\n', 'Epoch 14:\nvalidation_loss = 0.4500', '\\n', 'Epoch 15: validation_loss = 0.4445', '\\n',\n'Epoch 16: validation_loss = 0.4392', '\\n', 'Epoch 17: validation_loss =\n0.4343', '\\n', 'Epoch 18: validation_loss = 0.4294', '\\n', 'Epoch 19:\nvalidation_loss = 0.4248', '\\n', 'Epoch 20: validation_loss = 0.4203', '\\n',\n'Epoch 1: validation_loss = 0.4910', '\\n', 'Epoch 2: validation_loss = 0.4860',\n'\\n', 'Epoch 3: validation_loss = 0.4812', '\\n', 'Epoch 4: validation_loss =\n0.4764', '\\n', 'Epoch 5: validation_loss = 0.4719', '\\n', 'Epoch 6:\nvalidation_loss = 0.4673', '\\n', 'Epoch 7: validation_loss = 0.4630', '\\n',\n'Epoch 8: validation_loss = 0.4587', '\\n', 'Epoch 9: validation_loss = 0.4547',\n'\\n', 'Epoch 10: validation_loss = 0.4507', '\\n', 'Epoch 11: validation_loss =\n0.4469', '\\n', 'Epoch 12: validation_loss = 0.4431', '\\n', 'Epoch 13:\nvalidation_loss = 0.4394', '\\n', 'Epoch 14: validation_loss = 0.4358', '\\n',\n'Epoch 15: validation_loss = 0.4324', '\\n', 'Epoch 16: validation_loss =\n0.4291', '\\n', 'Epoch 17: validation_loss = 0.4257', '\\n', 'Epoch 18:\nvalidation_loss = 0.4226', '\\n', 'Epoch 19: validation_loss = 0.4195', '\\n',\n'Epoch 20: validation_loss = 0.4164', '\\n', 'Epoch 1: validation_loss = 0.7712',\n'\\n', 'Epoch 2: validation_loss = 0.6597', '\\n', 'Epoch 3: validation_loss =\n0.5745', '\\n', 'Epoch 4: validation_loss = 0.5120', '\\n', 'Epoch 5:\nvalidation_loss = 0.4648', '\\n', 'Epoch 6: validation_loss = 0.4290', '\\n',\n'Epoch 7: validation_loss = 0.4011', '\\n', 'Epoch 8: validation_loss = 0.3790',\n'\\n', 'Epoch 9: validation_loss = 0.3615', '\\n', 'Epoch 10: validation_loss =\n0.3461', '\\n', 'Epoch 11: validation_loss = 0.3337', '\\n', 'Epoch 12:\nvalidation_loss = 0.3234', '\\n', 'Epoch 13: validation_loss = 0.3142', '\\n',\n'Epoch 14: validation_loss = 0.3066', '\\n', 'Epoch 15: validation_loss =\n0.2998', '\\n', 'Epoch 16: validation_loss = 0.2940', '\\n', 'Epoch 17:\nvalidation_loss = 0.2887', '\\n', 'Epoch 18: validation_loss = 0.2839', '\\n',\n'Epoch 19: validation_loss = 0.2801', '\\n', 'Epoch 20: validation_loss =\n0.2761', '\\n', 'Epoch 1: validation_loss = 1.0616', '\\n', 'Epoch 2:\nvalidation_loss = 0.9727', '\\n', 'Epoch 3: validation_loss = 0.8935', '\\n',\n'Epoch 4: validation_loss = 0.8228', '\\n', 'Epoch 5: validation_loss = 0.7598',\n'\\n', 'Epoch 6: validation_loss = 0.7054', '\\n', 'Epoch 7: validation_loss =\n0.6587', '\\n', 'Epoch 8: validation_loss = 0.6171', '\\n', 'Epoch 9:\nvalidation_loss = 0.5818', '\\n', 'Epoch 10: validation_loss = 0.5507', '\\n',\n'Epoch 11: validation_loss = 0.5242', '\\n', 'Epoch 12: validation_loss =\n0.5005', '\\n', 'Epoch 13: validation_loss = 0.4797', '\\n', 'Epoch 14:\nvalidation_loss = 0.4612', '\\n', 'Epoch 15: validation_loss = 0.4446', '\\n',\n'Epoch 16: validation_loss = 0.4303', '\\n', 'Epoch 17: validation_loss =\n0.4169', '\\n', 'Epoch 18: validation_loss = 0.4050', '\\n', 'Epoch 19:\nvalidation_loss = 0.3945', '\\n', 'Epoch 20: validation_loss = 0.3846', '\\n',\n'Epoch 1: validation_loss = 0.7844', '\\n', 'Epoch 2: validation_loss = 0.7739',\n'\\n', 'Epoch 3: validation_loss = 0.7653', '\\n', 'Epoch 4: validation_loss =\n0.7555', '\\n', 'Epoch 5: validation_loss = 0.7484', '\\n', 'Epoch 6:\nvalidation_loss = 0.7408', '\\n', 'Epoch 7: validation_loss = 0.7343', '\\n',\n'Epoch 8: validation_loss = 0.7288', '\\n', 'Epoch 9: validation_loss = 0.7237',\n'\\n', 'Epoch 10: validation_loss = 0.7188', '\\n', 'Epoch 11: validation_loss =\n0.7150', '\\n', 'Epoch 12: validation_loss = 0.7112', '\\n', 'Epoch 13:\nvalidation_loss = 0.7083', '\\n', 'Epoch 14: validation_loss = 0.7052', '\\n',\n'Epoch 15: validation_loss = 0.7028', '\\n', 'Epoch 16: validation_loss =\n0.7011', '\\n', 'Epoch 17: validation_loss = 0.6994', '\\n', 'Epoch 18:\nvalidation_loss = 0.6981', '\\n', 'Epoch 19: validation_loss = 0.6970', '\\n',\n'Epoch 20: validation_loss = 0.6959', '\\n', 'Epoch 1: validation_loss = 0.6999',\n'\\n', 'Epoch 2: validation_loss = 0.7000', '\\n', 'Epoch 3: validation_loss =\n0.7000', '\\n', 'Epoch 4: validation_loss = 0.6999', '\\n', 'Epoch 5:\nvalidation_loss = 0.6998', '\\n', 'Epoch 6: validation_loss = 0.6998', '\\n',\n'Epoch 7: validation_loss = 0.6999', '\\n', 'Epoch 8: validation_loss = 0.6999',\n'\\n', 'Epoch 9: validation_loss = 0.6999', '\\n', 'Epoch 10: validation_loss =\n0.7000', '\\n', 'Epoch 11: validation_loss = 0.6998', '\\n', 'Epoch 12:\nvalidation_loss = 0.6997', '\\n', 'Epoch 13: validation_loss = 0.6996', '\\n',\n'Epoch 14: validation_loss = 0.6995', '\\n', 'Epoch 15: validation_loss =\n0.6996', '\\n', 'Epoch 16: validation_loss = 0.6994', '\\n', 'Epoch 17:\nvalidation_loss = 0.6993', '\\n', 'Epoch 18: validation_loss = 0.6995', '\\n',\n'Epoch 19: validation_loss = 0.6993', '\\n', 'Epoch 20: validation_loss =\n0.6995', '\\n', 'Epoch 1: validation_loss = 0.7537', '\\n', 'Epoch 2:\nvalidation_loss = 0.7287', '\\n', 'Epoch 3: validation_loss = 0.7089', '\\n',\n'Epoch 4: validation_loss = 0.7083', '\\n', 'Epoch 5: validation_loss = 0.7017',\n'\\n', 'Epoch 6: validation_loss = 0.7019', '\\n', 'Epoch 7: validation_loss =\n0.7009', '\\n', 'Epoch 8: validation_loss = 0.7026', '\\n', 'Epoch 9:\nvalidation_loss = 0.7033', '\\n', 'Epoch 10: validation_loss = 0.6998', '\\n',\n'Epoch 11: validation_loss = 0.6995', '\\n', 'Epoch 12: validation_loss =\n0.6997', '\\n', 'Epoch 13: validation_loss = 0.7044', '\\n', 'Epoch 14:\nvalidation_loss = 0.7009', '\\n', 'Epoch 15: validation_loss = 0.7019', '\\n',\n'Epoch 16: validation_loss = 0.6996', '\\n', 'Epoch 17: validation_loss =\n0.7054', '\\n', 'Epoch 18: validation_loss = 0.7029', '\\n', 'Epoch 19:\nvalidation_loss = 0.7017', '\\n', 'Epoch 20: validation_loss = 0.7053', '\\n',\n'Epoch 1: validation_loss = 0.7429', '\\n', 'Epoch 2: validation_loss = 0.7208',\n'\\n', 'Epoch 3: validation_loss = 0.7084', '\\n', 'Epoch 4: validation_loss =\n0.7021', '\\n', 'Epoch 5: validation_loss = 0.7002', '\\n', 'Epoch 6:\nvalidation_loss = 0.7002', '\\n', 'Epoch 7: validation_loss = 0.6992', '\\n',\n'Epoch 8: validation_loss = 0.6979', '\\n', 'Epoch 9: validation_loss = 0.6983',\n'\\n', 'Epoch 10: validation_loss = 0.6986', '\\n', 'Epoch 11: validation_loss =\n0.7001', '\\n', 'Epoch 12: validation_loss = 0.7004', '\\n', 'Epoch 13:\nvalidation_loss = 0.7012', '\\n', 'Epoch 14: validation_loss = 0.6986', '\\n',\n'Epoch 15: validation_loss = 0.6974', '\\n', 'Epoch 16: validation_loss =\n0.6978', '\\n', 'Epoch 17: validation_loss = 0.6986', '\\n', 'Epoch 18:\nvalidation_loss = 0.6980', '\\n', 'Epoch 19: validation_loss = 0.6985', '\\n',\n'Epoch 20: validation_loss = 0.6986', '\\n', 'Epoch 1: validation_loss = 0.7220',\n'\\n', 'Epoch 2: validation_loss = 0.7182', '\\n', 'Epoch 3: validation_loss =\n0.7149', '\\n', 'Epoch 4: validation_loss = 0.7118', '\\n', 'Epoch 5:\nvalidation_loss = 0.7089', '\\n', 'Epoch 6: validation_loss = 0.7063', '\\n',\n'Epoch 7: validation_loss = 0.7041', '\\n', 'Epoch 8: validation_loss = 0.7020',\n'\\n', 'Epoch 9: validation_loss = 0.7002', '\\n', 'Epoch 10: validation_loss =\n0.6988', '\\n', 'Epoch 11: validation_loss = 0.6974', '\\n', 'Epoch 12:\nvalidation_loss = 0.6963', '\\n', 'Epoch 13: validation_loss = 0.6952', '\\n',\n'Epoch 14: validation_loss = 0.6945', '\\n', 'Epoch 15: validation_loss =\n0.6939', '\\n', 'Epoch 16: validation_loss = 0.6934', '\\n', 'Epoch 17:\nvalidation_loss = 0.6929', '\\n', 'Epoch 18: validation_loss = 0.6925', '\\n',\n'Epoch 19: validation_loss = 0.6923', '\\n', 'Epoch 20: validation_loss =\n0.6920', '\\n', 'Epoch 1: validation_loss = 0.6978', '\\n', 'Epoch 2:\nvalidation_loss = 0.6981', '\\n', 'Epoch 3: validation_loss = 0.6982', '\\n',\n'Epoch 4: validation_loss = 0.6984', '\\n', 'Epoch 5: validation_loss = 0.6987',\n'\\n', 'Epoch 6: validation_loss = 0.6990', '\\n', 'Epoch 7: validation_loss =\n0.6993', '\\n', 'Epoch 8: validation_loss = 0.6993', '\\n', 'Epoch 9:\nvalidation_loss = 0.6995', '\\n', 'Epoch 10: validation_loss = 0.6998', '\\n',\n'Epoch 11: validation_loss = 0.7000', '\\n', 'Epoch 12: validation_loss =\n0.7003', '\\n', 'Epoch 13: validation_loss = 0.7004', '\\n', 'Epoch 14:\nvalidation_loss = 0.7004', '\\n', 'Epoch 15: validation_loss = 0.7007', '\\n',\n'Epoch 16: validation_loss = 0.7009', '\\n', 'Epoch 17: validation_loss =\n0.7011', '\\n', 'Epoch 18: validation_loss = 0.7013', '\\n', 'Epoch 19:\nvalidation_loss = 0.7015', '\\n', 'Epoch 20: validation_loss = 0.7017', '\\n',\n'Epoch 1: validation_loss = 0.7164', '\\n', 'Epoch 2: validation_loss = 0.7047',\n'\\n', 'Epoch 3: validation_loss = 0.7024', '\\n', 'Epoch 4: validation_loss =\n0.6997', '\\n', 'Epoch 5: validation_loss = 0.7006', '\\n', 'Epoch 6:\nvalidation_loss = 0.7005', '\\n', 'Epoch 7: validation_loss = 0.7016', '\\n',\n'Epoch 8: validation_loss = 0.6993', '\\n', 'Epoch 9: validation_loss = 0.7010',\n'\\n', 'Epoch 10: validation_loss = 0.6991', '\\n', 'Epoch 11: validation_loss =\n0.6988', '\\n', 'Epoch 12: validation_loss = 0.6991', '\\n', 'Epoch 13:\nvalidation_loss = 0.6990', '\\n', 'Epoch 14: validation_loss = 0.6998', '\\n',\n'Epoch 15: validation_loss = 0.6997', '\\n', 'Epoch 16: validation_loss =\n0.6993', '\\n', 'Epoch 17: validation_loss = 0.6997', '\\n', 'Epoch 18:\nvalidation_loss = 0.6998', '\\n', 'Epoch 19: validation_loss = 0.6996', '\\n',\n'Epoch 20: validation_loss = 0.6979', '\\n', 'Epoch 1: validation_loss = 0.7634',\n'\\n', 'Epoch 2: validation_loss = 0.7425', '\\n', 'Epoch 3: validation_loss =\n0.7289', '\\n', 'Epoch 4: validation_loss = 0.7192', '\\n', 'Epoch 5:\nvalidation_loss = 0.7124', '\\n', 'Epoch 6: validation_loss = 0.7092', '\\n',\n'Epoch 7: validation_loss = 0.7063', '\\n', 'Epoch 8: validation_loss = 0.7058',\n'\\n', 'Epoch 9: validation_loss = 0.7044', '\\n', 'Epoch 10: validation_loss =\n0.7039', '\\n', 'Epoch 11: validation_loss = 0.7026', '\\n', 'Epoch 12:\nvalidation_loss = 0.7039', '\\n', 'Epoch 13: validation_loss = 0.7032', '\\n',\n'Epoch 14: validation_loss = 0.7036', '\\n', 'Epoch 15: validation_loss =\n0.7035', '\\n', 'Epoch 16: validation_loss = 0.7049', '\\n', 'Epoch 17:\nvalidation_loss = 0.7038', '\\n', 'Epoch 18: validation_loss = 0.7036', '\\n',\n'Epoch 19: validation_loss = 0.7028', '\\n', 'Epoch 20: validation_loss =\n0.7041', '\\n', 'Execution time: 36 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 0.5973', '\\n', 'Epoch\n2: validation_loss = 0.5856', '\\n', 'Epoch 3: validation_loss = 0.5745', '\\n',\n'Epoch 4: validation_loss = 0.5637', '\\n', 'Epoch 5: validation_loss = 0.5530',\n'\\n', 'Epoch 6: validation_loss = 0.5430', '\\n', 'Epoch 7: validation_loss =\n0.5336', '\\n', 'Epoch 8: validation_loss = 0.5245', '\\n', 'Epoch 9:\nvalidation_loss = 0.5156', '\\n', 'Epoch 10: validation_loss = 0.5072', '\\n',\n'Epoch 11: validation_loss = 0.4989', '\\n', 'Epoch 12: validation_loss =\n0.4910', '\\n', 'Epoch 13: validation_loss = 0.4833', '\\n', 'Epoch 14:\nvalidation_loss = 0.4759', '\\n', 'Epoch 15: validation_loss = 0.4687', '\\n',\n'Epoch 16: validation_loss = 0.4619', '\\n', 'Epoch 17: validation_loss =\n0.4553', '\\n', 'Epoch 18: validation_loss = 0.4488', '\\n', 'Epoch 19:\nvalidation_loss = 0.4426', '\\n', 'Epoch 20: validation_loss = 0.4365', '\\n',\n'Epoch 1: validation_loss = 1.2080', '\\n', 'Epoch 2: validation_loss = 1.1960',\n'\\n', 'Epoch 3: validation_loss = 1.1841', '\\n', 'Epoch 4: validation_loss =\n1.1723', '\\n', 'Epoch 5: validation_loss = 1.1607', '\\n', 'Epoch 6:\nvalidation_loss = 1.1491', '\\n', 'Epoch 7: validation_loss = 1.1376', '\\n',\n'Epoch 8: validation_loss = 1.1263', '\\n', 'Epoch 9: validation_loss = 1.1151',\n'\\n', 'Epoch 10: validation_loss = 1.1040', '\\n', 'Epoch 11: validation_loss =\n1.0930', '\\n', 'Epoch 12: validation_loss = 1.0820', '\\n', 'Epoch 13:\nvalidation_loss = 1.0712', '\\n', 'Epoch 14: validation_loss = 1.0607', '\\n',\n'Epoch 15: validation_loss = 1.0501', '\\n', 'Epoch 16: validation_loss =\n1.0398', '\\n', 'Epoch 17: validation_loss = 1.0294', '\\n', 'Epoch 18:\nvalidation_loss = 1.0194', '\\n', 'Epoch 19: validation_loss = 1.0094', '\\n',\n'Epoch 20: validation_loss = 0.9994', '\\n', 'Epoch 1: validation_loss = 0.3660',\n'\\n', 'Epoch 2: validation_loss = 0.3229', '\\n', 'Epoch 3: validation_loss =\n0.2935', '\\n', 'Epoch 4: validation_loss = 0.2724', '\\n', 'Epoch 5:\nvalidation_loss = 0.2556', '\\n', 'Epoch 6: validation_loss = 0.2431', '\\n',\n'Epoch 7: validation_loss = 0.2334', '\\n', 'Epoch 8: validation_loss = 0.2248',\n'\\n', 'Epoch 9: validation_loss = 0.2181', '\\n', 'Epoch 10: validation_loss =\n0.2126', '\\n', 'Epoch 11: validation_loss = 0.2076', '\\n', 'Epoch 12:\nvalidation_loss = 0.2039', '\\n', 'Epoch 13: validation_loss = 0.2004', '\\n',\n'Epoch 14: validation_loss = 0.1973', '\\n', 'Epoch 15: validation_loss =\n0.1947', '\\n', 'Epoch 16: validation_loss = 0.1923', '\\n', 'Epoch 17:\nvalidation_loss = 0.1903', '\\n', 'Epoch 18: validation_loss = 0.1883', '\\n',\n'Epoch 19: validation_loss = 0.1869', '\\n', 'Epoch 20: validation_loss =\n0.1856', '\\n', 'Epoch 1: validation_loss = 0.5584', '\\n', 'Epoch 2:\nvalidation_loss = 0.5054', '\\n', 'Epoch 3: validation_loss = 0.4633', '\\n',\n'Epoch 4: validation_loss = 0.4295', '\\n', 'Epoch 5: validation_loss = 0.4013',\n'\\n', 'Epoch 6: validation_loss = 0.3770', '\\n', 'Epoch 7: validation_loss =\n0.3557', '\\n', 'Epoch 8: validation_loss = 0.3381', '\\n', 'Epoch 9:\nvalidation_loss = 0.3227', '\\n', 'Epoch 10: validation_loss = 0.3091', '\\n',\n'Epoch 11: validation_loss = 0.2971', '\\n', 'Epoch 12: validation_loss =\n0.2866', '\\n', 'Epoch 13: validation_loss = 0.2771', '\\n', 'Epoch 14:\nvalidation_loss = 0.2689', '\\n', 'Epoch 15: validation_loss = 0.2612', '\\n',\n'Epoch 16: validation_loss = 0.2543', '\\n', 'Epoch 17: validation_loss =\n0.2483', '\\n', 'Epoch 18: validation_loss = 0.2425', '\\n', 'Epoch 19:\nvalidation_loss = 0.2372', '\\n', 'Epoch 20: validation_loss = 0.2328', '\\n',\n'Epoch 1: validation_loss = 0.7382', '\\n', 'Epoch 2: validation_loss = 0.7348',\n'\\n', 'Epoch 3: validation_loss = 0.7313', '\\n', 'Epoch 4: validation_loss =\n0.7283', '\\n', 'Epoch 5: validation_loss = 0.7255', '\\n', 'Epoch 6:\nvalidation_loss = 0.7226', '\\n', 'Epoch 7: validation_loss = 0.7204', '\\n',\n'Epoch 8: validation_loss = 0.7183', '\\n', 'Epoch 9: validation_loss = 0.7162',\n'\\n', 'Epoch 10: validation_loss = 0.7143', '\\n', 'Epoch 11: validation_loss =\n0.7129', '\\n', 'Epoch 12: validation_loss = 0.7115', '\\n', 'Epoch 13:\nvalidation_loss = 0.7102', '\\n', 'Epoch 14: validation_loss = 0.7090', '\\n',\n'Epoch 15: validation_loss = 0.7079', '\\n', 'Epoch 16: validation_loss =\n0.7069', '\\n', 'Epoch 17: validation_loss = 0.7060', '\\n', 'Epoch 18:\nvalidation_loss = 0.7052', '\\n', 'Epoch 19: validation_loss = 0.7045', '\\n',\n'Epoch 20: validation_loss = 0.7039', '\\n', 'Epoch 1: validation_loss = 1.2723',\n'\\n', 'Epoch 2: validation_loss = 1.2585', '\\n', 'Epoch 3: validation_loss =\n1.2451', '\\n', 'Epoch 4: validation_loss = 1.2310', '\\n', 'Epoch 5:\nvalidation_loss = 1.2177', '\\n', 'Epoch 6: validation_loss = 1.2037', '\\n',\n'Epoch 7: validation_loss = 1.1906', '\\n', 'Epoch 8: validation_loss = 1.1767',\n'\\n', 'Epoch 9: validation_loss = 1.1640', '\\n', 'Epoch 10: validation_loss =\n1.1507', '\\n', 'Epoch 11: validation_loss = 1.1387', '\\n', 'Epoch 12:\nvalidation_loss = 1.1254', '\\n', 'Epoch 13: validation_loss = 1.1130', '\\n',\n'Epoch 14: validation_loss = 1.1002', '\\n', 'Epoch 15: validation_loss =\n1.0886', '\\n', 'Epoch 16: validation_loss = 1.0769', '\\n', 'Epoch 17:\nvalidation_loss = 1.0645', '\\n', 'Epoch 18: validation_loss = 1.0528', '\\n',\n'Epoch 19: validation_loss = 1.0415', '\\n', 'Epoch 20: validation_loss =\n1.0307', '\\n', 'Epoch 1: validation_loss = 0.7077', '\\n', 'Epoch 2:\nvalidation_loss = 0.7014', '\\n', 'Epoch 3: validation_loss = 0.7001', '\\n',\n'Epoch 4: validation_loss = 0.7016', '\\n', 'Epoch 5: validation_loss = 0.6982',\n'\\n', 'Epoch 6: validation_loss = 0.7017', '\\n', 'Epoch 7: validation_loss =\n0.7006', '\\n', 'Epoch 8: validation_loss = 0.7018', '\\n', 'Epoch 9:\nvalidation_loss = 0.7006', '\\n', 'Epoch 10: validation_loss = 0.7037', '\\n',\n'Epoch 11: validation_loss = 0.7001', '\\n', 'Epoch 12: validation_loss =\n0.7003', '\\n', 'Epoch 13: validation_loss = 0.6994', '\\n', 'Epoch 14:\nvalidation_loss = 0.6991', '\\n', 'Epoch 15: validation_loss = 0.7004', '\\n',\n'Epoch 16: validation_loss = 0.7000', '\\n', 'Epoch 17: validation_loss =\n0.7036', '\\n', 'Epoch 18: validation_loss = 0.6998', '\\n', 'Epoch 19:\nvalidation_loss = 0.7005', '\\n', 'Epoch 20: validation_loss = 0.7021', '\\n',\n'Epoch 1: validation_loss = 0.6931', '\\n', 'Epoch 2: validation_loss = 0.6953',\n'\\n', 'Epoch 3: validation_loss = 0.6978', '\\n', 'Epoch 4: validation_loss =\n0.6987', '\\n', 'Epoch 5: validation_loss = 0.6976', '\\n', 'Epoch 6:\nvalidation_loss = 0.6994', '\\n', 'Epoch 7: validation_loss = 0.6971', '\\n',\n'Epoch 8: validation_loss = 0.6985', '\\n', 'Epoch 9: validation_loss = 0.6986',\n'\\n', 'Epoch 10: validation_loss = 0.6977', '\\n', 'Epoch 11: validation_loss =\n0.6985', '\\n', 'Epoch 12: validation_loss = 0.6984', '\\n', 'Epoch 13:\nvalidation_loss = 0.6994', '\\n', 'Epoch 14: validation_loss = 0.6991', '\\n',\n'Epoch 15: validation_loss = 0.6991', '\\n', 'Epoch 16: validation_loss =\n0.6982', '\\n', 'Epoch 17: validation_loss = 0.6990', '\\n', 'Epoch 18:\nvalidation_loss = 0.6982', '\\n', 'Epoch 19: validation_loss = 0.6963', '\\n',\n'Epoch 20: validation_loss = 0.6978', '\\n', 'Epoch 1: validation_loss = 0.6990',\n'\\n', 'Epoch 2: validation_loss = 0.6976', '\\n', 'Epoch 3: validation_loss =\n0.6963', '\\n', 'Epoch 4: validation_loss = 0.6953', '\\n', 'Epoch 5:\nvalidation_loss = 0.6946', '\\n', 'Epoch 6: validation_loss = 0.6940', '\\n',\n'Epoch 7: validation_loss = 0.6936', '\\n', 'Epoch 8: validation_loss = 0.6932',\n'\\n', 'Epoch 9: validation_loss = 0.6930', '\\n', 'Epoch 10: validation_loss =\n0.6929', '\\n', 'Epoch 11: validation_loss = 0.6929', '\\n', 'Epoch 12:\nvalidation_loss = 0.6927', '\\n', 'Epoch 13: validation_loss = 0.6927', '\\n',\n'Epoch 14: validation_loss = 0.6929', '\\n', 'Epoch 15: validation_loss =\n0.6930', '\\n', 'Epoch 16: validation_loss = 0.6932', '\\n', 'Epoch 17:\nvalidation_loss = 0.6934', '\\n', 'Epoch 18: validation_loss = 0.6937', '\\n',\n'Epoch 19: validation_loss = 0.6940', '\\n', 'Epoch 20: validation_loss =\n0.6943', '\\n', 'Epoch 1: validation_loss = 0.7078', '\\n', 'Epoch 2:\nvalidation_loss = 0.7065', '\\n', 'Epoch 3: validation_loss = 0.7052', '\\n',\n'Epoch 4: validation_loss = 0.7040', '\\n', 'Epoch 5: validation_loss = 0.7029',\n'\\n', 'Epoch 6: validation_loss = 0.7019', '\\n', 'Epoch 7: validation_loss =\n0.7009', '\\n', 'Epoch 8: validation_loss = 0.7000', '\\n', 'Epoch 9:\nvalidation_loss = 0.6992', '\\n', 'Epoch 10: validation_loss = 0.6983', '\\n',\n'Epoch 11: validation_loss = 0.6976', '\\n', 'Epoch 12: validation_loss =\n0.6969', '\\n', 'Epoch 13: validation_loss = 0.6963', '\\n', 'Epoch 14:\nvalidation_loss = 0.6957', '\\n', 'Epoch 15: validation_loss = 0.6952', '\\n',\n'Epoch 16: validation_loss = 0.6947', '\\n', 'Epoch 17: validation_loss =\n0.6943', '\\n', 'Epoch 18: validation_loss = 0.6939', '\\n', 'Epoch 19:\nvalidation_loss = 0.6937', '\\n', 'Epoch 20: validation_loss = 0.6934', '\\n',\n'Epoch 1: validation_loss = 0.7972', '\\n', 'Epoch 2: validation_loss = 0.7556',\n'\\n', 'Epoch 3: validation_loss = 0.7247', '\\n', 'Epoch 4: validation_loss =\n0.7108', '\\n', 'Epoch 5: validation_loss = 0.7044', '\\n', 'Epoch 6:\nvalidation_loss = 0.7021', '\\n', 'Epoch 7: validation_loss = 0.7007', '\\n',\n'Epoch 8: validation_loss = 0.6999', '\\n', 'Epoch 9: validation_loss = 0.7023',\n'\\n', 'Epoch 10: validation_loss = 0.6991', '\\n', 'Epoch 11: validation_loss =\n0.7011', '\\n', 'Epoch 12: validation_loss = 0.6993', '\\n', 'Epoch 13:\nvalidation_loss = 0.6989', '\\n', 'Epoch 14: validation_loss = 0.6989', '\\n',\n'Epoch 15: validation_loss = 0.6977', '\\n', 'Epoch 16: validation_loss =\n0.6981', '\\n', 'Epoch 17: validation_loss = 0.6999', '\\n', 'Epoch 18:\nvalidation_loss = 0.7005', '\\n', 'Epoch 19: validation_loss = 0.6997', '\\n',\n'Epoch 20: validation_loss = 0.6962', '\\n', 'Epoch 1: validation_loss = 0.7101',\n'\\n', 'Epoch 2: validation_loss = 0.7044', '\\n', 'Epoch 3: validation_loss =\n0.7035', '\\n', 'Epoch 4: validation_loss = 0.7052', '\\n', 'Epoch 5:\nvalidation_loss = 0.7039', '\\n', 'Epoch 6: validation_loss = 0.7036', '\\n',\n'Epoch 7: validation_loss = 0.7044', '\\n', 'Epoch 8: validation_loss = 0.7051',\n'\\n', 'Epoch 9: validation_loss = 0.7051', '\\n', 'Epoch 10: validation_loss =\n0.7034', '\\n', 'Epoch 11: validation_loss = 0.7044', '\\n', 'Epoch 12:\nvalidation_loss = 0.7041', '\\n', 'Epoch 13: validation_loss = 0.7042', '\\n',\n'Epoch 14: validation_loss = 0.7032', '\\n', 'Epoch 15: validation_loss =\n0.7024', '\\n', 'Epoch 16: validation_loss = 0.7042', '\\n', 'Epoch 17:\nvalidation_loss = 0.7027', '\\n', 'Epoch 18: validation_loss = 0.7042', '\\n',\n'Epoch 19: validation_loss = 0.7038', '\\n', 'Epoch 20: validation_loss =\n0.7042', '\\n', 'Execution time: 36 seconds seconds (time limit is an hour).']", ""], "analysis": ["", "The script crashes because it attempts to call .numpy() on a tensor that still\nrequires gradients. To fix this, detach the tensor from the computation graph\nbefore converting to a NumPy array\u2014for example, use\ntorch.sigmoid(logits).detach().cpu().numpy() (and apply the same change in the\nvalidation loop).", "The script crashes at line 78 when calling .numpy() on a Tensor that still\nrequires grad. To fix, detach the tensor before converting to NumPy, e.g., use\ntorch.sigmoid(logits).detach().cpu().numpy().", "The script crashes because it calls .numpy() on a tensor that still requires\ngradients (the result of torch.sigmoid(logits)). Numpy conversion requires a\nnon-gradient-tracking tensor. The fix is to detach the tensor before moving to\nCPU and converting to numpy: e.g., use\ntorch.sigmoid(logits).detach().cpu().numpy() for both training and validation\nprediction collection.", "The script fails when calling load_dataset('ieee_fraud_detection') because that\ndataset identifier doesn\u2019t exist on Huggingface. Use the correct dataset slug\n(e.g., 'ieee-fraud-detection') or choose another valid dataset name, or wrap the\nload_dataset call in a try/except and verify available dataset names before\nloading.", "The script fails at `prepare_openml` when calling `load_dataset(\"openml\",\nstr(dataset_id))`, raising a DatasetNotFoundError because the HuggingFace\n`datasets` library does not include an `openml` builder by default. As a result,\nthe two new datasets (Pima Diabetes and Blood Transfusion) cannot be loaded and\nthe script aborts. To fix this, either install and enable the OpenML integration\nfor `datasets` (e.g., `pip install datasets[openml]` and ensure the `openml`\npackage is installed) or switch to using the OpenML Python API\n(`openml.datasets.get_dataset`) to fetch the data. Alternatively, use existing\nHuggingFace datasets (e.g., `load_dataset(\"pima_diabetes\")`,\n`load_dataset(\"blood_transfusion\")`) if available.", "", "The training script executed successfully without any errors. All \u03b22 settings\nconverge to a validation AUC of ~0.9572 on the synthetic binary classification\ntask, although for \u03b22=0.95 and 0.999 the model initially produces inverted\npredictions (AUC \u2248 0.0428) before quickly correcting. No runtime or logic bugs\nwere detected. To move the experiment forward, I recommend adding two real-world\ndatasets from Hugging Face\u2014for example, IMDB (sentiment classification) and AG\nNews (topic classification)\u2014to assess generalization and the impact of Adam\u2019s \u03b22\nhyperparameter on practical tasks.", "", "The script crashes with `RuntimeError: Can't call numpy() on Tensor that\nrequires grad` when converting model predictions to NumPy. The bug arises\nbecause the tensor still tracks gradients. To fix this, detach the tensor before\nconversion, for example: `torch.sigmoid(logits).detach().cpu().numpy()` (instead\nof `.cpu().numpy()`), both in the training and validation loops.", "The training loop crashes when converting a tensor that requires gradients to a\nNumPy array. The error arises from calling `.cpu().numpy()` directly on a tensor\nthat still has `requires_grad=True`. To fix this, detach the tensor from the\ncomputation graph before conversion. For example, replace:\ntorch.sigmoid(logits).cpu().numpy()  with:\ntorch.sigmoid(logits).detach().cpu().numpy()  (and similarly for other tensor-\nto-NumPy conversions in the code).", "The training loop attempts to call .numpy() on a tensor that still requires\ngradients (torch.sigmoid(logits).cpu().numpy()), causing a RuntimeError. To fix\nthis, detach the tensor from the computation graph before converting to a NumPy\narray. For example, use torch.sigmoid(logits).detach().cpu().numpy() (and\nsimilarly for v_preds) instead of .cpu().numpy().", "", "", "", "", ""], "exc_type": [null, "RuntimeError", "RuntimeError", "RuntimeError", "DatasetNotFoundError", "DatasetNotFoundError", null, null, null, "RuntimeError", "RuntimeError", "RuntimeError", null, null, null, null, null], "exc_info": [null, {"args": ["Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."]}, {"args": ["Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."]}, {"args": ["Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."]}, {"args": ["Dataset 'ieee_fraud_detection' doesn't exist on the Hub or cannot be accessed."]}, {"args": ["Dataset 'openml' doesn't exist on the Hub or cannot be accessed."]}, null, null, null, {"args": ["Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."]}, {"args": ["Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."]}, {"args": ["Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."]}, null, null, null, null, null], "exc_stack": [null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 80, "<module>", "all_preds.append(torch.sigmoid(logits).cpu().numpy())"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 78, "<module>", "all_preds.append(torch.sigmoid(logits).cpu().numpy())"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 72, "<module>", "t_preds.append(torch.sigmoid(logits).cpu().numpy())"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 107, "<module>", "train_loader, val_loader, input_dim = prep()"], ["runfile.py", 77, "<lambda>", "\"ieee_fraud_detection\": lambda: prepare_tabular("], ["runfile.py", 41, "prepare_tabular", "raw = load_dataset(dataset_name)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1652, "dataset_module_factory", "raise e1 from None"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1578, "dataset_module_factory", "raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 68, "<module>", "(train_X_dia, train_y_dia), (val_X_dia, val_y_dia) = prepare_openml(37)  # Pima Diabetes"], ["runfile.py", 50, "prepare_openml", "ds = load_dataset(\"openml\", str(dataset_id))"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1652, "dataset_module_factory", "raise e1 from None"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1578, "dataset_module_factory", "raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e"]], null, null, null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 150, "<module>", "all_preds.append(torch.sigmoid(logits).cpu().numpy())"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 73, "<module>", "tp.append(torch.sigmoid(logits).cpu().numpy())"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 77, "<module>", "t_preds.append(torch.sigmoid(logits).cpu().numpy())"]], null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "Training AUC", "lower_is_better": false, "description": "Area Under the ROC Curve on training data", "data": [{"dataset_name": "synthetic", "final_value": 0.9505, "best_value": 0.9505}]}, {"metric_name": "Validation AUC", "lower_is_better": false, "description": "Area Under the ROC Curve on validation data", "data": [{"dataset_name": "synthetic", "final_value": 0.9572, "best_value": 0.9572}]}, {"metric_name": "Training Loss", "lower_is_better": true, "description": "Binary cross-entropy loss on training data", "data": [{"dataset_name": "synthetic", "final_value": 0.2669, "best_value": 0.2669}]}, {"metric_name": "Validation Loss", "lower_is_better": true, "description": "Binary cross-entropy loss on validation data", "data": [{"dataset_name": "synthetic", "final_value": 0.2861, "best_value": 0.2861}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.95, "best_value": 0.9522}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9572, "best_value": 0.9572}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.2732, "best_value": 0.2593}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.2917, "best_value": 0.2782}]}]}, {"metric_names": [{"metric_name": "train AUC", "lower_is_better": false, "description": "Area Under the Receiver Operating Characteristic curve on the training dataset", "data": [{"dataset_name": "Synthetic", "final_value": 0.9503, "best_value": 0.9523}]}, {"metric_name": "validation AUC", "lower_is_better": false, "description": "Area Under the Receiver Operating Characteristic curve on the validation dataset", "data": [{"dataset_name": "Synthetic", "final_value": 0.9572, "best_value": 0.9572}]}]}, {"metric_names": [{"metric_name": "training AUC", "lower_is_better": false, "description": "Area under the ROC curve on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9523, "best_value": 0.9523}]}, {"metric_name": "validation AUC", "lower_is_better": false, "description": "Area under the ROC curve on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9572, "best_value": 0.9572}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy", "data": [{"dataset_name": "synthetic", "final_value": 0.9522, "best_value": 0.9522}, {"dataset_name": "sst2", "final_value": 0.5558, "best_value": 0.5558}, {"dataset_name": "yelp_polarity", "final_value": 0.593, "best_value": 0.593}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "synthetic", "final_value": 0.9572, "best_value": 0.9572}, {"dataset_name": "sst2", "final_value": 0.584, "best_value": 0.584}, {"dataset_name": "yelp_polarity", "final_value": 0.5828, "best_value": 0.5828}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "synthetic", "final_value": 0.2641, "best_value": 0.2641}, {"dataset_name": "sst2", "final_value": 0.6815, "best_value": 0.6815}, {"dataset_name": "yelp_polarity", "final_value": 0.68, "best_value": 0.68}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "synthetic", "final_value": 0.2876, "best_value": 0.2876}, {"dataset_name": "sst2", "final_value": 0.6891, "best_value": 0.6891}, {"dataset_name": "yelp_polarity", "final_value": 0.6924, "best_value": 0.6924}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Model accuracy on the training set", "data": [{"dataset_name": "synthetic", "final_value": 0.9643, "best_value": 0.9643}, {"dataset_name": "sst2", "final_value": 0.5528, "best_value": 0.5528}, {"dataset_name": "yelp_polarity", "final_value": 0.5939, "best_value": 0.5939}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Model accuracy on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.9718, "best_value": 0.9718}, {"dataset_name": "sst2", "final_value": 0.584, "best_value": 0.584}, {"dataset_name": "yelp_polarity", "final_value": 0.5828, "best_value": 0.5828}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Model loss on the training set", "data": [{"dataset_name": "synthetic", "final_value": 0.236, "best_value": 0.236}, {"dataset_name": "sst2", "final_value": 0.6825, "best_value": 0.6825}, {"dataset_name": "yelp_polarity", "final_value": 0.6797, "best_value": 0.6797}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Model loss on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.2255, "best_value": 0.2255}, {"dataset_name": "sst2", "final_value": 0.6976, "best_value": 0.6976}, {"dataset_name": "yelp_polarity", "final_value": 0.6915, "best_value": 0.6915}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training set", "data": [{"dataset_name": "synthetic", "final_value": 0.966, "best_value": 0.966}, {"dataset_name": "sst2", "final_value": 0.5501, "best_value": 0.5501}, {"dataset_name": "yelp_polarity", "final_value": 0.5933, "best_value": 0.5933}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.9624, "best_value": 0.9624}, {"dataset_name": "sst2", "final_value": 0.584, "best_value": 0.584}, {"dataset_name": "yelp_polarity", "final_value": 0.5828, "best_value": 0.5828}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "synthetic", "final_value": 0.2492, "best_value": 0.2492}, {"dataset_name": "sst2", "final_value": 0.683, "best_value": 0.683}, {"dataset_name": "yelp_polarity", "final_value": 0.6797, "best_value": 0.6797}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.2761, "best_value": 0.2761}, {"dataset_name": "sst2", "final_value": 0.6959, "best_value": 0.6959}, {"dataset_name": "yelp_polarity", "final_value": 0.692, "best_value": 0.692}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy", "data": [{"dataset_name": "synthetic", "final_value": 0.9598, "best_value": 0.9598}, {"dataset_name": "sst2", "final_value": 0.5495, "best_value": 0.5495}, {"dataset_name": "yelp_polarity", "final_value": 0.5968, "best_value": 0.5968}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "synthetic", "final_value": 0.9717, "best_value": 0.9717}, {"dataset_name": "sst2", "final_value": 0.416, "best_value": 0.416}, {"dataset_name": "yelp_polarity", "final_value": 0.5828, "best_value": 0.5828}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "synthetic", "final_value": 0.2391, "best_value": 0.2391}, {"dataset_name": "sst2", "final_value": 0.6824, "best_value": 0.6824}, {"dataset_name": "yelp_polarity", "final_value": 0.679, "best_value": 0.679}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "synthetic", "final_value": 0.1856, "best_value": 0.1856}, {"dataset_name": "sst2", "final_value": 0.6931, "best_value": 0.6931}, {"dataset_name": "yelp_polarity", "final_value": 0.6927, "best_value": 0.6927}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_56e15adc42294ddab0e8a5292281f6d3_proc_141122/synthetic_auc_curve.png", "../../logs/0-run/experiment_results/experiment_56e15adc42294ddab0e8a5292281f6d3_proc_141122/synthetic_loss_curve.png"], [], [], [], [], [], ["../../logs/0-run/experiment_results/experiment_4e5473e240474b9c986b5834df06fd36_proc_144144/synthetic_auc_curves.png", "../../logs/0-run/experiment_results/experiment_4e5473e240474b9c986b5834df06fd36_proc_144144/synthetic_roc_curves.png", "../../logs/0-run/experiment_results/experiment_4e5473e240474b9c986b5834df06fd36_proc_144144/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_152a8e148af24c118426c42fa77c46bc_proc_144145/synthetic_auc_curves.png", "../../logs/0-run/experiment_results/experiment_152a8e148af24c118426c42fa77c46bc_proc_144145/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_7947cd38c65c4adf88784add8b609bea_proc_144143/synthetic_lr_scheduler_gamma_loss_curves.png", "../../logs/0-run/experiment_results/experiment_7947cd38c65c4adf88784add8b609bea_proc_144143/synthetic_lr_scheduler_gamma_auc_curves.png"], [], [], [], ["../../logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/sst2_auc_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/sst2_loss_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/synthetic_auc_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/yelp_polarity_loss_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/synthetic_loss_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/yelp_polarity_auc_curve_bs32_lr0.001.png"], ["../../logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/sst2_auc_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/sst2_loss_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/synthetic_auc_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/yelp_polarity_loss_curve_bs64_lr0.001.png", "../../logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/synthetic_loss_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/yelp_polarity_auc_curve_bs64_lr0.001.png"], ["../../logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/sst2_auc_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/sst2_loss_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/synthetic_auc_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/yelp_polarity_loss_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/synthetic_loss_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/yelp_polarity_auc_curve_bs32_lr0.001.png"], ["../../logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/sst2_auc_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/sst2_loss_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/synthetic_auc_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/yelp_polarity_loss_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/synthetic_loss_curve_bs32_lr0.001.png", "../../logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/yelp_polarity_auc_curve_bs32_lr0.001.png"], ["../../logs/0-run/experiment_results/seed_aggregation_f3aeb81b24de423b9d52a18cb3418593/sst2_loss_aggregated.png", "../../logs/0-run/experiment_results/seed_aggregation_f3aeb81b24de423b9d52a18cb3418593/synthetic_auc_aggregated.png", "../../logs/0-run/experiment_results/seed_aggregation_f3aeb81b24de423b9d52a18cb3418593/yelp_polarity_auc_aggregated.png", "../../logs/0-run/experiment_results/seed_aggregation_f3aeb81b24de423b9d52a18cb3418593/yelp_polarity_loss_aggregated.png", "../../logs/0-run/experiment_results/seed_aggregation_f3aeb81b24de423b9d52a18cb3418593/sst2_auc_aggregated.png", "../../logs/0-run/experiment_results/seed_aggregation_f3aeb81b24de423b9d52a18cb3418593/synthetic_loss_aggregated.png"]], "plot_paths": [["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_56e15adc42294ddab0e8a5292281f6d3_proc_141122/synthetic_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_56e15adc42294ddab0e8a5292281f6d3_proc_141122/synthetic_loss_curve.png"], [], [], [], [], [], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_4e5473e240474b9c986b5834df06fd36_proc_144144/synthetic_auc_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_4e5473e240474b9c986b5834df06fd36_proc_144144/synthetic_roc_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_4e5473e240474b9c986b5834df06fd36_proc_144144/synthetic_loss_curves.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_152a8e148af24c118426c42fa77c46bc_proc_144145/synthetic_auc_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_152a8e148af24c118426c42fa77c46bc_proc_144145/synthetic_loss_curves.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7947cd38c65c4adf88784add8b609bea_proc_144143/synthetic_lr_scheduler_gamma_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7947cd38c65c4adf88784add8b609bea_proc_144143/synthetic_lr_scheduler_gamma_auc_curves.png"], [], [], [], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/sst2_auc_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/sst2_loss_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/synthetic_auc_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/yelp_polarity_loss_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/synthetic_loss_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/yelp_polarity_auc_curve_bs32_lr0.001.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/sst2_auc_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/sst2_loss_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/synthetic_auc_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/yelp_polarity_loss_curve_bs64_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/synthetic_loss_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/yelp_polarity_auc_curve_bs64_lr0.001.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/sst2_auc_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/sst2_loss_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/synthetic_auc_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/yelp_polarity_loss_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/synthetic_loss_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/yelp_polarity_auc_curve_bs32_lr0.001.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/sst2_auc_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/sst2_loss_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/synthetic_auc_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/yelp_polarity_loss_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/synthetic_loss_curve_bs32_lr0.001.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/yelp_polarity_auc_curve_bs32_lr0.001.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_f3aeb81b24de423b9d52a18cb3418593/sst2_loss_aggregated.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_f3aeb81b24de423b9d52a18cb3418593/synthetic_auc_aggregated.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_f3aeb81b24de423b9d52a18cb3418593/yelp_polarity_auc_aggregated.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_f3aeb81b24de423b9d52a18cb3418593/yelp_polarity_loss_aggregated.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_f3aeb81b24de423b9d52a18cb3418593/sst2_auc_aggregated.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_f3aeb81b24de423b9d52a18cb3418593/synthetic_loss_aggregated.png"]], "plot_analyses": [[{"analysis": "Rapid increase in training AUC from around 0.72 to approximately 0.95 by epoch 3, followed by a stable plateau through epoch 20. Validation AUC remains consistently high (~0.955) across all epochs, with only a slight margin above the training curve. This close alignment and high absolute performance indicate strong generalization and no sign of overfitting on the synthetic dataset. The early saturation suggests the model reaches capacity quickly; introducing harder examples or more diverse paraphrases could further probe its uncertainty quantification capabilities.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_56e15adc42294ddab0e8a5292281f6d3_proc_141122/synthetic_auc_curve.png"}, {"analysis": "Both training and validation losses drop steadily from about 0.70/0.65 at epoch 1 to roughly 0.26/0.29 by epoch 20. The curves remain closely matched, with validation loss slightly higher but without divergence, reinforcing the observation of good generalization. The loss reduction slows after epoch 10, indicating diminishing returns; an early stopping point around epoch 15 might optimize computation without sacrificing performance.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_56e15adc42294ddab0e8a5292281f6d3_proc_141122/synthetic_loss_curve.png"}], [], [], [], [], [], [{"analysis": "AUC curves over epochs on the synthetic dataset reveal that all \u03b21 settings converge rapidly to a high AUC (~0.95\u20130.97) by epoch 5. The \u03b21=0.5 run shows a slightly slower initial ramp-up but catches up by epoch 4. Variations in \u03b21 (0.5, 0.8, 0.9, 0.99) have negligible impact on final AUC, indicating that momentum hyperparameter \u03b21 is not a critical lever for AUC performance in this setting.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_4e5473e240474b9c986b5834df06fd36_proc_144144/synthetic_auc_curves.png"}, {"analysis": "ROC curves for final predictions on the synthetic dataset overlap nearly perfectly across all \u03b21 configurations, each achieving an AUC of 0.96. The curves maintain a high true positive rate at very low false positive rates, demonstrating robust discrimination capability that is invariant to the chosen \u03b21 value.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_4e5473e240474b9c986b5834df06fd36_proc_144144/synthetic_roc_curves.png"}, {"analysis": "Loss curves over epochs show that \u03b21=0.9 and \u03b21=0.5 begin with lower initial losses and marginally faster descent compared to \u03b21=0.8 and \u03b21=0.99. By epoch 10, however, all runs converge to a similar training loss (~0.25) and validation loss (~0.27). There is minimal gap between train and validation losses, suggesting little overfitting across settings.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_4e5473e240474b9c986b5834df06fd36_proc_144144/synthetic_loss_curves.png"}], [], [{"analysis": "Loss curves on the synthetic dataset show steady declines across all tested hyperparameter settings (dropout keep probabilities at 0.9, 0.95, and 0.99). Higher keep probability (0.99) yields the lowest training and validation losses by epoch 20, plateauing around 0.28 (train) and 0.30 (val). The gap between training and validation is minimal in all cases, indicating healthy generalization and little overfitting.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7947cd38c65c4adf88784add8b609bea_proc_144143/synthetic_lr_scheduler_gamma_loss_curves.png"}, {"analysis": "ROC AUC curves reveal that the 0.99 setting achieves strong discrimination from the very first epoch (~0.95 on both train and validation), while the 0.9 and 0.95 settings start much lower (especially 0.95 at ~0.05 in epoch 1) but rapidly improve and converge above 0.95 by epoch 5. All curves stabilize above 0.96 by epoch 20 with negligible train\u2013val divergence, confirming robustness of the method across hyperparameters.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7947cd38c65c4adf88784add8b609bea_proc_144143/synthetic_lr_scheduler_gamma_auc_curves.png"}], [], [], [], [{"analysis": "SST2 AUC shows train performance stuck around 0.45\u20130.46 across 20 epochs, while validation AUC sits consistently near 0.585. This gap suggests the model is underfitting on SST2: it fails to improve embedding discriminability on train data and validation remains only slightly above chance. Adjusting learning rate or batch size may help, but additional regularization or data augmentation could be needed to raise train AUC first.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/sst2_auc_curve_bs32_lr0.001.png"}, {"analysis": "SST2 loss curves indicate smooth, continuous decrease for both train and validation, with validation loss always slightly lower. Neither curve plateaus sharply before epoch 20, but convergence is slow and overall loss remains high (around 0.69). The combination of underfitting AUC and lingering loss suggests that simply training longer may not suffice; consider increasing learning rate or using more aggressive optimizer scheduling.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/sst2_loss_curve_bs32_lr0.001.png"}, {"analysis": "Synthetic dataset AUC jumps from near chance (0.05) to over 0.90 by epoch 4 for both train and validation, then asymptotically approaches ~0.93 (train) and ~0.95 (val). This rapid learning and small generalization gap imply the synthetic task is too easy or too homogeneous. To better stress-test PIU, the synthetic generation process should be made more complex or noisy to avoid trivial memorization.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/synthetic_auc_curve_bs32_lr0.001.png"}, {"analysis": "Yelp Polarity loss curves steadily decrease from ~0.813 to ~0.705 on train and from ~0.785 to ~0.701 on validation over 20 epochs, with nearly parallel trajectories. This steady progress without divergence indicates the current hyperparameters allow balanced learning and no visible overfitting, but final loss remains relatively high. A slight increase in learning rate or modified batch size could accelerate convergence.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/yelp_polarity_loss_curve_bs32_lr0.001.png"}, {"analysis": "Synthetic loss declines smoothly from ~0.76 to ~0.54 on train and from ~0.75 to ~0.55 on validation with minimal gap. The consistency with AUC performance confirms the model fits synthetic data quickly and generalizes nearly perfectly, again suggesting the synthetic task lacks sufficient difficulty to benchmark PIU\u2019s uncertainty estimates in realistic settings.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/synthetic_loss_curve_bs32_lr0.001.png"}, {"analysis": "Yelp Polarity AUC remains flat around 0.405\u20130.415 through epoch 18, then abruptly jumps to ~0.445 for train and ~0.58 for validation at epochs 19\u201320. Such a sudden gain likely points to a metric computation anomaly (e.g. threshold reassignment or validation shuffle) rather than genuine learning. This should be investigated by verifying AUC calculations and keeping random seeds consistent.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_55e8fb977bb9465aad39c197cfd1a278_proc_144143/yelp_polarity_auc_curve_bs32_lr0.001.png"}], [{"analysis": "Train AUC on SST2 starts around 0.45, dips at epoch 2, then jumps to ~0.53 by epoch 4 and gradually rises to ~0.55 before plateauing. Validation AUC begins high at ~0.58 but crashes to ~0.415 by epoch 3 and stays flat for the rest of training. This indicates severe overfitting or a possible issue in AUC computation on validation data after early epochs.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/sst2_auc_curve_bs32_lr0.001.png"}, {"analysis": "SST2 train loss steadily decreases from ~0.722 to ~0.689 over 20 epochs, while validation loss mildly declines from ~0.709 to ~0.702 and then stagnates. The stable val loss coupled with collapsing val AUC suggests inconsistency between loss and metric evaluation or an overfitting phenomenon signaled by divergence in performance metrics.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/sst2_loss_curve_bs32_lr0.001.png"}, {"analysis": "Synthetic dataset\u2019s train AUC leaps from 0.71 at epoch 1 to ~0.955 by epoch 3 and remains around 0.96 thereafter. Validation AUC holds at ~0.97 from start to finish, showing near-ceiling performance and minimal generalization gap. Model easily solves this synthetic task with stable behavior.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/synthetic_auc_curve_bs32_lr0.001.png"}, {"analysis": "Yelp polarity train loss gently declines from ~0.694 to ~0.683 by epoch 20 with batch size 64. Validation loss decreases slightly until epoch 8 (~0.693) then steadily rises to ~0.695, indicating mild overfitting setting in mid-training.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/yelp_polarity_loss_curve_bs64_lr0.001.png"}, {"analysis": "Synthetic dataset loss curves reveal smooth, consistent improvements: train loss drops from ~0.72 to ~0.515, and validation loss falls from ~0.707 to ~0.50 by epoch 20. No overfitting is evident, matching the high AUC scores.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/synthetic_loss_curve_bs32_lr0.001.png"}, {"analysis": "Yelp polarity train AUC increases from ~0.581 to ~0.593 by epoch 4, dips at epoch 6, then recovers to ~0.593\u20130.594. Validation AUC remains flat at 0.582 throughout, signaling minimal generalization gains and suggesting hyperparameters or model capacity aren\u2019t well-tuned for this dataset.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/yelp_polarity_auc_curve_bs64_lr0.001.png"}], [{"analysis": "Train AUC remains stuck around 0.45 while validation AUC is flat at roughly 0.585 over all epochs. The model struggles to separate positive/negative examples on SST-2 despite repeated training, and validation performance does not improve. This suggests underfitting on the training set or a mismatch in classification threshold or metric computation rather than overfitting, since neither split shows meaningful gains.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/sst2_auc_curve_bs32_lr0.001.png"}, {"analysis": "Both training and validation losses decrease monotonically from ~0.72/0.785 down to ~0.69/0.695 by epoch 20. Loss curves indicate steady optimization, but the lack of corresponding AUC gains implies that reducing cross-entropy loss is not translating into better ranking or discrimination for SST-2. Possible causes include class imbalance, suboptimal thresholding, or the model learning to minimize loss without boosting its confidence on hard examples.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/sst2_loss_curve_bs32_lr0.001.png"}, {"analysis": "Training AUC oscillates narrowly around 0.963\u20130.964 and validation AUC sits at a constant ~0.962. The tiny gap and minimal fluctuations indicate near-perfect discrimination on the synthetic dataset. This high, stable performance suggests minimal noise or difficulty in the synthetic task and low risk of overfitting despite 20 epochs.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/synthetic_auc_curve_bs32_lr0.001.png"}, {"analysis": "Yelp polarity training loss declines smoothly from ~0.740 to ~0.689, with validation loss dropping from ~0.722 to ~0.692. The parallel trends and small gap show good convergence and no obvious overfitting or underfitting in terms of loss.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/yelp_polarity_loss_curve_bs32_lr0.001.png"}, {"analysis": "On the synthetic task, training loss falls from about 0.54 to 0.41 and validation loss from 0.55 to 0.42, both in a nearly linear downward trend. The model continues to improve through all epochs, again confirming that this dataset is relatively easy and well-matched to the model capacity.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/synthetic_loss_curve_bs32_lr0.001.png"}, {"analysis": "Both train and validation AUC on Yelp polarity linger around 0.405 for the first 13 epochs, then jump suddenly to ~0.58 at epoch 15 and remain near that level. This abrupt change points to an external factor\u2014possibly a learning-rate scheduler step, threshold recalibration, or evaluation bug\u2014rather than genuine model learning. It warrants checking training logs, scheduler milestones, and AUC-threshold logic around epoch 14.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/yelp_polarity_auc_curve_bs32_lr0.001.png"}], [{"analysis": "Train AUC hovers around 0.54\u20130.55 over 20 epochs, while validation AUC remains flat at \u22480.415. This gap indicates the model is learning subtle patterns on SST-2 but not generalizing to validation. Validation performance stuck near random suggests underfitting to downstream classification boundaries or that hyperparameters (lr=0.001, bs=32) may not be optimal for improving generalization on this dataset.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/sst2_auc_curve_bs32_lr0.001.png"}, {"analysis": "Both train and validation losses steadily decrease from \u22480.735 to \u22480.69 (train) and \u22480.735 to \u22480.704 (val). The smooth decline indicates stable optimization, but the modest gap between train and val loss implies limited capacity to reduce validation error further. Lowering lr or increasing epochs yields diminishing returns, so exploring a smaller lr or alternative schedulers might help.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/sst2_loss_curve_bs32_lr0.001.png"}, {"analysis": "On the synthetic dataset, train AUC fluctuates between 0.955 and 0.96, whereas validation AUC is consistently \u22480.972. High and stable validation AUC above train AUC suggests that the synthetic setup is easy for the model or that the validation set is not challenging enough. There is no sign of overfitting, but the plateau indicates limited room for improvement\u2014possibly due to dataset simplicity.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/synthetic_auc_curve_bs32_lr0.001.png"}, {"analysis": "Yelp polarity train loss falls from \u22480.714 to \u22480.682, while validation loss decreases from \u22480.699 to \u22480.692 by epoch 12, then slowly rises to \u22480.694 by epoch 20. The U-shaped validation loss curve suggests slight overfitting beyond epoch ~12. Early stopping around epoch 12\u201314 could prevent this. The modest training\u2013validation gap indicates that regularization (dropout or weight decay) might further close the gap.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/yelp_polarity_loss_curve_bs32_lr0.001.png"}, {"analysis": "Synthetic loss curves show smooth descent for both splits: train loss from \u22480.61 to \u22480.435, val loss from \u22480.60 to \u22480.438. No overfitting or divergence is observed. The consistent decrease implies the model is well-capacitated for this synthetic task; longer training or a slightly higher lr might further reduce loss but with diminishing impact on generalization.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/synthetic_loss_curve_bs32_lr0.001.png"}, {"analysis": "Yelp polarity train AUC jumps from 0.42 to \u22480.59 by epoch 5, then plateaus around \u22480.59\u20130.595. Validation AUC quickly rises to \u22480.58 by epoch 2 and stays around \u22480.58\u20130.583. The rapid convergence indicates that most learnable signal is captured within 5 epochs. The persistent gap of \u22480.01\u20130.015 suggests limited generalization improvements; tuning the lr schedule or augmenting data might help boost validation AUC further.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/yelp_polarity_auc_curve_bs32_lr0.001.png"}], []], "vlm_feedback_summary": ["Model demonstrates high AUC and low loss with minimal gap between training and\nvalidation metrics, confirming robust performance and limited overfitting.\nHowever, performance plateaus early, suggesting the need for more challenging or\nvaried prompt perturbations to fully evaluate uncertainty quantification.", "[]", "[]", "[]", "[]", "[]", "All \u03b21 variants yield equivalent final performance, with rapid convergence in\nboth AUC and loss. Since \u03b21 tuning had limited effect, consider exploring other\nhyperparameters (e.g., learning rate schedules or batch size) or regularization\nstrategies to further improve early training dynamics. For broader validation of\nthe Perturbation-Induced Uncertainty framework, test on two additional\nHuggingFace datasets: TruthfulQA (for open-ended hallucination detection in\nnatural language QA) and MBPP (Mostly Basic Python Problems) for code generation\nuncertainty evaluation.", "[]", "Higher keep probability improves loss and early ROC AUC but all variants\nconverge to similar final performance. The model generalizes well with minimal\noverfitting. For faster uncertainty detection, lower keep probabilities (0.9)\nstill achieve competitive AUC within 3\u20135 epochs.", "[]", "[]", "[]", "Overall, experiments reveal underfitting on real sentiment tasks (SST2, Yelp)\nand trivial performance on the synthetic dataset. Loss curves decrease but AUC\non SST2 and Yelp stays near chance until a suspicious late jump. I recommend: 1)\ntuning learning rate schedules (e.g. warm restarts or cosine decay), 2)\nexploring data augmentations or paraphrase diversity to challenge the synthetic\ntask, and 3) investigating metric calculation on Yelp. For broader evaluation,\nincorporate HuggingFace\u2019s \u201cimdb\u201d and \u201cag_news\u201d datasets to test generalization\nacross different text classification scenarios.", "SST2 exhibits drastic validation AUC collapse despite stable validation loss,\npointing to either a metric evaluation bug or severe overfitting with current\nlr=0.001 and batch size 32. Synthetic data is too easy, yielding near-perfect\nand stable metrics, allowing reduction in training budget. Yelp sentiment shows\nmild overfitting in loss and stagnant validation AUC with batch size 64,\nindicating that lr may be too high or batch size too large for meaningful\ngeneralization. Overall: lower learning rate for SST2 and Yelp, introduce\nregularization or early stopping, adjust batch sizes, and verify metric\ncomputations before further tuning.", "SST-2 shows decreasing loss but stagnant AUC, indicating underfitting or\nthreshold issues. Synthetic data yields high, stable AUC and smoothly decreasing\nloss, suggesting low task difficulty. Yelp polarity loss converges well but its\nAUC exhibits an unexplained sudden jump mid-training, calling for debugging of\nschedulers or evaluation scripts. Recommendations: raise learning rate (e.g. to\n2e-3) for SST-2 and Yelp, experiment with larger batch sizes (e.g. 64) and a\nwarmup+cosine LR scheduler, apply early stopping on the synthetic task around\nepoch 5\u201310, and ensure AUC thresholds remain consistent. Additional datasets to\ntest generalization: \u201cdbpedia_14\u201d (14-class topic classification) and \u201cag_news\u201d\n(4-class news categorization) from HuggingFace.", "Across SST-2 and Yelp Polarity, training is stable but validation improvements\nplateau early; minor overfitting appears on Yelp beyond ~12 epochs. Synthetic\ndataset results show strong performance but little challenge. Suggest tuning\nlearning rate schedule, employing early stopping or regularization, and testing\non additional, more diverse datasets to better assess generalization.", "[]"], "exec_time": [2.6422998905181885, 1.871427297592163, 1.8631720542907715, 1.8547234535217285, 7.509050130844116, 1.4160058498382568, 5.116165637969971, 6.109659433364868, 4.556183338165283, 2.2470569610595703, 1.9794325828552246, 1.980597972869873, 38.09962868690491, 37.13753390312195, 36.99584913253784, 36.674092531204224, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['synthetic']"], [], [], [], [], [], ["[\"synthetic\"]"], [], ["[synthetic]"], [], [], [], ["['Synthetic']"], ["['synthetic']"], ["['synthetic']"], ["['synthetic']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    key = \"synthetic\"\n    losses = data[key][\"losses\"]\n    metrics = data[key][\"metrics\"]\n    epochs = list(range(1, len(losses[\"train\"]) + 1))\n    print(\n        f\"Final Train Loss: {losses['train'][-1]:.4f}, Final Val Loss: {losses['val'][-1]:.4f}\"\n    )\n    print(\n        f\"Final Train AUC: {metrics['train'][-1]:.4f}, Final Val AUC: {metrics['val'][-1]:.4f}\"\n    )\n\n    try:\n        plt.figure()\n        plt.plot(epochs, losses[\"train\"], label=\"Train Loss\")\n        plt.plot(epochs, losses[\"val\"], label=\"Val Loss\")\n        plt.title(\"Loss Curve\\nTraining vs Validation Loss on synthetic dataset\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    try:\n        plt.figure()\n        plt.plot(epochs, metrics[\"train\"], label=\"Train AUC\")\n        plt.plot(epochs, metrics[\"val\"], label=\"Val AUC\")\n        plt.title(\"AUC Curve\\nTraining vs Validation AUC on synthetic dataset\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"AUC\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_auc_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating AUC curve: {e}\")\n        plt.close()\n", null, null, null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\ndata = exp.get(\"adam_beta1\", {}).get(\"synthetic\", {})\nbetas = data.get(\"beta1_list\", [])\n\n# Plot AUC curves\ntry:\n    plt.figure()\n    for i, beta in enumerate(betas):\n        train_auc = data[\"metrics\"][\"train\"][i]\n        val_auc = data[\"metrics\"][\"val\"][i]\n        epochs = range(1, len(train_auc) + 1)\n        plt.plot(epochs, train_auc, label=f\"Train AUC \u03b21={beta}\")\n        plt.plot(epochs, val_auc, \"--\", label=f\"Val AUC \u03b21={beta}\")\n    plt.title(\"AUC Curves over Epochs\\nDataset: synthetic\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"AUC\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_auc_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating AUC curves plot: {e}\")\n    plt.close()\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for i, beta in enumerate(betas):\n        train_loss = data[\"losses\"][\"train\"][i]\n        val_loss = data[\"losses\"][\"val\"][i]\n        epochs = range(1, len(train_loss) + 1)\n        plt.plot(epochs, train_loss, label=f\"Train Loss \u03b21={beta}\")\n        plt.plot(epochs, val_loss, \"--\", label=f\"Val Loss \u03b21={beta}\")\n    plt.title(\"Loss Curves over Epochs\\nDataset: synthetic\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Plot ROC curves\ntry:\n    from sklearn.metrics import roc_curve, auc\n\n    plt.figure()\n    for i, beta in enumerate(betas):\n        preds = data[\"predictions\"][i]\n        labels = data[\"ground_truth\"][i]\n        fpr, tpr, _ = roc_curve(labels, preds)\n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, label=f\"\u03b21={beta} (AUC={roc_auc:.2f})\")\n    plt.plot([0, 1], [0, 1], ls=\"--\", color=\"gray\")\n    plt.title(\"ROC Curves for Final Predictions\\nDataset: synthetic\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_roc_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating ROC curves plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    sd = data[\"adam_beta2\"][\"synthetic\"]\n    beta2_values = sd[\"beta2_values\"]\n    losses_tr = sd[\"losses\"][\"train\"]\n    losses_val = sd[\"losses\"][\"val\"]\n    auc_tr = sd[\"metrics\"][\"train\"]\n    auc_val = sd[\"metrics\"][\"val\"]\n\n    # Print final validation AUC per beta2\n    print(\"Final validation AUC per beta2:\")\n    for b, aucs in zip(beta2_values, auc_val):\n        print(f\"beta2={b}: {aucs[-1]:.4f}\")\n\n    # Loss curves\n    try:\n        fig = plt.figure(figsize=(10, 5))\n        fig.suptitle(\n            \"Loss Curves on synthetic dataset\\nLeft: Training Loss, Right: Validation Loss\"\n        )\n        ax1 = fig.add_subplot(1, 2, 1)\n        for b, l in zip(beta2_values, losses_tr):\n            ax1.plot(range(1, len(l) + 1), l, label=f\"beta2={b}\")\n        ax1.set_xlabel(\"Epoch\")\n        ax1.set_ylabel(\"Loss\")\n        ax1.legend()\n        ax2 = fig.add_subplot(1, 2, 2)\n        for b, l in zip(beta2_values, losses_val):\n            ax2.plot(range(1, len(l) + 1), l, label=f\"beta2={b}\")\n        ax2.set_xlabel(\"Epoch\")\n        ax2.set_ylabel(\"Loss\")\n        ax2.legend()\n        plt.tight_layout(rect=[0, 0, 1, 0.95])\n        plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # AUC curves\n    try:\n        fig = plt.figure(figsize=(10, 5))\n        fig.suptitle(\n            \"AUC Curves on synthetic dataset\\nLeft: Training AUC, Right: Validation AUC\"\n        )\n        ax1 = fig.add_subplot(1, 2, 1)\n        for b, a in zip(beta2_values, auc_tr):\n            ax1.plot(range(1, len(a) + 1), a, label=f\"beta2={b}\")\n        ax1.set_xlabel(\"Epoch\")\n        ax1.set_ylabel(\"AUC\")\n        ax1.legend()\n        ax2 = fig.add_subplot(1, 2, 2)\n        for b, a in zip(beta2_values, auc_val):\n            ax2.plot(range(1, len(a) + 1), a, label=f\"beta2={b}\")\n        ax2.set_xlabel(\"Epoch\")\n        ax2.set_ylabel(\"AUC\")\n        ax2.legend()\n        plt.tight_layout(rect=[0, 0, 1, 0.95])\n        plt.savefig(os.path.join(working_dir, \"synthetic_auc_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating AUC plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = {}\nsynthetic = data.get(\"lr_scheduler_gamma\", {}).get(\"synthetic\", {})\nif not synthetic:\n    print(\"No synthetic experiment data found.\")\nelse:\n    gammas = sorted(synthetic.keys(), key=lambda x: float(x))\n    for g in gammas:\n        vals = synthetic[g][\"metrics\"][\"val\"]\n        if vals:\n            print(f\"Gamma {g}: Best Val AUC = {max(vals):.4f}\")\n\n    try:\n        plt.figure()\n        for g in gammas:\n            ep = range(1, len(synthetic[g][\"losses\"][\"train\"]) + 1)\n            plt.plot(ep, synthetic[g][\"losses\"][\"train\"], label=f\"{g} train\")\n            plt.plot(ep, synthetic[g][\"losses\"][\"val\"], label=f\"{g} val\")\n        plt.title(\"Loss Curves on Synthetic dataset\\nTrain vs Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(\n            os.path.join(working_dir, \"synthetic_lr_scheduler_gamma_loss_curves.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    try:\n        plt.figure()\n        for g in gammas:\n            ep = range(1, len(synthetic[g][\"metrics\"][\"train\"]) + 1)\n            plt.plot(ep, synthetic[g][\"metrics\"][\"train\"], label=f\"{g} train\")\n            plt.plot(ep, synthetic[g][\"metrics\"][\"val\"], label=f\"{g} val\")\n        plt.title(\"ROC AUC Curves on Synthetic dataset\\nTrain vs Validation\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"ROC AUC\")\n        plt.legend()\n        plt.savefig(\n            os.path.join(working_dir, \"synthetic_lr_scheduler_gamma_auc_curves.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating AUC plot: {e}\")\n        plt.close()\n", null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    for dataset, exp in experiment_data.items():\n        # find best hyperparams by max validation AUC\n        best_entry = max(exp[\"metrics\"][\"val\"], key=lambda x: x[\"auc\"])\n        bs, lr = best_entry[\"bs\"], best_entry[\"lr\"]\n        print(f\"{dataset}: Best Val AUC = {best_entry['auc']:.4f} at bs={bs}, lr={lr}\")\n        # collect losses\n        loss_train = [\n            (d[\"epoch\"], d[\"loss\"])\n            for d in exp[\"losses\"][\"train\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        loss_val = [\n            (d[\"epoch\"], d[\"loss\"])\n            for d in exp[\"losses\"][\"val\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        loss_train.sort()\n        loss_val.sort()\n        epochs = [e for e, _ in loss_train]\n        tr_loss = [l for _, l in loss_train]\n        vl_loss = [l for _, l in loss_val]\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, vl_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dataset} Loss Curve (Train vs Validation)\\nbs={bs}, lr={lr}\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset}_loss_curve_bs{bs}_lr{lr}.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {dataset}: {e}\")\n            plt.close()\n        # collect AUCs\n        auc_train = [\n            (d[\"epoch\"], d[\"auc\"])\n            for d in exp[\"metrics\"][\"train\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        auc_val = [\n            (d[\"epoch\"], d[\"auc\"])\n            for d in exp[\"metrics\"][\"val\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        auc_train.sort()\n        auc_val.sort()\n        epochs = [e for e, _ in auc_train]\n        tr_auc = [a for _, a in auc_train]\n        vl_auc = [a for _, a in auc_val]\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_auc, label=\"Train AUC\")\n            plt.plot(epochs, vl_auc, label=\"Val AUC\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"AUC\")\n            plt.title(f\"{dataset} AUC Curve (Train vs Validation)\\nbs={bs}, lr={lr}\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset}_auc_curve_bs{bs}_lr{lr}.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating AUC plot for {dataset}: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    for dataset, exp in experiment_data.items():\n        # find best hyperparams by max validation AUC\n        best_entry = max(exp[\"metrics\"][\"val\"], key=lambda x: x[\"auc\"])\n        bs, lr = best_entry[\"bs\"], best_entry[\"lr\"]\n        print(f\"{dataset}: Best Val AUC = {best_entry['auc']:.4f} at bs={bs}, lr={lr}\")\n        # collect losses\n        loss_train = [\n            (d[\"epoch\"], d[\"loss\"])\n            for d in exp[\"losses\"][\"train\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        loss_val = [\n            (d[\"epoch\"], d[\"loss\"])\n            for d in exp[\"losses\"][\"val\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        loss_train.sort()\n        loss_val.sort()\n        epochs = [e for e, _ in loss_train]\n        tr_loss = [l for _, l in loss_train]\n        vl_loss = [l for _, l in loss_val]\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, vl_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dataset} Loss Curve (Train vs Validation)\\nbs={bs}, lr={lr}\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset}_loss_curve_bs{bs}_lr{lr}.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {dataset}: {e}\")\n            plt.close()\n        # collect AUCs\n        auc_train = [\n            (d[\"epoch\"], d[\"auc\"])\n            for d in exp[\"metrics\"][\"train\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        auc_val = [\n            (d[\"epoch\"], d[\"auc\"])\n            for d in exp[\"metrics\"][\"val\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        auc_train.sort()\n        auc_val.sort()\n        epochs = [e for e, _ in auc_train]\n        tr_auc = [a for _, a in auc_train]\n        vl_auc = [a for _, a in auc_val]\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_auc, label=\"Train AUC\")\n            plt.plot(epochs, vl_auc, label=\"Val AUC\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"AUC\")\n            plt.title(f\"{dataset} AUC Curve (Train vs Validation)\\nbs={bs}, lr={lr}\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset}_auc_curve_bs{bs}_lr{lr}.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating AUC plot for {dataset}: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    for dataset, exp in experiment_data.items():\n        # find best hyperparams by max validation AUC\n        best_entry = max(exp[\"metrics\"][\"val\"], key=lambda x: x[\"auc\"])\n        bs, lr = best_entry[\"bs\"], best_entry[\"lr\"]\n        print(f\"{dataset}: Best Val AUC = {best_entry['auc']:.4f} at bs={bs}, lr={lr}\")\n        # collect losses\n        loss_train = [\n            (d[\"epoch\"], d[\"loss\"])\n            for d in exp[\"losses\"][\"train\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        loss_val = [\n            (d[\"epoch\"], d[\"loss\"])\n            for d in exp[\"losses\"][\"val\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        loss_train.sort()\n        loss_val.sort()\n        epochs = [e for e, _ in loss_train]\n        tr_loss = [l for _, l in loss_train]\n        vl_loss = [l for _, l in loss_val]\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, vl_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dataset} Loss Curve (Train vs Validation)\\nbs={bs}, lr={lr}\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset}_loss_curve_bs{bs}_lr{lr}.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {dataset}: {e}\")\n            plt.close()\n        # collect AUCs\n        auc_train = [\n            (d[\"epoch\"], d[\"auc\"])\n            for d in exp[\"metrics\"][\"train\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        auc_val = [\n            (d[\"epoch\"], d[\"auc\"])\n            for d in exp[\"metrics\"][\"val\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        auc_train.sort()\n        auc_val.sort()\n        epochs = [e for e, _ in auc_train]\n        tr_auc = [a for _, a in auc_train]\n        vl_auc = [a for _, a in auc_val]\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_auc, label=\"Train AUC\")\n            plt.plot(epochs, vl_auc, label=\"Val AUC\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"AUC\")\n            plt.title(f\"{dataset} AUC Curve (Train vs Validation)\\nbs={bs}, lr={lr}\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset}_auc_curve_bs{bs}_lr{lr}.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating AUC plot for {dataset}: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    for dataset, exp in experiment_data.items():\n        # find best hyperparams by max validation AUC\n        best_entry = max(exp[\"metrics\"][\"val\"], key=lambda x: x[\"auc\"])\n        bs, lr = best_entry[\"bs\"], best_entry[\"lr\"]\n        print(f\"{dataset}: Best Val AUC = {best_entry['auc']:.4f} at bs={bs}, lr={lr}\")\n        # collect losses\n        loss_train = [\n            (d[\"epoch\"], d[\"loss\"])\n            for d in exp[\"losses\"][\"train\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        loss_val = [\n            (d[\"epoch\"], d[\"loss\"])\n            for d in exp[\"losses\"][\"val\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        loss_train.sort()\n        loss_val.sort()\n        epochs = [e for e, _ in loss_train]\n        tr_loss = [l for _, l in loss_train]\n        vl_loss = [l for _, l in loss_val]\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, vl_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dataset} Loss Curve (Train vs Validation)\\nbs={bs}, lr={lr}\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset}_loss_curve_bs{bs}_lr{lr}.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {dataset}: {e}\")\n            plt.close()\n        # collect AUCs\n        auc_train = [\n            (d[\"epoch\"], d[\"auc\"])\n            for d in exp[\"metrics\"][\"train\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        auc_val = [\n            (d[\"epoch\"], d[\"auc\"])\n            for d in exp[\"metrics\"][\"val\"]\n            if d[\"bs\"] == bs and d[\"lr\"] == lr\n        ]\n        auc_train.sort()\n        auc_val.sort()\n        epochs = [e for e, _ in auc_train]\n        tr_auc = [a for _, a in auc_train]\n        vl_auc = [a for _, a in auc_val]\n        try:\n            plt.figure()\n            plt.plot(epochs, tr_auc, label=\"Train AUC\")\n            plt.plot(epochs, vl_auc, label=\"Val AUC\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"AUC\")\n            plt.title(f\"{dataset} AUC Curve (Train vs Validation)\\nbs={bs}, lr={lr}\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{dataset}_auc_curve_bs{bs}_lr{lr}.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating AUC plot for {dataset}: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nexperiment_data_path_list = [\n    \"experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_cde2566319a94c8ab6b12f9bf4bf3893_proc_144144/experiment_data.npy\",\n    \"experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_de20a07534e64095bd1e496b85c4cd9a_proc_144145/experiment_data.npy\",\n    \"experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_f1bbd7a781534559822427943d2c515a_proc_144143/experiment_data.npy\",\n]\n\nall_experiment_data = []\ntry:\n    for rel_path in experiment_data_path_list:\n        path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\"), rel_path)\n        exp = np.load(path, allow_pickle=True).item()\n        all_experiment_data.append(exp)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nelse:\n    # Gather all dataset names\n    datasets = set()\n    for exp in all_experiment_data:\n        datasets.update(exp.keys())\n    for dataset in datasets:\n        # collect runs\n        train_loss_runs, val_loss_runs = [], []\n        train_auc_runs, val_auc_runs = [], []\n        epochs_loss, epochs_auc = None, None\n        for exp in all_experiment_data:\n            if dataset not in exp:\n                continue\n            data = exp[dataset]\n            best = max(data[\"metrics\"][\"val\"], key=lambda d: d[\"auc\"])\n            bs, lr = best[\"bs\"], best[\"lr\"]\n            # extract and sort curves\n            tl = sorted(\n                (d[\"epoch\"], d[\"loss\"])\n                for d in data[\"losses\"][\"train\"]\n                if d[\"bs\"] == bs and d[\"lr\"] == lr\n            )\n            vl = sorted(\n                (d[\"epoch\"], d[\"loss\"])\n                for d in data[\"losses\"][\"val\"]\n                if d[\"bs\"] == bs and d[\"lr\"] == lr\n            )\n            ta = sorted(\n                (d[\"epoch\"], d[\"auc\"])\n                for d in data[\"metrics\"][\"train\"]\n                if d[\"bs\"] == bs and d[\"lr\"] == lr\n            )\n            va = sorted(\n                (d[\"epoch\"], d[\"auc\"])\n                for d in data[\"metrics\"][\"val\"]\n                if d[\"bs\"] == bs and d[\"lr\"] == lr\n            )\n            el, sl = zip(*tl)\n            _, vsl = zip(*vl)\n            ea, sa = zip(*ta)\n            _, vsa = zip(*va)\n            if epochs_loss is None:\n                epochs_loss = el\n            if epochs_auc is None:\n                epochs_auc = ea\n            train_loss_runs.append(sl)\n            val_loss_runs.append(vsl)\n            train_auc_runs.append(sa)\n            val_auc_runs.append(vsa)\n        # aggregated loss plot\n        if train_loss_runs:\n            arr_tl = np.array(train_loss_runs)\n            arr_vl = np.array(val_loss_runs)\n            mean_tl = arr_tl.mean(axis=0)\n            sem_tl = arr_tl.std(axis=0, ddof=1) / np.sqrt(arr_tl.shape[0])\n            mean_vl = arr_vl.mean(axis=0)\n            sem_vl = arr_vl.std(axis=0, ddof=1) / np.sqrt(arr_vl.shape[0])\n            try:\n                plt.figure()\n                plt.errorbar(epochs_loss, mean_tl, yerr=sem_tl, label=\"Train Loss\")\n                plt.errorbar(epochs_loss, mean_vl, yerr=sem_vl, label=\"Val Loss\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Loss\")\n                plt.title(\n                    f\"{dataset} Loss Curve (Aggregated Mean \u00b1 SEM)\\nLeft: Train, Right: Validation\"\n                )\n                plt.legend()\n                plt.savefig(os.path.join(working_dir, f\"{dataset}_loss_aggregated.png\"))\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating aggregated loss plot for {dataset}: {e}\")\n                plt.close()\n        # aggregated AUC plot\n        if train_auc_runs:\n            arr_ta = np.array(train_auc_runs)\n            arr_va = np.array(val_auc_runs)\n            mean_ta = arr_ta.mean(axis=0)\n            sem_ta = arr_ta.std(axis=0, ddof=1) / np.sqrt(arr_ta.shape[0])\n            mean_va = arr_va.mean(axis=0)\n            sem_va = arr_va.std(axis=0, ddof=1) / np.sqrt(arr_va.shape[0])\n            try:\n                plt.figure()\n                plt.errorbar(epochs_auc, mean_ta, yerr=sem_ta, label=\"Train AUC\")\n                plt.errorbar(epochs_auc, mean_va, yerr=sem_va, label=\"Val AUC\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"AUC\")\n                plt.title(\n                    f\"{dataset} AUC Curve (Aggregated Mean \u00b1 SEM)\\nLeft: Train, Right: Validation\"\n                )\n                plt.legend()\n                plt.savefig(os.path.join(working_dir, f\"{dataset}_auc_aggregated.png\"))\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating aggregated AUC plot for {dataset}: {e}\")\n                plt.close()\n"], "plot_plan": ["We load the saved experiment data and extract the synthetic dataset\u2019s losses and AUCs over epochs. After determining the number of epochs, we print out the final train/validation loss and AUC for quick reference. We then create two separate plots\u2014loss curves and AUC curves\u2014each wrapped in its own try-except block. Each plot uses a two-line title to clearly specify \u201cTraining vs Validation\u201d on the synthetic dataset, labels axes, and includes a legend. We save each figure under a descriptive filename in the working directory, and close the figure in all cases to free resources.", null, null, null, null, null, "We load the saved experiment data from the working directory and extract the synthetic sweep results under \u201cadam_beta1.\u201d We then create three separate plots in independent try\u2010except blocks: (1) training and validation AUC over epochs for each \u03b21 value, (2) training and validation loss over epochs for each \u03b21, and (3) ROC curves computed from the final validation predictions for each \u03b21. In each block we open a new figure, plot the relevant lines, add a title with a clear subtitle mentioning \u201cDataset: synthetic\u201d and the metric type, label axes, add a legend, and save the figure to working_dir with a descriptive filename. We always close the figure whether or not plotting succeeds. We ensure that no more than one figure per metric is produced and we only plot the data actually stored in experiment_data.npy. For the ROC curves we compute FPR and TPR via sklearn\u2019s roc_curve and annotate each curve with its AUC. Finally, we report any errors encountered during plotting.", "We load the experiment data from the working directory and extract the synthetic results under the \u2018adam_beta2\u2019 key.  \nWe first plot the loss curves in a two\u2010panel figure: the left panel shows training loss per epoch for each beta2 value, and the right shows validation loss.  \nEach curve is labeled by its beta2, with a shared suptitle clarifying \u201cLeft: Training Loss, Right: Validation Loss\u201d on the synthetic dataset.  \nWe wrap this in a try-except block to catch errors, print any message, save to working/synthetic_loss_curves.png, and always close the figure.  \nNext, we repeat for AUC curves, plotting training AUC (left) and validation AUC (right) per beta2, adding legends and subtitles similarly.  \nThis is also in its own try-except, saving to working/synthetic_auc_curves.png and closing the figure.  \nBefore plotting, we print out the final validation AUC for each beta2 to surface key evaluation metrics.  \nThis satisfies the guidelines for separate plots, descriptive titles/subtitles, closing figures, and clear naming.", "First, load the saved experiment data from the working directory. Extract the nested 'synthetic' results under the 'lr_scheduler_gamma' key. Iterate over each gamma value and compute the best validation AUC, printing it for quick reference. Build a sorted list of gamma strings to ensure consistent plotting order. In the first try-except block, plot the combined training and validation loss curves for all gamma values at each epoch. Label axes appropriately, set a clear title specifying the synthetic dataset and the Train vs Validation comparison, and save this figure to working_dir. In a separate try-except block, plot the corresponding training and validation ROC AUC curves with similar annotations. Use descriptive filenames that include the dataset name, plot type, and hyperparameter context. Always close figures after saving and handle any errors gracefully.", null, null, null, "We will load the saved experiment data and for each dataset pick the hyperparameter combination that gave the highest validation AUC.  Then for that best (bs, lr) we extract the per\u2010epoch train and val losses and AUCs, and produce two separate line plots: one for loss curves and one for AUC curves across epochs.  Each plot is wrapped in its own try\u2010except to catch any errors and ensure the figure is closed.  Titles include the dataset name and the best hyperparameters, and the x/y labels and legends clarify train vs validation.  Plots are saved in the working directory under descriptive filenames.  Finally, we print out the best validation AUC and its hyperparameters for each dataset.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "learning_rate", "batch_size", "weight_decay", null, null, "adam_beta1", "adam_beta2", "lr_scheduler_gamma", null, "adam_eps", "num_epochs", null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script below loads the saved `experiment_data.npy` file from the `working`\ndirectory, iterates over each dataset inside, and prints the dataset name\nfollowed by the final training and validation AUC as well as the final training\nand validation losses, all with clear, descriptive labels.", "", "", "", "", "", "The following script loads the saved experiment data from the `working`\ndirectory and iterates over each dataset and beta1 configuration to print the\nfinal epoch metrics. It clearly labels the dataset name and each metric\u2014train\naccuracy, validation accuracy, train loss, and validation loss\u2014and runs\nimmediately at global scope without generating any plots.", "Below is a script that locates the `experiment_data.npy` file in the `working`\ndirectory, loads the stored experiment data, and iterates over each experiment\nand dataset.  For each dataset it prints the dataset name, then for each \u03b2\u2082\nvalue it extracts and prints the final (last\u2010epoch) training and validation AUC\nwith explicit metric labels.", "The script loads the saved experiment data from the working directory and\naccesses the \u201csynthetic\u201d dataset under the \u201clr_scheduler_gamma\u201d key. It then\niterates over each gamma value, pulls out the final epoch\u2019s training and\nvalidation AUC from the stored metrics lists, and prints them with clear labels.\nBy printing \u201cDataset: synthetic\u201d first and using \u201cFinal training AUC\u201d and \u201cFinal\nvalidation AUC\u201d labels, the output remains explicit and self-contained.", "", "", "", "I will load the saved `experiment_data.npy` from the working directory, then\nloop over each dataset\u2019s records to pull out training and validation AUCs and\nlosses. For each dataset, I compute the highest training and validation AUC and\nthe lowest training and validation loss. The script prints the dataset name\nfollowed by clearly labeled metrics in a single pass, and it executes\nimmediately without any `if __name__ == \"__main__\":` guard.", "I will load the saved `experiment_data.npy` from the working directory, then\nloop over each dataset\u2019s records to pull out training and validation AUCs and\nlosses. For each dataset, I compute the highest training and validation AUC and\nthe lowest training and validation loss. The script prints the dataset name\nfollowed by clearly labeled metrics in a single pass, and it executes\nimmediately without any `if __name__ == \"__main__\":` guard.", "I will load the saved `experiment_data.npy` from the working directory, then\nloop over each dataset\u2019s records to pull out training and validation AUCs and\nlosses. For each dataset, I compute the highest training and validation AUC and\nthe lowest training and validation loss. The script prints the dataset name\nfollowed by clearly labeled metrics in a single pass, and it executes\nimmediately without any `if __name__ == \"__main__\":` guard.", "I will load the saved `experiment_data.npy` from the working directory, then\nloop over each dataset\u2019s records to pull out training and validation AUCs and\nlosses. For each dataset, I compute the highest training and validation AUC and\nthe lowest training and validation loss. The script prints the dataset name\nfollowed by clearly labeled metrics in a single pass, and it executes\nimmediately without any `if __name__ == \"__main__\":` guard.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Locate the working directory and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(experiment_path, allow_pickle=True).item()\n\n# Iterate over each dataset and print final metrics\nfor dataset_name, data in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract final metrics\n    final_train_auc = data[\"metrics\"][\"train\"][-1]\n    final_validation_auc = data[\"metrics\"][\"val\"][-1]\n    final_train_loss = data[\"losses\"][\"train\"][-1]\n    final_validation_loss = data[\"losses\"][\"val\"][-1]\n\n    # Print metrics with descriptive labels\n    print(f\"Final training AUC: {final_train_auc:.4f}\")\n    print(f\"Final validation AUC: {final_validation_auc:.4f}\")\n    print(f\"Final training loss: {final_train_loss:.4f}\")\n    print(f\"Final validation loss: {final_validation_loss:.4f}\")\n    print()  # Blank line between datasets\n", "", "", "", "", "", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over each dataset under the 'adam_beta1' sweep\nfor dataset_name, ds in experiment_data.get(\"adam_beta1\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    betas = ds.get(\"beta1_list\", [])\n    train_aucs = ds.get(\"metrics\", {}).get(\"train\", [])\n    val_aucs = ds.get(\"metrics\", {}).get(\"val\", [])\n    train_losses = ds.get(\"losses\", {}).get(\"train\", [])\n    val_losses = ds.get(\"losses\", {}).get(\"val\", [])\n\n    # Print final metrics for each beta1 value\n    for beta, t_auc, v_auc, t_loss, v_loss in zip(\n        betas, train_aucs, val_aucs, train_losses, val_losses\n    ):\n        print(f\"  Hyperparameter beta1={beta}\")\n        print(f\"    train accuracy: {t_auc[-1]:.4f}\")\n        print(f\"    validation accuracy: {v_auc[-1]:.4f}\")\n        print(f\"    train loss: {t_loss[-1]:.4f}\")\n        print(f\"    validation loss: {v_loss[-1]:.4f}\")\n", "import os\nimport numpy as np\n\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through experiments and datasets to print final metrics\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, results in datasets.items():\n        # Header for this dataset\n        print(f\"{dataset_name.capitalize()} dataset\")\n\n        beta2_values = results[\"beta2_values\"]\n        train_metrics = results[\"metrics\"][\"train\"]\n        val_metrics = results[\"metrics\"][\"val\"]\n\n        # Print final train and validation AUC for each beta2\n        for beta2, train_list, val_list in zip(\n            beta2_values, train_metrics, val_metrics\n        ):\n            final_train_auc = train_list[-1]\n            final_val_auc = val_list[-1]\n            print(\n                f\"beta2={beta2:.4f}  \"\n                f\"train AUC: {final_train_auc:.4f}  \"\n                f\"validation AUC: {final_val_auc:.4f}\"\n            )\n        print()\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Access the synthetic dataset under lr_scheduler_gamma\nsynthetic_data = experiment_data[\"lr_scheduler_gamma\"][\"synthetic\"]\n\n# Print final metrics for each hyperparameter setting\nprint(\"Dataset: synthetic\")\nfor gamma, exp in synthetic_data.items():\n    final_train_auc = exp[\"metrics\"][\"train\"][-1]\n    final_validation_auc = exp[\"metrics\"][\"val\"][-1]\n    print(f\"Hyperparameter gamma = {gamma}\")\n    print(f\"  Final training AUC: {final_train_auc:.4f}\")\n    print(f\"  Final validation AUC: {final_validation_auc:.4f}\")\n", "", "", "", "import os\nimport numpy as np\n\n# Define working directory and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through datasets and compute best/final metrics\nfor dataset_name, records in experiment_data.items():\n    # Extract lists of metric and loss dictionaries\n    train_metrics = records[\"metrics\"][\"train\"]\n    val_metrics = records[\"metrics\"][\"val\"]\n    train_losses = records[\"losses\"][\"train\"]\n    val_losses = records[\"losses\"][\"val\"]\n\n    # Compute best AUCs and lowest losses\n    best_train_auc = max(item[\"auc\"] for item in train_metrics)\n    best_val_auc = max(item[\"auc\"] for item in val_metrics)\n    lowest_train_loss = min(item[\"loss\"] for item in train_losses)\n    lowest_val_loss = min(item[\"loss\"] for item in val_losses)\n\n    # Print results\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"train accuracy: {best_train_auc:.4f}\")\n    print(f\"validation accuracy: {best_val_auc:.4f}\")\n    print(f\"train loss: {lowest_train_loss:.4f}\")\n    print(f\"validation loss: {lowest_val_loss:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Define working directory and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through datasets and compute best/final metrics\nfor dataset_name, records in experiment_data.items():\n    # Extract lists of metric and loss dictionaries\n    train_metrics = records[\"metrics\"][\"train\"]\n    val_metrics = records[\"metrics\"][\"val\"]\n    train_losses = records[\"losses\"][\"train\"]\n    val_losses = records[\"losses\"][\"val\"]\n\n    # Compute best AUCs and lowest losses\n    best_train_auc = max(item[\"auc\"] for item in train_metrics)\n    best_val_auc = max(item[\"auc\"] for item in val_metrics)\n    lowest_train_loss = min(item[\"loss\"] for item in train_losses)\n    lowest_val_loss = min(item[\"loss\"] for item in val_losses)\n\n    # Print results\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"train accuracy: {best_train_auc:.4f}\")\n    print(f\"validation accuracy: {best_val_auc:.4f}\")\n    print(f\"train loss: {lowest_train_loss:.4f}\")\n    print(f\"validation loss: {lowest_val_loss:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Define working directory and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through datasets and compute best/final metrics\nfor dataset_name, records in experiment_data.items():\n    # Extract lists of metric and loss dictionaries\n    train_metrics = records[\"metrics\"][\"train\"]\n    val_metrics = records[\"metrics\"][\"val\"]\n    train_losses = records[\"losses\"][\"train\"]\n    val_losses = records[\"losses\"][\"val\"]\n\n    # Compute best AUCs and lowest losses\n    best_train_auc = max(item[\"auc\"] for item in train_metrics)\n    best_val_auc = max(item[\"auc\"] for item in val_metrics)\n    lowest_train_loss = min(item[\"loss\"] for item in train_losses)\n    lowest_val_loss = min(item[\"loss\"] for item in val_losses)\n\n    # Print results\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"train accuracy: {best_train_auc:.4f}\")\n    print(f\"validation accuracy: {best_val_auc:.4f}\")\n    print(f\"train loss: {lowest_train_loss:.4f}\")\n    print(f\"validation loss: {lowest_val_loss:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Define working directory and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through datasets and compute best/final metrics\nfor dataset_name, records in experiment_data.items():\n    # Extract lists of metric and loss dictionaries\n    train_metrics = records[\"metrics\"][\"train\"]\n    val_metrics = records[\"metrics\"][\"val\"]\n    train_losses = records[\"losses\"][\"train\"]\n    val_losses = records[\"losses\"][\"val\"]\n\n    # Compute best AUCs and lowest losses\n    best_train_auc = max(item[\"auc\"] for item in train_metrics)\n    best_val_auc = max(item[\"auc\"] for item in val_metrics)\n    lowest_train_loss = min(item[\"loss\"] for item in train_losses)\n    lowest_val_loss = min(item[\"loss\"] for item in val_losses)\n\n    # Print results\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"train accuracy: {best_train_auc:.4f}\")\n    print(f\"validation accuracy: {best_val_auc:.4f}\")\n    print(f\"train loss: {lowest_train_loss:.4f}\")\n    print(f\"validation loss: {lowest_val_loss:.4f}\\n\")\n", ""], "parse_term_out": ["['Dataset: synthetic', '\\n', 'Final training AUC: 0.9505', '\\n', 'Final\nvalidation AUC: 0.9572', '\\n', 'Final training loss: 0.2669', '\\n', 'Final\nvalidation loss: 0.2861', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "", "", "", "", "", "['Dataset: synthetic', '\\n', '  Hyperparameter beta1=0.5', '\\n', '    train\naccuracy: 0.9508', '\\n', '    validation accuracy: 0.9572', '\\n', '    train\nloss: 0.2683', '\\n', '    validation loss: 0.2881', '\\n', '  Hyperparameter\nbeta1=0.8', '\\n', '    train accuracy: 0.9522', '\\n', '    validation accuracy:\n0.9572', '\\n', '    train loss: 0.3025', '\\n', '    validation loss: 0.3211',\n'\\n', '  Hyperparameter beta1=0.9', '\\n', '    train accuracy: 0.9491', '\\n', '\nvalidation accuracy: 0.9572', '\\n', '    train loss: 0.2593', '\\n', '\nvalidation loss: 0.2782', '\\n', '  Hyperparameter beta1=0.99', '\\n', '    train\naccuracy: 0.9500', '\\n', '    validation accuracy: 0.9572', '\\n', '    train\nloss: 0.2732', '\\n', '    validation loss: 0.2917', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['Synthetic dataset', '\\n', 'beta2=0.9000  train AUC: 0.9499  validation AUC:\n0.9572', '\\n', 'beta2=0.9500  train AUC: 0.9523  validation AUC: 0.9572', '\\n',\n'beta2=0.9900  train AUC: 0.9489  validation AUC: 0.9572', '\\n', 'beta2=0.9990\ntrain AUC: 0.9500  validation AUC: 0.9572', '\\n', 'beta2=0.9999  train AUC:\n0.9503  validation AUC: 0.9572', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Hyperparameter gamma = 0.9', '\\n', '  Final\ntraining AUC: 0.9510', '\\n', '  Final validation AUC: 0.9572', '\\n',\n'Hyperparameter gamma = 0.95', '\\n', '  Final training AUC: 0.9523', '\\n', '\nFinal validation AUC: 0.9572', '\\n', 'Hyperparameter gamma = 0.99', '\\n', '\nFinal training AUC: 0.9492', '\\n', '  Final validation AUC: 0.9572', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "", "", "", "['Dataset: synthetic', '\\n', 'train accuracy: 0.9522', '\\n', 'validation\naccuracy: 0.9572', '\\n', 'train loss: 0.2641', '\\n', 'validation loss:\n0.2876\\n', '\\n', 'Dataset: sst2', '\\n', 'train accuracy: 0.5558', '\\n',\n'validation accuracy: 0.5840', '\\n', 'train loss: 0.6815', '\\n', 'validation\nloss: 0.6891\\n', '\\n', 'Dataset: yelp_polarity', '\\n', 'train accuracy: 0.5930',\n'\\n', 'validation accuracy: 0.5828', '\\n', 'train loss: 0.6800', '\\n',\n'validation loss: 0.6924\\n', '\\n', 'Execution time: a moment seconds (time limit\nis an hour).']", "['Dataset: synthetic', '\\n', 'train accuracy: 0.9643', '\\n', 'validation\naccuracy: 0.9718', '\\n', 'train loss: 0.2360', '\\n', 'validation loss:\n0.2255\\n', '\\n', 'Dataset: sst2', '\\n', 'train accuracy: 0.5528', '\\n',\n'validation accuracy: 0.5840', '\\n', 'train loss: 0.6825', '\\n', 'validation\nloss: 0.6976\\n', '\\n', 'Dataset: yelp_polarity', '\\n', 'train accuracy: 0.5939',\n'\\n', 'validation accuracy: 0.5828', '\\n', 'train loss: 0.6797', '\\n',\n'validation loss: 0.6915\\n', '\\n', 'Execution time: a moment seconds (time limit\nis an hour).']", "['Dataset: synthetic', '\\n', 'train accuracy: 0.9660', '\\n', 'validation\naccuracy: 0.9624', '\\n', 'train loss: 0.2492', '\\n', 'validation loss:\n0.2761\\n', '\\n', 'Dataset: sst2', '\\n', 'train accuracy: 0.5501', '\\n',\n'validation accuracy: 0.5840', '\\n', 'train loss: 0.6830', '\\n', 'validation\nloss: 0.6959\\n', '\\n', 'Dataset: yelp_polarity', '\\n', 'train accuracy: 0.5933',\n'\\n', 'validation accuracy: 0.5828', '\\n', 'train loss: 0.6797', '\\n',\n'validation loss: 0.6920\\n', '\\n', 'Execution time: a moment seconds (time limit\nis an hour).']", "['Dataset: synthetic', '\\n', 'train accuracy: 0.9598', '\\n', 'validation\naccuracy: 0.9717', '\\n', 'train loss: 0.2391', '\\n', 'validation loss:\n0.1856\\n', '\\n', 'Dataset: sst2', '\\n', 'train accuracy: 0.5495', '\\n',\n'validation accuracy: 0.4160', '\\n', 'train loss: 0.6824', '\\n', 'validation\nloss: 0.6931\\n', '\\n', 'Dataset: yelp_polarity', '\\n', 'train accuracy: 0.5968',\n'\\n', 'validation accuracy: 0.5828', '\\n', 'train loss: 0.6790', '\\n',\n'validation loss: 0.6927\\n', '\\n', 'Execution time: a moment seconds (time limit\nis an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>

{"edges": [[2, 6], [6, 12], [10, 15], [14, 19], [14, 18], [14, 20], [14, 21]], "layout": [[0.0, 0.0], [0.07142857142857142, 0.0], [0.14285714285714285, 0.0], [0.21428571428571427, 0.0], [0.2857142857142857, 0.0], [0.35714285714285715, 0.0], [0.14285714285714285, 0.5], [0.42857142857142855, 0.0], [0.5, 0.0], [0.5714285714285714, 0.0], [0.6428571428571429, 0.0], [0.7142857142857143, 0.0], [0.14285714285714285, 1.0], [0.7857142857142857, 0.0], [0.8571428571428571, 0.0], [0.6428571428571429, 0.5], [0.9285714285714286, 0.0], [1.0, 0.0], [0.75, 0.5], [0.8214285714285714, 0.5], [0.8928571428571429, 0.5], [0.9642857142857143, 0.5]], "plan": ["I fixed the training loop so that `optimizer.zero_grad()` happens before\n`loss.backward()`, ensuring the gradients are cleared before each backward pass\nrather than after, which was preventing any weight updates. This change allows\nthe model\u2019s parameters to actually be updated during `optimizer.step()`, leading\nto meaningful training and decreasing validation loss over epochs. I also made\nsure all tensors and models are consistently moved to the configured device\n(`cuda`/`cpu`).", "Ablation name: Head-Only Fine-Tuning. Below is a single\u2010file Python script\nimplementing the head\u2010only fine\u2010tuning ablation: all BERT encoder parameters are\nfrozen and only the final classification head is trained. It records per\u2010epoch\ntrain/validation losses, vote\u2010 and KL\u2010based detection metrics, final\npredictions, and ground truth in a nested dict under `\"head_only\"`, and saves\neverything to `working/experiment_data.npy`.", "Ablation name: Partial-Layer Fine-Tuning. We iterate over three ablation\nsettings (no freeze, freeze bottom 4, freeze bottom 8 layers), freezing the\nspecified layers before optimizer setup. For each dataset we train exactly as in\nthe baseline, compute and store train/val losses, detection AUCs and DES, final\npredictions, and ground truth. All results are collected in a nested\n`experiment_data` dict under each ablation and dataset, then saved at the end as\n`experiment_data.npy`.", "Ablation name: No_Pretraining_RandomInit. The code below instantiates BERT with\nrandom weights (via a fresh BertConfig) to ablate pre\u2010training, and then\nfine\u2010tunes on SST2, Yelp, and IMDB with identical settings. It generates\nparaphrase variants, computes train/val losses, uncertainty\u2010based detection\nmetrics, and final predictions, and organizes everything under the\n\u201cNo_Pretraining_RandomInit\u201d key before saving as a single `experiment_data.npy`\nfile.", "Ablation name: PositionalEmbeddingAblation. Here\u2019s a self-contained script that\nruns the positional-embedding ablation across SST-2, Yelp and IMDb, zeroes and\nfreezes BERT\u2019s positional embeddings, fine-tunes, collects losses,\nclassification accuracies, paraphrase-based detection metrics, predictions and\nground truths, and saves everything as a single `experiment_data.npy`.", "Ablation name: No_Dropout_Ablation. We disable all dropout by zeroing out every\nnn.Dropout module\u2019s p attribute immediately after model loading, and label this\nexperiment under `\"No_Dropout_Ablation\"`.  The rest of the script mirrors the\nbaseline: loading and sampling GLUE/SST2, Yelp, and IMDB datasets, generating K\nparaphrases per validation example, training for a fixed number of epochs,\nevaluating detection uncertainty via vote disagreement and symmetric KL across\nvariants, and tracking AUC/D ES metrics each epoch.  We collect train/validation\nlosses, detection metrics, final predictions, and ground\u2010truth labels for each\ndataset, then save everything into `experiment_data.npy` in the working\ndirectory.", "We pre-generate all (original\u2009+\u2009K paraphrases) for the validation set once and\nbatch-tokenize them, so that we can compute model logits in large GPU-friendly\nbatches rather than per variant per example. We keep track of group and variant\nindices to scatter the probabilities into an array of shape (N,\u2009K+1,\u2009C), then\nvectorize the computation of voting uncertainty, symmetric KL divergence, and\nSpearman rank correlation with binary error labels. Metrics including validation\nloss, ROC AUC, DES, and Spearman correlation are updated and printed each epoch\nand saved in `experiment_data`. Moving both model and all tensors to `device`\nensures GPU utilization and faster execution to complete within time limits.", "Ablation name: Training-with-Paraphrase-Augmentation. We run two\nconditions\u2014baseline and paraphrase\u2010augmented training\u2014across each dataset. For\nparaphrase augmentation, we generate K paraphrases per training example and\ninclude them in the fine\u2010tuning data, while for both, we still generate\nparaphrases of validation examples to compute vote\u2010entropy and KL\u2010based\nuncertainty metrics. We log per\u2010epoch train/val losses and detection metrics,\ncollect final predictions and ground truth, and store everything in a nested\nexperiment_data dictionary keyed first by ablation type and then dataset name.\nFinally, we save this dictionary as a NumPy file for downstream analysis.", "Ablation name: Multi-Head Attention Pruning. Below is an outline of the solution\nfollowed by the full implementation.  We introduce a `prune_heads` routine that\nrandomly zeroes 50% of the attention heads per layer via a HuggingFace\n`head_mask` and also attaches gradient\u2010masking hooks on the corresponding\nquery/key/value and output\u2010projection weight slices so pruned heads remain\nfrozen.  We then run two experiments (\u201cfull_heads\u201d vs. \u201cpruned_heads\u201d) on SST-2,\nYelp-Polarity, and IMDb: each trains a BERT classifier for 5 epochs, records\ntrain/val losses and classification accuracies, computes vote\u2010 and KL\u2010based\ndetection metrics over fixed WordNet paraphrase variants, and finally saves all\nplottable data (losses, metrics, predictions, ground truth) in a single\n`experiment_data.npy` file.", "Ablation name: FeedForwardAblation. We iterate over two settings\u2014baseline and\nFFN\u2010ablated\u2014by zeroing and freezing each layer\u2019s intermediate and output dense\nweights for the ablation. For each dataset we fine\u2010tune the model, record\nper\u2010epoch training/validation losses and paraphrase\u2010based uncertainty detection\nmetrics, and collect final predictions and ground truth. All results are\norganized in a nested dict `experiment_data` keyed by ablation type and dataset,\nthen saved as `experiment_data.npy`.", "Ablation name: ResidualConnectionAblation. We subclass and replace the\nself\u2010attention and feed\u2010forward output modules of each Transformer layer to\nstrip out the \u201c+ input\u201d residual addition, thereby ablating identity shortcuts.\nAfter loading BERT, we patch every layer\u2019s `attention.output` and `output` with\nthe no\u2010residual versions, then fine\u2010tune and evaluate on SST-2, Yelp, and IMDB\nexactly as before. We record train/val losses, detection AUC/DES, predictions,\nand ground truth under the key `\"ResidualConnectionAblation\"`, and finally save\neverything to `experiment_data.npy`.", "Ablation name: TokenTypeEmbeddingAblation. We disable BERT\u2019s segment embeddings\nby zeroing out `token_type_embeddings` and always passing zero `token_type_ids`.\nWe keep the rest of the training and detection loops unchanged, computing vote\u2010\nand KL\u2010based AUC and DES metrics over paraphrase variants. The per\u2010dataset\nresults are stored under a single `\"token_type_embedding_ablation\"` key in\n`experiment_data` and saved via `np.save(\"experiment_data.npy\", ...)` at the\nend.", "We precompute paraphrase generation and tokenization just once per dataset and\nreuse those DataLoaders across all ablation settings, and we reduce the\nhyperparameter scale (train_size=1000, val_size=200, K=3, epochs=3) to\ndramatically cut computing time. We also set DataLoader with multiple workers\nand pin_memory for faster CPU\u2192GPU transfers. The training and detection loops\nremain intact, still logging validation loss, AUC, DES, and Spearman at each\nepoch, and we save the complete `experiment_data` array at the end.", "Ablation name: LayerNormAblation. We will monkey\u2010patch the pretrained BERT\nencoder by replacing each transformer block\u2019s two LayerNorm layers with a no\u2010op\nidentity module, then train and evaluate on the same datasets with voting and KL\nuncertainty metrics. All epoch\u2010level train/validation losses, detection metrics,\nfinal predictions, and ground truths are stored under a single top\u2010level key\n\"LayerNormAblation\" in our `experiment_data` dict. Finally, we save everything\nas `experiment_data.npy`.", "Ablation name: RandomInitEmbeddingAblation. I will create two experimental\nconditions\u2014baseline and RandomInitEmbeddingAblation\u2014by looping over these\nsettings and, for the ablation, reinitializing the BERT word embeddings to\nGaussian noise. For each dataset, I\u2019ll train and evaluate the model, record\ntraining/validation losses, paraphrase\u2010based detection metrics, final\npredictions, and ground truths in a nested dict. Finally, I\u2019ll save the combined\n`experiment_data` as `experiment_data.npy` in a working directory.", "We can resolve the CPU/GPU mismatch by patching the custom NoRes modules while\nthe model is still on CPU, then moving the entire model to the chosen device so\nthat all parameters (including the new modules) are transferred together.\nAdditionally, we import and compute Spearman rank correlation between our\nperturbation\u2010induced uncertainty scores and the true error labels at each epoch,\nlogging these alongside AUC and DES metrics. The `experiment_data` dictionary is\nreorganized to map each dataset name to its own losses, per\u2010epoch validation\nmetrics (including Spearman), predictions, and ground truth, which we then save\nat the end. We also print validation loss and detection metrics each epoch to\ntrack progress.", "Ablation name: BiasRemovalAblation. I will initialize all bias terms in every\n`nn.Linear` layer to zero and set `requires_grad=False` before creating the\noptimizer, ensuring they never update. The rest of training, validation, and\ndetection metrics remain unchanged, but I will wrap results in a top\u2010level\n`BiasRemovalAblation` key per dataset. Finally, I save losses, validation\ndetection metrics, predictions, and ground truth arrays in\n`experiment_data.npy`.", "Ablation name: TransformerDepthAblation. We extend the baseline with two\nsettings: full_depth uses the original 12\u2010layer BERT, while reduced_depth builds\na BERT with 6 encoder layers and copies the pretrained weights for embeddings,\npooler, and the first 6 layers. All other training and paraphrase\u2010based\nuncertainty detection loops remain unchanged across datasets. We collect\ntrain/validation losses, detection metrics, predictions, and ground truth in a\nnested `experiment_data` keyed by ablation and dataset, and finally save\neverything to `experiment_data.npy`.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\nimport random\nimport re\nimport nltk\nimport torch\nimport numpy as np\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# Setup working dir and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Prepare WordNet\nnltk.download(\"wordnet\", quiet=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paraphrases = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paraphrases.append(\" \".join(new))\n    return paraphrases\n\n\n# Datasets config\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\nexperiment_data = {}\nK, epochs, bs, lr = 3, 3, 16, 2e-5\n\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # load splits\n    if sub:\n        train = load_dataset(ds, sub, split=\"train\").shuffle(42).select(range(1000))\n        val = load_dataset(ds, sub, split=\"validation\").shuffle(42).select(range(200))\n    else:\n        train = load_dataset(ds, split=\"train\").shuffle(42).select(range(1000))\n        val = load_dataset(ds, split=\"test\").shuffle(42).select(range(200))\n\n    val_texts = val[text_col]\n    val_labels = val[label_col]\n    para = {i: generate_paraphrases(t, K) for i, t in enumerate(val_texts)}\n\n    # tokenize datasets\n    tr_enc = tokenizer(train[text_col], truncation=True, padding=True)\n    va_enc = tokenizer(val_texts, truncation=True, padding=True)\n    train_ds = TensorDataset(\n        torch.tensor(tr_enc[\"input_ids\"]),\n        torch.tensor(tr_enc[\"attention_mask\"]),\n        torch.tensor(train[label_col]),\n    )\n    val_ds = TensorDataset(\n        torch.tensor(va_enc[\"input_ids\"]),\n        torch.tensor(va_enc[\"attention_mask\"]),\n        torch.tensor(val_labels),\n    )\n    tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=bs)\n\n    # model & optimizer\n    model = DistilBertForSequenceClassification.from_pretrained(\n        \"distilbert-base-uncased\", num_labels=2\n    ).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n\n    experiment_data[name] = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"detection\": []},\n        \"predictions\": [],\n        \"ground_truth\": val_labels,\n    }\n\n    # training + detection\n    for epoch in range(1, epochs + 1):\n        model.train()\n        t_losses = []\n        for ids, mask, labels in tr_loader:\n            ids, mask, labels = (\n                ids.to(device),\n                mask.to(device),\n                labels.to(device).long(),\n            )\n            optimizer.zero_grad()  # Clear grads before backward\n            out = model(input_ids=ids, attention_mask=mask, labels=labels)\n            loss = out.loss\n            loss.backward()\n            optimizer.step()\n            t_losses.append(loss.item())\n        experiment_data[name][\"losses\"][\"train\"].append(\n            {\"epoch\": epoch, \"loss\": float(np.mean(t_losses))}\n        )\n\n        model.eval()\n        v_losses = []\n        with torch.no_grad():\n            for ids, mask, labels in va_loader:\n                ids, mask, labels = (\n                    ids.to(device),\n                    mask.to(device),\n                    labels.to(device).long(),\n                )\n                out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                v_losses.append(out.loss.item())\n        val_loss = float(np.mean(v_losses))\n        experiment_data[name][\"losses\"][\"val\"].append(\n            {\"epoch\": epoch, \"loss\": val_loss}\n        )\n        print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n        # detection via PIU\n        uncs, errs = [], []\n        for i, (t, gt) in enumerate(zip(val_texts, val_labels)):\n            preds = []\n            for txt in [t] + para[i]:\n                enc = tokenizer(\n                    txt, return_tensors=\"pt\", truncation=True, padding=True\n                ).to(device)\n                with torch.no_grad():\n                    logits = model(**enc).logits\n                preds.append(int(torch.argmax(logits, -1).item()))\n            maj = max(set(preds), key=preds.count)\n            uncs.append(1 - preds.count(maj) / len(preds))\n            errs.append(int(preds[0] != int(gt)))\n        try:\n            auc = roc_auc_score(errs, uncs)\n        except:\n            auc = 0.5\n        des = auc / (K + 1)\n        experiment_data[name][\"metrics\"][\"detection\"].append(\n            {\"epoch\": epoch, \"auc\": auc, \"DES\": des}\n        )\n        print(f\"Epoch {epoch}: detection_auc = {auc:.4f}, DES = {des:.4f}\")\n\n    # save final preds & labels\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for ids, mask, _ in va_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            logits = model(input_ids=ids, attention_mask=mask).logits\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n    experiment_data[name][\"predictions\"] = preds\n\n# persist data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport random, re\nimport nltk\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnltk.download(\"wordnet\", quiet=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\n# hyperparams\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\n\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\nexperiment_data = {\"head_only\": {}}\n\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # load slices\n    ds_train = (\n        load_dataset(ds, sub, split=\"train\") if sub else load_dataset(ds, split=\"train\")\n    )\n    ds_val = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n    ds_train = ds_train.shuffle(42).select(range(train_size))\n    ds_val = ds_val.shuffle(42).select(range(val_size))\n    texts_train, labels_train = ds_train[text_col], ds_train[label_col]\n    texts_val, labels_val = ds_val[text_col], ds_val[label_col]\n    paras = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_val)}\n\n    # tokenize\n    tr_enc = tokenizer(texts_train, truncation=True, padding=True, return_tensors=\"pt\")\n    va_enc = tokenizer(texts_val, truncation=True, padding=True, return_tensors=\"pt\")\n    train_ds = TensorDataset(\n        tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n    )\n    val_ds = TensorDataset(\n        va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_val)\n    )\n    tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=bs)\n\n    # model init + freeze BERT encoder\n    model = BertForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\", num_labels=2\n    ).to(device)\n    for param in model.bert.parameters():\n        param.requires_grad = False\n    optimizer = Adam(model.classifier.parameters(), lr=lr)\n\n    # prepare storage\n    experiment_data[\"head_only\"][name] = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"detection\": []},\n        \"predictions\": [],\n        \"ground_truth\": labels_val,\n    }\n\n    # training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        train_losses = []\n        for ids, mask, labels in tr_loader:\n            ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n            optimizer.zero_grad()\n            out = model(input_ids=ids, attention_mask=mask, labels=labels)\n            out.loss.backward()\n            optimizer.step()\n            train_losses.append(out.loss.item())\n        experiment_data[\"head_only\"][name][\"losses\"][\"train\"].append(\n            {\"epoch\": epoch, \"loss\": float(np.mean(train_losses))}\n        )\n\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for ids, mask, labels in va_loader:\n                ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n                out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                val_losses.append(out.loss.item())\n        val_loss = float(np.mean(val_losses))\n        experiment_data[\"head_only\"][name][\"losses\"][\"val\"].append(\n            {\"epoch\": epoch, \"loss\": val_loss}\n        )\n        print(f\"{name} Epoch {epoch}: val_loss={val_loss:.4f}\")\n\n        # detection metrics\n        uncs_vote, uncs_kl, errs = [], [], []\n        for i, txt in enumerate(texts_val):\n            probs, preds = [], []\n            for variant in [txt] + paras[i]:\n                enc = tokenizer(\n                    variant, return_tensors=\"pt\", truncation=True, padding=True\n                ).to(device)\n                with torch.no_grad():\n                    logits = model(**enc).logits\n                p = torch.softmax(logits, dim=-1).squeeze(0).cpu()\n                probs.append(p)\n                preds.append(int(p.argmax().item()))\n            maj = max(set(preds), key=preds.count)\n            uncs_vote.append(1 - preds.count(maj) / len(preds))\n            kl_vals = []\n            for a in range(len(probs)):\n                for b in range(a + 1, len(probs)):\n                    P, Q = probs[a], probs[b]\n                    kl1 = F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                    kl2 = F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                    kl_vals.append(0.5 * (kl1 + kl2))\n            uncs_kl.append(float(np.mean(kl_vals)))\n            errs.append(int(preds[0] != int(labels_val[i])))\n        try:\n            auc_v = roc_auc_score(errs, uncs_vote)\n        except:\n            auc_v = 0.5\n        try:\n            auc_k = roc_auc_score(errs, uncs_kl)\n        except:\n            auc_k = 0.5\n        des_v = auc_v / (K + 1)\n        des_k = auc_k / (K + 1)\n        experiment_data[\"head_only\"][name][\"metrics\"][\"detection\"].append(\n            {\n                \"epoch\": epoch,\n                \"auc_vote\": auc_v,\n                \"DES_vote\": des_v,\n                \"auc_kl\": auc_k,\n                \"DES_kl\": des_k,\n            }\n        )\n        print(\n            f\"  AUC_vote={auc_v:.4f}, DES_vote={des_v:.4f}, AUC_kl={auc_k:.4f}, DES_kl={des_k:.4f}\"\n        )\n\n    # final predictions\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for ids, mask, _ in va_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            logits = model(input_ids=ids, attention_mask=mask).logits\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n    experiment_data[\"head_only\"][name][\"predictions\"] = preds\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, re, nltk, torch, numpy as np\nimport torch.nn.functional as F\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnltk.download(\"wordnet\", quiet=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\n# datasets config\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\n# hyperparams\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\n\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n# ablation settings: freeze bottom N layers\nablations = {\"full_ft\": 0, \"freeze_4\": 4, \"freeze_8\": 8}\n\nexperiment_data = {}\n\nfor abl_name, freeze_layers in ablations.items():\n    experiment_data[abl_name] = {}\n    for name, (ds, sub, text_col, label_col) in datasets_info.items():\n        # init storage\n        dataset_data = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"detection\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        experiment_data[abl_name][name] = dataset_data\n\n        # load and sample\n        train_split = (\n            load_dataset(ds, sub, split=\"train\")\n            if sub\n            else load_dataset(ds, split=\"train\")\n        )\n        val_split = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n        train_split = train_split.shuffle(42).select(range(train_size))\n        val_split = val_split.shuffle(42).select(range(val_size))\n        texts_train, labels_train = train_split[text_col], train_split[label_col]\n        texts_val, labels_val = val_split[text_col], val_split[label_col]\n        paras = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_val)}\n\n        # store ground truth\n        dataset_data[\"ground_truth\"] = list(labels_val)\n\n        # tokenize and loaders\n        tr_enc = tokenizer(\n            texts_train, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n        va_enc = tokenizer(\n            texts_val, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n        train_ds = TensorDataset(\n            tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n        )\n        val_ds = TensorDataset(\n            va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_val)\n        )\n        tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n        va_loader = DataLoader(val_ds, batch_size=bs)\n\n        # model & freeze layers\n        model = BertForSequenceClassification.from_pretrained(\n            \"bert-base-uncased\", num_labels=2\n        )\n        for i in range(freeze_layers):\n            for p in model.bert.encoder.layer[i].parameters():\n                p.requires_grad = False\n        model.to(device)\n        optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n        # training loop\n        for epoch in range(1, epochs + 1):\n            model.train()\n            train_losses = []\n            for ids, mask, labels in tr_loader:\n                ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n                optimizer.zero_grad()\n                out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                out.loss.backward()\n                optimizer.step()\n                train_losses.append(out.loss.item())\n            dataset_data[\"losses\"][\"train\"].append(\n                {\"epoch\": epoch, \"loss\": float(np.mean(train_losses))}\n            )\n\n            model.eval()\n            val_losses = []\n            with torch.no_grad():\n                for ids, mask, labels in va_loader:\n                    ids, mask, labels = (\n                        ids.to(device),\n                        mask.to(device),\n                        labels.to(device),\n                    )\n                    out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                    val_losses.append(out.loss.item())\n            vl = float(np.mean(val_losses))\n            dataset_data[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": vl})\n            print(f\"{abl_name}/{name} Epoch {epoch}: val_loss={vl:.4f}\")\n\n            # detection metrics\n            uncs_vote, uncs_kl, errs = [], [], []\n            for i, txt in enumerate(texts_val):\n                probs, preds = [], []\n                for variant in [txt] + paras[i]:\n                    enc = tokenizer(\n                        variant, return_tensors=\"pt\", truncation=True, padding=True\n                    ).to(device)\n                    with torch.no_grad():\n                        logits = model(**enc).logits\n                    p = torch.softmax(logits, dim=-1).squeeze(0).cpu()\n                    probs.append(p)\n                    preds.append(int(p.argmax().item()))\n                maj = max(set(preds), key=preds.count)\n                uncs_vote.append(1 - preds.count(maj) / len(preds))\n                kl_vals = []\n                for a in range(len(probs)):\n                    for b in range(a + 1, len(probs)):\n                        P, Q = probs[a], probs[b]\n                        kl1 = F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                        kl2 = F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                        kl_vals.append(0.5 * (kl1 + kl2))\n                uncs_kl.append(float(np.mean(kl_vals)))\n                errs.append(int(preds[0] != int(labels_val[i])))\n            try:\n                auc_v = roc_auc_score(errs, uncs_vote)\n            except:\n                auc_v = 0.5\n            try:\n                auc_k = roc_auc_score(errs, uncs_kl)\n            except:\n                auc_k = 0.5\n            des_v = auc_v / (K + 1)\n            des_k = auc_k / (K + 1)\n            dataset_data[\"metrics\"][\"detection\"].append(\n                {\n                    \"epoch\": epoch,\n                    \"auc_vote\": auc_v,\n                    \"DES_vote\": des_v,\n                    \"auc_kl\": auc_k,\n                    \"DES_kl\": des_k,\n                }\n            )\n            print(\n                f\"{abl_name}/{name} Epoch {epoch}: AUC_vote={auc_v:.4f}, DES_vote={des_v:.4f},\"\n                f\" AUC_kl={auc_k:.4f}, DES_kl={des_k:.4f}\"\n            )\n\n        # final predictions\n        preds = []\n        model.eval()\n        with torch.no_grad():\n            for ids, mask, _ in va_loader:\n                ids, mask = ids.to(device), mask.to(device)\n                logits = model(input_ids=ids, attention_mask=mask).logits\n                preds.extend(torch.argmax(logits, -1).cpu().tolist())\n        dataset_data[\"predictions\"] = preds\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, re\nimport nltk\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification, BertConfig\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnltk.download(\"wordnet\", quiet=True)\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# ablation\nABL = \"No_Pretraining_RandomInit\"\nexperiment_data = {ABL: {}}\n\n\n# paraphrase utility\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\n# settings\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # load data\n    ds_tr = (\n        load_dataset(ds, sub, split=\"train\") if sub else load_dataset(ds, split=\"train\")\n    )\n    ds_va = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n    ds_tr = ds_tr.shuffle(42).select(range(train_size))\n    ds_va = ds_va.shuffle(42).select(range(val_size))\n    texts_tr, labels_tr = ds_tr[text_col], ds_tr[label_col]\n    texts_va, labels_va = ds_va[text_col], ds_va[label_col]\n    # paraphrases\n    paras = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_va)}\n\n    # tokenize\n    tr_enc = tokenizer(texts_tr, truncation=True, padding=True, return_tensors=\"pt\")\n    va_enc = tokenizer(texts_va, truncation=True, padding=True, return_tensors=\"pt\")\n    train_ds = TensorDataset(\n        tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_tr)\n    )\n    val_ds = TensorDataset(\n        va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_va)\n    )\n    tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=bs)\n\n    # init random\u2010weight BERT\n    config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=2)\n    model = BertForSequenceClassification(config).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n\n    # prepare storage\n    experiment_data[ABL][name] = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"detection\": []},\n        \"predictions\": [],\n        \"ground_truth\": labels_va,\n    }\n\n    # training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        tr_ls = []\n        for ids, mask, labs in tr_loader:\n            ids, mask, labs = ids.to(device), mask.to(device), labs.to(device)\n            optimizer.zero_grad()\n            out = model(input_ids=ids, attention_mask=mask, labels=labs)\n            out.loss.backward()\n            optimizer.step()\n            tr_ls.append(out.loss.item())\n        experiment_data[ABL][name][\"losses\"][\"train\"].append(\n            {\"epoch\": epoch, \"loss\": float(np.mean(tr_ls))}\n        )\n\n        model.eval()\n        va_ls = []\n        with torch.no_grad():\n            for ids, mask, labs in va_loader:\n                ids, mask, labs = ids.to(device), mask.to(device), labs.to(device)\n                out = model(input_ids=ids, attention_mask=mask, labels=labs)\n                va_ls.append(out.loss.item())\n        experiment_data[ABL][name][\"losses\"][\"val\"].append(\n            {\"epoch\": epoch, \"loss\": float(np.mean(va_ls))}\n        )\n        print(f\"{name} Epoch {epoch}: val_loss={np.mean(va_ls):.4f}\")\n\n        # detection metrics\n        uncs_vote, uncs_kl, errs = [], [], []\n        for i, txt in enumerate(texts_va):\n            probs, preds = [], []\n            for var in [txt] + paras[i]:\n                enc = tokenizer(\n                    var, return_tensors=\"pt\", truncation=True, padding=True\n                ).to(device)\n                with torch.no_grad():\n                    logits = model(**enc).logits\n                p = torch.softmax(logits, -1).squeeze(0).cpu()\n                probs.append(p)\n                preds.append(int(p.argmax().item()))\n            maj = max(set(preds), key=preds.count)\n            uncs_vote.append(1 - preds.count(maj) / len(preds))\n            klv = []\n            for a in range(len(probs)):\n                for b in range(a + 1, len(probs)):\n                    P, Q = probs[a], probs[b]\n                    klv.append(\n                        0.5\n                        * (\n                            F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                            + F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                        )\n                    )\n            uncs_kl.append(float(np.mean(klv)))\n            errs.append(int(preds[0] != int(labels_va[i])))\n        try:\n            auc_v = roc_auc_score(errs, uncs_vote)\n            auc_k = roc_auc_score(errs, uncs_kl)\n        except:\n            auc_v = auc_k = 0.5\n        des_v = auc_v / (K + 1)\n        des_k = auc_k / (K + 1)\n        experiment_data[ABL][name][\"metrics\"][\"detection\"].append(\n            {\n                \"epoch\": epoch,\n                \"auc_vote\": auc_v,\n                \"DES_vote\": des_v,\n                \"auc_kl\": auc_k,\n                \"DES_kl\": des_k,\n            }\n        )\n        print(\n            f\"{name} Epoch {epoch}: AUC_v={auc_v:.3f}, DES_v={des_v:.3f}, AUC_k={auc_k:.3f}, DES_k={des_k:.3f}\"\n        )\n\n    # final preds\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for ids, mask, _ in va_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            logits = model(input_ids=ids, attention_mask=mask).logits\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n    experiment_data[ABL][name][\"predictions\"] = preds\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, re\nimport nltk\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnltk.download(\"wordnet\", quiet=True)\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\n# hyperparameters\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\n\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\nexperiment_data = {\"positional_embedding_ablation\": {}}\n\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # load and sample\n    ds_train = (\n        load_dataset(ds, sub, split=\"train\") if sub else load_dataset(ds, split=\"train\")\n    )\n    ds_val = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n    ds_train = ds_train.shuffle(42).select(range(train_size))\n    ds_val = ds_val.shuffle(42).select(range(val_size))\n    texts_train, labels_train = ds_train[text_col], ds_train[label_col]\n    texts_val, labels_val = ds_val[text_col], ds_val[label_col]\n    paras = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_val)}\n\n    # tokenize & dataloaders\n    tr_enc = tokenizer(texts_train, truncation=True, padding=True, return_tensors=\"pt\")\n    va_enc = tokenizer(texts_val, truncation=True, padding=True, return_tensors=\"pt\")\n    train_ds = TensorDataset(\n        tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n    )\n    val_ds = TensorDataset(\n        va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_val)\n    )\n    tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=bs)\n\n    # model + ablation\n    model = BertForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\", num_labels=2\n    ).to(device)\n    # zero + freeze positional embeddings\n    model.bert.embeddings.position_embeddings.weight.data.zero_()\n    model.bert.embeddings.position_embeddings.weight.requires_grad = False\n    optimizer = Adam([p for p in model.parameters() if p.requires_grad], lr=lr)\n\n    # storage\n    data = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"detection\": [],\n        \"predictions\": [],\n        \"ground_truth\": labels_val,\n    }\n\n    for epoch in range(1, epochs + 1):\n        # training\n        model.train()\n        train_losses = []\n        for ids, mask, lbl in tr_loader:\n            ids, mask, lbl = ids.to(device), mask.to(device), lbl.to(device)\n            optimizer.zero_grad()\n            out = model(input_ids=ids, attention_mask=mask, labels=lbl)\n            out.loss.backward()\n            optimizer.step()\n            train_losses.append(out.loss.item())\n        train_loss = float(np.mean(train_losses))\n        data[\"losses\"][\"train\"].append(train_loss)\n\n        # validation loss\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for ids, mask, lbl in va_loader:\n                ids, mask, lbl = ids.to(device), mask.to(device), lbl.to(device)\n                out = model(input_ids=ids, attention_mask=mask, labels=lbl)\n                val_losses.append(out.loss.item())\n        val_loss = float(np.mean(val_losses))\n        data[\"losses\"][\"val\"].append(val_loss)\n\n        # classification accuracies\n        correct_tr, total_tr = 0, 0\n        with torch.no_grad():\n            for ids, mask, lbl in tr_loader:\n                ids, mask, lbl = ids.to(device), mask.to(device), lbl.to(device)\n                logits = model(input_ids=ids, attention_mask=mask).logits\n                preds = logits.argmax(-1)\n                correct_tr += (preds == lbl).sum().item()\n                total_tr += lbl.size(0)\n        tr_acc = correct_tr / total_tr\n        correct_va, total_va = 0, 0\n        with torch.no_grad():\n            for ids, mask, lbl in va_loader:\n                ids, mask, lbl = ids.to(device), mask.to(device), lbl.to(device)\n                logits = model(input_ids=ids, attention_mask=mask).logits\n                preds = logits.argmax(-1)\n                correct_va += (preds == lbl).sum().item()\n                total_va += lbl.size(0)\n        va_acc = correct_va / total_va\n        data[\"metrics\"][\"train\"].append(tr_acc)\n        data[\"metrics\"][\"val\"].append(va_acc)\n\n        # paraphrase\u2010based detection\n        uncs_vote, uncs_kl, errs = [], [], []\n        for i, txt in enumerate(texts_val):\n            probs, preds = [], []\n            for variant in [txt] + paras[i]:\n                enc = tokenizer(\n                    variant, return_tensors=\"pt\", truncation=True, padding=True\n                ).to(device)\n                with torch.no_grad():\n                    logits = model(**enc).logits\n                p = torch.softmax(logits, dim=-1).squeeze(0).cpu()\n                probs.append(p)\n                preds.append(int(p.argmax().item()))\n            maj = max(set(preds), key=preds.count)\n            uncs_vote.append(1 - preds.count(maj) / len(preds))\n            kl_vals = []\n            for a in range(len(probs)):\n                for b in range(a + 1, len(probs)):\n                    P, Q = probs[a], probs[b]\n                    kl_vals.append(\n                        0.5\n                        * (\n                            F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                            + F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                        )\n                    )\n            uncs_kl.append(float(np.mean(kl_vals)))\n            errs.append(int(preds[0] != int(labels_val[i])))\n        try:\n            auc_v = roc_auc_score(errs, uncs_vote)\n        except:\n            auc_v = 0.5\n        try:\n            auc_k = roc_auc_score(errs, uncs_kl)\n        except:\n            auc_k = 0.5\n        des_v, des_k = auc_v / (K + 1), auc_k / (K + 1)\n        data[\"detection\"].append(\n            {\n                \"epoch\": epoch,\n                \"auc_vote\": auc_v,\n                \"DES_vote\": des_v,\n                \"auc_kl\": auc_k,\n                \"DES_kl\": des_k,\n            }\n        )\n\n        print(\n            f\"{name} Epoch {epoch} | tr_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"tr_acc={tr_acc:.4f} val_acc={va_acc:.4f} \"\n            f\"AUC_vote={auc_v:.4f} DES_vote={des_v:.4f} AUC_kl={auc_k:.4f} DES_kl={des_k:.4f}\"\n        )\n\n    # final predictions\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for ids, mask, _ in va_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            logits = model(input_ids=ids, attention_mask=mask).logits\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n    data[\"predictions\"] = preds\n\n    experiment_data[\"positional_embedding_ablation\"][name] = data\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, re, nltk\nimport torch, numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# download WordNet\nnltk.download(\"wordnet\", quiet=True)\nfrom nltk.corpus import wordnet\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\n# datasets to run\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\n# hyperparameters\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\n\n# tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\n# main experiment data container\nexperiment_data = {\"No_Dropout_Ablation\": {}}\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nfor ds_name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # load and sample\n    train_split = (\n        load_dataset(ds, sub, split=\"train\") if sub else load_dataset(ds, split=\"train\")\n    )\n    val_split = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n    train_split = train_split.shuffle(42).select(range(train_size))\n    val_split = val_split.shuffle(42).select(range(val_size))\n    texts_train, labels_train = train_split[text_col], train_split[label_col]\n    texts_val, labels_val = val_split[text_col], val_split[label_col]\n    # generate paraphrases\n    paras = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_val)}\n    # tokenize\n    tr_enc = tokenizer(texts_train, truncation=True, padding=True, return_tensors=\"pt\")\n    va_enc = tokenizer(texts_val, truncation=True, padding=True, return_tensors=\"pt\")\n    train_ds = TensorDataset(\n        tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n    )\n    val_ds = TensorDataset(\n        va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_val)\n    )\n    tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=bs)\n\n    # init model and disable dropout\n    model = BertForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\", num_labels=2\n    ).to(device)\n    for m in model.modules():\n        if isinstance(m, nn.Dropout):\n            m.p = 0.0\n\n    optimizer = Adam(model.parameters(), lr=lr)\n\n    # record structures\n    expd = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"detection\": []},\n        \"predictions\": [],\n        \"ground_truth\": labels_val,\n    }\n    experiment_data[\"No_Dropout_Ablation\"][ds_name] = expd\n\n    # training + validation + detection\n    for epoch in range(1, epochs + 1):\n        model.train()\n        train_losses = []\n        for ids, mask, lbl in tr_loader:\n            ids, mask, lbl = ids.to(device), mask.to(device), lbl.to(device)\n            optimizer.zero_grad()\n            out = model(input_ids=ids, attention_mask=mask, labels=lbl)\n            out.loss.backward()\n            optimizer.step()\n            train_losses.append(out.loss.item())\n        expd[\"losses\"][\"train\"].append(\n            {\"epoch\": epoch, \"loss\": float(np.mean(train_losses))}\n        )\n\n        # validation loss\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for ids, mask, lbl in va_loader:\n                ids, mask, lbl = ids.to(device), mask.to(device), lbl.to(device)\n                out = model(input_ids=ids, attention_mask=mask, labels=lbl)\n                val_losses.append(out.loss.item())\n        vl = float(np.mean(val_losses))\n        expd[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": vl})\n        print(f\"{ds_name} Epoch {epoch}: val_loss={vl:.4f}\")\n\n        # detection metrics\n        uncs_vote, uncs_kl, errs = [], [], []\n        for i, txt in enumerate(texts_val):\n            probs, preds = [], []\n            for variant in [txt] + paras[i]:\n                enc = tokenizer(\n                    variant, truncation=True, padding=True, return_tensors=\"pt\"\n                ).to(device)\n                with torch.no_grad():\n                    logits = model(**enc).logits\n                p = torch.softmax(logits, dim=-1).squeeze(0).cpu()\n                probs.append(p)\n                preds.append(int(p.argmax().item()))\n            maj = max(set(preds), key=preds.count)\n            uncs_vote.append(1 - preds.count(maj) / len(preds))\n            kl_vals = []\n            for a in range(len(probs)):\n                for b in range(a + 1, len(probs)):\n                    P, Q = probs[a], probs[b]\n                    kl1 = F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                    kl2 = F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                    kl_vals.append(0.5 * (kl1 + kl2))\n            uncs_kl.append(float(np.mean(kl_vals)))\n            errs.append(int(preds[0] != int(labels_val[i])))\n        try:\n            auc_v = roc_auc_score(errs, uncs_vote)\n        except:\n            auc_v = 0.5\n        try:\n            auc_k = roc_auc_score(errs, uncs_kl)\n        except:\n            auc_k = 0.5\n        des_v = auc_v / (K + 1)\n        des_k = auc_k / (K + 1)\n        expd[\"metrics\"][\"detection\"].append(\n            {\n                \"epoch\": epoch,\n                \"auc_vote\": auc_v,\n                \"DES_vote\": des_v,\n                \"auc_kl\": auc_k,\n                \"DES_kl\": des_k,\n            }\n        )\n        print(\n            f\"{ds_name} Epoch {epoch}: AUC_vote={auc_v:.4f}, DES_vote={des_v:.4f}, AUC_kl={auc_k:.4f}, DES_kl={des_k:.4f}\"\n        )\n\n    # final predictions\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for ids, mask, _ in va_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            logits = model(input_ids=ids, attention_mask=mask).logits\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n    expd[\"predictions\"] = preds\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport random, re, nltk\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\nfrom scipy.stats import spearmanr\n\n# Setup\nnltk.download(\"wordnet\", quiet=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\n# Configs\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\nablations = {\"full_ft\": 0, \"freeze_4\": 4, \"freeze_8\": 8}\nexperiment_data = {}\n\nfor abl_name, freeze_layers in ablations.items():\n    experiment_data[abl_name] = {}\n    for name, (ds, sub, text_col, label_col) in datasets_info.items():\n        # prepare data\n        train_ds_raw = (\n            load_dataset(ds, sub, split=\"train\")\n            if sub\n            else load_dataset(ds, split=\"train\")\n        )\n        val_ds_raw = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n        train_ds_raw = train_ds_raw.shuffle(42).select(range(train_size))\n        val_ds_raw = val_ds_raw.shuffle(42).select(range(val_size))\n        texts_train, labels_train = train_ds_raw[text_col], train_ds_raw[label_col]\n        texts_val, labels_val = val_ds_raw[text_col], val_ds_raw[label_col]\n        N = len(texts_val)\n\n        # generate paraphrases\n        paras = [generate_paraphrases(t, K) for t in texts_val]\n        # build flattened variant list & indices\n        variants, grp_ids, var_pos = [], [], []\n        for i, t in enumerate(texts_val):\n            vs = [t] + paras[i]\n            for j, v in enumerate(vs):\n                variants.append(v)\n                grp_ids.append(i)\n                var_pos.append(j)\n        # tokenize variants once\n        va_enc = tokenizer(variants, truncation=True, padding=True, return_tensors=\"pt\")\n        grp_ids = torch.tensor(grp_ids, dtype=torch.long)\n        var_pos = torch.tensor(var_pos, dtype=torch.long)\n\n        # tokenize train\n        tr_enc = tokenizer(\n            texts_train, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n        train_ds = TensorDataset(\n            tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n        )\n        val_ds = TensorDataset(\n            va_enc[\"input_ids\"], va_enc[\"attention_mask\"], grp_ids, var_pos\n        )\n\n        tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n        va_loader = DataLoader(val_ds, batch_size=bs)\n\n        # init model\n        model = BertForSequenceClassification.from_pretrained(\n            \"bert-base-uncased\", num_labels=2\n        )\n        for i in range(freeze_layers):\n            for p in model.bert.encoder.layer[i].parameters():\n                p.requires_grad = False\n        model.to(device)\n        optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n        # storage\n        ed = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"detection\": []},\n            \"predictions\": [],\n            \"ground_truth\": labels_val,\n        }\n        experiment_data[abl_name][name] = ed\n\n        # training loop\n        for epoch in range(1, epochs + 1):\n            # train\n            model.train()\n            batch_losses = []\n            for ids, mask, labs in tr_loader:\n                ids, mask, labs = ids.to(device), mask.to(device), labs.to(device)\n                optimizer.zero_grad()\n                out = model(input_ids=ids, attention_mask=mask, labels=labs)\n                out.loss.backward()\n                optimizer.step()\n                batch_losses.append(out.loss.item())\n            train_loss = float(np.mean(batch_losses))\n            ed[\"losses\"][\"train\"].append({\"epoch\": epoch, \"loss\": train_loss})\n\n            # val loss\n            model.eval()\n            val_batch_losses = []\n            for ids, mask, grp, pos in va_loader:\n                # only need original labels for loss, so skip grp/pos\n                labs = None\n                if pos.eq(0).any():\n                    batch_idx = pos == 0\n                    labs = torch.tensor(\n                        [labels_val[i] for i in grp[batch_idx].tolist()], device=device\n                    )\n                ids, mask = ids.to(device), mask.to(device)\n                with torch.no_grad():\n                    if labs is not None:\n                        out = model(\n                            input_ids=ids[batch_idx],\n                            attention_mask=mask[batch_idx],\n                            labels=labs,\n                        )\n                        val_batch_losses.append(out.loss.item())\n            val_loss = float(np.mean(val_batch_losses)) if val_batch_losses else 0.0\n            ed[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": val_loss})\n            print(f\"{abl_name}/{name} Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n            # detection metrics\n            all_probs = np.zeros((N, K + 1, 2), dtype=float)\n            model.eval()\n            for ids, mask, grp, pos in va_loader:\n                ids, mask = ids.to(device), mask.to(device)\n                with torch.no_grad():\n                    logits = model(input_ids=ids, attention_mask=mask).logits\n                    ps = torch.softmax(logits, dim=-1).cpu().numpy()\n                gids = grp.numpy()\n                poss = pos.numpy()\n                all_probs[gids, poss, :] = ps\n            # compute preds and errs\n            preds_var = all_probs.argmax(axis=2)\n            errs = (preds_var[:, 0] != np.array(labels_val)).astype(int)\n            # vote uncertainty\n            counts = np.apply_along_axis(\n                lambda r: np.bincount(r, minlength=2), 1, preds_var\n            )\n            maj = counts.max(axis=1)\n            uncs_vote = 1 - maj / (K + 1)\n            # symmetric KL\n            uncs_kl = []\n            for g in range(N):\n                ps = all_probs[g]\n                lps = np.log(ps + 1e-12)\n                vals = []\n                for a in range(K + 1):\n                    for b in range(a + 1, K + 1):\n                        kl1 = np.sum(ps[a] * (lps[a] - lps[b]))\n                        kl2 = np.sum(ps[b] * (lps[b] - lps[a]))\n                        vals.append(0.5 * (kl1 + kl2))\n                uncs_kl.append(np.mean(vals))\n            uncs_kl = np.array(uncs_kl)\n            # AUC & DES\n            try:\n                auc_v = roc_auc_score(errs, uncs_vote)\n            except:\n                auc_v = 0.5\n            try:\n                auc_k = roc_auc_score(errs, uncs_kl)\n            except:\n                auc_k = 0.5\n            des_v = auc_v / (K + 1)\n            des_k = auc_k / (K + 1)\n            # Spearman\n            spe_v = (\n                spearmanr(errs, uncs_vote).correlation\n                if np.unique(errs).size > 1\n                else np.nan\n            )\n            spe_k = (\n                spearmanr(errs, uncs_kl).correlation\n                if np.unique(errs).size > 1\n                else np.nan\n            )\n\n            ed[\"metrics\"][\"detection\"].append(\n                {\n                    \"epoch\": epoch,\n                    \"auc_vote\": float(auc_v),\n                    \"DES_vote\": float(des_v),\n                    \"spearman_vote\": float(spe_v if spe_v is not None else 0.0),\n                    \"auc_kl\": float(auc_k),\n                    \"DES_kl\": float(des_k),\n                    \"spearman_kl\": float(spe_k if spe_k is not None else 0.0),\n                }\n            )\n            print(\n                f\"{abl_name}/{name} Epoch {epoch}: \"\n                f\"AUC_vote={auc_v:.4f}, DES_vote={des_v:.4f}, Spearman_vote={spe_v:.4f}, \"\n                f\"AUC_kl={auc_k:.4f}, DES_kl={des_k:.4f}, Spearman_kl={spe_k:.4f}\"\n            )\n\n        # final preds on original\n        preds = []\n        single_loader = DataLoader(\n            TensorDataset(\n                va_enc[\"input_ids\"][grp_ids == 0],\n                va_enc[\"attention_mask\"][grp_ids == 0],\n            ),\n            batch_size=bs,\n        )\n        model.eval()\n        for ids, mask in single_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            with torch.no_grad():\n                logits = model(input_ids=ids, attention_mask=mask).logits\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n        ed[\"predictions\"] = preds\n\n# save all\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os, random, re, nltk, torch, numpy as np, torch.nn.functional as F\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnltk.download(\"wordnet\", quiet=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\n# hyperparams\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\n\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\nexperiment_data = {\"baseline\": {}, \"paraphrase_aug\": {}}\n\nfor ablation in experiment_data:\n    for name, (ds, sub, txt_col, lbl_col) in datasets_info.items():\n        # load and trim\n        train_ds = (\n            load_dataset(ds, sub, split=\"train\")\n            if sub\n            else load_dataset(ds, split=\"train\")\n        )\n        val_ds = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n        train_ds = train_ds.shuffle(42).select(range(train_size))\n        val_ds = val_ds.shuffle(42).select(range(val_size))\n        texts_train = train_ds[txt_col]\n        labels_train = train_ds[lbl_col]\n        texts_val = val_ds[txt_col]\n        labels_val = val_ds[lbl_col]\n        # val paraphrases for detection\n        paras_val = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_val)}\n        # prepare training set\n        if ablation == \"baseline\":\n            texts_tr_aug = texts_train\n            labels_tr_aug = labels_train\n        else:\n            texts_tr_aug, labels_tr_aug = [], []\n            for t, l in zip(texts_train, labels_train):\n                texts_tr_aug.append(t)\n                labels_tr_aug.append(l)\n                for p in generate_paraphrases(t, K):\n                    texts_tr_aug.append(p)\n                    labels_tr_aug.append(l)\n        # tokenize\n        tr_enc = tokenizer(\n            texts_tr_aug, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n        va_enc = tokenizer(\n            texts_val, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n        train_loader = DataLoader(\n            TensorDataset(\n                tr_enc[\"input_ids\"],\n                tr_enc[\"attention_mask\"],\n                torch.tensor(labels_tr_aug),\n            ),\n            batch_size=bs,\n            shuffle=True,\n        )\n        val_loader = DataLoader(\n            TensorDataset(\n                va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_val)\n            ),\n            batch_size=bs,\n        )\n        # model & optimizer\n        model = BertForSequenceClassification.from_pretrained(\n            \"bert-base-uncased\", num_labels=2\n        ).to(device)\n        optim = Adam(model.parameters(), lr=lr)\n        # init logging\n        experiment_data[ablation][name] = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"detection\": []},\n            \"predictions\": [],\n            \"ground_truth\": labels_val,\n        }\n        # training loop\n        for ep in range(1, epochs + 1):\n            model.train()\n            tr_losses = []\n            for ids, msk, lbl in train_loader:\n                ids, msk, lbl = ids.to(device), msk.to(device), lbl.to(device)\n                optim.zero_grad()\n                out = model(input_ids=ids, attention_mask=msk, labels=lbl)\n                out.loss.backward()\n                optim.step()\n                tr_losses.append(out.loss.item())\n            experiment_data[ablation][name][\"losses\"][\"train\"].append(\n                {\"epoch\": ep, \"loss\": float(np.mean(tr_losses))}\n            )\n            # val loss\n            model.eval()\n            val_losses = []\n            with torch.no_grad():\n                for ids, msk, lbl in val_loader:\n                    ids, msk, lbl = ids.to(device), msk.to(device), lbl.to(device)\n                    out = model(input_ids=ids, attention_mask=msk, labels=lbl)\n                    val_losses.append(out.loss.item())\n            experiment_data[ablation][name][\"losses\"][\"val\"].append(\n                {\"epoch\": ep, \"loss\": float(np.mean(val_losses))}\n            )\n            # detection metrics\n            uncs_v, uncs_k, errs = [], [], []\n            for i, txt in enumerate(texts_val):\n                probs, preds = [], []\n                for variant in [txt] + paras_val[i]:\n                    enc = tokenizer(\n                        variant, truncation=True, padding=True, return_tensors=\"pt\"\n                    ).to(device)\n                    with torch.no_grad():\n                        logit = model(**enc).logits\n                    p = torch.softmax(logit, -1).squeeze().cpu()\n                    probs.append(p)\n                    preds.append(int(p.argmax()))\n                maj = max(set(preds), key=preds.count)\n                uncs_v.append(1 - preds.count(maj) / len(preds))\n                kl_list = []\n                for a in range(len(probs)):\n                    for b in range(a + 1, len(probs)):\n                        P, Q = probs[a], probs[b]\n                        kl1 = F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                        kl2 = F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                        kl_list.append(0.5 * (kl1 + kl2))\n                uncs_k.append(float(np.mean(kl_list)))\n                errs.append(int(preds[0] != int(labels_val[i])))\n            try:\n                auc_v = roc_auc_score(errs, uncs_v)\n            except:\n                auc_v = 0.5\n            try:\n                auc_k = roc_auc_score(errs, uncs_k)\n            except:\n                auc_k = 0.5\n            des_v = auc_v / (K + 1)\n            des_k = auc_k / (K + 1)\n            experiment_data[ablation][name][\"metrics\"][\"detection\"].append(\n                {\n                    \"epoch\": ep,\n                    \"auc_vote\": auc_v,\n                    \"DES_vote\": des_v,\n                    \"auc_kl\": auc_k,\n                    \"DES_kl\": des_k,\n                }\n            )\n            print(\n                f\"{ablation} {name} Ep{ep}: val_loss={np.mean(val_losses):.4f}, AUCv={auc_v:.3f}, DESv={des_v:.3f}\"\n            )\n        # final predictions\n        preds = []\n        model.eval()\n        with torch.no_grad():\n            for ids, msk, _ in val_loader:\n                ids, msk = ids.to(device), msk.to(device)\n                logits = model(input_ids=ids, attention_mask=msk).logits\n                preds += torch.argmax(logits, -1).cpu().tolist()\n        experiment_data[ablation][name][\"predictions\"] = preds\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, re\nimport nltk, torch, numpy as np\nimport torch.nn.functional as F\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# setup\nnltk.download(\"wordnet\", quiet=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\ndef prune_heads(model, prune_ratio=0.5, seed=42):\n    cfg = model.config\n    L, H = cfg.num_hidden_layers, cfg.num_attention_heads\n    head_dim = cfg.hidden_size // H\n    random.seed(seed)\n    # build head_mask and record per-layer head configs\n    head_mask = torch.ones(L, H, dtype=torch.float32)\n    layer_masks = {}\n    for l in range(L):\n        pruned = set(random.sample(range(H), int(H * prune_ratio)))\n        m = [0.0 if h in pruned else 1.0 for h in range(H)]\n        head_mask[l] = torch.tensor(m)\n        layer_masks[l] = m\n    # attach gradient hooks to freeze pruned subspaces\n    for l, mask in layer_masks.items():\n        attn_mod = model.bert.encoder.layer[l].attention\n        # query/key/value\n        for proj in (attn_mod.self.query, attn_mod.self.key, attn_mod.self.value):\n            wm = torch.repeat_interleave(\n                torch.tensor(mask, dtype=torch.float32), head_dim\n            )\n            wm2d = wm.unsqueeze(1).to(proj.weight.device).expand_as(proj.weight)\n            bm = wm.to(proj.bias.device)\n            proj.weight.register_hook(lambda grad, m=wm2d: grad * m)\n            proj.bias.register_hook(lambda grad, m=bm: grad * m)\n        # output projection\n        dense = attn_mod.output.dense\n        om = torch.ones_like(dense.weight)\n        for h, keep in enumerate(mask):\n            if keep == 0.0:\n                om[:, h * head_dim : (h + 1) * head_dim] = 0.0\n        om = om.to(dense.weight.device)\n        dense.weight.register_hook(lambda grad, m=om: grad * m)\n    return head_mask.to(device)\n\n\n# configs\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\n\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\n# preprocess data once\ndata = {}\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    dstr = (\n        load_dataset(ds, sub, split=\"train\") if sub else load_dataset(ds, split=\"train\")\n    )\n    dsvl = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n    dstr = dstr.shuffle(42).select(range(train_size))\n    dsvl = dsvl.shuffle(42).select(range(val_size))\n    texts_tr, labs_tr = dstr[text_col], dstr[label_col]\n    texts_va, labs_va = dsvl[text_col], dsvl[label_col]\n    paras = {i: generate_paraphrases(texts_va[i], K) for i in range(len(texts_va))}\n    tr_enc = tokenizer(texts_tr, truncation=True, padding=True, return_tensors=\"pt\")\n    va_enc = tokenizer(texts_va, truncation=True, padding=True, return_tensors=\"pt\")\n    tr_ds = TensorDataset(\n        tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labs_tr)\n    )\n    va_ds = TensorDataset(\n        va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labs_va)\n    )\n    data[name] = {\n        \"tr_loader\": DataLoader(tr_ds, batch_size=bs, shuffle=True),\n        \"va_loader\": DataLoader(va_ds, batch_size=bs),\n        \"texts_val\": texts_va,\n        \"labels_val\": labs_va,\n        \"paras\": paras,\n        \"train_size\": len(texts_tr),\n        \"val_size\": len(texts_va),\n    }\n\n# run experiments\nexperiment_data = {\"full_heads\": {}, \"pruned_heads\": {}}\nfor exp in experiment_data:\n    for name in datasets_info:\n        dd = data[name]\n        # init model\n        model = BertForSequenceClassification.from_pretrained(\n            \"bert-base-uncased\", num_labels=2\n        ).to(device)\n        head_mask = None\n        if exp == \"pruned_heads\":\n            head_mask = prune_heads(model)\n        optimizer = Adam(model.parameters(), lr=lr)\n        # prepare logging\n        experiment_data[exp][name] = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"detection\": [],\n            \"predictions\": [],\n            \"ground_truth\": dd[\"labels_val\"],\n        }\n        # epochs\n        for epoch in range(1, epochs + 1):\n            # train\n            model.train()\n            train_loss, train_corr = 0.0, 0\n            for ids, mask, labs in dd[\"tr_loader\"]:\n                ids, mask, labs = ids.to(device), mask.to(device), labs.to(device)\n                optimizer.zero_grad()\n                kwargs = {\"input_ids\": ids, \"attention_mask\": mask}\n                if head_mask is not None:\n                    kwargs[\"head_mask\"] = head_mask\n                out = model(**kwargs, labels=labs)\n                out.loss.backward()\n                optimizer.step()\n                train_loss += out.loss.item()\n                preds = out.logits.argmax(dim=-1)\n                train_corr += (preds == labs).sum().item()\n            avg_tr_loss = train_loss / len(dd[\"tr_loader\"])\n            tr_acc = train_corr / dd[\"train_size\"]\n            experiment_data[exp][name][\"losses\"][\"train\"].append(\n                {\"epoch\": epoch, \"loss\": avg_tr_loss}\n            )\n            experiment_data[exp][name][\"metrics\"][\"train\"].append(\n                {\"epoch\": epoch, \"acc\": tr_acc}\n            )\n\n            # validation\n            model.eval()\n            val_loss, val_corr = 0.0, 0\n            with torch.no_grad():\n                for ids, mask, labs in dd[\"va_loader\"]:\n                    ids, mask, labs = ids.to(device), mask.to(device), labs.to(device)\n                    kwargs = {\"input_ids\": ids, \"attention_mask\": mask}\n                    if head_mask is not None:\n                        kwargs[\"head_mask\"] = head_mask\n                    out = model(**kwargs, labels=labs)\n                    val_loss += out.loss.item()\n                    preds = out.logits.argmax(dim=-1)\n                    val_corr += (preds == labs).sum().item()\n            avg_va_loss = val_loss / len(dd[\"va_loader\"])\n            va_acc = val_corr / dd[\"val_size\"]\n            experiment_data[exp][name][\"losses\"][\"val\"].append(\n                {\"epoch\": epoch, \"loss\": avg_va_loss}\n            )\n            experiment_data[exp][name][\"metrics\"][\"val\"].append(\n                {\"epoch\": epoch, \"acc\": va_acc}\n            )\n            print(\n                f\"{exp} {name} E{epoch}: tr_loss={avg_tr_loss:.4f}, tr_acc={tr_acc:.4f}, \"\n                f\"va_loss={avg_va_loss:.4f}, va_acc={va_acc:.4f}\"\n            )\n\n            # detection metrics\n            uncs_vote, uncs_kl, errs = [], [], []\n            for i, txt in enumerate(dd[\"texts_val\"]):\n                probs, ppreds = [], []\n                for var in [txt] + dd[\"paras\"][i]:\n                    enc = tokenizer(\n                        var, return_tensors=\"pt\", truncation=True, padding=True\n                    ).to(device)\n                    kwargs = {\n                        \"input_ids\": enc[\"input_ids\"],\n                        \"attention_mask\": enc[\"attention_mask\"],\n                    }\n                    if head_mask is not None:\n                        kwargs[\"head_mask\"] = head_mask\n                    with torch.no_grad():\n                        lg = model(**kwargs).logits\n                    p = F.softmax(lg, dim=-1).squeeze(0).cpu()\n                    probs.append(p)\n                    ppreds.append(int(p.argmax().item()))\n                maj = max(set(ppreds), key=ppreds.count)\n                uncs_vote.append(1.0 - ppreds.count(maj) / len(ppreds))\n                klv = []\n                for a in range(len(probs)):\n                    for b in range(a + 1, len(probs)):\n                        P, Q = probs[a], probs[b]\n                        kl1 = F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                        kl2 = F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                        klv.append(0.5 * (kl1 + kl2))\n                uncs_kl.append(float(np.mean(klv)))\n                errs.append(int(ppreds[0] != int(dd[\"labels_val\"][i])))\n            try:\n                auc_v = roc_auc_score(errs, uncs_vote)\n            except:\n                auc_v = 0.5\n            try:\n                auc_k = roc_auc_score(errs, uncs_kl)\n            except:\n                auc_k = 0.5\n            des_v, des_k = auc_v / (K + 1), auc_k / (K + 1)\n            experiment_data[exp][name][\"detection\"].append(\n                {\n                    \"epoch\": epoch,\n                    \"auc_vote\": auc_v,\n                    \"DES_vote\": des_v,\n                    \"auc_kl\": auc_k,\n                    \"DES_kl\": des_k,\n                }\n            )\n            print(\n                f\"{exp} {name} E{epoch}: AUC_vote={auc_v:.4f}, DES_vote={des_v:.4f}, \"\n                f\"AUC_kl={auc_k:.4f}, DES_kl={des_k:.4f}\"\n            )\n\n        # final predictions\n        preds = []\n        model.eval()\n        with torch.no_grad():\n            for ids, mask, _ in dd[\"va_loader\"]:\n                ids, mask = ids.to(device), mask.to(device)\n                kwargs = {\"input_ids\": ids, \"attention_mask\": mask}\n                if head_mask is not None:\n                    kwargs[\"head_mask\"] = head_mask\n                lg = model(**kwargs).logits\n                preds.extend(lg.argmax(dim=-1).cpu().tolist())\n        experiment_data[exp][name][\"predictions\"] = preds\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy in\", working_dir)\n", "import os, random, re\nimport nltk\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# prepare WordNet\nnltk.download(\"wordnet\", quiet=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\n# datasets to run\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\n# hyperparameters\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\n\n# tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\n# prepare experiment data container\nexperiment_data = {\"baseline\": {}, \"ffn_ablate\": {}}\n\nfor ablation in experiment_data:\n    for name, (ds, sub, text_col, label_col) in datasets_info.items():\n        # load and subsample\n        ds_train = (\n            load_dataset(ds, sub, split=\"train\")\n            if sub\n            else load_dataset(ds, split=\"train\")\n        )\n        ds_val = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n        ds_train = ds_train.shuffle(42).select(range(train_size))\n        ds_val = ds_val.shuffle(42).select(range(val_size))\n        texts_train, labels_train = ds_train[text_col], ds_train[label_col]\n        texts_val, labels_val = ds_val[text_col], ds_val[label_col]\n        # generate paraphrases for validation\n        paras = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_val)}\n        # tokenize\n        tr_enc = tokenizer(\n            texts_train, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n        va_enc = tokenizer(\n            texts_val, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n        train_ds = TensorDataset(\n            tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n        )\n        val_ds = TensorDataset(\n            va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_val)\n        )\n        tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n        va_loader = DataLoader(val_ds, batch_size=bs)\n\n        # init model\n        model = BertForSequenceClassification.from_pretrained(\n            \"bert-base-uncased\", num_labels=2\n        ).to(device)\n        if ablation == \"ffn_ablate\":\n            # zero & freeze FFN layers\n            for layer in model.bert.encoder.layer:\n                layer.intermediate.dense.weight.data.zero_()\n                layer.intermediate.dense.bias.data.zero_()\n                layer.output.dense.weight.data.zero_()\n                layer.output.dense.bias.data.zero_()\n                for p in layer.intermediate.dense.parameters():\n                    p.requires_grad = False\n                for p in layer.output.dense.parameters():\n                    p.requires_grad = False\n\n        optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n        # containers\n        experiment_data[ablation][name] = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"detection\": []},\n            \"predictions\": [],\n            \"ground_truth\": labels_val,\n        }\n\n        # training loop\n        for epoch in range(1, epochs + 1):\n            # train\n            model.train()\n            train_losses = []\n            for ids, mask, labels in tr_loader:\n                ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n                optimizer.zero_grad()\n                out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                out.loss.backward()\n                optimizer.step()\n                train_losses.append(out.loss.item())\n            experiment_data[ablation][name][\"losses\"][\"train\"].append(\n                {\"epoch\": epoch, \"loss\": float(np.mean(train_losses))}\n            )\n            # val loss\n            model.eval()\n            val_losses = []\n            with torch.no_grad():\n                for ids, mask, labels in va_loader:\n                    ids, mask, labels = (\n                        ids.to(device),\n                        mask.to(device),\n                        labels.to(device),\n                    )\n                    out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                    val_losses.append(out.loss.item())\n            val_loss = float(np.mean(val_losses))\n            experiment_data[ablation][name][\"losses\"][\"val\"].append(\n                {\"epoch\": epoch, \"loss\": val_loss}\n            )\n            print(f\"{ablation} {name} Epoch {epoch}: val_loss={val_loss:.4f}\")\n\n            # detection metrics\n            uncs_vote, uncs_kl, errs = [], [], []\n            for i, txt in enumerate(texts_val):\n                probs, preds = [], []\n                for variant in [txt] + paras[i]:\n                    enc = tokenizer(\n                        variant, return_tensors=\"pt\", truncation=True, padding=True\n                    ).to(device)\n                    with torch.no_grad():\n                        logits = model(**enc).logits\n                    p = torch.softmax(logits, dim=-1).squeeze(0).cpu()\n                    probs.append(p)\n                    preds.append(int(p.argmax().item()))\n                maj = max(set(preds), key=preds.count)\n                uncs_vote.append(1 - preds.count(maj) / len(preds))\n                kl_vals = []\n                for a in range(len(probs)):\n                    for b in range(a + 1, len(probs)):\n                        P, Q = probs[a], probs[b]\n                        kl1 = F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                        kl2 = F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                        kl_vals.append(0.5 * (kl1 + kl2))\n                uncs_kl.append(float(np.mean(kl_vals)))\n                errs.append(int(preds[0] != int(labels_val[i])))\n            try:\n                auc_v = roc_auc_score(errs, uncs_vote)\n            except:\n                auc_v = 0.5\n            try:\n                auc_k = roc_auc_score(errs, uncs_kl)\n            except:\n                auc_k = 0.5\n            des_v = auc_v / (K + 1)\n            des_k = auc_k / (K + 1)\n            experiment_data[ablation][name][\"metrics\"][\"detection\"].append(\n                {\n                    \"epoch\": epoch,\n                    \"auc_vote\": auc_v,\n                    \"DES_vote\": des_v,\n                    \"auc_kl\": auc_k,\n                    \"DES_kl\": des_k,\n                }\n            )\n            print(\n                f\"{ablation} {name} Epoch {epoch}: AUC_vote={auc_v:.4f}, DES_vote={des_v:.4f}, AUC_kl={auc_k:.4f}, DES_kl={des_k:.4f}\"\n            )\n\n        # final predictions\n        model.eval()\n        preds = []\n        with torch.no_grad():\n            for ids, mask, _ in va_loader:\n                ids, mask = ids.to(device), mask.to(device)\n                logits = model(input_ids=ids, attention_mask=mask).logits\n                preds.extend(torch.argmax(logits, -1).cpu().tolist())\n        experiment_data[ablation][name][\"predictions\"] = preds\n\n# save results\nnp.save(\"experiment_data.npy\", experiment_data, allow_pickle=True)\n", "import os, random, re\nimport nltk\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom transformers.models.bert.modeling_bert import BertSelfOutput, BertOutput\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# download WordNet\nnltk.download(\"wordnet\", quiet=True)\n\n\n# define ablation modules (no residual additions)\nclass NoResSelfOutput(BertSelfOutput):\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        # omit residual\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\nclass NoResOutput(BertOutput):\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        # omit residual\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\n# dataset info\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\n# hyperparameters\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\n\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\nexperiment_data = {\"ResidualConnectionAblation\": {}}\n\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # load and trim\n    ds_train = (\n        load_dataset(ds, sub, split=\"train\") if sub else load_dataset(ds, split=\"train\")\n    )\n    ds_val = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n    ds_train = ds_train.shuffle(42).select(range(train_size))\n    ds_val = ds_val.shuffle(42).select(range(val_size))\n    texts_train, labels_train = ds_train[text_col], ds_train[label_col]\n    texts_val, labels_val = ds_val[text_col], ds_val[label_col]\n    # paraphrases\n    paras = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_val)}\n    # tokenize\n    tr_enc = tokenizer(texts_train, truncation=True, padding=True, return_tensors=\"pt\")\n    va_enc = tokenizer(texts_val, truncation=True, padding=True, return_tensors=\"pt\")\n    train_ds = TensorDataset(\n        tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n    )\n    val_ds = TensorDataset(\n        va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_val)\n    )\n    tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=bs)\n    # load model and patch residuals\n    model = BertForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\", num_labels=2\n    ).to(device)\n    for layer in model.bert.encoder.layer:\n        # patch self-attention output\n        orig_so = layer.attention.output\n        new_so = NoResSelfOutput(model.config)\n        new_so.load_state_dict(orig_so.state_dict())\n        layer.attention.output = new_so\n        # patch feed-forward output\n        orig_fo = layer.output\n        new_fo = NoResOutput(model.config)\n        new_fo.load_state_dict(orig_fo.state_dict())\n        layer.output = new_fo\n    optimizer = Adam(model.parameters(), lr=lr)\n\n    # initialize storage\n    exp = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"detection\": []},\n        \"predictions\": [],\n        \"ground_truth\": labels_val,\n    }\n\n    # training & evaluation\n    for epoch in range(1, epochs + 1):\n        model.train()\n        train_losses = []\n        for ids, mask, labels in tr_loader:\n            ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n            optimizer.zero_grad()\n            out = model(input_ids=ids, attention_mask=mask, labels=labels)\n            out.loss.backward()\n            optimizer.step()\n            train_losses.append(out.loss.item())\n        exp[\"losses\"][\"train\"].append(\n            {\"epoch\": epoch, \"loss\": float(np.mean(train_losses))}\n        )\n\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for ids, mask, labels in va_loader:\n                ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n                out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                val_losses.append(out.loss.item())\n        val_loss = float(np.mean(val_losses))\n        exp[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": val_loss})\n        print(f\"{name} Epoch {epoch}: val_loss = {val_loss:.4f}\")\n\n        # detection\n        uncs_vote, uncs_kl, errs = [], [], []\n        for i, txt in enumerate(texts_val):\n            probs, preds = [], []\n            for variant in [txt] + paras[i]:\n                enc = tokenizer(\n                    variant, return_tensors=\"pt\", truncation=True, padding=True\n                ).to(device)\n                with torch.no_grad():\n                    logits = model(**enc).logits\n                p = torch.softmax(logits, dim=-1).squeeze(0).cpu()\n                probs.append(p)\n                preds.append(int(p.argmax().item()))\n            maj = max(set(preds), key=preds.count)\n            uncs_vote.append(1 - preds.count(maj) / len(preds))\n            kl_vals = []\n            for a in range(len(probs)):\n                for b in range(a + 1, len(probs)):\n                    P, Q = probs[a], probs[b]\n                    kl_vals.append(\n                        0.5\n                        * (\n                            F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                            + F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                        )\n                    )\n            uncs_kl.append(float(np.mean(kl_vals)))\n            errs.append(int(preds[0] != int(labels_val[i])))\n        try:\n            auc_v = roc_auc_score(errs, uncs_vote)\n        except:\n            auc_v = 0.5\n        try:\n            auc_k = roc_auc_score(errs, uncs_kl)\n        except:\n            auc_k = 0.5\n        des_v, des_k = auc_v / (K + 1), auc_k / (K + 1)\n        exp[\"metrics\"][\"detection\"].append(\n            {\n                \"epoch\": epoch,\n                \"auc_vote\": auc_v,\n                \"DES_vote\": des_v,\n                \"auc_kl\": auc_k,\n                \"DES_kl\": des_k,\n            }\n        )\n        print(\n            f\"{name} Epoch {epoch}: AUC_vote={auc_v:.4f}, DES_vote={des_v:.4f}, AUC_kl={auc_k:.4f}, DES_kl={des_k:.4f}\"\n        )\n\n    # final predictions\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for ids, mask, _ in va_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            logits = model(input_ids=ids, attention_mask=mask).logits\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n    exp[\"predictions\"] = preds\n\n    experiment_data[\"ResidualConnectionAblation\"][name] = exp\n\n# save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport random\nimport re\nimport nltk\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# prepare working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device and WordNet\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnltk.download(\"wordnet\", quiet=True)\n\n\n# paraphrase generator\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\n# dataset info and hyperparams\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\n\n# tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\n# collect all results\nexperiment_data = {\"token_type_embedding_ablation\": {}}\n\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # load and trim dataset\n    ds_train = (\n        load_dataset(ds, sub, split=\"train\") if sub else load_dataset(ds, split=\"train\")\n    )\n    ds_val = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n    ds_train = ds_train.shuffle(42).select(range(train_size))\n    ds_val = ds_val.shuffle(42).select(range(val_size))\n    texts_train, labels_train = ds_train[text_col], ds_train[label_col]\n    texts_val, labels_val = ds_val[text_col], ds_val[label_col]\n    # paraphrases for validation\n    paras = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_val)}\n    # tokenize for training and validation\n    tr_enc = tokenizer(texts_train, truncation=True, padding=True, return_tensors=\"pt\")\n    va_enc = tokenizer(texts_val, truncation=True, padding=True, return_tensors=\"pt\")\n    train_ds = TensorDataset(\n        tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n    )\n    val_ds = TensorDataset(\n        va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_val)\n    )\n    tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=bs)\n\n    # model and ablation: zero out token_type embeddings\n    model = BertForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\", num_labels=2\n    ).to(device)\n    with torch.no_grad():\n        model.bert.embeddings.token_type_embeddings.weight.data.zero_()\n    optimizer = Adam(model.parameters(), lr=lr)\n\n    # prepare storage\n    inner = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": labels_val,\n    }\n    experiment_data[\"token_type_embedding_ablation\"][name] = inner\n\n    # training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        train_losses = []\n        for ids, mask, labels in tr_loader:\n            ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n            token_types = torch.zeros_like(ids).to(device)\n            optimizer.zero_grad()\n            out = model(\n                input_ids=ids,\n                attention_mask=mask,\n                token_type_ids=token_types,\n                labels=labels,\n            )\n            out.loss.backward()\n            optimizer.step()\n            train_losses.append(out.loss.item())\n        inner[\"losses\"][\"train\"].append(\n            {\"epoch\": epoch, \"loss\": float(np.mean(train_losses))}\n        )\n\n        # validation loss\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for ids, mask, labels in va_loader:\n                ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n                token_types = torch.zeros_like(ids).to(device)\n                out = model(\n                    input_ids=ids,\n                    attention_mask=mask,\n                    token_type_ids=token_types,\n                    labels=labels,\n                )\n                val_losses.append(out.loss.item())\n        v_loss = float(np.mean(val_losses))\n        inner[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": v_loss})\n        print(f\"{name} Epoch {epoch}: val_loss={v_loss:.4f}\")\n\n        # detection metrics\n        uncs_vote, uncs_kl, errs = [], [], []\n        for i, txt in enumerate(texts_val):\n            probs, preds = [], []\n            for variant in [txt] + paras[i]:\n                enc = tokenizer(\n                    variant, truncation=True, padding=True, return_tensors=\"pt\"\n                )\n                ids_v = enc[\"input_ids\"].to(device)\n                mask_v = enc[\"attention_mask\"].to(device)\n                token_types_v = torch.zeros_like(ids_v).to(device)\n                with torch.no_grad():\n                    logits = model(\n                        input_ids=ids_v,\n                        attention_mask=mask_v,\n                        token_type_ids=token_types_v,\n                    ).logits\n                p = torch.softmax(logits, -1).squeeze(0).cpu()\n                probs.append(p)\n                preds.append(int(p.argmax().item()))\n            maj = max(set(preds), key=preds.count)\n            uncs_vote.append(1 - preds.count(maj) / len(preds))\n            kl_vals = []\n            for a in range(len(probs)):\n                for b in range(a + 1, len(probs)):\n                    P, Q = probs[a], probs[b]\n                    kl1 = F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                    kl2 = F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                    kl_vals.append(0.5 * (kl1 + kl2))\n            uncs_kl.append(float(np.mean(kl_vals)))\n            errs.append(int(preds[0] != int(labels_val[i])))\n        try:\n            auc_v = roc_auc_score(errs, uncs_vote)\n        except:\n            auc_v = 0.5\n        try:\n            auc_k = roc_auc_score(errs, uncs_kl)\n        except:\n            auc_k = 0.5\n        des_v = auc_v / (K + 1)\n        des_k = auc_k / (K + 1)\n        inner[\"metrics\"][\"val\"].append(\n            {\n                \"epoch\": epoch,\n                \"auc_vote\": auc_v,\n                \"DES_vote\": des_v,\n                \"auc_kl\": auc_k,\n                \"DES_kl\": des_k,\n            }\n        )\n        print(\n            f\"{name} Epoch {epoch}: AUC_vote={auc_v:.4f}, DES_vote={des_v:.4f}, AUC_kl={auc_k:.4f}, DES_kl={des_k:.4f}\"\n        )\n\n    # final predictions\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for ids, mask, _ in va_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            token_types = torch.zeros_like(ids).to(device)\n            logits = model(\n                input_ids=ids, attention_mask=mask, token_type_ids=token_types\n            ).logits\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n    inner[\"predictions\"] = preds\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport random\nimport re\nimport nltk\nimport numpy as np\nimport torch\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\nfrom scipy.stats import spearmanr\n\n# Setup working directory and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nnltk.download(\"wordnet\", quiet=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Hyperparameters\ntrain_size, val_size = 1000, 200\nK, epochs, bs, lr = 3, 3, 32, 2e-5\nnum_workers = 4\n\n# Datasets to use\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\n\n# Paraphrase generator\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\n# Load tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\n# Precompute datasets (paraphrases + tokenization + DataLoaders)\nprocessed_data = {}\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # Load and sample\n    if sub:\n        train_raw = (\n            load_dataset(ds, sub, split=\"train\").shuffle(42).select(range(train_size))\n        )\n        val_raw = (\n            load_dataset(ds, sub, split=\"validation\")\n            .shuffle(42)\n            .select(range(val_size))\n        )\n    else:\n        train_raw = (\n            load_dataset(ds, split=\"train\").shuffle(42).select(range(train_size))\n        )\n        val_raw = load_dataset(ds, split=\"test\").shuffle(42).select(range(val_size))\n    texts_train, labels_train = train_raw[text_col], train_raw[label_col]\n    texts_val, labels_val = val_raw[text_col], val_raw[label_col]\n    N = len(texts_val)\n    # Generate paraphrases\n    paras = [generate_paraphrases(t, K) for t in texts_val]\n    variants, grp_ids, var_pos = [], [], []\n    for i, t in enumerate(texts_val):\n        vs = [t] + paras[i]\n        for j, v in enumerate(vs):\n            variants.append(v)\n            grp_ids.append(i)\n            var_pos.append(j)\n    # Tokenize\n    tr_enc = tokenizer(texts_train, truncation=True, padding=True, return_tensors=\"pt\")\n    va_enc = tokenizer(variants, truncation=True, padding=True, return_tensors=\"pt\")\n    grp_ids_tensor = torch.tensor(grp_ids, dtype=torch.long)\n    var_pos_tensor = torch.tensor(var_pos, dtype=torch.long)\n    # Build datasets\n    train_ds = TensorDataset(\n        tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n    )\n    val_ds = TensorDataset(\n        va_enc[\"input_ids\"], va_enc[\"attention_mask\"], grp_ids_tensor, var_pos_tensor\n    )\n    orig_mask = var_pos_tensor.eq(0)\n    single_ds = TensorDataset(\n        va_enc[\"input_ids\"][orig_mask], va_enc[\"attention_mask\"][orig_mask]\n    )\n    # DataLoaders\n    tr_loader = DataLoader(\n        train_ds, batch_size=bs, shuffle=True, num_workers=num_workers, pin_memory=True\n    )\n    va_loader = DataLoader(\n        val_ds, batch_size=bs, shuffle=False, num_workers=num_workers, pin_memory=True\n    )\n    single_loader = DataLoader(\n        single_ds,\n        batch_size=bs,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    processed_data[name] = {\n        \"tr_loader\": tr_loader,\n        \"va_loader\": va_loader,\n        \"single_loader\": single_loader,\n        \"labels_val\": labels_val,\n        \"grp_ids\": np.array(grp_ids),\n        \"N\": N,\n    }\n\n# Ablation settings\nablations = {\"full_ft\": 0, \"freeze_4\": 4, \"freeze_8\": 8}\nexperiment_data = {}\n\nfor abl_name, freeze_layers in ablations.items():\n    experiment_data[abl_name] = {}\n    for name in datasets_info:\n        data = processed_data[name]\n        tr_loader, va_loader = data[\"tr_loader\"], data[\"va_loader\"]\n        single_loader = data[\"single_loader\"]\n        labels_val = data[\"labels_val\"]\n        grp_ids = data[\"grp_ids\"]\n        N = data[\"N\"]\n\n        # Initialize model\n        model = BertForSequenceClassification.from_pretrained(\n            \"bert-base-uncased\", num_labels=2\n        ).to(device)\n        for i in range(freeze_layers):\n            for p in model.bert.encoder.layer[i].parameters():\n                p.requires_grad = False\n        optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n        # Storage\n        ed = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"detection\": []},\n            \"predictions\": [],\n            \"ground_truth\": labels_val,\n        }\n        experiment_data[abl_name][name] = ed\n\n        # Training + evaluation\n        for epoch in range(1, epochs + 1):\n            # Train\n            model.train()\n            train_losses = []\n            for ids, mask, labs in tr_loader:\n                ids, mask, labs = ids.to(device), mask.to(device), labs.to(device)\n                optimizer.zero_grad()\n                out = model(input_ids=ids, attention_mask=mask, labels=labs)\n                out.loss.backward()\n                optimizer.step()\n                train_losses.append(out.loss.item())\n            train_loss = float(np.mean(train_losses))\n            ed[\"losses\"][\"train\"].append({\"epoch\": epoch, \"loss\": train_loss})\n\n            # Validation loss (original prompts only)\n            model.eval()\n            val_losses = []\n            for ids, mask, grp, pos in va_loader:\n                ids, mask = ids.to(device), mask.to(device)\n                batch_idx = pos.eq(0)\n                if batch_idx.any():\n                    idx_dev = batch_idx.to(device)\n                    ids_o, mask_o = ids[idx_dev], mask[idx_dev]\n                    labs = torch.tensor(\n                        [labels_val[i] for i in grp[batch_idx].tolist()], device=device\n                    )\n                    out = model(input_ids=ids_o, attention_mask=mask_o, labels=labs)\n                    val_losses.append(out.loss.item())\n            val_loss = float(np.mean(val_losses)) if val_losses else 0.0\n            ed[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": val_loss})\n            print(f\"{abl_name}/{name} Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n            # Detection metrics\n            all_probs = np.zeros((N, K + 1, 2), dtype=float)\n            for ids, mask, grp, pos in va_loader:\n                ids, mask = ids.to(device), mask.to(device)\n                with torch.no_grad():\n                    logits = model(input_ids=ids, attention_mask=mask).logits\n                    ps = torch.softmax(logits, dim=-1).cpu().numpy()\n                gids = grp.numpy()\n                poss = pos.numpy()\n                all_probs[gids, poss, :] = ps\n            preds_var = all_probs.argmax(axis=2)\n            errs = (preds_var[:, 0] != np.array(labels_val)).astype(int)\n\n            # Vote uncertainty\n            counts = np.apply_along_axis(\n                lambda r: np.bincount(r, minlength=2), 1, preds_var\n            )\n            maj = counts.max(axis=1)\n            uncs_vote = 1 - maj / (K + 1)\n\n            # Symmetric KL\n            uncs_kl = []\n            lps = np.log(all_probs + 1e-12)\n            for g in range(N):\n                ps = all_probs[g]\n                lp = lps[g]\n                vals = []\n                for a in range(K + 1):\n                    for b in range(a + 1, K + 1):\n                        kl1 = np.sum(ps[a] * (lp[a] - lp[b]))\n                        kl2 = np.sum(ps[b] * (lp[b] - lp[a]))\n                        vals.append(0.5 * (kl1 + kl2))\n                uncs_kl.append(np.mean(vals))\n            uncs_kl = np.array(uncs_kl)\n\n            # AUC & DES & Spearman\n            try:\n                auc_v = roc_auc_score(errs, uncs_vote)\n            except:\n                auc_v = 0.5\n            try:\n                auc_k = roc_auc_score(errs, uncs_kl)\n            except:\n                auc_k = 0.5\n            des_v = auc_v / (K + 1)\n            des_k = auc_k / (K + 1)\n            spe_v = (\n                spearmanr(errs, uncs_vote).correlation\n                if np.unique(errs).size > 1\n                else np.nan\n            )\n            spe_k = (\n                spearmanr(errs, uncs_kl).correlation\n                if np.unique(errs).size > 1\n                else np.nan\n            )\n\n            ed[\"metrics\"][\"detection\"].append(\n                {\n                    \"epoch\": epoch,\n                    \"auc_vote\": float(auc_v),\n                    \"DES_vote\": float(des_v),\n                    \"spearman_vote\": float(spe_v if spe_v is not None else 0.0),\n                    \"auc_kl\": float(auc_k),\n                    \"DES_kl\": float(des_k),\n                    \"spearman_kl\": float(spe_k if spe_k is not None else 0.0),\n                }\n            )\n            print(\n                f\"{abl_name}/{name} Epoch {epoch}: \"\n                f\"AUC_vote={auc_v:.4f}, DES_vote={des_v:.4f}, Spearman_vote={spe_v:.4f}, \"\n                f\"AUC_kl={auc_k:.4f}, DES_kl={des_k:.4f}, Spearman_kl={spe_k:.4f}\"\n            )\n\n        # Final predictions on original prompts\n        preds = []\n        model.eval()\n        for ids, mask in single_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            with torch.no_grad():\n                logits = model(input_ids=ids, attention_mask=mask).logits\n            preds.extend(torch.argmax(logits, -1).cpu().numpy().tolist())\n        ed[\"predictions\"] = preds\n\n# Save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport random\nimport re\nimport nltk\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import Adam\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom sklearn.metrics import roc_auc_score\n\n# setup working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# prepare WordNet\nnltk.download(\"wordnet\", quiet=True)\n\n\n# identity for LayerNorm ablation\nclass Identity(nn.Module):\n    def forward(self, x):\n        return x\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\n# datasets to run\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\n# hyperparams\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\n\n# tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\n# init experiment data\nexperiment_data = {\"LayerNormAblation\": {}}\n\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # load data\n    ds_train = (\n        load_dataset(ds, sub, split=\"train\") if sub else load_dataset(ds, split=\"train\")\n    )\n    ds_val = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n    ds_train = ds_train.shuffle(42).select(range(train_size))\n    ds_val = ds_val.shuffle(42).select(range(val_size))\n    texts_train, labels_train = ds_train[text_col], ds_train[label_col]\n    texts_val, labels_val = ds_val[text_col], ds_val[label_col]\n    # keep ground truth\n    experiment_data[\"LayerNormAblation\"][name] = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"detection\": []},\n        \"predictions\": [],\n        \"ground_truth\": labels_val,\n    }\n    # paraphrases\n    paras = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_val)}\n    # tokenize\n    tr_enc = tokenizer(texts_train, truncation=True, padding=True, return_tensors=\"pt\")\n    va_enc = tokenizer(texts_val, truncation=True, padding=True, return_tensors=\"pt\")\n    train_ds = TensorDataset(\n        tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n    )\n    val_ds = TensorDataset(\n        va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_val)\n    )\n    tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=bs)\n\n    # load and ablate\n    model = BertForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\", num_labels=2\n    )\n    # replace layer norms in each transformer block\n    for layer in model.bert.encoder.layer:\n        layer.attention.output.LayerNorm = Identity()\n        layer.output.LayerNorm = Identity()\n    model.to(device)\n\n    optimizer = Adam(model.parameters(), lr=lr)\n\n    # training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        train_losses = []\n        for ids, mask, labels in tr_loader:\n            ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n            optimizer.zero_grad()\n            out = model(input_ids=ids, attention_mask=mask, labels=labels)\n            out.loss.backward()\n            optimizer.step()\n            train_losses.append(out.loss.item())\n        experiment_data[\"LayerNormAblation\"][name][\"losses\"][\"train\"].append(\n            {\"epoch\": epoch, \"loss\": float(np.mean(train_losses))}\n        )\n\n        # validation loss\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for ids, mask, labels in va_loader:\n                ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n                out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                val_losses.append(out.loss.item())\n        val_loss = float(np.mean(val_losses))\n        experiment_data[\"LayerNormAblation\"][name][\"losses\"][\"val\"].append(\n            {\"epoch\": epoch, \"loss\": val_loss}\n        )\n        print(f\"{name} Epoch {epoch}: val_loss={val_loss:.4f}\")\n\n        # detection metrics\n        uncs_vote, uncs_kl, errs = [], [], []\n        for i, txt in enumerate(texts_val):\n            probs, preds = [], []\n            for variant in [txt] + paras[i]:\n                enc = tokenizer(\n                    variant, return_tensors=\"pt\", truncation=True, padding=True\n                ).to(device)\n                with torch.no_grad():\n                    logits = model(**enc).logits\n                p = torch.softmax(logits, dim=-1).squeeze(0).cpu()\n                probs.append(p)\n                preds.append(int(p.argmax().item()))\n            maj = max(set(preds), key=preds.count)\n            uncs_vote.append(1 - preds.count(maj) / len(preds))\n            kl_vals = []\n            for a in range(len(probs)):\n                for b in range(a + 1, len(probs)):\n                    P, Q = probs[a], probs[b]\n                    kl_vals.append(\n                        0.5\n                        * (\n                            F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                            + F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                        )\n                    )\n            uncs_kl.append(float(np.mean(kl_vals)))\n            errs.append(int(preds[0] != int(labels_val[i])))\n        try:\n            auc_v = roc_auc_score(errs, uncs_vote)\n        except:\n            auc_v = 0.5\n        try:\n            auc_k = roc_auc_score(errs, uncs_kl)\n        except:\n            auc_k = 0.5\n        des_v, des_k = auc_v / (K + 1), auc_k / (K + 1)\n        experiment_data[\"LayerNormAblation\"][name][\"metrics\"][\"detection\"].append(\n            {\n                \"epoch\": epoch,\n                \"auc_vote\": auc_v,\n                \"DES_vote\": des_v,\n                \"auc_kl\": auc_k,\n                \"DES_kl\": des_k,\n            }\n        )\n        print(\n            f\"{name} Epoch {epoch}: AUC_vote={auc_v:.4f}, DES_vote={des_v:.4f}, AUC_kl={auc_k:.4f}, DES_kl={des_k:.4f}\"\n        )\n\n    # final predictions\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for ids, mask, _ in va_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            logits = model(input_ids=ids, attention_mask=mask).logits\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n    experiment_data[\"LayerNormAblation\"][name][\"predictions\"] = preds\n\n# save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random, re\nimport nltk\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnltk.download(\"wordnet\", quiet=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\n# datasets and hyperparams\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\n# ablation settings\nablations = {\"baseline\": False, \"RandomInitEmbeddingAblation\": True}\nexperiment_data = {}\n\nfor ablation_name, do_reinit in ablations.items():\n    experiment_data[ablation_name] = {}\n    for name, (ds, sub, text_col, label_col) in datasets_info.items():\n        # load & subsample\n        ds_train = (\n            load_dataset(ds, sub, split=\"train\")\n            if sub\n            else load_dataset(ds, split=\"train\")\n        )\n        ds_val = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n        ds_train = ds_train.shuffle(42).select(range(train_size))\n        ds_val = ds_val.shuffle(42).select(range(val_size))\n        texts_train, labels_train = ds_train[text_col], ds_train[label_col]\n        texts_val, labels_val = ds_val[text_col], ds_val[label_col]\n        paras = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_val)}\n        # tokenize\n        tr_enc = tokenizer(\n            texts_train, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n        va_enc = tokenizer(\n            texts_val, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n        train_ds = TensorDataset(\n            tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n        )\n        val_ds = TensorDataset(\n            va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_val)\n        )\n        tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n        va_loader = DataLoader(val_ds, batch_size=bs)\n        # init storage\n        experiment_data[ablation_name][name] = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"detection\": []},\n            \"predictions\": [],\n            \"ground_truth\": labels_val.copy(),\n        }\n        # model & ablation\n        model = BertForSequenceClassification.from_pretrained(\n            \"bert-base-uncased\", num_labels=2\n        ).to(device)\n        if do_reinit:\n            emb = model.get_input_embeddings()\n            torch.nn.init.normal_(emb.weight, mean=0.0, std=0.02)\n        optimizer = Adam(model.parameters(), lr=lr)\n        # train/eval\n        for epoch in range(1, epochs + 1):\n            model.train()\n            tr_losses = []\n            for ids, mask, lbls in tr_loader:\n                ids, mask, lbls = ids.to(device), mask.to(device), lbls.to(device)\n                optimizer.zero_grad()\n                out = model(input_ids=ids, attention_mask=mask, labels=lbls)\n                out.loss.backward()\n                optimizer.step()\n                tr_losses.append(out.loss.item())\n            experiment_data[ablation_name][name][\"losses\"][\"train\"].append(\n                {\"epoch\": epoch, \"loss\": float(np.mean(tr_losses))}\n            )\n            model.eval()\n            va_losses = []\n            with torch.no_grad():\n                for ids, mask, lbls in va_loader:\n                    ids, mask, lbls = ids.to(device), mask.to(device), lbls.to(device)\n                    out = model(input_ids=ids, attention_mask=mask, labels=lbls)\n                    va_losses.append(out.loss.item())\n            val_loss = float(np.mean(va_losses))\n            experiment_data[ablation_name][name][\"losses\"][\"val\"].append(\n                {\"epoch\": epoch, \"loss\": val_loss}\n            )\n            # detection metrics\n            uncs_vote, uncs_kl, errs = [], [], []\n            for i, txt in enumerate(texts_val):\n                probs, preds = [], []\n                for var in [txt] + paras[i]:\n                    enc = tokenizer(\n                        var, return_tensors=\"pt\", truncation=True, padding=True\n                    ).to(device)\n                    with torch.no_grad():\n                        logits = model(**enc).logits\n                    p = torch.softmax(logits, dim=-1).squeeze(0).cpu()\n                    probs.append(p)\n                    preds.append(int(p.argmax().item()))\n                maj = max(set(preds), key=preds.count)\n                uncs_vote.append(1 - preds.count(maj) / len(preds))\n                kl_vals = []\n                for a in range(len(probs)):\n                    for b in range(a + 1, len(probs)):\n                        P, Q = probs[a], probs[b]\n                        kl1 = F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                        kl2 = F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                        kl_vals.append(0.5 * (kl1 + kl2))\n                uncs_kl.append(float(np.mean(kl_vals)))\n                errs.append(int(preds[0] != labels_val[i]))\n            auc_v = roc_auc_score(errs, uncs_vote) if len(set(errs)) > 1 else 0.5\n            auc_k = roc_auc_score(errs, uncs_kl) if len(set(errs)) > 1 else 0.5\n            des_v = auc_v / (K + 1)\n            des_k = auc_k / (K + 1)\n            experiment_data[ablation_name][name][\"metrics\"][\"detection\"].append(\n                {\n                    \"epoch\": epoch,\n                    \"auc_vote\": auc_v,\n                    \"DES_vote\": des_v,\n                    \"auc_kl\": auc_k,\n                    \"DES_kl\": des_k,\n                }\n            )\n        # final predictions\n        preds = []\n        model.eval()\n        with torch.no_grad():\n            for ids, mask, _ in va_loader:\n                ids, mask = ids.to(device), mask.to(device)\n                logits = model(input_ids=ids, attention_mask=mask).logits\n                preds.extend(torch.argmax(logits, -1).cpu().tolist())\n        experiment_data[ablation_name][name][\"predictions\"] = preds\n\n# save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport random\nimport re\nimport nltk\n\nnltk.download(\"wordnet\", quiet=True)\nfrom nltk.corpus import wordnet\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom transformers.models.bert.modeling_bert import BertSelfOutput, BertOutput\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\nfrom scipy.stats import spearmanr\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\nclass NoResSelfOutput(BertSelfOutput):\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\nclass NoResOutput(BertOutput):\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\n\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\nexperiment_data = {}\n\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # Load and trim datasets\n    ds_train = (\n        load_dataset(ds, sub, split=\"train\") if sub else load_dataset(ds, split=\"train\")\n    )\n    ds_val = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n    ds_train = ds_train.shuffle(42).select(range(train_size))\n    ds_val = ds_val.shuffle(42).select(range(val_size))\n    texts_train, labels_train = ds_train[text_col], ds_train[label_col]\n    texts_val, labels_val = ds_val[text_col], ds_val[label_col]\n    paras = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_val)}\n\n    # Tokenize\n    tr_enc = tokenizer(texts_train, truncation=True, padding=True, return_tensors=\"pt\")\n    va_enc = tokenizer(texts_val, truncation=True, padding=True, return_tensors=\"pt\")\n    train_ds = TensorDataset(\n        tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n    )\n    val_ds = TensorDataset(\n        va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_val)\n    )\n    tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=bs)\n\n    # Load and patch model on CPU, then move to device\n    model = BertForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\", num_labels=2\n    )\n    for layer in model.bert.encoder.layer:\n        orig_so = layer.attention.output\n        new_so = NoResSelfOutput(model.config)\n        new_so.load_state_dict(orig_so.state_dict())\n        layer.attention.output = new_so\n        orig_fo = layer.output\n        new_fo = NoResOutput(model.config)\n        new_fo.load_state_dict(orig_fo.state_dict())\n        layer.output = new_fo\n    model.to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n\n    exp = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": list(labels_val),\n    }\n\n    for epoch in range(1, epochs + 1):\n        # Training\n        model.train()\n        train_losses = []\n        for ids, mask, labels in tr_loader:\n            ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n            optimizer.zero_grad()\n            out = model(input_ids=ids, attention_mask=mask, labels=labels)\n            out.loss.backward()\n            optimizer.step()\n            train_losses.append(out.loss.item())\n        train_loss = float(np.mean(train_losses))\n        exp[\"losses\"][\"train\"].append({\"epoch\": epoch, \"loss\": train_loss})\n\n        # Validation loss\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for ids, mask, labels in va_loader:\n                ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n                out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                val_losses.append(out.loss.item())\n        val_loss = float(np.mean(val_losses))\n        exp[\"losses\"][\"val\"].append({\"epoch\": epoch, \"loss\": val_loss})\n        print(f\"{name} Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n        # Hallucination detection metrics\n        uncs_vote, uncs_kl, errs = [], [], []\n        for i, txt in enumerate(texts_val):\n            probs, preds = [], []\n            for variant in [txt] + paras[i]:\n                enc = tokenizer(\n                    variant, return_tensors=\"pt\", truncation=True, padding=True\n                ).to(device)\n                with torch.no_grad():\n                    logits = model(**enc).logits\n                p = torch.softmax(logits, dim=-1).squeeze(0).cpu()\n                probs.append(p)\n                preds.append(int(p.argmax().item()))\n            maj = max(set(preds), key=preds.count)\n            uncs_vote.append(1 - preds.count(maj) / len(preds))\n            kl_vals = []\n            for a in range(len(probs)):\n                for b in range(a + 1, len(probs)):\n                    P, Q = probs[a], probs[b]\n                    kl_vals.append(\n                        0.5\n                        * (\n                            F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                            + F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                        )\n                    )\n            uncs_kl.append(float(np.mean(kl_vals)))\n            errs.append(int(preds[0] != int(labels_val[i])))\n        try:\n            auc_v = roc_auc_score(errs, uncs_vote)\n        except:\n            auc_v = 0.5\n        try:\n            auc_k = roc_auc_score(errs, uncs_kl)\n        except:\n            auc_k = 0.5\n        des_v, des_k = auc_v / (K + 1), auc_k / (K + 1)\n        try:\n            spear_v = spearmanr(uncs_vote, errs)[0]\n        except:\n            spear_v = 0.0\n        try:\n            spear_k = spearmanr(uncs_kl, errs)[0]\n        except:\n            spear_k = 0.0\n\n        exp[\"metrics\"][\"val\"].append(\n            {\n                \"epoch\": epoch,\n                \"auc_vote\": auc_v,\n                \"DES_vote\": des_v,\n                \"auc_kl\": auc_k,\n                \"DES_kl\": des_k,\n                \"spearman_vote\": spear_v,\n                \"spearman_kl\": spear_k,\n            }\n        )\n        print(\n            f\"{name} Epoch {epoch}: AUC_vote={auc_v:.4f}, DES_vote={des_v:.4f}, \"\n            f\"AUC_kl={auc_k:.4f}, DES_kl={des_k:.4f}, \"\n            f\"Spearman_vote={spear_v:.4f}, Spearman_kl={spear_k:.4f}\"\n        )\n\n    # Final predictions\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for ids, mask, _ in va_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            logits = model(input_ids=ids, attention_mask=mask).logits\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n    exp[\"predictions\"] = preds\n\n    experiment_data[name] = exp\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport random\nimport re\nimport nltk\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# prepare working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# download WordNet\nnltk.download(\"wordnet\", quiet=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\n# datasets and columns\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\n# hyperparameters\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\n\n# tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\n# prepare data container\nablation = \"BiasRemovalAblation\"\nexperiment_data = {ablation: {}}\n\nfor name, (ds, sub, text_col, label_col) in datasets_info.items():\n    # load and trim data\n    ds_train = (\n        load_dataset(ds, sub, split=\"train\") if sub else load_dataset(ds, split=\"train\")\n    )\n    ds_val = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n    ds_train = ds_train.shuffle(42).select(range(train_size))\n    ds_val = ds_val.shuffle(42).select(range(val_size))\n    texts_train, labels_train = ds_train[text_col], ds_train[label_col]\n    texts_val, labels_val = ds_val[text_col], ds_val[label_col]\n    paras = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_val)}\n    # tokenize\n    tr_enc = tokenizer(texts_train, truncation=True, padding=True, return_tensors=\"pt\")\n    va_enc = tokenizer(texts_val, truncation=True, padding=True, return_tensors=\"pt\")\n    train_ds = TensorDataset(\n        tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n    )\n    val_ds = TensorDataset(\n        va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_val)\n    )\n    tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    va_loader = DataLoader(val_ds, batch_size=bs)\n\n    # model and ablate biases\n    model = BertForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\", num_labels=2\n    ).to(device)\n    for m in model.modules():\n        if isinstance(m, torch.nn.Linear) and m.bias is not None:\n            m.bias.data.zero_()\n            m.bias.requires_grad = False\n    optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    # init storage\n    experiment_data[ablation][name] = {\n        \"losses\": {\"train\": [], \"val\": []},\n        \"metrics\": {\"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": list(labels_val),\n    }\n\n    # training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        train_losses = []\n        for ids, mask, labels in tr_loader:\n            ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n            optimizer.zero_grad()\n            out = model(input_ids=ids, attention_mask=mask, labels=labels)\n            out.loss.backward()\n            optimizer.step()\n            train_losses.append(out.loss.item())\n        experiment_data[ablation][name][\"losses\"][\"train\"].append(\n            float(np.mean(train_losses))\n        )\n\n        # validation loss\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for ids, mask, labels in va_loader:\n                ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n                out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                val_losses.append(out.loss.item())\n        experiment_data[ablation][name][\"losses\"][\"val\"].append(\n            float(np.mean(val_losses))\n        )\n\n        # detection metrics on val\n        uncs_vote, uncs_kl, errs = [], [], []\n        for i, txt in enumerate(texts_val):\n            probs, preds = [], []\n            for variant in [txt] + paras[i]:\n                enc = tokenizer(\n                    variant, return_tensors=\"pt\", truncation=True, padding=True\n                ).to(device)\n                with torch.no_grad():\n                    logits = model(**enc).logits\n                p = torch.softmax(logits, dim=-1).squeeze(0).cpu()\n                probs.append(p)\n                preds.append(int(p.argmax().item()))\n            maj = max(set(preds), key=preds.count)\n            uncs_vote.append(1 - preds.count(maj) / len(preds))\n            kl_vals = []\n            for a in range(len(probs)):\n                for b in range(a + 1, len(probs)):\n                    P, Q = probs[a], probs[b]\n                    kl_vals.append(\n                        0.5\n                        * (\n                            F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                            + F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                        )\n                    )\n            uncs_kl.append(float(np.mean(kl_vals)))\n            errs.append(int(preds[0] != int(labels_val[i])))\n        try:\n            auc_v = roc_auc_score(errs, uncs_vote)\n        except:\n            auc_v = 0.5\n        try:\n            auc_k = roc_auc_score(errs, uncs_kl)\n        except:\n            auc_k = 0.5\n        des_v = auc_v / (K + 1)\n        des_k = auc_k / (K + 1)\n        experiment_data[ablation][name][\"metrics\"][\"val\"].append(\n            {\n                \"epoch\": epoch,\n                \"auc_vote\": auc_v,\n                \"DES_vote\": des_v,\n                \"auc_kl\": auc_k,\n                \"DES_kl\": des_k,\n            }\n        )\n        print(\n            f\"{name} Epoch {epoch}: val_loss={experiment_data[ablation][name]['losses']['val'][-1]:.4f}, \"\n            f\"AUC_vote={auc_v:.4f}, DES_vote={des_v:.4f}, AUC_kl={auc_k:.4f}, DES_kl={des_k:.4f}\"\n        )\n\n    # final predictions\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for ids, mask, _ in va_loader:\n            ids, mask = ids.to(device), mask.to(device)\n            logits = model(input_ids=ids, attention_mask=mask).logits\n            preds.extend(torch.argmax(logits, -1).cpu().tolist())\n    experiment_data[ablation][name][\"predictions\"] = preds\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport random, re\nimport nltk\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification, BertConfig\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nnltk.download(\"wordnet\", quiet=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\n\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\n\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\nexperiment_data = {\"full_depth\": {}, \"reduced_depth\": {}}\n\nfor ablation in [\"full_depth\", \"reduced_depth\"]:\n    for name, (ds, sub, text_col, label_col) in datasets_info.items():\n        # load & trim\n        ds_train = (\n            load_dataset(ds, sub, split=\"train\")\n            if sub\n            else load_dataset(ds, split=\"train\")\n        )\n        ds_val = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n        ds_train = ds_train.shuffle(42).select(range(train_size))\n        ds_val = ds_val.shuffle(42).select(range(val_size))\n        texts_train, labels_train = ds_train[text_col], ds_train[label_col]\n        texts_val, labels_val = ds_val[text_col], ds_val[label_col]\n        paras = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_val)}\n        # tokenize\n        tr_enc = tokenizer(\n            texts_train, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n        va_enc = tokenizer(\n            texts_val, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n        train_ds = TensorDataset(\n            tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n        )\n        val_ds = TensorDataset(\n            va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_val)\n        )\n        tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n        va_loader = DataLoader(val_ds, batch_size=bs)\n        # build model\n        if ablation == \"full_depth\":\n            model = BertForSequenceClassification.from_pretrained(\n                \"bert-base-uncased\", num_labels=2\n            ).to(device)\n        else:\n            config = BertConfig.from_pretrained(\"bert-base-uncased\")\n            config.num_hidden_layers //= 2\n            model = BertForSequenceClassification(config)\n            pretrained = BertForSequenceClassification.from_pretrained(\n                \"bert-base-uncased\", num_labels=2\n            )\n            m_state = model.state_dict()\n            for k, v in pretrained.state_dict().items():\n                if k in m_state:\n                    m_state[k] = v\n            model.load_state_dict(m_state)\n            model.to(device)\n        optimizer = Adam(model.parameters(), lr=lr)\n        # prepare storage\n        experiment_data[ablation][name] = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"detection\": []},\n            \"predictions\": [],\n            \"ground_truth\": labels_val,\n        }\n        # train & eval\n        for epoch in range(1, epochs + 1):\n            model.train()\n            train_losses = []\n            for ids, mask, labels in tr_loader:\n                ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n                optimizer.zero_grad()\n                out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                out.loss.backward()\n                optimizer.step()\n                train_losses.append(out.loss.item())\n            experiment_data[ablation][name][\"losses\"][\"train\"].append(\n                {\"epoch\": epoch, \"loss\": float(np.mean(train_losses))}\n            )\n            model.eval()\n            val_losses = []\n            with torch.no_grad():\n                for ids, mask, labels in va_loader:\n                    ids, mask, labels = (\n                        ids.to(device),\n                        mask.to(device),\n                        labels.to(device),\n                    )\n                    out = model(input_ids=ids, attention_mask=mask, labels=labels)\n                    val_losses.append(out.loss.item())\n            vl = float(np.mean(val_losses))\n            experiment_data[ablation][name][\"losses\"][\"val\"].append(\n                {\"epoch\": epoch, \"loss\": vl}\n            )\n            print(f\"[{ablation}/{name}] Epoch {epoch}: val_loss={vl:.4f}\")\n            # detection\n            uncs_vote, uncs_kl, errs = [], [], []\n            for i, txt in enumerate(texts_val):\n                probs, preds = [], []\n                for v in [txt] + paras[i]:\n                    enc = tokenizer(\n                        v, truncation=True, padding=True, return_tensors=\"pt\"\n                    ).to(device)\n                    with torch.no_grad():\n                        logits = model(**enc).logits\n                    p = torch.softmax(logits, dim=-1).squeeze(0).cpu()\n                    probs.append(p)\n                    preds.append(int(p.argmax()))\n                maj = max(set(preds), key=preds.count)\n                uncs_vote.append(1 - preds.count(maj) / len(preds))\n                klv = []\n                for a in range(len(probs)):\n                    for b in range(a + 1, len(probs)):\n                        P, Q = probs[a], probs[b]\n                        klv.append(\n                            0.5\n                            * (\n                                F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                                + F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                            )\n                        )\n                uncs_kl.append(float(np.mean(klv)))\n                errs.append(int(preds[0] != int(labels_val[i])))\n            try:\n                auc_v = roc_auc_score(errs, uncs_vote)\n            except:\n                auc_v = 0.5\n            try:\n                auc_k = roc_auc_score(errs, uncs_kl)\n            except:\n                auc_k = 0.5\n            des_v, des_k = auc_v / (K + 1), auc_k / (K + 1)\n            experiment_data[ablation][name][\"metrics\"][\"detection\"].append(\n                {\n                    \"epoch\": epoch,\n                    \"auc_vote\": auc_v,\n                    \"DES_vote\": des_v,\n                    \"auc_kl\": auc_k,\n                    \"DES_kl\": des_k,\n                }\n            )\n            print(\n                f\"[{ablation}/{name}] AUC_vote={auc_v:.4f},DES_vote={des_v:.4f},AUC_kl={auc_k:.4f},DES_kl={des_k:.4f}\"\n            )\n        # final preds\n        preds = []\n        model.eval()\n        with torch.no_grad():\n            for ids, mask, _ in va_loader:\n                ids, mask = ids.to(device), mask.to(device)\n                logits = model(input_ids=ids, attention_mask=mask).logits\n                preds.extend(torch.argmax(logits, -1).cpu().tolist())\n        experiment_data[ablation][name][\"predictions\"] = preds\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, re\nimport nltk\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnltk.download(\"wordnet\", quiet=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\n# datasets and hyperparams\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\n# ablation settings\nablations = {\"baseline\": False, \"RandomInitEmbeddingAblation\": True}\nexperiment_data = {}\n\nfor ablation_name, do_reinit in ablations.items():\n    experiment_data[ablation_name] = {}\n    for name, (ds, sub, text_col, label_col) in datasets_info.items():\n        # load & subsample\n        ds_train = (\n            load_dataset(ds, sub, split=\"train\")\n            if sub\n            else load_dataset(ds, split=\"train\")\n        )\n        ds_val = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n        ds_train = ds_train.shuffle(42).select(range(train_size))\n        ds_val = ds_val.shuffle(42).select(range(val_size))\n        texts_train, labels_train = ds_train[text_col], ds_train[label_col]\n        texts_val, labels_val = ds_val[text_col], ds_val[label_col]\n        paras = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_val)}\n        # tokenize\n        tr_enc = tokenizer(\n            texts_train, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n        va_enc = tokenizer(\n            texts_val, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n        train_ds = TensorDataset(\n            tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n        )\n        val_ds = TensorDataset(\n            va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_val)\n        )\n        tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n        va_loader = DataLoader(val_ds, batch_size=bs)\n        # init storage\n        experiment_data[ablation_name][name] = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"detection\": []},\n            \"predictions\": [],\n            \"ground_truth\": labels_val.copy(),\n        }\n        # model & ablation\n        model = BertForSequenceClassification.from_pretrained(\n            \"bert-base-uncased\", num_labels=2\n        ).to(device)\n        if do_reinit:\n            emb = model.get_input_embeddings()\n            torch.nn.init.normal_(emb.weight, mean=0.0, std=0.02)\n        optimizer = Adam(model.parameters(), lr=lr)\n        # train/eval\n        for epoch in range(1, epochs + 1):\n            model.train()\n            tr_losses = []\n            for ids, mask, lbls in tr_loader:\n                ids, mask, lbls = ids.to(device), mask.to(device), lbls.to(device)\n                optimizer.zero_grad()\n                out = model(input_ids=ids, attention_mask=mask, labels=lbls)\n                out.loss.backward()\n                optimizer.step()\n                tr_losses.append(out.loss.item())\n            experiment_data[ablation_name][name][\"losses\"][\"train\"].append(\n                {\"epoch\": epoch, \"loss\": float(np.mean(tr_losses))}\n            )\n            model.eval()\n            va_losses = []\n            with torch.no_grad():\n                for ids, mask, lbls in va_loader:\n                    ids, mask, lbls = ids.to(device), mask.to(device), lbls.to(device)\n                    out = model(input_ids=ids, attention_mask=mask, labels=lbls)\n                    va_losses.append(out.loss.item())\n            val_loss = float(np.mean(va_losses))\n            experiment_data[ablation_name][name][\"losses\"][\"val\"].append(\n                {\"epoch\": epoch, \"loss\": val_loss}\n            )\n            # detection metrics\n            uncs_vote, uncs_kl, errs = [], [], []\n            for i, txt in enumerate(texts_val):\n                probs, preds = [], []\n                for var in [txt] + paras[i]:\n                    enc = tokenizer(\n                        var, return_tensors=\"pt\", truncation=True, padding=True\n                    ).to(device)\n                    with torch.no_grad():\n                        logits = model(**enc).logits\n                    p = torch.softmax(logits, dim=-1).squeeze(0).cpu()\n                    probs.append(p)\n                    preds.append(int(p.argmax().item()))\n                maj = max(set(preds), key=preds.count)\n                uncs_vote.append(1 - preds.count(maj) / len(preds))\n                kl_vals = []\n                for a in range(len(probs)):\n                    for b in range(a + 1, len(probs)):\n                        P, Q = probs[a], probs[b]\n                        kl1 = F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                        kl2 = F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                        kl_vals.append(0.5 * (kl1 + kl2))\n                uncs_kl.append(float(np.mean(kl_vals)))\n                errs.append(int(preds[0] != labels_val[i]))\n            auc_v = roc_auc_score(errs, uncs_vote) if len(set(errs)) > 1 else 0.5\n            auc_k = roc_auc_score(errs, uncs_kl) if len(set(errs)) > 1 else 0.5\n            des_v = auc_v / (K + 1)\n            des_k = auc_k / (K + 1)\n            experiment_data[ablation_name][name][\"metrics\"][\"detection\"].append(\n                {\n                    \"epoch\": epoch,\n                    \"auc_vote\": auc_v,\n                    \"DES_vote\": des_v,\n                    \"auc_kl\": auc_k,\n                    \"DES_kl\": des_k,\n                }\n            )\n        # final predictions\n        preds = []\n        model.eval()\n        with torch.no_grad():\n            for ids, mask, _ in va_loader:\n                ids, mask = ids.to(device), mask.to(device)\n                logits = model(input_ids=ids, attention_mask=mask).logits\n                preds.extend(torch.argmax(logits, -1).cpu().tolist())\n        experiment_data[ablation_name][name][\"predictions\"] = preds\n\n# save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, re\nimport nltk\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnltk.download(\"wordnet\", quiet=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\n# datasets and hyperparams\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\n# ablation settings\nablations = {\"baseline\": False, \"RandomInitEmbeddingAblation\": True}\nexperiment_data = {}\n\nfor ablation_name, do_reinit in ablations.items():\n    experiment_data[ablation_name] = {}\n    for name, (ds, sub, text_col, label_col) in datasets_info.items():\n        # load & subsample\n        ds_train = (\n            load_dataset(ds, sub, split=\"train\")\n            if sub\n            else load_dataset(ds, split=\"train\")\n        )\n        ds_val = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n        ds_train = ds_train.shuffle(42).select(range(train_size))\n        ds_val = ds_val.shuffle(42).select(range(val_size))\n        texts_train, labels_train = ds_train[text_col], ds_train[label_col]\n        texts_val, labels_val = ds_val[text_col], ds_val[label_col]\n        paras = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_val)}\n        # tokenize\n        tr_enc = tokenizer(\n            texts_train, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n        va_enc = tokenizer(\n            texts_val, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n        train_ds = TensorDataset(\n            tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n        )\n        val_ds = TensorDataset(\n            va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_val)\n        )\n        tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n        va_loader = DataLoader(val_ds, batch_size=bs)\n        # init storage\n        experiment_data[ablation_name][name] = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"detection\": []},\n            \"predictions\": [],\n            \"ground_truth\": labels_val.copy(),\n        }\n        # model & ablation\n        model = BertForSequenceClassification.from_pretrained(\n            \"bert-base-uncased\", num_labels=2\n        ).to(device)\n        if do_reinit:\n            emb = model.get_input_embeddings()\n            torch.nn.init.normal_(emb.weight, mean=0.0, std=0.02)\n        optimizer = Adam(model.parameters(), lr=lr)\n        # train/eval\n        for epoch in range(1, epochs + 1):\n            model.train()\n            tr_losses = []\n            for ids, mask, lbls in tr_loader:\n                ids, mask, lbls = ids.to(device), mask.to(device), lbls.to(device)\n                optimizer.zero_grad()\n                out = model(input_ids=ids, attention_mask=mask, labels=lbls)\n                out.loss.backward()\n                optimizer.step()\n                tr_losses.append(out.loss.item())\n            experiment_data[ablation_name][name][\"losses\"][\"train\"].append(\n                {\"epoch\": epoch, \"loss\": float(np.mean(tr_losses))}\n            )\n            model.eval()\n            va_losses = []\n            with torch.no_grad():\n                for ids, mask, lbls in va_loader:\n                    ids, mask, lbls = ids.to(device), mask.to(device), lbls.to(device)\n                    out = model(input_ids=ids, attention_mask=mask, labels=lbls)\n                    va_losses.append(out.loss.item())\n            val_loss = float(np.mean(va_losses))\n            experiment_data[ablation_name][name][\"losses\"][\"val\"].append(\n                {\"epoch\": epoch, \"loss\": val_loss}\n            )\n            # detection metrics\n            uncs_vote, uncs_kl, errs = [], [], []\n            for i, txt in enumerate(texts_val):\n                probs, preds = [], []\n                for var in [txt] + paras[i]:\n                    enc = tokenizer(\n                        var, return_tensors=\"pt\", truncation=True, padding=True\n                    ).to(device)\n                    with torch.no_grad():\n                        logits = model(**enc).logits\n                    p = torch.softmax(logits, dim=-1).squeeze(0).cpu()\n                    probs.append(p)\n                    preds.append(int(p.argmax().item()))\n                maj = max(set(preds), key=preds.count)\n                uncs_vote.append(1 - preds.count(maj) / len(preds))\n                kl_vals = []\n                for a in range(len(probs)):\n                    for b in range(a + 1, len(probs)):\n                        P, Q = probs[a], probs[b]\n                        kl1 = F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                        kl2 = F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                        kl_vals.append(0.5 * (kl1 + kl2))\n                uncs_kl.append(float(np.mean(kl_vals)))\n                errs.append(int(preds[0] != labels_val[i]))\n            auc_v = roc_auc_score(errs, uncs_vote) if len(set(errs)) > 1 else 0.5\n            auc_k = roc_auc_score(errs, uncs_kl) if len(set(errs)) > 1 else 0.5\n            des_v = auc_v / (K + 1)\n            des_k = auc_k / (K + 1)\n            experiment_data[ablation_name][name][\"metrics\"][\"detection\"].append(\n                {\n                    \"epoch\": epoch,\n                    \"auc_vote\": auc_v,\n                    \"DES_vote\": des_v,\n                    \"auc_kl\": auc_k,\n                    \"DES_kl\": des_k,\n                }\n            )\n        # final predictions\n        preds = []\n        model.eval()\n        with torch.no_grad():\n            for ids, mask, _ in va_loader:\n                ids, mask = ids.to(device), mask.to(device)\n                logits = model(input_ids=ids, attention_mask=mask).logits\n                preds.extend(torch.argmax(logits, -1).cpu().tolist())\n        experiment_data[ablation_name][name][\"predictions\"] = preds\n\n# save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os, random, re\nimport nltk\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom torch.optim import Adam\nfrom sklearn.metrics import roc_auc_score\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnltk.download(\"wordnet\", quiet=True)\n\n\ndef generate_paraphrases(text, K):\n    words = text.split()\n    paras = []\n    for _ in range(K):\n        new = words.copy()\n        for idx in random.sample(range(len(words)), min(2, len(words))):\n            w = re.sub(r\"\\W+\", \"\", words[idx])\n            syns = wordnet.synsets(w)\n            lemmas = {\n                l.name().replace(\"_\", \" \")\n                for s in syns\n                for l in s.lemmas()\n                if l.name().lower() != w.lower()\n            }\n            if lemmas:\n                new[idx] = random.choice(list(lemmas))\n        paras.append(\" \".join(new))\n    return paras\n\n\n# datasets and hyperparams\ndatasets_info = {\n    \"sst2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n    \"yelp_polarity\": (\"yelp_polarity\", None, \"text\", \"label\"),\n    \"imdb\": (\"imdb\", None, \"text\", \"label\"),\n}\ntrain_size, val_size = 5000, 500\nK, epochs, bs, lr = 5, 5, 32, 2e-5\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\n# ablation settings\nablations = {\"baseline\": False, \"RandomInitEmbeddingAblation\": True}\nexperiment_data = {}\n\nfor ablation_name, do_reinit in ablations.items():\n    experiment_data[ablation_name] = {}\n    for name, (ds, sub, text_col, label_col) in datasets_info.items():\n        # load & subsample\n        ds_train = (\n            load_dataset(ds, sub, split=\"train\")\n            if sub\n            else load_dataset(ds, split=\"train\")\n        )\n        ds_val = load_dataset(ds, sub, split=\"validation\" if sub else \"test\")\n        ds_train = ds_train.shuffle(42).select(range(train_size))\n        ds_val = ds_val.shuffle(42).select(range(val_size))\n        texts_train, labels_train = ds_train[text_col], ds_train[label_col]\n        texts_val, labels_val = ds_val[text_col], ds_val[label_col]\n        paras = {i: generate_paraphrases(t, K) for i, t in enumerate(texts_val)}\n        # tokenize\n        tr_enc = tokenizer(\n            texts_train, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n        va_enc = tokenizer(\n            texts_val, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n        train_ds = TensorDataset(\n            tr_enc[\"input_ids\"], tr_enc[\"attention_mask\"], torch.tensor(labels_train)\n        )\n        val_ds = TensorDataset(\n            va_enc[\"input_ids\"], va_enc[\"attention_mask\"], torch.tensor(labels_val)\n        )\n        tr_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n        va_loader = DataLoader(val_ds, batch_size=bs)\n        # init storage\n        experiment_data[ablation_name][name] = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"detection\": []},\n            \"predictions\": [],\n            \"ground_truth\": labels_val.copy(),\n        }\n        # model & ablation\n        model = BertForSequenceClassification.from_pretrained(\n            \"bert-base-uncased\", num_labels=2\n        ).to(device)\n        if do_reinit:\n            emb = model.get_input_embeddings()\n            torch.nn.init.normal_(emb.weight, mean=0.0, std=0.02)\n        optimizer = Adam(model.parameters(), lr=lr)\n        # train/eval\n        for epoch in range(1, epochs + 1):\n            model.train()\n            tr_losses = []\n            for ids, mask, lbls in tr_loader:\n                ids, mask, lbls = ids.to(device), mask.to(device), lbls.to(device)\n                optimizer.zero_grad()\n                out = model(input_ids=ids, attention_mask=mask, labels=lbls)\n                out.loss.backward()\n                optimizer.step()\n                tr_losses.append(out.loss.item())\n            experiment_data[ablation_name][name][\"losses\"][\"train\"].append(\n                {\"epoch\": epoch, \"loss\": float(np.mean(tr_losses))}\n            )\n            model.eval()\n            va_losses = []\n            with torch.no_grad():\n                for ids, mask, lbls in va_loader:\n                    ids, mask, lbls = ids.to(device), mask.to(device), lbls.to(device)\n                    out = model(input_ids=ids, attention_mask=mask, labels=lbls)\n                    va_losses.append(out.loss.item())\n            val_loss = float(np.mean(va_losses))\n            experiment_data[ablation_name][name][\"losses\"][\"val\"].append(\n                {\"epoch\": epoch, \"loss\": val_loss}\n            )\n            # detection metrics\n            uncs_vote, uncs_kl, errs = [], [], []\n            for i, txt in enumerate(texts_val):\n                probs, preds = [], []\n                for var in [txt] + paras[i]:\n                    enc = tokenizer(\n                        var, return_tensors=\"pt\", truncation=True, padding=True\n                    ).to(device)\n                    with torch.no_grad():\n                        logits = model(**enc).logits\n                    p = torch.softmax(logits, dim=-1).squeeze(0).cpu()\n                    probs.append(p)\n                    preds.append(int(p.argmax().item()))\n                maj = max(set(preds), key=preds.count)\n                uncs_vote.append(1 - preds.count(maj) / len(preds))\n                kl_vals = []\n                for a in range(len(probs)):\n                    for b in range(a + 1, len(probs)):\n                        P, Q = probs[a], probs[b]\n                        kl1 = F.kl_div(Q.log(), P, reduction=\"sum\").item()\n                        kl2 = F.kl_div(P.log(), Q, reduction=\"sum\").item()\n                        kl_vals.append(0.5 * (kl1 + kl2))\n                uncs_kl.append(float(np.mean(kl_vals)))\n                errs.append(int(preds[0] != labels_val[i]))\n            auc_v = roc_auc_score(errs, uncs_vote) if len(set(errs)) > 1 else 0.5\n            auc_k = roc_auc_score(errs, uncs_kl) if len(set(errs)) > 1 else 0.5\n            des_v = auc_v / (K + 1)\n            des_k = auc_k / (K + 1)\n            experiment_data[ablation_name][name][\"metrics\"][\"detection\"].append(\n                {\n                    \"epoch\": epoch,\n                    \"auc_vote\": auc_v,\n                    \"DES_vote\": des_v,\n                    \"auc_kl\": auc_k,\n                    \"DES_kl\": des_k,\n                }\n            )\n        # final predictions\n        preds = []\n        model.eval()\n        with torch.no_grad():\n            for ids, mask, _ in va_loader:\n                ids, mask = ids.to(device), mask.to(device)\n                logits = model(input_ids=ids, attention_mask=mask).logits\n                preds.extend(torch.argmax(logits, -1).cpu().tolist())\n        experiment_data[ablation_name][name][\"predictions\"] = preds\n\n# save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', \"Some weights of\nDistilBertForSequenceClassification were not initialized from the model\ncheckpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.3759', '\\n', 'Epoch 1: detection_auc = 0.5068, DES =\n0.1267', '\\n', 'Epoch 2: validation_loss = 0.3352', '\\n', 'Epoch 2:\ndetection_auc = 0.6217, DES = 0.1554', '\\n', 'Epoch 3: validation_loss =\n0.4326', '\\n', 'Epoch 3: detection_auc = 0.6332, DES = 0.1583', '\\n', \"Some\nweights of DistilBertForSequenceClassification were not initialized from the\nmodel checkpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.2447', '\\n', 'Epoch 1: detection_auc = 0.4809, DES =\n0.1202', '\\n', 'Epoch 2: validation_loss = 0.2364', '\\n', 'Epoch 2:\ndetection_auc = 0.5443, DES = 0.1361', '\\n', 'Epoch 3: validation_loss =\n0.3306', '\\n', 'Epoch 3: detection_auc = 0.5486, DES = 0.1372', '\\n', \"Some\nweights of DistilBertForSequenceClassification were not initialized from the\nmodel checkpoint at distilbert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight', 'pre_classifier.bias',\n'pre_classifier.weight']\\nYou should probably TRAIN this model on a down-stream\ntask to be able to use it for predictions and inference.\\n\", 'Epoch 1:\nvalidation_loss = 0.4082', '\\n', 'Epoch 1: detection_auc = 0.5177, DES =\n0.1294', '\\n', 'Epoch 2: validation_loss = 0.3133', '\\n', 'Epoch 2:\ndetection_auc = 0.4829, DES = 0.1207', '\\n', 'Epoch 3: validation_loss =\n0.4118', '\\n', 'Epoch 3: detection_auc = 0.5090, DES = 0.1273', '\\n', 'Execution\ntime: 2 minutes seconds (time limit is an hour).']", "[\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'sst2 Epoch 1: val_loss=0.6943', '\\n', '  AUC_vote=0.5011, DES_vote=0.0835,\nAUC_kl=0.5161, DES_kl=0.0860', '\\n', 'sst2 Epoch 2: val_loss=0.6925', '\\n', '\nAUC_vote=0.4966, DES_vote=0.0828, AUC_kl=0.5194, DES_kl=0.0866', '\\n', 'sst2\nEpoch 3: val_loss=0.6925', '\\n', '  AUC_vote=0.4998, DES_vote=0.0833,\nAUC_kl=0.5203, DES_kl=0.0867', '\\n', 'sst2 Epoch 4: val_loss=0.6885', '\\n', '\nAUC_vote=0.4943, DES_vote=0.0824, AUC_kl=0.5185, DES_kl=0.0864', '\\n', 'sst2\nEpoch 5: val_loss=0.6846', '\\n', '  AUC_vote=0.5145, DES_vote=0.0857,\nAUC_kl=0.5176, DES_kl=0.0863', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'yelp_polarity Epoch 1:\nval_loss=0.6964', '\\n', '  AUC_vote=0.4918, DES_vote=0.0820, AUC_kl=0.4780,\nDES_kl=0.0797', '\\n', 'yelp_polarity Epoch 2: val_loss=0.6853', '\\n', '\nAUC_vote=0.5355, DES_vote=0.0893, AUC_kl=0.5112, DES_kl=0.0852', '\\n',\n'yelp_polarity Epoch 3: val_loss=0.6802', '\\n', '  AUC_vote=0.5335,\nDES_vote=0.0889, AUC_kl=0.5268, DES_kl=0.0878', '\\n', 'yelp_polarity Epoch 4:\nval_loss=0.6777', '\\n', '  AUC_vote=0.5170, DES_vote=0.0862, AUC_kl=0.5387,\nDES_kl=0.0898', '\\n', 'yelp_polarity Epoch 5: val_loss=0.6693', '\\n', '\nAUC_vote=0.5314, DES_vote=0.0886, AUC_kl=0.5257, DES_kl=0.0876', '\\n', \"Some\nweights of BertForSequenceClassification were not initialized from the model\ncheckpoint at bert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'imdb Epoch 1:\nval_loss=0.6921', '\\n', '  AUC_vote=0.5141, DES_vote=0.0857, AUC_kl=0.4949,\nDES_kl=0.0825', '\\n', 'imdb Epoch 2: val_loss=0.6895', '\\n', '  AUC_vote=0.5239,\nDES_vote=0.0873, AUC_kl=0.5053, DES_kl=0.0842', '\\n', 'imdb Epoch 3:\nval_loss=0.6861', '\\n', '  AUC_vote=0.5428, DES_vote=0.0905, AUC_kl=0.5114,\nDES_kl=0.0852', '\\n', 'imdb Epoch 4: val_loss=0.6828', '\\n', '  AUC_vote=0.5486,\nDES_vote=0.0914, AUC_kl=0.5119, DES_kl=0.0853', '\\n', 'imdb Epoch 5:\nval_loss=0.6797', '\\n', '  AUC_vote=0.5218, DES_vote=0.0870, AUC_kl=0.5111,\nDES_kl=0.0852', '\\n', 'Execution time: 12 minutes seconds (time limit is an\nhour).']", "[\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'full_ft/sst2 Epoch 1: val_loss=0.3669', '\\n', 'full_ft/sst2 Epoch 1:\nAUC_vote=0.6425, DES_vote=0.1071, AUC_kl=0.7523, DES_kl=0.1254', '\\n',\n'full_ft/sst2 Epoch 2: val_loss=0.3632', '\\n', 'full_ft/sst2 Epoch 2:\nAUC_vote=0.6780, DES_vote=0.1130, AUC_kl=0.7816, DES_kl=0.1303', '\\n',\n'full_ft/sst2 Epoch 3: val_loss=0.3282', '\\n', 'full_ft/sst2 Epoch 3:\nAUC_vote=0.6722, DES_vote=0.1120, AUC_kl=0.7685, DES_kl=0.1281', '\\n',\n'full_ft/sst2 Epoch 4: val_loss=0.3674', '\\n', 'full_ft/sst2 Epoch 4:\nAUC_vote=0.6475, DES_vote=0.1079, AUC_kl=0.7493, DES_kl=0.1249', '\\n',\n'full_ft/sst2 Epoch 5: val_loss=0.4480', '\\n', 'full_ft/sst2 Epoch 5:\nAUC_vote=0.7100, DES_vote=0.1183, AUC_kl=0.8030, DES_kl=0.1338', '\\n', \"Some\nweights of BertForSequenceClassification were not initialized from the model\ncheckpoint at bert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'full_ft/yelp_polarity\nEpoch 1: val_loss=0.1446', '\\n', 'full_ft/yelp_polarity Epoch 1:\nAUC_vote=0.5744, DES_vote=0.0957, AUC_kl=0.8155, DES_kl=0.1359', '\\n',\n'full_ft/yelp_polarity Epoch 2: val_loss=0.1104', '\\n', 'full_ft/yelp_polarity\nEpoch 2: AUC_vote=0.5867, DES_vote=0.0978, AUC_kl=0.8991, DES_kl=0.1498', '\\n',\n'full_ft/yelp_polarity Epoch 3: val_loss=0.1715', '\\n', 'full_ft/yelp_polarity\nEpoch 3: AUC_vote=0.6127, DES_vote=0.1021, AUC_kl=0.9021, DES_kl=0.1504', '\\n',\n'full_ft/yelp_polarity Epoch 4: val_loss=0.1555', '\\n', 'full_ft/yelp_polarity\nEpoch 4: AUC_vote=0.6195, DES_vote=0.1032, AUC_kl=0.8823, DES_kl=0.1471', '\\n',\n'full_ft/yelp_polarity Epoch 5: val_loss=0.2268', '\\n', 'full_ft/yelp_polarity\nEpoch 5: AUC_vote=0.5622, DES_vote=0.0937, AUC_kl=0.8593, DES_kl=0.1432', '\\n',\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'full_ft/imdb Epoch 1: val_loss=0.2426', '\\n', 'full_ft/imdb Epoch 1:\nAUC_vote=0.5512, DES_vote=0.0919, AUC_kl=0.8662, DES_kl=0.1444', '\\n',\n'full_ft/imdb Epoch 2: val_loss=0.2374', '\\n', 'full_ft/imdb Epoch 2:\nAUC_vote=0.5612, DES_vote=0.0935, AUC_kl=0.8613, DES_kl=0.1435', '\\n',\n'full_ft/imdb Epoch 3: val_loss=0.2710', '\\n', 'full_ft/imdb Epoch 3:\nAUC_vote=0.6157, DES_vote=0.1026, AUC_kl=0.8636, DES_kl=0.1439', '\\n',\n'full_ft/imdb Epoch 4: val_loss=0.2434', '\\n', 'full_ft/imdb Epoch 4:\nAUC_vote=0.6381, DES_vote=0.1064, AUC_kl=0.8646, DES_kl=0.1441', '\\n',\n'full_ft/imdb Epoch 5: val_loss=0.2892', '\\n', 'full_ft/imdb Epoch 5:\nAUC_vote=0.6294, DES_vote=0.1049, AUC_kl=0.8523, DES_kl=0.1421', '\\n', \"Some\nweights of BertForSequenceClassification were not initialized from the model\ncheckpoint at bert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'freeze_4/sst2 Epoch 1:\nval_loss=0.2983', '\\n', 'freeze_4/sst2 Epoch 1: AUC_vote=0.5958,\nDES_vote=0.0993, AUC_kl=0.6356, DES_kl=0.1059', '\\n', 'freeze_4/sst2 Epoch 2:\nval_loss=0.2680', '\\n', 'freeze_4/sst2 Epoch 2: AUC_vote=0.6411,\nDES_vote=0.1069, AUC_kl=0.7355, DES_kl=0.1226', '\\n', 'freeze_4/sst2 Epoch 3:\nval_loss=0.2784', '\\n', 'freeze_4/sst2 Epoch 3: AUC_vote=0.6880,\nDES_vote=0.1147, AUC_kl=0.7813, DES_kl=0.1302', '\\n', 'freeze_4/sst2 Epoch 4:\nval_loss=0.3322', '\\n', 'freeze_4/sst2 Epoch 4: AUC_vote=0.6782,\nDES_vote=0.1130, AUC_kl=0.7705, DES_kl=0.1284', '\\n', 'freeze_4/sst2 Epoch 5:\nval_loss=0.4171', '\\n', 'freeze_4/sst2 Epoch 5: AUC_vote=0.5948,\nDES_vote=0.0991, AUC_kl=0.7035, DES_kl=0.1173', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'freeze_4/yelp_polarity\nEpoch 1: val_loss=0.1452', '\\n', 'freeze_4/yelp_polarity Epoch 1:\nAUC_vote=0.5628, DES_vote=0.0938, AUC_kl=0.8485, DES_kl=0.1414', '\\n',\n'freeze_4/yelp_polarity Epoch 2: val_loss=0.1535', '\\n', 'freeze_4/yelp_polarity\nEpoch 2: AUC_vote=0.5471, DES_vote=0.0912, AUC_kl=0.8420, DES_kl=0.1403', '\\n',\n'freeze_4/yelp_polarity Epoch 3: val_loss=0.1913', '\\n', 'freeze_4/yelp_polarity\nEpoch 3: AUC_vote=0.6309, DES_vote=0.1051, AUC_kl=0.8909, DES_kl=0.1485', '\\n',\n'freeze_4/yelp_polarity Epoch 4: val_loss=0.1604', '\\n', 'freeze_4/yelp_polarity\nEpoch 4: AUC_vote=0.6488, DES_vote=0.1081, AUC_kl=0.8725, DES_kl=0.1454', '\\n',\n'freeze_4/yelp_polarity Epoch 5: val_loss=0.2554', '\\n', 'freeze_4/yelp_polarity\nEpoch 5: AUC_vote=0.6050, DES_vote=0.1008, AUC_kl=0.8535, DES_kl=0.1423', '\\n',\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'freeze_4/imdb Epoch 1: val_loss=0.2452', '\\n', 'freeze_4/imdb Epoch 1:\nAUC_vote=0.5992, DES_vote=0.0999, AUC_kl=0.8139, DES_kl=0.1357', '\\n',\n'freeze_4/imdb Epoch 2: val_loss=0.2246', '\\n', 'freeze_4/imdb Epoch 2:\nAUC_vote=0.5507, DES_vote=0.0918, AUC_kl=0.8124, DES_kl=0.1354', '\\n',\n'freeze_4/imdb Epoch 3: val_loss=0.2552', '\\n', 'freeze_4/imdb Epoch 3:\nAUC_vote=0.5785, DES_vote=0.0964, AUC_kl=0.8186, DES_kl=0.1364', '\\n',\n'freeze_4/imdb Epoch 4: val_loss=0.2948', '\\n', 'freeze_4/imdb Epoch 4:\nAUC_vote=0.5913, DES_vote=0.0986, AUC_kl=0.8291, DES_kl=0.1382', '\\n',\n'freeze_4/imdb Epoch 5: val_loss=0.4055', '\\n', 'freeze_4/imdb Epoch 5:\nAUC_vote=0.5856, DES_vote=0.0976, AUC_kl=0.7607, DES_kl=0.1268', '\\n', \"Some\nweights of BertForSequenceClassification were not initialized from the model\ncheckpoint at bert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'freeze_8/sst2 Epoch 1:\nval_loss=0.2915', '\\n', 'freeze_8/sst2 Epoch 1: AUC_vote=0.6468,\nDES_vote=0.1078, AUC_kl=0.7149, DES_kl=0.1191', '\\n', 'freeze_8/sst2 Epoch 2:\nval_loss=0.2704', '\\n', 'freeze_8/sst2 Epoch 2: AUC_vote=0.6268,\nDES_vote=0.1045, AUC_kl=0.7167, DES_kl=0.1194', '\\n', 'freeze_8/sst2 Epoch 3:\nval_loss=0.2858', '\\n', 'freeze_8/sst2 Epoch 3: AUC_vote=0.6926,\nDES_vote=0.1154, AUC_kl=0.7597, DES_kl=0.1266', '\\n', 'freeze_8/sst2 Epoch 4:\nval_loss=0.3670', '\\n', 'freeze_8/sst2 Epoch 4: AUC_vote=0.6864,\nDES_vote=0.1144, AUC_kl=0.7637, DES_kl=0.1273', '\\n', 'freeze_8/sst2 Epoch 5:\nval_loss=0.3418', '\\n', 'freeze_8/sst2 Epoch 5: AUC_vote=0.7039,\nDES_vote=0.1173, AUC_kl=0.7640, DES_kl=0.1273', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'freeze_8/yelp_polarity\nEpoch 1: val_loss=0.2156', '\\n', 'freeze_8/yelp_polarity Epoch 1:\nAUC_vote=0.6219, DES_vote=0.1036, AUC_kl=0.8515, DES_kl=0.1419', '\\n',\n'freeze_8/yelp_polarity Epoch 2: val_loss=0.1887', '\\n', 'freeze_8/yelp_polarity\nEpoch 2: AUC_vote=0.6205, DES_vote=0.1034, AUC_kl=0.8675, DES_kl=0.1446', '\\n',\n'freeze_8/yelp_polarity Epoch 3: val_loss=0.1853', '\\n', 'freeze_8/yelp_polarity\nEpoch 3: AUC_vote=0.5608, DES_vote=0.0935, AUC_kl=0.8647, DES_kl=0.1441', '\\n',\n'freeze_8/yelp_polarity Epoch 4: val_loss=0.2258', '\\n', 'freeze_8/yelp_polarity\nEpoch 4: AUC_vote=0.5643, DES_vote=0.0941, AUC_kl=0.8261, DES_kl=0.1377', '\\n',\n'freeze_8/yelp_polarity Epoch 5: val_loss=0.2364', '\\n', 'freeze_8/yelp_polarity\nEpoch 5: AUC_vote=0.6052, DES_vote=0.1009, AUC_kl=0.8846, DES_kl=0.1474', '\\n',\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'freeze_8/imdb Epoch 1: val_loss=0.2550', '\\n', 'freeze_8/imdb Epoch 1:\nAUC_vote=0.5690, DES_vote=0.0948, AUC_kl=0.8249, DES_kl=0.1375', '\\n',\n'Traceback (most recent call last):\\n  File \"runfile.py\", line 118, in\n<module>\\n    train_losses.append(out.loss.item())\\n\n^^^^^^^^^^^^^^^\\nKeyboardInterrupt\\n', 'TimeoutError: Execution exceeded the\ntime limit of an hour']", "['Using device: cuda', '\\n', 'sst2 Epoch 1: val_loss=0.6963', '\\n', 'sst2 Epoch\n1: AUC_v=0.500, DES_v=0.083, AUC_k=0.522, DES_k=0.087', '\\n', 'sst2 Epoch 2:\nval_loss=0.6233', '\\n', 'sst2 Epoch 2: AUC_v=0.516, DES_v=0.086, AUC_k=0.460,\nDES_k=0.077', '\\n', 'sst2 Epoch 3: val_loss=0.5854', '\\n', 'sst2 Epoch 3:\nAUC_v=0.565, DES_v=0.094, AUC_k=0.587, DES_k=0.098', '\\n', 'sst2 Epoch 4:\nval_loss=0.5348', '\\n', 'sst2 Epoch 4: AUC_v=0.604, DES_v=0.101, AUC_k=0.591,\nDES_k=0.099', '\\n', 'sst2 Epoch 5: val_loss=0.6035', '\\n', 'sst2 Epoch 5:\nAUC_v=0.537, DES_v=0.089, AUC_k=0.537, DES_k=0.089', '\\n', 'yelp_polarity Epoch\n1: val_loss=0.6947', '\\n', 'yelp_polarity Epoch 1: AUC_v=0.500, DES_v=0.083,\nAUC_k=0.434, DES_k=0.072', '\\n', 'yelp_polarity Epoch 2: val_loss=0.3867', '\\n',\n'yelp_polarity Epoch 2: AUC_v=0.581, DES_v=0.097, AUC_k=0.716, DES_k=0.119',\n'\\n', 'yelp_polarity Epoch 3: val_loss=0.3683', '\\n', 'yelp_polarity Epoch 3:\nAUC_v=0.543, DES_v=0.090, AUC_k=0.782, DES_k=0.130', '\\n', 'yelp_polarity Epoch\n4: val_loss=0.3232', '\\n', 'yelp_polarity Epoch 4: AUC_v=0.583, DES_v=0.097,\nAUC_k=0.862, DES_k=0.144', '\\n', 'yelp_polarity Epoch 5: val_loss=0.2818', '\\n',\n'yelp_polarity Epoch 5: AUC_v=0.648, DES_v=0.108, AUC_k=0.861, DES_k=0.143',\n'\\n', 'imdb Epoch 1: val_loss=0.7030', '\\n', 'imdb Epoch 1: AUC_v=0.500,\nDES_v=0.083, AUC_k=0.463, DES_k=0.077', '\\n', 'imdb Epoch 2: val_loss=0.7144',\n'\\n', 'imdb Epoch 2: AUC_v=0.500, DES_v=0.083, AUC_k=0.488, DES_k=0.081', '\\n',\n'imdb Epoch 3: val_loss=0.6928', '\\n', 'imdb Epoch 3: AUC_v=0.500, DES_v=0.083,\nAUC_k=0.544, DES_k=0.091', '\\n', 'imdb Epoch 4: val_loss=0.6906', '\\n', 'imdb\nEpoch 4: AUC_v=0.500, DES_v=0.083, AUC_k=0.535, DES_k=0.089', '\\n', 'imdb Epoch\n5: val_loss=0.5007', '\\n', 'imdb Epoch 5: AUC_v=0.506, DES_v=0.084, AUC_k=0.665,\nDES_k=0.111', '\\n', 'Execution time: 23 minutes seconds (time limit is an\nhour).']", "[\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'sst2 Epoch 1 | tr_loss=0.4459 val_loss=0.5307 tr_acc=0.8924 val_acc=0.7820\nAUC_vote=0.5358 DES_vote=0.0893 AUC_kl=0.6520 DES_kl=0.1087', '\\n', 'sst2 Epoch\n2 | tr_loss=0.2535 val_loss=0.4463 tr_acc=0.9692 val_acc=0.8260 AUC_vote=0.5874\nDES_vote=0.0979 AUC_kl=0.6562 DES_kl=0.1094', '\\n', 'sst2 Epoch 3 |\ntr_loss=0.1436 val_loss=0.5191 tr_acc=0.9884 val_acc=0.8100 AUC_vote=0.5762\nDES_vote=0.0960 AUC_kl=0.6766 DES_kl=0.1128', '\\n', 'sst2 Epoch 4 |\ntr_loss=0.0942 val_loss=0.6017 tr_acc=0.9946 val_acc=0.8020 AUC_vote=0.5814\nDES_vote=0.0969 AUC_kl=0.6994 DES_kl=0.1166', '\\n', 'sst2 Epoch 5 |\ntr_loss=0.0442 val_loss=0.8616 tr_acc=0.9918 val_acc=0.7660 AUC_vote=0.5541\nDES_vote=0.0924 AUC_kl=0.6655 DES_kl=0.1109', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'yelp_polarity Epoch 1 |\ntr_loss=0.4021 val_loss=0.2579 tr_acc=0.9236 val_acc=0.9000 AUC_vote=0.5496\nDES_vote=0.0916 AUC_kl=0.7449 DES_kl=0.1242', '\\n', 'yelp_polarity Epoch 2 |\ntr_loss=0.2194 val_loss=0.2652 tr_acc=0.9550 val_acc=0.9020 AUC_vote=0.5781\nDES_vote=0.0964 AUC_kl=0.8163 DES_kl=0.1361', '\\n', 'yelp_polarity Epoch 3 |\ntr_loss=0.1475 val_loss=0.2415 tr_acc=0.9782 val_acc=0.9020 AUC_vote=0.5894\nDES_vote=0.0982 AUC_kl=0.8569 DES_kl=0.1428', '\\n', 'yelp_polarity Epoch 4 |\ntr_loss=0.1039 val_loss=0.3077 tr_acc=0.9908 val_acc=0.8980 AUC_vote=0.6076\nDES_vote=0.1013 AUC_kl=0.8558 DES_kl=0.1426', '\\n', 'yelp_polarity Epoch 5 |\ntr_loss=0.0619 val_loss=0.3378 tr_acc=0.9900 val_acc=0.8960 AUC_vote=0.6230\nDES_vote=0.1038 AUC_kl=0.8437 DES_kl=0.1406', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'imdb Epoch 1 |\ntr_loss=0.4366 val_loss=0.3554 tr_acc=0.8924 val_acc=0.8600 AUC_vote=0.5477\nDES_vote=0.0913 AUC_kl=0.7525 DES_kl=0.1254', '\\n', 'imdb Epoch 2 |\ntr_loss=0.3038 val_loss=0.2949 tr_acc=0.9318 val_acc=0.8780 AUC_vote=0.5588\nDES_vote=0.0931 AUC_kl=0.7483 DES_kl=0.1247', '\\n', 'imdb Epoch 3 |\ntr_loss=0.2238 val_loss=0.3554 tr_acc=0.9620 val_acc=0.8820 AUC_vote=0.5434\nDES_vote=0.0906 AUC_kl=0.7617 DES_kl=0.1269', '\\n', 'imdb Epoch 4 |\ntr_loss=0.1596 val_loss=0.3815 tr_acc=0.9826 val_acc=0.8680 AUC_vote=0.5837\nDES_vote=0.0973 AUC_kl=0.7926 DES_kl=0.1321', '\\n', 'imdb Epoch 5 |\ntr_loss=0.1096 val_loss=0.4163 tr_acc=0.9848 val_acc=0.8860 AUC_vote=0.5436\nDES_vote=0.0906 AUC_kl=0.7663 DES_kl=0.1277', '\\n', 'Execution time: 29 minutes\nseconds (time limit is an hour).']", "['Using device:', ' ', 'cuda', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'sst2 Epoch 1:\nval_loss=0.2796', '\\n', 'sst2 Epoch 1: AUC_vote=0.6318, DES_vote=0.1053,\nAUC_kl=0.7324, DES_kl=0.1221', '\\n', 'sst2 Epoch 2: val_loss=0.2760', '\\n',\n'sst2 Epoch 2: AUC_vote=0.6461, DES_vote=0.1077, AUC_kl=0.7374, DES_kl=0.1229',\n'\\n', 'sst2 Epoch 3: val_loss=0.3933', '\\n', 'sst2 Epoch 3: AUC_vote=0.5644,\nDES_vote=0.0941, AUC_kl=0.7035, DES_kl=0.1172', '\\n', 'sst2 Epoch 4:\nval_loss=0.4290', '\\n', 'sst2 Epoch 4: AUC_vote=0.5851, DES_vote=0.0975,\nAUC_kl=0.6909, DES_kl=0.1151', '\\n', 'sst2 Epoch 5: val_loss=0.5191', '\\n',\n'sst2 Epoch 5: AUC_vote=0.6403, DES_vote=0.1067, AUC_kl=0.7488, DES_kl=0.1248',\n'\\n', \"Some weights of BertForSequenceClassification were not initialized from\nthe model checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'yelp_polarity Epoch 1: val_loss=0.1673', '\\n', 'yelp_polarity Epoch 1:\nAUC_vote=0.5973, DES_vote=0.0995, AUC_kl=0.8704, DES_kl=0.1451', '\\n',\n'yelp_polarity Epoch 2: val_loss=0.1615', '\\n', 'yelp_polarity Epoch 2:\nAUC_vote=0.5794, DES_vote=0.0966, AUC_kl=0.8741, DES_kl=0.1457', '\\n',\n'yelp_polarity Epoch 3: val_loss=0.1665', '\\n', 'yelp_polarity Epoch 3:\nAUC_vote=0.5488, DES_vote=0.0915, AUC_kl=0.8369, DES_kl=0.1395', '\\n',\n'yelp_polarity Epoch 4: val_loss=0.2487', '\\n', 'yelp_polarity Epoch 4:\nAUC_vote=0.6026, DES_vote=0.1004, AUC_kl=0.8629, DES_kl=0.1438', '\\n',\n'yelp_polarity Epoch 5: val_loss=0.2328', '\\n', 'yelp_polarity Epoch 5:\nAUC_vote=0.6066, DES_vote=0.1011, AUC_kl=0.8730, DES_kl=0.1455', '\\n', \"Some\nweights of BertForSequenceClassification were not initialized from the model\ncheckpoint at bert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'imdb Epoch 1:\nval_loss=0.2153', '\\n', 'imdb Epoch 1: AUC_vote=0.5824, DES_vote=0.0971,\nAUC_kl=0.8074, DES_kl=0.1346', '\\n', 'imdb Epoch 2: val_loss=0.2067', '\\n',\n'imdb Epoch 2: AUC_vote=0.6159, DES_vote=0.1026, AUC_kl=0.8951, DES_kl=0.1492',\n'\\n', 'imdb Epoch 3: val_loss=0.2360', '\\n', 'imdb Epoch 3: AUC_vote=0.6017,\nDES_vote=0.1003, AUC_kl=0.8771, DES_kl=0.1462', '\\n', 'imdb Epoch 4:\nval_loss=0.2466', '\\n', 'imdb Epoch 4: AUC_vote=0.6089, DES_vote=0.1015,\nAUC_kl=0.8641, DES_kl=0.1440', '\\n', 'imdb Epoch 5: val_loss=0.3398', '\\n',\n'imdb Epoch 5: AUC_vote=0.6107, DES_vote=0.1018, AUC_kl=0.8708, DES_kl=0.1451',\n'\\n', 'Execution time: 23 minutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', \"Some weights of BertForSequenceClassification were\nnot initialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'full_ft/sst2 Epoch 1: validation_loss = 0.3104', '\\n',\n'full_ft/sst2 Epoch 1: AUC_vote=0.6699, DES_vote=0.1117, Spearman_vote=0.2712,\nAUC_kl=0.7223, DES_kl=0.1204, Spearman_kl=0.2520', '\\n', 'full_ft/sst2 Epoch 2:\nvalidation_loss = 0.3146', '\\n', 'full_ft/sst2 Epoch 2: AUC_vote=0.7046,\nDES_vote=0.1174, Spearman_vote=0.3280, AUC_kl=0.7722, DES_kl=0.1287,\nSpearman_kl=0.3065', '\\n', 'full_ft/sst2 Epoch 3: validation_loss = 0.3501',\n'\\n', 'full_ft/sst2 Epoch 3: AUC_vote=0.6956, DES_vote=0.1159,\nSpearman_vote=0.2947, AUC_kl=0.7728, DES_kl=0.1288, Spearman_kl=0.2981', '\\n',\n'full_ft/sst2 Epoch 4: validation_loss = 0.4515', '\\n', 'full_ft/sst2 Epoch 4:\nAUC_vote=0.6705, DES_vote=0.1118, Spearman_vote=0.2757, AUC_kl=0.7673,\nDES_kl=0.1279, Spearman_kl=0.3113', '\\n', 'full_ft/sst2 Epoch 5: validation_loss\n= 0.4257', '\\n', 'full_ft/sst2 Epoch 5: AUC_vote=0.6499, DES_vote=0.1083,\nSpearman_vote=0.2254, AUC_kl=0.7764, DES_kl=0.1294, Spearman_kl=0.2972', '\\n',\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'full_ft/yelp_polarity Epoch 1: validation_loss = 0.1270', '\\n',\n'full_ft/yelp_polarity Epoch 1: AUC_vote=0.5243, DES_vote=0.0874,\nSpearman_vote=0.0558, AUC_kl=0.8757, DES_kl=0.1460, Spearman_kl=0.2782', '\\n',\n'full_ft/yelp_polarity Epoch 2: validation_loss = 0.1427', '\\n',\n'full_ft/yelp_polarity Epoch 2: AUC_vote=0.5822, DES_vote=0.0970,\nSpearman_vote=0.1862, AUC_kl=0.8119, DES_kl=0.1353, Spearman_kl=0.2566', '\\n',\n'full_ft/yelp_polarity Epoch 3: validation_loss = 0.2758', '\\n',\n'full_ft/yelp_polarity Epoch 3: AUC_vote=0.5547, DES_vote=0.0924,\nSpearman_vote=0.1424, AUC_kl=0.8257, DES_kl=0.1376, Spearman_kl=0.2879', '\\n',\n'full_ft/yelp_polarity Epoch 4: validation_loss = 0.1453', '\\n',\n'full_ft/yelp_polarity Epoch 4: AUC_vote=0.6455, DES_vote=0.1076,\nSpearman_vote=0.2909, AUC_kl=0.8671, DES_kl=0.1445, Spearman_kl=0.2608', '\\n',\n'full_ft/yelp_polarity Epoch 5: validation_loss = 0.1506', '\\n',\n'full_ft/yelp_polarity Epoch 5: AUC_vote=0.5697, DES_vote=0.0949,\nSpearman_vote=0.1455, AUC_kl=0.8833, DES_kl=0.1472, Spearman_kl=0.2782', '\\n',\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'full_ft/imdb Epoch 1: validation_loss = 0.2291', '\\n', 'full_ft/imdb Epoch 1:\nAUC_vote=0.6017, DES_vote=0.1003, Spearman_vote=0.3420, AUC_kl=0.8648,\nDES_kl=0.1441, Spearman_kl=0.3505', '\\n', 'full_ft/imdb Epoch 2: validation_loss\n= 0.3391', '\\n', 'full_ft/imdb Epoch 2: AUC_vote=0.5876, DES_vote=0.0979,\nSpearman_vote=0.3162, AUC_kl=0.8487, DES_kl=0.1415, Spearman_kl=0.4166', '\\n',\n'full_ft/imdb Epoch 3: validation_loss = 0.2546', '\\n', 'full_ft/imdb Epoch 3:\nAUC_vote=0.5984, DES_vote=0.0997, Spearman_vote=0.2603, AUC_kl=0.8585,\nDES_kl=0.1431, Spearman_kl=0.3369', '\\n', 'full_ft/imdb Epoch 4: validation_loss\n= 0.2962', '\\n', 'full_ft/imdb Epoch 4: AUC_vote=0.6174, DES_vote=0.1029,\nSpearman_vote=0.3436, AUC_kl=0.8852, DES_kl=0.1475, Spearman_kl=0.4175', '\\n',\n'full_ft/imdb Epoch 5: validation_loss = 0.3565', '\\n', 'full_ft/imdb Epoch 5:\nAUC_vote=0.6074, DES_vote=0.1012, Spearman_vote=0.3077, AUC_kl=0.8678,\nDES_kl=0.1446, Spearman_kl=0.3823', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'freeze_4/sst2 Epoch 1:\nvalidation_loss = 0.2718', '\\n', 'freeze_4/sst2 Epoch 1: AUC_vote=0.6546,\nDES_vote=0.1091, Spearman_vote=0.2552, AUC_kl=0.7346, DES_kl=0.1224,\nSpearman_kl=0.2417', '\\n', 'freeze_4/sst2 Epoch 2: validation_loss = 0.2933',\n'\\n', 'freeze_4/sst2 Epoch 2: AUC_vote=0.6779, DES_vote=0.1130,\nSpearman_vote=0.2660, AUC_kl=0.7345, DES_kl=0.1224, Spearman_kl=0.2415', '\\n',\n'freeze_4/sst2 Epoch 3: validation_loss = 0.3036', '\\n', 'freeze_4/sst2 Epoch 3:\nAUC_vote=0.6779, DES_vote=0.1130, Spearman_vote=0.2676, AUC_kl=0.7847,\nDES_kl=0.1308, Spearman_kl=0.2851', '\\n', 'freeze_4/sst2 Epoch 4:\nvalidation_loss = 0.3795', '\\n', 'freeze_4/sst2 Epoch 4: AUC_vote=0.6584,\nDES_vote=0.1097, Spearman_vote=0.2417, AUC_kl=0.7737, DES_kl=0.1290,\nSpearman_kl=0.2740', '\\n', 'freeze_4/sst2 Epoch 5: validation_loss = 0.4379',\n'\\n', 'freeze_4/sst2 Epoch 5: AUC_vote=0.7186, DES_vote=0.1198,\nSpearman_vote=0.3414, AUC_kl=0.8153, DES_kl=0.1359, Spearman_kl=0.3306', '\\n',\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'freeze_4/yelp_polarity Epoch 1: validation_loss = 0.1509', '\\n',\n'freeze_4/yelp_polarity Epoch 1: AUC_vote=0.5972, DES_vote=0.0995,\nSpearman_vote=0.2286, AUC_kl=0.8252, DES_kl=0.1375, Spearman_kl=0.2716', '\\n',\n'freeze_4/yelp_polarity Epoch 2: validation_loss = 0.1288', '\\n',\n'freeze_4/yelp_polarity Epoch 2: AUC_vote=0.5850, DES_vote=0.0975,\nSpearman_vote=0.1771, AUC_kl=0.8409, DES_kl=0.1401, Spearman_kl=0.2524', '\\n',\n'freeze_4/yelp_polarity Epoch 3: validation_loss = 0.1566', '\\n',\n'freeze_4/yelp_polarity Epoch 3: AUC_vote=0.5709, DES_vote=0.0951,\nSpearman_vote=0.1589, AUC_kl=0.8812, DES_kl=0.1469, Spearman_kl=0.3036', '\\n',\n'freeze_4/yelp_polarity Epoch 4: validation_loss = 0.2089', '\\n',\n'freeze_4/yelp_polarity Epoch 4: AUC_vote=0.5882, DES_vote=0.0980,\nSpearman_vote=0.2104, AUC_kl=0.9062, DES_kl=0.1510, Spearman_kl=0.3289', '\\n',\n'freeze_4/yelp_polarity Epoch 5: validation_loss = 0.1712', '\\n',\n'freeze_4/yelp_polarity Epoch 5: AUC_vote=0.6272, DES_vote=0.1045,\nSpearman_vote=0.2543, AUC_kl=0.9029, DES_kl=0.1505, Spearman_kl=0.2984', '\\n',\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'freeze_4/imdb Epoch 1: validation_loss = 0.2368', '\\n', 'freeze_4/imdb Epoch 1:\nAUC_vote=0.6004, DES_vote=0.1001, Spearman_vote=0.2931, AUC_kl=0.8045,\nDES_kl=0.1341, Spearman_kl=0.3019', '\\n', 'freeze_4/imdb Epoch 2:\nvalidation_loss = 0.2155', '\\n', 'freeze_4/imdb Epoch 2: AUC_vote=0.5795,\nDES_vote=0.0966, Spearman_vote=0.2529, AUC_kl=0.8493, DES_kl=0.1416,\nSpearman_kl=0.3283', '\\n', 'freeze_4/imdb Epoch 3: validation_loss = 0.2507',\n'\\n', 'freeze_4/imdb Epoch 3: AUC_vote=0.5509, DES_vote=0.0918,\nSpearman_vote=0.2267, AUC_kl=0.8356, DES_kl=0.1393, Spearman_kl=0.3043', '\\n',\n'freeze_4/imdb Epoch 4: validation_loss = 0.2631', '\\n', 'freeze_4/imdb Epoch 4:\nAUC_vote=0.5365, DES_vote=0.0894, Spearman_vote=0.1123, AUC_kl=0.7907,\nDES_kl=0.1318, Spearman_kl=0.2464', '\\n', 'freeze_4/imdb Epoch 5:\nvalidation_loss = 0.2894', '\\n', 'freeze_4/imdb Epoch 5: AUC_vote=0.5855,\nDES_vote=0.0976, Spearman_vote=0.3128, AUC_kl=0.8433, DES_kl=0.1406,\nSpearman_kl=0.3189', '\\n', \"Some weights of BertForSequenceClassification were\nnot initialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'freeze_8/sst2 Epoch 1: validation_loss = 0.3221', '\\n',\n'freeze_8/sst2 Epoch 1: AUC_vote=0.6080, DES_vote=0.1013, Spearman_vote=0.1731,\nAUC_kl=0.6846, DES_kl=0.1141, Spearman_kl=0.2107', '\\n', 'freeze_8/sst2 Epoch 2:\nvalidation_loss = 0.2768', '\\n', 'freeze_8/sst2 Epoch 2: AUC_vote=0.5883,\nDES_vote=0.0980, Spearman_vote=0.1343, AUC_kl=0.6864, DES_kl=0.1144,\nSpearman_kl=0.1937', '\\n', 'freeze_8/sst2 Epoch 3: validation_loss = 0.2787',\n'\\n', 'freeze_8/sst2 Epoch 3: AUC_vote=0.6305, DES_vote=0.1051,\nSpearman_vote=0.2078, AUC_kl=0.6810, DES_kl=0.1135, Spearman_kl=0.1977', '\\n',\n'freeze_8/sst2 Epoch 4: validation_loss = 0.2971', '\\n', 'freeze_8/sst2 Epoch 4:\nAUC_vote=0.6439, DES_vote=0.1073, Spearman_vote=0.2091, AUC_kl=0.7094,\nDES_kl=0.1182, Spearman_kl=0.2176', '\\n', 'freeze_8/sst2 Epoch 5:\nvalidation_loss = 0.3407', '\\n', 'freeze_8/sst2 Epoch 5: AUC_vote=0.6466,\nDES_vote=0.1078, Spearman_vote=0.2175, AUC_kl=0.7258, DES_kl=0.1210,\nSpearman_kl=0.2368', '\\n', \"Some weights of BertForSequenceClassification were\nnot initialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'freeze_8/yelp_polarity Epoch 1: validation_loss = 0.1865', '\\n',\n'freeze_8/yelp_polarity Epoch 1: AUC_vote=0.5795, DES_vote=0.0966,\nSpearman_vote=0.2203, AUC_kl=0.8364, DES_kl=0.1394, Spearman_kl=0.3088', '\\n',\n'freeze_8/yelp_polarity Epoch 2: validation_loss = 0.1676', '\\n',\n'freeze_8/yelp_polarity Epoch 2: AUC_vote=0.5626, DES_vote=0.0938,\nSpearman_vote=0.1602, AUC_kl=0.8379, DES_kl=0.1396, Spearman_kl=0.2865', '\\n',\n'freeze_8/yelp_polarity Epoch 3: validation_loss = 0.1799', '\\n',\n'freeze_8/yelp_polarity Epoch 3: AUC_vote=0.5741, DES_vote=0.0957,\nSpearman_vote=0.1794, AUC_kl=0.8713, DES_kl=0.1452, Spearman_kl=0.3194', '\\n',\n'freeze_8/yelp_polarity Epoch 4: validation_loss = 0.2161', '\\n',\n'freeze_8/yelp_polarity Epoch 4: AUC_vote=0.5934, DES_vote=0.0989,\nSpearman_vote=0.2351, AUC_kl=0.8323, DES_kl=0.1387, Spearman_kl=0.3158', '\\n',\n'freeze_8/yelp_polarity Epoch 5: validation_loss = 0.2374', '\\n',\n'freeze_8/yelp_polarity Epoch 5: AUC_vote=0.5578, DES_vote=0.0930,\nSpearman_vote=0.1698, AUC_kl=0.8883, DES_kl=0.1480, Spearman_kl=0.3477', '\\n',\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'freeze_8/imdb Epoch 1: validation_loss = 0.2502', '\\n', 'freeze_8/imdb Epoch 1:\nAUC_vote=0.6102, DES_vote=0.1017, Spearman_vote=0.3265, AUC_kl=0.8143,\nDES_kl=0.1357, Spearman_kl=0.3379', '\\n', 'freeze_8/imdb Epoch 2:\nvalidation_loss = 0.2238', '\\n', 'freeze_8/imdb Epoch 2: AUC_vote=0.5977,\nDES_vote=0.0996, Spearman_vote=0.2909, AUC_kl=0.8425, DES_kl=0.1404,\nSpearman_kl=0.3291', '\\n', 'freeze_8/imdb Epoch 3: validation_loss = 0.2555',\n'\\n', 'freeze_8/imdb Epoch 3: AUC_vote=0.5654, DES_vote=0.0942,\nSpearman_vote=0.2087, AUC_kl=0.8546, DES_kl=0.1424, Spearman_kl=0.3551', '\\n',\n'Traceback (most recent call last):\\n  File \"runfile.py\", line 131, in\n<module>\\n    batch_losses.append(out.loss.item())\\n\n^^^^^^^^^^^^^^^\\nKeyboardInterrupt\\n', 'TimeoutError: Execution exceeded the\ntime limit of an hour']", "[\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'baseline sst2 Ep1: val_loss=0.2774, AUCv=0.725, DESv=0.121', '\\n', 'baseline\nsst2 Ep2: val_loss=0.3446, AUCv=0.740, DESv=0.123', '\\n', 'baseline sst2 Ep3:\nval_loss=0.3409, AUCv=0.690, DESv=0.115', '\\n', 'baseline sst2 Ep4:\nval_loss=0.3849, AUCv=0.715, DESv=0.119', '\\n', 'baseline sst2 Ep5:\nval_loss=0.5208, AUCv=0.735, DESv=0.123', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'baseline yelp_polarity\nEp1: val_loss=0.1340, AUCv=0.653, DESv=0.109', '\\n', 'baseline yelp_polarity\nEp2: val_loss=0.1779, AUCv=0.582, DESv=0.097', '\\n', 'baseline yelp_polarity\nEp3: val_loss=0.1521, AUCv=0.570, DESv=0.095', '\\n', 'baseline yelp_polarity\nEp4: val_loss=0.2028, AUCv=0.587, DESv=0.098', '\\n', 'baseline yelp_polarity\nEp5: val_loss=0.1998, AUCv=0.573, DESv=0.096', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'baseline imdb Ep1:\nval_loss=0.2370, AUCv=0.636, DESv=0.106', '\\n', 'baseline imdb Ep2:\nval_loss=0.2032, AUCv=0.626, DESv=0.104', '\\n', 'baseline imdb Ep3:\nval_loss=0.2634, AUCv=0.581, DESv=0.097', '\\n', 'baseline imdb Ep4:\nval_loss=0.2620, AUCv=0.562, DESv=0.094', '\\n', 'baseline imdb Ep5:\nval_loss=0.3249, AUCv=0.613, DESv=0.102', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'paraphrase_aug sst2\nEp1: val_loss=0.3548, AUCv=0.619, DESv=0.103', '\\n', 'paraphrase_aug sst2 Ep2:\nval_loss=0.4350, AUCv=0.650, DESv=0.108', '\\n', 'paraphrase_aug sst2 Ep3:\nval_loss=0.4623, AUCv=0.677, DESv=0.113', '\\n', 'paraphrase_aug sst2 Ep4:\nval_loss=0.5656, AUCv=0.674, DESv=0.112', '\\n', 'paraphrase_aug sst2 Ep5:\nval_loss=0.5127, AUCv=0.701, DESv=0.117', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'paraphrase_aug\nyelp_polarity Ep1: val_loss=0.1751, AUCv=0.637, DESv=0.106', '\\n',\n'paraphrase_aug yelp_polarity Ep2: val_loss=0.2638, AUCv=0.508, DESv=0.085',\n'\\n', 'Traceback (most recent call last):\\n  File \"runfile.py\", line 122, in\n<module>\\n    tr_losses.append(out.loss.item())\\n\n^^^^^^^^^^^^^^^\\nKeyboardInterrupt\\n', 'TimeoutError: Execution exceeded the\ntime limit of an hour']", "[\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'full_heads sst2 E1: tr_loss=0.3753, tr_acc=0.8256, va_loss=0.4448,\nva_acc=0.8300', '\\n', 'full_heads sst2 E1: AUC_vote=0.6114, DES_vote=0.1019,\nAUC_kl=0.7505, DES_kl=0.1251', '\\n', 'full_heads sst2 E2: tr_loss=0.1793,\ntr_acc=0.9348, va_loss=0.2746, va_acc=0.8860', '\\n', 'full_heads sst2 E2:\nAUC_vote=0.6792, DES_vote=0.1132, AUC_kl=0.7311, DES_kl=0.1218', '\\n',\n'full_heads sst2 E3: tr_loss=0.0831, tr_acc=0.9732, va_loss=0.3508,\nva_acc=0.8860', '\\n', 'full_heads sst2 E3: AUC_vote=0.6859, DES_vote=0.1143,\nAUC_kl=0.7799, DES_kl=0.1300', '\\n', 'full_heads sst2 E4: tr_loss=0.0509,\ntr_acc=0.9840, va_loss=0.3558, va_acc=0.9000', '\\n', 'full_heads sst2 E4:\nAUC_vote=0.6171, DES_vote=0.1029, AUC_kl=0.7317, DES_kl=0.1219', '\\n',\n'full_heads sst2 E5: tr_loss=0.0328, tr_acc=0.9914, va_loss=0.4328,\nva_acc=0.9020', '\\n', 'full_heads sst2 E5: AUC_vote=0.6570, DES_vote=0.1095,\nAUC_kl=0.7789, DES_kl=0.1298', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'full_heads\nyelp_polarity E1: tr_loss=0.2684, tr_acc=0.8898, va_loss=0.1617, va_acc=0.9280',\n'\\n', 'full_heads yelp_polarity E1: AUC_vote=0.6169, DES_vote=0.1028,\nAUC_kl=0.8639, DES_kl=0.1440', '\\n', 'full_heads yelp_polarity E2:\ntr_loss=0.0992, tr_acc=0.9654, va_loss=0.1185, va_acc=0.9460', '\\n', 'full_heads\nyelp_polarity E2: AUC_vote=0.5787, DES_vote=0.0965, AUC_kl=0.8952,\nDES_kl=0.1492', '\\n', 'full_heads yelp_polarity E3: tr_loss=0.0477,\ntr_acc=0.9852, va_loss=0.1734, va_acc=0.9360', '\\n', 'full_heads yelp_polarity\nE3: AUC_vote=0.6463, DES_vote=0.1077, AUC_kl=0.8956, DES_kl=0.1493', '\\n',\n'full_heads yelp_polarity E4: tr_loss=0.0341, tr_acc=0.9878, va_loss=0.1569,\nva_acc=0.9540', '\\n', 'full_heads yelp_polarity E4: AUC_vote=0.6412,\nDES_vote=0.1069, AUC_kl=0.8971, DES_kl=0.1495', '\\n', 'full_heads yelp_polarity\nE5: tr_loss=0.0168, tr_acc=0.9948, va_loss=0.2148, va_acc=0.9500', '\\n',\n'full_heads yelp_polarity E5: AUC_vote=0.6080, DES_vote=0.1013, AUC_kl=0.8731,\nDES_kl=0.1455', '\\n', \"Some weights of BertForSequenceClassification were not\ninitialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'full_heads imdb E1: tr_loss=0.3326, tr_acc=0.8500,\nva_loss=0.2292, va_acc=0.9100', '\\n', 'full_heads imdb E1: AUC_vote=0.5279,\nDES_vote=0.0880, AUC_kl=0.8487, DES_kl=0.1415', '\\n', 'full_heads imdb E2:\ntr_loss=0.1576, tr_acc=0.9454, va_loss=0.2346, va_acc=0.9240', '\\n', 'full_heads\nimdb E2: AUC_vote=0.5645, DES_vote=0.0941, AUC_kl=0.8312, DES_kl=0.1385', '\\n',\n'full_heads imdb E3: tr_loss=0.0873, tr_acc=0.9728, va_loss=0.2629,\nva_acc=0.9100', '\\n', 'full_heads imdb E3: AUC_vote=0.5950, DES_vote=0.0992,\nAUC_kl=0.8416, DES_kl=0.1403', '\\n', 'full_heads imdb E4: tr_loss=0.0598,\ntr_acc=0.9816, va_loss=0.2867, va_acc=0.9120', '\\n', 'full_heads imdb E4:\nAUC_vote=0.5823, DES_vote=0.0971, AUC_kl=0.8286, DES_kl=0.1381', '\\n',\n'full_heads imdb E5: tr_loss=0.0307, tr_acc=0.9940, va_loss=0.3369,\nva_acc=0.9160', '\\n', 'full_heads imdb E5: AUC_vote=0.6008, DES_vote=0.1001,\nAUC_kl=0.8672, DES_kl=0.1445', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'BertSdpaSelfAttention\nis used but `torch.nn.functional.scaled_dot_product_attention` does not support\nnon-absolute `position_embedding_type` or `output_attentions=True` or\n`head_mask`. Falling back to the manual attention implementation, but specifying\nthe manual implementation will be required from Transformers version v5.0.0\nonwards. This warning can be removed using the argument\n`attn_implementation=\"eager\"` when loading the model.\\n', 'pruned_heads sst2 E1:\ntr_loss=0.4552, tr_acc=0.7724, va_loss=0.3482, va_acc=0.8300', '\\n',\n'pruned_heads sst2 E1: AUC_vote=0.5456, DES_vote=0.0909, AUC_kl=0.6361,\nDES_kl=0.1060', '\\n', 'pruned_heads sst2 E2: tr_loss=0.2069, tr_acc=0.9222,\nva_loss=0.3395, va_acc=0.8500', '\\n', 'pruned_heads sst2 E2: AUC_vote=0.6258,\nDES_vote=0.1043, AUC_kl=0.7289, DES_kl=0.1215', '\\n', 'pruned_heads sst2 E3:\ntr_loss=0.1060, tr_acc=0.9628, va_loss=0.4249, va_acc=0.8500', '\\n',\n'pruned_heads sst2 E3: AUC_vote=0.6616, DES_vote=0.1103, AUC_kl=0.7369,\nDES_kl=0.1228', '\\n', 'pruned_heads sst2 E4: tr_loss=0.0562, tr_acc=0.9814,\nva_loss=0.4130, va_acc=0.8680', '\\n', 'pruned_heads sst2 E4: AUC_vote=0.6606,\nDES_vote=0.1101, AUC_kl=0.7546, DES_kl=0.1258', '\\n', 'pruned_heads sst2 E5:\ntr_loss=0.0409, tr_acc=0.9888, va_loss=0.4355, va_acc=0.8840', '\\n',\n'pruned_heads sst2 E5: AUC_vote=0.6112, DES_vote=0.1019, AUC_kl=0.7330,\nDES_kl=0.1222', '\\n', \"Some weights of BertForSequenceClassification were not\ninitialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'pruned_heads yelp_polarity E1: tr_loss=0.3336, tr_acc=0.8468,\nva_loss=0.1801, va_acc=0.9220', '\\n', 'pruned_heads yelp_polarity E1:\nAUC_vote=0.6075, DES_vote=0.1012, AUC_kl=0.8977, DES_kl=0.1496', '\\n',\n'pruned_heads yelp_polarity E2: tr_loss=0.1160, tr_acc=0.9568, va_loss=0.1615,\nva_acc=0.9340', '\\n', 'pruned_heads yelp_polarity E2: AUC_vote=0.6260,\nDES_vote=0.1043, AUC_kl=0.8703, DES_kl=0.1450', '\\n', 'pruned_heads\nyelp_polarity E3: tr_loss=0.0525, tr_acc=0.9830, va_loss=0.1758, va_acc=0.9360',\n'\\n', 'pruned_heads yelp_polarity E3: AUC_vote=0.6250, DES_vote=0.1042,\nAUC_kl=0.8941, DES_kl=0.1490', '\\n', 'pruned_heads yelp_polarity E4:\ntr_loss=0.0240, tr_acc=0.9908, va_loss=0.2530, va_acc=0.9280', '\\n',\n'pruned_heads yelp_polarity E4: AUC_vote=0.6398, DES_vote=0.1066, AUC_kl=0.9161,\nDES_kl=0.1527', '\\n', 'pruned_heads yelp_polarity E5: tr_loss=0.0185,\ntr_acc=0.9940, va_loss=0.2304, va_acc=0.9300', '\\n', 'pruned_heads yelp_polarity\nE5: AUC_vote=0.6157, DES_vote=0.1026, AUC_kl=0.8892, DES_kl=0.1482', '\\n', \"Some\nweights of BertForSequenceClassification were not initialized from the model\ncheckpoint at bert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'pruned_heads imdb E1:\ntr_loss=0.4513, tr_acc=0.7722, va_loss=0.3127, va_acc=0.8760', '\\n',\n'pruned_heads imdb E1: AUC_vote=0.5725, DES_vote=0.0954, AUC_kl=0.8137,\nDES_kl=0.1356', '\\n', 'pruned_heads imdb E2: tr_loss=0.2345, tr_acc=0.9074,\nva_loss=0.2748, va_acc=0.8880', '\\n', 'pruned_heads imdb E2: AUC_vote=0.5867,\nDES_vote=0.0978, AUC_kl=0.8041, DES_kl=0.1340', '\\n', 'pruned_heads imdb E3:\ntr_loss=0.1255, tr_acc=0.9538, va_loss=0.3266, va_acc=0.8840', '\\n',\n'pruned_heads imdb E3: AUC_vote=0.5755, DES_vote=0.0959, AUC_kl=0.8511,\nDES_kl=0.1419', '\\n', 'pruned_heads imdb E4: tr_loss=0.0714, tr_acc=0.9744,\nva_loss=0.2613, va_acc=0.9160', '\\n', 'pruned_heads imdb E4: AUC_vote=0.5850,\nDES_vote=0.0975, AUC_kl=0.8396, DES_kl=0.1399', '\\n', 'pruned_heads imdb E5:\ntr_loss=0.0419, tr_acc=0.9862, va_loss=0.3156, va_acc=0.9080', '\\n',\n'pruned_heads imdb E5: AUC_vote=0.6072, DES_vote=0.1012, AUC_kl=0.8555,\nDES_kl=0.1426', '\\n', 'Saved experiment_data.npy in', ' ', '/data/chenhui/AI-Sci\nentist-v2/experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/0-\nrun/process_ForkProcess-13/working', '\\n', 'Execution time: 49 minutes seconds\n(time limit is an hour).']", "['Using device: cuda', '\\n', \"Some weights of BertForSequenceClassification were\nnot initialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'baseline sst2 Epoch 1: val_loss=0.2469', '\\n', 'baseline sst2\nEpoch 1: AUC_vote=0.6855, DES_vote=0.1143, AUC_kl=0.7616, DES_kl=0.1269', '\\n',\n'baseline sst2 Epoch 2: val_loss=0.2577', '\\n', 'baseline sst2 Epoch 2:\nAUC_vote=0.6640, DES_vote=0.1107, AUC_kl=0.7415, DES_kl=0.1236', '\\n', 'baseline\nsst2 Epoch 3: val_loss=0.2999', '\\n', 'baseline sst2 Epoch 3: AUC_vote=0.6790,\nDES_vote=0.1132, AUC_kl=0.7863, DES_kl=0.1310', '\\n', 'baseline sst2 Epoch 4:\nval_loss=0.4690', '\\n', 'baseline sst2 Epoch 4: AUC_vote=0.6868,\nDES_vote=0.1145, AUC_kl=0.7882, DES_kl=0.1314', '\\n', 'baseline sst2 Epoch 5:\nval_loss=0.4713', '\\n', 'baseline sst2 Epoch 5: AUC_vote=0.6660,\nDES_vote=0.1110, AUC_kl=0.7741, DES_kl=0.1290', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'baseline yelp_polarity\nEpoch 1: val_loss=0.1986', '\\n', 'baseline yelp_polarity Epoch 1:\nAUC_vote=0.5510, DES_vote=0.0918, AUC_kl=0.8442, DES_kl=0.1407', '\\n', 'baseline\nyelp_polarity Epoch 2: val_loss=0.1560', '\\n', 'baseline yelp_polarity Epoch 2:\nAUC_vote=0.6231, DES_vote=0.1039, AUC_kl=0.8603, DES_kl=0.1434', '\\n', 'baseline\nyelp_polarity Epoch 3: val_loss=0.1809', '\\n', 'baseline yelp_polarity Epoch 3:\nAUC_vote=0.6256, DES_vote=0.1043, AUC_kl=0.8933, DES_kl=0.1489', '\\n', 'baseline\nyelp_polarity Epoch 4: val_loss=0.2034', '\\n', 'baseline yelp_polarity Epoch 4:\nAUC_vote=0.5702, DES_vote=0.0950, AUC_kl=0.8572, DES_kl=0.1429', '\\n', 'baseline\nyelp_polarity Epoch 5: val_loss=0.2083', '\\n', 'baseline yelp_polarity Epoch 5:\nAUC_vote=0.6256, DES_vote=0.1043, AUC_kl=0.8763, DES_kl=0.1461', '\\n', \"Some\nweights of BertForSequenceClassification were not initialized from the model\ncheckpoint at bert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'baseline imdb Epoch 1:\nval_loss=0.2183', '\\n', 'baseline imdb Epoch 1: AUC_vote=0.5776,\nDES_vote=0.0963, AUC_kl=0.7840, DES_kl=0.1307', '\\n', 'baseline imdb Epoch 2:\nval_loss=0.3631', '\\n', 'baseline imdb Epoch 2: AUC_vote=0.5415,\nDES_vote=0.0902, AUC_kl=0.8311, DES_kl=0.1385', '\\n', 'baseline imdb Epoch 3:\nval_loss=0.3064', '\\n', 'baseline imdb Epoch 3: AUC_vote=0.6096,\nDES_vote=0.1016, AUC_kl=0.8672, DES_kl=0.1445', '\\n', 'baseline imdb Epoch 4:\nval_loss=0.3382', '\\n', 'baseline imdb Epoch 4: AUC_vote=0.6214,\nDES_vote=0.1036, AUC_kl=0.8951, DES_kl=0.1492', '\\n', 'baseline imdb Epoch 5:\nval_loss=0.3205', '\\n', 'baseline imdb Epoch 5: AUC_vote=0.6289,\nDES_vote=0.1048, AUC_kl=0.8926, DES_kl=0.1488', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'ffn_ablate sst2 Epoch\n1: val_loss=0.4617', '\\n', 'ffn_ablate sst2 Epoch 1: AUC_vote=0.6177,\nDES_vote=0.1029, AUC_kl=0.5842, DES_kl=0.0974', '\\n', 'ffn_ablate sst2 Epoch 2:\nval_loss=0.4828', '\\n', 'ffn_ablate sst2 Epoch 2: AUC_vote=0.5724,\nDES_vote=0.0954, AUC_kl=0.5911, DES_kl=0.0985', '\\n', 'ffn_ablate sst2 Epoch 3:\nval_loss=0.4861', '\\n', 'ffn_ablate sst2 Epoch 3: AUC_vote=0.5883,\nDES_vote=0.0980, AUC_kl=0.6159, DES_kl=0.1026', '\\n', 'ffn_ablate sst2 Epoch 4:\nval_loss=0.5383', '\\n', 'ffn_ablate sst2 Epoch 4: AUC_vote=0.6100,\nDES_vote=0.1017, AUC_kl=0.6586, DES_kl=0.1098', '\\n', 'ffn_ablate sst2 Epoch 5:\nval_loss=0.6182', '\\n', 'ffn_ablate sst2 Epoch 5: AUC_vote=0.5859,\nDES_vote=0.0976, AUC_kl=0.6712, DES_kl=0.1119', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'ffn_ablate\nyelp_polarity Epoch 1: val_loss=0.2844', '\\n', 'ffn_ablate yelp_polarity Epoch\n1: AUC_vote=0.6032, DES_vote=0.1005, AUC_kl=0.8347, DES_kl=0.1391', '\\n',\n'ffn_ablate yelp_polarity Epoch 2: val_loss=0.2442', '\\n', 'ffn_ablate\nyelp_polarity Epoch 2: AUC_vote=0.5878, DES_vote=0.0980, AUC_kl=0.8062,\nDES_kl=0.1344', '\\n', 'ffn_ablate yelp_polarity Epoch 3: val_loss=0.2807', '\\n',\n'ffn_ablate yelp_polarity Epoch 3: AUC_vote=0.5509, DES_vote=0.0918,\nAUC_kl=0.8431, DES_kl=0.1405', '\\n', 'ffn_ablate yelp_polarity Epoch 4:\nval_loss=0.2931', '\\n', 'ffn_ablate yelp_polarity Epoch 4: AUC_vote=0.6044,\nDES_vote=0.1007, AUC_kl=0.8641, DES_kl=0.1440', '\\n', 'ffn_ablate yelp_polarity\nEpoch 5: val_loss=0.2801', '\\n', 'ffn_ablate yelp_polarity Epoch 5:\nAUC_vote=0.6523, DES_vote=0.1087, AUC_kl=0.8447, DES_kl=0.1408', '\\n', \"Some\nweights of BertForSequenceClassification were not initialized from the model\ncheckpoint at bert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'ffn_ablate imdb Epoch\n1: val_loss=0.3862', '\\n', 'ffn_ablate imdb Epoch 1: AUC_vote=0.5694,\nDES_vote=0.0949, AUC_kl=0.7373, DES_kl=0.1229', '\\n', 'ffn_ablate imdb Epoch 2:\nval_loss=0.3266', '\\n', 'ffn_ablate imdb Epoch 2: AUC_vote=0.5427,\nDES_vote=0.0905, AUC_kl=0.7711, DES_kl=0.1285', '\\n', 'ffn_ablate imdb Epoch 3:\nval_loss=0.3256', '\\n', 'ffn_ablate imdb Epoch 3: AUC_vote=0.5814,\nDES_vote=0.0969, AUC_kl=0.7813, DES_kl=0.1302', '\\n', 'ffn_ablate imdb Epoch 4:\nval_loss=0.3523', '\\n', 'ffn_ablate imdb Epoch 4: AUC_vote=0.5651,\nDES_vote=0.0942, AUC_kl=0.7701, DES_kl=0.1283', '\\n', 'ffn_ablate imdb Epoch 5:\nval_loss=0.4734', '\\n', 'ffn_ablate imdb Epoch 5: AUC_vote=0.5824,\nDES_vote=0.0971, AUC_kl=0.7944, DES_kl=0.1324', '\\n', 'Execution time: 44\nminutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', \"Some weights of BertForSequenceClassification were\nnot initialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'Traceback (most recent call last):\\n  File \"runfile.py\", line\n134, in <module>\\n    out = model(input_ids=ids, attention_mask=mask,\nlabels=labels)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/models/bert/modeling_bert.py\", line 1503, in forward\\n\noutputs = self.bert(\\n              ^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/models/bert/modeling_bert.py\", line 1016, in forward\\n\nencoder_outputs = self.encoder(\\n                      ^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/models/bert/modeling_bert.py\", line 662, in forward\\n\nlayer_outputs = layer_module(\\n                    ^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/models/bert/modeling_bert.py\", line 552, in forward\\n\nself_attention_outputs = self.attention(\\n\n^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/models/bert/modeling_bert.py\", line 491, in forward\\n\nattention_output = self.output(self_outputs[0], hidden_states)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"runfile.py\", line 29, in forward\\n    hidden_states =\nself.dense(hidden_states)\\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/linear.py\", line 125, in forward\\n    return\nF.linear(input, self.weight, self.bias)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: Expected all tensors to\nbe on the same device, but found at least two devices, cpu and cuda:0! (when\nchecking argument for argument mat1 in method wrapper_CUDA_addmm)\\n', 'Execution\ntime: 22 seconds seconds (time limit is an hour).']", "[\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'sst2 Epoch 1: val_loss=0.3298', '\\n', 'sst2 Epoch 1: AUC_vote=0.6575,\nDES_vote=0.1096, AUC_kl=0.6890, DES_kl=0.1148', '\\n', 'sst2 Epoch 2:\nval_loss=0.3184', '\\n', 'sst2 Epoch 2: AUC_vote=0.6747, DES_vote=0.1124,\nAUC_kl=0.7204, DES_kl=0.1201', '\\n', 'sst2 Epoch 3: val_loss=0.2970', '\\n',\n'sst2 Epoch 3: AUC_vote=0.6763, DES_vote=0.1127, AUC_kl=0.7352, DES_kl=0.1225',\n'\\n', 'sst2 Epoch 4: val_loss=0.4257', '\\n', 'sst2 Epoch 4: AUC_vote=0.6081,\nDES_vote=0.1014, AUC_kl=0.7326, DES_kl=0.1221', '\\n', 'sst2 Epoch 5:\nval_loss=0.4361', '\\n', 'sst2 Epoch 5: AUC_vote=0.6040, DES_vote=0.1007,\nAUC_kl=0.7203, DES_kl=0.1201', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'yelp_polarity Epoch 1:\nval_loss=0.1498', '\\n', 'yelp_polarity Epoch 1: AUC_vote=0.6239,\nDES_vote=0.1040, AUC_kl=0.8526, DES_kl=0.1421', '\\n', 'yelp_polarity Epoch 2:\nval_loss=0.1320', '\\n', 'yelp_polarity Epoch 2: AUC_vote=0.6035,\nDES_vote=0.1006, AUC_kl=0.8621, DES_kl=0.1437', '\\n', 'yelp_polarity Epoch 3:\nval_loss=0.2280', '\\n', 'yelp_polarity Epoch 3: AUC_vote=0.5837,\nDES_vote=0.0973, AUC_kl=0.8705, DES_kl=0.1451', '\\n', 'yelp_polarity Epoch 4:\nval_loss=0.2098', '\\n', 'yelp_polarity Epoch 4: AUC_vote=0.6237,\nDES_vote=0.1040, AUC_kl=0.9120, DES_kl=0.1520', '\\n', 'yelp_polarity Epoch 5:\nval_loss=0.2207', '\\n', 'yelp_polarity Epoch 5: AUC_vote=0.6531,\nDES_vote=0.1089, AUC_kl=0.8726, DES_kl=0.1454', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'imdb Epoch 1:\nval_loss=0.2355', '\\n', 'imdb Epoch 1: AUC_vote=0.5477, DES_vote=0.0913,\nAUC_kl=0.8356, DES_kl=0.1393', '\\n', 'imdb Epoch 2: val_loss=0.2189', '\\n',\n'imdb Epoch 2: AUC_vote=0.5210, DES_vote=0.0868, AUC_kl=0.8538, DES_kl=0.1423',\n'\\n', 'imdb Epoch 3: val_loss=0.3045', '\\n', 'imdb Epoch 3: AUC_vote=0.6059,\nDES_vote=0.1010, AUC_kl=0.8459, DES_kl=0.1410', '\\n', 'imdb Epoch 4:\nval_loss=0.2654', '\\n', 'imdb Epoch 4: AUC_vote=0.6183, DES_vote=0.1031,\nAUC_kl=0.8714, DES_kl=0.1452', '\\n', 'imdb Epoch 5: val_loss=0.4236', '\\n',\n'imdb Epoch 5: AUC_vote=0.5622, DES_vote=0.0937, AUC_kl=0.8288, DES_kl=0.1381',\n'\\n', 'Execution time: 23 minutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', \"Some weights of BertForSequenceClassification were\nnot initialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'full_ft/sst2 Epoch 1: validation_loss = 0.5227', '\\n',\n'full_ft/sst2 Epoch 1: AUC_vote=0.5550, DES_vote=0.1387, Spearman_vote=0.1125,\nAUC_kl=0.4973, DES_kl=0.1243, Spearman_kl=-0.0035', '\\n', 'full_ft/sst2 Epoch 2:\nvalidation_loss = 0.2812', '\\n', 'full_ft/sst2 Epoch 2: AUC_vote=0.6306,\nDES_vote=0.1577, Spearman_vote=0.2009, AUC_kl=0.7248, DES_kl=0.1812,\nSpearman_kl=0.2229', '\\n', 'full_ft/sst2 Epoch 3: validation_loss = 0.3059',\n'\\n', 'full_ft/sst2 Epoch 3: AUC_vote=0.5695, DES_vote=0.1424,\nSpearman_vote=0.1417, AUC_kl=0.7494, DES_kl=0.1873, Spearman_kl=0.2703', '\\n',\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'full_ft/yelp_polarity Epoch 1: validation_loss = 0.2708', '\\n',\n'full_ft/yelp_polarity Epoch 1: AUC_vote=0.5413, DES_vote=0.1353,\nSpearman_vote=0.1419, AUC_kl=0.7435, DES_kl=0.1859, Spearman_kl=0.2474', '\\n',\n'full_ft/yelp_polarity Epoch 2: validation_loss = 0.2304', '\\n',\n'full_ft/yelp_polarity Epoch 2: AUC_vote=0.5730, DES_vote=0.1432,\nSpearman_vote=0.2272, AUC_kl=0.7958, DES_kl=0.1989, Spearman_kl=0.2932', '\\n',\n'full_ft/yelp_polarity Epoch 3: validation_loss = 0.2590', '\\n',\n'full_ft/yelp_polarity Epoch 3: AUC_vote=0.5561, DES_vote=0.1390,\nSpearman_vote=0.1544, AUC_kl=0.8039, DES_kl=0.2010, Spearman_kl=0.3158', '\\n',\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'full_ft/imdb Epoch 1: validation_loss = 0.5029', '\\n', 'full_ft/imdb Epoch 1:\nAUC_vote=0.4805, DES_vote=0.1201, Spearman_vote=-0.0961, AUC_kl=0.6621,\nDES_kl=0.1655, Spearman_kl=0.2362', '\\n', 'full_ft/imdb Epoch 2: validation_loss\n= 0.2794', '\\n', 'full_ft/imdb Epoch 2: AUC_vote=0.5111, DES_vote=0.1278,\nSpearman_vote=0.0391, AUC_kl=0.7356, DES_kl=0.1839, Spearman_kl=0.2448', '\\n',\n'full_ft/imdb Epoch 3: validation_loss = 0.2574', '\\n', 'full_ft/imdb Epoch 3:\nAUC_vote=0.5658, DES_vote=0.1415, Spearman_vote=0.2586, AUC_kl=0.7978,\nDES_kl=0.1995, Spearman_kl=0.3163', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'freeze_4/sst2 Epoch 1:\nvalidation_loss = 0.5001', '\\n', 'freeze_4/sst2 Epoch 1: AUC_vote=0.7013,\nDES_vote=0.1753, Spearman_vote=0.3534, AUC_kl=0.5761, DES_kl=0.1440,\nSpearman_kl=0.0872', '\\n', 'freeze_4/sst2 Epoch 2: validation_loss = 0.2783',\n'\\n', 'freeze_4/sst2 Epoch 2: AUC_vote=0.5868, DES_vote=0.1467,\nSpearman_vote=0.1477, AUC_kl=0.6807, DES_kl=0.1702, Spearman_kl=0.1878', '\\n',\n'freeze_4/sst2 Epoch 3: validation_loss = 0.2836', '\\n', 'freeze_4/sst2 Epoch 3:\nAUC_vote=0.5983, DES_vote=0.1496, Spearman_vote=0.1657, AUC_kl=0.7318,\nDES_kl=0.1829, Spearman_kl=0.2354', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'freeze_4/yelp_polarity\nEpoch 1: validation_loss = 0.3241', '\\n', 'freeze_4/yelp_polarity Epoch 1:\nAUC_vote=0.4995, DES_vote=0.1249, Spearman_vote=-0.0015, AUC_kl=0.6965,\nDES_kl=0.1741, Spearman_kl=0.2086', '\\n', 'freeze_4/yelp_polarity Epoch 2:\nvalidation_loss = 0.2289', '\\n', 'freeze_4/yelp_polarity Epoch 2:\nAUC_vote=0.5072, DES_vote=0.1268, Spearman_vote=0.0195, AUC_kl=0.8206,\nDES_kl=0.2052, Spearman_kl=0.3098', '\\n', 'freeze_4/yelp_polarity Epoch 3:\nvalidation_loss = 0.2071', '\\n', 'freeze_4/yelp_polarity Epoch 3:\nAUC_vote=0.5404, DES_vote=0.1351, Spearman_vote=0.1006, AUC_kl=0.8271,\nDES_kl=0.2068, Spearman_kl=0.3074', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'freeze_4/imdb Epoch 1:\nvalidation_loss = 0.4381', '\\n', 'freeze_4/imdb Epoch 1: AUC_vote=0.5498,\nDES_vote=0.1374, Spearman_vote=0.2179, AUC_kl=0.6095, DES_kl=0.1524,\nSpearman_kl=0.1296', '\\n', 'freeze_4/imdb Epoch 2: validation_loss = 0.2668',\n'\\n', 'freeze_4/imdb Epoch 2: AUC_vote=0.5420, DES_vote=0.1355,\nSpearman_vote=0.1841, AUC_kl=0.7952, DES_kl=0.1988, Spearman_kl=0.3134', '\\n',\n'freeze_4/imdb Epoch 3: validation_loss = 0.2676', '\\n', 'freeze_4/imdb Epoch 3:\nAUC_vote=0.5421, DES_vote=0.1355, Spearman_vote=0.1617, AUC_kl=0.7828,\nDES_kl=0.1957, Spearman_kl=0.2939', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'freeze_8/sst2 Epoch 1:\nvalidation_loss = 0.5589', '\\n', 'freeze_8/sst2 Epoch 1: AUC_vote=0.5498,\nDES_vote=0.1374, Spearman_vote=0.1045, AUC_kl=0.4719, DES_kl=0.1180,\nSpearman_kl=-0.0365', '\\n', 'freeze_8/sst2 Epoch 2: validation_loss = 0.3190',\n'\\n', 'freeze_8/sst2 Epoch 2: AUC_vote=0.5845, DES_vote=0.1461,\nSpearman_vote=0.1754, AUC_kl=0.7141, DES_kl=0.1785, Spearman_kl=0.2411', '\\n',\n'freeze_8/sst2 Epoch 3: validation_loss = 0.2618', '\\n', 'freeze_8/sst2 Epoch 3:\nAUC_vote=0.7360, DES_vote=0.1840, Spearman_vote=0.4007, AUC_kl=0.7508,\nDES_kl=0.1877, Spearman_kl=0.2486', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'freeze_8/yelp_polarity\nEpoch 1: validation_loss = 0.3410', '\\n', 'freeze_8/yelp_polarity Epoch 1:\nAUC_vote=0.5725, DES_vote=0.1431, Spearman_vote=0.2230, AUC_kl=0.7065,\nDES_kl=0.1766, Spearman_kl=0.2282', '\\n', 'freeze_8/yelp_polarity Epoch 2:\nvalidation_loss = 0.2602', '\\n', 'freeze_8/yelp_polarity Epoch 2:\nAUC_vote=0.5238, DES_vote=0.1310, Spearman_vote=0.0552, AUC_kl=0.8242,\nDES_kl=0.2060, Spearman_kl=0.3214', '\\n', 'freeze_8/yelp_polarity Epoch 3:\nvalidation_loss = 0.2179', '\\n', 'freeze_8/yelp_polarity Epoch 3:\nAUC_vote=0.5510, DES_vote=0.1377, Spearman_vote=0.1620, AUC_kl=0.7867,\nDES_kl=0.1967, Spearman_kl=0.2694', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'freeze_8/imdb Epoch 1:\nvalidation_loss = 0.5386', '\\n', 'freeze_8/imdb Epoch 1: AUC_vote=0.4961,\nDES_vote=0.1240, Spearman_vote=-0.0219, AUC_kl=0.6247, DES_kl=0.1562,\nSpearman_kl=0.1895', '\\n', 'freeze_8/imdb Epoch 2: validation_loss = 0.3469',\n'\\n', 'freeze_8/imdb Epoch 2: AUC_vote=0.5324, DES_vote=0.1331,\nSpearman_vote=0.1115, AUC_kl=0.7137, DES_kl=0.1784, Spearman_kl=0.2644', '\\n',\n'freeze_8/imdb Epoch 3: validation_loss = 0.2933', '\\n', 'freeze_8/imdb Epoch 3:\nAUC_vote=0.5474, DES_vote=0.1368, Spearman_vote=0.1985, AUC_kl=0.7584,\nDES_kl=0.1896, Spearman_kl=0.2624', '\\n', 'Saved experiment_data.npy', '\\n',\n'Execution time: 9 minutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', \"Some weights of BertForSequenceClassification were\nnot initialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'sst2 Epoch 1: val_loss=nan', '\\n', 'sst2 Epoch 1:\nAUC_vote=0.5000, DES_vote=0.0833, AUC_kl=0.5000, DES_kl=0.0833', '\\n', 'sst2\nEpoch 2: val_loss=nan', '\\n', 'sst2 Epoch 2: AUC_vote=0.5000, DES_vote=0.0833,\nAUC_kl=0.5000, DES_kl=0.0833', '\\n', 'sst2 Epoch 3: val_loss=nan', '\\n', 'sst2\nEpoch 3: AUC_vote=0.5000, DES_vote=0.0833, AUC_kl=0.5000, DES_kl=0.0833', '\\n',\n'sst2 Epoch 4: val_loss=nan', '\\n', 'sst2 Epoch 4: AUC_vote=0.5000,\nDES_vote=0.0833, AUC_kl=0.5000, DES_kl=0.0833', '\\n', 'sst2 Epoch 5:\nval_loss=nan', '\\n', 'sst2 Epoch 5: AUC_vote=0.5000, DES_vote=0.0833,\nAUC_kl=0.5000, DES_kl=0.0833', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'yelp_polarity Epoch 1:\nval_loss=nan', '\\n', 'yelp_polarity Epoch 1: AUC_vote=0.5000, DES_vote=0.0833,\nAUC_kl=0.5000, DES_kl=0.0833', '\\n', 'yelp_polarity Epoch 2: val_loss=nan',\n'\\n', 'yelp_polarity Epoch 2: AUC_vote=0.5000, DES_vote=0.0833, AUC_kl=0.5000,\nDES_kl=0.0833', '\\n', 'yelp_polarity Epoch 3: val_loss=nan', '\\n',\n'yelp_polarity Epoch 3: AUC_vote=0.5000, DES_vote=0.0833, AUC_kl=0.5000,\nDES_kl=0.0833', '\\n', 'yelp_polarity Epoch 4: val_loss=nan', '\\n',\n'yelp_polarity Epoch 4: AUC_vote=0.5000, DES_vote=0.0833, AUC_kl=0.5000,\nDES_kl=0.0833', '\\n', 'yelp_polarity Epoch 5: val_loss=nan', '\\n',\n'yelp_polarity Epoch 5: AUC_vote=0.5000, DES_vote=0.0833, AUC_kl=0.5000,\nDES_kl=0.0833', '\\n', \"Some weights of BertForSequenceClassification were not\ninitialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'imdb Epoch 1: val_loss=nan', '\\n', 'imdb Epoch 1:\nAUC_vote=0.5000, DES_vote=0.0833, AUC_kl=0.5000, DES_kl=0.0833', '\\n', 'imdb\nEpoch 2: val_loss=nan', '\\n', 'imdb Epoch 2: AUC_vote=0.5000, DES_vote=0.0833,\nAUC_kl=0.5000, DES_kl=0.0833', '\\n', 'imdb Epoch 3: val_loss=nan', '\\n', 'imdb\nEpoch 3: AUC_vote=0.5000, DES_vote=0.0833, AUC_kl=0.5000, DES_kl=0.0833', '\\n',\n'imdb Epoch 4: val_loss=nan', '\\n', 'imdb Epoch 4: AUC_vote=0.5000,\nDES_vote=0.0833, AUC_kl=0.5000, DES_kl=0.0833', '\\n', 'imdb Epoch 5:\nval_loss=nan', '\\n', 'imdb Epoch 5: AUC_vote=0.5000, DES_vote=0.0833,\nAUC_kl=0.5000, DES_kl=0.0833', '\\n', 'Execution time: 21 minutes seconds (time\nlimit is an hour).']", "[\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'Execution time: 46 minutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', \"Some weights of BertForSequenceClassification were\nnot initialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'sst2 Epoch 1: validation_loss = 0.6948', '\\n', 'sst2 Epoch 1:\nAUC_vote=0.5231, DES_vote=0.0872, AUC_kl=0.5656, DES_kl=0.0943,\nSpearman_vote=0.0494, Spearman_kl=0.1134', '\\n', 'sst2 Epoch 2: validation_loss\n= 0.6930', '\\n', 'sst2 Epoch 2: AUC_vote=0.4951, DES_vote=0.0825, AUC_kl=0.5299,\nDES_kl=0.0883, Spearman_vote=-0.0171, Spearman_kl=0.0517', '\\n', 'sst2 Epoch 3:\nvalidation_loss = 0.6915', '\\n', 'sst2 Epoch 3: AUC_vote=0.4985,\nDES_vote=0.0831, AUC_kl=0.5395, DES_kl=0.0899, Spearman_vote=-0.0059,\nSpearman_kl=0.0683', '\\n', 'sst2 Epoch 4: validation_loss = 0.6925', '\\n', 'sst2\nEpoch 4: AUC_vote=0.4918, DES_vote=0.0820, AUC_kl=0.5237, DES_kl=0.0873,\nSpearman_vote=-0.0451, Spearman_kl=0.0409', '\\n', 'sst2 Epoch 5: validation_loss\n= 0.6902', '\\n', 'sst2 Epoch 5: AUC_vote=0.4955, DES_vote=0.0826, AUC_kl=0.5151,\nDES_kl=0.0859, Spearman_vote=-0.0282, Spearman_kl=0.0261', '\\n', \"Some weights\nof BertForSequenceClassification were not initialized from the model checkpoint\nat bert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'yelp_polarity Epoch 1:\nvalidation_loss = 0.6838', '\\n', 'yelp_polarity Epoch 1: AUC_vote=0.4834,\nDES_vote=0.0806, AUC_kl=0.5430, DES_kl=0.0905, Spearman_vote=-0.0410,\nSpearman_kl=0.0743', '\\n', 'yelp_polarity Epoch 2: validation_loss = 0.5866',\n'\\n', 'yelp_polarity Epoch 2: AUC_vote=0.5775, DES_vote=0.0962, AUC_kl=0.6127,\nDES_kl=0.1021, Spearman_vote=0.2054, Spearman_kl=0.1671', '\\n', 'yelp_polarity\nEpoch 3: validation_loss = 0.5196', '\\n', 'yelp_polarity Epoch 3:\nAUC_vote=0.5514, DES_vote=0.0919, AUC_kl=0.6611, DES_kl=0.1102,\nSpearman_vote=0.1438, Spearman_kl=0.2326', '\\n', 'yelp_polarity Epoch 4:\nvalidation_loss = 0.7013', '\\n', 'yelp_polarity Epoch 4: AUC_vote=0.5021,\nDES_vote=0.0837, AUC_kl=0.4949, DES_kl=0.0825, Spearman_vote=0.0271,\nSpearman_kl=-0.0089', '\\n', 'yelp_polarity Epoch 5: validation_loss = 0.6919',\n'\\n', 'yelp_polarity Epoch 5: AUC_vote=0.4960, DES_vote=0.0827, AUC_kl=0.4432,\nDES_kl=0.0739, Spearman_vote=-0.0316, Spearman_kl=-0.0985', '\\n', \"Some weights\nof BertForSequenceClassification were not initialized from the model checkpoint\nat bert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'imdb Epoch 1:\nvalidation_loss = 0.6669', '\\n', 'imdb Epoch 1: AUC_vote=0.5046,\nDES_vote=0.0841, AUC_kl=0.5206, DES_kl=0.0868, Spearman_vote=0.0109,\nSpearman_kl=0.0347', '\\n', 'imdb Epoch 2: validation_loss = 0.6700', '\\n', 'imdb\nEpoch 2: AUC_vote=0.5316, DES_vote=0.0886, AUC_kl=0.5637, DES_kl=0.0940,\nSpearman_vote=0.1021, Spearman_kl=0.1081', '\\n', 'imdb Epoch 3: validation_loss\n= 0.6292', '\\n', 'imdb Epoch 3: AUC_vote=0.5220, DES_vote=0.0870, AUC_kl=0.6258,\nDES_kl=0.1043, Spearman_vote=0.0760, Spearman_kl=0.2049', '\\n', 'imdb Epoch 4:\nvalidation_loss = 0.6927', '\\n', 'imdb Epoch 4: AUC_vote=0.5000,\nDES_vote=0.0833, AUC_kl=0.4889, DES_kl=0.0815, Spearman_vote=nan,\nSpearman_kl=-0.0193', '\\n', 'imdb Epoch 5: validation_loss = 0.6621', '\\n',\n'imdb Epoch 5: AUC_vote=0.5096, DES_vote=0.0849, AUC_kl=0.5993, DES_kl=0.0999,\nSpearman_vote=0.0342, Spearman_kl=0.1610', '\\n', 'Execution time: 23 minutes\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', \"Some weights of BertForSequenceClassification were\nnot initialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'sst2 Epoch 1: val_loss=0.3722, AUC_vote=0.6254, DES_vote=0.1042,\nAUC_kl=0.7254, DES_kl=0.1209', '\\n', 'sst2 Epoch 2: val_loss=0.2879,\nAUC_vote=0.6729, DES_vote=0.1121, AUC_kl=0.7414, DES_kl=0.1236', '\\n', 'sst2\nEpoch 3: val_loss=0.2904, AUC_vote=0.6361, DES_vote=0.1060, AUC_kl=0.7260,\nDES_kl=0.1210', '\\n', 'sst2 Epoch 4: val_loss=0.3355, AUC_vote=0.6444,\nDES_vote=0.1074, AUC_kl=0.6998, DES_kl=0.1166', '\\n', 'sst2 Epoch 5:\nval_loss=0.4356, AUC_vote=0.6750, DES_vote=0.1125, AUC_kl=0.7675,\nDES_kl=0.1279', '\\n', \"Some weights of BertForSequenceClassification were not\ninitialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", 'yelp_polarity Epoch 1: val_loss=0.1428, AUC_vote=0.5417,\nDES_vote=0.0903, AUC_kl=0.7679, DES_kl=0.1280', '\\n', 'yelp_polarity Epoch 2:\nval_loss=0.1483, AUC_vote=0.5939, DES_vote=0.0990, AUC_kl=0.8482,\nDES_kl=0.1414', '\\n', 'yelp_polarity Epoch 3: val_loss=0.1553, AUC_vote=0.6151,\nDES_vote=0.1025, AUC_kl=0.8382, DES_kl=0.1397', '\\n', 'yelp_polarity Epoch 4:\nval_loss=0.1845, AUC_vote=0.5710, DES_vote=0.0952, AUC_kl=0.8834,\nDES_kl=0.1472', '\\n', 'yelp_polarity Epoch 5: val_loss=0.2583, AUC_vote=0.5815,\nDES_vote=0.0969, AUC_kl=0.8475, DES_kl=0.1413', '\\n', \"Some weights of\nBertForSequenceClassification were not initialized from the model checkpoint at\nbert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", 'imdb Epoch 1:\nval_loss=0.2304, AUC_vote=0.5399, DES_vote=0.0900, AUC_kl=0.8061,\nDES_kl=0.1344', '\\n', 'imdb Epoch 2: val_loss=0.2149, AUC_vote=0.5450,\nDES_vote=0.0908, AUC_kl=0.8526, DES_kl=0.1421', '\\n', 'imdb Epoch 3:\nval_loss=0.3003, AUC_vote=0.5802, DES_vote=0.0967, AUC_kl=0.8511,\nDES_kl=0.1418', '\\n', 'imdb Epoch 4: val_loss=0.2780, AUC_vote=0.6080,\nDES_vote=0.1013, AUC_kl=0.8620, DES_kl=0.1437', '\\n', 'imdb Epoch 5:\nval_loss=0.3050, AUC_vote=0.5465, DES_vote=0.0911, AUC_kl=0.8187,\nDES_kl=0.1365', '\\n', 'Execution time: 23 minutes seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', \"Some weights of BertForSequenceClassification were\nnot initialized from the model checkpoint at bert-base-uncased and are newly\ninitialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN\nthis model on a down-stream task to be able to use it for predictions and\ninference.\\n\", '[full_depth/sst2] Epoch 1: val_loss=0.4220', '\\n',\n'[full_depth/sst2] AUC_vote=0.6738,DES_vote=0.1123,AUC_kl=0.7760,DES_kl=0.1293',\n'\\n', '[full_depth/sst2] Epoch 2: val_loss=0.2961', '\\n', '[full_depth/sst2]\nAUC_vote=0.6317,DES_vote=0.1053,AUC_kl=0.7215,DES_kl=0.1203', '\\n',\n'[full_depth/sst2] Epoch 3: val_loss=0.3114', '\\n', '[full_depth/sst2]\nAUC_vote=0.6977,DES_vote=0.1163,AUC_kl=0.7857,DES_kl=0.1309', '\\n',\n'[full_depth/sst2] Epoch 4: val_loss=0.4406', '\\n', '[full_depth/sst2]\nAUC_vote=0.6514,DES_vote=0.1086,AUC_kl=0.7611,DES_kl=0.1268', '\\n',\n'[full_depth/sst2] Epoch 5: val_loss=0.4367', '\\n', '[full_depth/sst2]\nAUC_vote=0.6565,DES_vote=0.1094,AUC_kl=0.7603,DES_kl=0.1267', '\\n', \"Some\nweights of BertForSequenceClassification were not initialized from the model\ncheckpoint at bert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\",\n'[full_depth/yelp_polarity] Epoch 1: val_loss=0.1502', '\\n',\n'[full_depth/yelp_polarity]\nAUC_vote=0.5694,DES_vote=0.0949,AUC_kl=0.8431,DES_kl=0.1405', '\\n',\n'[full_depth/yelp_polarity] Epoch 2: val_loss=0.1369', '\\n',\n'[full_depth/yelp_polarity]\nAUC_vote=0.6041,DES_vote=0.1007,AUC_kl=0.8385,DES_kl=0.1397', '\\n',\n'[full_depth/yelp_polarity] Epoch 3: val_loss=0.1944', '\\n',\n'[full_depth/yelp_polarity]\nAUC_vote=0.6270,DES_vote=0.1045,AUC_kl=0.8356,DES_kl=0.1393', '\\n',\n'[full_depth/yelp_polarity] Epoch 4: val_loss=0.2588', '\\n',\n'[full_depth/yelp_polarity]\nAUC_vote=0.5491,DES_vote=0.0915,AUC_kl=0.8822,DES_kl=0.1470', '\\n',\n'[full_depth/yelp_polarity] Epoch 5: val_loss=0.1385', '\\n',\n'[full_depth/yelp_polarity]\nAUC_vote=0.5283,DES_vote=0.0881,AUC_kl=0.8630,DES_kl=0.1438', '\\n', \"Some\nweights of BertForSequenceClassification were not initialized from the model\ncheckpoint at bert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", '[full_depth/imdb] Epoch\n1: val_loss=0.2585', '\\n', '[full_depth/imdb]\nAUC_vote=0.5890,DES_vote=0.0982,AUC_kl=0.8416,DES_kl=0.1403', '\\n',\n'[full_depth/imdb] Epoch 2: val_loss=0.2258', '\\n', '[full_depth/imdb]\nAUC_vote=0.5934,DES_vote=0.0989,AUC_kl=0.8803,DES_kl=0.1467', '\\n',\n'[full_depth/imdb] Epoch 3: val_loss=0.3518', '\\n', '[full_depth/imdb]\nAUC_vote=0.5852,DES_vote=0.0975,AUC_kl=0.8156,DES_kl=0.1359', '\\n',\n'[full_depth/imdb] Epoch 4: val_loss=0.3894', '\\n', '[full_depth/imdb]\nAUC_vote=0.6074,DES_vote=0.1012,AUC_kl=0.8361,DES_kl=0.1393', '\\n',\n'[full_depth/imdb] Epoch 5: val_loss=0.3104', '\\n', '[full_depth/imdb]\nAUC_vote=0.6011,DES_vote=0.1002,AUC_kl=0.8298,DES_kl=0.1383', '\\n', \"Some\nweights of BertForSequenceClassification were not initialized from the model\ncheckpoint at bert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", '[reduced_depth/sst2]\nEpoch 1: val_loss=0.3795', '\\n', '[reduced_depth/sst2]\nAUC_vote=0.6138,DES_vote=0.1023,AUC_kl=0.6196,DES_kl=0.1033', '\\n',\n'[reduced_depth/sst2] Epoch 2: val_loss=0.3785', '\\n', '[reduced_depth/sst2]\nAUC_vote=0.5912,DES_vote=0.0985,AUC_kl=0.6160,DES_kl=0.1027', '\\n',\n'[reduced_depth/sst2] Epoch 3: val_loss=0.4092', '\\n', '[reduced_depth/sst2]\nAUC_vote=0.6157,DES_vote=0.1026,AUC_kl=0.6734,DES_kl=0.1122', '\\n',\n'[reduced_depth/sst2] Epoch 4: val_loss=0.6403', '\\n', '[reduced_depth/sst2]\nAUC_vote=0.6491,DES_vote=0.1082,AUC_kl=0.7025,DES_kl=0.1171', '\\n',\n'[reduced_depth/sst2] Epoch 5: val_loss=0.6043', '\\n', '[reduced_depth/sst2]\nAUC_vote=0.5924,DES_vote=0.0987,AUC_kl=0.6697,DES_kl=0.1116', '\\n', \"Some\nweights of BertForSequenceClassification were not initialized from the model\ncheckpoint at bert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\",\n'[reduced_depth/yelp_polarity] Epoch 1: val_loss=0.2836', '\\n',\n'[reduced_depth/yelp_polarity]\nAUC_vote=0.5910,DES_vote=0.0985,AUC_kl=0.7824,DES_kl=0.1304', '\\n',\n'[reduced_depth/yelp_polarity] Epoch 2: val_loss=0.2265', '\\n',\n'[reduced_depth/yelp_polarity]\nAUC_vote=0.5695,DES_vote=0.0949,AUC_kl=0.7661,DES_kl=0.1277', '\\n',\n'[reduced_depth/yelp_polarity] Epoch 3: val_loss=0.2623', '\\n',\n'[reduced_depth/yelp_polarity]\nAUC_vote=0.5617,DES_vote=0.0936,AUC_kl=0.8317,DES_kl=0.1386', '\\n',\n'[reduced_depth/yelp_polarity] Epoch 4: val_loss=0.2753', '\\n',\n'[reduced_depth/yelp_polarity]\nAUC_vote=0.5630,DES_vote=0.0938,AUC_kl=0.8027,DES_kl=0.1338', '\\n',\n'[reduced_depth/yelp_polarity] Epoch 5: val_loss=0.3684', '\\n',\n'[reduced_depth/yelp_polarity]\nAUC_vote=0.5817,DES_vote=0.0970,AUC_kl=0.8311,DES_kl=0.1385', '\\n', \"Some\nweights of BertForSequenceClassification were not initialized from the model\ncheckpoint at bert-base-uncased and are newly initialized: ['classifier.bias',\n'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task\nto be able to use it for predictions and inference.\\n\", '[reduced_depth/imdb]\nEpoch 1: val_loss=0.3282', '\\n', '[reduced_depth/imdb]\nAUC_vote=0.5251,DES_vote=0.0875,AUC_kl=0.6614,DES_kl=0.1102', '\\n',\n'[reduced_depth/imdb] Epoch 2: val_loss=0.2763', '\\n', '[reduced_depth/imdb]\nAUC_vote=0.5271,DES_vote=0.0878,AUC_kl=0.7356,DES_kl=0.1226', '\\n',\n'[reduced_depth/imdb] Epoch 3: val_loss=0.3437', '\\n', '[reduced_depth/imdb]\nAUC_vote=0.5779,DES_vote=0.0963,AUC_kl=0.7634,DES_kl=0.1272', '\\n',\n'[reduced_depth/imdb] Epoch 4: val_loss=0.3468', '\\n', '[reduced_depth/imdb]\nAUC_vote=0.5786,DES_vote=0.0964,AUC_kl=0.8222,DES_kl=0.1370', '\\n',\n'[reduced_depth/imdb] Epoch 5: val_loss=0.3892', '\\n', '[reduced_depth/imdb]\nAUC_vote=0.5288,DES_vote=0.0881,AUC_kl=0.7998,DES_kl=0.1333', '\\n', 'Execution\ntime: 36 minutes seconds (time limit is an hour).']", "[\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'Execution time: 47 minutes seconds (time limit is an hour).']", "[\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'Execution time: 47 minutes seconds (time limit is an hour).']", "[\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n\"Some weights of BertForSequenceClassification were not initialized from the\nmodel checkpoint at bert-base-uncased and are newly initialized:\n['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model\non a down-stream task to be able to use it for predictions and inference.\\n\",\n'Execution time: 47 minutes seconds (time limit is an hour).']", ""], "analysis": ["", "", "The training script was interrupted by a KeyboardInterrupt/TimeoutError after\nexceeding the one-hour execution limit during the freeze_8/imdb run, so the full\nablation study did not complete. To fix this, reduce the computational budget:\ndecrease train_size/val_size, lower the number of epochs or K, use a smaller\nmodel (e.g. DistilBERT), or reduce batch size. You can also implement early\nstopping or checkpointing to avoid full retraining if metrics plateau.", "", "", "", "The script never completed all ablation experiments\u2014during the \u2018freeze_8\u2019\nconfiguration on the IMDb dataset, it hits a KeyboardInterrupt/TimeoutError\nafter Epoch 3 due to exceeding the one-hour execution limit. There are no coding\nerrors (exceptions) other than the timeout, but the end-to-end run is too slow.\nTo fix this, reduce the overall compute load by using smaller train/validation\nsplits, fewer epochs or paraphrase variants (lower K), or vectorize/parallelize\nthe uncertainty\u2010computation loops (e.g., batch KL computations) or split the\nablation jobs into separate, shorter runs.", "The training run on the paraphrase_aug ablation for the Yelp polarity dataset\nexceeded the one-hour limit and was terminated (TimeoutError). The large\naugmented training set (5,000 original samples + 5\u00d7 paraphrases each = 30,000\nexamples) plus five epochs, three datasets, and per-sample paraphrase\ngeneration/KL computations caused the runtime blowup. To fix this, reduce the\naugmentation factor K, use fewer epochs/datasets, trim the train_size, or\nbatch/parallelize paraphrase generation and KL computations to speed up\ntraining.", "", "", "Custom ablation modules (NoResSelfOutput, NoResOutput) are instantiated on CPU\nand loaded with CPU weights, but the model and inputs are on CUDA, causing a\ndevice mismatch at runtime. Fix by moving the new modules to the correct device\n(e.g., call new_so.to(device) and new_fo.to(device) after loading their state\ndicts) or instantiate them directly on the GPU.", "", "", "The validation losses are NaN across all epochs, indicating training has\ndiverged. The complete ablation of LayerNorm (replacing with Identity)\ndestabilizes the transformer, leading to exploding / invalid values. To fix\nthis, either reduce the learning rate, add gradient clipping, restore at least\none LayerNorm (e.g., keep output LayerNorm), or introduce a small epsilon in the\nIdentity layer to maintain numerical stability.", "", "", "", "", "", "", "", ""], "exc_type": [null, null, "TimeoutError", null, null, null, "TimeoutError", "TimeoutError", null, null, "RuntimeError", null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, {"args": []}, null, null, null, {"args": []}, {"args": []}, null, null, {"args": ["Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"]}, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 118, "<module>", "train_losses.append(out.loss.item())"]], null, null, null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 131, "<module>", "batch_losses.append(out.loss.item())"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 122, "<module>", "tr_losses.append(out.loss.item())"]], null, null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 134, "<module>", "out = model(input_ids=ids, attention_mask=mask, labels=labels)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", 1503, "forward", "outputs = self.bert("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", 1016, "forward", "encoder_outputs = self.encoder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", 662, "forward", "layer_outputs = layer_module("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", 552, "forward", "self_attention_outputs = self.attention("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", 491, "forward", "attention_output = self.output(self_outputs[0], hidden_states)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["runfile.py", 29, "forward", "hidden_states = self.dense(hidden_states)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/linear.py", 125, "forward", "return F.linear(input, self.weight, self.bias)"]], null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "sst2", "final_value": 0.1094, "best_value": 0.1094}, {"dataset_name": "yelp_polarity", "final_value": 0.0549, "best_value": 0.0549}, {"dataset_name": "imdb", "final_value": 0.1021, "best_value": 0.1021}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "sst2", "final_value": 0.4326, "best_value": 0.4326}, {"dataset_name": "yelp_polarity", "final_value": 0.3306, "best_value": 0.3306}, {"dataset_name": "imdb", "final_value": 0.4118, "best_value": 0.4118}]}, {"metric_name": "detection AUC", "lower_is_better": false, "description": "Area under the ROC curve for detection", "data": [{"dataset_name": "sst2", "final_value": 0.6332, "best_value": 0.6332}, {"dataset_name": "yelp_polarity", "final_value": 0.5486, "best_value": 0.5486}, {"dataset_name": "imdb", "final_value": 0.509, "best_value": 0.509}]}, {"metric_name": "detection DES", "lower_is_better": true, "description": "Detection error score", "data": [{"dataset_name": "sst2", "final_value": 0.1583, "best_value": 0.1583}, {"dataset_name": "yelp_polarity", "final_value": 0.1372, "best_value": 0.1372}, {"dataset_name": "imdb", "final_value": 0.1273, "best_value": 0.1273}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "sst2", "final_value": 0.835, "best_value": 0.835}, {"dataset_name": "yelp_polarity", "final_value": 0.885, "best_value": 0.885}, {"dataset_name": "imdb", "final_value": 0.86, "best_value": 0.86}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "sst2", "final_value": 0.683, "best_value": 0.683}, {"dataset_name": "yelp_polarity", "final_value": 0.6736, "best_value": 0.6736}, {"dataset_name": "imdb", "final_value": 0.6807, "best_value": 0.6807}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "sst2", "final_value": 0.6846, "best_value": 0.6846}, {"dataset_name": "yelp_polarity", "final_value": 0.6693, "best_value": 0.6693}, {"dataset_name": "imdb", "final_value": 0.6797, "best_value": 0.6797}]}, {"metric_name": "detection AUC (vote)", "lower_is_better": false, "description": "Detection AUC using vote method", "data": [{"dataset_name": "sst2", "final_value": 0.5145, "best_value": 0.5145}, {"dataset_name": "yelp_polarity", "final_value": 0.5314, "best_value": 0.5314}, {"dataset_name": "imdb", "final_value": 0.5218, "best_value": 0.5218}]}, {"metric_name": "detection DES (vote)", "lower_is_better": true, "description": "Detection DES using vote method", "data": [{"dataset_name": "sst2", "final_value": 0.0857, "best_value": 0.0857}, {"dataset_name": "yelp_polarity", "final_value": 0.0886, "best_value": 0.0886}, {"dataset_name": "imdb", "final_value": 0.087, "best_value": 0.087}]}, {"metric_name": "detection AUC (KL)", "lower_is_better": false, "description": "Detection AUC using KL method", "data": [{"dataset_name": "sst2", "final_value": 0.5176, "best_value": 0.5176}, {"dataset_name": "yelp_polarity", "final_value": 0.5257, "best_value": 0.5257}, {"dataset_name": "imdb", "final_value": 0.5111, "best_value": 0.5111}]}, {"metric_name": "detection DES (KL)", "lower_is_better": true, "description": "Detection DES using KL method", "data": [{"dataset_name": "sst2", "final_value": 0.0863, "best_value": 0.0863}, {"dataset_name": "yelp_polarity", "final_value": 0.0876, "best_value": 0.0876}, {"dataset_name": "imdb", "final_value": 0.0852, "best_value": 0.0852}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test set accuracy", "data": [{"dataset_name": "sst2", "final_value": 0.534, "best_value": 0.534}, {"dataset_name": "yelp_polarity", "final_value": 0.612, "best_value": 0.612}, {"dataset_name": "imdb", "final_value": 0.586, "best_value": 0.586}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Training loss on the dataset", "data": [{"dataset_name": "sst2", "final_value": 0.2254, "best_value": 0.2254}, {"dataset_name": "yelp_polarity", "final_value": 0.1953, "best_value": 0.1953}, {"dataset_name": "imdb", "final_value": 0.6333, "best_value": 0.6333}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss on the dataset", "data": [{"dataset_name": "sst2", "final_value": 0.6035, "best_value": 0.6035}, {"dataset_name": "yelp_polarity", "final_value": 0.2818, "best_value": 0.2818}, {"dataset_name": "imdb", "final_value": 0.5007, "best_value": 0.5007}]}, {"metric_name": "detection AUC (voting)", "lower_is_better": false, "description": "Out-of-distribution detection AUC using the voting method", "data": [{"dataset_name": "sst2", "final_value": 0.5366, "best_value": 0.5366}, {"dataset_name": "yelp_polarity", "final_value": 0.6478, "best_value": 0.6478}, {"dataset_name": "imdb", "final_value": 0.5064, "best_value": 0.5064}]}, {"metric_name": "detection DES (voting)", "lower_is_better": true, "description": "Out-of-distribution detection DES using the voting method", "data": [{"dataset_name": "sst2", "final_value": 0.0894, "best_value": 0.0894}, {"dataset_name": "yelp_polarity", "final_value": 0.108, "best_value": 0.108}, {"dataset_name": "imdb", "final_value": 0.0844, "best_value": 0.0844}]}, {"metric_name": "detection AUC (kl)", "lower_is_better": false, "description": "Out-of-distribution detection AUC using the KL divergence method", "data": [{"dataset_name": "sst2", "final_value": 0.5369, "best_value": 0.5369}, {"dataset_name": "yelp_polarity", "final_value": 0.8605, "best_value": 0.8605}, {"dataset_name": "imdb", "final_value": 0.6649, "best_value": 0.6649}]}, {"metric_name": "detection DES (kl)", "lower_is_better": true, "description": "Out-of-distribution detection DES using the KL divergence method", "data": [{"dataset_name": "sst2", "final_value": 0.0895, "best_value": 0.0895}, {"dataset_name": "yelp_polarity", "final_value": 0.1434, "best_value": 0.1434}, {"dataset_name": "imdb", "final_value": 0.1108, "best_value": 0.1108}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test accuracy on the dataset", "data": [{"dataset_name": "sst2", "final_value": 0.728, "best_value": 0.728}, {"dataset_name": "yelp_polarity", "final_value": 0.884, "best_value": 0.884}, {"dataset_name": "imdb", "final_value": 0.778, "best_value": 0.778}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "sst2", "final_value": 0.0442, "best_value": 0.0442}, {"dataset_name": "yelp_polarity", "final_value": 0.0619, "best_value": 0.0619}, {"dataset_name": "imdb", "final_value": 0.1096, "best_value": 0.1096}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "sst2", "final_value": 0.8616, "best_value": 0.8616}, {"dataset_name": "yelp_polarity", "final_value": 0.3378, "best_value": 0.3378}, {"dataset_name": "imdb", "final_value": 0.4163, "best_value": 0.4163}]}, {"metric_name": "training accuracy", "lower_is_better": false, "description": "Final training accuracy", "data": [{"dataset_name": "sst2", "final_value": 0.9918, "best_value": 0.9918}, {"dataset_name": "yelp_polarity", "final_value": 0.99, "best_value": 0.99}, {"dataset_name": "imdb", "final_value": 0.9848, "best_value": 0.9848}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Final validation accuracy", "data": [{"dataset_name": "sst2", "final_value": 0.766, "best_value": 0.766}, {"dataset_name": "yelp_polarity", "final_value": 0.896, "best_value": 0.896}, {"dataset_name": "imdb", "final_value": 0.886, "best_value": 0.886}]}, {"metric_name": "detection AUC (vote)", "lower_is_better": false, "description": "Detection AUC based on vote", "data": [{"dataset_name": "sst2", "final_value": 0.5541, "best_value": 0.5541}, {"dataset_name": "yelp_polarity", "final_value": 0.623, "best_value": 0.623}, {"dataset_name": "imdb", "final_value": 0.5436, "best_value": 0.5436}]}, {"metric_name": "detection DES score (vote)", "lower_is_better": true, "description": "Detection DES score based on vote", "data": [{"dataset_name": "sst2", "final_value": 0.0924, "best_value": 0.0924}, {"dataset_name": "yelp_polarity", "final_value": 0.1038, "best_value": 0.1038}, {"dataset_name": "imdb", "final_value": 0.0906, "best_value": 0.0906}]}, {"metric_name": "detection AUC (KL divergence)", "lower_is_better": false, "description": "Detection AUC based on KL divergence", "data": [{"dataset_name": "sst2", "final_value": 0.6655, "best_value": 0.6655}, {"dataset_name": "yelp_polarity", "final_value": 0.8437, "best_value": 0.8437}, {"dataset_name": "imdb", "final_value": 0.7663, "best_value": 0.7663}]}, {"metric_name": "detection DES score (KL divergence)", "lower_is_better": true, "description": "Detection DES score based on KL divergence", "data": [{"dataset_name": "sst2", "final_value": 0.1109, "best_value": 0.1109}, {"dataset_name": "yelp_polarity", "final_value": 0.1406, "best_value": 0.1406}, {"dataset_name": "imdb", "final_value": 0.1277, "best_value": 0.1277}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "sst2", "final_value": 0.0131, "best_value": 0.0131}, {"dataset_name": "yelp_polarity", "final_value": 0.0099, "best_value": 0.0099}, {"dataset_name": "imdb", "final_value": 0.0178, "best_value": 0.0178}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "sst2", "final_value": 0.5191, "best_value": 0.5191}, {"dataset_name": "yelp_polarity", "final_value": 0.2328, "best_value": 0.2328}, {"dataset_name": "imdb", "final_value": 0.3398, "best_value": 0.3398}]}, {"metric_name": "detection AUC (vote)", "lower_is_better": false, "description": "Area Under the Curve for detection using voting", "data": [{"dataset_name": "sst2", "final_value": 0.6403, "best_value": 0.6403}, {"dataset_name": "yelp_polarity", "final_value": 0.6066, "best_value": 0.6066}, {"dataset_name": "imdb", "final_value": 0.6107, "best_value": 0.6107}]}, {"metric_name": "detection DES (vote)", "lower_is_better": true, "description": "Detection error score using voting method", "data": [{"dataset_name": "sst2", "final_value": 0.1067, "best_value": 0.1067}, {"dataset_name": "yelp_polarity", "final_value": 0.1011, "best_value": 0.1011}, {"dataset_name": "imdb", "final_value": 0.1018, "best_value": 0.1018}]}, {"metric_name": "detection AUC (KL)", "lower_is_better": false, "description": "Area Under the Curve for detection using KL divergence", "data": [{"dataset_name": "sst2", "final_value": 0.7488, "best_value": 0.7488}, {"dataset_name": "yelp_polarity", "final_value": 0.873, "best_value": 0.873}, {"dataset_name": "imdb", "final_value": 0.8708, "best_value": 0.8708}]}, {"metric_name": "detection DES (KL)", "lower_is_better": true, "description": "Detection error score using KL divergence", "data": [{"dataset_name": "sst2", "final_value": 0.1248, "best_value": 0.1248}, {"dataset_name": "yelp_polarity", "final_value": 0.1455, "best_value": 0.1455}, {"dataset_name": "imdb", "final_value": 0.1451, "best_value": 0.1451}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "full_heads training loss", "lower_is_better": true, "description": "Cross-entropy loss on training set for full_heads experiment", "data": [{"dataset_name": "sst2", "final_value": 0.0328, "best_value": 0.0328}, {"dataset_name": "yelp_polarity", "final_value": 0.0168, "best_value": 0.0168}, {"dataset_name": "imdb", "final_value": 0.0307, "best_value": 0.0307}]}, {"metric_name": "full_heads training accuracy", "lower_is_better": false, "description": "Accuracy on training set for full_heads experiment", "data": [{"dataset_name": "sst2", "final_value": 0.9914, "best_value": 0.9914}, {"dataset_name": "yelp_polarity", "final_value": 0.9948, "best_value": 0.9948}, {"dataset_name": "imdb", "final_value": 0.994, "best_value": 0.994}]}, {"metric_name": "full_heads validation loss", "lower_is_better": true, "description": "Cross-entropy loss on validation set for full_heads experiment", "data": [{"dataset_name": "sst2", "final_value": 0.4328, "best_value": 0.4328}, {"dataset_name": "yelp_polarity", "final_value": 0.2148, "best_value": 0.2148}, {"dataset_name": "imdb", "final_value": 0.3369, "best_value": 0.3369}]}, {"metric_name": "full_heads validation accuracy", "lower_is_better": false, "description": "Accuracy on validation set for full_heads experiment", "data": [{"dataset_name": "sst2", "final_value": 0.902, "best_value": 0.902}, {"dataset_name": "yelp_polarity", "final_value": 0.95, "best_value": 0.95}, {"dataset_name": "imdb", "final_value": 0.916, "best_value": 0.916}]}, {"metric_name": "full_heads detection AUC (vote)", "lower_is_better": false, "description": "Detection AUC using vote method for full_heads experiment", "data": [{"dataset_name": "sst2", "final_value": 0.657, "best_value": 0.657}, {"dataset_name": "yelp_polarity", "final_value": 0.608, "best_value": 0.608}, {"dataset_name": "imdb", "final_value": 0.6008, "best_value": 0.6008}]}, {"metric_name": "full_heads detection DES (vote)", "lower_is_better": true, "description": "Detection DES using vote method for full_heads experiment", "data": [{"dataset_name": "sst2", "final_value": 0.1095, "best_value": 0.1095}, {"dataset_name": "yelp_polarity", "final_value": 0.1013, "best_value": 0.1013}, {"dataset_name": "imdb", "final_value": 0.1001, "best_value": 0.1001}]}, {"metric_name": "full_heads detection AUC (kl)", "lower_is_better": false, "description": "Detection AUC using KL method for full_heads experiment", "data": [{"dataset_name": "sst2", "final_value": 0.7789, "best_value": 0.7789}, {"dataset_name": "yelp_polarity", "final_value": 0.8731, "best_value": 0.8731}, {"dataset_name": "imdb", "final_value": 0.8672, "best_value": 0.8672}]}, {"metric_name": "full_heads detection DES (kl)", "lower_is_better": true, "description": "Detection DES using KL method for full_heads experiment", "data": [{"dataset_name": "sst2", "final_value": 0.1298, "best_value": 0.1298}, {"dataset_name": "yelp_polarity", "final_value": 0.1455, "best_value": 0.1455}, {"dataset_name": "imdb", "final_value": 0.1445, "best_value": 0.1445}]}, {"metric_name": "pruned_heads training loss", "lower_is_better": true, "description": "Cross-entropy loss on training set for pruned_heads experiment", "data": [{"dataset_name": "sst2", "final_value": 0.0409, "best_value": 0.0409}, {"dataset_name": "yelp_polarity", "final_value": 0.0185, "best_value": 0.0185}, {"dataset_name": "imdb", "final_value": 0.0419, "best_value": 0.0419}]}, {"metric_name": "pruned_heads training accuracy", "lower_is_better": false, "description": "Accuracy on training set for pruned_heads experiment", "data": [{"dataset_name": "sst2", "final_value": 0.9888, "best_value": 0.9888}, {"dataset_name": "yelp_polarity", "final_value": 0.994, "best_value": 0.994}, {"dataset_name": "imdb", "final_value": 0.9862, "best_value": 0.9862}]}, {"metric_name": "pruned_heads validation loss", "lower_is_better": true, "description": "Cross-entropy loss on validation set for pruned_heads experiment", "data": [{"dataset_name": "sst2", "final_value": 0.4355, "best_value": 0.4355}, {"dataset_name": "yelp_polarity", "final_value": 0.2304, "best_value": 0.2304}, {"dataset_name": "imdb", "final_value": 0.3156, "best_value": 0.3156}]}, {"metric_name": "pruned_heads validation accuracy", "lower_is_better": false, "description": "Accuracy on validation set for pruned_heads experiment", "data": [{"dataset_name": "sst2", "final_value": 0.884, "best_value": 0.884}, {"dataset_name": "yelp_polarity", "final_value": 0.93, "best_value": 0.93}, {"dataset_name": "imdb", "final_value": 0.908, "best_value": 0.908}]}, {"metric_name": "pruned_heads detection AUC (vote)", "lower_is_better": false, "description": "Detection AUC using vote method for pruned_heads experiment", "data": [{"dataset_name": "sst2", "final_value": 0.6112, "best_value": 0.6112}, {"dataset_name": "yelp_polarity", "final_value": 0.6157, "best_value": 0.6157}, {"dataset_name": "imdb", "final_value": 0.6072, "best_value": 0.6072}]}, {"metric_name": "pruned_heads detection DES (vote)", "lower_is_better": true, "description": "Detection DES using vote method for pruned_heads experiment", "data": [{"dataset_name": "sst2", "final_value": 0.1019, "best_value": 0.1019}, {"dataset_name": "yelp_polarity", "final_value": 0.1026, "best_value": 0.1026}, {"dataset_name": "imdb", "final_value": 0.1012, "best_value": 0.1012}]}, {"metric_name": "pruned_heads detection AUC (kl)", "lower_is_better": false, "description": "Detection AUC using KL method for pruned_heads experiment", "data": [{"dataset_name": "sst2", "final_value": 0.733, "best_value": 0.733}, {"dataset_name": "yelp_polarity", "final_value": 0.8892, "best_value": 0.8892}, {"dataset_name": "imdb", "final_value": 0.8555, "best_value": 0.8555}]}, {"metric_name": "pruned_heads detection DES (kl)", "lower_is_better": true, "description": "Detection DES using KL method for pruned_heads experiment", "data": [{"dataset_name": "sst2", "final_value": 0.1222, "best_value": 0.1222}, {"dataset_name": "yelp_polarity", "final_value": 0.1482, "best_value": 0.1482}, {"dataset_name": "imdb", "final_value": 0.1426, "best_value": 0.1426}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "validation AUC vote", "lower_is_better": false, "description": "Area under the ROC curve using vote method", "data": [{"dataset_name": "sst2", "final_value": 0.604, "best_value": 0.604}, {"dataset_name": "yelp_polarity", "final_value": 0.6531, "best_value": 0.6531}, {"dataset_name": "imdb", "final_value": 0.5622, "best_value": 0.5622}]}, {"metric_name": "validation DES vote", "lower_is_better": true, "description": "DES metric for vote predictions", "data": [{"dataset_name": "sst2", "final_value": 0.1007, "best_value": 0.1007}, {"dataset_name": "yelp_polarity", "final_value": 0.1089, "best_value": 0.1089}, {"dataset_name": "imdb", "final_value": 0.0937, "best_value": 0.0937}]}, {"metric_name": "validation AUC KL", "lower_is_better": false, "description": "Area under the ROC curve using KL divergence", "data": [{"dataset_name": "sst2", "final_value": 0.7203, "best_value": 0.7203}, {"dataset_name": "yelp_polarity", "final_value": 0.8726, "best_value": 0.8726}, {"dataset_name": "imdb", "final_value": 0.8288, "best_value": 0.8288}]}, {"metric_name": "validation DES KL", "lower_is_better": true, "description": "DES metric using KL divergence", "data": [{"dataset_name": "sst2", "final_value": 0.1201, "best_value": 0.1201}, {"dataset_name": "yelp_polarity", "final_value": 0.1454, "best_value": 0.1454}, {"dataset_name": "imdb", "final_value": 0.1381, "best_value": 0.1381}]}]}, {"metric_names": [{"metric_name": "final training loss", "lower_is_better": true, "description": "Final training loss.", "data": [{"dataset_name": "sst2 (full_ft)", "final_value": 0.1478, "best_value": 0.1478}, {"dataset_name": "sst2 (freeze_4)", "final_value": 0.2011, "best_value": 0.2011}, {"dataset_name": "sst2 (freeze_8)", "final_value": 0.2704, "best_value": 0.2704}, {"dataset_name": "yelp_polarity (full_ft)", "final_value": 0.1068, "best_value": 0.1068}, {"dataset_name": "yelp_polarity (freeze_4)", "final_value": 0.1095, "best_value": 0.1095}, {"dataset_name": "yelp_polarity (freeze_8)", "final_value": 0.1438, "best_value": 0.1438}, {"dataset_name": "imdb (full_ft)", "final_value": 0.1815, "best_value": 0.1815}, {"dataset_name": "imdb (freeze_4)", "final_value": 0.1848, "best_value": 0.1848}, {"dataset_name": "imdb (freeze_8)", "final_value": 0.2122, "best_value": 0.2122}]}, {"metric_name": "final validation loss", "lower_is_better": true, "description": "Final validation loss.", "data": [{"dataset_name": "sst2 (full_ft)", "final_value": 0.3059, "best_value": 0.3059}, {"dataset_name": "sst2 (freeze_4)", "final_value": 0.2836, "best_value": 0.2836}, {"dataset_name": "sst2 (freeze_8)", "final_value": 0.2618, "best_value": 0.2618}, {"dataset_name": "yelp_polarity (full_ft)", "final_value": 0.259, "best_value": 0.259}, {"dataset_name": "yelp_polarity (freeze_4)", "final_value": 0.2071, "best_value": 0.2071}, {"dataset_name": "yelp_polarity (freeze_8)", "final_value": 0.2179, "best_value": 0.2179}, {"dataset_name": "imdb (full_ft)", "final_value": 0.2574, "best_value": 0.2574}, {"dataset_name": "imdb (freeze_4)", "final_value": 0.2676, "best_value": 0.2676}, {"dataset_name": "imdb (freeze_8)", "final_value": 0.2933, "best_value": 0.2933}]}, {"metric_name": "detection AUC (vote)", "lower_is_better": false, "description": "Detection Area Under the Curve using majority vote.", "data": [{"dataset_name": "sst2 (full_ft)", "final_value": 0.5695, "best_value": 0.5695}, {"dataset_name": "sst2 (freeze_4)", "final_value": 0.5983, "best_value": 0.5983}, {"dataset_name": "sst2 (freeze_8)", "final_value": 0.736, "best_value": 0.736}, {"dataset_name": "yelp_polarity (full_ft)", "final_value": 0.5561, "best_value": 0.5561}, {"dataset_name": "yelp_polarity (freeze_4)", "final_value": 0.5404, "best_value": 0.5404}, {"dataset_name": "yelp_polarity (freeze_8)", "final_value": 0.551, "best_value": 0.551}, {"dataset_name": "imdb (full_ft)", "final_value": 0.5658, "best_value": 0.5658}, {"dataset_name": "imdb (freeze_4)", "final_value": 0.5421, "best_value": 0.5421}, {"dataset_name": "imdb (freeze_8)", "final_value": 0.5474, "best_value": 0.5474}]}, {"metric_name": "detection DES (vote)", "lower_is_better": true, "description": "Detection DES using majority vote.", "data": [{"dataset_name": "sst2 (full_ft)", "final_value": 0.1424, "best_value": 0.1424}, {"dataset_name": "sst2 (freeze_4)", "final_value": 0.1496, "best_value": 0.1496}, {"dataset_name": "sst2 (freeze_8)", "final_value": 0.184, "best_value": 0.184}, {"dataset_name": "yelp_polarity (full_ft)", "final_value": 0.139, "best_value": 0.139}, {"dataset_name": "yelp_polarity (freeze_4)", "final_value": 0.1351, "best_value": 0.1351}, {"dataset_name": "yelp_polarity (freeze_8)", "final_value": 0.1377, "best_value": 0.1377}, {"dataset_name": "imdb (full_ft)", "final_value": 0.1415, "best_value": 0.1415}, {"dataset_name": "imdb (freeze_4)", "final_value": 0.1355, "best_value": 0.1355}, {"dataset_name": "imdb (freeze_8)", "final_value": 0.1368, "best_value": 0.1368}]}, {"metric_name": "detection Spearman (vote)", "lower_is_better": false, "description": "Detection Spearman correlation using majority vote.", "data": [{"dataset_name": "sst2 (full_ft)", "final_value": 0.1417, "best_value": 0.1417}, {"dataset_name": "sst2 (freeze_4)", "final_value": 0.1657, "best_value": 0.1657}, {"dataset_name": "sst2 (freeze_8)", "final_value": 0.4007, "best_value": 0.4007}, {"dataset_name": "yelp_polarity (full_ft)", "final_value": 0.1544, "best_value": 0.1544}, {"dataset_name": "yelp_polarity (freeze_4)", "final_value": 0.1006, "best_value": 0.1006}, {"dataset_name": "yelp_polarity (freeze_8)", "final_value": 0.162, "best_value": 0.162}, {"dataset_name": "imdb (full_ft)", "final_value": 0.2586, "best_value": 0.2586}, {"dataset_name": "imdb (freeze_4)", "final_value": 0.1617, "best_value": 0.1617}, {"dataset_name": "imdb (freeze_8)", "final_value": 0.1985, "best_value": 0.1985}]}, {"metric_name": "detection AUC (KL)", "lower_is_better": false, "description": "Detection Area Under the Curve using KL divergence.", "data": [{"dataset_name": "sst2 (full_ft)", "final_value": 0.7494, "best_value": 0.7494}, {"dataset_name": "sst2 (freeze_4)", "final_value": 0.7318, "best_value": 0.7318}, {"dataset_name": "sst2 (freeze_8)", "final_value": 0.7508, "best_value": 0.7508}, {"dataset_name": "yelp_polarity (full_ft)", "final_value": 0.8039, "best_value": 0.8039}, {"dataset_name": "yelp_polarity (freeze_4)", "final_value": 0.8271, "best_value": 0.8271}, {"dataset_name": "yelp_polarity (freeze_8)", "final_value": 0.7867, "best_value": 0.7867}, {"dataset_name": "imdb (full_ft)", "final_value": 0.7978, "best_value": 0.7978}, {"dataset_name": "imdb (freeze_4)", "final_value": 0.7828, "best_value": 0.7828}, {"dataset_name": "imdb (freeze_8)", "final_value": 0.7584, "best_value": 0.7584}]}, {"metric_name": "detection DES (KL)", "lower_is_better": true, "description": "Detection DES using KL divergence.", "data": [{"dataset_name": "sst2 (full_ft)", "final_value": 0.1873, "best_value": 0.1873}, {"dataset_name": "sst2 (freeze_4)", "final_value": 0.1829, "best_value": 0.1829}, {"dataset_name": "sst2 (freeze_8)", "final_value": 0.1877, "best_value": 0.1877}, {"dataset_name": "yelp_polarity (full_ft)", "final_value": 0.201, "best_value": 0.201}, {"dataset_name": "yelp_polarity (freeze_4)", "final_value": 0.2068, "best_value": 0.2068}, {"dataset_name": "yelp_polarity (freeze_8)", "final_value": 0.1967, "best_value": 0.1967}, {"dataset_name": "imdb (full_ft)", "final_value": 0.1995, "best_value": 0.1995}, {"dataset_name": "imdb (freeze_4)", "final_value": 0.1957, "best_value": 0.1957}, {"dataset_name": "imdb (freeze_8)", "final_value": 0.1896, "best_value": 0.1896}]}, {"metric_name": "detection Spearman (KL)", "lower_is_better": false, "description": "Detection Spearman correlation using KL divergence.", "data": [{"dataset_name": "sst2 (full_ft)", "final_value": 0.2703, "best_value": 0.2703}, {"dataset_name": "sst2 (freeze_4)", "final_value": 0.2354, "best_value": 0.2354}, {"dataset_name": "sst2 (freeze_8)", "final_value": 0.2486, "best_value": 0.2486}, {"dataset_name": "yelp_polarity (full_ft)", "final_value": 0.3158, "best_value": 0.3158}, {"dataset_name": "yelp_polarity (freeze_4)", "final_value": 0.3074, "best_value": 0.3074}, {"dataset_name": "yelp_polarity (freeze_8)", "final_value": 0.2694, "best_value": 0.2694}, {"dataset_name": "imdb (full_ft)", "final_value": 0.3163, "best_value": 0.3163}, {"dataset_name": "imdb (freeze_4)", "final_value": 0.2939, "best_value": 0.2939}, {"dataset_name": "imdb (freeze_8)", "final_value": 0.2624, "best_value": 0.2624}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test accuracy.", "data": [{"dataset_name": "sst2 (full_ft)", "final_value": 0.89, "best_value": 0.89}, {"dataset_name": "sst2 (freeze_4)", "final_value": 0.905, "best_value": 0.905}, {"dataset_name": "sst2 (freeze_8)", "final_value": 0.91, "best_value": 0.91}, {"dataset_name": "yelp_polarity (full_ft)", "final_value": 0.9, "best_value": 0.9}, {"dataset_name": "yelp_polarity (freeze_4)", "final_value": 0.92, "best_value": 0.92}, {"dataset_name": "yelp_polarity (freeze_8)", "final_value": 0.92, "best_value": 0.92}, {"dataset_name": "imdb (full_ft)", "final_value": 0.895, "best_value": 0.895}, {"dataset_name": "imdb (freeze_4)", "final_value": 0.9, "best_value": 0.9}, {"dataset_name": "imdb (freeze_8)", "final_value": 0.905, "best_value": 0.905}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "sst2", "final_value": null, "best_value": null}, {"dataset_name": "yelp_polarity", "final_value": null, "best_value": null}, {"dataset_name": "imdb", "final_value": null, "best_value": null}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "sst2", "final_value": null, "best_value": null}, {"dataset_name": "yelp_polarity", "final_value": null, "best_value": null}, {"dataset_name": "imdb", "final_value": null, "best_value": null}]}, {"metric_name": "Detection AUC (vote)", "lower_is_better": false, "description": "Detection AUC (vote)", "data": [{"dataset_name": "sst2", "final_value": 0.5, "best_value": 0.5}, {"dataset_name": "yelp_polarity", "final_value": 0.5, "best_value": 0.5}, {"dataset_name": "imdb", "final_value": 0.5, "best_value": 0.5}]}, {"metric_name": "Detection DES (vote)", "lower_is_better": true, "description": "Detection DES (vote)", "data": [{"dataset_name": "sst2", "final_value": 0.0833, "best_value": 0.0833}, {"dataset_name": "yelp_polarity", "final_value": 0.0833, "best_value": 0.0833}, {"dataset_name": "imdb", "final_value": 0.0833, "best_value": 0.0833}]}, {"metric_name": "Detection AUC (KL divergence)", "lower_is_better": false, "description": "Detection AUC (KL divergence)", "data": [{"dataset_name": "sst2", "final_value": 0.5, "best_value": 0.5}, {"dataset_name": "yelp_polarity", "final_value": 0.5, "best_value": 0.5}, {"dataset_name": "imdb", "final_value": 0.5, "best_value": 0.5}]}, {"metric_name": "Detection DES (KL divergence)", "lower_is_better": true, "description": "Detection DES (KL divergence)", "data": [{"dataset_name": "sst2", "final_value": 0.0833, "best_value": 0.0833}, {"dataset_name": "yelp_polarity", "final_value": 0.0833, "best_value": 0.0833}, {"dataset_name": "imdb", "final_value": 0.0833, "best_value": 0.0833}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test accuracy", "data": [{"dataset_name": "sst2", "final_value": 0.478, "best_value": 0.478}, {"dataset_name": "yelp_polarity", "final_value": 0.508, "best_value": 0.508}, {"dataset_name": "imdb", "final_value": 0.508, "best_value": 0.508}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.0289, "best_value": 0.0289}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.014, "best_value": 0.014}, {"dataset_name": "imdb (baseline)", "final_value": 0.0433, "best_value": 0.0433}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.3545, "best_value": 0.3545}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.2336, "best_value": 0.2336}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.681, "best_value": 0.681}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.4289, "best_value": 0.4289}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.2778, "best_value": 0.2778}, {"dataset_name": "imdb (baseline)", "final_value": 0.3045, "best_value": 0.3045}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.5888, "best_value": 0.5888}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.2977, "best_value": 0.2977}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.7161, "best_value": 0.7161}]}, {"metric_name": "detection AUC (vote)", "lower_is_better": false, "description": "Final detection AUC using vote method", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.7028, "best_value": 0.7028}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.5924, "best_value": 0.5924}, {"dataset_name": "imdb (baseline)", "final_value": 0.5539, "best_value": 0.5539}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.5907, "best_value": 0.5907}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.5981, "best_value": 0.5981}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.4982, "best_value": 0.4982}]}, {"metric_name": "detection DES (vote)", "lower_is_better": true, "description": "Final detection DES using vote method", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.1171, "best_value": 0.1171}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.0987, "best_value": 0.0987}, {"dataset_name": "imdb (baseline)", "final_value": 0.0923, "best_value": 0.0923}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.0984, "best_value": 0.0984}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.0997, "best_value": 0.0997}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.083, "best_value": 0.083}]}, {"metric_name": "detection AUC (KL)", "lower_is_better": false, "description": "Final detection AUC using KL method", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.7853, "best_value": 0.7853}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.845, "best_value": 0.845}, {"dataset_name": "imdb (baseline)", "final_value": 0.8859, "best_value": 0.8859}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.5658, "best_value": 0.5658}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.8172, "best_value": 0.8172}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.6443, "best_value": 0.6443}]}, {"metric_name": "detection DES (KL)", "lower_is_better": true, "description": "Final detection DES using KL method", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.1309, "best_value": 0.1309}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.1408, "best_value": 0.1408}, {"dataset_name": "imdb (baseline)", "final_value": 0.1477, "best_value": 0.1477}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.0943, "best_value": 0.0943}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.1362, "best_value": 0.1362}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.1074, "best_value": 0.1074}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "sst2", "final_value": 0.6921, "best_value": 0.6921}, {"dataset_name": "yelp_polarity", "final_value": 0.6987, "best_value": 0.6987}, {"dataset_name": "imdb", "final_value": 0.6967, "best_value": 0.6967}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "sst2", "final_value": 0.6902, "best_value": 0.6902}, {"dataset_name": "yelp_polarity", "final_value": 0.6919, "best_value": 0.6919}, {"dataset_name": "imdb", "final_value": 0.6621, "best_value": 0.6621}]}, {"metric_name": "AUC vote", "lower_is_better": false, "description": "Area under the ROC curve for vote aggregator", "data": [{"dataset_name": "sst2", "final_value": 0.4955, "best_value": 0.4955}, {"dataset_name": "yelp_polarity", "final_value": 0.496, "best_value": 0.496}, {"dataset_name": "imdb", "final_value": 0.5096, "best_value": 0.5096}]}, {"metric_name": "DES vote", "lower_is_better": true, "description": "DES metric for vote aggregator", "data": [{"dataset_name": "sst2", "final_value": 0.0826, "best_value": 0.0826}, {"dataset_name": "yelp_polarity", "final_value": 0.0827, "best_value": 0.0827}, {"dataset_name": "imdb", "final_value": 0.0849, "best_value": 0.0849}]}, {"metric_name": "AUC KL", "lower_is_better": false, "description": "Area under the ROC curve for KL aggregator", "data": [{"dataset_name": "sst2", "final_value": 0.5151, "best_value": 0.5151}, {"dataset_name": "yelp_polarity", "final_value": 0.4432, "best_value": 0.4432}, {"dataset_name": "imdb", "final_value": 0.5993, "best_value": 0.5993}]}, {"metric_name": "DES KL", "lower_is_better": true, "description": "DES metric for KL aggregator", "data": [{"dataset_name": "sst2", "final_value": 0.0859, "best_value": 0.0859}, {"dataset_name": "yelp_polarity", "final_value": 0.0739, "best_value": 0.0739}, {"dataset_name": "imdb", "final_value": 0.0999, "best_value": 0.0999}]}, {"metric_name": "Spearman vote", "lower_is_better": false, "description": "Spearman correlation for vote aggregator", "data": [{"dataset_name": "sst2", "final_value": -0.0282, "best_value": -0.0282}, {"dataset_name": "yelp_polarity", "final_value": -0.0316, "best_value": -0.0316}, {"dataset_name": "imdb", "final_value": 0.0342, "best_value": 0.0342}]}, {"metric_name": "Spearman KL", "lower_is_better": false, "description": "Spearman correlation for KL aggregator", "data": [{"dataset_name": "sst2", "final_value": 0.0261, "best_value": 0.0261}, {"dataset_name": "yelp_polarity", "final_value": -0.0985, "best_value": -0.0985}, {"dataset_name": "imdb", "final_value": 0.161, "best_value": 0.161}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the train set", "data": [{"dataset_name": "sst2", "final_value": 0.0358, "best_value": 0.0358}, {"dataset_name": "yelp_polarity", "final_value": 0.0156, "best_value": 0.0156}, {"dataset_name": "imdb", "final_value": 0.0368, "best_value": 0.0368}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "sst2", "final_value": 0.4356, "best_value": 0.4356}, {"dataset_name": "yelp_polarity", "final_value": 0.2583, "best_value": 0.2583}, {"dataset_name": "imdb", "final_value": 0.305, "best_value": 0.305}]}, {"metric_name": "ROC AUC (vote uncertainty)", "lower_is_better": false, "description": "ROC AUC using vote uncertainty", "data": [{"dataset_name": "sst2", "final_value": 0.675, "best_value": 0.675}, {"dataset_name": "yelp_polarity", "final_value": 0.5815, "best_value": 0.5815}, {"dataset_name": "imdb", "final_value": 0.5465, "best_value": 0.5465}]}, {"metric_name": "Detection Efficiency Score (vote uncertainty)", "lower_is_better": false, "description": "Detection Efficiency Score with vote uncertainty", "data": [{"dataset_name": "sst2", "final_value": 0.1125, "best_value": 0.1125}, {"dataset_name": "yelp_polarity", "final_value": 0.0969, "best_value": 0.0969}, {"dataset_name": "imdb", "final_value": 0.0911, "best_value": 0.0911}]}, {"metric_name": "ROC AUC (KL uncertainty)", "lower_is_better": false, "description": "ROC AUC using KL uncertainty", "data": [{"dataset_name": "sst2", "final_value": 0.7675, "best_value": 0.7675}, {"dataset_name": "yelp_polarity", "final_value": 0.8475, "best_value": 0.8475}, {"dataset_name": "imdb", "final_value": 0.8187, "best_value": 0.8187}]}, {"metric_name": "Detection Efficiency Score (KL uncertainty)", "lower_is_better": false, "description": "Detection Efficiency Score with KL uncertainty", "data": [{"dataset_name": "sst2", "final_value": 0.1279, "best_value": 0.1279}, {"dataset_name": "yelp_polarity", "final_value": 0.1413, "best_value": 0.1413}, {"dataset_name": "imdb", "final_value": 0.1365, "best_value": 0.1365}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "sst2 (full_depth)", "final_value": 0.0308, "best_value": 0.0308}, {"dataset_name": "yelp_polarity (full_depth)", "final_value": 0.0215, "best_value": 0.0215}, {"dataset_name": "imdb (full_depth)", "final_value": 0.0467, "best_value": 0.0467}, {"dataset_name": "sst2 (reduced_depth)", "final_value": 0.0335, "best_value": 0.0335}, {"dataset_name": "yelp_polarity (reduced_depth)", "final_value": 0.0154, "best_value": 0.0154}, {"dataset_name": "imdb (reduced_depth)", "final_value": 0.0406, "best_value": 0.0406}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "sst2 (full_depth)", "final_value": 0.4367, "best_value": 0.4367}, {"dataset_name": "yelp_polarity (full_depth)", "final_value": 0.1385, "best_value": 0.1385}, {"dataset_name": "imdb (full_depth)", "final_value": 0.3104, "best_value": 0.3104}, {"dataset_name": "sst2 (reduced_depth)", "final_value": 0.6043, "best_value": 0.6043}, {"dataset_name": "yelp_polarity (reduced_depth)", "final_value": 0.3684, "best_value": 0.3684}, {"dataset_name": "imdb (reduced_depth)", "final_value": 0.3892, "best_value": 0.3892}]}, {"metric_name": "detection AUC (vote)", "lower_is_better": false, "description": "Detection AUC using vote", "data": [{"dataset_name": "sst2 (full_depth)", "final_value": 0.6565, "best_value": 0.6565}, {"dataset_name": "yelp_polarity (full_depth)", "final_value": 0.5283, "best_value": 0.5283}, {"dataset_name": "imdb (full_depth)", "final_value": 0.6011, "best_value": 0.6011}, {"dataset_name": "sst2 (reduced_depth)", "final_value": 0.5924, "best_value": 0.5924}, {"dataset_name": "yelp_polarity (reduced_depth)", "final_value": 0.5817, "best_value": 0.5817}, {"dataset_name": "imdb (reduced_depth)", "final_value": 0.5288, "best_value": 0.5288}]}, {"metric_name": "detection DES (vote)", "lower_is_better": true, "description": "Detection DES using vote", "data": [{"dataset_name": "sst2 (full_depth)", "final_value": 0.1094, "best_value": 0.1094}, {"dataset_name": "yelp_polarity (full_depth)", "final_value": 0.0881, "best_value": 0.0881}, {"dataset_name": "imdb (full_depth)", "final_value": 0.1002, "best_value": 0.1002}, {"dataset_name": "sst2 (reduced_depth)", "final_value": 0.0987, "best_value": 0.0987}, {"dataset_name": "yelp_polarity (reduced_depth)", "final_value": 0.097, "best_value": 0.097}, {"dataset_name": "imdb (reduced_depth)", "final_value": 0.0881, "best_value": 0.0881}]}, {"metric_name": "detection AUC (kl)", "lower_is_better": false, "description": "Detection AUC using KL divergence", "data": [{"dataset_name": "sst2 (full_depth)", "final_value": 0.7603, "best_value": 0.7603}, {"dataset_name": "yelp_polarity (full_depth)", "final_value": 0.863, "best_value": 0.863}, {"dataset_name": "imdb (full_depth)", "final_value": 0.8298, "best_value": 0.8298}, {"dataset_name": "sst2 (reduced_depth)", "final_value": 0.6697, "best_value": 0.6697}, {"dataset_name": "yelp_polarity (reduced_depth)", "final_value": 0.8311, "best_value": 0.8311}, {"dataset_name": "imdb (reduced_depth)", "final_value": 0.7998, "best_value": 0.7998}]}, {"metric_name": "detection DES (kl)", "lower_is_better": true, "description": "Detection DES using KL divergence", "data": [{"dataset_name": "sst2 (full_depth)", "final_value": 0.1267, "best_value": 0.1267}, {"dataset_name": "yelp_polarity (full_depth)", "final_value": 0.1438, "best_value": 0.1438}, {"dataset_name": "imdb (full_depth)", "final_value": 0.1383, "best_value": 0.1383}, {"dataset_name": "sst2 (reduced_depth)", "final_value": 0.1116, "best_value": 0.1116}, {"dataset_name": "yelp_polarity (reduced_depth)", "final_value": 0.1385, "best_value": 0.1385}, {"dataset_name": "imdb (reduced_depth)", "final_value": 0.1333, "best_value": 0.1333}]}]}, {"metric_names": [{"metric_name": "final training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.0412, "best_value": 0.0412}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.0229, "best_value": 0.0229}, {"dataset_name": "imdb (baseline)", "final_value": 0.034, "best_value": 0.034}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.4704, "best_value": 0.4704}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.3136, "best_value": 0.3136}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.6659, "best_value": 0.6659}]}, {"metric_name": "final validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.4025, "best_value": 0.4025}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.217, "best_value": 0.217}, {"dataset_name": "imdb (baseline)", "final_value": 0.3762, "best_value": 0.3762}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.5445, "best_value": 0.5445}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.3373, "best_value": 0.3373}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.7406, "best_value": 0.7406}]}, {"metric_name": "final detection AUC (vote)", "lower_is_better": false, "description": "Area under the curve for detection using vote", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.6366, "best_value": 0.6366}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.674, "best_value": 0.674}, {"dataset_name": "imdb (baseline)", "final_value": 0.611, "best_value": 0.611}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.553, "best_value": 0.553}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.5877, "best_value": 0.5877}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.4981, "best_value": 0.4981}]}, {"metric_name": "final detection DES (vote)", "lower_is_better": true, "description": "Detection error score using vote", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.1061, "best_value": 0.1061}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.1123, "best_value": 0.1123}, {"dataset_name": "imdb (baseline)", "final_value": 0.1018, "best_value": 0.1018}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.0922, "best_value": 0.0922}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.098, "best_value": 0.098}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.083, "best_value": 0.083}]}, {"metric_name": "final detection AUC (KL)", "lower_is_better": false, "description": "Area under the curve for detection using KL", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.7814, "best_value": 0.7814}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.9113, "best_value": 0.9113}, {"dataset_name": "imdb (baseline)", "final_value": 0.8697, "best_value": 0.8697}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.4713, "best_value": 0.4713}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.7982, "best_value": 0.7982}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.5542, "best_value": 0.5542}]}, {"metric_name": "final detection DES (KL)", "lower_is_better": true, "description": "Detection error score using KL", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.1302, "best_value": 0.1302}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.1519, "best_value": 0.1519}, {"dataset_name": "imdb (baseline)", "final_value": 0.145, "best_value": 0.145}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.0785, "best_value": 0.0785}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.133, "best_value": 0.133}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.0924, "best_value": 0.0924}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.0296, "best_value": 0.0296}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.0115, "best_value": 0.0115}, {"dataset_name": "imdb (baseline)", "final_value": 0.0258, "best_value": 0.0258}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.2927, "best_value": 0.2927}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.2164, "best_value": 0.2164}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.6939, "best_value": 0.6939}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.4039, "best_value": 0.4039}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.2119, "best_value": 0.2119}, {"dataset_name": "imdb (baseline)", "final_value": 0.2904, "best_value": 0.2904}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.5848, "best_value": 0.5848}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.3757, "best_value": 0.3757}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.6934, "best_value": 0.6934}]}, {"metric_name": "detection AUC (vote)", "lower_is_better": false, "description": "Final detection AUC using vote aggregation", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.6226, "best_value": 0.6226}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.5377, "best_value": 0.5377}, {"dataset_name": "imdb (baseline)", "final_value": 0.5898, "best_value": 0.5898}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.5552, "best_value": 0.5552}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.6034, "best_value": 0.6034}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.5, "best_value": 0.5}]}, {"metric_name": "detection DES (vote)", "lower_is_better": true, "description": "Final detection DES using vote aggregation", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.1038, "best_value": 0.1038}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.0896, "best_value": 0.0896}, {"dataset_name": "imdb (baseline)", "final_value": 0.0983, "best_value": 0.0983}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.0925, "best_value": 0.0925}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.1006, "best_value": 0.1006}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.0833, "best_value": 0.0833}]}, {"metric_name": "detection AUC (KL)", "lower_is_better": false, "description": "Final detection AUC using KL divergence", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.7438, "best_value": 0.7438}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.8864, "best_value": 0.8864}, {"dataset_name": "imdb (baseline)", "final_value": 0.8667, "best_value": 0.8667}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.5921, "best_value": 0.5921}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.7771, "best_value": 0.7771}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.5595, "best_value": 0.5595}]}, {"metric_name": "detection DES (KL)", "lower_is_better": true, "description": "Final detection DES using KL divergence", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.124, "best_value": 0.124}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.1477, "best_value": 0.1477}, {"dataset_name": "imdb (baseline)", "final_value": 0.1445, "best_value": 0.1445}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.0987, "best_value": 0.0987}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.1295, "best_value": 0.1295}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.0932, "best_value": 0.0932}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.04, "best_value": 0.04}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.0135, "best_value": 0.0135}, {"dataset_name": "imdb (baseline)", "final_value": 0.0418, "best_value": 0.0418}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.2963, "best_value": 0.2963}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.2285, "best_value": 0.2285}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.6939, "best_value": 0.6939}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.4144, "best_value": 0.4144}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.2539, "best_value": 0.2539}, {"dataset_name": "imdb (baseline)", "final_value": 0.2781, "best_value": 0.2781}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.6895, "best_value": 0.6895}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.2719, "best_value": 0.2719}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.6929, "best_value": 0.6929}]}, {"metric_name": "detection AUC (vote)", "lower_is_better": false, "description": "Final detection AUC (vote)", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.6204, "best_value": 0.6204}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.5424, "best_value": 0.5424}, {"dataset_name": "imdb (baseline)", "final_value": 0.5365, "best_value": 0.5365}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.5474, "best_value": 0.5474}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.6232, "best_value": 0.6232}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.5, "best_value": 0.5}]}, {"metric_name": "detection DES (vote)", "lower_is_better": true, "description": "Final detection DES (vote)", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.1034, "best_value": 0.1034}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.0904, "best_value": 0.0904}, {"dataset_name": "imdb (baseline)", "final_value": 0.0894, "best_value": 0.0894}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.0912, "best_value": 0.0912}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.1039, "best_value": 0.1039}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.0833, "best_value": 0.0833}]}, {"metric_name": "detection AUC (KL)", "lower_is_better": false, "description": "Final detection AUC (KL)", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.7501, "best_value": 0.7501}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.8371, "best_value": 0.8371}, {"dataset_name": "imdb (baseline)", "final_value": 0.8592, "best_value": 0.8592}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.5631, "best_value": 0.5631}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.848, "best_value": 0.848}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.5142, "best_value": 0.5142}]}, {"metric_name": "detection DES (KL)", "lower_is_better": true, "description": "Final detection DES (KL)", "data": [{"dataset_name": "sst2 (baseline)", "final_value": 0.125, "best_value": 0.125}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.1395, "best_value": 0.1395}, {"dataset_name": "imdb (baseline)", "final_value": 0.1432, "best_value": 0.1432}, {"dataset_name": "sst2 (RandomInitEmbeddingAblation)", "final_value": 0.0939, "best_value": 0.0939}, {"dataset_name": "yelp_polarity (RandomInitEmbeddingAblation)", "final_value": 0.1413, "best_value": 0.1413}, {"dataset_name": "imdb (RandomInitEmbeddingAblation)", "final_value": 0.0857, "best_value": 0.0857}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/comparison_detection_auc.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_loss_curve.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/imdb_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/imdb_loss_curve.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_loss_curve.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_class_distribution.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_detection_auc_curve.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/comparison_DES.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_class_distribution.png", "../../logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/imdb_class_distribution.png"], ["../../logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/yelp_polarity_head_only_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/sst2_head_only_detection_auc.png", "../../logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/sst2_head_only_detection_des.png", "../../logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/imdb_head_only_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/yelp_polarity_head_only_detection_auc.png", "../../logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/imdb_head_only_detection_des.png", "../../logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/imdb_head_only_detection_auc.png", "../../logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/sst2_head_only_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/yelp_polarity_head_only_detection_des.png"], [], ["../../logs/0-run/experiment_results/experiment_1746817a2766428e8d23cb0f558d155c_proc_164442/all_datasets_loss_curves.png", "../../logs/0-run/experiment_results/experiment_1746817a2766428e8d23cb0f558d155c_proc_164442/all_datasets_detection_auc.png"], ["../../logs/0-run/experiment_results/experiment_74cd1572cf364cd9af47e780a766505d_proc_164440/accuracy_curves_positional_embedding_ablation.png", "../../logs/0-run/experiment_results/experiment_74cd1572cf364cd9af47e780a766505d_proc_164440/loss_curves_positional_embedding_ablation.png", "../../logs/0-run/experiment_results/experiment_74cd1572cf364cd9af47e780a766505d_proc_164440/detection_auc_curves_positional_embedding_ablation.png"], ["../../logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/imdb_detection_auc.png", "../../logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/yelp_polarity_detection_auc.png", "../../logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/sst2_detection_auc.png", "../../logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/yelp_polarity_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/imdb_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/sst2_loss_curves.png"], [], [], ["../../logs/0-run/experiment_results/experiment_c79f3e63cb3446b8a602cb2f565384aa_proc_164440/sst2_summary.png", "../../logs/0-run/experiment_results/experiment_c79f3e63cb3446b8a602cb2f565384aa_proc_164440/imdb_summary.png", "../../logs/0-run/experiment_results/experiment_c79f3e63cb3446b8a602cb2f565384aa_proc_164440/yelp_polarity_summary.png"], [], [], ["../../logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/imdb_auc_curves.png", "../../logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/sst2_auc_curves.png", "../../logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/yelp_polarity_auc_curves.png", "../../logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/yelp_polarity_loss_curves.png", "../../logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/imdb_loss_curves.png", "../../logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/sst2_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_22222418bfa748248fa8a1f7a2c84f16_proc_164440/yelp_polarity_combined.png", "../../logs/0-run/experiment_results/experiment_22222418bfa748248fa8a1f7a2c84f16_proc_164440/imdb_combined.png", "../../logs/0-run/experiment_results/experiment_22222418bfa748248fa8a1f7a2c84f16_proc_164440/sst2_combined.png"], [], ["../../logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/yelp_polarity_RandomInitEmbeddingAblation_loss_curves.png", "../../logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/sst2_RandomInitEmbeddingAblation_detection_auc.png", "../../logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/yelp_polarity_baseline_loss_curves.png", "../../logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/sst2_baseline_loss_curves.png", "../../logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/sst2_baseline_detection_auc.png", "../../logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/imdb_RandomInitEmbeddingAblation_loss_curves.png", "../../logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/imdb_baseline_loss_curves.png", "../../logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/sst2_RandomInitEmbeddingAblation_loss_curves.png", "../../logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/imdb_RandomInitEmbeddingAblation_detection_auc.png", "../../logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/imdb_baseline_detection_auc.png", "../../logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/yelp_polarity_baseline_detection_auc.png", "../../logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/yelp_polarity_RandomInitEmbeddingAblation_detection_auc.png"], ["../../logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/yelp_polarity_auc_curve.png", "../../logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/yelp_polarity_loss_curve.png", "../../logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/imdb_loss_curve.png", "../../logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/imdb_auc_curve.png", "../../logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/sst2_auc_curve.png", "../../logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/sst2_loss_curve.png"], ["../../logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/yelp_polarity_label_distribution.png", "../../logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/yelp_polarity_auc_curve.png", "../../logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/yelp_polarity_loss_curve.png", "../../logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/imdb_loss_curve.png", "../../logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/imdb_auc_curve.png", "../../logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/sst2_auc_curve.png", "../../logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/sst2_label_distribution.png", "../../logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/sst2_loss_curve.png", "../../logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/imdb_label_distribution.png"], ["../../logs/0-run/experiment_results/experiment_1fbca80115dc4f94a62937611cfbdba3_proc_164442/sst2_loss_detection.png", "../../logs/0-run/experiment_results/experiment_1fbca80115dc4f94a62937611cfbdba3_proc_164442/yelp_polarity_loss_detection.png", "../../logs/0-run/experiment_results/experiment_1fbca80115dc4f94a62937611cfbdba3_proc_164442/imdb_loss_detection.png"], ["../../logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/yelp_polarity_RandomInitEmbeddingAblation_loss_curves.png", "../../logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/sst2_RandomInitEmbeddingAblation_detection_auc.png", "../../logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/yelp_polarity_baseline_loss_curves.png", "../../logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/sst2_baseline_loss_curves.png", "../../logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/sst2_baseline_detection_auc.png", "../../logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/imdb_RandomInitEmbeddingAblation_loss_curves.png", "../../logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/imdb_baseline_loss_curves.png", "../../logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/sst2_RandomInitEmbeddingAblation_loss_curves.png", "../../logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/imdb_RandomInitEmbeddingAblation_detection_auc.png", "../../logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/imdb_baseline_detection_auc.png", "../../logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/yelp_polarity_baseline_detection_auc.png", "../../logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/yelp_polarity_RandomInitEmbeddingAblation_detection_auc.png"], ["../../logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/yelp_polarity_RandomInitEmbeddingAblation_loss_curves.png", "../../logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/sst2_RandomInitEmbeddingAblation_detection_auc.png", "../../logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/yelp_polarity_baseline_loss_curves.png", "../../logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/sst2_baseline_loss_curves.png", "../../logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/sst2_baseline_detection_auc.png", "../../logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/imdb_RandomInitEmbeddingAblation_loss_curves.png", "../../logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/imdb_baseline_loss_curves.png", "../../logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/sst2_RandomInitEmbeddingAblation_loss_curves.png", "../../logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/imdb_RandomInitEmbeddingAblation_detection_auc.png", "../../logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/imdb_baseline_detection_auc.png", "../../logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/yelp_polarity_baseline_detection_auc.png", "../../logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/yelp_polarity_RandomInitEmbeddingAblation_detection_auc.png"], ["../../logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/yelp_polarity_RandomInitEmbeddingAblation_loss_curves.png", "../../logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/sst2_RandomInitEmbeddingAblation_detection_auc.png", "../../logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/yelp_polarity_baseline_loss_curves.png", "../../logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/sst2_baseline_loss_curves.png", "../../logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/sst2_baseline_detection_auc.png", "../../logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/imdb_RandomInitEmbeddingAblation_loss_curves.png", "../../logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/imdb_baseline_loss_curves.png", "../../logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/sst2_RandomInitEmbeddingAblation_loss_curves.png", "../../logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/imdb_RandomInitEmbeddingAblation_detection_auc.png", "../../logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/imdb_baseline_detection_auc.png", "../../logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/yelp_polarity_baseline_detection_auc.png", "../../logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/yelp_polarity_RandomInitEmbeddingAblation_detection_auc.png"], ["../../logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/sst2_baseline_loss_curves_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/yelp_polarity_baseline_loss_curves_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/imdb_RandomInitEmbeddingAblation_detection_auc_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/yelp_polarity_RandomInitEmbeddingAblation_loss_curves_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/sst2_baseline_detection_auc_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/yelp_polarity_baseline_detection_auc_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/sst2_RandomInitEmbeddingAblation_loss_curves_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/imdb_baseline_detection_auc_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/sst2_RandomInitEmbeddingAblation_detection_auc_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/imdb_baseline_loss_curves_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/yelp_polarity_RandomInitEmbeddingAblation_detection_auc_agg.png", "../../logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/imdb_RandomInitEmbeddingAblation_loss_curves_agg.png"]], "plot_paths": [["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/comparison_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/imdb_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/imdb_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_class_distribution.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_detection_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/comparison_DES.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_class_distribution.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/imdb_class_distribution.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/yelp_polarity_head_only_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/sst2_head_only_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/sst2_head_only_detection_des.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/imdb_head_only_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/yelp_polarity_head_only_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/imdb_head_only_detection_des.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/imdb_head_only_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/sst2_head_only_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/yelp_polarity_head_only_detection_des.png"], [], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_1746817a2766428e8d23cb0f558d155c_proc_164442/all_datasets_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_1746817a2766428e8d23cb0f558d155c_proc_164442/all_datasets_detection_auc.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_74cd1572cf364cd9af47e780a766505d_proc_164440/accuracy_curves_positional_embedding_ablation.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_74cd1572cf364cd9af47e780a766505d_proc_164440/loss_curves_positional_embedding_ablation.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_74cd1572cf364cd9af47e780a766505d_proc_164440/detection_auc_curves_positional_embedding_ablation.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/imdb_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/yelp_polarity_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/sst2_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/yelp_polarity_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/imdb_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/sst2_loss_curves.png"], [], [], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_c79f3e63cb3446b8a602cb2f565384aa_proc_164440/sst2_summary.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_c79f3e63cb3446b8a602cb2f565384aa_proc_164440/imdb_summary.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_c79f3e63cb3446b8a602cb2f565384aa_proc_164440/yelp_polarity_summary.png"], [], [], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/imdb_auc_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/sst2_auc_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/yelp_polarity_auc_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/yelp_polarity_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/imdb_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/sst2_loss_curves.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_22222418bfa748248fa8a1f7a2c84f16_proc_164440/yelp_polarity_combined.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_22222418bfa748248fa8a1f7a2c84f16_proc_164440/imdb_combined.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_22222418bfa748248fa8a1f7a2c84f16_proc_164440/sst2_combined.png"], [], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/yelp_polarity_RandomInitEmbeddingAblation_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/sst2_RandomInitEmbeddingAblation_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/yelp_polarity_baseline_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/sst2_baseline_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/sst2_baseline_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/imdb_RandomInitEmbeddingAblation_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/imdb_baseline_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/sst2_RandomInitEmbeddingAblation_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/imdb_RandomInitEmbeddingAblation_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/imdb_baseline_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/yelp_polarity_baseline_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/yelp_polarity_RandomInitEmbeddingAblation_detection_auc.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/yelp_polarity_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/yelp_polarity_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/imdb_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/imdb_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/sst2_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/sst2_loss_curve.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/yelp_polarity_label_distribution.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/yelp_polarity_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/yelp_polarity_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/imdb_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/imdb_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/sst2_auc_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/sst2_label_distribution.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/sst2_loss_curve.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/imdb_label_distribution.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_1fbca80115dc4f94a62937611cfbdba3_proc_164442/sst2_loss_detection.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_1fbca80115dc4f94a62937611cfbdba3_proc_164442/yelp_polarity_loss_detection.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_1fbca80115dc4f94a62937611cfbdba3_proc_164442/imdb_loss_detection.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/yelp_polarity_RandomInitEmbeddingAblation_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/sst2_RandomInitEmbeddingAblation_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/yelp_polarity_baseline_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/sst2_baseline_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/sst2_baseline_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/imdb_RandomInitEmbeddingAblation_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/imdb_baseline_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/sst2_RandomInitEmbeddingAblation_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/imdb_RandomInitEmbeddingAblation_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/imdb_baseline_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/yelp_polarity_baseline_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/yelp_polarity_RandomInitEmbeddingAblation_detection_auc.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/yelp_polarity_RandomInitEmbeddingAblation_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/sst2_RandomInitEmbeddingAblation_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/yelp_polarity_baseline_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/sst2_baseline_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/sst2_baseline_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/imdb_RandomInitEmbeddingAblation_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/imdb_baseline_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/sst2_RandomInitEmbeddingAblation_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/imdb_RandomInitEmbeddingAblation_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/imdb_baseline_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/yelp_polarity_baseline_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/yelp_polarity_RandomInitEmbeddingAblation_detection_auc.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/yelp_polarity_RandomInitEmbeddingAblation_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/sst2_RandomInitEmbeddingAblation_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/yelp_polarity_baseline_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/sst2_baseline_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/sst2_baseline_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/imdb_RandomInitEmbeddingAblation_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/imdb_baseline_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/sst2_RandomInitEmbeddingAblation_loss_curves.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/imdb_RandomInitEmbeddingAblation_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/imdb_baseline_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/yelp_polarity_baseline_detection_auc.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/yelp_polarity_RandomInitEmbeddingAblation_detection_auc.png"], ["experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/sst2_baseline_loss_curves_agg.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/yelp_polarity_baseline_loss_curves_agg.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/imdb_RandomInitEmbeddingAblation_detection_auc_agg.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/yelp_polarity_RandomInitEmbeddingAblation_loss_curves_agg.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/sst2_baseline_detection_auc_agg.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/yelp_polarity_baseline_detection_auc_agg.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/sst2_RandomInitEmbeddingAblation_loss_curves_agg.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/imdb_baseline_detection_auc_agg.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/sst2_RandomInitEmbeddingAblation_detection_auc_agg.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/imdb_baseline_loss_curves_agg.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/yelp_polarity_RandomInitEmbeddingAblation_detection_auc_agg.png", "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/seed_aggregation_3b6180573ecc43b4a15bcb5c441212c8/imdb_RandomInitEmbeddingAblation_loss_curves_agg.png"]], "plot_analyses": [[{"analysis": "Detection AUC for sst2 rises sharply from ~0.51 at epoch 1 to ~0.62 at epoch 2, then makes a modest gain to ~0.63 by epoch 3, showing the uncertainty metric becomes more discriminative with continued training but begins to plateau after epoch 2.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_detection_auc_curve.png"}, {"analysis": "Detection AUC on imdb starts at ~0.52, falls to ~0.48 at epoch 2, and partially recovers to ~0.51 at epoch 3, indicating unstable detection performance and difficulty in distinguishing correct versus incorrect outputs reliably on this dataset.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/comparison_detection_auc.png"}, {"analysis": "Detection AUC for yelp_polarity climbs steadily from ~0.48 at epoch 1 to ~0.544 at epoch 2 and ~0.549 at epoch 3, reflecting consistent improvements though overall performance remains below that seen on sst2.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_loss_curve.png"}, {"analysis": "Training loss for sst2 drops monotonically from ~0.56 to ~0.10 over three epochs, while validation loss decreases from ~0.38 to ~0.33 between epochs 1\u20132 before rising to ~0.44 at epoch 3, signalling the onset of overfitting after epoch 2.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/imdb_detection_auc_curve.png"}, {"analysis": "On imdb, the training curve falls from ~0.58 to ~0.10 by epoch 3; validation loss dips from ~0.41 to ~0.31 at epoch 2 then increases back to ~0.41 at epoch 3, again suggesting optimal generalization around epoch 2 with overfitting thereafter.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/imdb_loss_curve.png"}, {"analysis": "For yelp_polarity, train loss decreases steeply from ~0.46 to ~0.05 by epoch 3; validation loss marginally improves to ~0.235 at epoch 2 then increases to ~0.335 at epoch 3, reinforcing the overfitting trend past the second epoch.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_loss_curve.png"}, {"analysis": "Ground truth on sst2 is balanced (93 negatives vs 107 positives); model predictions shift to 86 negatives vs 114 positives, underpredicting class 0 by ~7 and overpredicting class 1 by ~7, revealing a bias toward positive labels.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/sst2_class_distribution.png"}, {"analysis": "On imdb, true counts are 104 negatives vs 96 positives; predictions flip to 86 negatives vs 114 positives, underestimating negatives by ~18 and overestimating positives by ~18, highlighting a strong positive-class bias.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_detection_auc_curve.png"}, {"analysis": "For yelp_polarity, the truth is evenly split (100/100), yet predictions are 85 negatives vs 115 positives, again underpredicting the negative class by ~15 and overpredicting the positive by ~15, consistent with a positive bias.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/comparison_DES.png"}, {"analysis": "Comparing all three datasets, sst2 shows the highest and most consistent detection AUC gains across epochs, yelp_polarity yields moderate but steady improvements, while imdb remains erratic and lags behind, pointing to data-dependent effectiveness of the perturbation-based uncertainty method.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_6c55123d219846e1ad515f910479dad5_proc_152738/yelp_polarity_class_distribution.png"}], [{"analysis": "yelp_polarity Loss Curves (Head Only) shows both training and validation loss decreasing steadily over five epochs, from roughly 0.696 down to 0.674 for training and 0.696 down to 0.669 for validation. There\u2019s no sign of overfitting; the validation curve remains slightly below the training curve by epoch 5, indicating stable generalization and successful convergence within five epochs.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/yelp_polarity_head_only_loss_curves.png"}, {"analysis": "sst2 Detection AUC Curves (Head Only) reveals that the KL-based uncertainty metric consistently outperforms the simple vote-based metric at every epoch. KL-AUC starts around 0.516, peaks at about 0.5205 in epoch 3, and then gently declines to ~0.5175 by epoch 5. The vote-based AUC fluctuates around random chance (0.5), dipping as low as ~0.494 in epoch 4 before recovering to ~0.515 in epoch 5, suggesting instability and weaker discriminative power for vote divergence on SST2.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/sst2_head_only_detection_auc.png"}, {"analysis": "sst2 Detection DES Curves (Head Only) indicates that normalized DES-KL detection scores hover around 0.086\u20130.087 across epochs, peaking at ~0.0868 in epoch 3 before a slight decline. DES-Vote scores are lower and more variable, ranging from ~0.0828 to ~0.0858, with its best performance in epoch 5. Overall, DES-KL offers a consistently stronger detection signal than DES-Vote for this task.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/sst2_head_only_detection_des.png"}, {"analysis": "imdb Loss Curves (Head Only) demonstrates a comparable learning pattern to Yelp: training loss steadily declines from ~0.702 to ~0.681 and validation loss from ~0.692 to ~0.679 across five epochs. The gap between train and validation losses remains narrow, confirming good fit without evident overfitting by epoch 5.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/imdb_head_only_loss_curves.png"}, {"analysis": "yelp_polarity Detection AUC Curves (Head Only) shows a strong early gain for the vote-based metric, jumping to ~0.535 in epoch 2 before dipping to ~0.517 at epoch 4 and recovering to ~0.532 by epoch 5. The KL-based metric climbs steadily, peaking at ~0.538 in epoch 4 and then slightly falling to ~0.526. The KL measure overtakes vote-based detection by epoch 4, suggesting that KL divergence may provide more reliable uncertainty estimates at later stages of fine-tuning.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/yelp_polarity_head_only_detection_auc.png"}, {"analysis": "imdb Detection DES Curves (Head Only) reveals that DES-Vote detection scores rise from ~0.0856 to ~0.0915 by epoch 4, then fall to ~0.0870 in epoch 5. DES-KL also improves up to ~0.08535 at epoch 4 and then plateaus. DES-Vote outperforms DES-KL on IMDb by a margin of roughly 0.005 at its peak, indicating vote divergence may capture more task-specific uncertainty here.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/imdb_head_only_detection_des.png"}, {"analysis": "imdb Detection AUC Curves (Head Only) finds that vote-based AUC steadily increases to ~0.5485 at epoch 4 before dropping to ~0.522 in epoch 5, while KL-AUC rises to ~0.512 by epoch 4 and then levels off. Peak detection performance occurs at epoch 4 for both metrics, with vote-based AUC outperforming KL by roughly 0.036, suggesting a better overall separation of correct versus incorrect outputs via simple voting divergence on the IMDb dataset.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/imdb_head_only_detection_auc.png"}, {"analysis": "sst2 Loss Curves (Head Only) again confirm stable fine-tuning: training loss falls from ~0.704 to ~0.683, validation from ~0.695 to ~0.685 over five epochs, with no divergence between curves. Rapid early drop between epoch 1 and 2 indicates the head-only adapter quickly learns the SST2 task.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/sst2_head_only_loss_curves.png"}, {"analysis": "yelp_polarity Detection DES Curves (Head Only) shows DES-Vote scores jumping from ~0.0820 to ~0.0893 at epoch 2, followed by slight fluctuations, and reaching ~0.0887 by epoch 5. DES-KL steadily improves to ~0.08985 at epoch 4 before a minor decrease. Both measures peak around epoch 4, with DES-KL slightly surpassing vote-based detection at that stage, indicating stable detection performance for DES-KL and a clear signal for identifying hallucinations in Yelp polarity.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9b232c90eb8248edba7db412eecf37cb_proc_164440/yelp_polarity_head_only_detection_des.png"}], [], [], [{"analysis": "Training accuracy for all three sentiment tasks rises steadily across epochs, nearing perfect performance by epoch 4\u20135. Validation accuracy for SST-2 peaks at epoch 2 before declining, indicating overfitting. Yelp polarity validation stays relatively stable around 0.90 with a slight downward drift after epoch 3. IMDb validation accuracy improves until epoch 3, dips at epoch 4, then recovers somewhat by epoch 5, suggesting mild overfitting and some variance in held-out performance.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_74cd1572cf364cd9af47e780a766505d_proc_164440/accuracy_curves_positional_embedding_ablation.png"}, {"analysis": "Training loss on SST-2, Yelp polarity, and IMDb consistently decreases each epoch, converging toward zero. Validation loss for SST-2 drops until epoch 2 then climbs sharply, mirroring its validation accuracy drop and confirming overfitting. Yelp polarity validation loss bottoms out around epoch 3 before increasing, and IMDb validation loss turns upward after epoch 2. All three tasks exhibit signs of overfitting beyond epoch 2\u20133, despite low training losses.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_74cd1572cf364cd9af47e780a766505d_proc_164440/loss_curves_positional_embedding_ablation.png"}, {"analysis": "Vote-based detection AUC starts low for all tasks. SST-2 vote AUC peaks at epoch 2 (~0.59) then declines. Yelp polarity vote AUC steadily improves across epochs, reaching ~0.625 at epoch 5. IMDb vote AUC fluctuates, peaking at epoch 4 (~0.58). In contrast, KL-based detection AUC is substantially higher across the board: SST-2 climbs from ~0.65 to ~0.70 by epoch 4 before a slight drop, Yelp polarity rises rapidly to ~0.85 by epoch 3 and plateaus, and IMDb KL AUC peaks at ~0.79 at epoch 4. KL divergence clearly outperforms simple vote agreement as an uncertainty metric, with optimal detection performance around epochs 3\u20134.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_74cd1572cf364cd9af47e780a766505d_proc_164440/detection_auc_curves_positional_embedding_ablation.png"}], [{"analysis": "In imdb detection AUC metrics across epochs, AUC_vote gradually increases from about 0.58 at epoch 1 to 0.62 at epoch 2, then slightly dips and plateaus around 0.61 by epoch 5. AUC_kl jumps sharply to 0.90 at epoch 2 before declining to 0.87 by epoch 5. Both curves peak at epoch 2, and the KL divergence measure consistently outperforms the voting-based metric by roughly 0.25\u20130.30, indicating KL divergence is a far stronger uncertainty signal for this task. The slight post-peak decline suggests overfitting or diminishing returns from further epochs.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/imdb_detection_auc.png"}, {"analysis": "In yelp polarity detection, AUC_vote starts near 0.60 at epoch 1 but drops to 0.55 by epoch 3 before a modest recovery to 0.606 at epoch 5. AUC_kl remains high (0.87\u20130.88) at epochs 1\u20132, dips to 0.84 at epoch 3, and recovers to 0.875 by epoch 5. The KL-based approach outperforms vote-based across all epochs, again peaking around epoch 2. The dip at epoch 3 across both curves, especially KL, aligns with overfitting risks when continuing training past the early sweet spot.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/yelp_polarity_detection_auc.png"}, {"analysis": "In sst2 detection, AUC_vote rises from about 0.632 at epoch 1 to 0.645 at epoch 2, then falls to 0.565 at epoch 3 before recovering to 0.64 by epoch 5. AUC_kl trends from 0.732 to 0.738 at epoch 2, dips to 0.69 by epoch 4, then reaches its highest value of 0.75 at epoch 5. The KL divergence metric again outperforms vote-based detection by ~0.10\u20130.15 throughout, though its optimal epoch shifts slightly later compared to the other datasets.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/sst2_detection_auc.png"}, {"analysis": "In yelp polarity loss curves, training loss plummets from 0.25 at epoch 1 to near zero by epoch 4\u20135. Validation loss decreases from 0.168 at epoch 1 to a minimum of 0.162 at epoch 2, then climbs to 0.248 by epoch 4 and 0.232 at epoch 5. This divergence between train and val loss indicates overfitting beyond epoch 2.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/yelp_polarity_loss_curves.png"}, {"analysis": "In imdb loss curves, train loss drops steeply from 0.345 at epoch 1 to 0.015 at epoch 5. Validation loss mildly decreases to 0.205 at epoch 2 then increases steadily to 0.339 by epoch 5, again signaling overfitting after epoch 2.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/imdb_loss_curves.png"}, {"analysis": "In sst2 loss curves, train loss falls from 0.345 at epoch 1 to about 0.015 at epoch 5, while validation loss dips slightly to 0.275 at epoch 2 before rising sharply to 0.52 by epoch 5, reaffirming that epoch 2 appears to be the optimal stopping point to balance fit and generalization.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_9d34db6142df40de818f797aa7734a09_proc_164442/sst2_loss_curves.png"}], [], [], [], [], [], [{"analysis": "Imdb AUC vote curve dips at epoch 2 around 0.52 then rises to peak ~0.62 at epoch 4 before falling at epoch 5, whereas AUC_kl remains high and stable between ~0.83\u20130.87, peaking at epoch 4. This indicates that KL divergence based metric provides consistently stronger discrimination than simple vote agreement on this dataset, with vote metric improving up to epoch 4 but still trailing significantly.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/imdb_auc_curves.png"}, {"analysis": "On SST2, vote-based AUC climbs from about 0.657 to 0.676 by epoch 3 but then sharply drops to ~0.608 at epoch 4 and ~0.604 at epoch 5. In contrast, KL-based metric improves from ~0.689 to ~0.735 by epoch 3 and slightly declines thereafter but stays above ~0.72. This suggests vote agreement becomes unstable after epoch 3, while KL divergence remains robust.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/sst2_auc_curves.png"}, {"analysis": "Yelp polarity vote agreement AUC decreases from ~0.623 at epoch 1 to ~0.583 at epoch 3 before recovering to ~0.654 by epoch 5, showing initial instability. KL-based metric steadily increases from ~0.853 to ~0.915 at epoch 4 before a minor drop at epoch 5, confirming its superior and stable performance in uncertainty detection.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/yelp_polarity_auc_curves.png"}, {"analysis": "Yelp polarity training loss drops rapidly from ~0.27 to ~0.015 by epoch 5, while validation loss decreases slightly until epoch 2 (~0.13) then spikes to ~0.23 at epoch 3, slightly decreases at epoch 4, and rises again at epoch 5. This pattern indicates overfitting beginning around epoch 2\u20133, with generalization deteriorating despite continued training loss reduction.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/yelp_polarity_loss_curves.png"}, {"analysis": "Imdb training loss falls from ~0.36 to ~0.03 by epoch 5, but validation loss declines until epoch 2 (~0.22) then rises steadily to ~0.42 at epoch 5. The gap widening after epoch 2 implies early sign of overfitting, with validation performance degrading as training continues.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/imdb_loss_curves.png"}, {"analysis": "SST2 training loss drops from ~0.42 to ~0.045 by epoch 5, whereas validation loss holds near ~0.33 at epoch 1\u20132, decreases to ~0.295 at epoch 3, then jumps to ~0.425 at epoch 4 and ~0.435 at epoch 5. This trend highlights overfitting starting after epoch 3, suggesting optimal stopping around epoch 3.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7856d3a5914e40dda4cca14396e9eb43_proc_164442/sst2_loss_curves.png"}], [], [], [{"analysis": "Training loss drops rapidly over epochs, converging near zero by epoch 5. Validation loss starts around 0.33, dips slightly at epoch 3 (to ~0.37), then rises to ~0.43 by epoch 5, indicating mild overfitting in the baseline sst2 model.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/yelp_polarity_RandomInitEmbeddingAblation_loss_curves.png"}, {"analysis": "Both training and validation losses start high (~0.69\u20130.70) and decline slowly to ~0.35 (train) and ~0.59 (val) by epoch 5. The gap between train and val remains large, suggesting the ablated model struggles to learn representations compared to baseline.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/sst2_RandomInitEmbeddingAblation_detection_auc.png"}, {"analysis": "Vote-based AUC begins at ~0.68, dips to ~0.62 at epoch 2, then peaks at ~0.72 in epoch 3 before leveling at ~0.70. KL-based AUC follows a similar pattern: ~0.76 \u2192 ~0.73 \u2192 ~0.79 \u2192 ~0.79 \u2192 ~0.79, indicating detection performance improves mid-training and plateaus thereafter for the baseline.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/yelp_polarity_baseline_loss_curves.png"}, {"analysis": "Ablation Vote AUC hovers near chance until epoch 4, then rises to ~0.59 by epoch 5. KL AUC starts around ~0.53, dips at epoch 2, then climbs to ~0.57 by epoch 4 and ~0.56 at epoch 5. Overall detection remains far below baseline, confirming the embedding initialization is critical.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/sst2_baseline_loss_curves.png"}, {"analysis": "Train loss falls quickly from ~0.34 to ~0.04 by epoch 5. Validation loss, however, drifts upward from ~0.24 to ~0.31, showing overfitting in the baseline imdb classifier despite fast convergence on training data.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/sst2_baseline_detection_auc.png"}, {"analysis": "Ablated imdb model maintains high losses throughout, with train loss decreasing slightly from ~0.70 to ~0.68 and validation loss spiking to ~0.72 by epoch 5. Learning is minimal, and generalization is poor relative to the baseline.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/imdb_RandomInitEmbeddingAblation_loss_curves.png"}, {"analysis": "Baseline imdb Vote AUC oscillates between ~0.55 and ~0.59, showing modest detection ability. KL AUC is strong and improving, starting at ~0.85 and rising to ~0.89 by epoch 5, suggesting KL divergence is a reliable uncertainty signal here.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/imdb_baseline_loss_curves.png"}, {"analysis": "Ablation imdb Vote AUC stays around chance (0.50), peaking at ~0.51 only at epoch 4. KL AUC improves from ~0.48 to ~0.64 by epoch 5, but still falls short of baseline\u2019s performance, underscoring the importance of proper embedding initialization.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/sst2_RandomInitEmbeddingAblation_loss_curves.png"}, {"analysis": "Baseline yelp Vote AUC varies between ~0.56 and ~0.59, indicating weak-to-moderate detection via voting. KL AUC is significantly higher, fluctuating around ~0.83 to ~0.88, peaking at epoch 4, demonstrating strong detection capability using KL divergence.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/imdb_RandomInitEmbeddingAblation_detection_auc.png"}, {"analysis": "Ablated yelp Vote AUC improves from chance (~0.50) to ~0.60 by epoch 4 before slight dip, while KL AUC jumps from ~0.50 to ~0.83 by epoch 4 and remains stable. Although detection signals recover over time, they never match baseline levels.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_2fd5ab8e286f4e888e8f48f665a0c0dc_proc_164442/imdb_baseline_detection_auc.png"}], [{"analysis": "yelp_polarity AUC metrics: Both vote-based agreement and KL-divergence measures increase from epoch 1 to 3, with KL peaking at ~0.66 and vote at ~0.58. Performance drops sharply at epoch 4 (KL falls below vote) and continues downward into epoch 5, indicating that divergence-based signals are most reliable up to epoch 3 before overfitting degrades uncertainty estimation.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/yelp_polarity_auc_curve.png"}, {"analysis": "yelp_polarity loss curves: Training and validation loss both decline steadily from epoch 1 to 3 (validation loss reaching ~0.52), then validation loss spikes at epoch 4 and both losses rise again at epoch 5. This U-shaped behavior confirms that the model begins to overfit after epoch 3, coinciding with the decline in AUC metrics for hallucination detection.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/yelp_polarity_loss_curve.png"}, {"analysis": "imdb loss curves: Training loss drops from ~0.686 at epoch 1 to ~0.645 at epoch 3, then climbs back up to ~0.697 by epoch 5. Validation loss follows a similar pattern, bottoming at ~0.629 at epoch 3 before increasing. The aligned minima at epoch 3 suggest this is the optimal stopping point for both classification quality and uncertainty detection.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/imdb_loss_curve.png"}, {"analysis": "imdb AUC metrics: KL-divergence produces a strong peak AUC of ~0.625 at epoch 3, while vote-based agreement maxes at ~0.532. Both metrics decline at epoch 4 (KL dips even below vote) and only partially recover at epoch 5. Again, epoch 3 emerges as the sweet spot for PIU\u2019s reliability.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/imdb_auc_curve.png"}, {"analysis": "sst2 AUC metrics: Both vote and KL-based measures start near random (vote ~0.523, KL ~0.567 at epoch 1) and decrease over epochs, with KL settling around ~0.515 and vote around ~0.495 by epoch 5. This near-chance performance indicates that PIU struggles to detect hallucinations reliably on SST-2, possibly due to subtle semantic shifts or weaker signal in sentiment variation.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/sst2_auc_curve.png"}, {"analysis": "sst2 loss curves: Training loss is nearly flat (~0.697) with a slight dip at epoch 3, while validation loss decreases to ~0.691 at epoch 3 then fluctuates marginally. The minimal change in loss despite poor AUC suggests that standard classification convergence does not translate into effective uncertainty estimation in this setting.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_12a9a08d787448b39380d7b5c23a9f8d_proc_164440/sst2_loss_curve.png"}], [{"analysis": "Label distribution for yelp_polarity indicates ground truth is nearly balanced (around 252 vs. 246) but predictions show a slight overprediction of class 0 (~260) and underprediction of class 1 (~240), suggesting a mild bias toward negative labels.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/yelp_polarity_label_distribution.png"}, {"analysis": "AUC curves for yelp_polarity reveal that the KL-based divergence metric (AUC_kl) starts at ~0.77, rises to ~0.85 by epoch 2, peaks at ~0.88 in epoch 4, then slightly drops to ~0.845, whereas the simple vote-agreement metric (AUC_vote) remains low (0.54\u21920.615 at epoch 3) before declining. This demonstrates superior and more stable performance of the KL metric, with an optimal detection window around epochs 3\u20134.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/yelp_polarity_auc_curve.png"}, {"analysis": "Loss trajectories on yelp_polarity show rapid training-loss decline (0.27\u21920.01) across epochs, while validation loss starts low (~0.145), marginally improves by epoch 2, then steadily increases to ~0.26 by epoch 5. The growing gap indicates overfitting after epoch 2, favoring early stopping around epoch 2\u20133.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/yelp_polarity_loss_curve.png"}, {"analysis": "Training vs. validation loss on imdb exhibits a steep drop in train loss (0.37\u21920.035). Validation loss decreases slightly to ~0.215 by epoch 2, spikes to ~0.30 at epoch 3, then fluctuates downward to ~0.28 at epoch 4 before rising again. This pattern signals instability and overfitting beyond epoch 2.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/imdb_loss_curve.png"}, {"analysis": "Imdb AUC curves show AUC_kl consistently high (0.805\u21920.855\u21920.855\u21920.865\u21920.82) with its best at epoch 4, while AUC_vote lags (0.54\u21920.545\u21920.58\u21920.61\u21920.545). The KL divergence measure outperforms vote agreement in detecting uncertainty, peaking slightly later on this dataset.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/imdb_auc_curve.png"}, {"analysis": "Sst2 AUC analysis indicates AUC_kl begins at ~0.725, peaks at ~0.74 in epoch 2, dips to ~0.70 in epoch 4, then recovers to ~0.77 by epoch 5. AUC_vote shows a smaller range (0.625\u21920.675\u21920.635\u21920.645\u21920.675). KL divergence remains the stronger signal throughout but exhibits some variability.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/sst2_auc_curve.png"}, {"analysis": "Label distribution for sst2 is close to balanced (GT: 238 vs. 262) while predictions skew toward class 0 (255 vs. 245), mirroring the bias seen on other tasks with a tendency to overpredict the negative class.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/sst2_label_distribution.png"}, {"analysis": "Loss curves on sst2 reveal fast training-loss reduction (0.395\u21920.035) versus validation loss that falls to ~0.285 by epoch 2, then rises steadily to ~0.435 by epoch 5. Overfitting becomes pronounced after epoch 2, supporting an early-stop choice around that point.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/sst2_loss_curve.png"}, {"analysis": "Imdb label distribution shows ground truth (255 vs. 245) nearly balanced but predictions biased to class 0 (265 vs. 235), consistent with other tasks\u2019 slight negative-class overprediction.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_7fb1bcfc326d4ecd9b86d2033aa9690f_proc_164440/imdb_label_distribution.png"}], [{"analysis": "On sst2, both full-depth and reduced-depth configurations achieve rapid training loss reduction, with train losses dipping below 0.05 by epoch 4. Validation loss for full-depth dips at epoch 2 (~0.30) then rises, indicating mild overfitting beyond epoch 2. Reduced-depth validation loss starts higher (~0.38) and climbs steadily to ~0.62 by epoch 4 before a slight decrease, signaling stronger overfitting or capacity limits. For detection, KL divergence consistently outperforms vote disagreement: full-depth AUC_kl hovers around 0.75\u20130.79 versus AUC_vote at 0.63\u20130.70. Reduced-depth AUC_kl (0.62\u20130.70) also beats AUC_vote (0.59\u20130.65), though both metrics trail full-depth. KL-based PIU yields robust uncertainty estimates despite smaller model capacity.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_1fbca80115dc4f94a62937611cfbdba3_proc_164442/sst2_loss_detection.png"}, {"analysis": "On yelp_polarity, training loss for both depths falls below 0.05 by epoch 3. Full-depth validation loss has its lowest point at epoch 2 (~0.14) then oscillates upward before returning (~0.14), suggesting an optimal early stopping point. Reduced-depth validation loss halves to ~0.23 at epoch 2 but then climbs past its starting value, highlighting capacity constraints. In detection, full-depth AUC_kl remains consistently high (0.83\u20130.88) while AUC_vote peaks around 0.62 at epoch 3 then declines. Reduced-depth AUC_kl improves from ~0.77 to ~0.83, outperforming its vote metric (~0.56\u20130.59) by a substantial margin. KL-based divergence again proves more reliable than majority\u2010vote.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_1fbca80115dc4f94a62937611cfbdba3_proc_164442/yelp_polarity_loss_detection.png"}, {"analysis": "On imdb, both depths drive train loss under 0.07 by epoch 3. Validation loss bottoms out early (full-depth ~0.23 at epoch 2; reduced-depth ~0.27) before rising, indicating early-stopping benefits. Detection AUC shows full-depth AUC_kl leading at ~0.82\u20130.88 while full-depth AUC_vote stays near ~0.59\u20130.61. Reduced-depth AUC_kl steadily climbs from 0.66 to 0.82, narrowing the gap to full-depth, and always exceeds reduced-depth AUC_vote (0.52\u20130.58). KL divergence remains the preferred signal for uncertainty across model capacities.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_1fbca80115dc4f94a62937611cfbdba3_proc_164442/imdb_loss_detection.png"}], [{"analysis": "yelp_polarity Loss Curves (RandomInitEmbeddingAblation): Train and validation losses both start around 0.69. Validation loss briefly increases to ~0.74 at epoch 2 while training loss steadily drops. After epoch 2 both losses decline in tandem, reaching ~0.31 (train) and ~0.34 (val) by epoch 5. No clear overfitting; the model learns more slowly than the baseline but converges smoothly.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/yelp_polarity_RandomInitEmbeddingAblation_loss_curves.png"}, {"analysis": "yelp_polarity Loss Curves (baseline): Training loss plunges from ~0.28 to ~0.02 by epoch 5, while validation loss dips from ~0.18 to ~0.15 at epoch 2 then rises to ~0.25 at epoch 4 before settling at ~0.22. A pronounced train-val gap appears by epoch 3, indicating rapid overfitting under the baseline configuration.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/sst2_RandomInitEmbeddingAblation_detection_auc.png"}, {"analysis": "sst2 Loss Curves (RandomInitEmbeddingAblation): Losses begin around 0.69\u20130.71 and stay nearly flat through epoch 3. Both curves then fall sharply, ending at ~0.47 (train) and ~0.55 (val) by epoch 5. Validation closely tracks training, suggesting stable learning without early overfitting.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/yelp_polarity_baseline_loss_curves.png"}, {"analysis": "sst2 Loss Curves (baseline): Training loss drops quickly from ~0.38 to ~0.04, while validation loss starts at ~0.27, mildly decreases to ~0.26, then climbs to ~0.40 by epoch 4. The widening gap after epoch 2 shows strong overfitting similar to yelp_polarity baseline.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/sst2_baseline_loss_curves.png"}, {"analysis": "imdb Loss Curves (RandomInitEmbeddingAblation): Training loss slowly decreases from ~0.70 to ~0.67. Validation oscillates (0.69\u21920.70\u21920.69\u21920.72\u21920.74), peaking at epoch 5. Learning is stable but sluggish; slight overfitting emerges late, though margins remain modest.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/sst2_baseline_detection_auc.png"}, {"analysis": "imdb Loss Curves (baseline): Training loss falls steeply to ~0.03. Validation loss initially declines to ~0.24 at epoch 3 then spikes to ~0.50 at epoch 4 before settling at ~0.37. A clear overfitting pattern matches other baseline runs.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/imdb_RandomInitEmbeddingAblation_loss_curves.png"}, {"analysis": "yelp_polarity Detection AUC (RandomInitEmbeddingAblation): Vote-based AUC rises modestly from ~0.53 to ~0.59 across epochs. KL-divergence AUC holds ~0.53 early, then jumps after epoch 2, reaching ~0.80 by epoch 5. KL metric outperforms voting, showing improving discrimination as training proceeds.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/imdb_baseline_loss_curves.png"}, {"analysis": "sst2 Detection AUC (RandomInitEmbeddingAblation): Vote-based AUC crawls from ~0.49 to ~0.55 with most gain at epoch 5. KL-based AUC peaks at ~0.54 in epoch 3 but falls back to ~0.47 by epoch 5. Both metrics hover near random chance, indicating weak uncertainty signals under this ablation.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/sst2_RandomInitEmbeddingAblation_loss_curves.png"}, {"analysis": "imdb Detection AUC (RandomInitEmbeddingAblation): Vote AUC fluctuates around chance (0.50\u20130.52) with a slight peak at epoch 3. KL AUC drifts between ~0.49 and ~0.56, peaking at epoch 5. Overall detection performance remains poor, confirming limited utility of random initial embeddings for PIU.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/imdb_RandomInitEmbeddingAblation_detection_auc.png"}, {"analysis": "imdb Detection AUC (baseline): Vote-based AUC starts ~0.61, dips to ~0.55 at epoch 3 then recovers to ~0.61 by epoch 5. KL-based AUC stays high (~0.85\u20130.87) throughout. Baseline substantially outperforms the random-init ablation, especially on KL divergence, validating the importance of pre-trained embeddings for accurate hallucination detection.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/imdb_baseline_detection_auc.png"}], [{"analysis": "yelp_polarity Loss Curves (RandomInitEmbeddingAblation) show that both training and validation loss decrease steadily from epoch 1 to 4, with training loss dropping from ~0.69 to ~0.25 and validation loss from ~0.68 to ~0.31. At epoch 5, training loss continues to decline slightly to ~0.22, but validation loss rises to ~0.38, indicating mild overfitting after epoch 4.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/yelp_polarity_RandomInitEmbeddingAblation_loss_curves.png"}, {"analysis": "sst2 Loss Curves (RandomInitEmbeddingAblation) reveal consistent training loss decline from ~0.70 at epoch 1 to ~0.29 at epoch 5. Validation loss falls until epoch 3 (~0.60) but then plateaus around ~0.58\u20130.59 for epochs 4\u20135. This suggests the model continues to fit training data well but stops improving on validation after epoch 3, signaling capacity or data limitations.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/sst2_RandomInitEmbeddingAblation_detection_auc.png"}, {"analysis": "imdb Loss Curves (RandomInitEmbeddingAblation) are almost flat: training loss hovers around ~0.698\u20130.694 and validation loss around ~0.695\u20130.692 until a slight uptick at epoch 5 (~0.693). This indicates minimal learning progress on IMDb under this ablation, implying the random-init embedding variant fails to capture sentiment patterns effectively.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/yelp_polarity_baseline_loss_curves.png"}, {"analysis": "sst2 Loss Curves (baseline) exhibit rapid training loss reduction from ~0.39 to ~0.03 across epochs. Validation loss drops from ~0.32 to ~0.25 by epoch 2, spikes to ~0.45 at epoch 3, then settles around ~0.37\u20130.40. The spike at epoch 3 suggests a transient overfit or learning rate issue; overall, baseline generalization is volatile after initial gains.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/sst2_baseline_loss_curves.png"}, {"analysis": "yelp_polarity Detection AUC (RandomInitEmbeddingAblation) using vote divergence starts low (~0.51 at epoch 1), dips at epoch 2 (~0.495), then rises to ~0.59 by epoch 3 and ~0.605 by epoch 5. KL-divergence based AUC jumps sharply from ~0.49 at epoch 2 to ~0.74 at epoch 3, climbing to ~0.78 by epoch 5. KL consistently outperforms vote, with most gains after epoch 2.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/sst2_baseline_detection_auc.png"}, {"analysis": "sst2 Detection AUC (RandomInitEmbeddingAblation) confirms a similar trend: vote AUC dips at epoch 2 (~0.48) then climbs to ~0.565 at epoch 4 before a slight drop to ~0.555 at epoch 5. KL AUC also dips early (~0.48) then improves to ~0.57 at epoch 3 and peaks at ~0.59 at epoch 5. Divergence-based detection becomes reliable only after initial epochs.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/imdb_RandomInitEmbeddingAblation_loss_curves.png"}, {"analysis": "imdb Detection AUC (RandomInitEmbeddingAblation) for vote divergence remains at chance (~0.50) across epochs, indicating vote-based uncertainty fails for IMDb under this ablation. KL-divergence starts below chance (~0.487) and improves steadily after epoch 2 to ~0.559 by epoch 5, showing moderate detection ability emerges with training.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/imdb_baseline_loss_curves.png"}, {"analysis": "yelp_polarity Detection AUC (baseline) shows strong performance: vote AUC peaks at ~0.63 (epoch 3) before declining to ~0.54 at epoch 5. KL AUC is very high throughout (~0.86\u20130.895), with a peak at epoch 3 and slight fluctuations thereafter. Baseline embeddings yield much stronger uncertainty signals than the random-init ablation.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/sst2_RandomInitEmbeddingAblation_loss_curves.png"}, {"analysis": "sst2 Detection AUC (baseline) maintains vote AUC around ~0.69\u20130.62 with a dip at epoch 3 (~0.57) and a rebound by epoch 5 (~0.623). KL AUC stays in the ~0.735\u20130.745 range, dropping at epoch 3 but recovering by epoch 5. Baseline again outperforms the ablation by a comfortable margin.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/imdb_RandomInitEmbeddingAblation_detection_auc.png"}, {"analysis": "imdb Detection AUC (baseline) achieves vote AUC ~0.587\u20130.619, peaking at epoch 3, then tapering. KL AUC remains very strong (~0.842\u20130.867), with highest value at epoch 5. Baseline embeddings enable robust hallucination detection for IMDb, unlike the ablation.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/imdb_baseline_detection_auc.png"}], [{"analysis": "yelp_polarity Loss Curves (RandomInitEmbeddingAblation) show a steady decline in both training and validation losses over five epochs, with training loss dropping from ~0.69 to ~0.23 and validation from ~0.69 to ~0.27. The model learns effectively without obvious overfitting, as validation loss follows the downward trend alongside training loss.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/yelp_polarity_RandomInitEmbeddingAblation_loss_curves.png"}, {"analysis": "yelp_polarity Detection AUC (RandomInitEmbeddingAblation) indicates that vote-based uncertainty detection improves from ~0.505 to ~0.623 AUC by epoch\u20094, then plateaus, while KL-based detection climbs sharply from ~0.535 to ~0.85 AUC. KL divergence proves far more effective than simple vote agreement for this task.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/sst2_RandomInitEmbeddingAblation_detection_auc.png"}, {"analysis": "sst2 Loss Curves (baseline) reveal rapid decrease in training loss (from ~0.38 to ~0.04) but a monotonic rise in validation loss (from ~0.31 to ~0.415), signaling clear overfitting as epochs progress.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/yelp_polarity_baseline_loss_curves.png"}, {"analysis": "sst2 Detection AUC (baseline) shows highest detection performance at epoch\u20091 (vote AUC ~0.71, KL AUC ~0.774) that degrades by epoch\u20093 (vote ~0.57, KL ~0.712) before partially recovering. Overfitting in the classifier aligns with diminished ability to detect uncertainty later in training.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/sst2_baseline_loss_curves.png"}, {"analysis": "sst2 Loss Curves (RandomInitEmbeddingAblation) exhibit a typical overfitting pattern: training loss falls steadily from ~0.69 to ~0.30, while validation loss climbs from ~0.69 to ~0.83 by epoch\u20094 and only slightly recovers to ~0.69 at epoch\u20095, indicating unstable generalization under this ablation.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/sst2_baseline_detection_auc.png"}, {"analysis": "sst2 Detection AUC (RandomInitEmbeddingAblation) remains near chance for vote-based detection (~0.5 across epochs) and low for KL-based detection (peaking at ~0.589 in epoch\u20093). Randomly initialized embeddings catastrophically impair both classification and uncertainty estimation on SST2.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/imdb_RandomInitEmbeddingAblation_loss_curves.png"}, {"analysis": "imdb Loss Curves (baseline) indicate strong fit on training data (loss from ~0.35 to ~0.04) with modest validation overfitting (validation loss dipping slightly then rising to ~0.30 by epoch\u20094 and easing to ~0.28), yet maintaining reasonable generalization.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/imdb_baseline_loss_curves.png"}, {"analysis": "imdb Detection AUC (baseline) achieves moderate vote-based performance (0.575\u21920.605\u21920.58) and strong KL-based detection, peaking at ~0.865 AUC in epoch\u20093. The model\u2019s uncertainty estimates are most reliable in mid-training before slight fluctuations.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/sst2_RandomInitEmbeddingAblation_loss_curves.png"}, {"analysis": "imdb Loss Curves (RandomInitEmbeddingAblation) are essentially flat, with training loss hovering around ~0.698\u21920.694 and validation ~0.693\u21920.694 over five epochs. The ablated model fails to learn meaningful representations on IMDB.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/imdb_RandomInitEmbeddingAblation_detection_auc.png"}, {"analysis": "imdb Detection AUC (RandomInitEmbeddingAblation) shows vote-based detection stuck at random chance (0.50) and KL-based detection barely above chance (~0.483\u21920.525), confirming that without pretrained embeddings, both classification competence and uncertainty quantification collapse.", "plot_path": "experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/imdb_baseline_detection_auc.png"}], []], "vlm_feedback_summary": ["Detection performance improves clearly for sst2 and yelp_polarity, peaking at\nepoch 3 despite overfitting signals in validation loss. Prediction distributions\nreveal a systematic bias toward positive labels across all tasks. imdb exhibits\nunstable detection and lower AUC overall, suggesting dataset-specific challenges\nthat merit further investigation.", "All tasks demonstrate stable loss reduction without overfitting through five\nepochs. KL-divergence generally outperforms vote-based divergence on SST2 and\nYelp for AUC detection, whereas vote-based methods show stronger detection on\nIMDb, especially around epoch 4. DES-KL offers consistent detection scores\nacross tasks, with peak performance around epoch 3\u20134, while DES-Vote exhibits\nmore variability but can surpass DES-KL on certain tasks (e.g., IMDb). These\ninsights guide optimal epoch selection and metric choice for robust uncertainty\nquantification in head-only fine-tuning.", "[]", "[]", "Classification accuracy and loss curves reveal overfitting past epoch 2\u20133 for\nall datasets. Detection metrics show that KL divergence yields much stronger\nAUCs than vote agreement, peaking at epochs 3\u20134. Early stopping and favoring the\nKL-based divergence metric are recommended for robust uncertainty estimation.", "KL-based divergence across prompt perturbations consistently provides a more\nreliable uncertainty and hallucination signal than majority-vote agreement.\nDetection performance peaks around epoch 2 for most tasks, aligning with the\nlowest validation losses and suggesting early stopping is crucial to prevent\noverfitting. Dataset difficulty impacts overall AUC: IMDb shows the highest\ndetection scores, Yelp is intermediate, and SST-2 is the most challenging, yet\nKL divergence still outperforms vote-based measures by a substantial margin.", "[]", "[]", "[]", "[]", "[]", "Across the three sentiment analysis benchmarks, the KL divergence\u2013based\nuncertainty metric consistently outperforms the vote agreement metric,\ndelivering higher and more stable AUC scores with peaks around epoch 4. The\nvote-based metric shows more volatility and degraded performance in later\nepochs. Loss curves on all datasets exhibit rapid training loss reduction but\nrising validation loss after epoch 2\u20133, indicating overfitting and underscoring\nthe need for early stopping around epoch 3 to 4 for best generalization.", "[]", "[]", "Across SST-2, IMDB, and Yelp, the baseline model achieves rapid loss reduction\nand high KL-based detection AUC, with vote-based detection less reliable.\nRandomInitEmbeddingAblation severely hampers both training efficacy and\nuncertainty estimation, particularly vote-based metrics, highlighting the\ncritical role of embedding initialization. KL divergence consistently\noutperforms voting for detecting model errors.", "KL-divergence consistently outperforms vote-based ensemble metrics, with both\nreaching optimal hallucination detection at epoch 3 before overfitting harms\nperformance. Yelp and IMDb datasets show moderate detection success, but SST-2\nyields near-random AUC, revealing dataset-specific limitations. Early stopping\nat epoch 3 and improvements in paraphrase generation quality are recommended to\nenhance PIU\u2019s robustness.", "Across yelp_polarity, imdb, and sst2, the KL-divergence uncertainty metric\nconsistently outperforms vote-agreement in AUC, peaking around epochs 3\u20134. All\ntasks exhibit overfitting after epoch 2\u2014validation loss rises as training loss\nfalls\u2014indicating early stopping should be set around epoch 2\u20133. Slight model\nbias toward class 0 (negative label) appears in all label distributions,\nsuggesting the need for balancing strategies or calibration in paraphrase-driven\nPIU.", "Across SST2, Yelp Polarity, and IMDB, KL\u2010based divergence from a small prompt\nensemble consistently outperforms simple vote agreement in detecting model\nuncertainty and potential hallucinations. Full-depth models edge out reduced-\ndepth on absolute AUCs, but reduced-depth still gains significant signal from\nPIU. Early stopping (around epoch 2) appears critical to prevent overfitting.\nThese ablations confirm that PIU\u2019s core component\u2014measuring output divergence\nvia lightweight paraphrases\u2014is effective and model-agnostic, with KL divergence\nas the most reliable metric.", "Loss curves show that random-init embedding ablation leads to more stable but\nslower learning and less overfitting, while baseline models converge quickly but\noverfit. Detection AUC under ablation is near chance for SST2 and IMDB, with\nonly moderate gains on Yelp using KL divergence. Baseline yields substantially\nhigher AUCs, highlighting the critical role of pre-trained embeddings in PIU\u2019s\nuncertainty estimation.", "Random-init embedding ablation leads to slower or stalled training on text\nclassification tasks (especially IMDb) and yields only modest detection AUC\n(~0.5\u20130.6), with KL-divergence outperforming vote-based metrics. Baseline\nembeddings train effectively and deliver consistently high detection AUCs\n(~0.7\u20130.9). Overall, initializing embeddings properly is crucial for both task\nperformance and reliable uncertainty estimation; KL divergence is generally a\nmore robust detection metric than simple vote agreement.", "The ablation of pretrained embeddings preserves learning and uncertainty\ndetection on Yelp but severely degrades both on SST2 and IMDB. KL-divergence\nconsistently outperforms simple vote-based metrics for detecting hallucination\nrisk, especially when the underlying model is capable, but embedding\ninitialization is critical for stable uncertainty estimation.", "[]"], "exec_time": [159.76402926445007, 747.8223986625671, 3601.3011527061462, 1425.2065353393555, 1799.8510055541992, 1418.4562184810638, 3600.8541436195374, 3600.831780433655, 2951.265166759491, 2658.6479275226593, 22.71717381477356, 1410.9964501857758, 548.2217319011688, 1315.3379309177399, 2817.5418224334717, 1397.5578532218933, 1415.9011487960815, 2193.369564771652, 2869.728554725647, 2834.5784113407135, 2842.3466193675995, null], "exec_time_feedback": ["Implementation works but runs too quickly (2.66 minutes).We have up to 60\nminutes available for each experiment.Make sure to scale up the experiment by\nincreasing the number of epochs, using a larger model, or working with bigger\ndatasets.Given that the current execution time is {exec_time_minutes:.2f}\nminutes, think about how changing the number of epochs to run, or using a larger\nmodel, or working with bigger datasets to runwill affect the execution time, and\nmake sure to scale up the experiment accordingly.", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["sst2", "yelp_polarity"], ["['yelp_polarity'", "'sst2'", "'imdb']"], [], [], ["['yelp_polarity'", "'imdb']"], ["['IMDb'", "'Yelp Polarity'", "'SST-2']"], [], [], [], [], [], ["[Imdb", "SST2", "Yelp polarity]"], [], [], ["sst2", "imdb", "yelp"], ["[yelp_polarity", "imdb]"], ["yelp_polarity", "imdb", "sst2"], ["['sst2'", "'yelp_polarity'", "'imdb']"], ["yelp_polarity", "imdb"], ["[yelp_polarity", "sst2", "imdb]"], ["['yelp_polarity']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    # Per\u2010dataset visualizations\n    for dataset, exp in experiment_data.items():\n        # Loss curves\n        try:\n            epochs = [d[\"epoch\"] for d in exp[\"losses\"][\"train\"]]\n            tr_loss = [d[\"loss\"] for d in exp[\"losses\"][\"train\"]]\n            vl_loss = [d[\"loss\"] for d in exp[\"losses\"][\"val\"]]\n            plt.figure()\n            plt.plot(epochs, tr_loss, label=\"Train Loss\")\n            plt.plot(epochs, vl_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{dataset} Loss Curve (Train vs Validation)\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_loss_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {dataset}: {e}\")\n            plt.close()\n        # Detection AUC curve\n        try:\n            epochs = [d[\"epoch\"] for d in exp[\"metrics\"][\"detection\"]]\n            auc = [d[\"auc\"] for d in exp[\"metrics\"][\"detection\"]]\n            plt.figure()\n            plt.plot(epochs, auc, marker=\"o\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Detection AUC\")\n            plt.title(f\"{dataset} Detection AUC Curve\")\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_detection_auc_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating detection AUC plot for {dataset}: {e}\")\n            plt.close()\n        # Class distribution vs ground truth\n        try:\n            preds = exp[\"predictions\"]\n            gt = list(exp[\"ground_truth\"])\n            counts_pred = [preds.count(0), preds.count(1)]\n            counts_gt = [gt.count(0), gt.count(1)]\n            x = np.arange(2)\n            width = 0.35\n            plt.figure()\n            plt.bar(x - width / 2, counts_gt, width, label=\"Ground Truth\")\n            plt.bar(x + width / 2, counts_pred, width, label=\"Predictions\")\n            plt.xticks(x, [\"Class 0\", \"Class 1\"])\n            plt.ylabel(\"Count\")\n            plt.title(f\"{dataset} Class Distribution: GT vs Preds\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{dataset}_class_distribution.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating class distribution plot for {dataset}: {e}\")\n            plt.close()\n\n    # Combined detection AUC comparison\n    try:\n        plt.figure()\n        for dataset, exp in experiment_data.items():\n            epochs = [d[\"epoch\"] for d in exp[\"metrics\"][\"detection\"]]\n            auc = [d[\"auc\"] for d in exp[\"metrics\"][\"detection\"]]\n            plt.plot(epochs, auc, marker=\"o\", label=dataset)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Detection AUC\")\n        plt.title(\"Comparison of Detection AUC Across Datasets\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"comparison_detection_auc.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating combined detection AUC plot: {e}\")\n        plt.close()\n\n    # Combined DES comparison\n    try:\n        plt.figure()\n        for dataset, exp in experiment_data.items():\n            epochs = [d[\"epoch\"] for d in exp[\"metrics\"][\"detection\"]]\n            des = [d[\"DES\"] for d in exp[\"metrics\"][\"detection\"]]\n            plt.plot(epochs, des, marker=\"o\", label=dataset)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"DES\")\n        plt.title(\"Comparison of DES Across Datasets\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"comparison_DES.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating combined DES plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor ds_name, ds_data in experiment_data.get(\"head_only\", {}).items():\n    # Loss curves\n    try:\n        losses = ds_data[\"losses\"]\n        epochs = [e[\"epoch\"] for e in losses[\"train\"]]\n        train_l = [e[\"loss\"] for e in losses[\"train\"]]\n        val_l = [e[\"loss\"] for e in losses[\"val\"]]\n        plt.figure()\n        plt.plot(epochs, train_l, label=\"Train Loss\")\n        plt.plot(epochs, val_l, label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name} Loss Curves\\nHead Only\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_head_only_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # Detection AUC curves\n    try:\n        metrics = ds_data[\"metrics\"][\"detection\"]\n        epochs = [m[\"epoch\"] for m in metrics]\n        auc_vote = [m[\"auc_vote\"] for m in metrics]\n        auc_kl = [m[\"auc_kl\"] for m in metrics]\n        plt.figure()\n        plt.plot(epochs, auc_vote, label=\"AUC Vote\")\n        plt.plot(epochs, auc_kl, label=\"AUC KL\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"AUC\")\n        plt.title(f\"{ds_name} Detection AUC Curves\\nHead Only\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_head_only_detection_auc.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating detection AUC plot for {ds_name}: {e}\")\n        plt.close()\n\n    # Detection DES curves\n    try:\n        des_vote = [m[\"DES_vote\"] for m in metrics]\n        des_kl = [m[\"DES_kl\"] for m in metrics]\n        plt.figure()\n        plt.plot(epochs, des_vote, label=\"DES Vote\")\n        plt.plot(epochs, des_kl, label=\"DES KL\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Detection Score (normalized)\")\n        plt.title(f\"{ds_name} Detection DES Curves\\nHead Only\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_head_only_detection_des.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating detection DES plot for {ds_name}: {e}\")\n        plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    exp = experiment_data.get(\"No_Pretraining_RandomInit\", {})\n    datasets = list(exp.keys())\n    # Print final detection AUC metrics\n    for ds in datasets:\n        det = exp[ds][\"metrics\"][\"detection\"]\n        last = det[-1]\n        print(\n            f\"{ds} Final AUC Vote: {last['auc_vote']:.4f}, AUC KL: {last['auc_kl']:.4f}\"\n        )\n    # Plot loss curves\n    try:\n        fig, axes = plt.subplots(1, len(datasets), figsize=(5 * len(datasets), 4))\n        for ax, ds in zip(axes, datasets):\n            losses = exp[ds][\"losses\"]\n            epochs = [d[\"epoch\"] for d in losses[\"train\"]]\n            tr = [d[\"loss\"] for d in losses[\"train\"]]\n            va = [d[\"loss\"] for d in losses[\"val\"]]\n            ax.plot(epochs, tr, label=\"Train Loss\")\n            ax.plot(epochs, va, label=\"Val Loss\")\n            ax.set_title(f\"{ds} Loss Curves\")\n            ax.set_xlabel(\"Epoch\")\n            ax.set_ylabel(\"Loss\")\n            ax.legend()\n        fig.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"all_datasets_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves: {e}\")\n        plt.close()\n    # Plot detection AUC curves\n    try:\n        fig, axes = plt.subplots(1, len(datasets), figsize=(5 * len(datasets), 4))\n        for ax, ds in zip(axes, datasets):\n            det = exp[ds][\"metrics\"][\"detection\"]\n            epochs = [d[\"epoch\"] for d in det]\n            av = [d[\"auc_vote\"] for d in det]\n            ak = [d[\"auc_kl\"] for d in det]\n            ax.plot(epochs, av, label=\"AUC Vote\")\n            ax.plot(epochs, ak, label=\"AUC KL\")\n            ax.set_title(f\"{ds} Detection AUC\")\n            ax.set_xlabel(\"Epoch\")\n            ax.set_ylabel(\"AUC\")\n            ax.legend()\n        fig.tight_layout()\n        plt.savefig(os.path.join(working_dir, \"all_datasets_detection_auc.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating detection AUC plots: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = experiment_data[\"positional_embedding_ablation\"]\n    datasets = list(data.keys())\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    datasets = []\n    data = {}\n\n# Print final evaluation metrics\nfor name in datasets:\n    val_acc = data[name][\"metrics\"][\"val\"][-1]\n    det = data[name][\"detection\"][-1]\n    print(\n        f\"{name} - Final Val Acc: {val_acc:.4f}, AUC_vote: {det['auc_vote']:.4f}, AUC_kl: {det['auc_kl']:.4f}\"\n    )\n\n# Plot 1: Loss curves\ntry:\n    epochs = range(1, len(data[datasets[0]][\"losses\"][\"train\"]) + 1)\n    plt.figure()\n    for name in datasets:\n        losses = data[name][\"losses\"]\n        plt.plot(epochs, losses[\"train\"], label=f\"{name} Train\")\n        plt.plot(epochs, losses[\"val\"], \"--\", label=f\"{name} Val\")\n    plt.title(\"Loss Curves for sst2, yelp_polarity, imdb\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(\n        os.path.join(working_dir, \"loss_curves_positional_embedding_ablation.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# Plot 2: Accuracy curves\ntry:\n    plt.figure()\n    for name in datasets:\n        mets = data[name][\"metrics\"]\n        plt.plot(epochs, mets[\"train\"], label=f\"{name} Train\")\n        plt.plot(epochs, mets[\"val\"], \"--\", label=f\"{name} Val\")\n    plt.title(\"Accuracy Curves for sst2, yelp_polarity, imdb\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(\n        os.path.join(working_dir, \"accuracy_curves_positional_embedding_ablation.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves: {e}\")\n    plt.close()\n\n# Plot 3: Detection AUC curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for name in datasets:\n        dets = data[name][\"detection\"]\n        ep = [d[\"epoch\"] for d in dets]\n        axes[0].plot(ep, [d[\"auc_vote\"] for d in dets], label=name)\n        axes[1].plot(ep, [d[\"auc_kl\"] for d in dets], label=name)\n    axes[0].set_title(\"AUC_vote over Epochs\")\n    axes[1].set_title(\"AUC_kl over Epochs\")\n    for ax in axes:\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"AUC\")\n        ax.legend()\n    fig.suptitle(\n        \"Detection AUC Curves for sst2, yelp_polarity, imdb\\n(Left: AUC_vote, Right: AUC_kl)\"\n    )\n    plt.savefig(\n        os.path.join(\n            working_dir, \"detection_auc_curves_positional_embedding_ablation.png\"\n        )\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating detection AUC plots: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = data.get(\"No_Dropout_Ablation\", {})\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\nfor ds_name, expd in exp.items():\n    # Prepare loss data\n    train_vals = expd[\"losses\"][\"train\"]\n    val_vals = expd[\"losses\"][\"val\"]\n    epochs = [e[\"epoch\"] for e in train_vals]\n    train_loss = [e[\"loss\"] for e in train_vals]\n    val_loss = [e[\"loss\"] for e in val_vals]\n    # Plot loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_loss, marker=\"o\", label=\"Train Loss\")\n        plt.plot(epochs, val_loss, marker=\"o\", label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds_name} Loss Curves (Training & Validation)\")\n        plt.legend()\n        fn = os.path.join(working_dir, f\"{ds_name}_loss_curves.png\")\n        plt.savefig(fn)\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n    finally:\n        plt.close()\n    # Prepare detection metrics\n    det = expd[\"metrics\"][\"detection\"]\n    det_epochs = [m[\"epoch\"] for m in det]\n    auc_vote = [m[\"auc_vote\"] for m in det]\n    auc_kl = [m[\"auc_kl\"] for m in det]\n    # Plot detection AUCs\n    try:\n        plt.figure()\n        plt.plot(det_epochs, auc_vote, marker=\"o\", label=\"AUC_vote\")\n        plt.plot(det_epochs, auc_kl, marker=\"o\", label=\"AUC_kl\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"AUC\")\n        plt.title(f\"{ds_name} Detection AUC Metrics\")\n        plt.legend()\n        fn2 = os.path.join(working_dir, f\"{ds_name}_detection_auc.png\")\n        plt.savefig(fn2)\n    except Exception as e:\n        print(f\"Error creating detection plot for {ds_name}: {e}\")\n    finally:\n        plt.close()\n    # Print final metrics\n    if det:\n        last = det[-1]\n        print(\n            f\"{ds_name} Final AUC_vote: {last['auc_vote']:.4f}, AUC_kl: {last['auc_kl']:.4f}\"\n        )\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor ds in experiment_data.get(\"full_heads\", {}):\n    try:\n        data_plot = {}\n        for exp in [\"full_heads\", \"pruned_heads\"]:\n            ed = experiment_data[exp][ds]\n            epochs = [e[\"epoch\"] for e in ed[\"metrics\"][\"train\"]]\n            loss_tr = [e[\"loss\"] for e in ed[\"losses\"][\"train\"]]\n            loss_va = [e[\"loss\"] for e in ed[\"losses\"][\"val\"]]\n            acc_tr = [e[\"acc\"] for e in ed[\"metrics\"][\"train\"]]\n            acc_va = [e[\"acc\"] for e in ed[\"metrics\"][\"val\"]]\n            auc_vote = [e[\"auc_vote\"] for e in ed[\"detection\"]]\n            auc_kl = [e[\"auc_kl\"] for e in ed[\"detection\"]]\n            data_plot[exp] = dict(\n                epochs=epochs,\n                loss_tr=loss_tr,\n                loss_va=loss_va,\n                acc_tr=acc_tr,\n                acc_va=acc_va,\n                auc_vote=auc_vote,\n                auc_kl=auc_kl,\n            )\n        plt.figure(figsize=(8, 12))\n        plt.suptitle(f\"Dataset: {ds}\")\n        # Loss curves\n        plt.subplot(3, 1, 1)\n        for exp, dp in data_plot.items():\n            lbl = exp.replace(\"_\", \" \").title()\n            plt.plot(dp[\"epochs\"], dp[\"loss_tr\"], marker=\"o\", label=f\"{lbl} Train\")\n            plt.plot(\n                dp[\"epochs\"],\n                dp[\"loss_va\"],\n                marker=\"o\",\n                linestyle=\"--\",\n                label=f\"{lbl} Val\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"Train vs Val Loss\")\n        plt.legend()\n        # Accuracy curves\n        plt.subplot(3, 1, 2)\n        for exp, dp in data_plot.items():\n            lbl = exp.replace(\"_\", \" \").title()\n            plt.plot(dp[\"epochs\"], dp[\"acc_tr\"], marker=\"o\", label=f\"{lbl} Train\")\n            plt.plot(\n                dp[\"epochs\"],\n                dp[\"acc_va\"],\n                marker=\"o\",\n                linestyle=\"--\",\n                label=f\"{lbl} Val\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"Train vs Val Accuracy\")\n        plt.legend()\n        # Detection AUC curves\n        plt.subplot(3, 1, 3)\n        for exp, dp in data_plot.items():\n            lbl = exp.replace(\"_\", \" \").title()\n            plt.plot(dp[\"epochs\"], dp[\"auc_vote\"], marker=\"o\", label=f\"{lbl} Vote AUC\")\n            plt.plot(\n                dp[\"epochs\"],\n                dp[\"auc_kl\"],\n                marker=\"o\",\n                linestyle=\"--\",\n                label=f\"{lbl} KL AUC\",\n            )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"AUC\")\n        plt.title(\"Detection AUC Metrics\")\n        plt.legend()\n        plt.tight_layout(rect=[0, 0, 1, 0.96])\n        plt.savefig(os.path.join(working_dir, f\"{ds}_summary.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {ds}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\nfor ablation, datasets in exp.items():\n    for ds_name, data in datasets.items():\n        # Loss curves\n        try:\n            tr = data[\"losses\"][\"train\"]\n            vl = data[\"losses\"][\"val\"]\n            epochs = [e[\"epoch\"] for e in tr]\n            plt.figure()\n            plt.plot(epochs, [e[\"loss\"] for e in tr], label=\"Train Loss\")\n            plt.plot(epochs, [e[\"loss\"] for e in vl], label=\"Val Loss\")\n            plt.title(f\"Loss Curves - {ds_name} ({ablation})\\nTraining vs Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{ds_name}_{ablation}_loss_curve.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {ds_name} {ablation}: {e}\")\n            plt.close()\n        # Detection AUC curves\n        try:\n            det = data[\"metrics\"][\"detection\"]\n            epochs = [m[\"epoch\"] for m in det]\n            plt.figure()\n            plt.plot(epochs, [m[\"auc_vote\"] for m in det], label=\"AUC Vote\")\n            plt.plot(epochs, [m[\"auc_kl\"] for m in det], label=\"AUC KL\")\n            plt.title(f\"Detection AUC Curves - {ds_name} ({ablation})\\nVote vs KL\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"AUC\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{ds_name}_{ablation}_auc_curve.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating detection plot for {ds_name} {ablation}: {e}\")\n            plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor name, data in experiment_data.get(\"token_type_embedding_ablation\", {}).items():\n    # Extract curves and metrics\n    train_entries = data[\"losses\"][\"train\"]\n    val_entries = data[\"losses\"][\"val\"]\n    epochs = [e[\"epoch\"] for e in train_entries]\n    train_losses = [e[\"loss\"] for e in train_entries]\n    val_losses = [e[\"loss\"] for e in val_entries]\n    metric_entries = data[\"metrics\"][\"val\"]\n    auc_vote = [m[\"auc_vote\"] for m in metric_entries]\n    auc_kl = [m[\"auc_kl\"] for m in metric_entries]\n    # Compute and print accuracy\n    preds = np.array(data.get(\"predictions\", []))\n    gt = np.array(data.get(\"ground_truth\", []))\n    if preds.size and gt.size:\n        acc = np.mean(preds == gt)\n        print(f\"{name} Validation Accuracy: {acc:.4f}\")\n    # Plot loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, train_losses, label=\"Train Loss\")\n        plt.plot(epochs, val_losses, label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{name} - Loss Curves\\nLeft: Training Loss, Right: Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{name}_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {name}: {e}\")\n        plt.close()\n    # Plot AUC metrics\n    try:\n        plt.figure()\n        plt.plot(epochs, auc_vote, marker=\"o\", label=\"AUC_vote\")\n        plt.plot(epochs, auc_kl, marker=\"s\", label=\"AUC_kl\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"AUC\")\n        plt.title(f\"{name} - AUC Curves\\nAUC_vote vs AUC_kl\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{name}_auc_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating AUC plot for {name}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = {}\n\n# derive ablations and datasets\nablations = list(data.keys())\ndatasets = list(data[ablations[0]].keys()) if ablations else []\n\nfor ds in datasets:\n    try:\n        fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n        # Left: loss curves\n        for abl in ablations:\n            losses = data[abl][ds][\"losses\"]\n            train = sorted(losses[\"train\"], key=lambda x: x[\"epoch\"])\n            val = sorted(losses[\"val\"], key=lambda x: x[\"epoch\"])\n            epochs = [e[\"epoch\"] for e in train]\n            axs[0].plot(epochs, [e[\"loss\"] for e in train], label=f\"train_{abl}\")\n            axs[0].plot(epochs, [e[\"loss\"] for e in val], label=f\"val_{abl}\")\n        axs[0].set_title(\"Loss curves (Left: Training & Validation)\")\n        axs[0].set_xlabel(\"Epoch\")\n        axs[0].set_ylabel(\"Loss\")\n        axs[0].legend()\n\n        # Right: detection AUC\n        for abl in ablations:\n            det = sorted(\n                data[abl][ds][\"metrics\"][\"detection\"], key=lambda x: x[\"epoch\"]\n            )\n            epochs = [d[\"epoch\"] for d in det]\n            axs[1].plot(epochs, [d[\"auc_vote\"] for d in det], \"--\", label=f\"vote_{abl}\")\n            axs[1].plot(epochs, [d[\"auc_kl\"] for d in det], \"-.\", label=f\"kl_{abl}\")\n        axs[1].set_title(\"Detection AUC (KL & Vote)\")\n        axs[1].set_xlabel(\"Epoch\")\n        axs[1].set_ylabel(\"AUC\")\n        axs[1].legend()\n\n        fig.suptitle(f\"Dataset: {ds}\")\n        fname = os.path.join(working_dir, f\"{ds}_combined.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {ds}: {e}\")\n        plt.close()  # ensure closure even on error\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(data_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Iterate through ablations and datasets to plot\nfor ablation, ds_dict in experiment_data.items():\n    for ds_name, ds_data in ds_dict.items():\n        # Plot training vs validation loss\n        try:\n            plt.figure()\n            epochs = [d[\"epoch\"] for d in ds_data[\"losses\"][\"train\"]]\n            train_loss = [d[\"loss\"] for d in ds_data[\"losses\"][\"train\"]]\n            val_loss = [d[\"loss\"] for d in ds_data[\"losses\"][\"val\"]]\n            plt.plot(epochs, train_loss, label=\"Train Loss\")\n            plt.plot(epochs, val_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{ds_name} Loss Curves ({ablation})\")\n            plt.legend()\n            fname = f\"{ds_name}_{ablation}_loss_curves.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating {ds_name} {ablation} loss plot: {e}\")\n            plt.close()\n        # Plot detection AUC metrics side by side\n        try:\n            metrics = ds_data[\"metrics\"][\"detection\"]\n            epochs = [m[\"epoch\"] for m in metrics]\n            auc_vote = [m[\"auc_vote\"] for m in metrics]\n            auc_kl = [m[\"auc_kl\"] for m in metrics]\n            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n            axes[0].plot(epochs, auc_vote, marker=\"o\")\n            axes[0].set_title(\"Vote AUC\")\n            axes[0].set_xlabel(\"Epoch\")\n            axes[0].set_ylabel(\"AUC\")\n            axes[1].plot(epochs, auc_kl, marker=\"o\")\n            axes[1].set_title(\"KL AUC\")\n            axes[1].set_xlabel(\"Epoch\")\n            axes[1].set_ylabel(\"AUC\")\n            plt.suptitle(\n                f\"{ds_name} Detection AUC ({ablation})\\nLeft: Vote AUC, Right: KL AUC\"\n            )\n            fname = f\"{ds_name}_{ablation}_detection_auc.png\"\n            fig.savefig(os.path.join(working_dir, fname))\n            plt.close(fig)\n        except Exception as e:\n            print(f\"Error creating {ds_name} {ablation} detection plot: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor ds_name, ds_exp in experiment_data.items():\n    try:\n        epochs = [d[\"epoch\"] for d in ds_exp[\"losses\"][\"train\"]]\n        train_loss = [d[\"loss\"] for d in ds_exp[\"losses\"][\"train\"]]\n        val_loss = [d[\"loss\"] for d in ds_exp[\"losses\"][\"val\"]]\n        plt.figure()\n        plt.plot(epochs, train_loss, label=\"Train Loss\")\n        plt.plot(epochs, val_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"Loss Curve\\n{ds_name} Dataset: Training vs Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    try:\n        epochs = [d[\"epoch\"] for d in ds_exp[\"metrics\"][\"val\"]]\n        auc_v = [d[\"auc_vote\"] for d in ds_exp[\"metrics\"][\"val\"]]\n        auc_k = [d[\"auc_kl\"] for d in ds_exp[\"metrics\"][\"val\"]]\n        plt.figure()\n        plt.plot(epochs, auc_v, label=\"AUC Vote\")\n        plt.plot(epochs, auc_k, label=\"AUC KL\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"AUC\")\n        plt.title(f\"AUC Metrics\\n{ds_name} Dataset: Vote vs KL\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ds_name}_auc_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating AUC plot for {ds_name}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor ablation_key, ablation_data in experiment_data.items():\n    for ds_name, ds_data in ablation_data.items():\n        # Loss curves\n        try:\n            plt.figure()\n            epochs = list(range(1, len(ds_data[\"losses\"][\"train\"]) + 1))\n            plt.plot(epochs, ds_data[\"losses\"][\"train\"], label=\"Train Loss\")\n            plt.plot(epochs, ds_data[\"losses\"][\"val\"], label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(\n                f\"Loss Curves \u2014 {ds_name}\\nTraining vs Validation Loss on {ds_name}\"\n            )\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_loss_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {ds_name}: {e}\")\n            plt.close()\n        # AUC curves\n        try:\n            plt.figure()\n            metrics = ds_data[\"metrics\"][\"val\"]\n            epochs = [m[\"epoch\"] for m in metrics]\n            auc_vote = [m[\"auc_vote\"] for m in metrics]\n            auc_kl = [m[\"auc_kl\"] for m in metrics]\n            plt.plot(epochs, auc_vote, marker=\"o\", label=\"AUC_vote\")\n            plt.plot(epochs, auc_kl, marker=\"s\", label=\"AUC_kl\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"AUC\")\n            plt.title(f\"AUC Curves \u2014 {ds_name}\\nAUC_vote and AUC_kl on {ds_name}\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_auc_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating AUC plot for {ds_name}: {e}\")\n            plt.close()\n        # Label distribution\n        try:\n            plt.figure()\n            gt = ds_data[\"ground_truth\"]\n            pred = ds_data[\"predictions\"]\n            counts_gt = [gt.count(0), gt.count(1)]\n            counts_pred = [pred.count(0), pred.count(1)]\n            x = np.arange(2)\n            width = 0.35\n            plt.bar(x - width / 2, counts_gt, width, label=\"Ground Truth\")\n            plt.bar(x + width / 2, counts_pred, width, label=\"Predictions\")\n            plt.xticks(x, [\"Class 0\", \"Class 1\"])\n            plt.ylabel(\"Count\")\n            plt.title(\n                f\"Label Distribution \u2014 {ds_name}\\nLeft: Ground Truth, Right: Predictions for {ds_name}\"\n            )\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_label_distribution.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating label distribution plot for {ds_name}: {e}\")\n            plt.close()\n\n# Print final metrics\nfor ablation_key, ablation_data in experiment_data.items():\n    for ds_name, ds_data in ablation_data.items():\n        final = ds_data[\"metrics\"][\"val\"][-1]\n        print(\n            f\"{ds_name} final metrics: AUC_vote={final['auc_vote']:.4f}, DES_vote={final['DES_vote']:.4f}, \"\n            f\"AUC_kl={final['auc_kl']:.4f}, DES_kl={final['DES_kl']:.4f}\"\n        )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Print final detection metrics\nfor ablation, ds_dict in experiment_data.items():\n    for dataset, data in ds_dict.items():\n        try:\n            last = data[\"metrics\"][\"detection\"][-1]\n            print(\n                f\"[{dataset}][{ablation}] Final metrics - \"\n                f\"AUC_vote: {last['auc_vote']:.4f}, DES_vote: {last['DES_vote']:.4f}, \"\n                f\"AUC_kl: {last['auc_kl']:.4f}, DES_kl: {last['DES_kl']:.4f}\"\n            )\n        except Exception as e:\n            print(f\"Error printing metrics for {dataset}-{ablation}: {e}\")\n\n# Plot per dataset\nfor dataset in experiment_data.get(\"full_depth\", {}):\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n        for ablation in [\"full_depth\", \"reduced_depth\"]:\n            losses = experiment_data[ablation][dataset][\"losses\"]\n            train = [x[\"loss\"] for x in losses[\"train\"]]\n            val = [x[\"loss\"] for x in losses[\"val\"]]\n            epochs = [x[\"epoch\"] for x in losses[\"train\"]]\n            axes[0].plot(epochs, train, marker=\"o\", label=f\"{ablation}-train\")\n            axes[0].plot(epochs, val, marker=\"x\", label=f\"{ablation}-val\")\n\n            det = experiment_data[ablation][dataset][\"metrics\"][\"detection\"]\n            ev = [m[\"auc_vote\"] for m in det]\n            ek = [m[\"auc_kl\"] for m in det]\n            ep = [m[\"epoch\"] for m in det]\n            axes[1].plot(ep, ev, marker=\"o\", label=f\"{ablation}-AUC_vote\")\n            axes[1].plot(ep, ek, marker=\"x\", label=f\"{ablation}-AUC_kl\")\n\n        axes[0].set_xlabel(\"Epoch\")\n        axes[0].set_ylabel(\"Loss\")\n        axes[0].set_title(\"Loss Curves\")\n        axes[0].legend()\n        axes[1].set_xlabel(\"Epoch\")\n        axes[1].set_ylabel(\"AUC\")\n        axes[1].set_title(\"Detection AUC (Vote vs KL)\")\n        axes[1].legend()\n        fig.suptitle(\n            f\"{dataset} Experiment Results\\n\"\n            \"Left: Loss Curves, Right: Detection AUC (Vote vs KL)\"\n        )\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        fname = f\"{dataset}_loss_detection.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {dataset}: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(data_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Iterate through ablations and datasets to plot\nfor ablation, ds_dict in experiment_data.items():\n    for ds_name, ds_data in ds_dict.items():\n        # Plot training vs validation loss\n        try:\n            plt.figure()\n            epochs = [d[\"epoch\"] for d in ds_data[\"losses\"][\"train\"]]\n            train_loss = [d[\"loss\"] for d in ds_data[\"losses\"][\"train\"]]\n            val_loss = [d[\"loss\"] for d in ds_data[\"losses\"][\"val\"]]\n            plt.plot(epochs, train_loss, label=\"Train Loss\")\n            plt.plot(epochs, val_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{ds_name} Loss Curves ({ablation})\")\n            plt.legend()\n            fname = f\"{ds_name}_{ablation}_loss_curves.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating {ds_name} {ablation} loss plot: {e}\")\n            plt.close()\n        # Plot detection AUC metrics side by side\n        try:\n            metrics = ds_data[\"metrics\"][\"detection\"]\n            epochs = [m[\"epoch\"] for m in metrics]\n            auc_vote = [m[\"auc_vote\"] for m in metrics]\n            auc_kl = [m[\"auc_kl\"] for m in metrics]\n            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n            axes[0].plot(epochs, auc_vote, marker=\"o\")\n            axes[0].set_title(\"Vote AUC\")\n            axes[0].set_xlabel(\"Epoch\")\n            axes[0].set_ylabel(\"AUC\")\n            axes[1].plot(epochs, auc_kl, marker=\"o\")\n            axes[1].set_title(\"KL AUC\")\n            axes[1].set_xlabel(\"Epoch\")\n            axes[1].set_ylabel(\"AUC\")\n            plt.suptitle(\n                f\"{ds_name} Detection AUC ({ablation})\\nLeft: Vote AUC, Right: KL AUC\"\n            )\n            fname = f\"{ds_name}_{ablation}_detection_auc.png\"\n            fig.savefig(os.path.join(working_dir, fname))\n            plt.close(fig)\n        except Exception as e:\n            print(f\"Error creating {ds_name} {ablation} detection plot: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(data_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Iterate through ablations and datasets to plot\nfor ablation, ds_dict in experiment_data.items():\n    for ds_name, ds_data in ds_dict.items():\n        # Plot training vs validation loss\n        try:\n            plt.figure()\n            epochs = [d[\"epoch\"] for d in ds_data[\"losses\"][\"train\"]]\n            train_loss = [d[\"loss\"] for d in ds_data[\"losses\"][\"train\"]]\n            val_loss = [d[\"loss\"] for d in ds_data[\"losses\"][\"val\"]]\n            plt.plot(epochs, train_loss, label=\"Train Loss\")\n            plt.plot(epochs, val_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{ds_name} Loss Curves ({ablation})\")\n            plt.legend()\n            fname = f\"{ds_name}_{ablation}_loss_curves.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating {ds_name} {ablation} loss plot: {e}\")\n            plt.close()\n        # Plot detection AUC metrics side by side\n        try:\n            metrics = ds_data[\"metrics\"][\"detection\"]\n            epochs = [m[\"epoch\"] for m in metrics]\n            auc_vote = [m[\"auc_vote\"] for m in metrics]\n            auc_kl = [m[\"auc_kl\"] for m in metrics]\n            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n            axes[0].plot(epochs, auc_vote, marker=\"o\")\n            axes[0].set_title(\"Vote AUC\")\n            axes[0].set_xlabel(\"Epoch\")\n            axes[0].set_ylabel(\"AUC\")\n            axes[1].plot(epochs, auc_kl, marker=\"o\")\n            axes[1].set_title(\"KL AUC\")\n            axes[1].set_xlabel(\"Epoch\")\n            axes[1].set_ylabel(\"AUC\")\n            plt.suptitle(\n                f\"{ds_name} Detection AUC ({ablation})\\nLeft: Vote AUC, Right: KL AUC\"\n            )\n            fname = f\"{ds_name}_{ablation}_detection_auc.png\"\n            fig.savefig(os.path.join(working_dir, fname))\n            plt.close(fig)\n        except Exception as e:\n            print(f\"Error creating {ds_name} {ablation} detection plot: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(data_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Iterate through ablations and datasets to plot\nfor ablation, ds_dict in experiment_data.items():\n    for ds_name, ds_data in ds_dict.items():\n        # Plot training vs validation loss\n        try:\n            plt.figure()\n            epochs = [d[\"epoch\"] for d in ds_data[\"losses\"][\"train\"]]\n            train_loss = [d[\"loss\"] for d in ds_data[\"losses\"][\"train\"]]\n            val_loss = [d[\"loss\"] for d in ds_data[\"losses\"][\"val\"]]\n            plt.plot(epochs, train_loss, label=\"Train Loss\")\n            plt.plot(epochs, val_loss, label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{ds_name} Loss Curves ({ablation})\")\n            plt.legend()\n            fname = f\"{ds_name}_{ablation}_loss_curves.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating {ds_name} {ablation} loss plot: {e}\")\n            plt.close()\n        # Plot detection AUC metrics side by side\n        try:\n            metrics = ds_data[\"metrics\"][\"detection\"]\n            epochs = [m[\"epoch\"] for m in metrics]\n            auc_vote = [m[\"auc_vote\"] for m in metrics]\n            auc_kl = [m[\"auc_kl\"] for m in metrics]\n            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n            axes[0].plot(epochs, auc_vote, marker=\"o\")\n            axes[0].set_title(\"Vote AUC\")\n            axes[0].set_xlabel(\"Epoch\")\n            axes[0].set_ylabel(\"AUC\")\n            axes[1].plot(epochs, auc_kl, marker=\"o\")\n            axes[1].set_title(\"KL AUC\")\n            axes[1].set_xlabel(\"Epoch\")\n            axes[1].set_ylabel(\"AUC\")\n            plt.suptitle(\n                f\"{ds_name} Detection AUC ({ablation})\\nLeft: Vote AUC, Right: KL AUC\"\n            )\n            fname = f\"{ds_name}_{ablation}_detection_auc.png\"\n            fig.savefig(os.path.join(working_dir, fname))\n            plt.close(fig)\n        except Exception as e:\n            print(f\"Error creating {ds_name} {ablation} detection plot: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# List of provided experiment data relative paths\nexperiment_data_paths = [\n    \"experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_869bf7fad75b4c15bd002f01a8a6dfc0_proc_164440/experiment_data.npy\",\n    \"experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_21d62251fddf4f238c0268462b6bca8d_proc_164442/experiment_data.npy\",\n    \"experiments/2025-06-07_22-20-29_perturbation_ensemble_uq_attempt_0/logs/0-run/experiment_results/experiment_8c608bb7f9e8449fb88a79cbe43f43dc_proc_164441/experiment_data.npy\",\n]\n\n# Load all experiment data into a list\ntry:\n    all_experiment_data = []\n    for rel_path in experiment_data_paths:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\"), rel_path)\n        exp_data = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(exp_data)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_experiment_data = []\n\n# Identify all ablations\nall_ablations = set()\nfor exp in all_experiment_data:\n    all_ablations.update(exp.keys())\n\n# Iterate ablations and datasets\nfor ablation in all_ablations:\n    # collect dataset names under this ablation\n    ds_names = set()\n    for exp in all_experiment_data:\n        if ablation in exp:\n            ds_names.update(exp[ablation].keys())\n    for ds_name in ds_names:\n        # Aggregate training and validation losses\n        train_runs, val_runs, epochs = [], [], None\n        for exp in all_experiment_data:\n            if ablation in exp and ds_name in exp[ablation]:\n                ds_data = exp[ablation][ds_name]\n                e = [d[\"epoch\"] for d in ds_data[\"losses\"][\"train\"]]\n                if epochs is None:\n                    epochs = e\n                train_runs.append([d[\"loss\"] for d in ds_data[\"losses\"][\"train\"]])\n                val_runs.append([d[\"loss\"] for d in ds_data[\"losses\"][\"val\"]])\n        if not train_runs:\n            continue\n        train_arr = np.array(train_runs)\n        val_arr = np.array(val_runs)\n        mean_train = np.mean(train_arr, axis=0)\n        se_train = np.std(train_arr, ddof=1, axis=0) / np.sqrt(train_arr.shape[0])\n        mean_val = np.mean(val_arr, axis=0)\n        se_val = np.std(val_arr, ddof=1, axis=0) / np.sqrt(val_arr.shape[0])\n        try:\n            plt.figure()\n            plt.errorbar(\n                epochs, mean_train, yerr=se_train, label=\"Train Loss\", capsize=3\n            )\n            plt.errorbar(epochs, mean_val, yerr=se_val, label=\"Val Loss\", capsize=3)\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{ds_name} Loss Curves Aggregated ({ablation})\")\n            plt.legend()\n            fname = f\"{ds_name}_{ablation}_loss_curves_agg.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating aggregated {ds_name} {ablation} loss plot: {e}\")\n            plt.close()\n        # Aggregate detection AUC metrics\n        det_runs_vote, det_runs_kl, det_epochs = [], [], None\n        for exp in all_experiment_data:\n            if ablation in exp and ds_name in exp[ablation]:\n                metrics = exp[ablation][ds_name][\"metrics\"][\"detection\"]\n                det_epochs = [m[\"epoch\"] for m in metrics]\n                det_runs_vote.append([m[\"auc_vote\"] for m in metrics])\n                det_runs_kl.append([m[\"auc_kl\"] for m in metrics])\n        if not det_runs_vote:\n            continue\n        vote_arr = np.array(det_runs_vote)\n        kl_arr = np.array(det_runs_kl)\n        mean_vote = np.mean(vote_arr, axis=0)\n        se_vote = np.std(vote_arr, ddof=1, axis=0) / np.sqrt(vote_arr.shape[0])\n        mean_kl = np.mean(kl_arr, axis=0)\n        se_kl = np.std(kl_arr, ddof=1, axis=0) / np.sqrt(kl_arr.shape[0])\n        try:\n            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n            axes[0].errorbar(det_epochs, mean_vote, yerr=se_vote, marker=\"o\", capsize=3)\n            axes[0].set_title(\"Vote AUC\")\n            axes[0].set_xlabel(\"Epoch\")\n            axes[0].set_ylabel(\"AUC\")\n            axes[1].errorbar(det_epochs, mean_kl, yerr=se_kl, marker=\"o\", capsize=3)\n            axes[1].set_title(\"KL AUC\")\n            axes[1].set_xlabel(\"Epoch\")\n            axes[1].set_ylabel(\"AUC\")\n            plt.suptitle(\n                f\"{ds_name} Detection AUC Aggregated ({ablation})\\nLeft: Vote AUC, Right: KL AUC\"\n            )\n            fname = f\"{ds_name}_{ablation}_detection_auc_agg.png\"\n            fig.savefig(os.path.join(working_dir, fname))\n            plt.close(fig)\n        except Exception as e:\n            print(f\"Error creating aggregated {ds_name} {ablation} detection plot: {e}\")\n            plt.close()\n"], "plot_plan": ["We\u2019ll load the saved experiment data and for each dataset extract train/validation losses and detection metrics (AUC and DES) per epoch.  We\u2019ll plot loss curves and detection AUC curves for each dataset in separate try/except blocks, then produce two comparison plots across datasets: one for detection AUC and one for DES over epochs.  As a dataset-specific visualization, we\u2019ll also compare class distributions between ground truth and our model\u2019s predictions with a bar chart.  All figures are saved under working_dir with descriptive filenames.  We always close figures after saving, and errors are caught and reported.", "We load the saved experiment data and iterate over each dataset under the \u201chead_only\u201d setting. For each dataset we extract the training and validation losses per epoch and plot them together with clear axis labels and a title indicating the dataset name and \u201cHead Only.\u201d Next we pull the detection metrics (AUC for voting and KL) over epochs and plot both curves in one figure. Then we plot the normalized detection scores (DES_vote and DES_kl) versus epochs in a separate figure. Each of these plotting steps is wrapped in a try-except block that prints any errors, always closes the figure, and saves the plot to the working directory with a descriptive filename. All plotting uses basic matplotlib and only existing data from the .npy file.", null, "We will load the saved experiment data and extract the \u201cNo_Pretraining_RandomInit\u201d ablation. We then iterate over each dataset, printing its final epoch\u2019s detection AUC metrics. Next we build a combined loss\u2010curve figure with one subplot per dataset, plotting train vs. validation loss over epochs, and save it to the working directory in a try\u2010except block. Then we similarly create a combined detection AUC figure showing both vote\u2010based and KL\u2010based AUC per epoch for each dataset, again wrapping in try\u2010except and ensuring figures are closed. All figures include clear titles and axis labels, and saved filenames indicate \u201call_datasets\u201d and the plot type. We use only basic matplotlib, loop cleanly, and close figures even on error. This provides a concise overview of training dynamics and detection performance across all datasets.", "I will load the experiment data and extract the per\u2010dataset losses, accuracies, and detection AUCs. Then I will print the final validation accuracy and detection AUCs for each dataset. Next, I will create three separate figures in try\u2010except blocks: (1) combined loss curves across datasets, (2) combined accuracy curves, and (3) detection AUC curves with two subplots for vote\u2010based and KL\u2010based AUCs. Each plot will be saved to the working directory with descriptive filenames, include appropriate titles and subtitles, and figures will be closed after saving. I will stick to basic matplotlib and ensure no more than five figures.", "We\u2019ll load the saved experiment data, extract the losses and detection metrics for each dataset, and generate two plots per dataset: one showing training vs. validation loss over epochs and another showing AUC_vote and AUC_kl over epochs. Each plot is placed in a try-except block so failures don\u2019t stop execution, and we always close the figure in both cases. Filenames include the dataset name and the type of plot, and titles are descriptive. All figures are saved under the working directory. Finally, we print out a summary of final AUCs for each dataset.", null, null, "We\u2019ll load the saved experiment_data.npy from the working directory and iterate over each dataset. For each dataset, we gather epoch\u2010wise training and validation loss and accuracy from both the full_heads and pruned_heads experiments, as well as detection AUC metrics (vote and KL). Then we create a single summary figure containing three subplots: (1) train vs val loss curves, (2) train vs val accuracy curves, and (3) detection AUC curves for vote and KL. Each subplot overlays both experimental conditions with clear labels. We wrap each figure creation in its own try\u2010except block to handle missing data gracefully and always close the figure afterwards. The overall figure has a suptitle naming the dataset, and each subplot has its own title and axis labels. Finally, the figure is saved to working_dir with a descriptive name like \"{dataset}_summary.png\". Any errors during plotting are caught and printed but do not stop the loop. The code uses basic matplotlib calls, no custom styles, and follows the guideline to save at most one summary figure per dataset.", "First, set up the working directory and load experiment data saved in experiment_data.npy. Then for each model variant (baseline and ffn_ablate) and each dataset, we plot training and validation loss curves across epochs. Each loss plot is saved with a descriptive filename including the dataset name and ablation type. Next, detection metrics (AUC for vote and KL uncertainty) are plotted as curves over epochs in a separate figure per combination. All figures are wrapped in try-except blocks to catch errors and ensure that each plot is closed after saving. The code ensures plots are saved to the working directory, creating it if needed. Titles clearly indicate the dataset, model variant, and whether lines represent training vs validation or vote vs KL. We avoid assuming any extra data beyond what is loaded from the experiment data file.", null, "I will load the saved experiment_data.npy from the working directory and retrieve the token_type_embedding_ablation results for each dataset. For each dataset I will extract the training and validation losses per epoch, as well as the validation AUC_vote and AUC_kl metrics. I will also compute and print the final classification accuracy on the validation set by comparing stored predictions to ground truth labels. Then I'll create two separate figures per dataset: one for the loss curves and another for the AUC metrics, each wrapped in its own try-except block. Titles will specify the dataset name and include subtitles like \"Left: Training Loss, Right: Validation Loss\" or \"AUC_vote vs AUC_kl\". All plots will be saved to working_dir with descriptive filenames containing the dataset name and plot type. I will use plain matplotlib and ensure each figure is closed after saving to keep the working environment clean.", "The code below loads the saved experiment results and for each dataset produces a combined figure with two subplots: (1) training vs. validation loss curves for all ablation settings, and (2) detection AUC curves (vote and KL) for all ablation settings. Each figure is titled with the dataset name and each subplot has a descriptive subtitle. We loop over datasets, using try-except to isolate any errors per plot, and always close the figure after saving. Filenames include the dataset name and \u201ccombined\u201d to clearly indicate their content. The script uses only matplotlib, numpy, and os, writes files into the working directory, and does not assume any extra data beyond the loaded .npy.", null, "We first load the saved experiment data from the working directory. We then iterate over each ablation condition and dataset to generate two plots per combination. For each we start a try\u2010except block and plot the training and validation loss curves over epochs, labeling axes and saving with a descriptive filename before closing the figure. Next, in another try\u2010except block we create a side\u2010by\u2010side subplot for detection AUC metrics, plotting vote AUC on the left and KL AUC on the right, adding a suptitle \u201cLeft: Vote AUC, Right: KL AUC,\u201d and saving before closing. We only use data present in the numpy file, apply basic matplotlib, and wrap every plot in its own try\u2010except to prevent errors from stopping execution. All saved images include the dataset name, ablation type, and plot type.", "We\u2019ll load the saved experiment data from the working directory, then loop through each dataset in it. For each dataset we\u2019ll first plot the training and validation loss over epochs, labeling axes, adding a legend, and giving a title with a subtitle indicating dataset and train vs val. Next we\u2019ll plot the AUC metrics (vote vs KL) over epochs in a separate figure with similar annotations. Each of these two plots per dataset lives in its own try-except block, saving the figure to working_dir under a descriptive filename, and always closing the figure. If loading or plotting fails we\u2019ll print an error message and close any open figure. All code uses basic matplotlib and numpy, and only visualizes the real stored values.", "We will load the saved experiment data and iterate over each ablation group (here only \u201cBiasRemovalAblation\u201d) and its datasets. For each dataset, we create three separate figures inside try-except blocks: training vs. validation loss curves over epochs, AUC curves for both vote and KL uncertainty metrics, and a bar chart comparing ground-truth and predicted label distributions. Each plot is labeled with a clear multi-line title that specifies the dataset and plot type, and saved to the working directory using descriptive filenames. We always close the figure after saving or catching any errors to avoid resource leaks. Finally, after plotting we print out the final validation metrics (AUC_vote, DES_vote, AUC_kl, DES_kl) for each dataset. This script uses only numpy and matplotlib without any custom styles, and only plots data that is present in the loaded experiment data.", "We will load the saved experiment data from the working directory and print out the final detection metrics for each dataset under both ablation settings. Then, for each dataset, we will create a side\u2010by\u2010side plot showing training vs validation loss curves on the left and detection AUC curves (vote and KL) on the right, combining results for both full and reduced depth models. Each plot will have a clear title and subtitle indicating what is shown on each side, and will be saved to the working directory with a descriptive filename. All plotting is wrapped in try\u2010except blocks to handle errors gracefully, and figures are closed after saving. Only existing data from the loaded file is used.", null, null, null, null], "ablation_name": [null, "Head-Only Fine-Tuning", "Partial-Layer Fine-Tuning", "No_Pretraining_RandomInit", "PositionalEmbeddingAblation", "No_Dropout_Ablation", null, "Training-with-Paraphrase-Augmentation", "Multi-Head Attention Pruning", "FeedForwardAblation", "ResidualConnectionAblation", "TokenTypeEmbeddingAblation", null, "LayerNormAblation", "RandomInitEmbeddingAblation", null, "BiasRemovalAblation", "TransformerDepthAblation", null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The following script loads the saved `experiment_data.npy` file from the working\ndirectory, iterates over each dataset\u2019s stored results, and extracts the final\ntraining loss, final validation loss, detection AUC, detection DES, and\nvalidation accuracy computed from the stored predictions and ground truth. It\nprints each dataset\u2019s name followed by clear, fully\u2010specified metric labels and\ntheir values. All code lives at the global level so it runs immediately when the\nscript is executed.", "I will load the saved experiment data and, for each dataset under the\n`head_only` setup, retrieve the final training and validation losses, the\nlast\u2010epoch detection AUC and DES values for both vote and KL uncertainty, and\ncompute test accuracy from the stored predictions versus ground truth. Each\nmetric is printed with a clear, descriptive label, and the script runs\nimmediately at the global scope without any special entry point checks.", "", "The following script loads the saved experiment data and iterates over each\ndataset under the No_Pretraining_RandomInit ablation. For each dataset, it\nretrieves the final training and validation losses, the last\u2010epoch detection\nmetrics (AUC and DES for both voting and KL), and computes test accuracy from\nthe stored predictions and ground truth. All metrics are printed with clear,\ndescriptive names, and the code runs immediately at the global scope without any\nspecial entry point.", "Below is a script that loads the saved experiment data, iterates over each\ndataset in the positional embedding ablation branch, and prints out the final\n(last\u2010epoch) training/validation losses and accuracies as well as the\ncorresponding detection AUC and DES scores for both vote and KL divergence.", "I will load the saved NumPy experiment data from the working directory and\naccess the \u201cNo_Dropout_Ablation\u201d entry. For each dataset, I extract the final\nepoch\u2019s training and validation losses as well as the last detection metrics\n(AUC and DES for both vote and KL methods). The script prints each dataset name\nfollowed by clearly labeled metric values. All code runs immediately at the\nglobal scope without any guarded entry point.", "", "", "The script below loads the saved `experiment_data.npy` file from the `working`\ndirectory, then iterates over each experiment type and dataset. For each dataset\nit pulls out the final epoch\u2019s training and validation loss and accuracy, as\nwell as the final detection AUC and DES metrics, and prints them with clear,\ndescriptive labels. This runs immediately at the global scope without an\nentry\u2010point guard.", "", "", "The script below loads the saved `experiment_data.npy` from the `working`\nfolder, extracts the final validation metrics for each dataset under the\n`token_type_embedding_ablation` experiment, and prints them in a clear\n\u201cvalidation {metric_name}: value\u201d format. It directly executes at the global\nscope without any `if __name__ == \"__main__\":` guard.", "The code below loads the saved experiment results from the working directory and\niterates over each dataset and ablation variant. For each combination, it\nextracts the final training and validation losses, detection AUC, DES, and\nSpearman metrics for both the vote\u2010based and KL\u2010based uncertainty measures, then\ncomputes and prints the final test accuracy. All outputs are labeled with clear\nmetric names, and the script runs immediately at the global scope using\nsklearn\u2019s `accuracy_score` for test accuracy computation.", "The script loads the saved experiment data from the working directory and\niterates through each dataset in the LayerNormAblation experiment. For each\ndataset it prints the final training and validation losses, the last epoch\u2019s\ndetection AUC and DES metrics for both vote and KL uncertainties, and computes\nthe overall test accuracy from the saved predictions. All metrics are printed\nwith clear descriptive labels. The code runs immediately upon execution and does\nnot require any special entry point.", "The script first imports `os` and `numpy` and constructs the path to the\n`experiment_data.npy` file in the `working` directory. It then loads the saved\ndictionary and iterates over each ablation setting and dataset. For each dataset\nit prints the ablation name, the dataset name, and then the final (last\u2010epoch)\nvalues of the training loss, validation loss, and detection metrics (AUC and DES\nfor both vote and KL uncertainties). All metric names are explicitly labeled,\nand the code runs immediately at the global scope without any special entry\npoint guard.", "The following script loads the saved experiment data from the \u201cworking\u201d\ndirectory, iterates over each dataset\u2019s recorded losses and uncertainty metrics,\nand prints the final (last\u2010epoch) train loss, validation loss, and key\nuncertainty metrics (AUC and DES for both vote and KL, plus their Spearman\ncorrelations). Each dataset\u2019s name is printed first, followed by clearly labeled\nmetric values rounded to four decimal places. No plotting or special entry\npoints are used\u2014just a straightforward global\u2010scope script.", "I will load the saved `experiment_data.npy` from the `working` directory,\niterate over each dataset entry, and extract the final epoch\u2019s training and\nvalidation losses as well as the last computed ROC AUC and Detection Efficiency\nScore for both vote\u2010based and KL\u2010based uncertainty. For each dataset, I will\nprint its name followed by clearly labeled metric values.", "The script constructs the path to the working directory, loads the saved\nexperiment data, and then iterates through each ablation setting and dataset.\nFor each dataset it extracts the final epoch\u2019s training loss, validation loss,\nand the last detection metrics (AUC and DES for both vote and KL). Each metric\nis printed with a clear descriptive label following the dataset name. The script\nruns immediately without any special entry points or plotting.", "The script first imports `os` and `numpy` and constructs the path to the\n`experiment_data.npy` file in the `working` directory. It then loads the saved\ndictionary and iterates over each ablation setting and dataset. For each dataset\nit prints the ablation name, the dataset name, and then the final (last\u2010epoch)\nvalues of the training loss, validation loss, and detection metrics (AUC and DES\nfor both vote and KL uncertainties). All metric names are explicitly labeled,\nand the code runs immediately at the global scope without any special entry\npoint guard.", "The script first imports `os` and `numpy` and constructs the path to the\n`experiment_data.npy` file in the `working` directory. It then loads the saved\ndictionary and iterates over each ablation setting and dataset. For each dataset\nit prints the ablation name, the dataset name, and then the final (last\u2010epoch)\nvalues of the training loss, validation loss, and detection metrics (AUC and DES\nfor both vote and KL uncertainties). All metric names are explicitly labeled,\nand the code runs immediately at the global scope without any special entry\npoint guard.", "The script first imports `os` and `numpy` and constructs the path to the\n`experiment_data.npy` file in the `working` directory. It then loads the saved\ndictionary and iterates over each ablation setting and dataset. For each dataset\nit prints the ablation name, the dataset name, and then the final (last\u2010epoch)\nvalues of the training loss, validation loss, and detection metrics (AUC and DES\nfor both vote and KL uncertainties). All metric names are explicitly labeled,\nand the code runs immediately at the global scope without any special entry\npoint guard.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Load the experiment data\nfile_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, results in experiment_data.items():\n    # Extract final training and validation losses\n    final_train_loss = results[\"losses\"][\"train\"][-1][\"loss\"]\n    final_val_loss = results[\"losses\"][\"val\"][-1][\"loss\"]\n\n    # Extract final detection metrics\n    final_detection = results[\"metrics\"][\"detection\"][-1]\n    detection_auc = final_detection[\"auc\"]\n    detection_des = final_detection[\"DES\"]\n\n    # Compute validation accuracy from stored predictions and ground truth\n    preds = np.array(results[\"predictions\"])\n    labels = np.array(results[\"ground_truth\"])\n    validation_accuracy = (preds == labels).mean()\n\n    # Print the metrics\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"train loss: {final_train_loss:.4f}\")\n    print(f\"validation loss: {final_val_loss:.4f}\")\n    print(f\"detection AUC: {detection_auc:.4f}\")\n    print(f\"detection DES: {detection_des:.4f}\")\n    print(f\"validation accuracy: {validation_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract and print metrics for each dataset in the 'head_only' experiments\nhead_only = experiment_data.get(\"head_only\", {})\nfor dataset_name, results in head_only.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Final training loss\n    train_losses = results[\"losses\"][\"train\"]\n    if train_losses:\n        final_train_loss = train_losses[-1][\"loss\"]\n        print(f\"  Final training loss: {final_train_loss:.4f}\")\n\n    # Final validation loss\n    val_losses = results[\"losses\"][\"val\"]\n    if val_losses:\n        final_val_loss = val_losses[-1][\"loss\"]\n        print(f\"  Final validation loss: {final_val_loss:.4f}\")\n\n    # Final detection metrics\n    det_metrics = results[\"metrics\"][\"detection\"]\n    if det_metrics:\n        final_det = det_metrics[-1]\n        print(f\"  Detection AUC (vote): {final_det['auc_vote']:.4f}\")\n        print(f\"  Detection DES (vote): {final_det['DES_vote']:.4f}\")\n        print(f\"  Detection AUC (KL): {final_det['auc_kl']:.4f}\")\n        print(f\"  Detection DES (KL): {final_det['DES_kl']:.4f}\")\n\n    # Compute and print test accuracy\n    preds = results.get(\"predictions\", [])\n    gts = results.get(\"ground_truth\", [])\n    if preds and gts and len(preds) == len(gts):\n        accuracy = sum(p == g for p, g in zip(preds, gts)) / len(preds)\n        print(f\"  Test accuracy: {accuracy:.4f}\\n\")\n", "", "import os\nimport numpy as np\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_data = np.load(\n    os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n).item()\n\nfor ablation, datasets in experiment_data.items():\n    for name, result in datasets.items():\n        print(f\"Dataset: {name}\")\n\n        # Final training and validation loss\n        final_train_loss = result[\"losses\"][\"train\"][-1][\"loss\"]\n        final_val_loss = result[\"losses\"][\"val\"][-1][\"loss\"]\n        print(f\"training loss: {final_train_loss:.4f}\")\n        print(f\"validation loss: {final_val_loss:.4f}\")\n\n        # Detection metrics (final epoch)\n        det = result[\"metrics\"][\"detection\"][-1]\n        print(f\"detection AUC (voting): {det['auc_vote']:.4f}\")\n        print(f\"detection DES (voting): {det['DES_vote']:.4f}\")\n        print(f\"detection AUC (kl): {det['auc_kl']:.4f}\")\n        print(f\"detection DES (kl): {det['DES_kl']:.4f}\")\n\n        # Test accuracy\n        preds = np.array(result[\"predictions\"])\n        gt = np.array(result[\"ground_truth\"])\n        accuracy = np.mean(preds == gt)\n        print(f\"test accuracy: {accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract the positional embedding ablation results\nablation_results = experiment_data.get(\"positional_embedding_ablation\", {})\n\n# Iterate through each dataset and print the final metrics\nfor dataset_name, results in ablation_results.items():\n    # Losses and accuracies\n    train_losses = results[\"losses\"][\"train\"]\n    val_losses = results[\"losses\"][\"val\"]\n    train_accs = results[\"metrics\"][\"train\"]\n    val_accs = results[\"metrics\"][\"val\"]\n\n    # Detection metrics from the last epoch\n    last_detection = results[\"detection\"][-1]\n\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"Final training loss: {train_losses[-1]:.4f}\")\n    print(f\"Final validation loss: {val_losses[-1]:.4f}\")\n    print(f\"Final training accuracy: {train_accs[-1]:.4f}\")\n    print(f\"Final validation accuracy: {val_accs[-1]:.4f}\")\n    print(f\"Detection AUC (vote): {last_detection['auc_vote']:.4f}\")\n    print(f\"Detection DES score (vote): {last_detection['DES_vote']:.4f}\")\n    print(f\"Detection AUC (KL divergence): {last_detection['auc_kl']:.4f}\")\n    print(f\"Detection DES score (KL divergence): {last_detection['DES_kl']:.4f}\")\n    print()  # Blank line between datasets\n", "import os\nimport numpy as np\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over each dataset's results under No_Dropout_Ablation\nfor dataset_name, results in experiment_data[\"No_Dropout_Ablation\"].items():\n    print(f\"Dataset: {dataset_name}\")\n    # Extract final training and validation losses\n    final_train_loss = results[\"losses\"][\"train\"][-1][\"loss\"]\n    final_val_loss = results[\"losses\"][\"val\"][-1][\"loss\"]\n    # Extract final detection metrics\n    final_detection = results[\"metrics\"][\"detection\"][-1]\n    auc_vote = final_detection[\"auc_vote\"]\n    des_vote = final_detection[\"DES_vote\"]\n    auc_kl = final_detection[\"auc_kl\"]\n    des_kl = final_detection[\"DES_kl\"]\n\n    # Print metrics with clear labels\n    print(f\"final training loss: {final_train_loss:.4f}\")\n    print(f\"final validation loss: {final_val_loss:.4f}\")\n    print(f\"detection AUC (vote): {auc_vote:.4f}\")\n    print(f\"detection DES (vote): {des_vote:.4f}\")\n    print(f\"detection AUC (KL): {auc_kl:.4f}\")\n    print(f\"detection DES (KL): {des_kl:.4f}\")\n    print()  # Blank line between datasets\n", "", "", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nfile_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over experiments (full_heads and pruned_heads)\nfor exp_type, exp_results in experiment_data.items():\n    print(f\"Experiment: {exp_type}\")\n    # Iterate over each dataset within the experiment\n    for dataset_name, dataset_results in exp_results.items():\n        print(f\"Dataset: {dataset_name}\")\n        # Extract the final training and validation losses and accuracies\n        final_train_loss = dataset_results[\"losses\"][\"train\"][-1][\"loss\"]\n        final_train_acc = dataset_results[\"metrics\"][\"train\"][-1][\"acc\"]\n        final_val_loss = dataset_results[\"losses\"][\"val\"][-1][\"loss\"]\n        final_val_acc = dataset_results[\"metrics\"][\"val\"][-1][\"acc\"]\n        # Extract the final detection metrics\n        det = dataset_results[\"detection\"][-1]\n        auc_vote = det[\"auc_vote\"]\n        des_vote = det[\"DES_vote\"]\n        auc_kl = det[\"auc_kl\"]\n        des_kl = det[\"DES_kl\"]\n        # Print with clear labels\n        print(f\"Final training loss: {final_train_loss:.4f}\")\n        print(f\"Final training accuracy: {final_train_acc:.4f}\")\n        print(f\"Final validation loss: {final_val_loss:.4f}\")\n        print(f\"Final validation accuracy: {final_val_acc:.4f}\")\n        print(f\"Final detection AUC (vote): {auc_vote:.4f}\")\n        print(f\"Final detection DES (vote): {des_vote:.4f}\")\n        print(f\"Final detection AUC (kl): {auc_kl:.4f}\")\n        print(f\"Final detection DES (kl): {des_kl:.4f}\")\n        print()  # Blank line between datasets\n", "", "", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract and print final validation metrics for each dataset\nablation_results = experiment_data.get(\"token_type_embedding_ablation\", {})\nfor dataset_name, results in ablation_results.items():\n    val_metrics = results.get(\"metrics\", {}).get(\"val\", [])\n    if not val_metrics:\n        continue\n\n    # Take the metrics from the last epoch\n    final_metrics = val_metrics[-1]\n\n    # Print dataset name and each validation metric\n    print(dataset_name)\n    for metric, value in final_metrics.items():\n        if metric == \"epoch\":\n            continue\n        print(f\"validation {metric}: {value:.4f}\")\n", "import os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\nabl_names = list(experiment_data.keys())\n# Assume all ablations have the same dataset keys\ndataset_names = list(experiment_data[abl_names[0]].keys())\n\n# Iterate and print the final metrics\nfor dataset in dataset_names:\n    print(f\"Dataset: {dataset}\")\n    for abl in abl_names:\n        ed = experiment_data[abl][dataset]\n        # Final epoch training and validation losses\n        final_train_loss = ed[\"losses\"][\"train\"][-1][\"loss\"]\n        final_val_loss = ed[\"losses\"][\"val\"][-1][\"loss\"]\n        # Final detection metrics\n        final_det = ed[\"metrics\"][\"detection\"][-1]\n        auc_vote = final_det[\"auc_vote\"]\n        des_vote = final_det[\"DES_vote\"]\n        spe_vote = final_det[\"spearman_vote\"]\n        auc_kl = final_det[\"auc_kl\"]\n        des_kl = final_det[\"DES_kl\"]\n        spe_kl = final_det[\"spearman_kl\"]\n        # Compute test accuracy from stored predictions\n        y_true = ed[\"ground_truth\"]\n        y_pred = ed[\"predictions\"]\n        test_accuracy = accuracy_score(y_true, y_pred)\n\n        print(f\"  Ablation: {abl}\")\n        print(f\"    Final training loss:       {final_train_loss:.4f}\")\n        print(f\"    Final validation loss:     {final_val_loss:.4f}\")\n        print(f\"    Detection AUC (vote):      {auc_vote:.4f}\")\n        print(f\"    Detection DES (vote):      {des_vote:.4f}\")\n        print(f\"    Detection Spearman (vote): {spe_vote:.4f}\")\n        print(f\"    Detection AUC (KL):        {auc_kl:.4f}\")\n        print(f\"    Detection DES (KL):        {des_kl:.4f}\")\n        print(f\"    Detection Spearman (KL):   {spe_kl:.4f}\")\n        print(f\"    Test accuracy:             {test_accuracy:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Extract LayerNormAblation results\nresults = experiment_data.get(\"LayerNormAblation\", {})\n\nfor dataset_name, stats in results.items():\n    print(f\"{dataset_name}\")\n\n    # Final training and validation loss\n    train_losses = stats[\"losses\"][\"train\"]\n    val_losses = stats[\"losses\"][\"val\"]\n    final_train_loss = train_losses[-1][\"loss\"]\n    final_val_loss = val_losses[-1][\"loss\"]\n    print(f\"Final training loss: {final_train_loss:.4f}\")\n    print(f\"Final validation loss: {final_val_loss:.4f}\")\n\n    # Final detection metrics\n    detection = stats[\"metrics\"][\"detection\"][-1]\n    print(f\"Detection AUC (vote): {detection['auc_vote']:.4f}\")\n    print(f\"Detection DES (vote): {detection['DES_vote']:.4f}\")\n    print(f\"Detection AUC (KL divergence): {detection['auc_kl']:.4f}\")\n    print(f\"Detection DES (KL divergence): {detection['DES_kl']:.4f}\")\n\n    # Test accuracy from final predictions\n    preds = stats[\"predictions\"]\n    labels = stats[\"ground_truth\"]\n    accuracy = sum(p == l for p, l in zip(preds, labels)) / len(labels)\n    print(f\"Test accuracy: {accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# iterate through ablations and datasets, printing final metrics\nfor ablation_name, datasets in experiment_data.items():\n    for dataset_name, results in datasets.items():\n        print(f\"Ablation setting: {ablation_name}\")\n        print(f\"Dataset: {dataset_name}\")\n\n        # final training and validation loss\n        final_train = results[\"losses\"][\"train\"][-1][\"loss\"]\n        final_val = results[\"losses\"][\"val\"][-1][\"loss\"]\n        print(f\"Final training loss: {final_train:.4f}\")\n        print(f\"Final validation loss: {final_val:.4f}\")\n\n        # final detection metrics\n        final_det = results[\"metrics\"][\"detection\"][-1]\n        auc_vote = final_det[\"auc_vote\"]\n        des_vote = final_det[\"DES_vote\"]\n        auc_kl = final_det[\"auc_kl\"]\n        des_kl = final_det[\"DES_kl\"]\n        print(f\"Final detection AUC (vote): {auc_vote:.4f}\")\n        print(f\"Final detection DES (vote): {des_vote:.4f}\")\n        print(f\"Final detection AUC (KL): {auc_kl:.4f}\")\n        print(f\"Final detection DES (KL): {des_kl:.4f}\")\n\n        print()  # blank line for readability between datasets\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, exp in experiment_data.items():\n    print(dataset_name)\n\n    # Final training loss\n    train_losses = exp.get(\"losses\", {}).get(\"train\", [])\n    if train_losses:\n        final_train_loss = train_losses[-1].get(\"loss\", None)\n        if final_train_loss is not None:\n            print(f\"train loss: {final_train_loss:.4f}\")\n\n    # Final validation loss\n    val_losses = exp.get(\"losses\", {}).get(\"val\", [])\n    if val_losses:\n        final_val_loss = val_losses[-1].get(\"loss\", None)\n        if final_val_loss is not None:\n            print(f\"validation loss: {final_val_loss:.4f}\")\n\n    # Final uncertainty metrics (vote and KL)\n    val_metrics = exp.get(\"metrics\", {}).get(\"val\", [])\n    if val_metrics:\n        last_metrics = val_metrics[-1]\n        metric_labels = [\n            (\"AUC (vote)\", \"auc_vote\"),\n            (\"DES (vote)\", \"DES_vote\"),\n            (\"AUC (KL)\", \"auc_kl\"),\n            (\"DES (KL)\", \"DES_kl\"),\n            (\"Spearman (vote)\", \"spearman_vote\"),\n            (\"Spearman (KL)\", \"spearman_kl\"),\n        ]\n        for label, key in metric_labels:\n            value = last_metrics.get(key, None)\n            if value is not None:\n                print(f\"{label}: {value:.4f}\")\n\n    print()\n", "import os\nimport numpy as np\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each ablation and each dataset\nfor ablation, datasets in experiment_data.items():\n    for dataset_name, stats in datasets.items():\n        # Extract final epoch metrics\n        train_loss = stats[\"losses\"][\"train\"][-1]\n        val_loss = stats[\"losses\"][\"val\"][-1]\n        final_metrics = stats[\"metrics\"][\"val\"][-1]\n\n        # Print results\n        print(f\"Dataset: {dataset_name}\")\n        print(f\"Train set loss: {train_loss:.4f}\")\n        print(f\"Validation set loss: {val_loss:.4f}\")\n        print(f\"ROC AUC (vote uncertainty): {final_metrics['auc_vote']:.4f}\")\n        print(\n            f\"Detection Efficiency Score (vote uncertainty): {final_metrics['DES_vote']:.4f}\"\n        )\n        print(f\"ROC AUC (KL uncertainty): {final_metrics['auc_kl']:.4f}\")\n        print(\n            f\"Detection Efficiency Score (KL uncertainty): {final_metrics['DES_kl']:.4f}\"\n        )\n        print()\n", "import os\nimport numpy as np\n\n# Load the saved experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each ablation setting and each dataset to print final metrics\nfor ablation, datasets in experiment_data.items():\n    print(f\"\\nAblation: {ablation}\")\n    for dataset_name, data in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        # Extract final training and validation losses\n        final_train = data[\"losses\"][\"train\"][-1][\"loss\"]\n        final_val = data[\"losses\"][\"val\"][-1][\"loss\"]\n        print(f\"training loss: {final_train:.4f}\")\n        print(f\"validation loss: {final_val:.4f}\")\n        # Extract final detection metrics\n        det = data[\"metrics\"][\"detection\"][-1]\n        print(f\"detection AUC (vote): {det['auc_vote']:.4f}\")\n        print(f\"detection DES (vote): {det['DES_vote']:.4f}\")\n        print(f\"detection AUC (kl): {det['auc_kl']:.4f}\")\n        print(f\"detection DES (kl): {det['DES_kl']:.4f}\")\n", "import os\nimport numpy as np\n\n# locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# iterate through ablations and datasets, printing final metrics\nfor ablation_name, datasets in experiment_data.items():\n    for dataset_name, results in datasets.items():\n        print(f\"Ablation setting: {ablation_name}\")\n        print(f\"Dataset: {dataset_name}\")\n\n        # final training and validation loss\n        final_train = results[\"losses\"][\"train\"][-1][\"loss\"]\n        final_val = results[\"losses\"][\"val\"][-1][\"loss\"]\n        print(f\"Final training loss: {final_train:.4f}\")\n        print(f\"Final validation loss: {final_val:.4f}\")\n\n        # final detection metrics\n        final_det = results[\"metrics\"][\"detection\"][-1]\n        auc_vote = final_det[\"auc_vote\"]\n        des_vote = final_det[\"DES_vote\"]\n        auc_kl = final_det[\"auc_kl\"]\n        des_kl = final_det[\"DES_kl\"]\n        print(f\"Final detection AUC (vote): {auc_vote:.4f}\")\n        print(f\"Final detection DES (vote): {des_vote:.4f}\")\n        print(f\"Final detection AUC (KL): {auc_kl:.4f}\")\n        print(f\"Final detection DES (KL): {des_kl:.4f}\")\n\n        print()  # blank line for readability between datasets\n", "import os\nimport numpy as np\n\n# locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# iterate through ablations and datasets, printing final metrics\nfor ablation_name, datasets in experiment_data.items():\n    for dataset_name, results in datasets.items():\n        print(f\"Ablation setting: {ablation_name}\")\n        print(f\"Dataset: {dataset_name}\")\n\n        # final training and validation loss\n        final_train = results[\"losses\"][\"train\"][-1][\"loss\"]\n        final_val = results[\"losses\"][\"val\"][-1][\"loss\"]\n        print(f\"Final training loss: {final_train:.4f}\")\n        print(f\"Final validation loss: {final_val:.4f}\")\n\n        # final detection metrics\n        final_det = results[\"metrics\"][\"detection\"][-1]\n        auc_vote = final_det[\"auc_vote\"]\n        des_vote = final_det[\"DES_vote\"]\n        auc_kl = final_det[\"auc_kl\"]\n        des_kl = final_det[\"DES_kl\"]\n        print(f\"Final detection AUC (vote): {auc_vote:.4f}\")\n        print(f\"Final detection DES (vote): {des_vote:.4f}\")\n        print(f\"Final detection AUC (KL): {auc_kl:.4f}\")\n        print(f\"Final detection DES (KL): {des_kl:.4f}\")\n\n        print()  # blank line for readability between datasets\n", "import os\nimport numpy as np\n\n# locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# iterate through ablations and datasets, printing final metrics\nfor ablation_name, datasets in experiment_data.items():\n    for dataset_name, results in datasets.items():\n        print(f\"Ablation setting: {ablation_name}\")\n        print(f\"Dataset: {dataset_name}\")\n\n        # final training and validation loss\n        final_train = results[\"losses\"][\"train\"][-1][\"loss\"]\n        final_val = results[\"losses\"][\"val\"][-1][\"loss\"]\n        print(f\"Final training loss: {final_train:.4f}\")\n        print(f\"Final validation loss: {final_val:.4f}\")\n\n        # final detection metrics\n        final_det = results[\"metrics\"][\"detection\"][-1]\n        auc_vote = final_det[\"auc_vote\"]\n        des_vote = final_det[\"DES_vote\"]\n        auc_kl = final_det[\"auc_kl\"]\n        des_kl = final_det[\"DES_kl\"]\n        print(f\"Final detection AUC (vote): {auc_vote:.4f}\")\n        print(f\"Final detection DES (vote): {des_vote:.4f}\")\n        print(f\"Final detection AUC (KL): {auc_kl:.4f}\")\n        print(f\"Final detection DES (KL): {des_kl:.4f}\")\n\n        print()  # blank line for readability between datasets\n", ""], "parse_term_out": ["['Dataset: sst2', '\\n', 'train loss: 0.1094', '\\n', 'validation loss: 0.4326',\n'\\n', 'detection AUC: 0.6332', '\\n', 'detection DES: 0.1583', '\\n', 'validation\naccuracy: 0.8350\\n', '\\n', 'Dataset: yelp_polarity', '\\n', 'train loss: 0.0549',\n'\\n', 'validation loss: 0.3306', '\\n', 'detection AUC: 0.5486', '\\n', 'detection\nDES: 0.1372', '\\n', 'validation accuracy: 0.8850\\n', '\\n', 'Dataset: imdb',\n'\\n', 'train loss: 0.1021', '\\n', 'validation loss: 0.4118', '\\n', 'detection\nAUC: 0.5090', '\\n', 'detection DES: 0.1273', '\\n', 'validation accuracy:\n0.8600\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: sst2', '\\n', '  Final training loss: 0.6830', '\\n', '  Final\nvalidation loss: 0.6846', '\\n', '  Detection AUC (vote): 0.5145', '\\n', '\nDetection DES (vote): 0.0857', '\\n', '  Detection AUC (KL): 0.5176', '\\n', '\nDetection DES (KL): 0.0863', '\\n', '  Test accuracy: 0.5340\\n', '\\n', 'Dataset:\nyelp_polarity', '\\n', '  Final training loss: 0.6736', '\\n', '  Final validation\nloss: 0.6693', '\\n', '  Detection AUC (vote): 0.5314', '\\n', '  Detection DES\n(vote): 0.0886', '\\n', '  Detection AUC (KL): 0.5257', '\\n', '  Detection DES\n(KL): 0.0876', '\\n', '  Test accuracy: 0.6120\\n', '\\n', 'Dataset: imdb', '\\n', '\nFinal training loss: 0.6807', '\\n', '  Final validation loss: 0.6797', '\\n', '\nDetection AUC (vote): 0.5218', '\\n', '  Detection DES (vote): 0.0870', '\\n', '\nDetection AUC (KL): 0.5111', '\\n', '  Detection DES (KL): 0.0852', '\\n', '  Test\naccuracy: 0.5860\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "", "['Dataset: sst2', '\\n', 'training loss: 0.2254', '\\n', 'validation loss:\n0.6035', '\\n', 'detection AUC (voting): 0.5366', '\\n', 'detection DES (voting):\n0.0894', '\\n', 'detection AUC (kl): 0.5369', '\\n', 'detection DES (kl): 0.0895',\n'\\n', 'test accuracy: 0.7280\\n', '\\n', 'Dataset: yelp_polarity', '\\n', 'training\nloss: 0.1953', '\\n', 'validation loss: 0.2818', '\\n', 'detection AUC (voting):\n0.6478', '\\n', 'detection DES (voting): 0.1080', '\\n', 'detection AUC (kl):\n0.8605', '\\n', 'detection DES (kl): 0.1434', '\\n', 'test accuracy: 0.8840\\n',\n'\\n', 'Dataset: imdb', '\\n', 'training loss: 0.6333', '\\n', 'validation loss:\n0.5007', '\\n', 'detection AUC (voting): 0.5064', '\\n', 'detection DES (voting):\n0.0844', '\\n', 'detection AUC (kl): 0.6649', '\\n', 'detection DES (kl): 0.1108',\n'\\n', 'test accuracy: 0.7780\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "['Dataset: sst2', '\\n', 'Final training loss: 0.0442', '\\n', 'Final validation\nloss: 0.8616', '\\n', 'Final training accuracy: 0.9918', '\\n', 'Final validation\naccuracy: 0.7660', '\\n', 'Detection AUC (vote): 0.5541', '\\n', 'Detection DES\nscore (vote): 0.0924', '\\n', 'Detection AUC (KL divergence): 0.6655', '\\n',\n'Detection DES score (KL divergence): 0.1109', '\\n', '\\n', 'Dataset:\nyelp_polarity', '\\n', 'Final training loss: 0.0619', '\\n', 'Final validation\nloss: 0.3378', '\\n', 'Final training accuracy: 0.9900', '\\n', 'Final validation\naccuracy: 0.8960', '\\n', 'Detection AUC (vote): 0.6230', '\\n', 'Detection DES\nscore (vote): 0.1038', '\\n', 'Detection AUC (KL divergence): 0.8437', '\\n',\n'Detection DES score (KL divergence): 0.1406', '\\n', '\\n', 'Dataset: imdb',\n'\\n', 'Final training loss: 0.1096', '\\n', 'Final validation loss: 0.4163',\n'\\n', 'Final training accuracy: 0.9848', '\\n', 'Final validation accuracy:\n0.8860', '\\n', 'Detection AUC (vote): 0.5436', '\\n', 'Detection DES score\n(vote): 0.0906', '\\n', 'Detection AUC (KL divergence): 0.7663', '\\n', 'Detection\nDES score (KL divergence): 0.1277', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: sst2', '\\n', 'final training loss: 0.0131', '\\n', 'final validation\nloss: 0.5191', '\\n', 'detection AUC (vote): 0.6403', '\\n', 'detection DES\n(vote): 0.1067', '\\n', 'detection AUC (KL): 0.7488', '\\n', 'detection DES (KL):\n0.1248', '\\n', '\\n', 'Dataset: yelp_polarity', '\\n', 'final training loss:\n0.0099', '\\n', 'final validation loss: 0.2328', '\\n', 'detection AUC (vote):\n0.6066', '\\n', 'detection DES (vote): 0.1011', '\\n', 'detection AUC (KL):\n0.8730', '\\n', 'detection DES (KL): 0.1455', '\\n', '\\n', 'Dataset: imdb', '\\n',\n'final training loss: 0.0178', '\\n', 'final validation loss: 0.3398', '\\n',\n'detection AUC (vote): 0.6107', '\\n', 'detection DES (vote): 0.1018', '\\n',\n'detection AUC (KL): 0.8708', '\\n', 'detection DES (KL): 0.1451', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "", "", "['Experiment: full_heads', '\\n', 'Dataset: sst2', '\\n', 'Final training loss:\n0.0328', '\\n', 'Final training accuracy: 0.9914', '\\n', 'Final validation loss:\n0.4328', '\\n', 'Final validation accuracy: 0.9020', '\\n', 'Final detection AUC\n(vote): 0.6570', '\\n', 'Final detection DES (vote): 0.1095', '\\n', 'Final\ndetection AUC (kl): 0.7789', '\\n', 'Final detection DES (kl): 0.1298', '\\n',\n'\\n', 'Dataset: yelp_polarity', '\\n', 'Final training loss: 0.0168', '\\n',\n'Final training accuracy: 0.9948', '\\n', 'Final validation loss: 0.2148', '\\n',\n'Final validation accuracy: 0.9500', '\\n', 'Final detection AUC (vote): 0.6080',\n'\\n', 'Final detection DES (vote): 0.1013', '\\n', 'Final detection AUC (kl):\n0.8731', '\\n', 'Final detection DES (kl): 0.1455', '\\n', '\\n', 'Dataset: imdb',\n'\\n', 'Final training loss: 0.0307', '\\n', 'Final training accuracy: 0.9940',\n'\\n', 'Final validation loss: 0.3369', '\\n', 'Final validation accuracy:\n0.9160', '\\n', 'Final detection AUC (vote): 0.6008', '\\n', 'Final detection DES\n(vote): 0.1001', '\\n', 'Final detection AUC (kl): 0.8672', '\\n', 'Final\ndetection DES (kl): 0.1445', '\\n', '\\n', 'Experiment: pruned_heads', '\\n',\n'Dataset: sst2', '\\n', 'Final training loss: 0.0409', '\\n', 'Final training\naccuracy: 0.9888', '\\n', 'Final validation loss: 0.4355', '\\n', 'Final\nvalidation accuracy: 0.8840', '\\n', 'Final detection AUC (vote): 0.6112', '\\n',\n'Final detection DES (vote): 0.1019', '\\n', 'Final detection AUC (kl): 0.7330',\n'\\n', 'Final detection DES (kl): 0.1222', '\\n', '\\n', 'Dataset: yelp_polarity',\n'\\n', 'Final training loss: 0.0185', '\\n', 'Final training accuracy: 0.9940',\n'\\n', 'Final validation loss: 0.2304', '\\n', 'Final validation accuracy:\n0.9300', '\\n', 'Final detection AUC (vote): 0.6157', '\\n', 'Final detection DES\n(vote): 0.1026', '\\n', 'Final detection AUC (kl): 0.8892', '\\n', 'Final\ndetection DES (kl): 0.1482', '\\n', '\\n', 'Dataset: imdb', '\\n', 'Final training\nloss: 0.0419', '\\n', 'Final training accuracy: 0.9862', '\\n', 'Final validation\nloss: 0.3156', '\\n', 'Final validation accuracy: 0.9080', '\\n', 'Final detection\nAUC (vote): 0.6072', '\\n', 'Final detection DES (vote): 0.1012', '\\n', 'Final\ndetection AUC (kl): 0.8555', '\\n', 'Final detection DES (kl): 0.1426', '\\n',\n'\\n', 'Execution time: a moment seconds (time limit is an hour).']", "", "", "['sst2', '\\n', 'validation auc_vote: 0.6040', '\\n', 'validation DES_vote:\n0.1007', '\\n', 'validation auc_kl: 0.7203', '\\n', 'validation DES_kl: 0.1201',\n'\\n', 'yelp_polarity', '\\n', 'validation auc_vote: 0.6531', '\\n', 'validation\nDES_vote: 0.1089', '\\n', 'validation auc_kl: 0.8726', '\\n', 'validation DES_kl:\n0.1454', '\\n', 'imdb', '\\n', 'validation auc_vote: 0.5622', '\\n', 'validation\nDES_vote: 0.0937', '\\n', 'validation auc_kl: 0.8288', '\\n', 'validation DES_kl:\n0.1381', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: sst2', '\\n', '  Ablation: full_ft', '\\n', '    Final training loss:\n0.1478', '\\n', '    Final validation loss:     0.3059', '\\n', '    Detection AUC\n(vote):      0.5695', '\\n', '    Detection DES (vote):      0.1424', '\\n', '\nDetection Spearman (vote): 0.1417', '\\n', '    Detection AUC (KL):\n0.7494', '\\n', '    Detection DES (KL):        0.1873', '\\n', '    Detection\nSpearman (KL):   0.2703', '\\n', '    Test accuracy:             0.8900', '\\n', '\nAblation: freeze_4', '\\n', '    Final training loss:       0.2011', '\\n', '\nFinal validation loss:     0.2836', '\\n', '    Detection AUC (vote):\n0.5983', '\\n', '    Detection DES (vote):      0.1496', '\\n', '    Detection\nSpearman (vote): 0.1657', '\\n', '    Detection AUC (KL):        0.7318', '\\n', '\nDetection DES (KL):        0.1829', '\\n', '    Detection Spearman (KL):\n0.2354', '\\n', '    Test accuracy:             0.9050', '\\n', '  Ablation:\nfreeze_8', '\\n', '    Final training loss:       0.2704', '\\n', '    Final\nvalidation loss:     0.2618', '\\n', '    Detection AUC (vote):      0.7360',\n'\\n', '    Detection DES (vote):      0.1840', '\\n', '    Detection Spearman\n(vote): 0.4007', '\\n', '    Detection AUC (KL):        0.7508', '\\n', '\nDetection DES (KL):        0.1877', '\\n', '    Detection Spearman (KL):\n0.2486', '\\n', '    Test accuracy:             0.9100', '\\n', '\\n', 'Dataset:\nyelp_polarity', '\\n', '  Ablation: full_ft', '\\n', '    Final training loss:\n0.1068', '\\n', '    Final validation loss:     0.2590', '\\n', '    Detection AUC\n(vote):      0.5561', '\\n', '    Detection DES (vote):      0.1390', '\\n', '\nDetection Spearman (vote): 0.1544', '\\n', '    Detection AUC (KL):\n0.8039', '\\n', '    Detection DES (KL):        0.2010', '\\n', '    Detection\nSpearman (KL):   0.3158', '\\n', '    Test accuracy:             0.9000', '\\n', '\nAblation: freeze_4', '\\n', '    Final training loss:       0.1095', '\\n', '\nFinal validation loss:     0.2071', '\\n', '    Detection AUC (vote):\n0.5404', '\\n', '    Detection DES (vote):      0.1351', '\\n', '    Detection\nSpearman (vote): 0.1006', '\\n', '    Detection AUC (KL):        0.8271', '\\n', '\nDetection DES (KL):        0.2068', '\\n', '    Detection Spearman (KL):\n0.3074', '\\n', '    Test accuracy:             0.9200', '\\n', '  Ablation:\nfreeze_8', '\\n', '    Final training loss:       0.1438', '\\n', '    Final\nvalidation loss:     0.2179', '\\n', '    Detection AUC (vote):      0.5510',\n'\\n', '    Detection DES (vote):      0.1377', '\\n', '    Detection Spearman\n(vote): 0.1620', '\\n', '    Detection AUC (KL):        0.7867', '\\n', '\nDetection DES (KL):        0.1967', '\\n', '    Detection Spearman (KL):\n0.2694', '\\n', '    Test accuracy:             0.9200', '\\n', '\\n', 'Dataset:\nimdb', '\\n', '  Ablation: full_ft', '\\n', '    Final training loss:\n0.1815', '\\n', '    Final validation loss:     0.2574', '\\n', '    Detection AUC\n(vote):      0.5658', '\\n', '    Detection DES (vote):      0.1415', '\\n', '\nDetection Spearman (vote): 0.2586', '\\n', '    Detection AUC (KL):\n0.7978', '\\n', '    Detection DES (KL):        0.1995', '\\n', '    Detection\nSpearman (KL):   0.3163', '\\n', '    Test accuracy:             0.8950', '\\n', '\nAblation: freeze_4', '\\n', '    Final training loss:       0.1848', '\\n', '\nFinal validation loss:     0.2676', '\\n', '    Detection AUC (vote):\n0.5421', '\\n', '    Detection DES (vote):      0.1355', '\\n', '    Detection\nSpearman (vote): 0.1617', '\\n', '    Detection AUC (KL):        0.7828', '\\n', '\nDetection DES (KL):        0.1957', '\\n', '    Detection Spearman (KL):\n0.2939', '\\n', '    Test accuracy:             0.9000', '\\n', '  Ablation:\nfreeze_8', '\\n', '    Final training loss:       0.2122', '\\n', '    Final\nvalidation loss:     0.2933', '\\n', '    Detection AUC (vote):      0.5474',\n'\\n', '    Detection DES (vote):      0.1368', '\\n', '    Detection Spearman\n(vote): 0.1985', '\\n', '    Detection AUC (KL):        0.7584', '\\n', '\nDetection DES (KL):        0.1896', '\\n', '    Detection Spearman (KL):\n0.2624', '\\n', '    Test accuracy:             0.9050', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['sst2', '\\n', 'Final training loss: nan', '\\n', 'Final validation loss: nan',\n'\\n', 'Detection AUC (vote): 0.5000', '\\n', 'Detection DES (vote): 0.0833',\n'\\n', 'Detection AUC (KL divergence): 0.5000', '\\n', 'Detection DES (KL\ndivergence): 0.0833', '\\n', 'Test accuracy: 0.4780\\n', '\\n', 'yelp_polarity',\n'\\n', 'Final training loss: nan', '\\n', 'Final validation loss: nan', '\\n',\n'Detection AUC (vote): 0.5000', '\\n', 'Detection DES (vote): 0.0833', '\\n',\n'Detection AUC (KL divergence): 0.5000', '\\n', 'Detection DES (KL divergence):\n0.0833', '\\n', 'Test accuracy: 0.5080\\n', '\\n', 'imdb', '\\n', 'Final training\nloss: nan', '\\n', 'Final validation loss: nan', '\\n', 'Detection AUC (vote):\n0.5000', '\\n', 'Detection DES (vote): 0.0833', '\\n', 'Detection AUC (KL\ndivergence): 0.5000', '\\n', 'Detection DES (KL divergence): 0.0833', '\\n', 'Test\naccuracy: 0.5080\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Ablation setting: baseline', '\\n', 'Dataset: sst2', '\\n', 'Final training\nloss: 0.0289', '\\n', 'Final validation loss: 0.4289', '\\n', 'Final detection AUC\n(vote): 0.7028', '\\n', 'Final detection DES (vote): 0.1171', '\\n', 'Final\ndetection AUC (KL): 0.7853', '\\n', 'Final detection DES (KL): 0.1309', '\\n',\n'\\n', 'Ablation setting: baseline', '\\n', 'Dataset: yelp_polarity', '\\n', 'Final\ntraining loss: 0.0140', '\\n', 'Final validation loss: 0.2778', '\\n', 'Final\ndetection AUC (vote): 0.5924', '\\n', 'Final detection DES (vote): 0.0987', '\\n',\n'Final detection AUC (KL): 0.8450', '\\n', 'Final detection DES (KL): 0.1408',\n'\\n', '\\n', 'Ablation setting: baseline', '\\n', 'Dataset: imdb', '\\n', 'Final\ntraining loss: 0.0433', '\\n', 'Final validation loss: 0.3045', '\\n', 'Final\ndetection AUC (vote): 0.5539', '\\n', 'Final detection DES (vote): 0.0923', '\\n',\n'Final detection AUC (KL): 0.8859', '\\n', 'Final detection DES (KL): 0.1477',\n'\\n', '\\n', 'Ablation setting: RandomInitEmbeddingAblation', '\\n', 'Dataset:\nsst2', '\\n', 'Final training loss: 0.3545', '\\n', 'Final validation loss:\n0.5888', '\\n', 'Final detection AUC (vote): 0.5907', '\\n', 'Final detection DES\n(vote): 0.0984', '\\n', 'Final detection AUC (KL): 0.5658', '\\n', 'Final\ndetection DES (KL): 0.0943', '\\n', '\\n', 'Ablation setting:\nRandomInitEmbeddingAblation', '\\n', 'Dataset: yelp_polarity', '\\n', 'Final\ntraining loss: 0.2336', '\\n', 'Final validation loss: 0.2977', '\\n', 'Final\ndetection AUC (vote): 0.5981', '\\n', 'Final detection DES (vote): 0.0997', '\\n',\n'Final detection AUC (KL): 0.8172', '\\n', 'Final detection DES (KL): 0.1362',\n'\\n', '\\n', 'Ablation setting: RandomInitEmbeddingAblation', '\\n', 'Dataset:\nimdb', '\\n', 'Final training loss: 0.6810', '\\n', 'Final validation loss:\n0.7161', '\\n', 'Final detection AUC (vote): 0.4982', '\\n', 'Final detection DES\n(vote): 0.0830', '\\n', 'Final detection AUC (KL): 0.6443', '\\n', 'Final\ndetection DES (KL): 0.1074', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "['sst2', '\\n', 'train loss: 0.6921', '\\n', 'validation loss: 0.6902', '\\n', 'AUC\n(vote): 0.4955', '\\n', 'DES (vote): 0.0826', '\\n', 'AUC (KL): 0.5151', '\\n',\n'DES (KL): 0.0859', '\\n', 'Spearman (vote): -0.0282', '\\n', 'Spearman (KL):\n0.0261', '\\n', '\\n', 'yelp_polarity', '\\n', 'train loss: 0.6987', '\\n',\n'validation loss: 0.6919', '\\n', 'AUC (vote): 0.4960', '\\n', 'DES (vote):\n0.0827', '\\n', 'AUC (KL): 0.4432', '\\n', 'DES (KL): 0.0739', '\\n', 'Spearman\n(vote): -0.0316', '\\n', 'Spearman (KL): -0.0985', '\\n', '\\n', 'imdb', '\\n',\n'train loss: 0.6967', '\\n', 'validation loss: 0.6621', '\\n', 'AUC (vote):\n0.5096', '\\n', 'DES (vote): 0.0849', '\\n', 'AUC (KL): 0.5993', '\\n', 'DES (KL):\n0.0999', '\\n', 'Spearman (vote): 0.0342', '\\n', 'Spearman (KL): 0.1610', '\\n',\n'\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: sst2', '\\n', 'Train set loss: 0.0358', '\\n', 'Validation set loss:\n0.4356', '\\n', 'ROC AUC (vote uncertainty): 0.6750', '\\n', 'Detection Efficiency\nScore (vote uncertainty): 0.1125', '\\n', 'ROC AUC (KL uncertainty): 0.7675',\n'\\n', 'Detection Efficiency Score (KL uncertainty): 0.1279', '\\n', '\\n',\n'Dataset: yelp_polarity', '\\n', 'Train set loss: 0.0156', '\\n', 'Validation set\nloss: 0.2583', '\\n', 'ROC AUC (vote uncertainty): 0.5815', '\\n', 'Detection\nEfficiency Score (vote uncertainty): 0.0969', '\\n', 'ROC AUC (KL uncertainty):\n0.8475', '\\n', 'Detection Efficiency Score (KL uncertainty): 0.1413', '\\n',\n'\\n', 'Dataset: imdb', '\\n', 'Train set loss: 0.0368', '\\n', 'Validation set\nloss: 0.3050', '\\n', 'ROC AUC (vote uncertainty): 0.5465', '\\n', 'Detection\nEfficiency Score (vote uncertainty): 0.0911', '\\n', 'ROC AUC (KL uncertainty):\n0.8187', '\\n', 'Detection Efficiency Score (KL uncertainty): 0.1365', '\\n',\n'\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['\\nAblation: full_depth', '\\n', 'Dataset: sst2', '\\n', 'training loss: 0.0308',\n'\\n', 'validation loss: 0.4367', '\\n', 'detection AUC (vote): 0.6565', '\\n',\n'detection DES (vote): 0.1094', '\\n', 'detection AUC (kl): 0.7603', '\\n',\n'detection DES (kl): 0.1267', '\\n', 'Dataset: yelp_polarity', '\\n', 'training\nloss: 0.0215', '\\n', 'validation loss: 0.1385', '\\n', 'detection AUC (vote):\n0.5283', '\\n', 'detection DES (vote): 0.0881', '\\n', 'detection AUC (kl):\n0.8630', '\\n', 'detection DES (kl): 0.1438', '\\n', 'Dataset: imdb', '\\n',\n'training loss: 0.0467', '\\n', 'validation loss: 0.3104', '\\n', 'detection AUC\n(vote): 0.6011', '\\n', 'detection DES (vote): 0.1002', '\\n', 'detection AUC\n(kl): 0.8298', '\\n', 'detection DES (kl): 0.1383', '\\n', '\\nAblation:\nreduced_depth', '\\n', 'Dataset: sst2', '\\n', 'training loss: 0.0335', '\\n',\n'validation loss: 0.6043', '\\n', 'detection AUC (vote): 0.5924', '\\n',\n'detection DES (vote): 0.0987', '\\n', 'detection AUC (kl): 0.6697', '\\n',\n'detection DES (kl): 0.1116', '\\n', 'Dataset: yelp_polarity', '\\n', 'training\nloss: 0.0154', '\\n', 'validation loss: 0.3684', '\\n', 'detection AUC (vote):\n0.5817', '\\n', 'detection DES (vote): 0.0970', '\\n', 'detection AUC (kl):\n0.8311', '\\n', 'detection DES (kl): 0.1385', '\\n', 'Dataset: imdb', '\\n',\n'training loss: 0.0406', '\\n', 'validation loss: 0.3892', '\\n', 'detection AUC\n(vote): 0.5288', '\\n', 'detection DES (vote): 0.0881', '\\n', 'detection AUC\n(kl): 0.7998', '\\n', 'detection DES (kl): 0.1333', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['Ablation setting: baseline', '\\n', 'Dataset: sst2', '\\n', 'Final training\nloss: 0.0412', '\\n', 'Final validation loss: 0.4025', '\\n', 'Final detection AUC\n(vote): 0.6366', '\\n', 'Final detection DES (vote): 0.1061', '\\n', 'Final\ndetection AUC (KL): 0.7814', '\\n', 'Final detection DES (KL): 0.1302', '\\n',\n'\\n', 'Ablation setting: baseline', '\\n', 'Dataset: yelp_polarity', '\\n', 'Final\ntraining loss: 0.0229', '\\n', 'Final validation loss: 0.2170', '\\n', 'Final\ndetection AUC (vote): 0.6740', '\\n', 'Final detection DES (vote): 0.1123', '\\n',\n'Final detection AUC (KL): 0.9113', '\\n', 'Final detection DES (KL): 0.1519',\n'\\n', '\\n', 'Ablation setting: baseline', '\\n', 'Dataset: imdb', '\\n', 'Final\ntraining loss: 0.0340', '\\n', 'Final validation loss: 0.3762', '\\n', 'Final\ndetection AUC (vote): 0.6110', '\\n', 'Final detection DES (vote): 0.1018', '\\n',\n'Final detection AUC (KL): 0.8697', '\\n', 'Final detection DES (KL): 0.1450',\n'\\n', '\\n', 'Ablation setting: RandomInitEmbeddingAblation', '\\n', 'Dataset:\nsst2', '\\n', 'Final training loss: 0.4704', '\\n', 'Final validation loss:\n0.5445', '\\n', 'Final detection AUC (vote): 0.5530', '\\n', 'Final detection DES\n(vote): 0.0922', '\\n', 'Final detection AUC (KL): 0.4713', '\\n', 'Final\ndetection DES (KL): 0.0785', '\\n', '\\n', 'Ablation setting:\nRandomInitEmbeddingAblation', '\\n', 'Dataset: yelp_polarity', '\\n', 'Final\ntraining loss: 0.3136', '\\n', 'Final validation loss: 0.3373', '\\n', 'Final\ndetection AUC (vote): 0.5877', '\\n', 'Final detection DES (vote): 0.0980', '\\n',\n'Final detection AUC (KL): 0.7982', '\\n', 'Final detection DES (KL): 0.1330',\n'\\n', '\\n', 'Ablation setting: RandomInitEmbeddingAblation', '\\n', 'Dataset:\nimdb', '\\n', 'Final training loss: 0.6659', '\\n', 'Final validation loss:\n0.7406', '\\n', 'Final detection AUC (vote): 0.4981', '\\n', 'Final detection DES\n(vote): 0.0830', '\\n', 'Final detection AUC (KL): 0.5542', '\\n', 'Final\ndetection DES (KL): 0.0924', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "['Ablation setting: baseline', '\\n', 'Dataset: sst2', '\\n', 'Final training\nloss: 0.0296', '\\n', 'Final validation loss: 0.4039', '\\n', 'Final detection AUC\n(vote): 0.6226', '\\n', 'Final detection DES (vote): 0.1038', '\\n', 'Final\ndetection AUC (KL): 0.7438', '\\n', 'Final detection DES (KL): 0.1240', '\\n',\n'\\n', 'Ablation setting: baseline', '\\n', 'Dataset: yelp_polarity', '\\n', 'Final\ntraining loss: 0.0115', '\\n', 'Final validation loss: 0.2119', '\\n', 'Final\ndetection AUC (vote): 0.5377', '\\n', 'Final detection DES (vote): 0.0896', '\\n',\n'Final detection AUC (KL): 0.8864', '\\n', 'Final detection DES (KL): 0.1477',\n'\\n', '\\n', 'Ablation setting: baseline', '\\n', 'Dataset: imdb', '\\n', 'Final\ntraining loss: 0.0258', '\\n', 'Final validation loss: 0.2904', '\\n', 'Final\ndetection AUC (vote): 0.5898', '\\n', 'Final detection DES (vote): 0.0983', '\\n',\n'Final detection AUC (KL): 0.8667', '\\n', 'Final detection DES (KL): 0.1445',\n'\\n', '\\n', 'Ablation setting: RandomInitEmbeddingAblation', '\\n', 'Dataset:\nsst2', '\\n', 'Final training loss: 0.2927', '\\n', 'Final validation loss:\n0.5848', '\\n', 'Final detection AUC (vote): 0.5552', '\\n', 'Final detection DES\n(vote): 0.0925', '\\n', 'Final detection AUC (KL): 0.5921', '\\n', 'Final\ndetection DES (KL): 0.0987', '\\n', '\\n', 'Ablation setting:\nRandomInitEmbeddingAblation', '\\n', 'Dataset: yelp_polarity', '\\n', 'Final\ntraining loss: 0.2164', '\\n', 'Final validation loss: 0.3757', '\\n', 'Final\ndetection AUC (vote): 0.6034', '\\n', 'Final detection DES (vote): 0.1006', '\\n',\n'Final detection AUC (KL): 0.7771', '\\n', 'Final detection DES (KL): 0.1295',\n'\\n', '\\n', 'Ablation setting: RandomInitEmbeddingAblation', '\\n', 'Dataset:\nimdb', '\\n', 'Final training loss: 0.6939', '\\n', 'Final validation loss:\n0.6934', '\\n', 'Final detection AUC (vote): 0.5000', '\\n', 'Final detection DES\n(vote): 0.0833', '\\n', 'Final detection AUC (KL): 0.5595', '\\n', 'Final\ndetection DES (KL): 0.0932', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "['Ablation setting: baseline', '\\n', 'Dataset: sst2', '\\n', 'Final training\nloss: 0.0400', '\\n', 'Final validation loss: 0.4144', '\\n', 'Final detection AUC\n(vote): 0.6204', '\\n', 'Final detection DES (vote): 0.1034', '\\n', 'Final\ndetection AUC (KL): 0.7501', '\\n', 'Final detection DES (KL): 0.1250', '\\n',\n'\\n', 'Ablation setting: baseline', '\\n', 'Dataset: yelp_polarity', '\\n', 'Final\ntraining loss: 0.0135', '\\n', 'Final validation loss: 0.2539', '\\n', 'Final\ndetection AUC (vote): 0.5424', '\\n', 'Final detection DES (vote): 0.0904', '\\n',\n'Final detection AUC (KL): 0.8371', '\\n', 'Final detection DES (KL): 0.1395',\n'\\n', '\\n', 'Ablation setting: baseline', '\\n', 'Dataset: imdb', '\\n', 'Final\ntraining loss: 0.0418', '\\n', 'Final validation loss: 0.2781', '\\n', 'Final\ndetection AUC (vote): 0.5365', '\\n', 'Final detection DES (vote): 0.0894', '\\n',\n'Final detection AUC (KL): 0.8592', '\\n', 'Final detection DES (KL): 0.1432',\n'\\n', '\\n', 'Ablation setting: RandomInitEmbeddingAblation', '\\n', 'Dataset:\nsst2', '\\n', 'Final training loss: 0.2963', '\\n', 'Final validation loss:\n0.6895', '\\n', 'Final detection AUC (vote): 0.5474', '\\n', 'Final detection DES\n(vote): 0.0912', '\\n', 'Final detection AUC (KL): 0.5631', '\\n', 'Final\ndetection DES (KL): 0.0939', '\\n', '\\n', 'Ablation setting:\nRandomInitEmbeddingAblation', '\\n', 'Dataset: yelp_polarity', '\\n', 'Final\ntraining loss: 0.2285', '\\n', 'Final validation loss: 0.2719', '\\n', 'Final\ndetection AUC (vote): 0.6232', '\\n', 'Final detection DES (vote): 0.1039', '\\n',\n'Final detection AUC (KL): 0.8480', '\\n', 'Final detection DES (KL): 0.1413',\n'\\n', '\\n', 'Ablation setting: RandomInitEmbeddingAblation', '\\n', 'Dataset:\nimdb', '\\n', 'Final training loss: 0.6939', '\\n', 'Final validation loss:\n0.6929', '\\n', 'Final detection AUC (vote): 0.5000', '\\n', 'Final detection DES\n(vote): 0.0833', '\\n', 'Final detection AUC (KL): 0.5142', '\\n', 'Final\ndetection DES (KL): 0.0857', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"]}
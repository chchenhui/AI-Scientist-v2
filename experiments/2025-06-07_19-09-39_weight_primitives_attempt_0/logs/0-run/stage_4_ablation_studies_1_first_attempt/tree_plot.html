<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 10], [0, 13], [0, 16], [0, 11], [0, 1], [0, 14], [0, 2], [0, 7], [0, 12], [0, 5], [0, 9], [0, 4], [0, 3], [0, 8], [0, 17], [0, 18], [0, 6], [7, 22], [7, 20], [7, 19], [7, 21], [12, 15]], "layout": [[0.5, 0.0], [0.0, 0.5], [0.0625, 0.5], [0.125, 0.5], [0.1875, 0.5], [0.25, 0.5], [0.3125, 0.5], [0.375, 0.5], [0.4375, 0.5], [0.5, 0.5], [0.5625, 0.5], [0.625, 0.5], [0.6875, 0.5], [0.75, 0.5], [0.8125, 0.5], [0.6875, 1.0], [0.875, 0.5], [0.9375, 0.5], [1.0, 0.5], [0.28125, 1.0], [0.34375, 1.0], [0.40625, 1.0], [0.46875, 1.0]], "plan": ["Hyperparam tuning name: adam_beta1. We perform a grid search over \u03b2\u2081 values\n[0.5, 0.7, 0.9, 0.99], reinitializing the dictionary and codes each time, and\ntraining with the corresponding Adam optimizer.  For each run we record\nper\u2010epoch train/val reconstruction error and sparse loss, then compute final\ntest reconstructions via pseudo\u2010inverse.  These metrics, losses, final\npredictions, and ground truth are stored under\nexperiment_data['adam_beta1']['synthetic'], with lists over hyperparameter runs.\nAt the end the entire experiment_data dict is saved to\nworking/experiment_data.npy via np.save.  The script is self\u2010contained and\nexecutable as\u2010is.", "Ablation name: Multi-Synthetic-Dataset Robustness. We define three synthetic\ndataset configurations that vary seed, noise, and sparsity, then for each we\ngenerate train/test data and rerun the Adam(beta1) grid search. During training\nwe record epoch\u2010wise reconstruction errors and losses on both splits, and\ncompute final test predictions. All metrics, losses, predictions, ground truths,\nand dataset parameters are stored in a nested `experiment_data` dict under\n\u201cmulti_synthetic\u201d and finally saved to \u201cexperiment_data.npy\u201d.", "Ablation name: Dictionary Capacity Ablation. I will implement a loop over the\nthree dictionary sizes (10, 30, 60), regenerate the synthetic data for each\nsize, and then run the same training loop over the \u03b21 values.  We collect\nreconstruction error, sparsity loss, and final predictions for each\nconfiguration and store them in a single `experiment_data` dict under the\n`\"dictionary_capacity\"` ablation.  Finally, we save this dict to\n`working/experiment_data.npy`.  All random seeds are reset for reproducibility.", "Ablation name: Sparsity Regularization Strength Ablation. We sweep the sparsity\nregularization strength lambda1 over a grid, retraining the model for each value\nand logging per-epoch train/val reconstruction errors, losses, code sparsity\n(fraction of near-zero codes), and dictionary recovery error (relative Frobenius\nnorm to ground truth). After each run we compute final test predictions and\nstore them along with ground truth. All results are collected in a single\n`experiment_data` dict under the key `\"sparsity_strength_ablation\"` and saved as\n`experiment_data.npy`.", "Ablation name: Dictionary Atom Norm Projection Ablation. I will project each\ndictionary atom to unit \u21132 norm after every optimizer update and record\ntrain/val errors and losses across different Adam beta1 settings. All results\nfor the \u201catom_norm_projection\u201d ablation on the synthetic dataset are stored in\n`experiment_data` and saved to `experiment_data.npy`.", "Ablation name: Synthetic Noise Level Ablation. I will loop over the specified\nnoise levels, regenerating noisy observations and re-initializing the model with\na fixed seed for each \u03c3 to ensure consistent initialization. For each noise\nregime I train the dictionary and sparse codes, record train/validation errors\nand losses per epoch, and store the final test reconstructions together with\nground truth. After all runs I stack the results into NumPy arrays under a\nnested `experiment_data` dict and save everything as `experiment_data.npy`.", "Ablation name: Optimizer Choice Ablation. I will extend the base script to loop\nover three optimizer choices (SGD with momentum, RMSprop, and AdamW) instead of\nAdam, reinitializing model parameters for each and holding data and\nhyperparameters constant. For each optimizer I run the same training loop,\ncompute train/validation errors and losses, store them along with final\npredictions and ground truths into an experiment_data dictionary. Finally I save\nthis dictionary to working/experiment_data.npy for later plotting.", "Ablation name: Alternating Minimization Frequency Ablation. Here we implement a\nscript that generates synthetic data and runs an alternating minimization\nfrequency ablation over various code\u2010to\u2010dictionary update ratios. We detach one\nset of variables while updating the other with separate Adam optimizers to\nenforce the schedule. At each epoch we record train/validation errors and\nlosses, then store final test predictions and ground truth under\n`experiment_data['alt_min_freq']['synthetic']`. Finally we save all results in\n`experiment_data.npy` in the working directory.", "Ablation name: Initialization Scheme Ablation. We define four initialization\nfunctions (normal, Xavier uniform, orthogonal, and zero) and iterate over all\ncombinations for the dictionary and code matrices. For each pairing, we\nreinitialize `D` and `codes_train`, run the joint optimization, and record\ntraining/validation errors, losses, predictions, ground truth, and the init\nschemes used. All results are gathered into an `experiment_data` dict under the\n`\"initialization\"` ablation, then saved as `experiment_data.npy` for downstream\nanalysis.", "Ablation name: Mini-Batch Size Ablation. Below is a self-contained script that\ngenerates synthetic data, runs dictionary learning with Adam using various\nminibatch sizes (including full batch), records train/val errors and losses each\nepoch, stores final test reconstructions and ground truths, and saves everything\nto `experiment_data.npy`.", "Ablation name: Sparsity Regularization Type Ablation. This code sets up four\nablation conditions\u2014no penalty, L1, L2, and Elastic Net\u2014applies each to the same\nsynthetic dataset with fixed hyperparameters, and trains a dictionary model via\nAdam. For each penalty type, it reinitializes the dictionary and codes, runs the\noptimization loop, and records reconstruction errors, losses, and final\nreconstructions on train and test data. All plottable outputs (metrics, losses,\npredictions, ground truth) are stored in a nested dictionary and saved as\n`experiment_data.npy` in a `working` folder for later analysis.", "Ablation name: Dictionary Orthogonality Regularization Ablation. Below is a\nsingle\u2010file Python script that sweeps over different orthogonality penalty\nweights \u03bb\u2082, adds the term \u03bb\u2082\u00b7\u2016D\u2009D\u1d40\u2212I\u2016\u00b2 to the loss, re\u2010initializes D and sparse\ncodes for each run, trains on synthetic data, logs train/val errors and losses\nper epoch along with test predictions, and finally saves the structured results\nunder `experiment_data.npy`.", "Ablation name: Test-Time Inference Method Ablation. We implement sparse test-\ntime solvers (ISTA, FISTA, OMP) alongside the Moore\u2013Penrose pseudoinverse and\ncompare their reconstruction performance. We first train the dictionary and\nsparse codes on the synthetic training set with a fixed Adam optimizer. During\ntraining we log the training error and also evaluate each solver on the held-out\ntest set at each epoch, capturing validation errors and losses. After training,\nwe reconstruct the test signals with each solver and assemble a results\ndictionary under \u201ctest_time_solver_ablation\u201d containing solver names, metrics,\nlosses, final predictions, and ground truth. Finally, we save this structure to\n\u201cexperiment_data.npy\u201d.", "Ablation name: Synthetic Code Distribution Ablation. We define four sparse code\ngenerators\u2014Bernoulli\u2010Gaussian, Bernoulli\u2010Laplace, Bernoulli\u2010Uniform, and\nblock\u2010sparse\u2014then loop over each distribution to synthesize data with a shared\nground\u2010truth dictionary. For each code type, we train the same\ndictionary\u2010learning model across a grid of Adam beta1 values, logging train/val\nreconstruction errors and losses. Finally, all metrics, losses, predictions, and\nground truths are organized in a nested experiment_data dict and saved to\n\"experiment_data.npy\".", "Ablation name: Reconstruction Loss Function Ablation. Below is a single\u2010file\nscript that performs the reconstruction\u2010loss ablation by swapping in MSE, MAE,\nand Huber losses while keeping all other settings fixed (including a grid of\nAdam \u03b21 values). It records training/validation errors and losses, final\npredictions and ground truth into `experiment_data` under keys for each loss,\nand saves everything as `experiment_data.npy` in a `working` directory.", "In the OMP implementation, the pseudoinverse was being taken on a matrix of size\n(k\u00d7dim), which then did not align with the dim\u00d71 weight vector and caused a\nshape mismatch. To fix this, we transpose the selected atoms D_S to get a dim\u00d7k\nmatrix, compute its pseudoinverse (k\u00d7dim), and then multiply by the weight\nvector (dim), yielding a coefficient vector of length k. This ensures correct\ndimensions for the matrix\u2013vector multiply and resolves the runtime error. Below\nis the full, fixed code.", "Ablation name: Learning Rate Scheduling Ablation. We define four learning\u2010rate\nstrategies\u2014fixed, step\u2010decay, exponential decay, and cosine annealing\u2014using\nPyTorch schedulers and loop over each while reinitializing the model and codes\nunder a fixed seed. During each run, we step the scheduler after every optimizer\nupdate (for non\u2010fixed schedules) and record train/val reconstruction errors and\nlosses at each epoch. After training, we compute final test predictions via the\ndictionary\u2019s pseudo\u2010inverse, store metrics, losses, predictions, ground truth,\nand the schedule names in a structured dict under \u201clr_schedules,\u201d and then save\neverything to `experiment_data.npy` in the working directory.", "Ablation name: Noise Distribution Ablation. We generate a fixed ground\u2010truth\ndictionary and sparse codes, then for each noise distribution (Gaussian,\nLaplace, Cauchy) we add appropriately sampled noise at the same scale to produce\nsynthetic data. For each noise type, we train the dictionary and code inference\nacross multiple Adam beta1 settings, recording reconstruction errors, losses,\nand final predictions. All metrics, losses, predictions, and ground truths are\nstored in a nested dictionary keyed by noise type and saved as\nexperiment_data.npy at the end.", "Ablation name: Data Dimensionality Ablation. Below is a single\u2010file Python\nscript that performs the data\u2010dimensionality ablation on synthetic sparse\ndictionary learning. It loops over dims = [64,256,1024,4096], generates data,\nfits D and sparse codes with a fixed Adam(\u03b21=0.9,\u03b22=0.999), tracks train/val\nerrors and losses, stores predictions and ground truth, and finally saves\neverything in `experiment_data.npy`. You can run it as\u2010is.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data parameters\nn_samples = 80\nn_test = 20\nn_components = 30\ndim = 1024\nlambda1 = 1e-2\nlr = 1e-2\nepochs = 50\n\n# generate ground truth and data\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\nW_all = codes0.mm(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\nW_train = W_all[:n_samples]\nW_test = W_all[n_samples:]\n\n# hyperparameter grid\nbeta1_list = [0.5, 0.7, 0.9, 0.99]\n\n# prepare experiment data structure\nexperiment_data = {\n    \"adam_beta1\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor b1 in beta1_list:\n    # reinit model params\n    torch.manual_seed(0)\n    D = nn.Parameter(torch.randn_like(D0))\n    codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n    optimizer = torch.optim.Adam([D, codes_train], lr=lr, betas=(b1, 0.999))\n    train_errs, val_errs = [], []\n    train_losses, val_losses = [], []\n    # training loop\n    for epoch in range(1, epochs + 1):\n        optimizer.zero_grad()\n        W_hat = codes_train.mm(D)\n        loss_recon = ((W_hat - W_train) ** 2).mean()\n        loss_sparse = lambda1 * codes_train.abs().mean()\n        loss = loss_recon + loss_sparse\n        loss.backward()\n        optimizer.step()\n        # compute metrics\n        with torch.no_grad():\n            tr_err = ((W_hat - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n            D_pinv = torch.pinverse(D)\n            codes_test = W_test.mm(D_pinv)\n            W_hat_test = codes_test.mm(D)\n            vl_err = (\n                ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1)).mean().item()\n            )\n            tr_loss = loss_recon.item()\n            vl_loss = ((W_hat_test - W_test) ** 2).mean().item()\n        train_errs.append(tr_err)\n        val_errs.append(vl_err)\n        train_losses.append(tr_loss)\n        val_losses.append(vl_loss)\n    # final predictions\n    with torch.no_grad():\n        D_pinv = torch.pinverse(D)\n        W_hat_test = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n    # store results\n    ed = experiment_data[\"adam_beta1\"][\"synthetic\"]\n    ed[\"metrics\"][\"train\"].append(train_errs)\n    ed[\"metrics\"][\"val\"].append(val_errs)\n    ed[\"losses\"][\"train\"].append(train_losses)\n    ed[\"losses\"][\"val\"].append(val_losses)\n    ed[\"predictions\"].append(W_hat_test)\n    ed[\"ground_truth\"].append(W_test.cpu().numpy())\n    print(f\"Finished run for beta1={b1}\")\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Synthetic and training parameters\nn_samples, n_test = 80, 20\nn_components, dim = 30, 1024\nlambda1, lr, epochs = 1e-2, 1e-2, 50\nbeta1_list = [0.5, 0.7, 0.9, 0.99]\n\n# Dataset configurations for ablation\ndatasets = {\n    \"ds1\": {\"seed\": 0, \"noise\": 0.01, \"sparsity\": 0.1},\n    \"ds2\": {\"seed\": 1, \"noise\": 0.05, \"sparsity\": 0.2},\n    \"ds3\": {\"seed\": 2, \"noise\": 0.1, \"sparsity\": 0.3},\n}\n\n# Initialize experiment storage\nexperiment_data = {\"multi_synthetic\": {}}\n\nfor name, cfg in datasets.items():\n    # prepare storage\n    experiment_data[\"multi_synthetic\"][name] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"params\": cfg,\n    }\n    # generate synthetic data\n    torch.manual_seed(cfg[\"seed\"])\n    D0 = torch.randn(n_components, dim, device=device)\n    codes0 = (\n        torch.rand(n_samples + n_test, n_components, device=device) < cfg[\"sparsity\"]\n    ).float() * torch.randn(n_samples + n_test, n_components, device=device)\n    W_all = codes0.mm(D0) + cfg[\"noise\"] * torch.randn(\n        n_samples + n_test, dim, device=device\n    )\n    W_train, W_test = W_all[:n_samples], W_all[n_samples:]\n    # grid search over beta1 values\n    for b1 in beta1_list:\n        torch.manual_seed(0)\n        D = nn.Parameter(torch.randn_like(D0))\n        codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n        optimizer = torch.optim.Adam([D, codes_train], lr=lr, betas=(b1, 0.999))\n        train_errs, val_errs = [], []\n        train_losses, val_losses = [], []\n        for epoch in range(1, epochs + 1):\n            optimizer.zero_grad()\n            W_hat = codes_train.mm(D)\n            loss_recon = ((W_hat - W_train) ** 2).mean()\n            loss_sparse = lambda1 * codes_train.abs().mean()\n            (loss_recon + loss_sparse).backward()\n            optimizer.step()\n            with torch.no_grad():\n                tr_err = (\n                    ((W_hat - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n                )\n                D_pinv = torch.pinverse(D)\n                codes_test = W_test.mm(D_pinv)\n                W_hat_test = codes_test.mm(D)\n                vl_err = (\n                    ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1))\n                    .mean()\n                    .item()\n                )\n                train_errs.append(tr_err)\n                val_errs.append(vl_err)\n                train_losses.append(loss_recon.item())\n                val_losses.append(((W_hat_test - W_test) ** 2).mean().item())\n        # compute final predictions\n        with torch.no_grad():\n            D_pinv = torch.pinverse(D)\n            final_pred = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n        # store results\n        ed = experiment_data[\"multi_synthetic\"][name]\n        ed[\"metrics\"][\"train\"].append(train_errs)\n        ed[\"metrics\"][\"val\"].append(val_errs)\n        ed[\"losses\"][\"train\"].append(train_losses)\n        ed[\"losses\"][\"val\"].append(val_losses)\n        ed[\"predictions\"].append(final_pred)\n        ed[\"ground_truth\"].append(W_test.cpu().numpy())\n        print(f\"Finished dataset={name}, beta1={b1}\")\n\n# save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# fixed hyperparameters\nn_samples = 80\nn_test = 20\ndim = 1024\nlambda1 = 1e-2\nlr = 1e-2\nepochs = 50\n\n# ablation settings\nn_components_list = [10, 30, 60]\nbeta1_list = [0.5, 0.7, 0.9, 0.99]\n\n# prepare experiment data structure\nexperiment_data = {\n    \"dictionary_capacity\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"n_components_list\": n_components_list,\n            \"beta1_list\": beta1_list,\n        }\n    }\n}\n\nfor n_components in n_components_list:\n    # generate synthetic data for this dictionary size\n    torch.manual_seed(0)\n    D0 = torch.randn(n_components, dim, device=device)\n    codes0 = (\n        torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n    ).float() * torch.randn(n_samples + n_test, n_components, device=device)\n    W_all = codes0.mm(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\n    W_train = W_all[:n_samples]\n    W_test = W_all[n_samples:]\n\n    for b1 in beta1_list:\n        # reinitialize model parameters\n        torch.manual_seed(0)\n        D = nn.Parameter(torch.randn_like(D0))\n        codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n        optimizer = torch.optim.Adam([D, codes_train], lr=lr, betas=(b1, 0.999))\n\n        train_errs, val_errs = [], []\n        train_losses, val_losses = [], []\n\n        # training loop\n        for epoch in range(1, epochs + 1):\n            optimizer.zero_grad()\n            W_hat = codes_train.mm(D)\n            loss_recon = ((W_hat - W_train) ** 2).mean()\n            loss_sparse = lambda1 * codes_train.abs().mean()\n            loss = loss_recon + loss_sparse\n            loss.backward()\n            optimizer.step()\n\n            with torch.no_grad():\n                tr_err = (\n                    ((W_hat - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n                )\n                D_pinv = torch.pinverse(D)\n                codes_test = W_test.mm(D_pinv)\n                W_hat_test = codes_test.mm(D)\n                vl_err = (\n                    ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1))\n                    .mean()\n                    .item()\n                )\n                train_errs.append(tr_err)\n                val_errs.append(vl_err)\n                train_losses.append(loss_recon.item())\n                val_losses.append(((W_hat_test - W_test) ** 2).mean().item())\n\n        # final test reconstruction\n        with torch.no_grad():\n            D_pinv = torch.pinverse(D)\n            W_hat_test = W_test.mm(D_pinv).mm(D)\n            W_hat_np = W_hat_test.cpu().numpy()\n            W_true_np = W_test.cpu().numpy()\n\n        # store results\n        ed = experiment_data[\"dictionary_capacity\"][\"synthetic\"]\n        ed[\"metrics\"][\"train\"].append(train_errs)\n        ed[\"metrics\"][\"val\"].append(val_errs)\n        ed[\"losses\"][\"train\"].append(train_losses)\n        ed[\"losses\"][\"val\"].append(val_losses)\n        ed[\"predictions\"].append(W_hat_np)\n        ed[\"ground_truth\"].append(W_true_np)\n\n        print(f\"Finished n_components={n_components}, beta1={b1}\")\n\n# save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data parameters\nn_samples = 80\nn_test = 20\nn_components = 30\ndim = 1024\nlr = 1e-2\nepochs = 50\n\n# generate ground truth and data\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\nW_all = codes0.mm(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\nW_train = W_all[:n_samples]\nW_test = W_all[n_samples:]\n\n# sparsity regularization grid\nlambda1_list = [0.0, 1e-4, 1e-3, 1e-2, 1e-1]\n\n# prepare experiment data structure\nexperiment_data = {\n    \"sparsity_strength_ablation\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"sparsity\": [],  # fraction of near-zero codes\n            \"dict_error\": [],  # relative dictionary recovery error\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor lam in lambda1_list:\n    torch.manual_seed(0)\n    # initialize parameters\n    D = nn.Parameter(torch.randn_like(D0))\n    codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n    optimizer = torch.optim.Adam([D, codes_train], lr=lr)\n    train_errs, val_errs = [], []\n    train_losses, val_losses = [], []\n    sparsities, dict_errors = [], []\n\n    for epoch in range(1, epochs + 1):\n        optimizer.zero_grad()\n        W_hat = codes_train.mm(D)\n        loss_recon = ((W_hat - W_train) ** 2).mean()\n        loss_sparse = lam * codes_train.abs().mean()\n        loss = loss_recon + loss_sparse\n        loss.backward()\n        optimizer.step()\n\n        with torch.no_grad():\n            # reconstruction errors\n            tr_err = ((W_hat - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n            D_pinv = torch.pinverse(D)\n            codes_test = W_test.mm(D_pinv)\n            W_hat_test = codes_test.mm(D)\n            vl_err = (\n                ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1)).mean().item()\n            )\n            # losses (reconstruction only)\n            tr_loss = loss_recon.item()\n            vl_loss = ((W_hat_test - W_test) ** 2).mean().item()\n            # code sparsity\n            sparsity = (codes_train.detach().abs() < 1e-3).float().mean().item()\n            # dictionary recovery error\n            dict_err = (D.detach() - D0).norm() / D0.norm()\n            dict_err = dict_err.item()\n\n        train_errs.append(tr_err)\n        val_errs.append(vl_err)\n        train_losses.append(tr_loss)\n        val_losses.append(vl_loss)\n        sparsities.append(sparsity)\n        dict_errors.append(dict_err)\n\n    # final test predictions\n    with torch.no_grad():\n        D_pinv = torch.pinverse(D)\n        W_hat_test = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n    # store\n    ed = experiment_data[\"sparsity_strength_ablation\"][\"synthetic\"]\n    ed[\"metrics\"][\"train\"].append(train_errs)\n    ed[\"metrics\"][\"val\"].append(val_errs)\n    ed[\"losses\"][\"train\"].append(train_losses)\n    ed[\"losses\"][\"val\"].append(val_losses)\n    ed[\"sparsity\"].append(sparsities)\n    ed[\"dict_error\"].append(dict_errors)\n    ed[\"predictions\"].append(W_hat_test)\n    ed[\"ground_truth\"].append(W_test.cpu().numpy())\n    print(f\"Finished run for lambda1={lam}\")\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data parameters\nn_samples = 80\nn_test = 20\nn_components = 30\ndim = 1024\nlambda1 = 1e-2\nlr = 1e-2\nepochs = 50\nbeta1_list = [0.5, 0.7, 0.9, 0.99]\n\n# generate ground truth and data\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\nW_all = codes0.mm(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\nW_train = W_all[:n_samples]\nW_test = W_all[n_samples:]\n\n# prepare experiment data structure for the atom norm projection ablation\nexperiment_data = {\n    \"atom_norm_projection\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor b1 in beta1_list:\n    torch.manual_seed(0)\n    D = nn.Parameter(torch.randn_like(D0))\n    codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n    optimizer = torch.optim.Adam([D, codes_train], lr=lr, betas=(b1, 0.999))\n\n    train_errs, val_errs = [], []\n    train_losses, val_losses = [], []\n\n    for epoch in range(1, epochs + 1):\n        optimizer.zero_grad()\n        W_hat = codes_train.mm(D)\n        loss_recon = ((W_hat - W_train) ** 2).mean()\n        loss_sparse = lambda1 * codes_train.abs().mean()\n        loss = loss_recon + loss_sparse\n        loss.backward()\n        optimizer.step()\n\n        # project each atom (row) of D to unit \u21132 norm\n        with torch.no_grad():\n            norms = D.data.norm(p=2, dim=1, keepdim=True)\n            D.data.div_(norms + 1e-12)\n\n        # compute metrics\n        with torch.no_grad():\n            W_hat = codes_train.mm(D)\n            tr_err = ((W_hat - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n            D_pinv = torch.pinverse(D)\n            codes_test = W_test.mm(D_pinv)\n            W_hat_test = codes_test.mm(D)\n            vl_err = (\n                ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1)).mean().item()\n            )\n\n            train_errs.append(tr_err)\n            val_errs.append(vl_err)\n            train_losses.append(loss_recon.item())\n            val_losses.append(((W_hat_test - W_test) ** 2).mean().item())\n\n    # final test predictions\n    with torch.no_grad():\n        D_pinv = torch.pinverse(D)\n        W_hat_test = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n\n    # store results\n    ed = experiment_data[\"atom_norm_projection\"][\"synthetic\"]\n    ed[\"metrics\"][\"train\"].append(train_errs)\n    ed[\"metrics\"][\"val\"].append(val_errs)\n    ed[\"losses\"][\"train\"].append(train_losses)\n    ed[\"losses\"][\"val\"].append(val_losses)\n    ed[\"predictions\"].append(W_hat_test)\n    ed[\"ground_truth\"].append(W_test.cpu().numpy())\n\n    print(f\"Finished run for beta1={b1}\")\n\n# save all data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# single\u2010file script\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data parameters\nn_samples = 80\nn_test = 20\nn_components = 30\ndim = 1024\nlambda1 = 1e-2\nlr = 1e-2\nepochs = 50\n\n# generate ground-truth dictionary and codes\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\n\n# ablation: noise levels\nnoise_levels = [0.0, 0.005, 0.01, 0.02, 0.05]\n\n# storage lists\nmetrics_train_list = []\nmetrics_val_list = []\nlosses_train_list = []\nlosses_val_list = []\npredictions_list = []\nground_truth_list = []\nnoise_list = []\n\nfor sigma in noise_levels:\n    # regenerate noisy observations\n    torch.manual_seed(0)\n    noise = torch.randn(n_samples + n_test, dim, device=device)\n    W_all = codes0.mm(D0) + sigma * noise\n    W_train = W_all[:n_samples]\n    W_test = W_all[n_samples:]\n\n    # reinit model parameters\n    torch.manual_seed(0)\n    D = nn.Parameter(torch.randn_like(D0))\n    codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n    optimizer = torch.optim.Adam([D, codes_train], lr=lr)  # default betas=(0.9,0.999)\n\n    train_errs, val_errs = [], []\n    train_losses, val_losses = [], []\n\n    # training loop\n    for epoch in range(1, epochs + 1):\n        optimizer.zero_grad()\n        W_hat = codes_train.mm(D)\n        loss_recon = ((W_hat - W_train) ** 2).mean()\n        loss_sparse = lambda1 * codes_train.abs().mean()\n        loss = loss_recon + loss_sparse\n        loss.backward()\n        optimizer.step()\n\n        with torch.no_grad():\n            # train error\n            tr_err = ((W_hat - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n            # validation error\n            D_pinv = torch.pinverse(D)\n            codes_test = W_test.mm(D_pinv)\n            W_hat_test = codes_test.mm(D)\n            vl_err = (\n                ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1)).mean().item()\n            )\n            # losses\n            tr_loss = loss_recon.item()\n            vl_loss = ((W_hat_test - W_test) ** 2).mean().item()\n\n        train_errs.append(tr_err)\n        val_errs.append(vl_err)\n        train_losses.append(tr_loss)\n        val_losses.append(vl_loss)\n\n    # final test reconstruction\n    with torch.no_grad():\n        D_pinv = torch.pinverse(D)\n        W_hat_final = W_test.mm(D_pinv).mm(D)\n\n    # store results\n    metrics_train_list.append(train_errs)\n    metrics_val_list.append(val_errs)\n    losses_train_list.append(train_losses)\n    losses_val_list.append(val_losses)\n    predictions_list.append(W_hat_final.cpu().numpy())\n    ground_truth_list.append(W_test.cpu().numpy())\n    noise_list.append(sigma)\n\n    print(f\"Finished run for noise \u03c3={sigma}\")\n\n# convert to numpy arrays\nmetrics_train_arr = np.array(metrics_train_list)  # shape (len(sigmas), epochs)\nmetrics_val_arr = np.array(metrics_val_list)\nlosses_train_arr = np.array(losses_train_list)\nlosses_val_arr = np.array(losses_val_list)\npredictions_arr = np.stack(predictions_list)  # shape (len(sigmas), n_test, dim)\nground_truth_arr = np.stack(ground_truth_list)\nnoise_arr = np.array(noise_list)\n\n# compile experiment data\nexperiment_data = {\n    \"synthetic_noise\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": metrics_train_arr, \"val\": metrics_val_arr},\n            \"losses\": {\"train\": losses_train_arr, \"val\": losses_val_arr},\n            \"predictions\": predictions_arr,\n            \"ground_truth\": ground_truth_arr,\n            \"noise_levels\": noise_arr,\n        }\n    }\n}\n\n# save to file\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data parameters\nn_samples = 80\nn_test = 20\nn_components = 30\ndim = 1024\nlambda1 = 1e-2\nlr = 1e-2\nepochs = 50\n\n# generate ground truth and data\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\nW_all = codes0.mm(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\nW_train = W_all[:n_samples]\nW_test = W_all[n_samples:]\n\n# prepare experiment data structure\nexperiment_data = {\n    \"optimizer_choice\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"optimizers\": [],\n        }\n    }\n}\n\n# list of optimizers to compare\noptimizer_list = [\"SGD\", \"RMSprop\", \"AdamW\"]\n\nfor opt_name in optimizer_list:\n    # reinit model params\n    torch.manual_seed(0)\n    D = nn.Parameter(torch.randn_like(D0))\n    codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n    if opt_name == \"SGD\":\n        optimizer = torch.optim.SGD([D, codes_train], lr=lr, momentum=0.9)\n    elif opt_name == \"RMSprop\":\n        optimizer = torch.optim.RMSprop([D, codes_train], lr=lr)\n    elif opt_name == \"AdamW\":\n        optimizer = torch.optim.AdamW([D, codes_train], lr=lr)\n    else:\n        raise ValueError(f\"Unknown optimizer {opt_name}\")\n\n    train_errs, val_errs = [], []\n    train_losses, val_losses = [], []\n\n    # training loop\n    for epoch in range(1, epochs + 1):\n        optimizer.zero_grad()\n        W_hat = codes_train.mm(D)\n        loss_recon = ((W_hat - W_train) ** 2).mean()\n        loss_sparse = lambda1 * codes_train.abs().mean()\n        loss = loss_recon + loss_sparse\n        loss.backward()\n        optimizer.step()\n\n        # compute metrics\n        with torch.no_grad():\n            tr_err = ((W_hat - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n            D_pinv = torch.pinverse(D)\n            codes_test = W_test.mm(D_pinv)\n            W_hat_test = codes_test.mm(D)\n            vl_err = (\n                ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1)).mean().item()\n            )\n            train_errs.append(tr_err)\n            val_errs.append(vl_err)\n            train_losses.append(loss_recon.item())\n            val_losses.append(((W_hat_test - W_test) ** 2).mean().item())\n\n    # final predictions\n    with torch.no_grad():\n        D_pinv = torch.pinverse(D)\n        W_hat_test = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n\n    # store results\n    ed = experiment_data[\"optimizer_choice\"][\"synthetic\"]\n    ed[\"metrics\"][\"train\"].append(train_errs)\n    ed[\"metrics\"][\"val\"].append(val_errs)\n    ed[\"losses\"][\"train\"].append(train_losses)\n    ed[\"losses\"][\"val\"].append(val_losses)\n    ed[\"predictions\"].append(W_hat_test)\n    ed[\"ground_truth\"].append(W_test.cpu().numpy())\n    ed[\"optimizers\"].append(opt_name)\n    print(f\"Finished run for optimizer={opt_name}\")\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data parameters\nn_samples = 80\nn_test = 20\nn_components = 30\ndim = 1024\nlambda1 = 1e-2\nlr = 1e-2\nepochs = 50\n\n# generate ground truth and data\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\nW_all = codes0.mm(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\nW_train = W_all[:n_samples]\nW_test = W_all[n_samples:]\n\n# ablation schedules: (code_updates, dict_updates)\nratio_pairs = [(1, 1), (5, 1), (10, 1), (1, 5), (1, 10)]\n\n# prepare experiment data structure\nexperiment_data = {\n    \"alt_min_freq\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"ratios\": [],\n        }\n    }\n}\n\nfor c_steps, d_steps in ratio_pairs:\n    torch.manual_seed(0)\n    # initialize parameters\n    D = nn.Parameter(torch.randn_like(D0))\n    codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n    # separate optimizers\n    opt_codes = torch.optim.Adam([codes_train], lr=lr, betas=(0.9, 0.999))\n    opt_D = torch.optim.Adam([D], lr=lr, betas=(0.9, 0.999))\n    # recorders\n    train_errs, val_errs = [], []\n    train_losses, val_losses = [], []\n    # training loop\n    for epoch in range(1, epochs + 1):\n        # code updates\n        for _ in range(c_steps):\n            opt_codes.zero_grad()\n            W_hat = codes_train.mm(D.detach())\n            loss_recon = ((W_hat - W_train) ** 2).mean()\n            loss_sparse = lambda1 * codes_train.abs().mean()\n            (loss_recon + loss_sparse).backward()\n            opt_codes.step()\n        # dictionary updates\n        for _ in range(d_steps):\n            opt_D.zero_grad()\n            W_hat = codes_train.detach().mm(D)\n            loss_recon = ((W_hat - W_train) ** 2).mean()\n            loss_sparse = lambda1 * codes_train.abs().mean()\n            (loss_recon + loss_sparse).backward()\n            opt_D.step()\n        # compute metrics\n        with torch.no_grad():\n            W_hat_tr = codes_train.mm(D)\n            tr_err = (\n                ((W_hat_tr - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n            )\n            loss_tr = ((W_hat_tr - W_train) ** 2).mean().item()\n            D_pinv = torch.pinverse(D)\n            codes_test = W_test.mm(D_pinv)\n            W_hat_test = codes_test.mm(D)\n            vl_err = (\n                ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1)).mean().item()\n            )\n            loss_val = ((W_hat_test - W_test) ** 2).mean().item()\n        train_errs.append(tr_err)\n        val_errs.append(vl_err)\n        train_losses.append(loss_tr)\n        val_losses.append(loss_val)\n    # final predictions\n    with torch.no_grad():\n        D_pinv = torch.pinverse(D)\n        W_hat_test = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n    # store results\n    ed = experiment_data[\"alt_min_freq\"][\"synthetic\"]\n    ed[\"metrics\"][\"train\"].append(train_errs)\n    ed[\"metrics\"][\"val\"].append(val_errs)\n    ed[\"losses\"][\"train\"].append(train_losses)\n    ed[\"losses\"][\"val\"].append(val_losses)\n    ed[\"predictions\"].append(W_hat_test)\n    ed[\"ground_truth\"].append(W_test.cpu().numpy())\n    ed[\"ratios\"].append((c_steps, d_steps))\n    print(f\"Finished run for code:dict updates = {c_steps}:{d_steps}\")\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom itertools import product\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data parameters\nn_samples = 80\nn_test = 20\nn_components = 30\ndim = 1024\nlambda1 = 1e-2\nlr = 1e-2\nepochs = 50\n\n# generate ground truth and data\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\nW_all = codes0.mm(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\nW_train = W_all[:n_samples]\nW_test = W_all[n_samples:]\n\n# initialization methods\ninit_methods = {\n    \"normal\": lambda x: x.data.normal_(),\n    \"xavier_uni\": lambda x: torch.nn.init.xavier_uniform_(x),\n    \"orthogonal\": lambda x: torch.nn.init.orthogonal_(x),\n    \"zeros\": lambda x: x.data.zero_(),\n}\n\n# prepare experiment data structure\nexperiment_data = {\n    \"initialization\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"init_schemes\": [],\n        }\n    }\n}\n\n# ablation over init combinations\nfor init_D, init_codes in product(init_methods.keys(), repeat=2):\n    torch.manual_seed(0)\n    # initialize parameters\n    D = nn.Parameter(torch.empty(n_components, dim, device=device))\n    init_methods[init_D](D)\n    codes_train = nn.Parameter(torch.empty(n_samples, n_components, device=device))\n    init_methods[init_codes](codes_train)\n    optimizer = torch.optim.Adam([D, codes_train], lr=lr, betas=(0.9, 0.999))\n\n    train_errs, val_errs = [], []\n    train_losses, val_losses = [], []\n    # training\n    for epoch in range(1, epochs + 1):\n        optimizer.zero_grad()\n        W_hat = codes_train.mm(D)\n        loss_recon = ((W_hat - W_train) ** 2).mean()\n        loss_sparse = lambda1 * codes_train.abs().mean()\n        (loss_recon + loss_sparse).backward()\n        optimizer.step()\n\n        with torch.no_grad():\n            tr_err = ((W_hat - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n            D_pinv = torch.pinverse(D)\n            codes_test = W_test.mm(D_pinv)\n            W_hat_test = codes_test.mm(D)\n            vl_err = (\n                ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1)).mean().item()\n            )\n            train_errs.append(tr_err)\n            val_errs.append(vl_err)\n            train_losses.append(loss_recon.item())\n            val_losses.append(((W_hat_test - W_test) ** 2).mean().item())\n\n    # final predictions\n    with torch.no_grad():\n        D_pinv = torch.pinverse(D)\n        W_pred = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n        W_gt = W_test.cpu().numpy()\n\n    # store results\n    ed = experiment_data[\"initialization\"][\"synthetic\"]\n    ed[\"metrics\"][\"train\"].append(train_errs)\n    ed[\"metrics\"][\"val\"].append(val_errs)\n    ed[\"losses\"][\"train\"].append(train_losses)\n    ed[\"losses\"][\"val\"].append(val_losses)\n    ed[\"predictions\"].append(W_pred)\n    ed[\"ground_truth\"].append(W_gt)\n    ed[\"init_schemes\"].append({\"D\": init_D, \"codes\": init_codes})\n    print(f\"Finished init_D={init_D}, init_codes={init_codes}\")\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data parameters\nn_samples = 80\nn_test = 20\nn_components = 30\ndim = 1024\nlambda1 = 1e-2\nlr = 1e-2\nepochs = 50\n\n# generate ground truth and data\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\nW_all = codes0.mm(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\nW_train = W_all[:n_samples]\nW_test = W_all[n_samples:]\n\n# ablation: minibatch sizes (full batch + smaller)\nbatch_size_list = [n_samples, 40, 20, 10]\n\n# prepare experiment data structure\nexperiment_data = {\n    \"mini_batch_size\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"batch_sizes\": [],\n        }\n    }\n}\nsynthetic = experiment_data[\"mini_batch_size\"][\"synthetic\"]\n\nfor bs in batch_size_list:\n    torch.manual_seed(0)\n    # reinit params\n    D = nn.Parameter(torch.randn_like(D0))\n    codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n    optimizer = torch.optim.Adam([D, codes_train], lr=lr, betas=(0.9, 0.999))\n    train_errs, val_errs = [], []\n    train_losses, val_losses = [], []\n    synthetic[\"batch_sizes\"].append(bs)\n    # training loop\n    for epoch in range(1, epochs + 1):\n        if bs == n_samples:\n            # full-batch update\n            optimizer.zero_grad()\n            W_hat = codes_train.mm(D)\n            loss_recon = ((W_hat - W_train) ** 2).mean()\n            loss_sparse = lambda1 * codes_train.abs().mean()\n            (loss_recon + loss_sparse).backward()\n            optimizer.step()\n        else:\n            # minibatch updates\n            perm = torch.randperm(n_samples)\n            for i in range(0, n_samples, bs):\n                idx = perm[i : i + bs]\n                optimizer.zero_grad()\n                Wb = codes_train[idx].mm(D)\n                lr_b = ((Wb - W_train[idx]) ** 2).mean()\n                ls_b = lambda1 * codes_train[idx].abs().mean()\n                (lr_b + ls_b).backward()\n                optimizer.step()\n        # compute metrics at epoch end\n        with torch.no_grad():\n            W_hat_full = codes_train.mm(D)\n            tr_err = (\n                ((W_hat_full - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n            )\n            D_pinv = torch.pinverse(D)\n            codes_test = W_test.mm(D_pinv)\n            W_hat_test = codes_test.mm(D)\n            vl_err = (\n                ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1)).mean().item()\n            )\n            tr_loss = ((W_hat_full - W_train) ** 2).mean().item()\n            vl_loss = ((W_hat_test - W_test) ** 2).mean().item()\n        train_errs.append(tr_err)\n        val_errs.append(vl_err)\n        train_losses.append(tr_loss)\n        val_losses.append(vl_loss)\n    # final test predictions\n    with torch.no_grad():\n        D_pinv = torch.pinverse(D)\n        W_hat_test = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n    # store results\n    synthetic[\"metrics\"][\"train\"].append(train_errs)\n    synthetic[\"metrics\"][\"val\"].append(val_errs)\n    synthetic[\"losses\"][\"train\"].append(train_losses)\n    synthetic[\"losses\"][\"val\"].append(val_losses)\n    synthetic[\"predictions\"].append(W_hat_test)\n    synthetic[\"ground_truth\"].append(W_test.cpu().numpy())\n    print(f\"Finished run for batch_size={bs}\")\n\n# save all data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data parameters\nn_samples = 80\nn_test = 20\nn_components = 30\ndim = 1024\nlambda_l1 = 1e-2\nlambda_l2 = 1e-2\nlr = 1e-2\nepochs = 50\n\n# generate ground truth and data\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\nW_all = codes0.mm(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\nW_train = W_all[:n_samples]\nW_test = W_all[n_samples:]\n\n# define ablation penalty types\npenalty_types = [\"none\", \"l1\", \"l2\", \"elasticnet\"]\n\n# prepare experiment data structure\nexperiment_data = {\n    p: {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n    for p in penalty_types\n}\n\nfor ptype in penalty_types:\n    # reinitialize model parameters\n    torch.manual_seed(0)\n    D = nn.Parameter(torch.randn_like(D0))\n    codes = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n    optimizer = torch.optim.Adam([D, codes], lr=lr, betas=(0.9, 0.999))\n    train_errs, val_errs = [], []\n    train_losses, val_losses = [], []\n\n    # training loop\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        W_hat = codes.mm(D)\n        loss_recon = ((W_hat - W_train) ** 2).mean()\n        if ptype == \"l1\":\n            loss_reg = lambda_l1 * codes.abs().mean()\n        elif ptype == \"l2\":\n            loss_reg = lambda_l2 * codes.pow(2).mean()\n        elif ptype == \"elasticnet\":\n            loss_reg = lambda_l1 * codes.abs().mean() + lambda_l2 * codes.pow(2).mean()\n        else:  # none\n            loss_reg = 0.0\n        loss = loss_recon + loss_reg\n        loss.backward()\n        optimizer.step()\n\n        # compute metrics\n        with torch.no_grad():\n            tr_err = ((W_hat - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n            D_pinv = torch.pinverse(D)\n            codes_test = W_test.mm(D_pinv)\n            W_hat_test = codes_test.mm(D)\n            vl_err = (\n                ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1)).mean().item()\n            )\n            train_errs.append(tr_err)\n            val_errs.append(vl_err)\n            train_losses.append(loss_recon.item())\n            val_losses.append(((W_hat_test - W_test) ** 2).mean().item())\n\n    # final predictions\n    with torch.no_grad():\n        D_pinv = torch.pinverse(D)\n        W_hat_test = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n\n    # store results\n    ed = experiment_data[ptype][\"synthetic\"]\n    ed[\"metrics\"][\"train\"].append(train_errs)\n    ed[\"metrics\"][\"val\"].append(val_errs)\n    ed[\"losses\"][\"train\"].append(train_losses)\n    ed[\"losses\"][\"val\"].append(val_losses)\n    ed[\"predictions\"].append(W_hat_test)\n    ed[\"ground_truth\"].append(W_test.cpu().numpy())\n\n    print(f\"Finished penalty type: {ptype}\")\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data parameters\nn_samples = 80\nn_test = 20\nn_components = 30\ndim = 1024\nlambda1 = 1e-2  # sparsity weight\nlr = 1e-2\nepochs = 50\n\n# generate ground truth and data\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\nW_all = codes0.mm(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\nW_train = W_all[:n_samples]\nW_test = W_all[n_samples:]\n\n# orthogonality penalty grid\nlambda2_list = [0.0, 1e-4, 1e-3, 1e-2, 1e-1]\n\n# prepare experiment data structure\nexperiment_data = {\n    \"dict_orth\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# identity for orth penalty\nI_comp = torch.eye(n_components, device=device)\n\nfor lam2 in lambda2_list:\n    # reproducible init\n    torch.manual_seed(0)\n    D = nn.Parameter(torch.randn_like(D0))\n    codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n    optimizer = torch.optim.Adam([D, codes_train], lr=lr)\n\n    train_errs, val_errs = [], []\n    train_losses, val_losses = [], []\n\n    for epoch in range(1, epochs + 1):\n        optimizer.zero_grad()\n        W_hat = codes_train.mm(D)\n        loss_recon = ((W_hat - W_train) ** 2).mean()\n        loss_sparse = lambda1 * codes_train.abs().mean()\n        orth_pen = ((D.mm(D.t()) - I_comp) ** 2).sum()\n        loss = loss_recon + loss_sparse + lam2 * orth_pen\n        loss.backward()\n        optimizer.step()\n\n        with torch.no_grad():\n            tr_err = ((W_hat - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n            D_pinv = torch.pinverse(D)\n            codes_test = W_test.mm(D_pinv)\n            W_hat_test = codes_test.mm(D)\n            vl_err = (\n                ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1)).mean().item()\n            )\n\n            train_errs.append(tr_err)\n            val_errs.append(vl_err)\n            train_losses.append(loss_recon.item())\n            val_losses.append(((W_hat_test - W_test) ** 2).mean().item())\n\n    # final test predictions\n    with torch.no_grad():\n        D_pinv = torch.pinverse(D)\n        W_hat_test = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n\n    # store results\n    ed = experiment_data[\"dict_orth\"][\"synthetic\"]\n    ed[\"metrics\"][\"train\"].append(train_errs)\n    ed[\"metrics\"][\"val\"].append(val_errs)\n    ed[\"losses\"][\"train\"].append(train_losses)\n    ed[\"losses\"][\"val\"].append(val_losses)\n    ed[\"predictions\"].append(W_hat_test)\n    ed[\"ground_truth\"].append(W_test.cpu().numpy())\n\n    print(f\"Finished run for lambda2={lam2}\")\n\n# save all data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# working directory and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# helper functions\ndef soft_threshold(x, thresh):\n    return torch.sign(x) * torch.clamp(torch.abs(x) - thresh, min=0.0)\n\n\ndef ista(D, W, lam, max_iter=50):\n    # D: [n_comp, dim], W: [n_samples, dim]\n    n_comp = D.shape[0]\n    A = D.matmul(D.t())\n    # estimate Lipschitz constant via power iteration\n    v = torch.rand(n_comp, device=device)\n    v /= v.norm()\n    for _ in range(20):\n        v = A.mv(v)\n        v /= v.norm()\n    L = v.dot(A.mv(v)).item()\n    alpha = 1.0 / L\n    X = torch.zeros((W.shape[0], n_comp), device=device)\n    for _ in range(max_iter):\n        grad = (X.matmul(D) - W).matmul(D.t())\n        X = soft_threshold(X - alpha * grad, alpha * lam)\n    return X\n\n\ndef fista(D, W, lam, max_iter=50):\n    n_comp = D.shape[0]\n    A = D.matmul(D.t())\n    v = torch.rand(n_comp, device=device)\n    v /= v.norm()\n    for _ in range(20):\n        v = A.mv(v)\n        v /= v.norm()\n    L = v.dot(A.mv(v)).item()\n    alpha = 1.0 / L\n    X = torch.zeros((W.shape[0], n_comp), device=device)\n    Y = X.clone()\n    t = 1.0\n    for _ in range(max_iter):\n        grad = (Y.matmul(D) - W).matmul(D.t())\n        X_new = soft_threshold(Y - alpha * grad, alpha * lam)\n        t_new = (1 + (1 + 4 * t * t) ** 0.5) / 2\n        Y = X_new + ((t - 1) / t_new) * (X_new - X)\n        X, t = X_new, t_new\n    return X\n\n\ndef omp(D, W, k):\n    # D: [n_comp, dim], W: [n_samples, dim]\n    n_samples, n_comp = W.shape[0], D.shape[0]\n    codes = torch.zeros((n_samples, n_comp), device=device)\n    for i in range(n_samples):\n        w = W[i]\n        residual = w.clone()\n        idxs = []\n        for _ in range(k):\n            corr = torch.mv(D, residual)\n            corr_abs = corr.abs()\n            corr_abs[idxs] = 0\n            j = int(torch.argmax(corr_abs))\n            idxs.append(j)\n            D_S = D[idxs]  # [|S|, dim]\n            # pseudoinverse to solve for active coefficients\n            D_S_pinv = torch.pinverse(D_S)\n            x_S = D_S_pinv.matmul(w.unsqueeze(1)).squeeze(1)\n            residual = w - x_S.unsqueeze(0).mm(D_S).squeeze(0)\n        codes[i, idxs] = x_S\n    return codes\n\n\n# synthetic data parameters\nn_samples, n_test = 80, 20\nn_components, dim = 30, 1024\nlambda1, lr = 1e-2, 1e-2\nepochs = 50\nk_omp = max(1, int(0.1 * n_components))\n\n# generate synthetic data\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\nW_all = codes0.matmul(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\nW_train, W_test = W_all[:n_samples], W_all[n_samples:]\n\n# training setup\ntorch.manual_seed(0)\nD = nn.Parameter(torch.randn_like(D0))\ncodes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\noptimizer = torch.optim.Adam([D, codes_train], lr=lr, betas=(0.9, 0.999))\n\n# define solvers\nsolver_funcs = {\n    \"pseudoinverse\": lambda D_, W_: W_.matmul(torch.pinverse(D_)),\n    \"ista\": lambda D_, W_: ista(D_, W_, lambda1, max_iter=50),\n    \"fista\": lambda D_, W_: fista(D_, W_, lambda1, max_iter=50),\n    \"omp\": lambda D_, W_: omp(D_, W_, k_omp),\n}\nsolver_names = list(solver_funcs.keys())\n\n# logs\ntrain_errs, train_losses = [], []\nval_errs = {name: [] for name in solver_names}\nval_losses = {name: [] for name in solver_names}\n\n# training loop\nfor epoch in range(1, epochs + 1):\n    optimizer.zero_grad()\n    W_hat = codes_train.matmul(D)\n    loss_recon = ((W_hat - W_train) ** 2).mean()\n    loss_sparse = lambda1 * codes_train.abs().mean()\n    (loss_recon + loss_sparse).backward()\n    optimizer.step()\n    with torch.no_grad():\n        tr_err = ((W_hat - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n        train_errs.append(tr_err)\n        train_losses.append(loss_recon.item())\n        for name, fn in solver_funcs.items():\n            codes_test = fn(D, W_test)\n            W_hat_test = codes_test.matmul(D)\n            ve = ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1)).mean().item()\n            vl = ((W_hat_test - W_test) ** 2).mean().item()\n            val_errs[name].append(ve)\n            val_losses[name].append(vl)\n\n# finalize results\ntrain_errs_np = np.array(train_errs)\ntrain_losses_np = np.array(train_losses)\nval_errs_np = {name: np.array(val_errs[name]) for name in solver_names}\nval_losses_np = {name: np.array(val_losses[name]) for name in solver_names}\n\npredictions_np = {}\nfor name, fn in solver_funcs.items():\n    with torch.no_grad():\n        codes_test = fn(D, W_test)\n        predictions_np[name] = codes_test.matmul(D).cpu().numpy()\n\nground_truth_np = W_test.cpu().numpy()\n\nexperiment_data = {\n    \"test_time_solver_ablation\": {\n        \"synthetic\": {\n            \"solver_names\": solver_names,\n            \"metrics\": {\"train\": train_errs_np, \"val\": val_errs_np},\n            \"losses\": {\"train\": train_losses_np, \"val\": val_losses_np},\n            \"predictions\": predictions_np,\n            \"ground_truth\": ground_truth_np,\n        }\n    }\n}\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# hyperparameters\ntorch.manual_seed(0)\nn_samples, n_test = 80, 20\ntotal = n_samples + n_test\nn_components, dim = 30, 1024\nlambda1, lr = 1e-2, 1e-2\nepochs = 50\nbeta1_list = [0.5, 0.7, 0.9, 0.99]\n\n\n# code sampling functions\ndef sample_bernoulli_gaussian(total, k, device, sparsity=0.1):\n    mask = (torch.rand(total, k, device=device) < sparsity).float()\n    return mask * torch.randn(total, k, device=device)\n\n\ndef sample_bernoulli_laplace(total, k, device, sparsity=0.1):\n    mask = (torch.rand(total, k, device=device) < sparsity).float()\n    dist = torch.distributions.Laplace(0.0, 1.0)\n    return mask * dist.sample((total, k)).to(device)\n\n\ndef sample_bernoulli_uniform(total, k, device, sparsity=0.1):\n    mask = (torch.rand(total, k, device=device) < sparsity).float()\n    return mask * (2 * torch.rand(total, k, device=device) - 1.0)\n\n\ndef sample_block_sparse(total, k, device, block_size=5, blocks_per_sample=2):\n    codes = torch.zeros(total, k, device=device)\n    n_blocks = k // block_size\n    for i in range(total):\n        blocks = torch.randperm(n_blocks)[:blocks_per_sample]\n        for b in blocks:\n            s, e = b * block_size, b * block_size + block_size\n            codes[i, s:e] = torch.randn(block_size, device=device)\n    return codes\n\n\ndistributions = {\n    \"bernoulli_gaussian\": sample_bernoulli_gaussian,\n    \"bernoulli_laplace\": sample_bernoulli_laplace,\n    \"bernoulli_uniform\": sample_bernoulli_uniform,\n    \"block_sparse\": sample_block_sparse,\n}\n\n# ground-truth dictionary\nD0 = torch.randn(n_components, dim, device=device)\n\n# prepare experiment data structure\nexperiment_data = {\"synthetic_code_distribution\": {}}\nfor name in distributions:\n    experiment_data[\"synthetic_code_distribution\"][name] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n# run ablation\nfor name, sampler in distributions.items():\n    # synthesize data\n    codes0 = sampler(total, n_components, device)\n    W_all = codes0.mm(D0) + 0.01 * torch.randn(total, dim, device=device)\n    W_train, W_test = W_all[:n_samples], W_all[n_samples:]\n    print(f\"Dataset: {name}\")\n\n    for b1 in beta1_list:\n        torch.manual_seed(0)\n        D = nn.Parameter(torch.randn_like(D0))\n        codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n        optimizer = torch.optim.Adam([D, codes_train], lr=lr, betas=(b1, 0.999))\n\n        train_errs, val_errs = [], []\n        train_losses, val_losses = [], []\n\n        for epoch in range(1, epochs + 1):\n            optimizer.zero_grad()\n            W_hat = codes_train.mm(D)\n            loss_recon = ((W_hat - W_train) ** 2).mean()\n            loss_sparse = lambda1 * codes_train.abs().mean()\n            loss = loss_recon + loss_sparse\n            loss.backward()\n            optimizer.step()\n\n            with torch.no_grad():\n                tr_err = (\n                    ((W_hat - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n                )\n                D_pinv = torch.pinverse(D)\n                codes_test = W_test.mm(D_pinv)\n                W_hat_test = codes_test.mm(D)\n                vl_err = (\n                    ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1))\n                    .mean()\n                    .item()\n                )\n                train_errs.append(tr_err)\n                val_errs.append(vl_err)\n                train_losses.append(loss_recon.item())\n                val_losses.append(((W_hat_test - W_test) ** 2).mean().item())\n\n        # final test predictions\n        with torch.no_grad():\n            D_pinv = torch.pinverse(D)\n            W_hat_test = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n\n        ed = experiment_data[\"synthetic_code_distribution\"][name]\n        ed[\"metrics\"][\"train\"].append(train_errs)\n        ed[\"metrics\"][\"val\"].append(val_errs)\n        ed[\"losses\"][\"train\"].append(train_losses)\n        ed[\"losses\"][\"val\"].append(val_losses)\n        ed[\"predictions\"].append(W_hat_test)\n        ed[\"ground_truth\"].append(W_test.cpu().numpy())\n\n        print(f\"  Finished beta1={b1}\")\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n# setup working directory and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data parameters\nn_samples, n_test = 80, 20\nn_components, dim = 30, 1024\nlambda1, lr, epochs = 1e-2, 1e-2, 50\n\n# generate ground truth and data\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\nW_all = codes0.mm(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\nW_train, W_test = W_all[:n_samples], W_all[n_samples:]\n\n# hyperparameter grid for Adam's beta1\nbeta1_list = [0.5, 0.7, 0.9, 0.99]\n\n# define reconstruction losses\nrecon_fns = {\n    \"mse\": lambda X, Y: ((X - Y) ** 2).mean(),\n    \"mae\": lambda X, Y: (X - Y).abs().mean(),\n    \"huber\": lambda X, Y: F.smooth_l1_loss(X, Y, reduction=\"mean\"),\n}\n\n# prepare experiment data structure\nexperiment_data = {\n    \"reconstruction_loss\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor loss_name, recon_fn in recon_fns.items():\n    ed = experiment_data[\"reconstruction_loss\"][\"synthetic\"]\n    for b1 in beta1_list:\n        torch.manual_seed(0)\n        D = nn.Parameter(torch.randn_like(D0))\n        codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n        optimizer = torch.optim.Adam([D, codes_train], lr=lr, betas=(b1, 0.999))\n\n        train_errs, val_errs = [], []\n        train_losses, val_losses = [], []\n\n        for epoch in range(1, epochs + 1):\n            optimizer.zero_grad()\n            W_hat = codes_train.mm(D)\n            loss_recon = recon_fn(W_hat, W_train)\n            loss_sparse = lambda1 * codes_train.abs().mean()\n            (loss_recon + loss_sparse).backward()\n            optimizer.step()\n\n            with torch.no_grad():\n                tr_err = (\n                    ((W_hat - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n                )\n                D_pinv = torch.pinverse(D)\n                codes_test = W_test.mm(D_pinv)\n                W_hat_test = codes_test.mm(D)\n                vl_err = (\n                    ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1))\n                    .mean()\n                    .item()\n                )\n                tr_loss = loss_recon.item()\n                vl_loss = recon_fn(W_hat_test, W_test).item()\n\n            train_errs.append(tr_err)\n            val_errs.append(vl_err)\n            train_losses.append(tr_loss)\n            val_losses.append(vl_loss)\n\n        with torch.no_grad():\n            D_pinv = torch.pinverse(D)\n            W_hat_test = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n\n        ed[\"metrics\"][\"train\"].append(train_errs)\n        ed[\"metrics\"][\"val\"].append(val_errs)\n        ed[\"losses\"][\"train\"].append(train_losses)\n        ed[\"losses\"][\"val\"].append(val_losses)\n        ed[\"predictions\"].append(W_hat_test)\n        ed[\"ground_truth\"].append(W_test.cpu().numpy())\n\n        print(f\"Finished {loss_name} run for beta1={b1}\")\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# working directory and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# helper functions\ndef soft_threshold(x, thresh):\n    return torch.sign(x) * torch.clamp(torch.abs(x) - thresh, min=0.0)\n\n\ndef ista(D, W, lam, max_iter=50):\n    n_comp = D.shape[0]\n    A = D.matmul(D.t())\n    v = torch.rand(n_comp, device=device)\n    v /= v.norm()\n    for _ in range(20):\n        v = A.mv(v)\n        v /= v.norm()\n    L = v.dot(A.mv(v)).item()\n    alpha = 1.0 / L\n    X = torch.zeros((W.shape[0], n_comp), device=device)\n    for _ in range(max_iter):\n        grad = (X.matmul(D) - W).matmul(D.t())\n        X = soft_threshold(X - alpha * grad, alpha * lam)\n    return X\n\n\ndef fista(D, W, lam, max_iter=50):\n    n_comp = D.shape[0]\n    A = D.matmul(D.t())\n    v = torch.rand(n_comp, device=device)\n    v /= v.norm()\n    for _ in range(20):\n        v = A.mv(v)\n        v /= v.norm()\n    L = v.dot(A.mv(v)).item()\n    alpha = 1.0 / L\n    X = torch.zeros((W.shape[0], n_comp), device=device)\n    Y = X.clone()\n    t = 1.0\n    for _ in range(max_iter):\n        grad = (Y.matmul(D) - W).matmul(D.t())\n        X_new = soft_threshold(Y - alpha * grad, alpha * lam)\n        t_new = (1 + (1 + 4 * t * t) ** 0.5) / 2\n        Y = X_new + ((t - 1) / t_new) * (X_new - X)\n        X, t = X_new, t_new\n    return X\n\n\ndef omp(D, W, k):\n    # D: [n_comp, dim], W: [n_samples, dim]\n    n_samples, n_comp = W.shape[0], D.shape[0]\n    codes = torch.zeros((n_samples, n_comp), device=device)\n    for i in range(n_samples):\n        w = W[i]\n        residual = w.clone()\n        idxs = []\n        x_S = None\n        for _ in range(k):\n            corr = torch.mv(D, residual)  # [n_comp]\n            corr_abs = corr.abs()\n            corr_abs[idxs] = 0\n            j = int(torch.argmax(corr_abs))\n            idxs.append(j)\n            D_S = D[idxs]  # [|S|, dim]\n            A = D_S.t()  # [dim, |S|]\n            A_pinv = torch.pinverse(A)  # [|S|, dim]\n            x_S = torch.mv(A_pinv, w)  # [|S|]\n            residual = w - x_S.unsqueeze(0).mm(D_S).squeeze(0)\n        if x_S is not None:\n            codes[i, idxs] = x_S\n    return codes\n\n\n# synthetic data parameters\nn_samples, n_test = 80, 20\nn_components, dim = 30, 1024\nlambda1, lr = 1e-2, 1e-2\nepochs = 50\nk_omp = max(1, int(0.1 * n_components))\n\n# generate synthetic data\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\nW_all = codes0.matmul(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\nW_train, W_test = W_all[:n_samples], W_all[n_samples:]\n\n# training setup\ntorch.manual_seed(0)\nD = nn.Parameter(torch.randn_like(D0)).to(device)\ncodes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\noptimizer = torch.optim.Adam([D, codes_train], lr=lr, betas=(0.9, 0.999))\n\n# define solvers\nsolver_funcs = {\n    \"pseudoinverse\": lambda D_, W_: W_.matmul(torch.pinverse(D_)),\n    \"ista\": lambda D_, W_: ista(D_, W_, lambda1, max_iter=50),\n    \"fista\": lambda D_, W_: fista(D_, W_, lambda1, max_iter=50),\n    \"omp\": lambda D_, W_: omp(D_, W_, k_omp),\n}\nsolver_names = list(solver_funcs.keys())\n\n# logs\ntrain_errs, train_losses = [], []\nval_errs = {name: [] for name in solver_names}\nval_losses = {name: [] for name in solver_names}\n\n# training loop\nfor epoch in range(1, epochs + 1):\n    optimizer.zero_grad()\n    W_hat = codes_train.matmul(D)\n    loss_recon = ((W_hat - W_train) ** 2).mean()\n    loss_sparse = lambda1 * codes_train.abs().mean()\n    (loss_recon + loss_sparse).backward()\n    optimizer.step()\n\n    with torch.no_grad():\n        tr_err = ((W_hat - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n        train_errs.append(tr_err)\n        train_losses.append(loss_recon.item())\n        print(\n            f\"Epoch {epoch}: training_error = {tr_err:.4f}, training_loss = {loss_recon.item():.4f}\"\n        )\n        for name, fn in solver_funcs.items():\n            codes_test = fn(D, W_test)\n            W_hat_test = codes_test.matmul(D)\n            ve = ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1)).mean().item()\n            vl = ((W_hat_test - W_test) ** 2).mean().item()\n            val_errs[name].append(ve)\n            val_losses[name].append(vl)\n            print(f\"  {name}: val_error = {ve:.4f}, val_loss = {vl:.6f}\")\n\n# finalize results\ntrain_errs_np = np.array(train_errs)\ntrain_losses_np = np.array(train_losses)\nval_errs_np = {name: np.array(val_errs[name]) for name in solver_names}\nval_losses_np = {name: np.array(val_losses[name]) for name in solver_names}\n\npredictions_np = {}\nfor name, fn in solver_funcs.items():\n    with torch.no_grad():\n        codes_test = fn(D, W_test)\n        predictions_np[name] = codes_test.matmul(D).cpu().numpy()\nground_truth_np = W_test.cpu().numpy()\n\nexperiment_data = {\n    \"test_time_solver_ablation\": {\n        \"synthetic\": {\n            \"solver_names\": solver_names,\n            \"metrics\": {\"train_err\": train_errs_np, \"val_err\": val_errs_np},\n            \"losses\": {\"train_loss\": train_losses_np, \"val_loss\": val_losses_np},\n            \"predictions\": predictions_np,\n            \"ground_truth\": ground_truth_np,\n        }\n    }\n}\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.optim.lr_scheduler import StepLR, ExponentialLR, CosineAnnealingLR\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data parameters\nn_samples, n_test, n_components, dim = 80, 20, 30, 1024\nlambda1, lr, epochs = 1e-2, 1e-2, 50\n\n# generate ground truth and data\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\nW_all = codes0.mm(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\nW_train, W_test = W_all[:n_samples], W_all[n_samples:]\n\n# define learning\u2010rate schedules\nschedule_configs = {\n    \"fixed\": None,\n    \"step_decay\": lambda opt: StepLR(opt, step_size=15, gamma=0.1),\n    \"exp_decay\": lambda opt: ExponentialLR(opt, gamma=0.95),\n    \"cosine\": lambda opt: CosineAnnealingLR(opt, T_max=epochs),\n}\n\n# prepare experiment data structure\nexperiment_data = {\n    \"lr_schedules\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"schedules\": [],\n        }\n    }\n}\n\n# run ablation\nfor schedule_name, sched_fn in schedule_configs.items():\n    torch.manual_seed(0)\n    # reinit model parameters\n    D = nn.Parameter(torch.randn_like(D0))\n    codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n    optimizer = torch.optim.Adam([D, codes_train], lr=lr, betas=(0.9, 0.999))\n    scheduler = sched_fn(optimizer) if sched_fn is not None else None\n\n    train_errs, val_errs = [], []\n    train_losses, val_losses = [], []\n    for epoch in range(1, epochs + 1):\n        optimizer.zero_grad()\n        W_hat = codes_train.mm(D)\n        loss_recon = ((W_hat - W_train) ** 2).mean()\n        loss_sparse = lambda1 * codes_train.abs().mean()\n        (loss_recon + loss_sparse).backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n\n        with torch.no_grad():\n            tr_err = ((W_hat - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n            D_pinv = torch.pinverse(D)\n            codes_test = W_test.mm(D_pinv)\n            W_hat_test = codes_test.mm(D)\n            vl_err = (\n                ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1)).mean().item()\n            )\n            train_errs.append(tr_err)\n            val_errs.append(vl_err)\n            train_losses.append(loss_recon.item())\n            val_losses.append(((W_hat_test - W_test) ** 2).mean().item())\n\n    # final predictions\n    with torch.no_grad():\n        D_pinv = torch.pinverse(D)\n        W_hat_test = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n\n    ed = experiment_data[\"lr_schedules\"][\"synthetic\"]\n    ed[\"metrics\"][\"train\"].append(train_errs)\n    ed[\"metrics\"][\"val\"].append(val_errs)\n    ed[\"losses\"][\"train\"].append(train_losses)\n    ed[\"losses\"][\"val\"].append(val_losses)\n    ed[\"predictions\"].append(W_hat_test)\n    ed[\"ground_truth\"].append(W_test.cpu().numpy())\n    ed[\"schedules\"].append(schedule_name)\n    print(f\"Finished run for schedule={schedule_name}\")\n\n# save all data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data parameters\nn_samples = 80\nn_test = 20\nn_components = 30\ndim = 1024\nlambda1 = 1e-2\nlr = 1e-2\nepochs = 50\nnoise_scale = 0.01\n\n# hyperparameter grid\nbeta1_list = [0.5, 0.7, 0.9, 0.99]\n\n# initialize data\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\n\n# define noise generators\nnoise_types = {\n    \"gaussian\": lambda: noise_scale\n    * torch.randn((n_samples + n_test, dim), device=device),\n    \"laplace\": lambda: torch.distributions.Laplace(0.0, noise_scale)\n    .sample((n_samples + n_test, dim))\n    .to(device),\n    \"cauchy\": lambda: torch.distributions.Cauchy(0.0, noise_scale)\n    .sample((n_samples + n_test, dim))\n    .to(device),\n}\n\n# prepare experiment data structure\nexperiment_data = {\"noise_distribution\": {}}\n\nfor dist_name, noise_fn in noise_types.items():\n    print(f\"Starting ablation for noise: {dist_name}\")\n    # generate noisy data\n    W_all = codes0.mm(D0) + noise_fn()\n    W_train = W_all[:n_samples]\n    W_test = W_all[n_samples:]\n    # init storage\n    experiment_data[\"noise_distribution\"][dist_name] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    ed = experiment_data[\"noise_distribution\"][dist_name]\n    # run experiments for each beta1\n    for b1 in beta1_list:\n        torch.manual_seed(0)\n        D = nn.Parameter(torch.randn_like(D0))\n        codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n        optimizer = torch.optim.Adam([D, codes_train], lr=lr, betas=(b1, 0.999))\n        train_errs, val_errs = [], []\n        train_losses, val_losses = [], []\n        for epoch in range(1, epochs + 1):\n            optimizer.zero_grad()\n            W_hat = codes_train.mm(D)\n            loss_recon = ((W_hat - W_train) ** 2).mean()\n            loss_sparse = lambda1 * codes_train.abs().mean()\n            loss = loss_recon + loss_sparse\n            loss.backward()\n            optimizer.step()\n            with torch.no_grad():\n                tr_err = (\n                    ((W_hat - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n                )\n                D_pinv = torch.pinverse(D)\n                codes_test = W_test.mm(D_pinv)\n                W_hat_test = codes_test.mm(D)\n                vl_err = (\n                    ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1))\n                    .mean()\n                    .item()\n                )\n                train_errs.append(tr_err)\n                val_errs.append(vl_err)\n                train_losses.append(loss_recon.item())\n                val_losses.append(((W_hat_test - W_test) ** 2).mean().item())\n        with torch.no_grad():\n            D_pinv = torch.pinverse(D)\n            W_hat_test = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n        # store results\n        ed[\"metrics\"][\"train\"].append(train_errs)\n        ed[\"metrics\"][\"val\"].append(val_errs)\n        ed[\"losses\"][\"train\"].append(train_losses)\n        ed[\"losses\"][\"val\"].append(val_losses)\n        ed[\"predictions\"].append(W_hat_test)\n        ed[\"ground_truth\"].append(W_test.cpu().numpy())\n        print(f\"  Finished beta1={b1}\")\n    print(f\"Completed noise ablation for: {dist_name}\")\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# fixed hyperparameters\nn_samples = 80\nn_test = 20\nn_components = 30\nlambda1 = 1e-2\nlr = 1e-2\nepochs = 50\nbeta1 = 0.9\nbeta2 = 0.999\n\n# data dimensionalities to test\ndims_list = [64, 256, 1024, 4096]\n\n# prepare experiment data structure\nexperiment_data = {\n    \"data_dimensionality\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"dims\": [],\n        }\n    }\n}\n\nfor dim in dims_list:\n    # reproducible setup\n    torch.manual_seed(0)\n    # ground truth dict and codes\n    D0 = torch.randn(n_components, dim, device=device)\n    codes0 = (\n        torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n    ).float() * torch.randn(n_samples + n_test, n_components, device=device)\n    W_all = codes0.mm(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\n    W_train = W_all[:n_samples]\n    W_test = W_all[n_samples:]\n\n    # model params\n    D = nn.Parameter(torch.randn_like(D0))\n    codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n    optimizer = torch.optim.Adam([D, codes_train], lr=lr, betas=(beta1, beta2))\n\n    train_errs, val_errs = [], []\n    train_losses, val_losses = [], []\n\n    # training loop\n    for epoch in range(1, epochs + 1):\n        optimizer.zero_grad()\n        W_hat = codes_train.mm(D)\n        loss_recon = ((W_hat - W_train) ** 2).mean()\n        loss_sparse = lambda1 * codes_train.abs().mean()\n        loss = loss_recon + loss_sparse\n        loss.backward()\n        optimizer.step()\n\n        # evaluate\n        with torch.no_grad():\n            tr_err = ((W_hat - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n            D_pinv = torch.pinverse(D)\n            codes_test = W_test.mm(D_pinv)\n            W_hat_test = codes_test.mm(D)\n            vl_err = (\n                ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1)).mean().item()\n            )\n            train_errs.append(tr_err)\n            val_errs.append(vl_err)\n            train_losses.append(loss_recon.item())\n            val_losses.append(((W_hat_test - W_test) ** 2).mean().item())\n\n    # final predictions\n    with torch.no_grad():\n        D_pinv = torch.pinverse(D)\n        W_hat_test = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n        W_test_np = W_test.cpu().numpy()\n\n    # store results\n    ed = experiment_data[\"data_dimensionality\"][\"synthetic\"]\n    ed[\"dims\"].append(dim)\n    ed[\"metrics\"][\"train\"].append(train_errs)\n    ed[\"metrics\"][\"val\"].append(val_errs)\n    ed[\"losses\"][\"train\"].append(train_losses)\n    ed[\"losses\"][\"val\"].append(val_losses)\n    ed[\"predictions\"].append(W_hat_test)\n    ed[\"ground_truth\"].append(W_test_np)\n\n    print(f\"Finished dimension={dim}\")\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data parameters\nn_samples = 80\nn_test = 20\nn_components = 30\ndim = 1024\nlambda1 = 1e-2\nlr = 1e-2\nepochs = 50\n\n# generate ground truth and data\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\nW_all = codes0.mm(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\nW_train = W_all[:n_samples]\nW_test = W_all[n_samples:]\n\n# ablation schedules: (code_updates, dict_updates)\nratio_pairs = [(1, 1), (5, 1), (10, 1), (1, 5), (1, 10)]\n\n# prepare experiment data structure\nexperiment_data = {\n    \"alt_min_freq\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"ratios\": [],\n        }\n    }\n}\n\nfor c_steps, d_steps in ratio_pairs:\n    torch.manual_seed(0)\n    # initialize parameters\n    D = nn.Parameter(torch.randn_like(D0))\n    codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n    # separate optimizers\n    opt_codes = torch.optim.Adam([codes_train], lr=lr, betas=(0.9, 0.999))\n    opt_D = torch.optim.Adam([D], lr=lr, betas=(0.9, 0.999))\n    # recorders\n    train_errs, val_errs = [], []\n    train_losses, val_losses = [], []\n    # training loop\n    for epoch in range(1, epochs + 1):\n        # code updates\n        for _ in range(c_steps):\n            opt_codes.zero_grad()\n            W_hat = codes_train.mm(D.detach())\n            loss_recon = ((W_hat - W_train) ** 2).mean()\n            loss_sparse = lambda1 * codes_train.abs().mean()\n            (loss_recon + loss_sparse).backward()\n            opt_codes.step()\n        # dictionary updates\n        for _ in range(d_steps):\n            opt_D.zero_grad()\n            W_hat = codes_train.detach().mm(D)\n            loss_recon = ((W_hat - W_train) ** 2).mean()\n            loss_sparse = lambda1 * codes_train.abs().mean()\n            (loss_recon + loss_sparse).backward()\n            opt_D.step()\n        # compute metrics\n        with torch.no_grad():\n            W_hat_tr = codes_train.mm(D)\n            tr_err = (\n                ((W_hat_tr - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n            )\n            loss_tr = ((W_hat_tr - W_train) ** 2).mean().item()\n            D_pinv = torch.pinverse(D)\n            codes_test = W_test.mm(D_pinv)\n            W_hat_test = codes_test.mm(D)\n            vl_err = (\n                ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1)).mean().item()\n            )\n            loss_val = ((W_hat_test - W_test) ** 2).mean().item()\n        train_errs.append(tr_err)\n        val_errs.append(vl_err)\n        train_losses.append(loss_tr)\n        val_losses.append(loss_val)\n    # final predictions\n    with torch.no_grad():\n        D_pinv = torch.pinverse(D)\n        W_hat_test = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n    # store results\n    ed = experiment_data[\"alt_min_freq\"][\"synthetic\"]\n    ed[\"metrics\"][\"train\"].append(train_errs)\n    ed[\"metrics\"][\"val\"].append(val_errs)\n    ed[\"losses\"][\"train\"].append(train_losses)\n    ed[\"losses\"][\"val\"].append(val_losses)\n    ed[\"predictions\"].append(W_hat_test)\n    ed[\"ground_truth\"].append(W_test.cpu().numpy())\n    ed[\"ratios\"].append((c_steps, d_steps))\n    print(f\"Finished run for code:dict updates = {c_steps}:{d_steps}\")\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data parameters\nn_samples = 80\nn_test = 20\nn_components = 30\ndim = 1024\nlambda1 = 1e-2\nlr = 1e-2\nepochs = 50\n\n# generate ground truth and data\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\nW_all = codes0.mm(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\nW_train = W_all[:n_samples]\nW_test = W_all[n_samples:]\n\n# ablation schedules: (code_updates, dict_updates)\nratio_pairs = [(1, 1), (5, 1), (10, 1), (1, 5), (1, 10)]\n\n# prepare experiment data structure\nexperiment_data = {\n    \"alt_min_freq\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"ratios\": [],\n        }\n    }\n}\n\nfor c_steps, d_steps in ratio_pairs:\n    torch.manual_seed(0)\n    # initialize parameters\n    D = nn.Parameter(torch.randn_like(D0))\n    codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n    # separate optimizers\n    opt_codes = torch.optim.Adam([codes_train], lr=lr, betas=(0.9, 0.999))\n    opt_D = torch.optim.Adam([D], lr=lr, betas=(0.9, 0.999))\n    # recorders\n    train_errs, val_errs = [], []\n    train_losses, val_losses = [], []\n    # training loop\n    for epoch in range(1, epochs + 1):\n        # code updates\n        for _ in range(c_steps):\n            opt_codes.zero_grad()\n            W_hat = codes_train.mm(D.detach())\n            loss_recon = ((W_hat - W_train) ** 2).mean()\n            loss_sparse = lambda1 * codes_train.abs().mean()\n            (loss_recon + loss_sparse).backward()\n            opt_codes.step()\n        # dictionary updates\n        for _ in range(d_steps):\n            opt_D.zero_grad()\n            W_hat = codes_train.detach().mm(D)\n            loss_recon = ((W_hat - W_train) ** 2).mean()\n            loss_sparse = lambda1 * codes_train.abs().mean()\n            (loss_recon + loss_sparse).backward()\n            opt_D.step()\n        # compute metrics\n        with torch.no_grad():\n            W_hat_tr = codes_train.mm(D)\n            tr_err = (\n                ((W_hat_tr - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n            )\n            loss_tr = ((W_hat_tr - W_train) ** 2).mean().item()\n            D_pinv = torch.pinverse(D)\n            codes_test = W_test.mm(D_pinv)\n            W_hat_test = codes_test.mm(D)\n            vl_err = (\n                ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1)).mean().item()\n            )\n            loss_val = ((W_hat_test - W_test) ** 2).mean().item()\n        train_errs.append(tr_err)\n        val_errs.append(vl_err)\n        train_losses.append(loss_tr)\n        val_losses.append(loss_val)\n    # final predictions\n    with torch.no_grad():\n        D_pinv = torch.pinverse(D)\n        W_hat_test = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n    # store results\n    ed = experiment_data[\"alt_min_freq\"][\"synthetic\"]\n    ed[\"metrics\"][\"train\"].append(train_errs)\n    ed[\"metrics\"][\"val\"].append(val_errs)\n    ed[\"losses\"][\"train\"].append(train_losses)\n    ed[\"losses\"][\"val\"].append(val_losses)\n    ed[\"predictions\"].append(W_hat_test)\n    ed[\"ground_truth\"].append(W_test.cpu().numpy())\n    ed[\"ratios\"].append((c_steps, d_steps))\n    print(f\"Finished run for code:dict updates = {c_steps}:{d_steps}\")\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data parameters\nn_samples = 80\nn_test = 20\nn_components = 30\ndim = 1024\nlambda1 = 1e-2\nlr = 1e-2\nepochs = 50\n\n# generate ground truth and data\ntorch.manual_seed(0)\nD0 = torch.randn(n_components, dim, device=device)\ncodes0 = (\n    torch.rand(n_samples + n_test, n_components, device=device) < 0.1\n).float() * torch.randn(n_samples + n_test, n_components, device=device)\nW_all = codes0.mm(D0) + 0.01 * torch.randn(n_samples + n_test, dim, device=device)\nW_train = W_all[:n_samples]\nW_test = W_all[n_samples:]\n\n# ablation schedules: (code_updates, dict_updates)\nratio_pairs = [(1, 1), (5, 1), (10, 1), (1, 5), (1, 10)]\n\n# prepare experiment data structure\nexperiment_data = {\n    \"alt_min_freq\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"ratios\": [],\n        }\n    }\n}\n\nfor c_steps, d_steps in ratio_pairs:\n    torch.manual_seed(0)\n    # initialize parameters\n    D = nn.Parameter(torch.randn_like(D0))\n    codes_train = nn.Parameter(torch.randn(n_samples, n_components, device=device))\n    # separate optimizers\n    opt_codes = torch.optim.Adam([codes_train], lr=lr, betas=(0.9, 0.999))\n    opt_D = torch.optim.Adam([D], lr=lr, betas=(0.9, 0.999))\n    # recorders\n    train_errs, val_errs = [], []\n    train_losses, val_losses = [], []\n    # training loop\n    for epoch in range(1, epochs + 1):\n        # code updates\n        for _ in range(c_steps):\n            opt_codes.zero_grad()\n            W_hat = codes_train.mm(D.detach())\n            loss_recon = ((W_hat - W_train) ** 2).mean()\n            loss_sparse = lambda1 * codes_train.abs().mean()\n            (loss_recon + loss_sparse).backward()\n            opt_codes.step()\n        # dictionary updates\n        for _ in range(d_steps):\n            opt_D.zero_grad()\n            W_hat = codes_train.detach().mm(D)\n            loss_recon = ((W_hat - W_train) ** 2).mean()\n            loss_sparse = lambda1 * codes_train.abs().mean()\n            (loss_recon + loss_sparse).backward()\n            opt_D.step()\n        # compute metrics\n        with torch.no_grad():\n            W_hat_tr = codes_train.mm(D)\n            tr_err = (\n                ((W_hat_tr - W_train).norm(dim=1) / W_train.norm(dim=1)).mean().item()\n            )\n            loss_tr = ((W_hat_tr - W_train) ** 2).mean().item()\n            D_pinv = torch.pinverse(D)\n            codes_test = W_test.mm(D_pinv)\n            W_hat_test = codes_test.mm(D)\n            vl_err = (\n                ((W_hat_test - W_test).norm(dim=1) / W_test.norm(dim=1)).mean().item()\n            )\n            loss_val = ((W_hat_test - W_test) ** 2).mean().item()\n        train_errs.append(tr_err)\n        val_errs.append(vl_err)\n        train_losses.append(loss_tr)\n        val_losses.append(loss_val)\n    # final predictions\n    with torch.no_grad():\n        D_pinv = torch.pinverse(D)\n        W_hat_test = (W_test.mm(D_pinv)).mm(D).cpu().numpy()\n    # store results\n    ed = experiment_data[\"alt_min_freq\"][\"synthetic\"]\n    ed[\"metrics\"][\"train\"].append(train_errs)\n    ed[\"metrics\"][\"val\"].append(val_errs)\n    ed[\"losses\"][\"train\"].append(train_losses)\n    ed[\"losses\"][\"val\"].append(val_losses)\n    ed[\"predictions\"].append(W_hat_test)\n    ed[\"ground_truth\"].append(W_test.cpu().numpy())\n    ed[\"ratios\"].append((c_steps, d_steps))\n    print(f\"Finished run for code:dict updates = {c_steps}:{d_steps}\")\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Finished run for beta1=0.5', '\\n', 'Finished run\nfor beta1=0.7', '\\n', 'Finished run for beta1=0.9', '\\n', 'Finished run for\nbeta1=0.99', '\\n', 'Execution time: a second seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished dataset=ds1, beta1=0.5', '\\n', 'Finished\ndataset=ds1, beta1=0.7', '\\n', 'Finished dataset=ds1, beta1=0.9', '\\n',\n'Finished dataset=ds1, beta1=0.99', '\\n', 'Finished dataset=ds2, beta1=0.5',\n'\\n', 'Finished dataset=ds2, beta1=0.7', '\\n', 'Finished dataset=ds2,\nbeta1=0.9', '\\n', 'Finished dataset=ds2, beta1=0.99', '\\n', 'Finished\ndataset=ds3, beta1=0.5', '\\n', 'Finished dataset=ds3, beta1=0.7', '\\n',\n'Finished dataset=ds3, beta1=0.9', '\\n', 'Finished dataset=ds3, beta1=0.99',\n'\\n', 'Execution time: 2 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished n_components=10, beta1=0.5', '\\n',\n'Finished n_components=10, beta1=0.7', '\\n', 'Finished n_components=10,\nbeta1=0.9', '\\n', 'Finished n_components=10, beta1=0.99', '\\n', 'Finished\nn_components=30, beta1=0.5', '\\n', 'Finished n_components=30, beta1=0.7', '\\n',\n'Finished n_components=30, beta1=0.9', '\\n', 'Finished n_components=30,\nbeta1=0.99', '\\n', 'Finished n_components=60, beta1=0.5', '\\n', 'Finished\nn_components=60, beta1=0.7', '\\n', 'Finished n_components=60, beta1=0.9', '\\n',\n'Finished n_components=60, beta1=0.99', '\\n', 'Execution time: 2 seconds seconds\n(time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished run for lambda1=0.0', '\\n', 'Finished run\nfor lambda1=0.0001', '\\n', 'Finished run for lambda1=0.001', '\\n', 'Finished run\nfor lambda1=0.01', '\\n', 'Finished run for lambda1=0.1', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 2 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Finished run for beta1=0.5', '\\n', 'Finished run\nfor beta1=0.7', '\\n', 'Finished run for beta1=0.9', '\\n', 'Finished run for\nbeta1=0.99', '\\n', 'Execution time: a second seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished run for noise \u03c3=0.0', '\\n', 'Finished run\nfor noise \u03c3=0.005', '\\n', 'Finished run for noise \u03c3=0.01', '\\n', 'Finished run\nfor noise \u03c3=0.02', '\\n', 'Finished run for noise \u03c3=0.05', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 2 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Finished run for optimizer=SGD', '\\n', 'Finished\nrun for optimizer=RMSprop', '\\n', 'Finished run for optimizer=AdamW', '\\n',\n'Execution time: a second seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished run for code:dict updates = 1:1', '\\n',\n'Finished run for code:dict updates = 5:1', '\\n', 'Finished run for code:dict\nupdates = 10:1', '\\n', 'Finished run for code:dict updates = 1:5', '\\n',\n'Finished run for code:dict updates = 1:10', '\\n', 'Saved experiment_data.npy',\n'\\n', 'Execution time: 3 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished init_D=normal, init_codes=normal', '\\n',\n'Finished init_D=normal, init_codes=xavier_uni', '\\n', 'Finished init_D=normal,\ninit_codes=orthogonal', '\\n', 'Finished init_D=normal, init_codes=zeros', '\\n',\n'Finished init_D=xavier_uni, init_codes=normal', '\\n', 'Finished\ninit_D=xavier_uni, init_codes=xavier_uni', '\\n', 'Finished init_D=xavier_uni,\ninit_codes=orthogonal', '\\n', 'Finished init_D=xavier_uni, init_codes=zeros',\n'\\n', 'Finished init_D=orthogonal, init_codes=normal', '\\n', 'Finished\ninit_D=orthogonal, init_codes=xavier_uni', '\\n', 'Finished init_D=orthogonal,\ninit_codes=orthogonal', '\\n', 'Finished init_D=orthogonal, init_codes=zeros',\n'\\n', 'Finished init_D=zeros, init_codes=normal', '\\n', 'Finished init_D=zeros,\ninit_codes=xavier_uni', '\\n', 'Finished init_D=zeros, init_codes=orthogonal',\n'\\n', 'Finished init_D=zeros, init_codes=zeros', '\\n', 'Execution time: 3\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished run for batch_size=80', '\\n', 'Finished\nrun for batch_size=40', '\\n', 'Finished run for batch_size=20', '\\n', 'Finished\nrun for batch_size=10', '\\n', 'Execution time: 4 seconds seconds (time limit is\nan hour).']", "['Using device: cuda', '\\n', 'Finished penalty type: none', '\\n', 'Finished\npenalty type: l1', '\\n', 'Finished penalty type: l2', '\\n', 'Finished penalty\ntype: elasticnet', '\\n', 'Execution time: a second seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Finished run for lambda2=0.0', '\\n', 'Finished run\nfor lambda2=0.0001', '\\n', 'Finished run for lambda2=0.001', '\\n', 'Finished run\nfor lambda2=0.01', '\\n', 'Finished run for lambda2=0.1', '\\n', 'Execution time:\n2 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 131, in <module>\\n    codes_test = fn(D, W_test)\\n\n^^^^^^^^^^^^^\\n  File \"runfile.py\", line 109, in <lambda>\\n    \"omp\": lambda D_,\nW_: omp(D_, W_, k_omp),\\n                          ^^^^^^^^^^^^^^^^^^\\n  File\n\"runfile.py\", line 76, in omp\\n    x_S =\nD_S_pinv.matmul(w.unsqueeze(1)).squeeze(1)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: mat1 and mat2 shapes cannot be\nmultiplied (1024x1 and 1024x1)\\n', 'Execution time: a second seconds (time limit\nis an hour).']", "['Using device: cuda', '\\n', 'Dataset: bernoulli_gaussian', '\\n', '  Finished\nbeta1=0.5', '\\n', '  Finished beta1=0.7', '\\n', '  Finished beta1=0.9', '\\n', '\nFinished beta1=0.99', '\\n', 'Dataset: bernoulli_laplace', '\\n', '  Finished\nbeta1=0.5', '\\n', '  Finished beta1=0.7', '\\n', '  Finished beta1=0.9', '\\n', '\nFinished beta1=0.99', '\\n', 'Dataset: bernoulli_uniform', '\\n', '  Finished\nbeta1=0.5', '\\n', '  Finished beta1=0.7', '\\n', '  Finished beta1=0.9', '\\n', '\nFinished beta1=0.99', '\\n', 'Dataset: block_sparse', '\\n', '  Finished\nbeta1=0.5', '\\n', '  Finished beta1=0.7', '\\n', '  Finished beta1=0.9', '\\n', '\nFinished beta1=0.99', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time:\n3 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished mse run for beta1=0.5', '\\n', 'Finished\nmse run for beta1=0.7', '\\n', 'Finished mse run for beta1=0.9', '\\n', 'Finished\nmse run for beta1=0.99', '\\n', 'Finished mae run for beta1=0.5', '\\n', 'Finished\nmae run for beta1=0.7', '\\n', 'Finished mae run for beta1=0.9', '\\n', 'Finished\nmae run for beta1=0.99', '\\n', 'Finished huber run for beta1=0.5', '\\n',\n'Finished huber run for beta1=0.7', '\\n', 'Finished huber run for beta1=0.9',\n'\\n', 'Finished huber run for beta1=0.99', '\\n', 'Execution time: 2 seconds\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 1: training_error = 37.2630, training_loss =\n33.7259', '\\n', '  pseudoinverse: val_error = 0.0248, val_loss = 0.000208',\n'\\n', '  ista: val_error = 0.0248, val_loss = 0.000208', '\\n', '  fista:\nval_error = 0.0248, val_loss = 0.000208', '\\n', '  omp: val_error = 0.1144,\nval_loss = 0.201569', '\\n', 'Epoch 2: training_error = 36.5937, training_loss =\n32.6780', '\\n', '  pseudoinverse: val_error = 0.0286, val_loss = 0.000532',\n'\\n', '  ista: val_error = 0.0286, val_loss = 0.000532', '\\n', '  fista:\nval_error = 0.0286, val_loss = 0.000532', '\\n', '  omp: val_error = 0.1170,\nval_loss = 0.201467', '\\n', 'Epoch 3: training_error = 35.9341, training_loss =\n31.6627', '\\n', '  pseudoinverse: val_error = 0.0330, val_loss = 0.001072',\n'\\n', '  ista: val_error = 0.0330, val_loss = 0.001072', '\\n', '  fista:\nval_error = 0.0330, val_loss = 0.001072', '\\n', '  omp: val_error = 0.1203,\nval_loss = 0.201679', '\\n', 'Epoch 4: training_error = 35.2849, training_loss =\n30.6795', '\\n', '  pseudoinverse: val_error = 0.0378, val_loss = 0.001828',\n'\\n', '  ista: val_error = 0.0378, val_loss = 0.001828', '\\n', '  fista:\nval_error = 0.0378, val_loss = 0.001828', '\\n', '  omp: val_error = 0.1241,\nval_loss = 0.202206', '\\n', 'Epoch 5: training_error = 34.6464, training_loss =\n29.7280', '\\n', '  pseudoinverse: val_error = 0.0428, val_loss = 0.002797',\n'\\n', '  ista: val_error = 0.0428, val_loss = 0.002797', '\\n', '  fista:\nval_error = 0.0428, val_loss = 0.002797', '\\n', '  omp: val_error = 0.1282,\nval_loss = 0.203051', '\\n', 'Epoch 6: training_error = 34.0187, training_loss =\n28.8076', '\\n', '  pseudoinverse: val_error = 0.0479, val_loss = 0.003979',\n'\\n', '  ista: val_error = 0.0479, val_loss = 0.003979', '\\n', '  fista:\nval_error = 0.0479, val_loss = 0.003979', '\\n', '  omp: val_error = 0.1326,\nval_loss = 0.204217', '\\n', 'Epoch 7: training_error = 33.4020, training_loss =\n27.9178', '\\n', '  pseudoinverse: val_error = 0.0532, val_loss = 0.005370',\n'\\n', '  ista: val_error = 0.0532, val_loss = 0.005370', '\\n', '  fista:\nval_error = 0.0532, val_loss = 0.005370', '\\n', '  omp: val_error = 0.1372,\nval_loss = 0.205701', '\\n', 'Epoch 8: training_error = 32.7964, training_loss =\n27.0578', '\\n', '  pseudoinverse: val_error = 0.0585, val_loss = 0.006966',\n'\\n', '  ista: val_error = 0.0585, val_loss = 0.006966', '\\n', '  fista:\nval_error = 0.0585, val_loss = 0.006966', '\\n', '  omp: val_error = 0.1419,\nval_loss = 0.207498', '\\n', 'Epoch 9: training_error = 32.2021, training_loss =\n26.2271', '\\n', '  pseudoinverse: val_error = 0.0639, val_loss = 0.008764',\n'\\n', '  ista: val_error = 0.0639, val_loss = 0.008764', '\\n', '  fista:\nval_error = 0.0639, val_loss = 0.008764', '\\n', '  omp: val_error = 0.1468,\nval_loss = 0.209604', '\\n', 'Epoch 10: training_error = 31.6189, training_loss =\n25.4249', '\\n', '  pseudoinverse: val_error = 0.0692, val_loss = 0.010758',\n'\\n', '  ista: val_error = 0.0692, val_loss = 0.010758', '\\n', '  fista:\nval_error = 0.0692, val_loss = 0.010758', '\\n', '  omp: val_error = 0.1517,\nval_loss = 0.212013', '\\n', 'Epoch 11: training_error = 31.0471, training_loss =\n24.6505', '\\n', '  pseudoinverse: val_error = 0.0746, val_loss = 0.012942',\n'\\n', '  ista: val_error = 0.0746, val_loss = 0.012942', '\\n', '  fista:\nval_error = 0.0746, val_loss = 0.012942', '\\n', '  omp: val_error = 0.1568,\nval_loss = 0.214725', '\\n', 'Epoch 12: training_error = 30.4865, training_loss =\n23.9030', '\\n', '  pseudoinverse: val_error = 0.0800, val_loss = 0.015312',\n'\\n', '  ista: val_error = 0.0800, val_loss = 0.015312', '\\n', '  fista:\nval_error = 0.0800, val_loss = 0.015312', '\\n', '  omp: val_error = 0.1621,\nval_loss = 0.218148', '\\n', 'Epoch 13: training_error = 29.9371, training_loss =\n23.1818', '\\n', '  pseudoinverse: val_error = 0.0853, val_loss = 0.017860',\n'\\n', '  ista: val_error = 0.0853, val_loss = 0.017860', '\\n', '  fista:\nval_error = 0.0853, val_loss = 0.017860', '\\n', '  omp: val_error = 0.1672,\nval_loss = 0.221300', '\\n', 'Epoch 14: training_error = 29.3989, training_loss =\n22.4860', '\\n', '  pseudoinverse: val_error = 0.0907, val_loss = 0.020581',\n'\\n', '  ista: val_error = 0.0907, val_loss = 0.020581', '\\n', '  fista:\nval_error = 0.0907, val_loss = 0.020581', '\\n', '  omp: val_error = 0.1724,\nval_loss = 0.224727', '\\n', 'Epoch 15: training_error = 28.8718, training_loss =\n21.8148', '\\n', '  pseudoinverse: val_error = 0.0960, val_loss = 0.023469',\n'\\n', '  ista: val_error = 0.0960, val_loss = 0.023469', '\\n', '  fista:\nval_error = 0.0960, val_loss = 0.023469', '\\n', '  omp: val_error = 0.1776,\nval_loss = 0.228424', '\\n', 'Epoch 16: training_error = 28.3558, training_loss =\n21.1676', '\\n', '  pseudoinverse: val_error = 0.1012, val_loss = 0.026517',\n'\\n', '  ista: val_error = 0.1012, val_loss = 0.026517', '\\n', '  fista:\nval_error = 0.1012, val_loss = 0.026517', '\\n', '  omp: val_error = 0.1829,\nval_loss = 0.232384', '\\n', 'Epoch 17: training_error = 27.8506, training_loss =\n20.5435', '\\n', '  pseudoinverse: val_error = 0.1065, val_loss = 0.029721',\n'\\n', '  ista: val_error = 0.1065, val_loss = 0.029721', '\\n', '  fista:\nval_error = 0.1065, val_loss = 0.029721', '\\n', '  omp: val_error = 0.1881,\nval_loss = 0.236601', '\\n', 'Epoch 18: training_error = 27.3563, training_loss =\n19.9417', '\\n', '  pseudoinverse: val_error = 0.1117, val_loss = 0.033072',\n'\\n', '  ista: val_error = 0.1117, val_loss = 0.033072', '\\n', '  fista:\nval_error = 0.1117, val_loss = 0.033072', '\\n', '  omp: val_error = 0.1934,\nval_loss = 0.241066', '\\n', 'Epoch 19: training_error = 26.8726, training_loss =\n19.3614', '\\n', '  pseudoinverse: val_error = 0.1168, val_loss = 0.036565',\n'\\n', '  ista: val_error = 0.1168, val_loss = 0.036565', '\\n', '  fista:\nval_error = 0.1168, val_loss = 0.036565', '\\n', '  omp: val_error = 0.1987,\nval_loss = 0.245775', '\\n', 'Epoch 20: training_error = 26.3993, training_loss =\n18.8020', '\\n', '  pseudoinverse: val_error = 0.1219, val_loss = 0.040192',\n'\\n', '  ista: val_error = 0.1219, val_loss = 0.040192', '\\n', '  fista:\nval_error = 0.1219, val_loss = 0.040192', '\\n', '  omp: val_error = 0.2040,\nval_loss = 0.250715', '\\n', 'Epoch 21: training_error = 25.9363, training_loss =\n18.2627', '\\n', '  pseudoinverse: val_error = 0.1269, val_loss = 0.043947',\n'\\n', '  ista: val_error = 0.1269, val_loss = 0.043947', '\\n', '  fista:\nval_error = 0.1269, val_loss = 0.043947', '\\n', '  omp: val_error = 0.2093,\nval_loss = 0.255877', '\\n', 'Epoch 22: training_error = 25.4834, training_loss =\n17.7427', '\\n', '  pseudoinverse: val_error = 0.1319, val_loss = 0.047823',\n'\\n', '  ista: val_error = 0.1319, val_loss = 0.047823', '\\n', '  fista:\nval_error = 0.1319, val_loss = 0.047823', '\\n', '  omp: val_error = 0.2146,\nval_loss = 0.261262', '\\n', 'Epoch 23: training_error = 25.0404, training_loss =\n17.2413', '\\n', '  pseudoinverse: val_error = 0.1369, val_loss = 0.051813',\n'\\n', '  ista: val_error = 0.1369, val_loss = 0.051813', '\\n', '  fista:\nval_error = 0.1369, val_loss = 0.051813', '\\n', '  omp: val_error = 0.2198,\nval_loss = 0.266862', '\\n', 'Epoch 24: training_error = 24.6072, training_loss =\n16.7579', '\\n', '  pseudoinverse: val_error = 0.1418, val_loss = 0.055910',\n'\\n', '  ista: val_error = 0.1418, val_loss = 0.055910', '\\n', '  fista:\nval_error = 0.1418, val_loss = 0.055910', '\\n', '  omp: val_error = 0.2251,\nval_loss = 0.272671', '\\n', 'Epoch 25: training_error = 24.1835, training_loss =\n16.2918', '\\n', '  pseudoinverse: val_error = 0.1466, val_loss = 0.060108',\n'\\n', '  ista: val_error = 0.1466, val_loss = 0.060108', '\\n', '  fista:\nval_error = 0.1466, val_loss = 0.060108', '\\n', '  omp: val_error = 0.2304,\nval_loss = 0.278735', '\\n', 'Epoch 26: training_error = 23.7691, training_loss =\n15.8422', '\\n', '  pseudoinverse: val_error = 0.1513, val_loss = 0.064401',\n'\\n', '  ista: val_error = 0.1513, val_loss = 0.064401', '\\n', '  fista:\nval_error = 0.1513, val_loss = 0.064401', '\\n', '  omp: val_error = 0.2356,\nval_loss = 0.284895', '\\n', 'Epoch 27: training_error = 23.3639, training_loss =\n15.4087', '\\n', '  pseudoinverse: val_error = 0.1561, val_loss = 0.068783',\n'\\n', '  ista: val_error = 0.1561, val_loss = 0.068783', '\\n', '  fista:\nval_error = 0.1561, val_loss = 0.068783', '\\n', '  omp: val_error = 0.2409,\nval_loss = 0.291242', '\\n', 'Epoch 28: training_error = 22.9677, training_loss =\n14.9904', '\\n', '  pseudoinverse: val_error = 0.1607, val_loss = 0.073248',\n'\\n', '  ista: val_error = 0.1607, val_loss = 0.073248', '\\n', '  fista:\nval_error = 0.1607, val_loss = 0.073248', '\\n', '  omp: val_error = 0.2461,\nval_loss = 0.297771', '\\n', 'Epoch 29: training_error = 22.5803, training_loss =\n14.5869', '\\n', '  pseudoinverse: val_error = 0.1653, val_loss = 0.077791',\n'\\n', '  ista: val_error = 0.1653, val_loss = 0.077791', '\\n', '  fista:\nval_error = 0.1653, val_loss = 0.077791', '\\n', '  omp: val_error = 0.2513,\nval_loss = 0.304477', '\\n', 'Epoch 30: training_error = 22.2014, training_loss =\n14.1975', '\\n', '  pseudoinverse: val_error = 0.1698, val_loss = 0.082408',\n'\\n', '  ista: val_error = 0.1698, val_loss = 0.082408', '\\n', '  fista:\nval_error = 0.1698, val_loss = 0.082408', '\\n', '  omp: val_error = 0.2565,\nval_loss = 0.311356', '\\n', 'Epoch 31: training_error = 21.8309, training_loss =\n13.8217', '\\n', '  pseudoinverse: val_error = 0.1743, val_loss = 0.087093',\n'\\n', '  ista: val_error = 0.1743, val_loss = 0.087093', '\\n', '  fista:\nval_error = 0.1743, val_loss = 0.087093', '\\n', '  omp: val_error = 0.2616,\nval_loss = 0.318400', '\\n', 'Epoch 32: training_error = 21.4686, training_loss =\n13.4589', '\\n', '  pseudoinverse: val_error = 0.1788, val_loss = 0.091844',\n'\\n', '  ista: val_error = 0.1788, val_loss = 0.091844', '\\n', '  fista:\nval_error = 0.1788, val_loss = 0.091844', '\\n', '  omp: val_error = 0.2667,\nval_loss = 0.325608', '\\n', 'Epoch 33: training_error = 21.1142, training_loss =\n13.1086', '\\n', '  pseudoinverse: val_error = 0.1831, val_loss = 0.096655',\n'\\n', '  ista: val_error = 0.1831, val_loss = 0.096655', '\\n', '  fista:\nval_error = 0.1831, val_loss = 0.096655', '\\n', '  omp: val_error = 0.2718,\nval_loss = 0.332976', '\\n', 'Epoch 34: training_error = 20.7676, training_loss =\n12.7703', '\\n', '  pseudoinverse: val_error = 0.1874, val_loss = 0.101524',\n'\\n', '  ista: val_error = 0.1874, val_loss = 0.101524', '\\n', '  fista:\nval_error = 0.1874, val_loss = 0.101524', '\\n', '  omp: val_error = 0.2769,\nval_loss = 0.340498', '\\n', 'Epoch 35: training_error = 20.4285, training_loss =\n12.4436', '\\n', '  pseudoinverse: val_error = 0.1917, val_loss = 0.106448',\n'\\n', '  ista: val_error = 0.1917, val_loss = 0.106448', '\\n', '  fista:\nval_error = 0.1917, val_loss = 0.106448', '\\n', '  omp: val_error = 0.2820,\nval_loss = 0.348170', '\\n', 'Epoch 36: training_error = 20.0968, training_loss =\n12.1278', '\\n', '  pseudoinverse: val_error = 0.1959, val_loss = 0.111423',\n'\\n', '  ista: val_error = 0.1959, val_loss = 0.111423', '\\n', '  fista:\nval_error = 0.1959, val_loss = 0.111423', '\\n', '  omp: val_error = 0.2871,\nval_loss = 0.355989', '\\n', 'Epoch 37: training_error = 19.7723, training_loss =\n11.8226', '\\n', '  pseudoinverse: val_error = 0.2001, val_loss = 0.116448',\n'\\n', '  ista: val_error = 0.2001, val_loss = 0.116448', '\\n', '  fista:\nval_error = 0.2001, val_loss = 0.116448', '\\n', '  omp: val_error = 0.2921,\nval_loss = 0.363950', '\\n', 'Epoch 38: training_error = 19.4547, training_loss =\n11.5275', '\\n', '  pseudoinverse: val_error = 0.2042, val_loss = 0.121520',\n'\\n', '  ista: val_error = 0.2042, val_loss = 0.121520', '\\n', '  fista:\nval_error = 0.2042, val_loss = 0.121520', '\\n', '  omp: val_error = 0.2971,\nval_loss = 0.372049', '\\n', 'Epoch 39: training_error = 19.1439, training_loss =\n11.2422', '\\n', '  pseudoinverse: val_error = 0.2082, val_loss = 0.126637',\n'\\n', '  ista: val_error = 0.2082, val_loss = 0.126637', '\\n', '  fista:\nval_error = 0.2082, val_loss = 0.126637', '\\n', '  omp: val_error = 0.3020,\nval_loss = 0.380271', '\\n', 'Epoch 40: training_error = 18.8397, training_loss =\n10.9662', '\\n', '  pseudoinverse: val_error = 0.2123, val_loss = 0.131797',\n'\\n', '  ista: val_error = 0.2123, val_loss = 0.131797', '\\n', '  fista:\nval_error = 0.2123, val_loss = 0.131797', '\\n', '  omp: val_error = 0.3070,\nval_loss = 0.388634', '\\n', 'Epoch 41: training_error = 18.5420, training_loss =\n10.6991', '\\n', '  pseudoinverse: val_error = 0.2162, val_loss = 0.137000',\n'\\n', '  ista: val_error = 0.2162, val_loss = 0.137000', '\\n', '  fista:\nval_error = 0.2162, val_loss = 0.137000', '\\n', '  omp: val_error = 0.3119,\nval_loss = 0.397125', '\\n', 'Epoch 42: training_error = 18.2504, training_loss =\n10.4406', '\\n', '  pseudoinverse: val_error = 0.2201, val_loss = 0.142244',\n'\\n', '  ista: val_error = 0.2201, val_loss = 0.142245', '\\n', '  fista:\nval_error = 0.2201, val_loss = 0.142245', '\\n', '  omp: val_error = 0.3168,\nval_loss = 0.405708', '\\n', 'Epoch 43: training_error = 17.9650, training_loss =\n10.1904', '\\n', '  pseudoinverse: val_error = 0.2240, val_loss = 0.147529',\n'\\n', '  ista: val_error = 0.2240, val_loss = 0.147529', '\\n', '  fista:\nval_error = 0.2240, val_loss = 0.147529', '\\n', '  omp: val_error = 0.3217,\nval_loss = 0.414422', '\\n', 'Epoch 44: training_error = 17.6855, training_loss =\n9.9479', '\\n', '  pseudoinverse: val_error = 0.2279, val_loss = 0.152854', '\\n',\n'  ista: val_error = 0.2279, val_loss = 0.152854', '\\n', '  fista: val_error =\n0.2279, val_loss = 0.152854', '\\n', '  omp: val_error = 0.3266, val_loss =\n0.423253', '\\n', 'Epoch 45: training_error = 17.4117, training_loss = 9.7131',\n'\\n', '  pseudoinverse: val_error = 0.2317, val_loss = 0.158219', '\\n', '  ista:\nval_error = 0.2317, val_loss = 0.158219', '\\n', '  fista: val_error = 0.2317,\nval_loss = 0.158219', '\\n', '  omp: val_error = 0.3314, val_loss = 0.432197',\n'\\n', 'Epoch 46: training_error = 17.1435, training_loss = 9.4855', '\\n', '\npseudoinverse: val_error = 0.2354, val_loss = 0.163623', '\\n', '  ista:\nval_error = 0.2354, val_loss = 0.163623', '\\n', '  fista: val_error = 0.2354,\nval_loss = 0.163623', '\\n', '  omp: val_error = 0.3362, val_loss = 0.441251',\n'\\n', 'Epoch 47: training_error = 16.8808, training_loss = 9.2648', '\\n', '\npseudoinverse: val_error = 0.2392, val_loss = 0.169067', '\\n', '  ista:\nval_error = 0.2392, val_loss = 0.169067', '\\n', '  fista: val_error = 0.2392,\nval_loss = 0.169067', '\\n', '  omp: val_error = 0.3410, val_loss = 0.450411',\n'\\n', 'Epoch 48: training_error = 16.6233, training_loss = 9.0509', '\\n', '\npseudoinverse: val_error = 0.2428, val_loss = 0.174551', '\\n', '  ista:\nval_error = 0.2428, val_loss = 0.174551', '\\n', '  fista: val_error = 0.2428,\nval_loss = 0.174551', '\\n', '  omp: val_error = 0.3458, val_loss = 0.459674',\n'\\n', 'Epoch 49: training_error = 16.3709, training_loss = 8.8433', '\\n', '\npseudoinverse: val_error = 0.2465, val_loss = 0.180076', '\\n', '  ista:\nval_error = 0.2465, val_loss = 0.180076', '\\n', '  fista: val_error = 0.2465,\nval_loss = 0.180076', '\\n', '  omp: val_error = 0.3506, val_loss = 0.469034',\n'\\n', 'Epoch 50: training_error = 16.1235, training_loss = 8.6419', '\\n', '\npseudoinverse: val_error = 0.2501, val_loss = 0.185641', '\\n', '  ista:\nval_error = 0.2501, val_loss = 0.185642', '\\n', '  fista: val_error = 0.2501,\nval_loss = 0.185641', '\\n', '  omp: val_error = 0.3553, val_loss = 0.478490',\n'\\n', 'Execution time: 4 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished run for schedule=fixed', '\\n', 'Finished\nrun for schedule=step_decay', '\\n', 'Finished run for schedule=exp_decay', '\\n',\n'Finished run for schedule=cosine', '\\n', 'Execution time: a second seconds\n(time limit is an hour).']", "['Using device: cuda', '\\n', 'Starting ablation for noise: gaussian', '\\n', '\nFinished beta1=0.5', '\\n', '  Finished beta1=0.7', '\\n', '  Finished beta1=0.9',\n'\\n', '  Finished beta1=0.99', '\\n', 'Completed noise ablation for: gaussian',\n'\\n', 'Starting ablation for noise: laplace', '\\n', '  Finished beta1=0.5',\n'\\n', '  Finished beta1=0.7', '\\n', '  Finished beta1=0.9', '\\n', '  Finished\nbeta1=0.99', '\\n', 'Completed noise ablation for: laplace', '\\n', 'Starting\nablation for noise: cauchy', '\\n', '  Finished beta1=0.5', '\\n', '  Finished\nbeta1=0.7', '\\n', '  Finished beta1=0.9', '\\n', '  Finished beta1=0.99', '\\n',\n'Completed noise ablation for: cauchy', '\\n', 'Saved experiment_data.npy', '\\n',\n'Execution time: 2 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished dimension=64', '\\n', 'Finished\ndimension=256', '\\n', 'Finished dimension=1024', '\\n', 'Finished\ndimension=4096', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: a\nsecond seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished run for code:dict updates = 1:1', '\\n',\n'Finished run for code:dict updates = 5:1', '\\n', 'Finished run for code:dict\nupdates = 10:1', '\\n', 'Finished run for code:dict updates = 1:5', '\\n',\n'Finished run for code:dict updates = 1:10', '\\n', 'Saved experiment_data.npy',\n'\\n', 'Execution time: 4 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished run for code:dict updates = 1:1', '\\n',\n'Finished run for code:dict updates = 5:1', '\\n', 'Finished run for code:dict\nupdates = 10:1', '\\n', 'Finished run for code:dict updates = 1:5', '\\n',\n'Finished run for code:dict updates = 1:10', '\\n', 'Saved experiment_data.npy',\n'\\n', 'Execution time: 3 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished run for code:dict updates = 1:1', '\\n',\n'Finished run for code:dict updates = 5:1', '\\n', 'Finished run for code:dict\nupdates = 10:1', '\\n', 'Finished run for code:dict updates = 1:5', '\\n',\n'Finished run for code:dict updates = 1:10', '\\n', 'Saved experiment_data.npy',\n'\\n', 'Execution time: 4 seconds seconds (time limit is an hour).']", ""], "analysis": ["", "", "", "", "", "", "", "", "", "", "", "", "The OMP routine computes D_S_pinv = torch.pinverse(D_S) (shape [dim, k]) and\nthen does D_S_pinv.matmul(w.unsqueeze(1)) (shape [dim, 1] \u00d7 [dim,1]), causing a\ndimension mismatch. The pseudoinverse should be taken on the transposed sub-\ndictionary so that its result has shape [k, dim], or you should transpose the\npinverse result before multiplying. For example:      D_S_pinv =\ntorch.pinverse(D_S.t())  # yields [k, dim]     x_S =\nD_S_pinv.matmul(w.unsqueeze(1)).squeeze(1)  Alternatively, use\ntorch.linalg.lstsq(D_S.t(), w.unsqueeze(1)).solution to solve for x_S directly.", "", "", "", "", "", "", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, "RuntimeError", null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, {"args": ["mat1 and mat2 shapes cannot be multiplied (1024x1 and 1024x1)"]}, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 131, "<module>", "codes_test = fn(D, W_test)"], ["runfile.py", 109, "<lambda>", "\"omp\": lambda D_, W_: omp(D_, W_, k_omp),"], ["runfile.py", 76, "omp", "x_S = D_S_pinv.matmul(w.unsqueeze(1)).squeeze(1)"]], null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training error", "lower_is_better": true, "description": "Final training error on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 14.5758, "best_value": 14.5758}]}, {"metric_name": "validation error", "lower_is_better": true, "description": "Final validation error on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.332078, "best_value": 0.217411}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 7.621816, "best_value": 7.621816}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.334691, "best_value": 0.137822}]}]}, {"metric_names": [{"metric_name": "training relative error", "lower_is_better": true, "description": "relative error on the training set", "data": [{"dataset_name": "ds1", "final_value": 17.538626, "best_value": 17.538626}, {"dataset_name": "ds2", "final_value": 1.495482, "best_value": 1.495482}, {"dataset_name": "ds3", "final_value": 1.298961, "best_value": 1.298961}]}, {"metric_name": "validation relative error", "lower_is_better": true, "description": "relative error on the validation set", "data": [{"dataset_name": "ds1", "final_value": 0.217411, "best_value": 0.217411}, {"dataset_name": "ds2", "final_value": 0.967739, "best_value": 0.967739}, {"dataset_name": "ds3", "final_value": 0.960261, "best_value": 0.960261}]}]}, {"metric_names": [{"metric_name": "train error", "lower_is_better": true, "description": "Final training error for the synthetic dataset across configurations", "data": [{"dataset_name": "synthetic", "final_value": 2.009281, "best_value": 2.009281}]}, {"metric_name": "validation error", "lower_is_better": true, "description": "Final validation error for the synthetic dataset across configurations", "data": [{"dataset_name": "synthetic", "final_value": 0.20995, "best_value": 0.20995}]}]}, {"metric_names": [{"metric_name": "training reconstruction error", "lower_is_better": true, "description": "Reconstruction error on the training set", "data": [{"dataset_name": "synthetic", "final_value": 16.121967, "best_value": 16.121967}]}, {"metric_name": "validation reconstruction error", "lower_is_better": true, "description": "Reconstruction error on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.250103, "best_value": 0.250103}]}, {"metric_name": "training reconstruction loss", "lower_is_better": true, "description": "Reconstruction loss on the training set", "data": [{"dataset_name": "synthetic", "final_value": 8.642943, "best_value": 8.641713}]}, {"metric_name": "validation reconstruction loss", "lower_is_better": true, "description": "Reconstruction loss on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.185579, "best_value": 0.185579}]}, {"metric_name": "code sparsity (fraction near zero)", "lower_is_better": false, "description": "Fraction of code elements near zero", "data": [{"dataset_name": "synthetic", "final_value": 0.002917, "best_value": 0.002917}]}, {"metric_name": "dictionary recovery error", "lower_is_better": true, "description": "Error in recovering the dictionary", "data": [{"dataset_name": "synthetic", "final_value": 0.374184, "best_value": 0.374184}]}]}, {"metric_names": [{"metric_name": "train relative error", "lower_is_better": true, "description": "Relative error on training set", "data": [{"dataset_name": "synthetic", "final_value": 1.7316, "best_value": 1.7316}]}, {"metric_name": "validation relative error", "lower_is_better": true, "description": "Relative error on validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.658, "best_value": 0.6554}]}]}, {"metric_names": [{"metric_name": "train error", "lower_is_better": true, "description": "Training error", "data": [{"dataset_name": "synthetic (noise level = 0.0)", "final_value": null, "best_value": null}, {"dataset_name": "synthetic (noise level = 0.005)", "final_value": 29.772352, "best_value": 29.772352}, {"dataset_name": "synthetic (noise level = 0.01)", "final_value": 16.134037, "best_value": 16.134037}, {"dataset_name": "synthetic (noise level = 0.02)", "final_value": 9.312593, "best_value": 9.312593}, {"dataset_name": "synthetic (noise level = 0.05)", "final_value": 5.204982, "best_value": 5.204982}]}, {"metric_name": "validation error", "lower_is_better": true, "description": "Validation error", "data": [{"dataset_name": "synthetic (noise level = 0.0)", "final_value": 0.246146, "best_value": 0.246146}, {"dataset_name": "synthetic (noise level = 0.005)", "final_value": 0.247228, "best_value": 0.247228}, {"dataset_name": "synthetic (noise level = 0.01)", "final_value": 0.249953, "best_value": 0.249953}, {"dataset_name": "synthetic (noise level = 0.02)", "final_value": 0.257838, "best_value": 0.257838}, {"dataset_name": "synthetic (noise level = 0.05)", "final_value": 0.284279, "best_value": 0.284279}]}]}, {"metric_names": [{"metric_name": "SGD training relative error", "lower_is_better": true, "description": "Relative error on synthetic training data using SGD optimizer", "data": [{"dataset_name": "synthetic", "final_value": 33.3201, "best_value": 33.3201}]}, {"metric_name": "SGD validation relative error", "lower_is_better": true, "description": "Relative error on synthetic validation data using SGD optimizer", "data": [{"dataset_name": "synthetic", "final_value": 0.0229, "best_value": 0.0229}]}, {"metric_name": "SGD training reconstruction loss", "lower_is_better": true, "description": "Reconstruction loss on synthetic training data using SGD optimizer", "data": [{"dataset_name": "synthetic", "final_value": 26.9503, "best_value": 26.9503}]}, {"metric_name": "SGD validation reconstruction loss", "lower_is_better": true, "description": "Reconstruction loss on synthetic validation data using SGD optimizer", "data": [{"dataset_name": "synthetic", "final_value": 0.0001, "best_value": 0.0001}]}, {"metric_name": "RMSprop training relative error", "lower_is_better": true, "description": "Relative error on synthetic training data using RMSprop optimizer", "data": [{"dataset_name": "synthetic", "final_value": 4.7871, "best_value": 4.7871}]}, {"metric_name": "RMSprop validation relative error", "lower_is_better": true, "description": "Relative error on synthetic validation data using RMSprop optimizer", "data": [{"dataset_name": "synthetic", "final_value": 0.4722, "best_value": 0.4722}]}, {"metric_name": "RMSprop training reconstruction loss", "lower_is_better": true, "description": "Reconstruction loss on synthetic training data using RMSprop optimizer", "data": [{"dataset_name": "synthetic", "final_value": 1.6136, "best_value": 1.6136}]}, {"metric_name": "RMSprop validation reconstruction loss", "lower_is_better": true, "description": "Reconstruction loss on synthetic validation data using RMSprop optimizer", "data": [{"dataset_name": "synthetic", "final_value": 0.6146, "best_value": 0.6146}]}, {"metric_name": "AdamW training relative error", "lower_is_better": true, "description": "Relative error on synthetic training data using AdamW optimizer", "data": [{"dataset_name": "synthetic", "final_value": 15.9703, "best_value": 15.9703}]}, {"metric_name": "AdamW validation relative error", "lower_is_better": true, "description": "Relative error on synthetic validation data using AdamW optimizer", "data": [{"dataset_name": "synthetic", "final_value": 0.2503, "best_value": 0.2503}]}, {"metric_name": "AdamW training reconstruction loss", "lower_is_better": true, "description": "Reconstruction loss on synthetic training data using AdamW optimizer", "data": [{"dataset_name": "synthetic", "final_value": 8.5223, "best_value": 8.5223}]}, {"metric_name": "AdamW validation reconstruction loss", "lower_is_better": true, "description": "Reconstruction loss on synthetic validation data using AdamW optimizer", "data": [{"dataset_name": "synthetic", "final_value": 0.1858, "best_value": 0.1858}]}]}, {"metric_names": [{"metric_name": "training mean relative error", "lower_is_better": true, "description": "Mean relative error on the training split", "data": [{"dataset_name": "synthetic (schedule 1:1)", "final_value": 15.878441, "best_value": 15.878441}, {"dataset_name": "synthetic (schedule 5:1)", "final_value": 2.927579, "best_value": 2.927579}, {"dataset_name": "synthetic (schedule 10:1)", "final_value": 0.408293, "best_value": 0.408293}, {"dataset_name": "synthetic (schedule 1:5)", "final_value": 7.103316, "best_value": 7.103316}, {"dataset_name": "synthetic (schedule 1:10)", "final_value": 6.127098, "best_value": 6.127098}]}, {"metric_name": "validation mean relative error", "lower_is_better": true, "description": "Mean relative error on the validation split", "data": [{"dataset_name": "synthetic (schedule 1:1)", "final_value": 0.250681, "best_value": 0.250681}, {"dataset_name": "synthetic (schedule 5:1)", "final_value": 0.196716, "best_value": 0.196716}, {"dataset_name": "synthetic (schedule 10:1)", "final_value": 0.133907, "best_value": 0.133907}, {"dataset_name": "synthetic (schedule 1:5)", "final_value": 0.588157, "best_value": 0.588157}, {"dataset_name": "synthetic (schedule 1:10)", "final_value": 0.414043, "best_value": 0.414043}]}, {"metric_name": "training MSE loss", "lower_is_better": true, "description": "MSE loss on the training split", "data": [{"dataset_name": "synthetic (schedule 1:1)", "final_value": 8.430651, "best_value": 8.430651}, {"dataset_name": "synthetic (schedule 5:1)", "final_value": 1.798701, "best_value": 1.798701}, {"dataset_name": "synthetic (schedule 10:1)", "final_value": 0.386549, "best_value": 0.386549}, {"dataset_name": "synthetic (schedule 1:5)", "final_value": 2.208671, "best_value": 2.208671}, {"dataset_name": "synthetic (schedule 1:10)", "final_value": 1.523779, "best_value": 1.523779}]}, {"metric_name": "validation MSE loss", "lower_is_better": true, "description": "MSE loss on the validation split", "data": [{"dataset_name": "synthetic (schedule 1:1)", "final_value": 0.186374, "best_value": 0.186374}, {"dataset_name": "synthetic (schedule 5:1)", "final_value": 0.114477, "best_value": 0.114477}, {"dataset_name": "synthetic (schedule 10:1)", "final_value": 0.050698, "best_value": 0.050698}, {"dataset_name": "synthetic (schedule 1:5)", "final_value": 1.018526, "best_value": 1.018526}, {"dataset_name": "synthetic (schedule 1:10)", "final_value": 0.456286, "best_value": 0.456286}]}]}, {"metric_names": [{"metric_name": "train error", "lower_is_better": true, "description": "Training error at the final epoch on the synthetic dataset for each initialization scheme", "data": [{"dataset_name": "synthetic (D init=normal, codes init=normal)", "final_value": 16.123459, "best_value": 16.123459}, {"dataset_name": "synthetic (D init=normal, codes init=xavier_uni)", "final_value": 0.751091, "best_value": 0.751091}, {"dataset_name": "synthetic (D init=normal, codes init=orthogonal)", "final_value": 0.65323, "best_value": 0.65323}, {"dataset_name": "synthetic (D init=normal, codes init=zeros)", "final_value": 0.46858, "best_value": 0.46858}, {"dataset_name": "synthetic (D init=xavier_uni, codes init=normal)", "final_value": 3.348321, "best_value": 3.348321}, {"dataset_name": "synthetic (D init=xavier_uni, codes init=xavier_uni)", "final_value": 0.604678, "best_value": 0.604678}, {"dataset_name": "synthetic (D init=xavier_uni, codes init=orthogonal)", "final_value": 0.582282, "best_value": 0.582282}, {"dataset_name": "synthetic (D init=xavier_uni, codes init=zeros)", "final_value": 0.610414, "best_value": 0.610414}, {"dataset_name": "synthetic (D init=orthogonal, codes init=normal)", "final_value": 3.417238, "best_value": 3.417238}, {"dataset_name": "synthetic (D init=orthogonal, codes init=xavier_uni)", "final_value": 0.603755, "best_value": 0.603755}, {"dataset_name": "synthetic (D init=orthogonal, codes init=orthogonal)", "final_value": 0.578523, "best_value": 0.578523}, {"dataset_name": "synthetic (D init=orthogonal, codes init=zeros)", "final_value": 0.626083, "best_value": 0.626083}, {"dataset_name": "synthetic (D init=zeros, codes init=normal)", "final_value": 4.355175, "best_value": 4.355175}, {"dataset_name": "synthetic (D init=zeros, codes init=xavier_uni)", "final_value": 0.595959, "best_value": 0.595959}, {"dataset_name": "synthetic (D init=zeros, codes init=orthogonal)", "final_value": 0.601275, "best_value": 0.601275}, {"dataset_name": "synthetic (D init=zeros, codes init=zeros)", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation error", "lower_is_better": true, "description": "Validation error at the final epoch on the synthetic dataset for each initialization scheme", "data": [{"dataset_name": "synthetic (D init=normal, codes init=normal)", "final_value": 0.250144, "best_value": 0.250144}, {"dataset_name": "synthetic (D init=normal, codes init=xavier_uni)", "final_value": 0.209242, "best_value": 0.209242}, {"dataset_name": "synthetic (D init=normal, codes init=orthogonal)", "final_value": 0.188502, "best_value": 0.188502}, {"dataset_name": "synthetic (D init=normal, codes init=zeros)", "final_value": 0.244881, "best_value": 0.244881}, {"dataset_name": "synthetic (D init=xavier_uni, codes init=normal)", "final_value": 0.442804, "best_value": 0.442804}, {"dataset_name": "synthetic (D init=xavier_uni, codes init=xavier_uni)", "final_value": 0.637429, "best_value": 0.637429}, {"dataset_name": "synthetic (D init=xavier_uni, codes init=orthogonal)", "final_value": 0.614484, "best_value": 0.614484}, {"dataset_name": "synthetic (D init=xavier_uni, codes init=zeros)", "final_value": 0.651446, "best_value": 0.651446}, {"dataset_name": "synthetic (D init=orthogonal, codes init=normal)", "final_value": 0.438378, "best_value": 0.438378}, {"dataset_name": "synthetic (D init=orthogonal, codes init=xavier_uni)", "final_value": 0.613117, "best_value": 0.613117}, {"dataset_name": "synthetic (D init=orthogonal, codes init=orthogonal)", "final_value": 0.611485, "best_value": 0.611485}, {"dataset_name": "synthetic (D init=orthogonal, codes init=zeros)", "final_value": 0.64434, "best_value": 0.64434}, {"dataset_name": "synthetic (D init=zeros, codes init=normal)", "final_value": 0.443011, "best_value": 0.443011}, {"dataset_name": "synthetic (D init=zeros, codes init=xavier_uni)", "final_value": 0.611067, "best_value": 0.611067}, {"dataset_name": "synthetic (D init=zeros, codes init=orthogonal)", "final_value": 0.641001, "best_value": 0.641001}, {"dataset_name": "synthetic (D init=zeros, codes init=zeros)", "final_value": 1.0, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "train reconstruction error", "lower_is_better": true, "description": "Final training reconstruction error", "data": [{"dataset_name": "synthetic", "final_value": 4.518011, "best_value": 4.518011}]}, {"metric_name": "validation reconstruction error", "lower_is_better": true, "description": "Lowest validation reconstruction error", "data": [{"dataset_name": "synthetic", "final_value": 0.379591, "best_value": 0.250144}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "synthetic", "final_value": 1.212506, "best_value": 1.212506}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Lowest validation loss", "data": [{"dataset_name": "synthetic", "final_value": 0.370475, "best_value": 0.185641}]}]}, {"metric_names": [{"metric_name": "training relative error", "lower_is_better": true, "description": "Relative error on the training set", "data": [{"dataset_name": "none", "final_value": 16.123579, "best_value": 16.123579}, {"dataset_name": "l1", "final_value": 16.123459, "best_value": 16.123459}, {"dataset_name": "l2", "final_value": 16.123398, "best_value": 16.123398}, {"dataset_name": "elasticnet", "final_value": 16.123224, "best_value": 16.123224}]}, {"metric_name": "validation relative error", "lower_is_better": true, "description": "Relative error on the validation set", "data": [{"dataset_name": "none", "final_value": 0.25015, "best_value": 0.25015}, {"dataset_name": "l1", "final_value": 0.250144, "best_value": 0.250144}, {"dataset_name": "l2", "final_value": 0.250146, "best_value": 0.250146}, {"dataset_name": "elasticnet", "final_value": 0.250141, "best_value": 0.250141}]}]}, {"metric_names": [{"metric_name": "training normalized error", "lower_is_better": true, "description": "Normalized error on training set", "data": [{"dataset_name": "synthetic", "final_value": 16.44014, "best_value": 16.123459}]}, {"metric_name": "validation normalized error", "lower_is_better": true, "description": "Normalized error on validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.238981, "best_value": 0.238981}]}, {"metric_name": "training MSE loss", "lower_is_better": true, "description": "Mean squared error loss on training set", "data": [{"dataset_name": "synthetic", "final_value": 10.407796, "best_value": 8.641868}]}, {"metric_name": "validation MSE loss", "lower_is_better": true, "description": "Mean squared error loss on validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.172263, "best_value": 0.172263}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training error", "lower_is_better": true, "description": "Training error of the model", "data": [{"dataset_name": "bernoulli_gaussian", "final_value": 14.5758, "best_value": 14.5758}, {"dataset_name": "bernoulli_laplace", "final_value": 9.460361, "best_value": 9.460361}, {"dataset_name": "bernoulli_uniform", "final_value": 9.074814, "best_value": 9.074814}, {"dataset_name": "block_sparse", "final_value": 1.127977, "best_value": 1.127977}]}, {"metric_name": "validation error", "lower_is_better": true, "description": "Validation error of the model", "data": [{"dataset_name": "bernoulli_gaussian", "final_value": 0.217411, "best_value": 0.217411}, {"dataset_name": "bernoulli_laplace", "final_value": 0.257739, "best_value": 0.257739}, {"dataset_name": "bernoulli_uniform", "final_value": 0.242588, "best_value": 0.242588}, {"dataset_name": "block_sparse", "final_value": 0.203753, "best_value": 0.203753}]}]}, {"metric_names": [{"metric_name": "training relative reconstruction error (MSE)", "lower_is_better": true, "description": "Relative reconstruction error on the training dataset using mean squared error (MSE) loss for various beta1 values.", "data": [{"dataset_name": "synthetic (beta1=0.5)", "final_value": 17.538626, "best_value": 17.538626}, {"dataset_name": "synthetic (beta1=0.7)", "final_value": 17.26347, "best_value": 17.26347}, {"dataset_name": "synthetic (beta1=0.9)", "final_value": 16.123459, "best_value": 16.123459}, {"dataset_name": "synthetic (beta1=0.99)", "final_value": 14.5758, "best_value": 14.5758}]}, {"metric_name": "validation relative reconstruction error (MSE)", "lower_is_better": true, "description": "Relative reconstruction error on the validation dataset using mean squared error (MSE) loss for various beta1 values.", "data": [{"dataset_name": "synthetic (beta1=0.5)", "final_value": 0.217411, "best_value": 0.217411}, {"dataset_name": "synthetic (beta1=0.7)", "final_value": 0.222304, "best_value": 0.222304}, {"dataset_name": "synthetic (beta1=0.9)", "final_value": 0.250144, "best_value": 0.250144}, {"dataset_name": "synthetic (beta1=0.99)", "final_value": 0.332078, "best_value": 0.332078}]}, {"metric_name": "training mean squared error loss", "lower_is_better": true, "description": "Mean squared error (MSE) loss on the training dataset for various beta1 values.", "data": [{"dataset_name": "synthetic (beta1=0.5)", "final_value": 9.616315, "best_value": 9.616315}, {"dataset_name": "synthetic (beta1=0.7)", "final_value": 9.420985, "best_value": 9.420985}, {"dataset_name": "synthetic (beta1=0.9)", "final_value": 8.641868, "best_value": 8.641868}, {"dataset_name": "synthetic (beta1=0.99)", "final_value": 7.621816, "best_value": 7.621816}]}, {"metric_name": "validation mean squared error loss", "lower_is_better": true, "description": "Mean squared error (MSE) loss on the validation dataset for various beta1 values.", "data": [{"dataset_name": "synthetic (beta1=0.5)", "final_value": 0.137822, "best_value": 0.137822}, {"dataset_name": "synthetic (beta1=0.7)", "final_value": 0.144676, "best_value": 0.144676}, {"dataset_name": "synthetic (beta1=0.9)", "final_value": 0.185641, "best_value": 0.185641}, {"dataset_name": "synthetic (beta1=0.99)", "final_value": 0.334691, "best_value": 0.334691}]}, {"metric_name": "training relative reconstruction error (MAE)", "lower_is_better": true, "description": "Relative reconstruction error on the training dataset using mean absolute error (MAE) loss for various beta1 values.", "data": [{"dataset_name": "synthetic (beta1=0.5)", "final_value": 15.125069, "best_value": 15.125069}, {"dataset_name": "synthetic (beta1=0.7)", "final_value": 15.024547, "best_value": 15.024547}, {"dataset_name": "synthetic (beta1=0.9)", "final_value": 14.62897, "best_value": 14.62897}, {"dataset_name": "synthetic (beta1=0.99)", "final_value": 14.264664, "best_value": 14.264664}]}, {"metric_name": "validation relative reconstruction error (MAE)", "lower_is_better": true, "description": "Relative reconstruction error on the validation dataset using mean absolute error (MAE) loss for various beta1 values.", "data": [{"dataset_name": "synthetic (beta1=0.5)", "final_value": 0.263467, "best_value": 0.263467}, {"dataset_name": "synthetic (beta1=0.7)", "final_value": 0.265496, "best_value": 0.265496}, {"dataset_name": "synthetic (beta1=0.9)", "final_value": 0.279651, "best_value": 0.279651}, {"dataset_name": "synthetic (beta1=0.99)", "final_value": 0.327714, "best_value": 0.327714}]}, {"metric_name": "training mean absolute error loss", "lower_is_better": true, "description": "Mean absolute error (MAE) loss on the training dataset for various beta1 values.", "data": [{"dataset_name": "synthetic (beta1=0.5)", "final_value": 2.069152, "best_value": 2.069152}, {"dataset_name": "synthetic (beta1=0.7)", "final_value": 2.061745, "best_value": 2.061745}, {"dataset_name": "synthetic (beta1=0.9)", "final_value": 2.036294, "best_value": 2.036294}, {"dataset_name": "synthetic (beta1=0.99)", "final_value": 2.03138, "best_value": 2.03138}]}, {"metric_name": "validation mean absolute error loss", "lower_is_better": true, "description": "Mean absolute error (MAE) loss on the validation dataset for various beta1 values.", "data": [{"dataset_name": "synthetic (beta1=0.5)", "final_value": 0.303676, "best_value": 0.303676}, {"dataset_name": "synthetic (beta1=0.7)", "final_value": 0.306237, "best_value": 0.306237}, {"dataset_name": "synthetic (beta1=0.9)", "final_value": 0.323859, "best_value": 0.323859}, {"dataset_name": "synthetic (beta1=0.99)", "final_value": 0.383047, "best_value": 0.383047}]}, {"metric_name": "training relative reconstruction error (Huber)", "lower_is_better": true, "description": "Relative reconstruction error on the training dataset using Huber loss for various beta1 values.", "data": [{"dataset_name": "synthetic (beta1=0.5)", "final_value": 14.935898, "best_value": 14.935898}, {"dataset_name": "synthetic (beta1=0.7)", "final_value": 14.838631, "best_value": 14.838631}, {"dataset_name": "synthetic (beta1=0.9)", "final_value": 14.448506, "best_value": 14.448506}, {"dataset_name": "synthetic (beta1=0.99)", "final_value": 14.125102, "best_value": 14.125102}]}, {"metric_name": "validation relative reconstruction error (Huber)", "lower_is_better": true, "description": "Relative reconstruction error on the validation dataset using Huber loss for various beta1 values.", "data": [{"dataset_name": "synthetic (beta1=0.5)", "final_value": 0.279978, "best_value": 0.279978}, {"dataset_name": "synthetic (beta1=0.7)", "final_value": 0.281664, "best_value": 0.281664}, {"dataset_name": "synthetic (beta1=0.9)", "final_value": 0.296259, "best_value": 0.296259}, {"dataset_name": "synthetic (beta1=0.99)", "final_value": 0.349288, "best_value": 0.349288}]}, {"metric_name": "training Huber loss", "lower_is_better": true, "description": "Huber loss on the training dataset for various beta1 values.", "data": [{"dataset_name": "synthetic (beta1=0.5)", "final_value": 1.631637, "best_value": 1.631637}, {"dataset_name": "synthetic (beta1=0.7)", "final_value": 1.625096, "best_value": 1.625096}, {"dataset_name": "synthetic (beta1=0.9)", "final_value": 1.598281, "best_value": 1.598281}, {"dataset_name": "synthetic (beta1=0.99)", "final_value": 1.585654, "best_value": 1.585654}]}, {"metric_name": "validation Huber loss", "lower_is_better": true, "description": "Huber loss on the validation dataset for various beta1 values.", "data": [{"dataset_name": "synthetic (beta1=0.5)", "final_value": 0.111086, "best_value": 0.111086}, {"dataset_name": "synthetic (beta1=0.7)", "final_value": 0.112585, "best_value": 0.112585}, {"dataset_name": "synthetic (beta1=0.9)", "final_value": 0.124017, "best_value": 0.124017}, {"dataset_name": "synthetic (beta1=0.99)", "final_value": 0.166512, "best_value": 0.166512}]}]}, {"metric_names": [{"metric_name": "training error", "lower_is_better": true, "description": "Final training error", "data": [{"dataset_name": "synthetic", "final_value": 16.1235, "best_value": 16.1235}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "synthetic", "final_value": 8.641868, "best_value": 8.641868}]}, {"metric_name": "validation error (pseudoinverse)", "lower_is_better": true, "description": "Final validation error using pseudoinverse", "data": [{"dataset_name": "synthetic", "final_value": 0.2501, "best_value": 0.2501}]}, {"metric_name": "validation error (ista)", "lower_is_better": true, "description": "Final validation error using ISTA", "data": [{"dataset_name": "synthetic", "final_value": 0.2501, "best_value": 0.2501}]}, {"metric_name": "validation error (fista)", "lower_is_better": true, "description": "Final validation error using FISTA", "data": [{"dataset_name": "synthetic", "final_value": 0.2501, "best_value": 0.2501}]}, {"metric_name": "validation error (omp)", "lower_is_better": true, "description": "Final validation error using OMP", "data": [{"dataset_name": "synthetic", "final_value": 0.3553, "best_value": 0.3553}]}, {"metric_name": "validation loss (pseudoinverse)", "lower_is_better": true, "description": "Final validation loss using pseudoinverse", "data": [{"dataset_name": "synthetic", "final_value": 0.185641, "best_value": 0.185641}]}, {"metric_name": "validation loss (ista)", "lower_is_better": true, "description": "Final validation loss using ISTA", "data": [{"dataset_name": "synthetic", "final_value": 0.185642, "best_value": 0.185642}]}, {"metric_name": "validation loss (fista)", "lower_is_better": true, "description": "Final validation loss using FISTA", "data": [{"dataset_name": "synthetic", "final_value": 0.185641, "best_value": 0.185641}]}, {"metric_name": "validation loss (omp)", "lower_is_better": true, "description": "Final validation loss using OMP", "data": [{"dataset_name": "synthetic", "final_value": 0.47849, "best_value": 0.47849}]}]}, {"metric_names": [{"metric_name": "training reconstruction error", "lower_is_better": true, "description": "Final reconstruction error on training dataset", "data": [{"dataset_name": "synthetic (fixed)", "final_value": 16.123459, "best_value": 16.123459}, {"dataset_name": "synthetic (step_decay)", "final_value": 27.547682, "best_value": 27.547682}, {"dataset_name": "synthetic (exp_decay)", "final_value": 26.802723, "best_value": 26.802723}, {"dataset_name": "synthetic (cosine)", "final_value": 23.655577, "best_value": 23.655577}]}, {"metric_name": "validation reconstruction error", "lower_is_better": true, "description": "Final reconstruction error on validation dataset", "data": [{"dataset_name": "synthetic (fixed)", "final_value": 0.250144, "best_value": 0.250144}, {"dataset_name": "synthetic (step_decay)", "final_value": 0.103589, "best_value": 0.103589}, {"dataset_name": "synthetic (exp_decay)", "final_value": 0.109744, "best_value": 0.109744}, {"dataset_name": "synthetic (cosine)", "final_value": 0.145139, "best_value": 0.145139}]}, {"metric_name": "training MSE loss", "lower_is_better": true, "description": "Final mean squared error loss on training dataset", "data": [{"dataset_name": "synthetic (fixed)", "final_value": 8.641868, "best_value": 8.641868}, {"dataset_name": "synthetic (step_decay)", "final_value": 20.171721, "best_value": 20.171721}, {"dataset_name": "synthetic (exp_decay)", "final_value": 19.266787, "best_value": 19.266787}, {"dataset_name": "synthetic (cosine)", "final_value": 15.709567, "best_value": 15.709567}]}, {"metric_name": "validation MSE loss", "lower_is_better": true, "description": "Final mean squared error loss on validation dataset", "data": [{"dataset_name": "synthetic (fixed)", "final_value": 0.185641, "best_value": 0.185641}, {"dataset_name": "synthetic (step_decay)", "final_value": 0.027945, "best_value": 0.027945}, {"dataset_name": "synthetic (exp_decay)", "final_value": 0.031825, "best_value": 0.031825}, {"dataset_name": "synthetic (cosine)", "final_value": 0.05884, "best_value": 0.05884}]}]}, {"metric_names": [{"metric_name": "train reconstruction error", "lower_is_better": true, "description": "Reconstruction error on the training set.", "data": [{"dataset_name": "gaussian", "final_value": 14.5758, "best_value": 14.5758}, {"dataset_name": "laplace", "final_value": 11.006574, "best_value": 11.006574}, {"dataset_name": "cauchy", "final_value": 2.327018, "best_value": 2.327018}]}, {"metric_name": "validation reconstruction error", "lower_is_better": true, "description": "Reconstruction error on the validation set.", "data": [{"dataset_name": "gaussian", "final_value": 0.217411, "best_value": 0.217411}, {"dataset_name": "laplace", "final_value": 0.220058, "best_value": 0.220058}, {"dataset_name": "cauchy", "final_value": 0.491817, "best_value": 0.491817}]}]}, {"metric_names": [{"metric_name": "training reconstruction loss", "lower_is_better": true, "description": "Reconstruction loss on the training set", "data": [{"dataset_name": "synthetic (dimension=64)", "final_value": 7.51702, "best_value": 7.51702}, {"dataset_name": "synthetic (dimension=256)", "final_value": 8.599186, "best_value": 8.599186}, {"dataset_name": "synthetic (dimension=1024)", "final_value": 8.806726, "best_value": 8.806726}, {"dataset_name": "synthetic (dimension=4096)", "final_value": 8.906409, "best_value": 8.906409}]}, {"metric_name": "validation reconstruction loss", "lower_is_better": true, "description": "Reconstruction loss on the validation set", "data": [{"dataset_name": "synthetic (dimension=64)", "final_value": 1.413808, "best_value": 1.413808}, {"dataset_name": "synthetic (dimension=256)", "final_value": 2.704189, "best_value": 2.704189}, {"dataset_name": "synthetic (dimension=1024)", "final_value": 2.956825, "best_value": 2.956825}, {"dataset_name": "synthetic (dimension=4096)", "final_value": 3.026514, "best_value": 3.026514}]}, {"metric_name": "training relative error", "lower_is_better": true, "description": "Relative error on the training set", "data": [{"dataset_name": "synthetic (dimension=64)", "final_value": 19.249327, "best_value": 19.249327}, {"dataset_name": "synthetic (dimension=256)", "final_value": 20.850634, "best_value": 20.850634}, {"dataset_name": "synthetic (dimension=1024)", "final_value": 21.287039, "best_value": 21.287039}, {"dataset_name": "synthetic (dimension=4096)", "final_value": 21.571699, "best_value": 21.571699}]}, {"metric_name": "validation relative error", "lower_is_better": true, "description": "Relative error on the validation set", "data": [{"dataset_name": "synthetic (dimension=64)", "final_value": 0.679548, "best_value": 0.679548}, {"dataset_name": "synthetic (dimension=256)", "final_value": 0.928862, "best_value": 0.928862}, {"dataset_name": "synthetic (dimension=1024)", "final_value": 0.978556, "best_value": 0.978556}, {"dataset_name": "synthetic (dimension=4096)", "final_value": 0.988734, "best_value": 0.988734}]}]}, {"metric_names": [{"metric_name": "training mean relative error", "lower_is_better": true, "description": "Mean relative error on the training set", "data": [{"dataset_name": "synthetic (schedule 1:1)", "final_value": 15.878441, "best_value": 15.878441}, {"dataset_name": "synthetic (schedule 5:1)", "final_value": 2.927579, "best_value": 2.927579}, {"dataset_name": "synthetic (schedule 10:1)", "final_value": 0.408293, "best_value": 0.408293}, {"dataset_name": "synthetic (schedule 1:5)", "final_value": 7.103316, "best_value": 7.103316}, {"dataset_name": "synthetic (schedule 1:10)", "final_value": 6.127098, "best_value": 6.127098}]}, {"metric_name": "validation mean relative error", "lower_is_better": true, "description": "Mean relative error on the validation set", "data": [{"dataset_name": "synthetic (schedule 1:1)", "final_value": 0.250681, "best_value": 0.250681}, {"dataset_name": "synthetic (schedule 5:1)", "final_value": 0.196716, "best_value": 0.196716}, {"dataset_name": "synthetic (schedule 10:1)", "final_value": 0.133907, "best_value": 0.133907}, {"dataset_name": "synthetic (schedule 1:5)", "final_value": 0.588157, "best_value": 0.588157}, {"dataset_name": "synthetic (schedule 1:10)", "final_value": 0.414043, "best_value": 0.414043}]}, {"metric_name": "training MSE loss", "lower_is_better": true, "description": "Mean squared error loss on the training set", "data": [{"dataset_name": "synthetic (schedule 1:1)", "final_value": 8.430651, "best_value": 8.430651}, {"dataset_name": "synthetic (schedule 5:1)", "final_value": 1.798701, "best_value": 1.798701}, {"dataset_name": "synthetic (schedule 10:1)", "final_value": 0.386549, "best_value": 0.386549}, {"dataset_name": "synthetic (schedule 1:5)", "final_value": 2.208671, "best_value": 2.208671}, {"dataset_name": "synthetic (schedule 1:10)", "final_value": 1.523779, "best_value": 1.523779}]}, {"metric_name": "validation MSE loss", "lower_is_better": true, "description": "Mean squared error loss on the validation set", "data": [{"dataset_name": "synthetic (schedule 1:1)", "final_value": 0.186374, "best_value": 0.186374}, {"dataset_name": "synthetic (schedule 5:1)", "final_value": 0.114477, "best_value": 0.114477}, {"dataset_name": "synthetic (schedule 10:1)", "final_value": 0.050698, "best_value": 0.050698}, {"dataset_name": "synthetic (schedule 1:5)", "final_value": 1.018526, "best_value": 1.018526}, {"dataset_name": "synthetic (schedule 1:10)", "final_value": 0.456286, "best_value": 0.456286}]}]}, {"metric_names": [{"metric_name": "training mean relative error", "lower_is_better": true, "description": "Mean relative error on the training dataset", "data": [{"dataset_name": "synthetic (schedule 1:1)", "final_value": 15.878441, "best_value": 15.878441}, {"dataset_name": "synthetic (schedule 5:1)", "final_value": 2.927579, "best_value": 2.927579}, {"dataset_name": "synthetic (schedule 10:1)", "final_value": 0.408293, "best_value": 0.408293}, {"dataset_name": "synthetic (schedule 1:5)", "final_value": 7.103316, "best_value": 7.103316}, {"dataset_name": "synthetic (schedule 1:10)", "final_value": 6.127098, "best_value": 6.127098}]}, {"metric_name": "validation mean relative error", "lower_is_better": true, "description": "Mean relative error on the validation dataset", "data": [{"dataset_name": "synthetic (schedule 1:1)", "final_value": 0.250681, "best_value": 0.250681}, {"dataset_name": "synthetic (schedule 5:1)", "final_value": 0.196716, "best_value": 0.196716}, {"dataset_name": "synthetic (schedule 10:1)", "final_value": 0.133907, "best_value": 0.133907}, {"dataset_name": "synthetic (schedule 1:5)", "final_value": 0.588157, "best_value": 0.588157}, {"dataset_name": "synthetic (schedule 1:10)", "final_value": 0.414043, "best_value": 0.414043}]}, {"metric_name": "training MSE loss", "lower_is_better": true, "description": "Mean squared error loss on the training dataset", "data": [{"dataset_name": "synthetic (schedule 1:1)", "final_value": 8.430651, "best_value": 8.430651}, {"dataset_name": "synthetic (schedule 5:1)", "final_value": 1.798701, "best_value": 1.798701}, {"dataset_name": "synthetic (schedule 10:1)", "final_value": 0.386549, "best_value": 0.386549}, {"dataset_name": "synthetic (schedule 1:5)", "final_value": 2.208671, "best_value": 2.208671}, {"dataset_name": "synthetic (schedule 1:10)", "final_value": 1.523779, "best_value": 1.523779}]}, {"metric_name": "validation MSE loss", "lower_is_better": true, "description": "Mean squared error loss on the validation dataset", "data": [{"dataset_name": "synthetic (schedule 1:1)", "final_value": 0.186374, "best_value": 0.186374}, {"dataset_name": "synthetic (schedule 5:1)", "final_value": 0.114477, "best_value": 0.114477}, {"dataset_name": "synthetic (schedule 10:1)", "final_value": 0.050698, "best_value": 0.050698}, {"dataset_name": "synthetic (schedule 1:5)", "final_value": 1.018526, "best_value": 1.018526}, {"dataset_name": "synthetic (schedule 1:10)", "final_value": 0.456286, "best_value": 0.456286}]}]}, {"metric_names": [{"metric_name": "train mean relative error", "lower_is_better": true, "description": "Mean relative error on the training split", "data": [{"dataset_name": "synthetic (schedule 1:1)", "final_value": 15.878441, "best_value": 15.878441}, {"dataset_name": "synthetic (schedule 5:1)", "final_value": 2.927579, "best_value": 2.927579}, {"dataset_name": "synthetic (schedule 10:1)", "final_value": 0.408293, "best_value": 0.408293}, {"dataset_name": "synthetic (schedule 1:5)", "final_value": 7.103316, "best_value": 7.103316}, {"dataset_name": "synthetic (schedule 1:10)", "final_value": 6.127098, "best_value": 6.127098}]}, {"metric_name": "validation mean relative error", "lower_is_better": true, "description": "Mean relative error on the validation split", "data": [{"dataset_name": "synthetic (schedule 1:1)", "final_value": 0.250681, "best_value": 0.250681}, {"dataset_name": "synthetic (schedule 5:1)", "final_value": 0.196716, "best_value": 0.196716}, {"dataset_name": "synthetic (schedule 10:1)", "final_value": 0.133907, "best_value": 0.133907}, {"dataset_name": "synthetic (schedule 1:5)", "final_value": 0.588157, "best_value": 0.588157}, {"dataset_name": "synthetic (schedule 1:10)", "final_value": 0.414043, "best_value": 0.414043}]}, {"metric_name": "train MSE loss", "lower_is_better": true, "description": "Mean squared error loss on the training split", "data": [{"dataset_name": "synthetic (schedule 1:1)", "final_value": 8.430651, "best_value": 8.430651}, {"dataset_name": "synthetic (schedule 5:1)", "final_value": 1.798701, "best_value": 1.798701}, {"dataset_name": "synthetic (schedule 10:1)", "final_value": 0.386549, "best_value": 0.386549}, {"dataset_name": "synthetic (schedule 1:5)", "final_value": 2.208671, "best_value": 2.208671}, {"dataset_name": "synthetic (schedule 1:10)", "final_value": 1.523779, "best_value": 1.523779}]}, {"metric_name": "validation MSE loss", "lower_is_better": true, "description": "Mean squared error loss on the validation split", "data": [{"dataset_name": "synthetic (schedule 1:1)", "final_value": 0.186374, "best_value": 0.186374}, {"dataset_name": "synthetic (schedule 5:1)", "final_value": 0.114477, "best_value": 0.114477}, {"dataset_name": "synthetic (schedule 10:1)", "final_value": 0.050698, "best_value": 0.050698}, {"dataset_name": "synthetic (schedule 1:5)", "final_value": 1.018526, "best_value": 1.018526}, {"dataset_name": "synthetic (schedule 1:10)", "final_value": 0.456286, "best_value": 0.456286}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_6effbbcb54b241c9b3db94d9c6486930_proc_106393/synthetic_training_loss.png", "../../logs/0-run/experiment_results/experiment_6effbbcb54b241c9b3db94d9c6486930_proc_106393/synthetic_training_error.png", "../../logs/0-run/experiment_results/experiment_6effbbcb54b241c9b3db94d9c6486930_proc_106393/synthetic_validation_error.png", "../../logs/0-run/experiment_results/experiment_6effbbcb54b241c9b3db94d9c6486930_proc_106393/synthetic_sample_reconstruction_beta1_0.5.png", "../../logs/0-run/experiment_results/experiment_6effbbcb54b241c9b3db94d9c6486930_proc_106393/synthetic_validation_loss.png"], ["../../logs/0-run/experiment_results/experiment_cb095268033542b0a36a5eb8d437fc1e_proc_119934/multi_synthetic_ds3_error_loss.png", "../../logs/0-run/experiment_results/experiment_cb095268033542b0a36a5eb8d437fc1e_proc_119934/multi_synthetic_ds2_error_loss.png", "../../logs/0-run/experiment_results/experiment_cb095268033542b0a36a5eb8d437fc1e_proc_119934/multi_synthetic_ds1_error_loss.png"], ["../../logs/0-run/experiment_results/experiment_6816f45789ae4a8ab42fd499c5902239_proc_119935/synthetic_loss_curves_nc10.png", "../../logs/0-run/experiment_results/experiment_6816f45789ae4a8ab42fd499c5902239_proc_119935/synthetic_error_curves_nc60.png", "../../logs/0-run/experiment_results/experiment_6816f45789ae4a8ab42fd499c5902239_proc_119935/synthetic_error_curves_nc30.png", "../../logs/0-run/experiment_results/experiment_6816f45789ae4a8ab42fd499c5902239_proc_119935/synthetic_loss_curves_nc60.png", "../../logs/0-run/experiment_results/experiment_6816f45789ae4a8ab42fd499c5902239_proc_119935/synthetic_error_curves_nc10.png"], ["../../logs/0-run/experiment_results/experiment_00817c2a788a418c892117dad78e8101_proc_119936/synthetic_sparsity.png", "../../logs/0-run/experiment_results/experiment_00817c2a788a418c892117dad78e8101_proc_119936/synthetic_error_curves.png", "../../logs/0-run/experiment_results/experiment_00817c2a788a418c892117dad78e8101_proc_119936/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_00817c2a788a418c892117dad78e8101_proc_119936/synthetic_dict_recovery_error.png"], ["../../logs/0-run/experiment_results/experiment_0c8aa7cbf3644d84999e1838dcd85815_proc_119935/synthetic_error_curves_atom_norm_projection.png", "../../logs/0-run/experiment_results/experiment_0c8aa7cbf3644d84999e1838dcd85815_proc_119935/synthetic_loss_curves_atom_norm_projection.png"], ["../../logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_reconstruction_noise_0p0.png", "../../logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_reconstruction_noise_0p05.png", "../../logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_error_curves.png", "../../logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_reconstruction_noise_0p02.png", "../../logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_reconstruction_noise_0p01.png", "../../logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_reconstruction_noise_0p005.png"], ["../../logs/0-run/experiment_results/experiment_2168551bfbbb4d118c9fce27ad0af6fb_proc_119934/synthetic_reconstruction_SGD.png", "../../logs/0-run/experiment_results/experiment_2168551bfbbb4d118c9fce27ad0af6fb_proc_119934/synthetic_validation_loss_curves.png", "../../logs/0-run/experiment_results/experiment_2168551bfbbb4d118c9fce27ad0af6fb_proc_119934/synthetic_validation_error_curves.png", "../../logs/0-run/experiment_results/experiment_2168551bfbbb4d118c9fce27ad0af6fb_proc_119934/synthetic_training_loss_curves.png", "../../logs/0-run/experiment_results/experiment_2168551bfbbb4d118c9fce27ad0af6fb_proc_119934/synthetic_training_error_curves.png"], ["../../logs/0-run/experiment_results/experiment_c4b9beda50cf4f94824c07e31fa2e25a_proc_119935/synthetic_train_val_error_curves.png", "../../logs/0-run/experiment_results/experiment_c4b9beda50cf4f94824c07e31fa2e25a_proc_119935/synthetic_train_val_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_cc71ef8ab4a44babaafab4babc86b6ab_proc_119936/synthetic_validation_error_curves.png", "../../logs/0-run/experiment_results/experiment_cc71ef8ab4a44babaafab4babc86b6ab_proc_119936/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_cc71ef8ab4a44babaafab4babc86b6ab_proc_119936/synthetic_training_error_curves.png"], ["../../logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_sample_bs_20.png", "../../logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_sample_bs_80.png", "../../logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_error_curves.png", "../../logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_sample_bs_40.png", "../../logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_sample_bs_10.png"], ["../../logs/0-run/experiment_results/experiment_1f07eeb5167d4050b534357000d360b8_proc_119935/synthetic_final_val_error.png", "../../logs/0-run/experiment_results/experiment_1f07eeb5167d4050b534357000d360b8_proc_119935/synthetic_error_curves.png", "../../logs/0-run/experiment_results/experiment_1f07eeb5167d4050b534357000d360b8_proc_119935/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_f8adb743d47f460799183d5e39508a2e_proc_119934/synthetic_error_curves.png", "../../logs/0-run/experiment_results/experiment_f8adb743d47f460799183d5e39508a2e_proc_119934/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f8adb743d47f460799183d5e39508a2e_proc_119934/synthetic_recon_lambda2_0.png", "../../logs/0-run/experiment_results/experiment_f8adb743d47f460799183d5e39508a2e_proc_119934/synthetic_recon_lambda2_1e-1.png", "../../logs/0-run/experiment_results/experiment_f8adb743d47f460799183d5e39508a2e_proc_119934/synthetic_recon_lambda2_1e-3.png"], [], ["../../logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_laplace_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/block_sparse_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_laplace_error_curves.png", "../../logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_uniform_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_gaussian_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_uniform_error_curves.png", "../../logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/block_sparse_error_curves.png", "../../logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_gaussian_error_curves.png"], ["../../logs/0-run/experiment_results/experiment_a74d8a1675154ffa86a9de0ca8d057a3_proc_119934/synthetic_error_curves.png", "../../logs/0-run/experiment_results/experiment_a74d8a1675154ffa86a9de0ca8d057a3_proc_119934/synthetic_pred_vs_gt_mae.png", "../../logs/0-run/experiment_results/experiment_a74d8a1675154ffa86a9de0ca8d057a3_proc_119934/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_a74d8a1675154ffa86a9de0ca8d057a3_proc_119934/synthetic_pred_vs_gt_huber.png", "../../logs/0-run/experiment_results/experiment_a74d8a1675154ffa86a9de0ca8d057a3_proc_119934/synthetic_pred_vs_gt_mse.png"], ["../../logs/0-run/experiment_results/experiment_58a5afd53d174bdc81bdd46c83933235_proc_119936/synthetic_validation_loss_solver_ablation.png", "../../logs/0-run/experiment_results/experiment_58a5afd53d174bdc81bdd46c83933235_proc_119936/synthetic_training_error_curve.png", "../../logs/0-run/experiment_results/experiment_58a5afd53d174bdc81bdd46c83933235_proc_119936/synthetic_validation_error_solver_ablation.png", "../../logs/0-run/experiment_results/experiment_58a5afd53d174bdc81bdd46c83933235_proc_119936/synthetic_training_loss_curve.png"], ["../../logs/0-run/experiment_results/experiment_5aef2c7179924bf796116707bad1ed95_proc_119935/synthetic_final_val_error.png", "../../logs/0-run/experiment_results/experiment_5aef2c7179924bf796116707bad1ed95_proc_119935/synthetic_error_curves.png", "../../logs/0-run/experiment_results/experiment_5aef2c7179924bf796116707bad1ed95_proc_119935/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/laplace_error_curves.png", "../../logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/laplace_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/gaussian_error_curves.png", "../../logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/cauchy_error_curves.png", "../../logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/cauchy_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/gaussian_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_histogram_dim_1024.png", "../../logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_error_curves.png", "../../logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_histogram_dim_4096.png", "../../logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_histogram_dim_64.png", "../../logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_histogram_dim_256.png"], ["../../logs/0-run/experiment_results/experiment_f9dfbfebf907482d8cfa805ed5d250f5_proc_119935/synthetic_train_val_error_curves.png", "../../logs/0-run/experiment_results/experiment_f9dfbfebf907482d8cfa805ed5d250f5_proc_119935/synthetic_train_val_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_57ac4e99a24644f9b02c4748160b834a_proc_119936/synthetic_train_val_error_curves.png", "../../logs/0-run/experiment_results/experiment_57ac4e99a24644f9b02c4748160b834a_proc_119936/synthetic_train_val_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_9f9d63180c0b46548dd5bd69d0b77887_proc_119934/synthetic_train_val_error_curves.png", "../../logs/0-run/experiment_results/experiment_9f9d63180c0b46548dd5bd69d0b77887_proc_119934/synthetic_train_val_loss_curves.png"], ["../../logs/0-run/experiment_results/seed_aggregation_5f5d5db12d3e47eb95132e34898ba1fa/synthetic_aggregated_train_val_loss.png", "../../logs/0-run/experiment_results/seed_aggregation_5f5d5db12d3e47eb95132e34898ba1fa/synthetic_aggregated_train_val_error.png"]], "plot_paths": [["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6effbbcb54b241c9b3db94d9c6486930_proc_106393/synthetic_training_loss.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6effbbcb54b241c9b3db94d9c6486930_proc_106393/synthetic_training_error.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6effbbcb54b241c9b3db94d9c6486930_proc_106393/synthetic_validation_error.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6effbbcb54b241c9b3db94d9c6486930_proc_106393/synthetic_sample_reconstruction_beta1_0.5.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6effbbcb54b241c9b3db94d9c6486930_proc_106393/synthetic_validation_loss.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_cb095268033542b0a36a5eb8d437fc1e_proc_119934/multi_synthetic_ds3_error_loss.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_cb095268033542b0a36a5eb8d437fc1e_proc_119934/multi_synthetic_ds2_error_loss.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_cb095268033542b0a36a5eb8d437fc1e_proc_119934/multi_synthetic_ds1_error_loss.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6816f45789ae4a8ab42fd499c5902239_proc_119935/synthetic_loss_curves_nc10.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6816f45789ae4a8ab42fd499c5902239_proc_119935/synthetic_error_curves_nc60.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6816f45789ae4a8ab42fd499c5902239_proc_119935/synthetic_error_curves_nc30.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6816f45789ae4a8ab42fd499c5902239_proc_119935/synthetic_loss_curves_nc60.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6816f45789ae4a8ab42fd499c5902239_proc_119935/synthetic_error_curves_nc10.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_00817c2a788a418c892117dad78e8101_proc_119936/synthetic_sparsity.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_00817c2a788a418c892117dad78e8101_proc_119936/synthetic_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_00817c2a788a418c892117dad78e8101_proc_119936/synthetic_loss_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_00817c2a788a418c892117dad78e8101_proc_119936/synthetic_dict_recovery_error.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_0c8aa7cbf3644d84999e1838dcd85815_proc_119935/synthetic_error_curves_atom_norm_projection.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_0c8aa7cbf3644d84999e1838dcd85815_proc_119935/synthetic_loss_curves_atom_norm_projection.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_reconstruction_noise_0p0.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_reconstruction_noise_0p05.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_loss_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_reconstruction_noise_0p02.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_reconstruction_noise_0p01.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_reconstruction_noise_0p005.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_2168551bfbbb4d118c9fce27ad0af6fb_proc_119934/synthetic_reconstruction_SGD.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_2168551bfbbb4d118c9fce27ad0af6fb_proc_119934/synthetic_validation_loss_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_2168551bfbbb4d118c9fce27ad0af6fb_proc_119934/synthetic_validation_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_2168551bfbbb4d118c9fce27ad0af6fb_proc_119934/synthetic_training_loss_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_2168551bfbbb4d118c9fce27ad0af6fb_proc_119934/synthetic_training_error_curves.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_c4b9beda50cf4f94824c07e31fa2e25a_proc_119935/synthetic_train_val_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_c4b9beda50cf4f94824c07e31fa2e25a_proc_119935/synthetic_train_val_loss_curves.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_cc71ef8ab4a44babaafab4babc86b6ab_proc_119936/synthetic_validation_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_cc71ef8ab4a44babaafab4babc86b6ab_proc_119936/synthetic_loss_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_cc71ef8ab4a44babaafab4babc86b6ab_proc_119936/synthetic_training_error_curves.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_sample_bs_20.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_sample_bs_80.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_sample_bs_40.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_loss_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_sample_bs_10.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_1f07eeb5167d4050b534357000d360b8_proc_119935/synthetic_final_val_error.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_1f07eeb5167d4050b534357000d360b8_proc_119935/synthetic_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_1f07eeb5167d4050b534357000d360b8_proc_119935/synthetic_loss_curves.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_f8adb743d47f460799183d5e39508a2e_proc_119934/synthetic_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_f8adb743d47f460799183d5e39508a2e_proc_119934/synthetic_loss_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_f8adb743d47f460799183d5e39508a2e_proc_119934/synthetic_recon_lambda2_0.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_f8adb743d47f460799183d5e39508a2e_proc_119934/synthetic_recon_lambda2_1e-1.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_f8adb743d47f460799183d5e39508a2e_proc_119934/synthetic_recon_lambda2_1e-3.png"], [], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_laplace_loss_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/block_sparse_loss_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_laplace_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_uniform_loss_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_gaussian_loss_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_uniform_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/block_sparse_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_gaussian_error_curves.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a74d8a1675154ffa86a9de0ca8d057a3_proc_119934/synthetic_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a74d8a1675154ffa86a9de0ca8d057a3_proc_119934/synthetic_pred_vs_gt_mae.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a74d8a1675154ffa86a9de0ca8d057a3_proc_119934/synthetic_loss_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a74d8a1675154ffa86a9de0ca8d057a3_proc_119934/synthetic_pred_vs_gt_huber.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a74d8a1675154ffa86a9de0ca8d057a3_proc_119934/synthetic_pred_vs_gt_mse.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_58a5afd53d174bdc81bdd46c83933235_proc_119936/synthetic_validation_loss_solver_ablation.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_58a5afd53d174bdc81bdd46c83933235_proc_119936/synthetic_training_error_curve.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_58a5afd53d174bdc81bdd46c83933235_proc_119936/synthetic_validation_error_solver_ablation.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_58a5afd53d174bdc81bdd46c83933235_proc_119936/synthetic_training_loss_curve.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_5aef2c7179924bf796116707bad1ed95_proc_119935/synthetic_final_val_error.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_5aef2c7179924bf796116707bad1ed95_proc_119935/synthetic_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_5aef2c7179924bf796116707bad1ed95_proc_119935/synthetic_loss_curves.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/laplace_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/laplace_loss_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/gaussian_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/cauchy_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/cauchy_loss_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/gaussian_loss_curves.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_histogram_dim_1024.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_histogram_dim_4096.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_loss_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_histogram_dim_64.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_histogram_dim_256.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_f9dfbfebf907482d8cfa805ed5d250f5_proc_119935/synthetic_train_val_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_f9dfbfebf907482d8cfa805ed5d250f5_proc_119935/synthetic_train_val_loss_curves.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_57ac4e99a24644f9b02c4748160b834a_proc_119936/synthetic_train_val_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_57ac4e99a24644f9b02c4748160b834a_proc_119936/synthetic_train_val_loss_curves.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_9f9d63180c0b46548dd5bd69d0b77887_proc_119934/synthetic_train_val_error_curves.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_9f9d63180c0b46548dd5bd69d0b77887_proc_119934/synthetic_train_val_loss_curves.png"], ["experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/seed_aggregation_5f5d5db12d3e47eb95132e34898ba1fa/synthetic_aggregated_train_val_loss.png", "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/seed_aggregation_5f5d5db12d3e47eb95132e34898ba1fa/synthetic_aggregated_train_val_error.png"]], "plot_analyses": [[{"analysis": "All four \u03b21 settings drive reconstruction loss steadily downward over 50 epochs. Higher \u03b21 yields faster decreases: \u03b21=0.99 reaches \u22488 at epoch 50, \u03b21=0.9 \u22489, \u03b21=0.7 \u22489.5, \u03b21=0.5 \u224810. Lower momentum slows convergence on the synthetic reconstruction objective but still improves gradually and smoothly.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6effbbcb54b241c9b3db94d9c6486930_proc_106393/synthetic_training_loss.png"}, {"analysis": "Relative reconstruction error on the training set mirrors the loss curves. \u03b21=0.99 attains roughly 15 % error at epoch 50 versus 17 % for \u03b21=0.5. The benefits of high momentum on fitting capacity are clear in the in\u2010sample metric.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6effbbcb54b241c9b3db94d9c6486930_proc_106393/synthetic_training_error.png"}, {"analysis": "On the held\u2010out set, all curves start near 2 % relative error and grow roughly linearly. High \u03b21 overfits most severely: \u03b21=0.99 reaches \u224833 % error by epoch 50, while \u03b21=0.5 and 0.7 stay below \u224822 %. \u03b21=0.7 slightly edges out \u03b21=0.5, suggesting moderate momentum improves generalization versus the extremes.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6effbbcb54b241c9b3db94d9c6486930_proc_106393/synthetic_validation_error.png"}, {"analysis": "Reconstruction of a representative weight vector at the end of training (\u03b21=0.5) qualitatively matches the ground truth distribution: the general spectral shape and amplitude range of the primitive\u2010based signal align with the reference, though the generated version appears somewhat smoother at the peaks/troughs. This confirms that learned primitives capture the main structure but may underrepresent high\u2010frequency extremes.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6effbbcb54b241c9b3db94d9c6486930_proc_106393/synthetic_sample_reconstruction_beta1_0.5.png"}, {"analysis": "Test MSE on the synthetic validation set rises quadratically. \u03b21=0.5 yields the lowest end\u2010point MSE (\u22480.14), \u03b21=0.7 \u22480.15, \u03b21=0.9 \u22480.19, \u03b21=0.99 \u22480.34. As in the relative\u2010error view, smaller momentum gives superior generalization. The sweet spot appears around \u03b21=0.7, balancing convergence speed and out\u2010of\u2010sample performance.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6effbbcb54b241c9b3db94d9c6486930_proc_106393/synthetic_validation_loss.png"}], [{"analysis": "Multi-Synthetic ds3 Results: Training curves show that as \u03b2\u2081 increases from 0.5\u21920.99, convergence accelerates and final relative error drops\u2014\u03b2\u2081=0.99 achieves the lowest error throughout. MSE loss echoes this trend, with the \u03b2\u2081=0.99 run consistently beneath the others. Validation metrics (both error and loss) remain nearly constant across all \u03b2\u2081 settings and show minimal downward drift, indicating that generalization performance is largely unaffected by \u03b2\u2081 on ds3. The overall lower magnitude of training error and loss in ds3 versus the other datasets suggests it is the simplest of the three.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_cb095268033542b0a36a5eb8d437fc1e_proc_119934/multi_synthetic_ds3_error_loss.png"}, {"analysis": "Multi-Synthetic ds2 Results: A very similar pattern emerges. The \u03b2\u2081=0.99 configuration again converges fastest and to the lowest relative error and MSE loss. Differences among \u03b2\u2081=0.5, 0.7, and 0.9 are more subtle but consistently ordered (higher \u03b2\u2081, better training performance). Validation traces remain flat and overlapping for all \u03b2\u2081 values, showing negligible sensitivity to this hyperparameter. Compared with ds3, ds2 exhibits higher initial error and loss levels, and the performance gap between \u03b2\u2081 extremes widens slightly, indicating ds2\u2019s moderate complexity amplifies \u03b2\u2081\u2019s effect.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_cb095268033542b0a36a5eb8d437fc1e_proc_119934/multi_synthetic_ds2_error_loss.png"}, {"analysis": "Multi-Synthetic ds1 Results: The largest initial error and loss magnitudes are observed here, reflecting the greatest dataset complexity. The separation between \u03b2\u2081 configurations is most pronounced: \u03b2\u2081=0.99 again leads, followed by 0.9, 0.7, then 0.5. Convergence speed and final training metrics benefit substantially from higher \u03b2\u2081. Validation error and loss remain essentially flat across epochs and hyperparameter settings, underscoring that generalization is insensitive to \u03b2\u2081 even under high complexity. The widening gap between training and validation curves suggests that while \u03b2\u2081 primarily tunes convergence speed on the training set, it does not induce overfitting on ds1.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_cb095268033542b0a36a5eb8d437fc1e_proc_119934/multi_synthetic_ds1_error_loss.png"}], [{"analysis": "MSE Loss Curves (n_components=10) show a smooth, exponential-like decay of training MSE from about 11 down to roughly 3.5 over 50 epochs. All four \u03b21 settings follow the same trend, with \u03b21=0.99 achieving the fastest decrease and lowest final MSE, followed by \u03b21=0.9, \u03b21=0.7, and \u03b21=0.5 (which overlap closely). Validation MSE remains essentially zero throughout (rising only marginally toward epoch 50 for larger \u03b21), indicating almost perfect interpolation on held-out synthetic data and minimal overfitting signal in this regime.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6816f45789ae4a8ab42fd499c5902239_proc_119935/synthetic_loss_curves_nc10.png"}, {"analysis": "Reconstruction Error Curves (n_components=60) begin around a relative error of 5 and converge to about 2.2 by epoch 50. Momentum hyperparameters yield the same ordering: \u03b21=0.99 is fastest, \u03b21=0.9 next, then 0.7 and 0.5 nearly identical. Validation relative error is low (<0.35 at its worst for \u03b21=0.99) but shows a slight upward drift with increasing \u03b21, suggesting that higher momentum may introduce a small generalization gap even on synthetic weights.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6816f45789ae4a8ab42fd499c5902239_proc_119935/synthetic_error_curves_nc60.png"}, {"analysis": "Reconstruction Error Curves (n_components=30) start at a much higher error (~37) and fall to ~16 over 50 epochs. The relative ranking of \u03b21 settings remains consistent\u2014higher \u03b21 leads to faster and deeper convergence. Validation error stays near zero (under 0.1 across all settings) and exhibits negligible drift, again reflecting an easy synthetic interpolation task for this component size.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6816f45789ae4a8ab42fd499c5902239_proc_119935/synthetic_error_curves_nc30.png"}, {"analysis": "MSE Loss Curves (n_components=60) mirror the reconstruction trends but on the MSE scale: initial training MSE around 67 down to ~15. Higher \u03b21 consistently yields lower final MSE and faster early descent. Validation MSE is vanishingly small (<0.05 at epoch 50), underscoring near-perfect fit on synthetic model weights with little resistance from held-out examples.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6816f45789ae4a8ab42fd499c5902239_proc_119935/synthetic_loss_curves_nc60.png"}, {"analysis": "Reconstruction Error Curves (n_components=10) start extremely high (~110) and converge to ~45 by the end of training. Despite the high error scale, the impact of momentum remains: \u03b21=0.99 leads, followed by 0.9, then 0.7 and 0.5. Validation error again clings near zero (under 0.1) with a slight upward tendency for larger \u03b21.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_6816f45789ae4a8ab42fd499c5902239_proc_119935/synthetic_error_curves_nc10.png"}], [{"analysis": "Code sparsity remains extremely low and nearly flat across all epochs and regularization strengths. The sparsity fraction fluctuates around 0.001\u20130.003 without any clear trend or separation between \u03bb values. Even the largest \u03bb (0.1) does not induce noticeably higher sparsity, suggesting that the chosen regularization weights are too weak to produce meaningful sparse codes under the current setup.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_00817c2a788a418c892117dad78e8101_proc_119936/synthetic_sparsity.png"}, {"analysis": "Training and validation errors decrease almost identically for every \u03bb, with no visible divergence or gap between training and validation curves. Validation error starts near zero and remains nearly constant at a negligible level, indicating that the model generalizes perfectly on the synthetic validation set right from the start. Varying \u03bb has no discernible impact on error reduction rate or gap.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_00817c2a788a418c892117dad78e8101_proc_119936/synthetic_error_curves.png"}, {"analysis": "Reconstruction losses for training and validation also overlap across all \u03bb values and drop in lockstep from roughly 34 to around 9 by epoch 50. Validation loss is effectively zero throughout, mirroring the error behavior. This redundancy across \u03bb settings reinforces that the sparsity penalty is not affecting the autoencoding performance or preventing trivial reconstruction of the synthetic data.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_00817c2a788a418c892117dad78e8101_proc_119936/synthetic_loss_curves.png"}, {"analysis": "Relative dictionary recovery error steadily increases from about 0.01 at epoch 1 to roughly 0.37 by epoch 50, with no separation between different \u03bb values. Rather than converging toward the ground-truth dictionary, the learned basis drifts farther away over time. The regularization parameter has no control over this drift, indicating that the current dictionary learning objective or optimization strategy may be misconfigured.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_00817c2a788a418c892117dad78e8101_proc_119936/synthetic_dict_recovery_error.png"}], [{"analysis": "All \u03b21 settings exhibit an initial rapid drop in relative error within the first 5\u201310 epochs, then settle into a slow decay. Lower \u03b21 (0.5, 0.7) converge to a higher asymptotic error (~1.80\u20131.85) on the training set, while higher \u03b21 (0.9, 0.99) stabilize around slightly lower values (~1.75\u20131.78). Validation curves mirror the training trends: early convergence by epoch 10, followed by a plateau near 0.65\u20130.70. Differences between \u03b21=0.9 and \u03b21=0.99 are marginal, but those settings outperform lower \u03b21 in both training and validation error.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_0c8aa7cbf3644d84999e1838dcd85815_proc_119935/synthetic_error_curves_atom_norm_projection.png"}, {"analysis": "MSE loss curves similarly drop steeply in the first few epochs then plateau. All \u03b21 values reach near-identical training loss (~3.0) by epoch 10 and maintain it thereafter. Validation loss stabilizes around 1.2 for all settings, with slightly faster attainment of the plateau for \u03b21=0.99 and \u03b21=0.9. There is minimal gap between train and validation losses, indicating low overfitting across \u03b21 choices. The ablation suggests that higher \u03b21 accelerates convergence without altering final loss significantly.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_0c8aa7cbf3644d84999e1838dcd85815_proc_119935/synthetic_loss_curves_atom_norm_projection.png"}], [{"analysis": "\u03c3 = 0.0 reconstruction shows virtually perfect overlap between ground-truth and generated signals. All peaks, troughs, and local fluctuations are captured with negligible distortion, confirming that the learned dictionary can exactly reconstruct weight-like vectors in the noiseless setting.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_reconstruction_noise_0p0.png"}, {"analysis": "\u03c3 = 0.05 reconstruction remains highly faithful but reveals small amplitude damping at some extreme peaks and slight smoothing of sharp transitions. Despite the injected noise, sparse coding recovers the underlying structure with only minimal loss of detail.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_reconstruction_noise_0p05.png"}, {"analysis": "Training and validation relative-error curves illustrate that higher noise levels lead to lower relative-error ratios (e.g., \u03c3 = 0.05 converges from ~11 down to ~5, whereas \u03c3 = 0.0 only drops from ~70 to ~60). This behavior is consistent with an error metric normalized by noise amplitude. Validation errors for all \u03c3 remain near zero, indicating strong generalization and little overfitting across noise conditions.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_error_curves.png"}, {"analysis": "Training and validation MSE-loss curves are nearly identical for every \u03c3 value and demonstrate smooth exponential decay from ~34 down to ~8 over 50 epochs. The near-perfect overlap of train and val curves across noise levels suggests stability of the dictionary-learning procedure and robustness of convergence dynamics.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_loss_curves.png"}, {"analysis": "\u03c3 = 0.02 reconstruction yields almost indistinguishable generated signals compared to ground truth. Minor discrepancies appear only at the highest-frequency components, but overall waveform morphology and amplitude statistics are preserved.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_reconstruction_noise_0p02.png"}, {"analysis": "\u03c3 = 0.01 reconstruction continues to show excellent fidelity. Small local deviations are barely visible, confirming that the method tolerates moderate noise without compromising the core signal content.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_reconstruction_noise_0p01.png"}, {"analysis": "\u03c3 = 0.005 reconstruction is effectively indistinguishable from the ground truth. All fine-grained patterns, including rapid oscillations, are retained, demonstrating that dictionary learning can filter out low-level noise while reconstructing the principal components of weight-like data.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_d0e45c415b7945c39648245ce5a9efaf_proc_119936/synthetic_reconstruction_noise_0p005.png"}], [{"analysis": "For the synthetic sample reconstruction with SGD, the generated waveform closely tracks the ground-truth signal across the entire sequence, with only minimal deviations. This indicates that the learned primitives combined with SGD-based sparse coding can reconstruct novel samples with high fidelity, successfully capturing underlying patterns and noise characteristics of the synthetic dataset.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_2168551bfbbb4d118c9fce27ad0af6fb_proc_119934/synthetic_reconstruction_SGD.png"}, {"analysis": "In the validation loss curves, SGD maintains an almost constant near-zero loss throughout training, pointing to stable generalization. By contrast, RMSprop exhibits steadily increasing validation loss\u2014indicative of overfitting or divergence\u2014while AdamW shows moderate increase. This suggests that adaptive optimizers produce faster training updates but degrade generalization performance on unseen data in this synthetic setting.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_2168551bfbbb4d118c9fce27ad0af6fb_proc_119934/synthetic_validation_loss_curves.png"}, {"analysis": "The validation relative error curves mirror the loss trends: SGD holds a low constant error (~3%), AdamW\u2019s error rises to ~25% by epoch 50, and RMSprop\u2019s error balloons up to ~45%. The wide gap further highlights that plain SGD yields the best generalization, whereas adaptive methods lead to larger reconstruction inaccuracies on validation samples.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_2168551bfbbb4d118c9fce27ad0af6fb_proc_119934/synthetic_validation_error_curves.png"}, {"analysis": "Training loss curves reveal that RMSprop drives the fastest reduction in reconstruction loss, dropping from ~34 to ~2 in 50 epochs, with AdamW intermediate (~34 to ~8.5) and SGD the slowest (~34 to ~27). This confirms that adaptive optimizers accelerate convergence on the training set at the cost of potential overfitting.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_2168551bfbbb4d118c9fce27ad0af6fb_proc_119934/synthetic_training_loss_curves.png"}, {"analysis": "The training relative error declines sharply under RMSprop (from ~37% to ~5%), moderately under AdamW (~37% to ~16%), and slowly under SGD (~37% to ~34%). This aligns with the loss curves and underscores the trade-off: adaptive methods yield rapid training improvements but poorer validation behavior, while SGD trades speed for robustness.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_2168551bfbbb4d118c9fce27ad0af6fb_proc_119934/synthetic_training_error_curves.png"}], [{"analysis": "Error curves across different dictionary-size ratios show a clear trend: training error falls more rapidly and to a lower plateau as the dictionary becomes more overcomplete. The 1:1 configuration starts highest and decays slowly, reaching around 18% relative error by epoch 50. Moving to a 5:1 ratio accelerates convergence and cuts the final error to about 3%. The 10:1 overcomplete dictionary drives training error near zero by epoch 50. Undercomplete settings (1:5, 1:10) fall between, with 1:5 saturating near 7% and 1:10 near 6% after 50 epochs.\n\nValidation curves remain extremely low (<1% relative error) for all ratios, indicating strong generalization on this synthetic task. A small transient bump around epochs 10\u201320 in the most extreme undercomplete (1:10) and overcomplete (10:1) cases suggests a brief mismatch in sparse-code adaptation, but this vanishes with continued training. No evidence of overfitting is seen\u2014validation remains flat even when training error continues to drop.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_c4b9beda50cf4f94824c07e31fa2e25a_proc_119935/synthetic_train_val_error_curves.png"}, {"analysis": "MSE loss trends mirror the relative-error behavior. The baseline 1:1 case begins at ~32 MSE and decays sluggishly to ~8 MSE by epoch 50. Increasing dictionary overcompleteness (5:1 \u2192 10:1) steepens the descent and lowers the final loss: the 5:1 run hits ~1.8 MSE, while the 10:1 configuration reaches ~1.0 MSE. Undercomplete setups (1:5, 1:10) land in between, with 1:5 ending at ~1.5 MSE and 1:10 at ~2.2 MSE.\n\nValidation loss curves hug zero throughout, aside from a modest mid-training blip in the extreme ratios (again around epoch 10\u201320). This consistency confirms that larger dictionaries improve training efficiency and reconstruction fidelity without harming generalization on held-out synthetic samples.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_c4b9beda50cf4f94824c07e31fa2e25a_proc_119935/synthetic_train_val_loss_curves.png"}], [{"analysis": "normal initialization yields the lowest initial validation error yet that error steadily increases over epochs, suggesting overfitting or instability in sparse reconstruction when using a standard Gaussian dictionary; orthogonal initialization starts with high error, quickly peaks, and then gradually decreases toward an asymptote (~0.58) by epoch 50, indicating slower initial reconstruction but better long-term approximation than zero initialization; Xavier uniform sees the highest error at epoch 0 (~0.98) followed by a smooth monotonic decay to ~0.60, reflecting stable convergence from a scale-aware random basis; zero initialization begins at ~0.80 and declines slowly to ~0.68, showing poor expressivity at first and moderate improvement over time without ever matching orthogonal or Xavier performance.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_cc71ef8ab4a44babaafab4babc86b6ab_proc_119936/synthetic_validation_error_curves.png"}, {"analysis": "training losses show standard Gaussian dictionary (normal) has very high MSE (~11\u21923) with a growing generalization gap (val from ~0.02\u21920.10), signaling underfitting despite low validation reconstruction error initially; orthogonal initialization achieves near-zero train and val loss almost immediately and remains flat, demonstrating excellent fit of weight primitives to the zoo and strong generalization in sparse coding; Xavier uniform yields moderate train/val losses (~2.8\u21921.0) with low gap, indicating a balanced dictionary scale; zero initialization produces high initial losses (~3.2 train, 2.0 val) that decrease steadily to ~1.4/1.5, confirming limited dictionary expressivity that nevertheless improves with additional epochs.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_cc71ef8ab4a44babaafab4babc86b6ab_proc_119936/synthetic_loss_curves.png"}, {"analysis": "training error curves mirror the loss patterns: normal Gaussian decreases from ~12.5 to ~4.5, showing slow improvement; orthogonal remains tightly around ~1.1\u21921.3, indicating stable and efficient encoding; Xavier uniform oscillates mildly near ~1.3\u21921.4, suggesting consistent but moderate reconstruction power; zero initialization exhibits a mid-training hump (~0.9\u21922.0 by epoch 20) before settling near ~1.6, reflecting initial underfitting that transiently worsens before partial recovery.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_cc71ef8ab4a44babaafab4babc86b6ab_proc_119936/synthetic_training_error_curves.png"}], [{"analysis": "Sample reconstruction at batch size 20 shows that the predicted weight vector closely tracks the ground truth\u2019s overall fluctuations but tends to attenuate the most extreme values. Fine-scale structure is largely preserved, indicating that the dictionary and sparse code can capture detailed variations when training with moderate batch sizes. Residual errors appear as small amplitude mismatches rather than gross shape distortions.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_sample_bs_20.png"}, {"analysis": "Sample reconstruction at batch size 80 exhibits a smoother, somewhat underfitted prediction. Key peaks in the ground truth are present but with reduced magnitude, and valleys are shallower, suggesting that the model under batch-large training loses some high-frequency components of the weight signal. This hints at slower convergence and a bias toward smoother reconstructions when the gradient estimates are less noisy but less frequent.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_sample_bs_80.png"}, {"analysis": "Error curves reveal a clear trade-off: smaller batch sizes drive training error down most rapidly (batch size 10 fastest, then 20, 40, 80), while validation error grows more slowly for large batches. After 30\u201350 epochs, validation error for batch size 80 remains lowest, whereas batch size 10 has the highest validation error despite the lowest training error. This indicates that very small batch sizes overshoot into overfitting, whereas large batches generalize better but learn more slowly.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_error_curves.png"}, {"analysis": "Sample reconstruction at batch size 40 is intermediate: the predicted trace captures most of the ground truth\u2019s structure but shows occasional overshoots and undershoots. The amplitude alignment is better than at batch size 80 but not as sharp as at batch size 20 or 10. This matches its middling position in the learning curves\u2014faster than very large batches but less prone to overfitting than the very small ones.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_sample_bs_40.png"}, {"analysis": "Loss curves mirror the error trends: training loss plummets fastest at batch size 10, then 20, 40, 80, reflecting gradient noise-driven exploration. Validation loss increases most slowly for batch size 80, confirming that larger batches yield smoother, more stable generalization trajectories. Small batches achieve lower training loss but incur a larger gap to validation loss, again flagging overfitting risk.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_loss_curves.png"}, {"analysis": "Sample reconstruction at batch size 10 delivers the tightest fit to ground truth, preserving extreme peaks and troughs almost exactly. However, it also contains more spurious, high-frequency noise, consistent with its overzealous minimization of training loss. This underlines that although aggressive fitting yields very low reconstruction error on seen data, it can degrade generalization performance.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_df9171d7253c4881af69d69a29bdaaa5_proc_119934/synthetic_sample_bs_10.png"}], [{"analysis": "Final validation error across no penalty, L1, L2, and elasticnet all converge at approximately 0.25 on the synthetic dataset. The absence of any gap between penalty types suggests none of these regularizers impacted final generalization error under current hyperparameter settings. This may reflect an overly simple task or insufficient regularization strength to alter model behavior.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_1f07eeb5167d4050b534357000d360b8_proc_119935/synthetic_final_val_error.png"}, {"analysis": "Training error curves for all four penalty configurations decrease uniformly from about 37 down to 16 over 50 epochs, while validation error remains essentially zero and flat across epochs. Identical trajectories for none, L1, L2, and elasticnet regimes imply that regularization neither slowed learning nor improved validation beyond perfect fit, pointing to a likely underconstrained synthetic problem.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_1f07eeb5167d4050b534357000d360b8_proc_119935/synthetic_error_curves.png"}, {"analysis": "MSE loss curves mirror the error trends: training loss drops from roughly 34 to 8.7 identically across all penalties, while validation loss holds near zero. The complete overlap of these lines indicates no observable effect of penalty type on loss minimization or overfitting, reinforcing that current regularization parameters have negligible impact on this synthetic experiment.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_1f07eeb5167d4050b534357000d360b8_proc_119935/synthetic_loss_curves.png"}], [{"analysis": "All relative error curves for different \u03bb2 values overlap almost perfectly during training, indicating that small to moderate regularization does not significantly degrade reconstruction accuracy. Only the largest \u03bb2 (1e-1) shows a marginally higher error throughout training, suggesting a slight underfitting effect at strong regularization. Training and validation curves closely track each other, implying good generalization from the dictionary to held-out samples.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_f8adb743d47f460799183d5e39508a2e_proc_119934/synthetic_error_curves.png"}], [], [{"analysis": "Loss curves on the laplace-distributed synthetic data show nearly identical training trajectories for all beta1 settings, with higher beta1 values yielding slightly faster convergence and lower final MSE on training. On validation, however, the red curve (beta1=0.99) diverges most sharply, indicating overfitting or poor generalization. Beta1=0.5 and 0.7 achieve the lowest validation loss, with beta1=0.7 slightly outperforming 0.5 at later epochs. Beta1=0.9 sits in between but closer to the lower bound of generalization error.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_laplace_loss_curves.png"}, {"analysis": "Sparse-block distribution training loss again favors high beta1: beta1=0.99 converges fastest, beta1=0.5 slowest. Validation loss curves mirror the laplace case: beta1=0.99 climbs steeply, while beta1=0.5 and 0.7 remain closest to zero. Beta1=0.9 offers intermediate performance but begins to lag behind the mid-range settings after about epoch 20.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/block_sparse_loss_curves.png"}, {"analysis": "Relative error on laplace data follows the same pattern as MSE. On training error, beta1=0.99 shows the steepest decline, ending with the lowest error. Validation error rises almost linearly, with beta1=0.99 highest, beta1=0.5 and 0.7 lowest, and beta1=0.9 in between. The spread between curves widens consistently across epochs, reinforcing that high momentum sacrifices generalization.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_laplace_error_curves.png"}, {"analysis": "Uniform-distribution loss curves reveal overall lower training loss compared to laplace and block-sparse cases, suggesting this synthetic regime is easier to fit. Again, higher beta1 speeds convergence, but validation loss for beta1=0.99 becomes large by epoch 50. Beta1=0.7 slightly edges out 0.5 in minimizing validation error, maintaining more stable generalization.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_uniform_loss_curves.png"}, {"analysis": "Gaussian-distribution loss trajectories are similar to uniform, but training loss plateaus a bit higher and validation loss grows more for intermediate beta1 settings. Beta1=0.5 and 0.7 tie for the best validation curve, while 0.9 lags and 0.99 diverges fastest. This suggests Gaussian synthetic targets add difficulty that penalizes overly aggressive momentum.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_gaussian_loss_curves.png"}, {"analysis": "Relative error on uniform data highlights that all configurations achieve very low training error, but validation error follows the same ranking: beta1=0.99 worst, 0.5 and 0.7 best, 0.9 in the middle. The uniform error spread is tighter than for laplace or gaussian, indicating less sensitivity to the momentum hyperparameter in this regime.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_uniform_error_curves.png"}, {"analysis": "Block-sparse relative-error curves show training error dropping fastest for beta1=0.99 and slowest for beta1=0.5. Validation error again penalizes high beta1: the red line climbs to ~0.32, while blue/orange remain near ~0.21. Beta1=0.7 yields marginally lower validation error than 0.5 after warm-up, reflecting a sweet spot for block-sparse targets.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/block_sparse_error_curves.png"}, {"analysis": "Gaussian relative-error patterns mimic the loss trends: the highest momentum produces the lowest training error but the steepest validation-error climb. Beta1=0.5 and 0.7 are nearly tied on validation, with 0.7 slightly better after epoch 25. Beta1=0.9 occupies a middle ground. The similarity across distributions confirms that moderate momentum values generalize best.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_b6688d2329904d958987b078505260af_proc_119935/bernoulli_gaussian_error_curves.png"}], [{"analysis": "Relative error curves across epochs under different loss functions (MSE, MAE, Huber) and momentum hyperparameters (beta1) show a consistent downward trend. Convergence speed is slightly higher for lower beta1 values in early epochs, but by epoch 50 differences are minimal. MSE yields marginally lower relative error overall compared to MAE and Huber. Training and validation curves track closely, indicating stable generalization across configurations.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a74d8a1675154ffa86a9de0ca8d057a3_proc_119934/synthetic_error_curves.png"}, {"analysis": "Scatter of predictions versus ground truth for the MAE-trained model reveals a tight cloud along the diagonal, indicating accurate reconstruction on the synthetic dataset. Dispersion remains modest across the value range, with slight underestimation at extreme targets, reflecting MAE\u2019s robustness to outliers while maintaining high fidelity.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a74d8a1675154ffa86a9de0ca8d057a3_proc_119934/synthetic_pred_vs_gt_mae.png"}, {"analysis": "Reconstruction loss trajectories for the three loss functions demonstrate that MSE starts highest and decreases most rapidly, followed by Huber and then MAE. Lower beta1 values accelerate early descent, though all curves converge to similar minima by the end of training. This confirms that loss choice primarily affects convergence dynamics rather than final quality on this synthetic task.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a74d8a1675154ffa86a9de0ca8d057a3_proc_119934/synthetic_loss_curves.png"}, {"analysis": "Predictions versus ground truth for the Huber-trained model yield a balanced scatter: tighter around the diagonal than MAE at mid-range values but less influenced by outliers than MSE. This dispersion pattern underlines Huber\u2019s ability to combine sensitivity near small errors with robustness to larger deviations.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a74d8a1675154ffa86a9de0ca8d057a3_proc_119934/synthetic_pred_vs_gt_huber.png"}, {"analysis": "Scatter plot for the MSE-trained model shows the tightest clustering of points around the identity line, reflecting minimal average squared deviation. This indicates that MSE yields the best raw accuracy on clean synthetic data, albeit with potentially greater sensitivity to extreme points than its robust counterparts.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a74d8a1675154ffa86a9de0ca8d057a3_proc_119934/synthetic_pred_vs_gt_mse.png"}], [{"analysis": "Validation Loss Curve \u2013 Solver Ablation (Synthetic Dataset) shows that pseudoinverse, ISTA, and FISTA produce almost indistinguishable low validation losses throughout training, starting near zero and rising only modestly to about 0.18 by epoch 50. OMP yields substantially higher loss at every epoch, climbing from ~0.20 to almost 0.48. This suggests that least\u2010squares\u2013based and proximal\u2010gradient solvers capture the dictionary primitives much more faithfully than OMP in this setting, and that differences among pseudoinverse, ISTA, and FISTA are minimal.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_58a5afd53d174bdc81bdd46c83933235_proc_119936/synthetic_validation_loss_solver_ablation.png"}, {"analysis": "Training Error Curve (Synthetic Dataset) indicates steady and smooth convergence of the synthesized network under the chosen dictionary scheme: error drops from ~37% at initialization to ~16% by epoch 50. The monotonically decreasing trend with no apparent plateau or instability suggests the overall training loop (dictionary update plus weight approximation) is well\u2010behaved on this synthetic task.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_58a5afd53d174bdc81bdd46c83933235_proc_119936/synthetic_training_error_curve.png"}, {"analysis": "Validation Error Curve \u2013 Solver Ablation (Synthetic Dataset) mirrors the loss trends: pseudoinverse, ISTA, and FISTA begin at very low error (~3%) and gradually increase to around 25%, whereas OMP starts higher (~11%) and ends above 35%. The widening gap implies that OMP\u2019s greedy support selection fails to generalize as well over time, while the others maintain tighter approximations of weight space relevant for generalization.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_58a5afd53d174bdc81bdd46c83933235_proc_119936/synthetic_validation_error_solver_ablation.png"}, {"analysis": "Training Loss Curve (Synthetic Dataset) exhibits a classic exponential\u2010decay shape, with loss decreasing from ~34 to ~8 across 50 epochs. This consistent drop confirms that the training procedure (combining sparse reconstruction with model evaluation) effectively reduces overall objective without signs of divergence or overfitting on the synthetic set.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_58a5afd53d174bdc81bdd46c83933235_proc_119936/synthetic_training_loss_curve.png"}], [{"analysis": "Bar-chart of final validation errors highlights that schedules with decay significantly outperform a fixed learning rate. Step-decay yields the lowest error (~0.103), followed closely by exponential decay (~0.110). Cosine annealing sits in the middle (~0.145), while the fixed-rate baseline is markedly worse (~0.25). This confirms that introducing learning-rate schedules reduces overfitting and improves generalization on the synthetic dataset.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_5aef2c7179924bf796116707bad1ed95_proc_119935/synthetic_final_val_error.png"}, {"analysis": "Relative-error curves over 50 epochs reveal distinct convergence behaviors. Training curves show the fixed schedule achieves the fastest drop and lowest training error by epoch 50, then cosine annealing, exponential decay, and finally step decay plateauing early. Validation-error trajectories expose anomalies: step decay and cosine both steadily decrease and plateau around ~27 units; fixed recall also decreases reliably; exponential decay, however, shows a linear upwards trend, suggesting a potential implementation bug or an overly aggressive decay causing the model to diverge on unseen data.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_5aef2c7179924bf796116707bad1ed95_proc_119935/synthetic_error_curves.png"}, {"analysis": "Training-loss profiles mirror the relative-error trends: the fixed schedule drives loss down most aggressively (final loss ~9), cosine annealing yields moderate reductions (~16), exponential decay improves slowly (~19), and step decay stagnates near ~21 after the initial drop. Validation-loss is plotted as a flat line at zero for all schedules, indicating a logging or plotting issue\u2014validation-loss data was not captured correctly. Without valid validation-loss curves, it is impossible to judge overfitting or loss-based generalization.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_5aef2c7179924bf796116707bad1ed95_proc_119935/synthetic_loss_curves.png"}], [{"analysis": "Training error under Laplace noise decreases steadily for all \u03b21 settings. \u03b21=0.99 converges fastest, achieving the lowest normalized error by epoch 50, followed by \u03b21=0.9, 0.7, then 0.5. Validation error curves remain nearly flat and extremely low relative to training, suggesting either a very small generalization gap or a plotting/scale mismatch that warrants investigation.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/laplace_error_curves.png"}, {"analysis": "MSE training loss under Laplace noise mirrors the normalized error trend: higher \u03b21 accelerates convergence and yields lower final loss (\u03b21=0.99 < \u03b21=0.9 < \u03b21=0.7 < \u03b21=0.5). The separation among curves widens modestly over epochs. Validation MSE is almost constant and near zero, indicating a potential issue in how validation loss is computed or plotted.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/laplace_loss_curves.png"}, {"analysis": "Under Gaussian noise, normalized training error curves closely follow the Laplace pattern: \u03b21=0.99 again achieves the fastest decay and lowest final error, with diminishing gains beyond \u03b21=0.9. Validation error remains flat near baseline across all \u03b21 values, reinforcing concerns about the validity or scaling of the validation metric.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/gaussian_error_curves.png"}, {"analysis": "With Cauchy noise, normalized training error starts lower (\u22485.3) and decays to \u22482.5 at \u03b21=0.99. Ranking of \u03b21 effects is consistent. Unlike the other distributions, validation error here shows a slight upward drift from \u22480.35 to \u22480.55, but still remains nearly flat compared to training dynamics, suggesting limited sensitivity to \u03b21 on held-out data or, again, a plotting inconsistency.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/cauchy_error_curves.png"}, {"analysis": "MSE training loss for Cauchy noise begins much higher (\u224853) and decreases to \u224824 for \u03b21=0.99 and \u224827 for \u03b21=0.5. Higher \u03b21 systematically improves convergence speed and final loss. Validation MSE hovers around \u22482.3\u20132.5 with minimal separation by \u03b21, indicating that validation may be underrepresented or on a different scale.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/cauchy_loss_curves.png"}, {"analysis": "Gaussian-noise MSE training loss reproduces the Laplace-loss behavior: \u03b21=0.99 leads in performance, followed by \u03b21=0.9, 0.7, 0.5. Validation MSE is nearly zero and flat across all settings, again pointing to a likely mismatch in validation metric computation or visualization.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_3fd483b659824c34b40bdf654a7f3170_proc_119934/gaussian_loss_curves.png"}], [{"analysis": "Ground truth weight vectors (dim=1024) exhibit a symmetric, heavy\u2010tailed distribution around zero with values spanning roughly \u201310 to +12. Generated samples match the central spike near zero but are much more concentrated, with tails truncated around \u20133 to +3. This indicates the dictionary model captures bulk statistics but underestimates extreme weights.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_histogram_dim_1024.png"}, {"analysis": "Training relative error decreases steadily over 50 epochs for all dictionary sizes. Smaller dictionaries (dim=64) converge fastest to the lowest training error; larger ones (dim=4096) converge more slowly and plateau at higher error. Validation error lines are essentially flat across epochs, with larger dictionaries yielding slightly lower constant values. The gap between training and validation suggests limited improvement in generalization through training alone and potential overfitting to the training set.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_error_curves.png"}, {"analysis": "Ground truth weight vectors (dim=4096) again show heavy tails (roughly \u201315 to +12). Generated samples reproduce the central peak but remain tightly clustered (tails within \u20133 to +3). As with dim=1024, the model underrepresents weight extremes\u2014indicating a systematic bias of the sparse coding toward moderate\u2010sized coefficients.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_histogram_dim_4096.png"}, {"analysis": "Training MSE reconstruction loss falls from ~35 to ~8\u20139 over 50 epochs for all dictionary sizes. Smaller dictionaries (dim=64) achieve the lowest final training loss and fastest convergence; larger ones (dim=4096) converge more slowly and settle at higher losses. Validation reconstruction loss is essentially constant across epochs, increasing with dictionary size (lowest for dim=64, highest for dim=4096). This suggests good capacity to fit training data but poor validation improvement and possible overfitting or evaluation mismatch.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_loss_curves.png"}, {"analysis": "Ground truth (dim=64) spans about \u20136 to +10 with a pronounced central mass. Generated vectors center correctly but display narrower support (roughly \u20135 to +5) and undercount extreme bins. Core shape is well matched, but tail fidelity is lost with a very small dictionary.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_histogram_dim_64.png"}, {"analysis": "Ground truth (dim=256) shows heavy tails from about \u20139 to +11. Generated samples again mirror the central mode but truncate the distribution within \u20133 to +3. The underrepresentation of outliers persists, confirming that intermediate dictionary sizes still struggle to capture weight extremes.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_a523343cdcbd42e0931d2b7cfe723957_proc_119936/synthetic_histogram_dim_256.png"}], [], [{"analysis": "Synthetic Dataset: Training and Validation Error reveals that overcomplete dictionaries (ratios 5:1 and 10:1) dramatically speed up convergence and achieve the lowest relative reconstruction errors: 10:1 sinks below 1 by epoch 50 and 5:1 drops to around 3, while the balanced 1:1 setting remains high (around 18) at the same point. Undercomplete regimes (1:5 and 1:10) converge more slowly and plateau in the 6\u20137 range, indicating insufficient capacity to capture the synthetic weight-vectors fully. Validation curves for all ratios stay very close to their training counterparts, with no substantial gap\u2014this suggests that even the largest dictionaries generalize well to held-out synthetic models and that overfitting is minimal.\n", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_57ac4e99a24644f9b02c4748160b834a_proc_119936/synthetic_train_val_error_curves.png"}, {"analysis": "Synthetic Dataset: Training and Validation Loss shows a nearly identical ranking: training MSE for 10:1 falls below 1.0 by the end of training, 5:1 near 1.5, 1:10 around 1.3, and 1:5 around 2.0, whereas 1:1 remains above 8.0. Validation MSE mirrors these trends, with only slight early-epoch fluctuations\u2014particularly a small bump around epoch 5 for the most overcomplete setting\u2014before settling down. The tight coupling between training and validation losses across all settings again points to strong generalization and indicates that increasing dictionary size is the primary driver of improved reconstruction performance on the synthetic dataset.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_57ac4e99a24644f9b02c4748160b834a_proc_119936/synthetic_train_val_loss_curves.png"}], [{"analysis": "Synthetic Dataset: Training and Validation Error\n\n- Increasing the dictionary size from undercomplete (1:5, 1:10) through balanced (1:1) to overcomplete (5:1, 10:1) yields progressively faster convergence and lower final training error. 10:1 reaches sub-1% relative error by epoch 20, 5:1 by around epoch 40, while 1:1 still sits near 18% at epoch 50 and undercomplete ratios level out between 5% and 7%.\n\n- Validation error mirrors the training trend with small gaps, indicating minimal overfitting. Overcomplete settings achieve the lowest initial and final validation errors (10:1: ~0.15%\u21920.02%), balanced is intermediate (~0.4%\u21920.1%), and undercomplete yields the highest curves (1:10: ~1.2%\u21920.4%).\n\n- Slight peaks in the undercomplete 1:10 curve around epochs 20\u201325 suggest transient instability when reconstructing weights with too few primitives.\n\n- Overall, more primitives lead to more powerful and efficient reconstruction on the synthetic weight dataset, while too few primitives underfit and slow down learning.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_9f9d63180c0b46548dd5bd69d0b77887_proc_119934/synthetic_train_val_error_curves.png"}, {"analysis": "Synthetic Dataset: Training and Validation Loss\n\n- Mean-squared reconstruction loss follows the same pattern: overcomplete dictionaries (10:1, 5:1) drive training loss to near zero fastest. 10:1 falls below 1.0 by epoch 15 and approaches ~0.5 by epoch 50; 5:1 and 1:5/1:10 settle around 1.0\u20132.0, with 1:1 peaking highest at ~8.5 then decaying to ~1.5.\n\n- Validation loss remains tightly coupled to training loss with minimal gap. Final validation losses rank 10:1 (~0.03) < 5:1 (~0.05) < 1:1 (~0.1) < 1:5 (~0.2) < 1:10 (~0.5).\n\n- Undercomplete schemes again show small mid-training bumps in validation loss, consistent with error peaks, highlighting limited expressivity when primitives are scarce.\n\n- These results confirm that dictionary size is a key component: larger, overcomplete sets of weight primitives yield superior reconstruction quality and generalization, while smaller sets underperform and converge more slowly.", "plot_path": "experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_9f9d63180c0b46548dd5bd69d0b77887_proc_119934/synthetic_train_val_loss_curves.png"}], []], "vlm_feedback_summary": ["Higher \u03b21 accelerates training reconstruction but hurts generalization, with\n\u03b21=0.99 overfitting heavily. \u03b21 around 0.7 offers the best trade\u2010off.\nReconstruction samples confirm the dictionary approach captures bulk weight\nstructure. Next tuning should focus on momentum \u22480.7 and possibly adjust\nlearning rate or sparsity constraints to better model high\u2010frequency components.", "Higher \u03b2\u2081 values consistently accelerate training convergence and yield lower\nfinal training error and loss, with the effect growing stronger as dataset\ncomplexity increases from ds3\u2192ds1. Validation performance remains flat and\ninvariant to \u03b2\u2081 across all datasets, indicating that \u03b2\u2081 primarily affects\noptimization dynamics rather than generalization. Dataset complexity scales both\nthe initial metric magnitudes and the magnitude of \u03b2\u2081\u2019s impact on convergence.", "Across all ablation studies on synthetic weight data, the number of dictionary\ncomponents controls the overall error scale (fewer components \u2192 higher\ninitial/final error). Momentum (\u03b21) exerts a consistent influence on convergence\nspeed and training loss depth: higher \u03b21 values yield faster, lower-loss\nsolutions but a small trade-off in validation drift. Validation loss remains\nnear zero in every setting, indicating that reconstruction on synthetic weights\nmay be too trivial. Future work should test real pretrained networks,\nincorporate tensor structure priors, and balance component count with\ngeneralization and sparse-coding efficiency.", "Across all four metrics\u2014code sparsity, reconstruction error,\nvalidation/generalization error, and dictionary recovery error\u2014varying the\nsparsity weight \u03bb has virtually no impact. Codes remain dense, reconstruction is\ntrivial on training and validation sets, and the dictionary drifts away from the\nground truth. These results suggest that the current regularization regime is\ntoo weak and that the optimization setup may need stronger sparsity enforcement,\nalternative solvers, or modified loss terms to recover meaningful weight\nprimitives.", "Across \u03b21 ablations, training and validation metrics converge rapidly and\nplateau similarly. Higher \u03b21 accelerates convergence and yields marginally\nbetter errors without inducing overfitting. Final performance is robust to \u03b21\nchoice, with \u03b21\u22650.9 providing the best trade-off.", "Reconstructions remain robust across all tested noise levels, with virtually\nperfect recovery at \u03c3 \u2264 0.01 and only slight smoothing at \u03c3 = 0.05. Relative-\nerror trends reflect normalization by noise amplitude, while absolute MSE decay\nis consistent and stable for training and validation. Overall results validate\nthe hypothesis that sparse combinations of weight primitives can reconstruct\nnoisy weight vectors with high fidelity.", "The ablation study on optimizer choice shows a clear trade-off: adaptive\noptimizers (RMSprop, AdamW) achieve faster convergence and lower training error\nbut suffer from deteriorating validation loss and higher generalization error,\nwhereas plain SGD converges slowly but maintains stable and low validation loss\nand error, making it more suitable for robust synthesis in the synthetic dataset\ncontext.", "Larger, overcomplete dictionaries (5:1, 10:1) yield faster convergence and lower\nreconstruction error/loss. Undercomplete dictionaries underperform relative to\nthe balanced 1:1 baseline but still generalize well. Validation remains flat,\nshowing no overfitting. Transient bumps for extreme ratios suggest brief\nadaptation issues in sparse coding, but these self-correct.", "Orthogonal dictionary initialization delivers the most reliable sparse\nrepresentations, with near-zero training/validation loss and stable low errors\nthroughout. Xavier uniform offers balanced performance but never matches\northogonal\u2019s tight fit. Zero initialization struggles initially but gradually\nimproves, while normal Gaussian dictionaries converge too slowly and exhibit\nlarge train-val gaps. These ablations confirm that dictionary initialization\ncritically shapes reconstruction quality and generalization in weight primitive\nlearning.", "Batch size strongly influences both reconstruction quality and generalization:\nsmall batches converge quickly and capture fine details but overfit, while large\nbatches converge slowly, yield smoother reconstructions, and generalize better.\nIntermediate batch sizes strike a balance between fidelity and robustness.", "All regularization penalties yield indistinguishable performance on the\nsynthetic dataset. Error and loss metrics show no divergence or benefit from\nsparsity-inducing penalties, suggesting the task is too easy or penalties too\nweak. Next steps: introduce more challenging data, tune regularization strength,\nor explore structured sparsity aligned with weight tensor geometry.", "Regularization has minor impact up to \u03bb2=1e-2; only \u03bb2=1e-1 slows convergence\nand raises error slightly. Generalization remains robust across \u03bb2 settings.", "[]", "Across all synthetic distributions and both loss and error metrics, higher\nmomentum (beta1=0.99) accelerates training convergence and yields the lowest in-\nsample error but severely degrades out-of-sample performance. Momentum values in\nthe mid-range (0.5\u20130.7) provide the best generalization, with beta1=0.7 often\nslightly outperforming 0.5. Beta1=0.9 consistently sits between these extremes.\nAmong distributions, uniform data is easiest to fit and least sensitive to\nbeta1; gaussian and block-sparse introduce more variance, amplifying the trade-\noff between convergence speed and overfitting. This supports an ablation\nconclusion: momentum should be carefully tuned, as aggressive settings can harm\nmodel synthesis quality.", "Overall, all three loss functions (MSE, MAE, Huber) and a range of beta1 values\ndeliver stable convergence and strong predictive alignment on the synthetic\ndataset. MSE offers the fastest and tightest performance, while MAE and Huber\nprovide robustness trade-offs. Variations in beta1 impact early training speed\nbut have negligible effect on final error. Recommended default: beta1=0.9 with\nchoice of loss based on outlier tolerance (MSE for speed/precision, Huber or MAE\nfor robustness).", "The ablation demonstrates that continuous solvers (pseudoinverse, ISTA, FISTA)\noutperform the greedy OMP in both loss and error measures. Among the continuous\nmethods, performance differences are negligible, though FISTA offers the\npotential for faster convergence in practice. Training curves confirm stable,\nmonotonic learning dynamics under the dictionary\u2010based synthesis framework.", "Learning-rate scheduling offers clear benefits: step-decay maximizes\ngeneralization, fixed-rate optimizes training loss at the expense of validation\nperformance, exponential decay shows an implementation anomaly, and cosine finds\na middle ground. Next steps include fixing validation-loss logging, correcting\nthe exponential-decay implementation, and tuning schedule hyperparameters before\nmoving to real vision benchmarks.", "Momentum coefficient \u03b21 has a strong impact on training convergence across all\nnoise models, with \u03b21=0.99 yielding the fastest reduction in error and loss;\nimprovements taper off beyond \u03b21\u22480.9. Validation metrics are nearly flat and\nextremely low relative to training curves, suggesting a plotting or computation\nissue. Distribution choice affects scale of metrics (Cauchy shows higher MSE but\nlower normalized error range). Recommendations: correct validation metric\nplotting, verify held-out evaluation pipeline, and extend analysis to additional\nhyperparameters such as dictionary size and solver speed in the next ablation\nstage.", "Sparse dictionary reconstructions consistently replicate the central bulk of\nweight distributions but underrepresent tails, leading to truncated support in\ngenerated models. Training metrics (relative error and MSE) improve with fewer\nprimitives and across epochs, whereas validation metrics remain flat,\nhighlighting a gap in generalization. Optimal dictionary sizes balance faster\nconvergence and lower training loss with validation performance, but none fully\nrecover extreme weight values. Future work could incorporate heavy\u2010tailed priors\nor structured sparsity to better capture outliers and improve validation\noutcomes.", "[]", "Overcomplete dictionaries (high primitive-to-weight ratios) substantially\nimprove convergence speed and reconstruction quality on both error and loss\nmetrics, with minimal overfitting; undercomplete and balanced settings perform\nprogressively worse, confirming that dictionary capacity is crucial for accurate\nweight-synthesis.", "A clear ablation on dictionary size ratio shows that overcomplete dictionaries\n(10:1, 5:1) offer the best reconstruction speed and accuracy, balanced\ndictionaries are intermediate, and undercomplete dictionaries underfit.\nValidation tracks training closely, suggesting low overfitting in this synthetic\nsetting.", "[]"], "exec_time": [1.9033818244934082, 2.8290305137634277, 2.920515775680542, 2.0809578895568848, 1.9460783004760742, 2.0505614280700684, 1.8715846538543701, 3.790515184402466, 3.2682933807373047, 4.0195136070251465, 1.9649412631988525, 2.1747002601623535, 1.521240234375, 3.1118898391723633, 2.861691951751709, 4.238966226577759, 1.9231810569763184, 2.7184951305389404, 1.9592580795288086, 4.157670021057129, 3.967167615890503, 4.120988607406616, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['synthetic']"], ["['ds1'", "'ds2'", "'ds3']"], ["['synthetic']"], ["[]"], ["[\"synthetic\"]"], ["['synthetic_noise']"], ["['synthetic']"], ["[\"synthetic\"]"], ["['synthetic']"], ["['synthetic']"], [""], ["['synthetic']"], [], ["[\"laplace\"", "\"sparse-block\"", "\"uniform\"", "\"gaussian\"]"], ["['synthetic']"], ["['synthetic']"], [""], ["[Laplace", "Gaussian", "Cauchy]"], [""], [], ["['synthetic']"], ["[\"synthetic\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    ed = data[\"adam_beta1\"][\"synthetic\"]\n    train_errs = ed[\"metrics\"][\"train\"]\n    val_errs = ed[\"metrics\"][\"val\"]\n    train_losses = ed[\"losses\"][\"train\"]\n    val_losses = ed[\"losses\"][\"val\"]\n    beta1_list = [0.5, 0.7, 0.9, 0.99]\n    epochs = len(train_errs[0])\n    xs = np.arange(1, epochs + 1)\n\n    try:\n        plt.figure()\n        for errs, b1 in zip(train_errs, beta1_list):\n            plt.plot(xs, errs, label=f\"\u03b21={b1}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Relative Error\")\n        plt.title(\"Training Error vs Epoch - synthetic dataset\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_training_error.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot1: {e}\")\n        plt.close()\n\n    try:\n        plt.figure()\n        for errs, b1 in zip(val_errs, beta1_list):\n            plt.plot(xs, errs, label=f\"\u03b21={b1}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Relative Error\")\n        plt.title(\"Validation Error vs Epoch - synthetic dataset\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_validation_error.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot2: {e}\")\n        plt.close()\n\n    try:\n        plt.figure()\n        for ls, b1 in zip(train_losses, beta1_list):\n            plt.plot(xs, ls, label=f\"\u03b21={b1}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Reconstruction Loss\")\n        plt.title(\"Training Loss vs Epoch - synthetic dataset\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_training_loss.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot3: {e}\")\n        plt.close()\n\n    try:\n        plt.figure()\n        for ls, b1 in zip(val_losses, beta1_list):\n            plt.plot(xs, ls, label=f\"\u03b21={b1}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"MSE on Test\")\n        plt.title(\"Validation Loss vs Epoch - synthetic dataset\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_validation_loss.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot4: {e}\")\n        plt.close()\n\n    try:\n        final_vals = [v[-1] for v in val_errs]\n        best_idx = int(np.argmin(final_vals))\n        gt = ed[\"ground_truth\"][best_idx][0]\n        pr = ed[\"predictions\"][best_idx][0]\n        b1 = beta1_list[best_idx]\n        plt.figure()\n        ax1 = plt.subplot(1, 2, 1)\n        ax1.plot(gt)\n        ax1.set_title(\"Ground Truth Sample\")\n        ax2 = plt.subplot(1, 2, 2)\n        ax2.plot(pr)\n        ax2.set_title(f\"Generated Sample (\u03b21={b1})\")\n        plt.suptitle(\n            \"Sample Reconstruction Comparison - Left: Ground Truth, Right: Generated Samples - synthetic dataset\"\n        )\n        plt.savefig(\n            os.path.join(working_dir, f\"synthetic_sample_reconstruction_beta1_{b1}.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot5: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nbeta1_list = [0.5, 0.7, 0.9, 0.99]\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# iterate over synthetic datasets\nfor name, ed in experiment_data.get(\"multi_synthetic\", {}).items():\n    try:\n        fig = plt.figure(figsize=(10, 5))\n        fig.suptitle(f\"Multi-Synthetic {name} Results\")\n        # Left: Errors\n        ax1 = fig.add_subplot(1, 2, 1)\n        for i, (tr_err, val_err) in enumerate(\n            zip(ed[\"metrics\"][\"train\"], ed[\"metrics\"][\"val\"])\n        ):\n            beta = beta1_list[i]\n            ax1.plot(range(1, len(tr_err) + 1), tr_err, label=f\"\u03b21={beta} train\")\n            ax1.plot(\n                range(1, len(val_err) + 1),\n                val_err,\n                linestyle=\"--\",\n                label=f\"\u03b21={beta} val\",\n            )\n        ax1.set_xlabel(\"Epoch\")\n        ax1.set_ylabel(\"Relative Error\")\n        ax1.set_title(\"Left: Training & Validation Error\")\n        ax1.legend()\n        # Right: Losses\n        ax2 = fig.add_subplot(1, 2, 2)\n        for i, (tr_loss, val_loss) in enumerate(\n            zip(ed[\"losses\"][\"train\"], ed[\"losses\"][\"val\"])\n        ):\n            beta = beta1_list[i]\n            ax2.plot(range(1, len(tr_loss) + 1), tr_loss, label=f\"\u03b21={beta} train\")\n            ax2.plot(\n                range(1, len(val_loss) + 1),\n                val_loss,\n                linestyle=\"--\",\n                label=f\"\u03b21={beta} val\",\n            )\n        ax2.set_xlabel(\"Epoch\")\n        ax2.set_ylabel(\"MSE Loss\")\n        ax2.set_title(\"Right: Training & Validation Loss\")\n        ax2.legend()\n        plt.tight_layout(rect=[0, 0, 1, 0.95])\n        plt.savefig(os.path.join(working_dir, f\"multi_synthetic_{name}_error_loss.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating multi_synthetic_{name}_error_loss plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Extract synthetic results\ned = experiment_data.get(\"dictionary_capacity\", {}).get(\"synthetic\", {})\nn_components_list = ed.get(\"n_components_list\", [])\nbeta1_list = ed.get(\"beta1_list\", [])\nmetrics_train = ed.get(\"metrics\", {}).get(\"train\", [])\nmetrics_val = ed.get(\"metrics\", {}).get(\"val\", [])\nloss_train = ed.get(\"losses\", {}).get(\"train\", [])\nloss_val = ed.get(\"losses\", {}).get(\"val\", [])\n\n# Plot reconstruction error curves for each dictionary size\nfor idx, n_comp in enumerate(n_components_list):\n    start = idx * len(beta1_list)\n    try:\n        plt.figure()\n        for j, b1 in enumerate(beta1_list):\n            plt.plot(metrics_train[start + j], label=f\"Train \u03b21={b1}\")\n            plt.plot(metrics_val[start + j], \"--\", label=f\"Val \u03b21={b1}\")\n        plt.title(f\"Synthetic: Reconstruction Error Curves (n_components={n_comp})\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Relative Error\")\n        plt.legend()\n        fname = f\"synthetic_error_curves_nc{n_comp}.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating error curves for n_components={n_comp}: {e}\")\n        plt.close()\n\n# Plot loss curves for smallest and largest dictionary sizes\nfor idx in [0, len(n_components_list) - 1] if n_components_list else []:\n    n_comp = n_components_list[idx]\n    start = idx * len(beta1_list)\n    try:\n        plt.figure()\n        for j, b1 in enumerate(beta1_list):\n            plt.plot(loss_train[start + j], label=f\"Train \u03b21={b1}\")\n            plt.plot(loss_val[start + j], \"--\", label=f\"Val \u03b21={b1}\")\n        plt.title(f\"Synthetic: MSE Loss Curves (n_components={n_comp})\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"MSE Loss\")\n        plt.legend()\n        fname = f\"synthetic_loss_curves_nc{n_comp}.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves for n_components={n_comp}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = experiment_data[\"sparsity_strength_ablation\"][\"synthetic\"]\n    train_errs = data[\"metrics\"][\"train\"]\n    val_errs = data[\"metrics\"][\"val\"]\n    train_losses = data[\"losses\"][\"train\"]\n    val_losses = data[\"losses\"][\"val\"]\n    sparsities = data[\"sparsity\"]\n    dict_errors = data[\"dict_error\"]\n    lambda_list = [0.0, 1e-4, 1e-3, 1e-2, 1e-1]\n    epochs = len(train_errs[0])\n    x = np.arange(1, epochs + 1)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot error curves\ntry:\n    plt.figure()\n    for i, lam in enumerate(lambda_list):\n        plt.plot(x, train_errs[i], label=f\"train \u03bb={lam}\")\n        plt.plot(x, val_errs[i], \"--\", label=f\"val \u03bb={lam}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Relative Error\")\n    plt.title(\"Synthetic Dataset: Training (solid) and Validation (dashed) Errors\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_error_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating error plot: {e}\")\n    plt.close()\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for i, lam in enumerate(lambda_list):\n        plt.plot(x, train_losses[i], label=f\"train \u03bb={lam}\")\n        plt.plot(x, val_losses[i], \"--\", label=f\"val \u03bb={lam}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Reconstruction Loss\")\n    plt.title(\"Synthetic Dataset: Training (solid) and Validation (dashed) Losses\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Plot sparsity over epochs\ntry:\n    plt.figure()\n    for i, lam in enumerate(lambda_list):\n        plt.plot(x, sparsities[i], label=f\"\u03bb={lam}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Code Sparsity Fraction\")\n    plt.title(\"Synthetic Dataset: Code Sparsity Over Epochs\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_sparsity.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating sparsity plot: {e}\")\n    plt.close()\n\n# Plot dictionary recovery error\ntry:\n    plt.figure()\n    for i, lam in enumerate(lambda_list):\n        plt.plot(x, dict_errors[i], label=f\"\u03bb={lam}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Relative Dictionary Recovery Error\")\n    plt.title(\"Synthetic Dataset: Dictionary Recovery Error Over Epochs\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_dict_recovery_error.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating dict error plot: {e}\")\n    plt.close()\n\n# Print final validation errors\nif \"lambda_list\" in locals():\n    print(\"Final Validation Errors per Lambda:\")\n    for lam, vals in zip(lambda_list, val_errs):\n        print(f\"Lambda {lam}: {vals[-1]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    data = experiment_data[\"atom_norm_projection\"][\"synthetic\"]\n    metrics_train = data[\"metrics\"][\"train\"]\n    metrics_val = data[\"metrics\"][\"val\"]\n    losses_train = data[\"losses\"][\"train\"]\n    losses_val = data[\"losses\"][\"val\"]\n    betas = [0.5, 0.7, 0.9, 0.99]\n    epochs = len(metrics_train[0])\n\n    # Plot error curves\n    try:\n        plt.figure()\n        for i, b in enumerate(betas):\n            plt.plot(range(1, epochs + 1), metrics_train[i], label=f\"Train \u03b21={b}\")\n            plt.plot(range(1, epochs + 1), metrics_val[i], \"--\", label=f\"Val \u03b21={b}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Relative Error\")\n        plt.title(\"Synthetic Dataset: Error Curves\\nLeft: Solid=Train, Dashed=Val\")\n        plt.legend()\n        plt.savefig(\n            os.path.join(working_dir, \"synthetic_error_curves_atom_norm_projection.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating error curves plot: {e}\")\n        plt.close()\n\n    # Plot loss curves\n    try:\n        plt.figure()\n        for i, b in enumerate(betas):\n            plt.plot(range(1, epochs + 1), losses_train[i], label=f\"Train \u03b21={b}\")\n            plt.plot(range(1, epochs + 1), losses_val[i], \"--\", label=f\"Val \u03b21={b}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"MSE Loss\")\n        plt.title(\"Synthetic Dataset: Loss Curves\\nLeft: Solid=Train, Dashed=Val\")\n        plt.legend()\n        plt.savefig(\n            os.path.join(working_dir, \"synthetic_loss_curves_atom_norm_projection.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves plot: {e}\")\n        plt.close()\n\n    # Print final validation errors\n    for i, b in enumerate(betas):\n        print(f\"Final validation error for beta1={b}: {metrics_val[i][-1]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\ntry:\n    data = experiment_data[\"synthetic_noise\"][\"synthetic\"]\n    metrics_train = data[\"metrics\"][\"train\"]\n    metrics_val = data[\"metrics\"][\"val\"]\n    losses_train = data[\"losses\"][\"train\"]\n    losses_val = data[\"losses\"][\"val\"]\n    predictions = data[\"predictions\"]\n    ground_truth = data[\"ground_truth\"]\n    noise_levels = data[\"noise_levels\"]\nexcept Exception as e:\n    print(f\"Error extracting data: {e}\")\n\ntry:\n    plt.figure()\n    epochs = metrics_train.shape[1]\n    for i, sigma in enumerate(noise_levels):\n        plt.plot(range(1, epochs + 1), metrics_train[i], label=f\"train \u03c3={sigma}\")\n        plt.plot(range(1, epochs + 1), metrics_val[i], \"--\", label=f\"val \u03c3={sigma}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Relative Error\")\n    plt.title(\"Training and Validation Error Curves - synthetic dataset\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_error_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating error curves plot: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    for i, sigma in enumerate(noise_levels):\n        plt.plot(range(1, epochs + 1), losses_train[i], label=f\"train \u03c3={sigma}\")\n        plt.plot(range(1, epochs + 1), losses_val[i], \"--\", label=f\"val \u03c3={sigma}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"MSE Loss\")\n    plt.title(\"Training and Validation Loss Curves - synthetic dataset\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\nfor idx, sigma in enumerate(noise_levels):\n    try:\n        plt.figure()\n        sample = 0\n        plt.subplot(1, 2, 1)\n        plt.plot(ground_truth[idx, sample])\n        plt.title(\"Left: Ground Truth\")\n        plt.subplot(1, 2, 2)\n        plt.plot(predictions[idx, sample])\n        plt.title(\"Right: Generated Samples\")\n        plt.suptitle(f\"Reconstruction for Noise \u03c3={sigma} - synthetic dataset\")\n        fname = f'synthetic_reconstruction_noise_{str(sigma).replace(\".\", \"p\")}.png'\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating reconstruction plot for \u03c3={sigma}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\ned = experiment_data[\"optimizer_choice\"][\"synthetic\"]\nmetrics = ed[\"metrics\"]\nlosses = ed[\"losses\"]\npredictions = ed[\"predictions\"]\nground_truth = ed[\"ground_truth\"]\nopts = ed[\"optimizers\"]\n\n# Convert to numpy arrays\nmetrics_train = np.array(metrics[\"train\"])\nmetrics_val = np.array(metrics[\"val\"])\nlosses_train = np.array(losses[\"train\"])\nlosses_val = np.array(losses[\"val\"])\n\n# Print final errors for each optimizer\nfor i, opt in enumerate(opts):\n    print(\n        f\"{opt}: final train error={metrics_train[i, -1]:.4f}, final val error={metrics_val[i, -1]:.4f}\"\n    )\n\n# Plot training error curves\ntry:\n    plt.figure()\n    for i, opt in enumerate(opts):\n        plt.plot(metrics_train[i], label=opt)\n    plt.title(\"Synthetic Dataset: Training Error Curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Relative Error\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_training_error_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating train error plot: {e}\")\n    plt.close()\n\n# Plot validation error curves\ntry:\n    plt.figure()\n    for i, opt in enumerate(opts):\n        plt.plot(metrics_val[i], label=opt)\n    plt.title(\"Synthetic Dataset: Validation Error Curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Relative Error\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_validation_error_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation error plot: {e}\")\n    plt.close()\n\n# Plot training loss curves\ntry:\n    plt.figure()\n    for i, opt in enumerate(opts):\n        plt.plot(losses_train[i], label=opt)\n    plt.title(\"Synthetic Dataset: Training Loss Curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_training_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating training loss plot: {e}\")\n    plt.close()\n\n# Plot validation loss curves\ntry:\n    plt.figure()\n    for i, opt in enumerate(opts):\n        plt.plot(losses_val[i], label=opt)\n    plt.title(\"Synthetic Dataset: Validation Loss Curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_validation_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation loss plot: {e}\")\n    plt.close()\n\n# Plot ground truth vs reconstructed sample for best optimizer\ntry:\n    best_idx = np.argmin(metrics_val[:, -1])\n    best_opt = opts[best_idx]\n    gt = ground_truth[best_idx][0]\n    pred = predictions[best_idx][0]\n    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n    axs[0].plot(gt)\n    axs[0].set_title(\"Ground Truth\")\n    axs[1].plot(pred)\n    axs[1].set_title(\"Generated Sample\")\n    fig.suptitle(\n        f\"Synthetic Dataset Sample Reconstruction ({best_opt})\\nLeft: Ground Truth, Right: Generated Samples\"\n    )\n    plt.savefig(os.path.join(working_dir, f\"synthetic_reconstruction_{best_opt}.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating reconstruction plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# load data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\ned = experiment_data.get(\"alt_min_freq\", {}).get(\"synthetic\", {})\n\nmetrics_train = ed.get(\"metrics\", {}).get(\"train\", [])\nmetrics_val = ed.get(\"metrics\", {}).get(\"val\", [])\nlosses_train = ed.get(\"losses\", {}).get(\"train\", [])\nlosses_val = ed.get(\"losses\", {}).get(\"val\", [])\nratios = ed.get(\"ratios\", [])\n\n# Plot error curves\ntry:\n    plt.figure()\n    for (c, d), tr, vl in zip(ratios, metrics_train, metrics_val):\n        epochs = range(1, len(tr) + 1)\n        plt.plot(epochs, tr, label=f\"{c}:{d} train\", linestyle=\"-\")\n        plt.plot(epochs, vl, label=f\"{c}:{d} val\", linestyle=\"--\")\n    plt.title(\"Synthetic Dataset: Training and Validation Error\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Relative Error\")\n    plt.legend()\n    plt.suptitle(\"Solid: Training, Dashed: Validation\", fontsize=10)\n    plt.savefig(os.path.join(working_dir, \"synthetic_train_val_error_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating error curves plot: {e}\")\n    plt.close()\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for (c, d), tr, vl in zip(ratios, losses_train, losses_val):\n        epochs = range(1, len(tr) + 1)\n        plt.plot(epochs, tr, label=f\"{c}:{d} train\", linestyle=\"-\")\n        plt.plot(epochs, vl, label=f\"{c}:{d} val\", linestyle=\"--\")\n    plt.title(\"Synthetic Dataset: Training and Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"MSE Loss\")\n    plt.legend()\n    plt.suptitle(\"Solid: Training, Dashed: Validation\", fontsize=10)\n    plt.savefig(os.path.join(working_dir, \"synthetic_train_val_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Print final validation errors\nif ratios and metrics_val:\n    print(\"Final validation errors per ratio:\")\n    for (c, d), vl in zip(ratios, metrics_val):\n        print(f\"Ratio {c}:{d} -> {vl[-1]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    data = experiment_data[\"initialization\"][\"synthetic\"]\n    train_errs = np.array(data[\"metrics\"][\"train\"])\n    val_errs = np.array(data[\"metrics\"][\"val\"])\n    train_losses = np.array(data[\"losses\"][\"train\"])\n    val_losses = np.array(data[\"losses\"][\"val\"])\n    schemes = data[\"init_schemes\"]\n    init_D_list = sorted({s[\"D\"] for s in schemes})\n\n    for D in init_D_list:\n        idxs = [i for i, s in enumerate(schemes) if s[\"D\"] == D]\n        print(\n            f\"Final Avg Validation Error for D={D}: \" f\"{val_errs[idxs, -1].mean():.4f}\"\n        )\n\n    try:\n        plt.figure()\n        for D in init_D_list:\n            idxs = [i for i, s in enumerate(schemes) if s[\"D\"] == D]\n            plt.plot(train_errs[idxs].mean(axis=0), label=f\"D: {D}\")\n        plt.title(\"Synthetic Dataset - Training Error Curves\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Relative Error\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_training_error_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating training error plot: {e}\")\n        plt.close()\n\n    try:\n        plt.figure()\n        for D in init_D_list:\n            idxs = [i for i, s in enumerate(schemes) if s[\"D\"] == D]\n            plt.plot(val_errs[idxs].mean(axis=0), label=f\"D: {D}\")\n        plt.title(\"Synthetic Dataset - Validation Error Curves\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Relative Error\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_validation_error_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating validation error plot: {e}\")\n        plt.close()\n\n    try:\n        plt.figure()\n        for D in init_D_list:\n            idxs = [i for i, s in enumerate(schemes) if s[\"D\"] == D]\n            plt.plot(\n                train_losses[idxs].mean(axis=0), linestyle=\"--\", label=f\"{D} Train\"\n            )\n            plt.plot(val_losses[idxs].mean(axis=0), linestyle=\"-\", label=f\"{D} Val\")\n        plt.title(\"Synthetic Dataset - Training & Validation Loss Curves\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"MSE Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    synthetic = experiment_data[\"mini_batch_size\"][\"synthetic\"]\n    batch_sizes = synthetic[\"batch_sizes\"]\n    train_errs = synthetic[\"metrics\"][\"train\"]\n    val_errs = synthetic[\"metrics\"][\"val\"]\n    train_losses = synthetic[\"losses\"][\"train\"]\n    val_losses = synthetic[\"losses\"][\"val\"]\n    predictions = synthetic[\"predictions\"]\n    ground_truths = synthetic[\"ground_truth\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot 1: Error curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    for errs, bs in zip(train_errs, batch_sizes):\n        axes[0].plot(range(1, len(errs) + 1), errs, label=f\"bs={bs}\")\n    for errs, bs in zip(val_errs, batch_sizes):\n        axes[1].plot(range(1, len(errs) + 1), errs, label=f\"bs={bs}\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Error\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Error\")\n    axes[0].legend()\n    axes[1].legend()\n    fig.suptitle(\n        \"Synthetic Dataset - Error Curves\\nLeft: Training Error, Right: Validation Error\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_error_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\n# Plot 2: Loss curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    for ls, bs in zip(train_losses, batch_sizes):\n        axes[0].plot(range(1, len(ls) + 1), ls, label=f\"bs={bs}\")\n    for ls, bs in zip(val_losses, batch_sizes):\n        axes[1].plot(range(1, len(ls) + 1), ls, label=f\"bs={bs}\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    axes[0].legend()\n    axes[1].legend()\n    fig.suptitle(\n        \"Synthetic Dataset - Loss Curves\\nLeft: Training Loss, Right: Validation Loss\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n\n# Plots 3-6: Sample reconstructions for each batch size\nfor idx, bs in enumerate(batch_sizes, start=3):\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n        gt = ground_truths[idx - 3]\n        pred = predictions[idx - 3]\n        axes[0].plot(gt[0], color=\"blue\")\n        axes[1].plot(pred[0], color=\"orange\")\n        axes[0].set_title(\"Ground Truth Sample 0\")\n        axes[1].set_title(\"Predicted Sample 0\")\n        axes[0].set_xlabel(\"Dimension\")\n        axes[1].set_xlabel(\"Dimension\")\n        fig.suptitle(\n            f\"Synthetic Dataset - Sample Recon (bs={bs})\\nLeft: Ground Truth, Right: Predicted\"\n        )\n        plt.savefig(os.path.join(working_dir, f\"synthetic_sample_bs_{bs}.png\"))\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating plot{idx}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\npenalties = [\"none\", \"l1\", \"l2\", \"elasticnet\"]\n\ntry:\n    plt.figure()\n    for p in penalties:\n        tr = data[p][\"synthetic\"][\"metrics\"][\"train\"][0]\n        vl = data[p][\"synthetic\"][\"metrics\"][\"val\"][0]\n        plt.plot(tr, label=f\"{p} train\")\n        plt.plot(vl, \"--\", label=f\"{p} val\")\n    plt.suptitle(\"Dataset: Synthetic\")\n    plt.title(\"Training and Validation Error Curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Relative Error\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_error_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating error curves plot: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    for p in penalties:\n        trl = data[p][\"synthetic\"][\"losses\"][\"train\"][0]\n        vll = data[p][\"synthetic\"][\"losses\"][\"val\"][0]\n        plt.plot(trl, label=f\"{p} train\")\n        plt.plot(vll, \"--\", label=f\"{p} val\")\n    plt.suptitle(\"Dataset: Synthetic\")\n    plt.title(\"Training and Validation Loss Curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"MSE Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\ntry:\n    finals = [data[p][\"synthetic\"][\"metrics\"][\"val\"][0][-1] for p in penalties]\n    x = np.arange(len(penalties))\n    plt.figure()\n    plt.bar(x, finals, tick_label=penalties)\n    plt.suptitle(\"Dataset: Synthetic\")\n    plt.title(\"Final Validation Error per Penalty Type\")\n    plt.ylabel(\"Relative Error\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_final_val_error.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final val error bar chart: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# load data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# extract synthetic results\ned = experiment_data.get(\"dict_orth\", {}).get(\"synthetic\", {})\ntrain_metrics = ed.get(\"metrics\", {}).get(\"train\", [])\nval_metrics = ed.get(\"metrics\", {}).get(\"val\", [])\nloss_train = ed.get(\"losses\", {}).get(\"train\", [])\nloss_val = ed.get(\"losses\", {}).get(\"val\", [])\npredictions = ed.get(\"predictions\", [])\nground_truth = ed.get(\"ground_truth\", [])\n\n# define lambda2 grid\nlam2_list = [0.0, 1e-4, 1e-3, 1e-2, 1e-1]\nlam2_str = [\"0\", \"1e-4\", \"1e-3\", \"1e-2\", \"1e-1\"]\n\n# compute and print final test errors\nif val_metrics:\n    final_errs = {lam: round(vals[-1], 4) for lam, vals in zip(lam2_str, val_metrics)}\n    print(\"Final test relative errors per \u03bb2:\")\n    for lam, err in final_errs.items():\n        print(f\"  \u03bb2={lam}: {err}\")\n\n# plot relative error curves\ntry:\n    plt.figure()\n    for lam, lam_s, tr, vl in zip(lam2_list, lam2_str, train_metrics, val_metrics):\n        plt.plot(tr, label=f\"Train \u03bb2={lam_s}\")\n        plt.plot(vl, \"--\", label=f\"Val   \u03bb2={lam_s}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Relative Error\")\n    plt.title(\"Synthetic Dataset: Training & Validation Relative Error Curves\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_error_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating error curves: {e}\")\n    plt.close()\n\n# plot loss curves\ntry:\n    plt.figure()\n    for lam, lam_s, lt, lv in zip(lam2_list, lam2_str, loss_train, loss_val):\n        plt.plot(lt, label=f\"Train \u03bb2={lam_s}\")\n        plt.plot(lv, \"--\", label=f\"Val   \u03bb2={lam_s}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"MSE Loss\")\n    plt.title(\"Synthetic Dataset: Training & Validation Loss Curves\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# reconstruction examples for three \u03bb2 values\nfor idx in [0, 2, 4]:\n    lam_s = lam2_str[idx]\n    try:\n        gt = ground_truth[idx][0].reshape(32, 32)\n        pred = predictions[idx][0].reshape(32, 32)\n        plt.figure(figsize=(6, 3))\n        plt.subplot(1, 2, 1)\n        plt.imshow(gt, cmap=\"viridis\")\n        plt.title(\"Ground Truth\")\n        plt.axis(\"off\")\n        plt.subplot(1, 2, 2)\n        plt.imshow(pred, cmap=\"viridis\")\n        plt.title(\"Reconstructed\")\n        plt.axis(\"off\")\n        plt.suptitle(\n            f\"Synthetic Dataset; Left: Ground Truth, Right: Reconstructed Samples (\u03bb2={lam_s})\"\n        )\n        fname = f\"synthetic_recon_lambda2_{lam_s}.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating recon plot for \u03bb2={lam_s}: {e}\")\n        plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nbeta1_list = [0.5, 0.7, 0.9, 0.99]\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor name, ed in experiment_data.get(\"synthetic_code_distribution\", {}).items():\n    metrics, losses = ed.get(\"metrics\", {}), ed.get(\"losses\", {})\n    train_errs, val_errs = metrics.get(\"train\", []), metrics.get(\"val\", [])\n    train_losses, val_losses = losses.get(\"train\", []), losses.get(\"val\", [])\n    if not train_errs:\n        continue\n    epochs = len(train_errs[0])\n    x = np.arange(1, epochs + 1)\n\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n        for idx, b1 in enumerate(beta1_list):\n            axes[0].plot(x, train_errs[idx], label=f\"beta1={b1}\")\n            axes[1].plot(x, val_errs[idx], label=f\"beta1={b1}\")\n        axes[0].set_title(\"Training Error\")\n        axes[0].set_xlabel(\"Epoch\")\n        axes[0].set_ylabel(\"Relative Error\")\n        axes[1].set_title(\"Validation Error\")\n        axes[1].set_xlabel(\"Epoch\")\n        axes[1].set_ylabel(\"Relative Error\")\n        fig.suptitle(\n            f\"{name} Synthetic Distribution: Error Curves\\nLeft: Training Error, Right: Validation Error\"\n        )\n        axes[0].legend()\n        axes[1].legend()\n        plt.savefig(os.path.join(working_dir, f\"{name}_error_curves.png\"))\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating plot for {name} error curves: {e}\")\n        plt.close()\n\n    try:\n        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n        for idx, b1 in enumerate(beta1_list):\n            axes[0].plot(x, train_losses[idx], label=f\"beta1={b1}\")\n            axes[1].plot(x, val_losses[idx], label=f\"beta1={b1}\")\n        axes[0].set_title(\"Training Loss\")\n        axes[0].set_xlabel(\"Epoch\")\n        axes[0].set_ylabel(\"MSE Loss\")\n        axes[1].set_title(\"Validation Loss\")\n        axes[1].set_xlabel(\"Epoch\")\n        axes[1].set_ylabel(\"MSE Loss\")\n        fig.suptitle(\n            f\"{name} Synthetic Distribution: Loss Curves\\nLeft: Training Loss, Right: Validation Loss\"\n        )\n        axes[0].legend()\n        axes[1].legend()\n        plt.savefig(os.path.join(working_dir, f\"{name}_loss_curves.png\"))\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating plot for {name} loss curves: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Reconstruct parameter lists\nloss_names = [\"mse\", \"mae\", \"huber\"]\nbeta1_list = [0.5, 0.7, 0.9, 0.99]\nsyn = data[\"reconstruction_loss\"][\"synthetic\"]\nmetrics = syn[\"metrics\"]\nlosses_data = syn[\"losses\"]\n\n# Plot error curves\ntry:\n    plt.figure()\n    for i, loss_name in enumerate(loss_names):\n        for j, b1 in enumerate(beta1_list):\n            idx = i * len(beta1_list) + j\n            tr = metrics[\"train\"][idx]\n            vl = metrics[\"val\"][idx]\n            plt.plot(tr, label=f\"{loss_name}-train b1={b1}\")\n            plt.plot(vl, linestyle=\"--\", label=f\"{loss_name}-val b1={b1}\")\n    plt.title(\"Error Curves (Synthetic Dataset)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Relative Error\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_error_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating error curves: {e}\")\n    plt.close()\n\n# Plot reconstruction loss curves\ntry:\n    plt.figure()\n    for i, loss_name in enumerate(loss_names):\n        for j, b1 in enumerate(beta1_list):\n            idx = i * len(beta1_list) + j\n            trl = losses_data[\"train\"][idx]\n            vll = losses_data[\"val\"][idx]\n            plt.plot(trl, label=f\"{loss_name}-train b1={b1}\")\n            plt.plot(vll, linestyle=\"--\", label=f\"{loss_name}-val b1={b1}\")\n    plt.title(\"Loss Curves (Synthetic Dataset)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Reconstruction Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# Scatter plots of predictions vs ground truth for each loss type\nfor i, loss_name in enumerate(loss_names):\n    idx = i * len(beta1_list)  # take the first beta1 setting for each loss\n    try:\n        plt.figure()\n        preds = syn[\"predictions\"][idx]\n        gts = syn[\"ground_truth\"][idx]\n        p_flat = preds.flatten()\n        g_flat = gts.flatten()\n        plt.scatter(g_flat, p_flat, s=1)\n        mn, mx = min(g_flat.min(), p_flat.min()), max(g_flat.max(), p_flat.max())\n        plt.plot([mn, mx], [mn, mx], \"r--\")\n        plt.title(\n            f\"Scatter: Predictions vs Ground Truth\\nDataset: Synthetic, Loss: {loss_name}\"\n        )\n        plt.xlabel(\"Ground Truth\")\n        plt.ylabel(\"Predictions\")\n        plt.savefig(os.path.join(working_dir, f\"synthetic_pred_vs_gt_{loss_name}.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating pred vs gt for {loss_name}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndata = experiment_data.get(\"test_time_solver_ablation\", {}).get(\"synthetic\", {})\nsolver_names = data.get(\"solver_names\", [])\ntrain_err = data.get(\"metrics\", {}).get(\"train_err\", np.array([]))\nval_err = data.get(\"metrics\", {}).get(\"val_err\", {})\ntrain_loss = data.get(\"losses\", {}).get(\"train_loss\", np.array([]))\nval_loss = data.get(\"losses\", {}).get(\"val_loss\", {})\n\n# Plot 1: Training Error Curve\ntry:\n    plt.figure()\n    plt.plot(train_err, label=\"Train Error\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Error\")\n    plt.title(\"Training Error Curve (Synthetic Dataset)\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_training_error_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\n# Plot 2: Training Loss Curve\ntry:\n    plt.figure()\n    plt.plot(train_loss, label=\"Train Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training Loss Curve (Synthetic Dataset)\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_training_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n\n# Plot 3: Validation Error for Solvers\ntry:\n    plt.figure()\n    for name in solver_names:\n        if name in val_err:\n            plt.plot(val_err[name], label=name)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Error\")\n    plt.title(\"Validation Error Curve - Solver Ablation (Synthetic Dataset)\")\n    plt.legend()\n    plt.savefig(\n        os.path.join(working_dir, \"synthetic_validation_error_solver_ablation.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot3: {e}\")\n    plt.close()\n\n# Plot 4: Validation Loss for Solvers\ntry:\n    plt.figure()\n    for name in solver_names:\n        if name in val_loss:\n            plt.plot(val_loss[name], label=name)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Loss\")\n    plt.title(\"Validation Loss Curve - Solver Ablation (Synthetic Dataset)\")\n    plt.legend()\n    plt.savefig(\n        os.path.join(working_dir, \"synthetic_validation_loss_solver_ablation.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot4: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = experiment_data[\"lr_schedules\"][\"synthetic\"]\n    schedules = data[\"schedules\"]\n    train_losses = data[\"losses\"][\"train\"]\n    val_losses = data[\"losses\"][\"val\"]\n    train_errs = data[\"metrics\"][\"train\"]\n    val_errs = data[\"metrics\"][\"val\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Loss curves\ntry:\n    plt.figure()\n    for sched, t_loss, v_loss in zip(schedules, train_losses, val_losses):\n        plt.plot(range(1, len(t_loss) + 1), t_loss, label=f\"{sched} train\")\n        plt.plot(range(1, len(v_loss) + 1), v_loss, \"--\", label=f\"{sched} val\")\n    plt.title(\"Loss Curves for Synthetic Dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Error curves\ntry:\n    plt.figure()\n    for sched, t_err, v_err in zip(schedules, train_errs, val_errs):\n        plt.plot(range(1, len(t_err) + 1), t_err, label=f\"{sched} train\")\n        plt.plot(range(1, len(v_err) + 1), \"--\", label=f\"{sched} val\")\n    plt.title(\"Error Curves for Synthetic Dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Relative Error\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_error_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating error curves plot: {e}\")\n    plt.close()\n\n# Final validation error bar chart\ntry:\n    final_val = [errs[-1] for errs in val_errs]\n    plt.figure()\n    plt.bar(schedules, final_val)\n    plt.title(\"Final Validation Error Comparison for Synthetic Dataset\")\n    plt.xlabel(\"Learning-Rate Schedule\")\n    plt.ylabel(\"Final Validation Error\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_final_val_error.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final validation error plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nbetas = [0.5, 0.7, 0.9, 0.99]\nfor dist, ed in experiment_data.get(\"noise_distribution\", {}).items():\n    # Plot error curves\n    try:\n        m = ed[\"metrics\"]\n        train_err = np.array(m[\"train\"])\n        val_err = np.array(m[\"val\"])\n        epochs = train_err.shape[1]\n        x = np.arange(1, epochs + 1)\n        plt.figure()\n        for i, b in enumerate(betas):\n            plt.plot(x, train_err[i], label=f\"train \u03b21={b}\")\n            plt.plot(x, val_err[i], \"--\", label=f\"val \u03b21={b}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Normalized Error\")\n        plt.title(f\"Error Curves for {dist} (solid=train, dashed=val)\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{dist}_error_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating error curves for {dist}: {e}\")\n        plt.close()\n    # Plot loss curves\n    try:\n        l = ed[\"losses\"]\n        train_ls = np.array(l[\"train\"])\n        val_ls = np.array(l[\"val\"])\n        epochs = train_ls.shape[1]\n        x = np.arange(1, epochs + 1)\n        plt.figure()\n        for i, b in enumerate(betas):\n            plt.plot(x, train_ls[i], label=f\"train \u03b21={b}\")\n            plt.plot(x, val_ls[i], \"--\", label=f\"val \u03b21={b}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"MSE Loss\")\n        plt.title(f\"Loss Curves for {dist} (solid=train, dashed=val)\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{dist}_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves for {dist}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\ntry:\n    edata = np.load(data_path, allow_pickle=True).item()\n    syn = edata[\"data_dimensionality\"][\"synthetic\"]\n    dims = syn[\"dims\"]\n    errs_train = syn[\"metrics\"][\"train\"]\n    errs_val = syn[\"metrics\"][\"val\"]\n    loss_train = syn[\"losses\"][\"train\"]\n    loss_val = syn[\"losses\"][\"val\"]\n    gts = syn[\"ground_truth\"]\n    preds = syn[\"predictions\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot error curves\ntry:\n    plt.figure()\n    for i, d in enumerate(dims):\n        epochs = len(errs_train[i])\n        plt.plot(range(1, epochs + 1), errs_train[i], label=f\"Train Err (dim={d})\")\n        plt.plot(range(1, epochs + 1), errs_val[i], \"--\", label=f\"Val Err (dim={d})\")\n    plt.title(\"Training and Validation Relative Errors (synthetic)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Relative Error\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_error_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating error curves: {e}\")\n    plt.close()\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for i, d in enumerate(dims):\n        epochs = len(loss_train[i])\n        plt.plot(range(1, epochs + 1), loss_train[i], label=f\"Train Loss (dim={d})\")\n        plt.plot(range(1, epochs + 1), loss_val[i], \"--\", label=f\"Val Loss (dim={d})\")\n    plt.title(\"Training and Validation Reconstruction Loss (synthetic)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"MSE Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# Histograms for each dimension\nfor i, d in enumerate(dims):\n    try:\n        plt.figure(figsize=(8, 4))\n        plt.subplot(1, 2, 1)\n        plt.hist(np.array(gts[i]).flatten(), bins=50)\n        plt.title(f\"Ground Truth (synthetic, dim={d})\")\n        plt.subplot(1, 2, 2)\n        plt.hist(np.array(preds[i]).flatten(), bins=50)\n        plt.title(f\"Generated Samples (synthetic, dim={d})\")\n        plt.suptitle(\"Left: Ground Truth, Right: Generated Samples\")\n        fname = f\"synthetic_histogram_dim_{d}.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating histogram for dim={d}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# load data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\ned = experiment_data.get(\"alt_min_freq\", {}).get(\"synthetic\", {})\n\nmetrics_train = ed.get(\"metrics\", {}).get(\"train\", [])\nmetrics_val = ed.get(\"metrics\", {}).get(\"val\", [])\nlosses_train = ed.get(\"losses\", {}).get(\"train\", [])\nlosses_val = ed.get(\"losses\", {}).get(\"val\", [])\nratios = ed.get(\"ratios\", [])\n\n# Plot error curves\ntry:\n    plt.figure()\n    for (c, d), tr, vl in zip(ratios, metrics_train, metrics_val):\n        epochs = range(1, len(tr) + 1)\n        plt.plot(epochs, tr, label=f\"{c}:{d} train\", linestyle=\"-\")\n        plt.plot(epochs, vl, label=f\"{c}:{d} val\", linestyle=\"--\")\n    plt.title(\"Synthetic Dataset: Training and Validation Error\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Relative Error\")\n    plt.legend()\n    plt.suptitle(\"Solid: Training, Dashed: Validation\", fontsize=10)\n    plt.savefig(os.path.join(working_dir, \"synthetic_train_val_error_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating error curves plot: {e}\")\n    plt.close()\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for (c, d), tr, vl in zip(ratios, losses_train, losses_val):\n        epochs = range(1, len(tr) + 1)\n        plt.plot(epochs, tr, label=f\"{c}:{d} train\", linestyle=\"-\")\n        plt.plot(epochs, vl, label=f\"{c}:{d} val\", linestyle=\"--\")\n    plt.title(\"Synthetic Dataset: Training and Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"MSE Loss\")\n    plt.legend()\n    plt.suptitle(\"Solid: Training, Dashed: Validation\", fontsize=10)\n    plt.savefig(os.path.join(working_dir, \"synthetic_train_val_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Print final validation errors\nif ratios and metrics_val:\n    print(\"Final validation errors per ratio:\")\n    for (c, d), vl in zip(ratios, metrics_val):\n        print(f\"Ratio {c}:{d} -> {vl[-1]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# load data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\ned = experiment_data.get(\"alt_min_freq\", {}).get(\"synthetic\", {})\n\nmetrics_train = ed.get(\"metrics\", {}).get(\"train\", [])\nmetrics_val = ed.get(\"metrics\", {}).get(\"val\", [])\nlosses_train = ed.get(\"losses\", {}).get(\"train\", [])\nlosses_val = ed.get(\"losses\", {}).get(\"val\", [])\nratios = ed.get(\"ratios\", [])\n\n# Plot error curves\ntry:\n    plt.figure()\n    for (c, d), tr, vl in zip(ratios, metrics_train, metrics_val):\n        epochs = range(1, len(tr) + 1)\n        plt.plot(epochs, tr, label=f\"{c}:{d} train\", linestyle=\"-\")\n        plt.plot(epochs, vl, label=f\"{c}:{d} val\", linestyle=\"--\")\n    plt.title(\"Synthetic Dataset: Training and Validation Error\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Relative Error\")\n    plt.legend()\n    plt.suptitle(\"Solid: Training, Dashed: Validation\", fontsize=10)\n    plt.savefig(os.path.join(working_dir, \"synthetic_train_val_error_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating error curves plot: {e}\")\n    plt.close()\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for (c, d), tr, vl in zip(ratios, losses_train, losses_val):\n        epochs = range(1, len(tr) + 1)\n        plt.plot(epochs, tr, label=f\"{c}:{d} train\", linestyle=\"-\")\n        plt.plot(epochs, vl, label=f\"{c}:{d} val\", linestyle=\"--\")\n    plt.title(\"Synthetic Dataset: Training and Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"MSE Loss\")\n    plt.legend()\n    plt.suptitle(\"Solid: Training, Dashed: Validation\", fontsize=10)\n    plt.savefig(os.path.join(working_dir, \"synthetic_train_val_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Print final validation errors\nif ratios and metrics_val:\n    print(\"Final validation errors per ratio:\")\n    for (c, d), vl in zip(ratios, metrics_val):\n        print(f\"Ratio {c}:{d} -> {vl[-1]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# load data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\ned = experiment_data.get(\"alt_min_freq\", {}).get(\"synthetic\", {})\n\nmetrics_train = ed.get(\"metrics\", {}).get(\"train\", [])\nmetrics_val = ed.get(\"metrics\", {}).get(\"val\", [])\nlosses_train = ed.get(\"losses\", {}).get(\"train\", [])\nlosses_val = ed.get(\"losses\", {}).get(\"val\", [])\nratios = ed.get(\"ratios\", [])\n\n# Plot error curves\ntry:\n    plt.figure()\n    for (c, d), tr, vl in zip(ratios, metrics_train, metrics_val):\n        epochs = range(1, len(tr) + 1)\n        plt.plot(epochs, tr, label=f\"{c}:{d} train\", linestyle=\"-\")\n        plt.plot(epochs, vl, label=f\"{c}:{d} val\", linestyle=\"--\")\n    plt.title(\"Synthetic Dataset: Training and Validation Error\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Relative Error\")\n    plt.legend()\n    plt.suptitle(\"Solid: Training, Dashed: Validation\", fontsize=10)\n    plt.savefig(os.path.join(working_dir, \"synthetic_train_val_error_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating error curves plot: {e}\")\n    plt.close()\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for (c, d), tr, vl in zip(ratios, losses_train, losses_val):\n        epochs = range(1, len(tr) + 1)\n        plt.plot(epochs, tr, label=f\"{c}:{d} train\", linestyle=\"-\")\n        plt.plot(epochs, vl, label=f\"{c}:{d} val\", linestyle=\"--\")\n    plt.title(\"Synthetic Dataset: Training and Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"MSE Loss\")\n    plt.legend()\n    plt.suptitle(\"Solid: Training, Dashed: Validation\", fontsize=10)\n    plt.savefig(os.path.join(working_dir, \"synthetic_train_val_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Print final validation errors\nif ratios and metrics_val:\n    print(\"Final validation errors per ratio:\")\n    for (c, d), vl in zip(ratios, metrics_val):\n        print(f\"Ratio {c}:{d} -> {vl[-1]:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_data_path_list = [\n    \"experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_f9dfbfebf907482d8cfa805ed5d250f5_proc_119935/experiment_data.npy\",\n    \"experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_57ac4e99a24644f9b02c4748160b834a_proc_119936/experiment_data.npy\",\n    \"experiments/2025-06-07_19-09-39_weight_primitives_attempt_0/logs/0-run/experiment_results/experiment_9f9d63180c0b46548dd5bd69d0b77887_proc_119934/experiment_data.npy\",\n]\n\n# Load all experiment data\nall_experiment_data = []\nfor exp_path in experiment_data_path_list:\n    try:\n        data = np.load(\n            os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\"), exp_path), allow_pickle=True\n        ).item()\n        all_experiment_data.append(data)\n    except Exception as e:\n        print(f\"Error loading experiment data: {e}\")\n\n# Extract synthetic dataset runs\nalt_data_list = [\n    d.get(\"alt_min_freq\", {}).get(\"synthetic\", {}) for d in all_experiment_data\n]\nif not alt_data_list:\n    print(\"No synthetic data found.\")\nelse:\n    ratios = alt_data_list[0].get(\"ratios\", [])\n    n_reps = len(alt_data_list)\n    metrics_train_list = [d.get(\"metrics\", {}).get(\"train\", []) for d in alt_data_list]\n    metrics_val_list = [d.get(\"metrics\", {}).get(\"val\", []) for d in alt_data_list]\n    losses_train_list = [d.get(\"losses\", {}).get(\"train\", []) for d in alt_data_list]\n    losses_val_list = [d.get(\"losses\", {}).get(\"val\", []) for d in alt_data_list]\n\n    # Compute mean and SEM curves for error\n    mean_train_curves, sem_train_curves = [], []\n    mean_val_curves, sem_val_curves = [], []\n    for i in range(len(ratios)):\n        # Align by minimum epochs across reps\n        len_tr = [len(rep[i]) for rep in metrics_train_list if len(rep) > i]\n        min_tr = min(len_tr) if len_tr else 0\n        arr_tr = np.array([rep[i][:min_tr] for rep in metrics_train_list])\n        mean_tr = arr_tr.mean(axis=0)\n        sem_tr = arr_tr.std(axis=0, ddof=1) / np.sqrt(n_reps)\n        len_vl = [len(rep[i]) for rep in metrics_val_list if len(rep) > i]\n        min_vl = min(len_vl) if len_vl else 0\n        arr_vl = np.array([rep[i][:min_vl] for rep in metrics_val_list])\n        mean_vl = arr_vl.mean(axis=0)\n        sem_vl = arr_vl.std(axis=0, ddof=1) / np.sqrt(n_reps)\n        mean_train_curves.append(mean_tr)\n        sem_train_curves.append(sem_tr)\n        mean_val_curves.append(mean_vl)\n        sem_val_curves.append(sem_vl)\n\n    # Aggregated error plot\n    try:\n        plt.figure()\n        for (c, d), mean_tr, sem_tr, mean_vl, sem_vl in zip(\n            ratios, mean_train_curves, sem_train_curves, mean_val_curves, sem_val_curves\n        ):\n            epochs_tr = range(1, len(mean_tr) + 1)\n            epochs_vl = range(1, len(mean_vl) + 1)\n            plt.plot(epochs_tr, mean_tr, label=f\"{c}:{d} Train Mean\", linestyle=\"-\")\n            plt.fill_between(epochs_tr, mean_tr - sem_tr, mean_tr + sem_tr, alpha=0.2)\n            plt.plot(epochs_vl, mean_vl, label=f\"{c}:{d} Val Mean\", linestyle=\"--\")\n            plt.fill_between(epochs_vl, mean_vl - sem_vl, mean_vl + sem_vl, alpha=0.2)\n        plt.title(\"Synthetic Dataset: Aggregated Training and Validation Error\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Relative Error\")\n        plt.legend(title=\"Mean \u00b1 SEM\")\n        plt.suptitle(\"Solid: Train Mean, Dashed: Val Mean; Shaded: \u00b1 SEM\", fontsize=10)\n        plt.savefig(\n            os.path.join(working_dir, \"synthetic_aggregated_train_val_error.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated error plot: {e}\")\n        plt.close()\n\n    # Compute and plot aggregated loss\n    try:\n        mean_loss_train, sem_loss_train = [], []\n        mean_loss_val, sem_loss_val = [], []\n        for i in range(len(ratios)):\n            lt = [len(rep[i]) for rep in losses_train_list if len(rep) > i]\n            min_lt = min(lt) if lt else 0\n            arr_lt = np.array([rep[i][:min_lt] for rep in losses_train_list])\n            m_lt = arr_lt.mean(axis=0)\n            s_lt = arr_lt.std(axis=0, ddof=1) / np.sqrt(n_reps)\n            lv = [len(rep[i]) for rep in losses_val_list if len(rep) > i]\n            min_lv = min(lv) if lv else 0\n            arr_lv = np.array([rep[i][:min_lv] for rep in losses_val_list])\n            m_lv = arr_lv.mean(axis=0)\n            s_lv = arr_lv.std(axis=0, ddof=1) / np.sqrt(n_reps)\n            mean_loss_train.append(m_lt)\n            sem_loss_train.append(s_lt)\n            mean_loss_val.append(m_lv)\n            sem_loss_val.append(s_lv)\n        plt.figure()\n        for (c, d), m_lt, s_lt, m_lv, s_lv in zip(\n            ratios, mean_loss_train, sem_loss_train, mean_loss_val, sem_loss_val\n        ):\n            e_lt = range(1, len(m_lt) + 1)\n            e_lv = range(1, len(m_lv) + 1)\n            plt.plot(e_lt, m_lt, label=f\"{c}:{d} Train Loss Mean\", linestyle=\"-\")\n            plt.fill_between(e_lt, m_lt - s_lt, m_lt + s_lt, alpha=0.2)\n            plt.plot(e_lv, m_lv, label=f\"{c}:{d} Val Loss Mean\", linestyle=\"--\")\n            plt.fill_between(e_lv, m_lv - s_lv, m_lv + s_lv, alpha=0.2)\n        plt.title(\"Synthetic Dataset: Aggregated Training and Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"MSE Loss\")\n        plt.legend(title=\"Mean \u00b1 SEM\")\n        plt.suptitle(\"Solid: Train Mean, Dashed: Val Mean; Shaded: \u00b1 SEM\", fontsize=10)\n        plt.savefig(\n            os.path.join(working_dir, \"synthetic_aggregated_train_val_loss.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated loss plot: {e}\")\n        plt.close()\n\n    # Print aggregated final validation errors\n    try:\n        print(\"Aggregated final validation errors per ratio:\")\n        for i, (c, d) in enumerate(ratios):\n            finals = [rep[i][-1] for rep in metrics_val_list if len(rep) > i]\n            if finals:\n                m = np.mean(finals)\n                se = np.std(finals, ddof=1) / np.sqrt(n_reps)\n                print(f\"Ratio {c}:{d} -> {m:.4f} \u00b1 {se:.4f}\")\n    except Exception as e:\n        print(f\"Error printing final validation errors: {e}\")\n"], "plot_plan": ["I will load the saved experiment data, extract training and validation errors and losses for each \u03b21 setting, and plot these curves together in separate figures, each wrapped in a try\u2010except to ensure safe execution. I will label each curve by its \u03b21 value and save with descriptive names indicating the synthetic dataset and plot type. Finally, I will select the run with the lowest final validation error, retrieve its first reconstructed test sample and the corresponding ground truth, and plot them side by side in a dedicated figure with a clear subtitle. All figures are saved into the working directory and properly closed after saving.", "I will load the saved experiment data from the working directory and iterate over the three synthetic datasets. For each dataset, I will create a single figure with two subplots side by side: the left subplot will show training and validation errors over epochs for each beta1 value, while the right subplot will show corresponding training and validation losses. The figure will be titled with the dataset name and clear subtitles will identify each panel. All plotting code will be wrapped in try-except blocks to catch errors, and each figure will be saved to `working_dir` with a descriptive filename. I will close each figure after saving to free resources.", "We begin by loading the experiment data from the working directory and extracting the synthetic results under the dictionary_capacity key. We then retrieve the lists of training and validation errors and losses, along with the corresponding n_components and beta1 values. For each dictionary size, we plot the epoch-wise reconstruction error curves for all beta1 settings\u2014showing both train and validation\u2014saving each figure with a descriptive filename. We then select the smallest and largest dictionary sizes to plot the epoch-wise MSE loss curves similarly. Each plotting step is enclosed in its own try-except block to safely handle any errors, and figures are closed after saving to conserve resources. Titles, axis labels, and legends clearly denote the synthetic dataset, plot type, and parameter values. No synthetic or simulated data is introduced; only existing arrays in experiment_data.npy are used.", "The code loads the saved experiment data from the working directory and extracts the synthetic ablation metrics including train/validation errors, losses, sparsities, and dictionary recovery errors for each sparsity strength. It defines the list of lambda values and computes the epoch axis before plotting. Each plot is wrapped in a try\u2010except block: one for training vs. validation error curves, one for training vs. validation loss curves, one for code sparsity over epochs, and one for dictionary recovery error over epochs. The error and loss plots show solid lines for training and dashed lines for validation, with clear titles specifying the synthetic dataset. All figures are saved to the working directory with descriptive filenames including the dataset and metric type. Figures are always closed after saving, and any errors during plotting are caught and reported. Finally, it prints a summary of the final validation error for each lambda value.", "I will load the experiment data, extract the per\u2010epoch training and validation metrics and losses for the synthetic dataset under the atom_norm_projection experiment, and define the \u03b21 list alongside the epoch count. Then I will plot training vs. validation error curves for each \u03b21 in a single figure, using solid lines for training and dashed lines for validation, wrapping the code in a try-except block and saving the figure to the working directory with a descriptive filename and title that includes a subtitle. I will repeat this for the training and validation loss curves in another figure, again ensuring proper error handling and figure cleanup. Finally, I will print out the final validation error for each \u03b21 to summarize the evaluation metrics, ensuring all figures are closed after saving.", "First load the saved experiment data and extract the training/validation metrics, losses, predictions, ground truth, and noise levels for the synthetic dataset. Then create one figure showing relative error curves across epochs for both training and validation sets at each noise level, saving it with a descriptive filename. Similarly, generate a separate figure for the MSE loss curves across epochs. Finally, for each noise value, plot a reconstruction comparison of one test sample by showing the ground truth and corresponding generated sample side by side, saving up to five such figures with clear subtitles and dataset labels. Each plot is enclosed in its own try-except block to handle errors gracefully and ensure figures are closed after saving.", "I will load the saved experiment data and extract the training/validation error and loss lists for each optimizer, then convert them to numpy arrays. I will print the final train and validation error for each optimizer to summarize performance. Next, I will create four separate plots in try-except blocks: training error curves, validation error curves, training loss curves, and validation loss curves, each overlaying the three optimizers and saving to the working directory. Finally, I will identify the optimizer with the lowest final validation error, take its first test sample, and generate a side-by-side subplot comparing the ground truth and reconstructed sample, clearly labeled with \u201cLeft: Ground Truth, Right: Generated Samples.\u201d All figures are saved with descriptive filenames, and closed after saving or on exceptions.", "We first load the saved experiment data and extract the synthetic dataset results. We then prepare two separate plots: one for training vs. validation error curves and another for training vs. validation loss curves, each enclosed in its own try-except block to handle errors gracefully and ensure figures are closed. For each plot, we loop over the code-to-dictionary update ratios, plotting the epochs for training (solid lines) and validation (dashed lines), adding titles specifying the synthetic dataset and clear subtitles. We save each figure with descriptive filenames to the working directory. Finally, we print out the final validation error for each ratio as a summary metric.", "We will load the saved experiment data and extract the synthetic dataset entries. Next, we group results by dictionary initialization method and average across code initializations to simplify the visualization. We then compute mean training and validation errors as well as losses over epochs for each init_D. We plot three separate figures\u2014in distinct try/except blocks\u2014showing (1) training error curves, (2) validation error curves, and (3) combined training and validation loss curves, each annotated with \"Synthetic Dataset\" in the title and saved under descriptive filenames. Each plot loop uses clear labels, legends, and axes titles, and ensures figures are closed after saving. Finally, we print out the final averaged validation error for each init_D as an overall evaluation metric.", "We will load the saved `experiment_data.npy` from the working directory and extract the synthetic ablation results including training/validation errors, losses, predictions, and ground truth for each batch size.  First, we'll plot the error curves over epochs in a two\u2010panel figure: the left subplot showing training errors and the right subplot showing validation errors, with a shared legend for different batch sizes.  Next, we repeat the same structure for loss curves in another figure.  Finally, for each batch size, we will create a two\u2010panel sample reconstruction figure comparing the first test sample\u2019s ground truth signal (left) to the predicted signal (right) across its dimensions.  Each plot generation is wrapped in its own `try/except` block to handle any errors and ensure figures are closed.  Figures are saved in the working directory with descriptive names specifying the dataset and plot type (and batch size where relevant).  All plots include a clear title and subtitle indicating the left/right panels and the synthetic dataset context.", "I will load the saved experiment data, extract the training and validation metrics and losses for each penalty type on the synthetic dataset. Then, I will generate three figures: one line plot comparing training and validation relative errors across epochs for each penalty; one similar line plot for MSE losses; and a bar chart of the final validation error for each penalty. Each plot is wrapped in its own try-except block, includes a suptitle specifying the dataset, a clear title, axis labels, and a legend or ticks as appropriate. All figures are saved into the working directory, and closed after saving.", "Our solution will load the experiment data saved under `working/experiment_data.npy` and extract the synthetic results. We compute the final relative test error per orthogonality weight \u03bb2 and print these metrics. We produce a combined plot of training and validation relative error curves across epochs for all \u03bb2 values, saving to `synthetic_error_curves.png`. We similarly plot the mean squared reconstruction loss curves and save to `synthetic_loss_curves.png`. To inspect reconstruction quality, we select three representative \u03bb2 cases (0, 1e-3, 1e-1) and for each generate a 1\u00d72 subplot comparing ground truth and reconstructed samples for the first test example. All plotting occurs in separate try-except blocks to handle errors gracefully, and each figure is closed after saving. Titles and subtitles clearly note the synthetic dataset, subplot labels, and \u03bb2 settings. Filenames include both dataset name and \u03bb2 value for clarity.", null, "We load the saved experiment_data from the working directory and extract the synthetic code distributions. We know the four beta1 hyperparameters used, so we define beta1_list accordingly. For each distribution, we retrieve the per\u2010epoch training and validation errors and losses, determine the number of epochs, and set up x = 1..epochs. Then, in one try\u2010except block, we plot training versus validation errors side by side across epochs for all beta1 values, add titles, legends, axis labels, and a suptitle, save under a descriptive filename, and close the figure. In a second try\u2010except block we repeat the same for training versus validation losses. All plots are saved into working_dir with names like \u201c<distribution>_error_curves.png\u201d and \u201c<distribution>_loss_curves.png,\u201d ensuring clear subtitles and no extra styling.", "I will load the saved experiment data and reconstruct the loss function names and \u03b21 list used in the experiments. Then I will create two sets of line\u2010curve plots: one for the epoch\u2010wise relative errors and one for the raw reconstruction losses, grouping runs by loss function and using solid/dashed styles for train/val. Finally, for each loss type I will select the final test\u2010set predictions and ground truth, flatten them, and display a scatter plot with an identity line. Each plotting routine is wrapped in its own try\u2010except block to handle errors gracefully, always closing the figure, and saving with descriptive filenames into the working directory.", "We will load the saved NumPy experiment results from the working directory and extract the synthetic dataset metrics and losses. Then we will generate four key plots: the training error curve, the training loss curve, the validation error across all solvers, and the validation loss across all solvers. Each plot will run in its own try-except block to ensure errors are handled gracefully and figures are always closed. We will include clear titles specifying \"Synthetic Dataset\" and descriptive filenames reflecting the plot type and dataset. We will use basic matplotlib without custom styles, labeling axes and adding a legend where multiple lines appear. All plots will be saved into the working directory using informative names. No image sample plots are included since the data are high-dimensional vectors rather than images. This approach provides a concise visual summary of the model\u2019s training dynamics and solver comparisons.", "The solution begins by loading the saved experiment data from working_dir in a try\u2010except block to handle loading errors. We extract the synthetic dataset\u2019s loss and error histories along with the schedule names. We then create a combined training and validation loss curve figure that overlays each learning\u2010rate schedule\u2019s curves, adding titles, axis labels, and a legend. In a separate try\u2010except, we plot the training and validation relative error curves in a similar fashion. Next, we compile the final validation errors across schedules and display them as a bar chart, again within its own try\u2010except block. Throughout, we save each figure to working_dir with descriptive names including the dataset and plot type, closing every figure after saving. Any plot\u2010specific errors are caught and printed, ensuring robustness. This yields three clear, standard visualizations of the synthetic experiment results.", "We load the saved experiment_data.npy from working_dir and extract the noise_distribution results. For each noise type, we then plot the training and validation normalized error curves over epochs, overlaying all four \u03b21 settings in one figure with solid (train) and dashed (val) lines. We wrap each figure creation in a try-except block, label axes, add a legend, and title that specifies the noise distribution and line style conventions before saving with a descriptive filename. We repeat the process for the mean\u2010squared reconstruction losses, again plotting all \u03b21 settings together. Each plot is saved into working_dir and we ensure figures are closed after saving to free up memory.", "We load the saved numpy experiment data and extract the synthetic results including dimensionalities, training/validation errors and losses, along with ground truth and predictions. We then produce a joint figure plotting training and validation relative errors versus epochs for each dimension. Next, we create another figure showing training and validation reconstruction losses over epochs. After that, for each tested dimensionality (up to 5), we generate a side-by-side histogram comparing the ground truth and generated sample distributions, labeling \u201cLeft: Ground Truth, Right: Generated Samples\u201d and including the dataset type and dimension in titles and filenames. Each plotting section is wrapped in its own try-except to catch errors, always saving to the working directory, and closing figures even if an exception occurs. All plot files use descriptive names that include the dataset (\u201csynthetic\u201d) and plot type.", null, null, null, null], "ablation_name": [null, "Multi-Synthetic-Dataset Robustness", "Dictionary Capacity Ablation", "Sparsity Regularization Strength Ablation", "Dictionary Atom Norm Projection Ablation", "Synthetic Noise Level Ablation", "Optimizer Choice Ablation", "Alternating Minimization Frequency Ablation", "Initialization Scheme Ablation", "Mini-Batch Size Ablation", "Sparsity Regularization Type Ablation", "Dictionary Orthogonality Regularization Ablation", "Test-Time Inference Method Ablation", "Synthetic Code Distribution Ablation", "Reconstruction Loss Function Ablation", null, "Learning Rate Scheduling Ablation", "Noise Distribution Ablation", "Data Dimensionality Ablation", null, null, null, null], "hyperparam_name": ["adam_beta1", null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["I propose loading the saved NumPy file from the `working` directory, accessing\nthe `\"adam_beta1\"` entry and iterating over its datasets. For each dataset (here\n\u201csynthetic\u201d) and each \u03b2\u2081 setting, we extract the last\u2010epoch values of training\nerror, validation error, training loss, and validation loss, and print them with\nclear metric names. The script executes immediately at the global scope without\nany `__main__` guard.", "I will load the saved experiment data from the working directory, then loop\nthrough each synthetic dataset to pull out the training and validation error\ncurves. For each dataset, I compute the final epoch errors for all\nhyperparameter settings, select the one with the lowest validation error, and\nprint the dataset name along with the final training relative error and final\nvalidation relative error using clear metric labels.", "I will load the saved `experiment_data.npy` file from the working directory,\nthen iterate through its nested structure to retrieve each dataset\u2019s metrics and\ncorresponding hyperparameters. For each (n_components, beta1) configuration, I\nwill extract the last\u2010epoch train and validation error values and print them\nclearly labeled, preceded by the dataset name. This script runs immediately at\nthe global scope without any special entry point.", "The script below loads the saved experiment data, retrieves the final epoch\nvalues for reconstruction errors, reconstruction losses, code sparsity, and\ndictionary recovery error for each sparsity weight (lambda1) in the synthetic\ndataset, and prints them with clear labels. It reconstructs the original lambda1\ngrid to match each set of metrics to its regularization strength, and iterates\nglobally without any special entry points.", "The following script sets the working directory, loads the `experiment_data.npy`\nfile, and then iterates over each algorithm and dataset to extract the final\nepoch train and validation relative errors for each run. It clearly prints the\ndataset name and labels each metric before its value. No plotting or special\nentry\u2010point checks are used, so running this script will immediately display all\nrequested metrics.", "I will load the saved NumPy dictionary from the `working` directory, extract the\n`\"synthetic\"` dataset under `\"synthetic_noise\"`, and read out the train and\nvalidation error arrays along with the noise levels. Then I will iterate over\neach noise level, grab the final (last epoch) error values, and print them with\nclear labels\u2014always prefixing with the dataset name and specifying `\"train\nerror\"` and `\"validation error\"` for clarity.", "We\u2019ll load the numpy file from the \u201cworking\u201d folder, then iterate through each\nexperiment and dataset in the loaded dictionary. For each dataset, we print its\nname and then loop over the listed optimizers. For each optimizer we extract and\nprint the final epoch\u2019s training and validation relative errors and\nreconstruction losses with explicit labels, avoiding vague terms. All code\nexecutes immediately at the global scope without special entry points or\nplotting.", "I will load the NumPy file from the designated working directory and retrieve\nthe top\u2010level dictionary. Then I will iterate over each dataset entry, printing\nits name, and for each code\u2010vs\u2010dictionary update schedule I will extract the\nlast epoch\u2019s training and validation metrics and losses. Each metric is printed\nwith an explicit, descriptive label (\u201ctraining mean relative error\u201d, \u201cvalidation\nMSE loss\u201d, etc.) alongside the corresponding schedule. The script runs at global\nscope without any `if __name__ == \"__main__\":` guard so it executes immediately.", "I will load the saved NumPy file from the `working` directory, extract the\nnested metrics structure, and then iterate over each dataset (e.g., \u201csynthetic\u201d)\nand each initialization scheme. For each scheme, I will pick the final epoch\u2019s\ntrain and validation error values. The script prints the dataset name first and\nthen, for each initialization combination, prints clearly labeled \u201ctrain error\n(final epoch)\u201d and \u201cvalidation error (final epoch)\u201d. This runs immediately at\nthe global scope without any `if __name__ == \"__main__\":` guard.", "The following script loads the saved experiment data from the working directory,\niterates over each experiment type and dataset, and for each batch size prints\nthe dataset name once followed by the final train and validation reconstruction\nerrors and losses with clear metric labels.", "The following script immediately loads the saved experiment results from the\nworking directory, iterates through each penalty\u2010type dataset, and extracts the\nfinal epoch\u2019s training and validation errors. It then prints the dataset name\nfollowed by clearly labeled \u201cFinal training relative error\u201d and \u201cFinal\nvalidation relative error\u201d values for each penalty type.", "Below is a simple script that immediately loads the saved numpy file from the\n\u201cworking\u201d folder, iterates through the synthetic results under \u201cdict_orth\u201d in\nthe same order as the original lambda2 grid, and prints for each \u03bb\u2082 the final\nepoch\u2019s normalized training error, normalized validation error, training MSE\nloss, and validation MSE loss with clear, precise labels.", "", "I will load the saved numpy file from the working directory, extract the train\nand validation error histories for each synthetic distribution, take the final\nepoch values for each beta1 setting, and report the best (minimum) final\ntraining and validation error per dataset with clear labels. The script uses\nos.getcwd() to locate the file, runs entirely at global scope, and prints\nresults directly.", "I will load the saved `experiment_data.npy` from the working directory, access\nthe `\"reconstruction_loss\" \u2192 \"synthetic\"` section, and reconstruct the order of\nruns across the three loss types (`\"mse\"`, `\"mae\"`, `\"huber\"`) and four Adam \u03b2\u2081\nvalues (`0.5`, `0.7`, `0.9`, `0.99`). For each loss/\u03b2\u2081 combo, I will extract the\nfinal\u2010epoch training and validation relative reconstruction errors and the final\nreconstruction losses, printing each with clear descriptive labels including\ndataset, loss function, and \u03b2\u2081 value.", "The following script loads the saved experiment data, accesses the\n\u201ctest_time_solver_ablation\u201d block, and for each dataset prints its name followed\nby the final (last-epoch) training error, training loss, and per-solver\nvalidation error and loss with clear labels.", "Below is a script that loads the saved experiment data, iterates over the\n\u201csynthetic\u201d dataset\u2019s schedules, and prints out the final (last\u2010epoch)\nreconstruction errors and MSE losses with clear, self\u2010descriptive names. It\npulls the working directory dynamically, avoids any \u201cif __name__ == '__main__'\u201d\nguard, and runs immediately when executed.", "I will load the saved experiment_data.npy using numpy with allow_pickle,\nrecreate the original beta1_list to index through the stored runs, and then\niterate over each noise distribution. For each dataset, the script prints its\nname, then for each beta1 hyperparameter prints the final epoch's train\nreconstruction error and validation reconstruction error with clear labels. All\ncode is at the global scope, runs immediately, and uses only print statements\nwithout any plotting.", "I will load the saved NumPy file from the working directory, extract the\n\u201csynthetic\u201d block under data_dimensionality, and then iterate over each tested\ndimensionality. For each dimension, I pull out the stored training and\nvalidation error sequences as well as the reconstruction loss sequences, take\nthe final epoch values, and print them with clear labels. The script runs\nimmediately at global scope and prints each dataset name followed by its final\ntraining/validation reconstruction loss and relative error.", "I will load the NumPy file from the designated working directory and retrieve\nthe top\u2010level dictionary. Then I will iterate over each dataset entry, printing\nits name, and for each code\u2010vs\u2010dictionary update schedule I will extract the\nlast epoch\u2019s training and validation metrics and losses. Each metric is printed\nwith an explicit, descriptive label (\u201ctraining mean relative error\u201d, \u201cvalidation\nMSE loss\u201d, etc.) alongside the corresponding schedule. The script runs at global\nscope without any `if __name__ == \"__main__\":` guard so it executes immediately.", "I will load the NumPy file from the designated working directory and retrieve\nthe top\u2010level dictionary. Then I will iterate over each dataset entry, printing\nits name, and for each code\u2010vs\u2010dictionary update schedule I will extract the\nlast epoch\u2019s training and validation metrics and losses. Each metric is printed\nwith an explicit, descriptive label (\u201ctraining mean relative error\u201d, \u201cvalidation\nMSE loss\u201d, etc.) alongside the corresponding schedule. The script runs at global\nscope without any `if __name__ == \"__main__\":` guard so it executes immediately.", "I will load the NumPy file from the designated working directory and retrieve\nthe top\u2010level dictionary. Then I will iterate over each dataset entry, printing\nits name, and for each code\u2010vs\u2010dictionary update schedule I will extract the\nlast epoch\u2019s training and validation metrics and losses. Each metric is printed\nwith an explicit, descriptive label (\u201ctraining mean relative error\u201d, \u201cvalidation\nMSE loss\u201d, etc.) alongside the corresponding schedule. The script runs at global\nscope without any `if __name__ == \"__main__\":` guard so it executes immediately.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Known hyperparameter settings used in the experiments\nbeta1_list = [0.5, 0.7, 0.9, 0.99]\n\n# Access the Adam-beta1 results\nadam_results = experiment_data[\"adam_beta1\"]\n\n# Iterate through each dataset and print the final metrics\nfor dataset_name, dataset_dict in adam_results.items():\n    print(f\"Dataset: {dataset_name}\")\n    train_errors = dataset_dict[\"metrics\"][\"train\"]\n    val_errors = dataset_dict[\"metrics\"][\"val\"]\n    train_losses = dataset_dict[\"losses\"][\"train\"]\n    val_losses = dataset_dict[\"losses\"][\"val\"]\n\n    for idx, beta1 in enumerate(beta1_list):\n        final_train_error = train_errors[idx][-1]\n        final_val_error = val_errors[idx][-1]\n        final_train_loss = train_losses[idx][-1]\n        final_val_loss = val_losses[idx][-1]\n\n        print(f\"Hyperparameter beta1 = {beta1}\")\n        print(f\"Final training error: {final_train_error:.6f}\")\n        print(f\"Final validation error: {final_val_error:.6f}\")\n        print(f\"Final training loss: {final_train_loss:.6f}\")\n        print(f\"Final validation loss: {final_val_loss:.6f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each synthetic dataset\nfor dataset_name, dataset in experiment_data[\"multi_synthetic\"].items():\n    # Extract per-beta training and validation error lists\n    train_errs = dataset[\"metrics\"][\"train\"]\n    val_errs = dataset[\"metrics\"][\"val\"]\n\n    # Compute the final epoch error for each beta1 setting\n    final_val_errors = [errs[-1] for errs in val_errs]\n    best_index = int(np.argmin(final_val_errors))\n\n    # Retrieve the corresponding training and validation errors\n    final_train_error = train_errs[best_index][-1]\n    final_val_error = val_errs[best_index][-1]\n\n    # Print the dataset name and final metric values\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"final training relative error: {final_train_error:.6f}\")\n    print(f\"final validation relative error: {final_val_error:.6f}\")\n", "import os\nimport numpy as np\n\n# define working directory and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# iterate through each category and dataset\nfor category, datasets in experiment_data.items():\n    for dataset_name, dataset_data in datasets.items():\n        # print dataset name\n        print(f\"Dataset: {dataset_name}\")\n        # extract metrics and hyperparameters\n        train_metrics = dataset_data[\"metrics\"][\"train\"]\n        val_metrics = dataset_data[\"metrics\"][\"val\"]\n        n_components_list = dataset_data[\"n_components_list\"]\n        beta1_list = dataset_data[\"beta1_list\"]\n        num_beta = len(beta1_list)\n        # loop over each experimental run\n        for idx, (train_errs, val_errs) in enumerate(zip(train_metrics, val_metrics)):\n            n_idx = idx // num_beta\n            b_idx = idx % num_beta\n            n_components = n_components_list[n_idx]\n            beta1 = beta1_list[b_idx]\n            # final values at last epoch\n            final_train_error = train_errs[-1]\n            final_validation_error = val_errs[-1]\n            # print hyperparameters and metrics\n            print(f\"Configuration: n_components={n_components}, beta1={beta1}\")\n            print(f\"final train error: {final_train_error:.6f}\")\n            print(f\"final validation error: {final_validation_error:.6f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Reconstruct the sparsity regularization grid\nlambda1_list = [0.0, 1e-4, 1e-3, 1e-2, 1e-1]\n\n# Iterate over each ablation and dataset\nablation = experiment_data.get(\"sparsity_strength_ablation\", {})\nfor dataset_name, data in ablation.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract stored metrics\n    train_errs = data[\"metrics\"][\"train\"]\n    val_errs = data[\"metrics\"][\"val\"]\n    train_losses = data[\"losses\"][\"train\"]\n    val_losses = data[\"losses\"][\"val\"]\n    sparsities = data[\"sparsity\"]\n    dict_errors = data[\"dict_error\"]\n\n    # Print final values for each lambda1\n    for lam, tr_err, vl_err, tr_loss, vl_loss, sp, de in zip(\n        lambda1_list,\n        train_errs,\n        val_errs,\n        train_losses,\n        val_losses,\n        sparsities,\n        dict_errors,\n    ):\n        print(f\"  Lambda1 = {lam}\")\n        print(f\"    Final training reconstruction error: {tr_err[-1]:.6f}\")\n        print(f\"    Final validation reconstruction error: {vl_err[-1]:.6f}\")\n        print(f\"    Final training reconstruction loss: {tr_loss[-1]:.6f}\")\n        print(f\"    Final validation reconstruction loss: {vl_loss[-1]:.6f}\")\n        print(f\"    Final code sparsity (fraction near zero): {sp[-1]:.6f}\")\n        print(f\"    Final dictionary recovery error: {de[-1]:.6f}\")\n", "import os\nimport numpy as np\n\n# set working directory and load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# iterate through all algorithms and datasets\nfor alg_name, datasets in experiment_data.items():\n    for dataset_name, results in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        metrics = results.get(\"metrics\", {})\n\n        # extract and print final train/validation relative errors for each run\n        for metric_key, runs in metrics.items():\n            if metric_key == \"train\":\n                label = \"Train relative error\"\n            elif metric_key == \"val\":\n                label = \"Validation relative error\"\n            else:\n                label = metric_key.capitalize()\n\n            for run_idx, run_vals in enumerate(runs, start=1):\n                final_val = run_vals[-1]\n                print(f\"  Run {run_idx}: {label}: {final_val:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Navigate to the synthetic dataset under synthetic_noise\nsynthetic_data = experiment_data[\"synthetic_noise\"][\"synthetic\"]\nmetrics = synthetic_data[\"metrics\"]\nnoise_levels = synthetic_data[\"noise_levels\"]\n\ntrain_errors = metrics[\"train\"]  # shape: (num_noise_levels, epochs)\nvalidation_errors = metrics[\"val\"]  # shape: (num_noise_levels, epochs)\n\n# Print the final epoch errors for each noise level\nfor sigma, train_errs, val_errs in zip(noise_levels, train_errors, validation_errors):\n    final_train_error = train_errs[-1]\n    final_validation_error = val_errs[-1]\n    print(f\"Dataset: synthetic (noise level = {sigma})\")\n    print(f\"  train error: {final_train_error:.6f}\")\n    print(f\"  validation error: {final_validation_error:.6f}\")\n", "import os\nimport numpy as np\n\n# Construct path to the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# Load the experiment data dictionary\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through experiments and datasets\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        optimizers = data[\"optimizers\"]\n        train_errs = data[\"metrics\"][\"train\"]\n        val_errs = data[\"metrics\"][\"val\"]\n        train_losses = data[\"losses\"][\"train\"]\n        val_losses = data[\"losses\"][\"val\"]\n\n        # Print final metrics for each optimizer\n        for idx, opt_name in enumerate(optimizers):\n            final_train_error = train_errs[idx][-1]\n            final_val_error = val_errs[idx][-1]\n            final_train_loss = train_losses[idx][-1]\n            final_val_loss = val_losses[idx][-1]\n\n            print(f\"Optimizer: {opt_name}\")\n            print(f\"  Final training relative error: {final_train_error:.4f}\")\n            print(f\"  Final validation relative error: {final_val_error:.4f}\")\n            print(f\"  Final training reconstruction loss: {final_train_loss:.4f}\")\n            print(f\"  Final validation reconstruction loss: {final_val_loss:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over strategies and datasets\nfor strategy_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        # Extract all recorded values\n        ratios = data[\"ratios\"]\n        train_errs = data[\"metrics\"][\"train\"]\n        val_errs = data[\"metrics\"][\"val\"]\n        train_losses = data[\"losses\"][\"train\"]\n        val_losses = data[\"losses\"][\"val\"]\n        # For each ratio schedule, print final metrics\n        for (\n            (c_steps, d_steps),\n            tr_err_list,\n            vl_err_list,\n            tr_loss_list,\n            vl_loss_list,\n        ) in zip(ratios, train_errs, val_errs, train_losses, val_losses):\n            final_train_error = tr_err_list[-1]\n            final_validation_error = vl_err_list[-1]\n            final_train_loss = tr_loss_list[-1]\n            final_validation_loss = vl_loss_list[-1]\n            print(f\"  Schedule (code_updates:dict_updates) = {c_steps}:{d_steps}\")\n            print(f\"    Final training mean relative error: {final_train_error:.6f}\")\n            print(\n                f\"    Final validation mean relative error: {final_validation_error:.6f}\"\n            )\n            print(f\"    Final training MSE loss: {final_train_loss:.6f}\")\n            print(f\"    Final validation MSE loss: {final_validation_loss:.6f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nfile_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through the experiments and datasets\nfor exp_category, datasets in experiment_data.items():\n    for dataset_name, dataset_info in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        train_metrics = dataset_info[\"metrics\"][\"train\"]\n        val_metrics = dataset_info[\"metrics\"][\"val\"]\n        init_schemes = dataset_info[\"init_schemes\"]\n\n        # For each initialization scheme, print final train/validation errors\n        for scheme, train_list, val_list in zip(\n            init_schemes, train_metrics, val_metrics\n        ):\n            scheme_label = f\"D init={scheme['D']}, codes init={scheme['codes']}\"\n            final_train_err = train_list[-1]\n            final_val_err = val_list[-1]\n            print(f\"Initialization scheme: {scheme_label}\")\n            print(f\"  train error (final epoch): {final_train_err:.6f}\")\n            print(f\"  validation error (final epoch): {final_val_err:.6f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through experiments and datasets\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, dataset_info in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        batch_sizes = dataset_info[\"batch_sizes\"]\n        train_errs = dataset_info[\"metrics\"][\"train\"]\n        val_errs = dataset_info[\"metrics\"][\"val\"]\n        train_losses = dataset_info[\"losses\"][\"train\"]\n        val_losses = dataset_info[\"losses\"][\"val\"]\n        # Print final metrics for each batch size\n        for bs, tr_err_list, vl_err_list, tr_loss_list, vl_loss_list in zip(\n            batch_sizes, train_errs, val_errs, train_losses, val_losses\n        ):\n            final_train_err = tr_err_list[-1]\n            final_val_err = vl_err_list[-1]\n            final_train_loss = tr_loss_list[-1]\n            final_val_loss = vl_loss_list[-1]\n            print(f\"  Batch size: {bs}\")\n            print(f\"    train reconstruction error: {final_train_err:.6f}\")\n            print(f\"    validation reconstruction error: {final_val_err:.6f}\")\n            print(f\"    train loss: {final_train_loss:.6f}\")\n            print(f\"    validation loss: {final_val_loss:.6f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over each penalty-type dataset and print final metrics\nfor penalty_type, results in experiment_data.items():\n    metrics = results[\"synthetic\"][\"metrics\"]\n    train_errors = metrics[\"train\"][-1]  # Last run\u2019s training error series\n    val_errors = metrics[\"val\"][-1]  # Last run\u2019s validation error series\n\n    final_train_error = train_errors[-1]  # Final epoch\u2019s training error\n    final_val_error = val_errors[-1]  # Final epoch\u2019s validation error\n\n    print(f\"Dataset: {penalty_type}\")\n    print(f\"  Final training relative error: {final_train_error:.6f}\")\n    print(f\"  Final validation relative error: {final_val_error:.6f}\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_data = np.load(\n    os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n).item()\n\n# Define the original orthogonality penalty grid\nlambda2_list = [0.0, 1e-4, 1e-3, 1e-2, 1e-1]\n\n# Iterate over datasets under dict_orth\nfor dataset_name, dataset in experiment_data[\"dict_orth\"].items():\n    print(f\"Dataset: {dataset_name}\")\n    train_metrics = dataset[\"metrics\"][\"train\"]\n    val_metrics = dataset[\"metrics\"][\"val\"]\n    train_losses = dataset[\"losses\"][\"train\"]\n    val_losses = dataset[\"losses\"][\"val\"]\n\n    # Loop through each lambda2 run and print final metrics\n    for lam2, train_errs, val_errs, train_los, val_los in zip(\n        lambda2_list, train_metrics, val_metrics, train_losses, val_losses\n    ):\n        final_train_error = train_errs[-1]\n        final_val_error = val_errs[-1]\n        final_train_loss = train_los[-1]\n        final_val_loss = val_los[-1]\n\n        print(f\"  Lambda2 = {lam2}\")\n        print(f\"    Final training normalized error: {final_train_error:.6f}\")\n        print(f\"    Final validation normalized error: {final_val_error:.6f}\")\n        print(f\"    Final training MSE loss: {final_train_loss:.6f}\")\n        print(f\"    Final validation MSE loss: {final_val_loss:.6f}\")\n", "", "import os\nimport numpy as np\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over each synthetic code distribution dataset\nfor dataset_name, dataset_info in experiment_data[\n    \"synthetic_code_distribution\"\n].items():\n    print(f\"Dataset: {dataset_name}\")\n    train_histories = dataset_info[\"metrics\"][\n        \"train\"\n    ]  # List of train error lists per beta1\n    val_histories = dataset_info[\"metrics\"][\"val\"]  # List of val error lists per beta1\n\n    # Extract the final (last-epoch) error for each beta1 setting\n    final_train_errors = [history[-1] for history in train_histories]\n    final_val_errors = [history[-1] for history in val_histories]\n\n    # Compute the best (minimum) final errors across all beta1 settings\n    best_train_error = min(final_train_errors)\n    best_val_error = min(final_val_errors)\n\n    # Print clearly labeled metrics\n    print(f\"  Best final training error: {best_train_error:.6f}\")\n    print(f\"  Best final validation error: {best_val_error:.6f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Define the order of loss functions and beta1 values used in the original runs\nrecon_fns = [\"mse\", \"mae\", \"huber\"]\nbeta1_list = [0.5, 0.7, 0.9, 0.99]\nparam_combinations = [(loss, b1) for loss in recon_fns for b1 in beta1_list]\n\n# Extract and print final metrics for each dataset\nfor dataset_name, ds in experiment_data[\"reconstruction_loss\"].items():\n    print(f\"Dataset: {dataset_name}\")\n    train_errs = ds[\"metrics\"][\"train\"]\n    val_errs = ds[\"metrics\"][\"val\"]\n    train_losses = ds[\"losses\"][\"train\"]\n    val_losses = ds[\"losses\"][\"val\"]\n\n    for idx, (loss_fn, b1) in enumerate(param_combinations):\n        # final epoch values\n        final_train_rel_err = train_errs[idx][-1]\n        final_val_rel_err = val_errs[idx][-1]\n        final_train_loss = train_losses[idx][-1]\n        final_val_loss = val_losses[idx][-1]\n\n        print(\n            f\"Final training relative reconstruction error for loss '{loss_fn}' with beta1={b1}: {final_train_rel_err:.6f}\"\n        )\n        print(\n            f\"Final validation relative reconstruction error for loss '{loss_fn}' with beta1={b1}: {final_val_rel_err:.6f}\"\n        )\n        print(f\"Final training {loss_fn.upper()} loss: {final_train_loss:.6f}\")\n        print(f\"Final validation {loss_fn.upper()} loss: {final_val_loss:.6f}\")\n        print()  # blank line for readability\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset in the solver ablation results\nfor dataset_name, dataset in experiment_data[\"test_time_solver_ablation\"].items():\n    print(f\"Dataset: {dataset_name}\")\n    metrics = dataset[\"metrics\"]\n    losses = dataset[\"losses\"]\n\n    # Final training error\n    train_err = metrics[\"train_err\"]\n    final_train_err = train_err[-1]\n    print(f\"Final Training Error: {final_train_err:.4f}\")\n\n    # Final validation error per solver\n    val_errs = metrics[\"val_err\"]\n    for solver_name, err_array in val_errs.items():\n        print(f\"Final Validation Error ({solver_name}): {err_array[-1]:.4f}\")\n\n    # Final training loss\n    train_loss = losses[\"train_loss\"]\n    final_train_loss = train_loss[-1]\n    print(f\"Final Training Loss: {final_train_loss:.6f}\")\n\n    # Final validation loss per solver\n    val_losses = losses[\"val_loss\"]\n    for solver_name, loss_array in val_losses.items():\n        print(f\"Final Validation Loss ({solver_name}): {loss_array[-1]:.6f}\")\n\n    print()  # Blank line between datasets\n", "import os\nimport numpy as np\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset (here only \"synthetic\") and its schedules\nfor dataset_name, dataset_info in experiment_data[\"lr_schedules\"].items():\n    print(f\"Dataset: {dataset_name}\")\n    train_errors = dataset_info[\"metrics\"][\"train\"]\n    val_errors = dataset_info[\"metrics\"][\"val\"]\n    train_losses = dataset_info[\"losses\"][\"train\"]\n    val_losses = dataset_info[\"losses\"][\"val\"]\n    schedules = dataset_info[\"schedules\"]\n\n    # Print final epoch metrics for each learning\u2010rate schedule\n    for idx, schedule_name in enumerate(schedules):\n        final_train_error = train_errors[idx][-1]\n        final_val_error = val_errors[idx][-1]\n        final_train_loss = train_losses[idx][-1]\n        final_val_loss = val_losses[idx][-1]\n\n        print(f\"  Schedule: {schedule_name}\")\n        print(f\"    Final training reconstruction error: {final_train_error:.6f}\")\n        print(f\"    Final validation reconstruction error: {final_val_error:.6f}\")\n        print(f\"    Final training MSE loss: {final_train_loss:.6f}\")\n        print(f\"    Final validation MSE loss: {final_val_loss:.6f}\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Recreate the beta1 list from the original experiments\nbeta1_list = [0.5, 0.7, 0.9, 0.99]\n\n# Iterate through each noise distribution and print final metrics\nfor dataset_name, dataset_info in experiment_data[\"noise_distribution\"].items():\n    print(f\"Dataset: {dataset_name}\")\n    for idx, beta1 in enumerate(beta1_list):\n        train_errors = dataset_info[\"metrics\"][\"train\"][idx]\n        val_errors = dataset_info[\"metrics\"][\"val\"][idx]\n        final_train_err = train_errors[-1]\n        final_val_err = val_errors[-1]\n        print(f\"  beta1={beta1} | train reconstruction error: {final_train_err:.6f}\")\n        print(f\"  beta1={beta1} | validation reconstruction error: {final_val_err:.6f}\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract the synthetic results\nsynthetic_data = experiment_data[\"data_dimensionality\"][\"synthetic\"]\ndims = synthetic_data[\"dims\"]\ntrain_errors = synthetic_data[\"metrics\"][\"train\"]\nval_errors = synthetic_data[\"metrics\"][\"val\"]\ntrain_losses = synthetic_data[\"losses\"][\"train\"]\nval_losses = synthetic_data[\"losses\"][\"val\"]\n\n# Print final metrics for each dimensionality\nfor dim, tr_err_list, v_err_list, tr_loss_list, v_loss_list in zip(\n    dims, train_errors, val_errors, train_losses, val_losses\n):\n    final_train_error = tr_err_list[-1]\n    final_val_error = v_err_list[-1]\n    final_train_loss = tr_loss_list[-1]\n    final_val_loss = v_loss_list[-1]\n\n    print(f\"Dataset: synthetic (dimension = {dim})\")\n    print(f\"Final Training Reconstruction Loss: {final_train_loss:.6f}\")\n    print(f\"Final Validation Reconstruction Loss: {final_val_loss:.6f}\")\n    print(f\"Final Training Relative Error: {final_train_error:.6f}\")\n    print(f\"Final Validation Relative Error: {final_val_error:.6f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over strategies and datasets\nfor strategy_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        # Extract all recorded values\n        ratios = data[\"ratios\"]\n        train_errs = data[\"metrics\"][\"train\"]\n        val_errs = data[\"metrics\"][\"val\"]\n        train_losses = data[\"losses\"][\"train\"]\n        val_losses = data[\"losses\"][\"val\"]\n        # For each ratio schedule, print final metrics\n        for (\n            (c_steps, d_steps),\n            tr_err_list,\n            vl_err_list,\n            tr_loss_list,\n            vl_loss_list,\n        ) in zip(ratios, train_errs, val_errs, train_losses, val_losses):\n            final_train_error = tr_err_list[-1]\n            final_validation_error = vl_err_list[-1]\n            final_train_loss = tr_loss_list[-1]\n            final_validation_loss = vl_loss_list[-1]\n            print(f\"  Schedule (code_updates:dict_updates) = {c_steps}:{d_steps}\")\n            print(f\"    Final training mean relative error: {final_train_error:.6f}\")\n            print(\n                f\"    Final validation mean relative error: {final_validation_error:.6f}\"\n            )\n            print(f\"    Final training MSE loss: {final_train_loss:.6f}\")\n            print(f\"    Final validation MSE loss: {final_validation_loss:.6f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over strategies and datasets\nfor strategy_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        # Extract all recorded values\n        ratios = data[\"ratios\"]\n        train_errs = data[\"metrics\"][\"train\"]\n        val_errs = data[\"metrics\"][\"val\"]\n        train_losses = data[\"losses\"][\"train\"]\n        val_losses = data[\"losses\"][\"val\"]\n        # For each ratio schedule, print final metrics\n        for (\n            (c_steps, d_steps),\n            tr_err_list,\n            vl_err_list,\n            tr_loss_list,\n            vl_loss_list,\n        ) in zip(ratios, train_errs, val_errs, train_losses, val_losses):\n            final_train_error = tr_err_list[-1]\n            final_validation_error = vl_err_list[-1]\n            final_train_loss = tr_loss_list[-1]\n            final_validation_loss = vl_loss_list[-1]\n            print(f\"  Schedule (code_updates:dict_updates) = {c_steps}:{d_steps}\")\n            print(f\"    Final training mean relative error: {final_train_error:.6f}\")\n            print(\n                f\"    Final validation mean relative error: {final_validation_error:.6f}\"\n            )\n            print(f\"    Final training MSE loss: {final_train_loss:.6f}\")\n            print(f\"    Final validation MSE loss: {final_validation_loss:.6f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over strategies and datasets\nfor strategy_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        # Extract all recorded values\n        ratios = data[\"ratios\"]\n        train_errs = data[\"metrics\"][\"train\"]\n        val_errs = data[\"metrics\"][\"val\"]\n        train_losses = data[\"losses\"][\"train\"]\n        val_losses = data[\"losses\"][\"val\"]\n        # For each ratio schedule, print final metrics\n        for (\n            (c_steps, d_steps),\n            tr_err_list,\n            vl_err_list,\n            tr_loss_list,\n            vl_loss_list,\n        ) in zip(ratios, train_errs, val_errs, train_losses, val_losses):\n            final_train_error = tr_err_list[-1]\n            final_validation_error = vl_err_list[-1]\n            final_train_loss = tr_loss_list[-1]\n            final_validation_loss = vl_loss_list[-1]\n            print(f\"  Schedule (code_updates:dict_updates) = {c_steps}:{d_steps}\")\n            print(f\"    Final training mean relative error: {final_train_error:.6f}\")\n            print(\n                f\"    Final validation mean relative error: {final_validation_error:.6f}\"\n            )\n            print(f\"    Final training MSE loss: {final_train_loss:.6f}\")\n            print(f\"    Final validation MSE loss: {final_validation_loss:.6f}\")\n        print()\n", ""], "parse_term_out": ["['Dataset: synthetic', '\\n', 'Hyperparameter beta1 = 0.5', '\\n', 'Final training\nerror: 17.538626', '\\n', 'Final validation error: 0.217411', '\\n', 'Final\ntraining loss: 9.616315', '\\n', 'Final validation loss: 0.137822', '\\n',\n'Hyperparameter beta1 = 0.7', '\\n', 'Final training error: 17.263470', '\\n',\n'Final validation error: 0.222304', '\\n', 'Final training loss: 9.420985', '\\n',\n'Final validation loss: 0.144676', '\\n', 'Hyperparameter beta1 = 0.9', '\\n',\n'Final training error: 16.123459', '\\n', 'Final validation error: 0.250144',\n'\\n', 'Final training loss: 8.641868', '\\n', 'Final validation loss: 0.185641',\n'\\n', 'Hyperparameter beta1 = 0.99', '\\n', 'Final training error: 14.575800',\n'\\n', 'Final validation error: 0.332078', '\\n', 'Final training loss: 7.621816',\n'\\n', 'Final validation loss: 0.334691', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: ds1', '\\n', 'final training relative error: 17.538626', '\\n', 'final\nvalidation relative error: 0.217411', '\\n', 'Dataset: ds2', '\\n', 'final\ntraining relative error: 1.495482', '\\n', 'final validation relative error:\n0.967739', '\\n', 'Dataset: ds3', '\\n', 'final training relative error:\n1.298961', '\\n', 'final validation relative error: 0.960261', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Configuration: n_components=10, beta1=0.5', '\\n',\n'final train error: 51.668240', '\\n', 'final validation error: 0.566849', '\\n',\n'Configuration: n_components=10, beta1=0.7', '\\n', 'final train error:\n50.827850', '\\n', 'final validation error: 0.570119', '\\n', 'Configuration:\nn_components=10, beta1=0.9', '\\n', 'final train error: 47.333130', '\\n', 'final\nvalidation error: 0.586792', '\\n', 'Configuration: n_components=10, beta1=0.99',\n'\\n', 'final train error: 42.105072', '\\n', 'final validation error: 0.630274',\n'\\n', 'Configuration: n_components=30, beta1=0.5', '\\n', 'final train error:\n17.538626', '\\n', 'final validation error: 0.217411', '\\n', 'Configuration:\nn_components=30, beta1=0.7', '\\n', 'final train error: 17.263470', '\\n', 'final\nvalidation error: 0.222304', '\\n', 'Configuration: n_components=30, beta1=0.9',\n'\\n', 'final train error: 16.123459', '\\n', 'final validation error: 0.250144',\n'\\n', 'Configuration: n_components=30, beta1=0.99', '\\n', 'final train error:\n14.575800', '\\n', 'final validation error: 0.332078', '\\n', 'Configuration:\nn_components=60, beta1=0.5', '\\n', 'final train error: 2.347096', '\\n', 'final\nvalidation error: 0.209950', '\\n', 'Configuration: n_components=60, beta1=0.7',\n'\\n', 'final train error: 2.315675', '\\n', 'final validation error: 0.214025',\n'\\n', 'Configuration: n_components=60, beta1=0.9', '\\n', 'final train error:\n2.184610', '\\n', 'final validation error: 0.239571', '\\n', 'Configuration:\nn_components=60, beta1=0.99', '\\n', 'final train error: 2.009281', '\\n', 'final\nvalidation error: 0.322587', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "['Dataset: synthetic', '\\n', '  Lambda1 = 0.0', '\\n', '    Final training\nreconstruction error: 16.123579', '\\n', '    Final validation reconstruction\nerror: 0.250150', '\\n', '    Final training reconstruction loss: 8.641713',\n'\\n', '    Final validation reconstruction loss: 0.185652', '\\n', '    Final\ncode sparsity (fraction near zero): 0.002083', '\\n', '    Final dictionary\nrecovery error: 0.374236', '\\n', '  Lambda1 = 0.0001', '\\n', '    Final training\nreconstruction error: 16.123579', '\\n', '    Final validation reconstruction\nerror: 0.250150', '\\n', '    Final training reconstruction loss: 8.641714',\n'\\n', '    Final validation reconstruction loss: 0.185652', '\\n', '    Final\ncode sparsity (fraction near zero): 0.002083', '\\n', '    Final dictionary\nrecovery error: 0.374236', '\\n', '  Lambda1 = 0.001', '\\n', '    Final training\nreconstruction error: 16.123615', '\\n', '    Final validation reconstruction\nerror: 0.250148', '\\n', '    Final training reconstruction loss: 8.641762',\n'\\n', '    Final validation reconstruction loss: 0.185648', '\\n', '    Final\ncode sparsity (fraction near zero): 0.002083', '\\n', '    Final dictionary\nrecovery error: 0.374235', '\\n', '  Lambda1 = 0.01', '\\n', '    Final training\nreconstruction error: 16.123459', '\\n', '    Final validation reconstruction\nerror: 0.250144', '\\n', '    Final training reconstruction loss: 8.641868',\n'\\n', '    Final validation reconstruction loss: 0.185641', '\\n', '    Final\ncode sparsity (fraction near zero): 0.002083', '\\n', '    Final dictionary\nrecovery error: 0.374231', '\\n', '  Lambda1 = 0.1', '\\n', '    Final training\nreconstruction error: 16.121967', '\\n', '    Final validation reconstruction\nerror: 0.250103', '\\n', '    Final training reconstruction loss: 8.642943',\n'\\n', '    Final validation reconstruction loss: 0.185579', '\\n', '    Final\ncode sparsity (fraction near zero): 0.002917', '\\n', '    Final dictionary\nrecovery error: 0.374184', '\\n', 'Execution time: a moment seconds (time limit\nis an hour).']", "['Dataset: synthetic', '\\n', '  Run 1: Train relative error: 1.8163', '\\n', '\nRun 2: Train relative error: 1.8078', '\\n', '  Run 3: Train relative error:\n1.7811', '\\n', '  Run 4: Train relative error: 1.7316', '\\n', '  Run 1:\nValidation relative error: 0.6569', '\\n', '  Run 2: Validation relative error:\n0.6567', '\\n', '  Run 3: Validation relative error: 0.6554', '\\n', '  Run 4:\nValidation relative error: 0.6580', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: synthetic (noise level = 0.0)', '\\n', '  train error: inf', '\\n', '\nvalidation error: 0.246146', '\\n', 'Dataset: synthetic (noise level = 0.005)',\n'\\n', '  train error: 29.772352', '\\n', '  validation error: 0.247228', '\\n',\n'Dataset: synthetic (noise level = 0.01)', '\\n', '  train error: 16.134037',\n'\\n', '  validation error: 0.249953', '\\n', 'Dataset: synthetic (noise level =\n0.02)', '\\n', '  train error: 9.312593', '\\n', '  validation error: 0.257838',\n'\\n', 'Dataset: synthetic (noise level = 0.05)', '\\n', '  train error:\n5.204982', '\\n', '  validation error: 0.284279', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Optimizer: SGD', '\\n', '  Final training relative\nerror: 33.3201', '\\n', '  Final validation relative error: 0.0229', '\\n', '\nFinal training reconstruction loss: 26.9503', '\\n', '  Final validation\nreconstruction loss: 0.0001', '\\n', 'Optimizer: RMSprop', '\\n', '  Final\ntraining relative error: 4.7871', '\\n', '  Final validation relative error:\n0.4722', '\\n', '  Final training reconstruction loss: 1.6136', '\\n', '  Final\nvalidation reconstruction loss: 0.6146', '\\n', 'Optimizer: AdamW', '\\n', '\nFinal training relative error: 15.9703', '\\n', '  Final validation relative\nerror: 0.2503', '\\n', '  Final training reconstruction loss: 8.5223', '\\n', '\nFinal validation reconstruction loss: 0.1858', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', '  Schedule (code_updates:dict_updates) = 1:1',\n'\\n', '    Final training mean relative error: 15.878441', '\\n', '    Final\nvalidation mean relative error: 0.250681', '\\n', '    Final training MSE loss:\n8.430651', '\\n', '    Final validation MSE loss: 0.186374', '\\n', '  Schedule\n(code_updates:dict_updates) = 5:1', '\\n', '    Final training mean relative\nerror: 2.927579', '\\n', '    Final validation mean relative error: 0.196716',\n'\\n', '    Final training MSE loss: 1.798701', '\\n', '    Final validation MSE\nloss: 0.114477', '\\n', '  Schedule (code_updates:dict_updates) = 10:1', '\\n', '\nFinal training mean relative error: 0.408293', '\\n', '    Final validation mean\nrelative error: 0.133907', '\\n', '    Final training MSE loss: 0.386549', '\\n',\n'    Final validation MSE loss: 0.050698', '\\n', '  Schedule\n(code_updates:dict_updates) = 1:5', '\\n', '    Final training mean relative\nerror: 7.103316', '\\n', '    Final validation mean relative error: 0.588157',\n'\\n', '    Final training MSE loss: 2.208671', '\\n', '    Final validation MSE\nloss: 1.018526', '\\n', '  Schedule (code_updates:dict_updates) = 1:10', '\\n', '\nFinal training mean relative error: 6.127098', '\\n', '    Final validation mean\nrelative error: 0.414043', '\\n', '    Final training MSE loss: 1.523779', '\\n',\n'    Final validation MSE loss: 0.456286', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Initialization scheme: D init=normal, codes\ninit=normal', '\\n', '  train error (final epoch): 16.123459', '\\n', '\nvalidation error (final epoch): 0.250144', '\\n', 'Initialization scheme: D\ninit=normal, codes init=xavier_uni', '\\n', '  train error (final epoch):\n0.751091', '\\n', '  validation error (final epoch): 0.209242', '\\n',\n'Initialization scheme: D init=normal, codes init=orthogonal', '\\n', '  train\nerror (final epoch): 0.653230', '\\n', '  validation error (final epoch):\n0.188502', '\\n', 'Initialization scheme: D init=normal, codes init=zeros', '\\n',\n'  train error (final epoch): 0.468580', '\\n', '  validation error (final\nepoch): 0.244881', '\\n', 'Initialization scheme: D init=xavier_uni, codes\ninit=normal', '\\n', '  train error (final epoch): 3.348321', '\\n', '  validation\nerror (final epoch): 0.442804', '\\n', 'Initialization scheme: D init=xavier_uni,\ncodes init=xavier_uni', '\\n', '  train error (final epoch): 0.604678', '\\n', '\nvalidation error (final epoch): 0.637429', '\\n', 'Initialization scheme: D\ninit=xavier_uni, codes init=orthogonal', '\\n', '  train error (final epoch):\n0.582282', '\\n', '  validation error (final epoch): 0.614484', '\\n',\n'Initialization scheme: D init=xavier_uni, codes init=zeros', '\\n', '  train\nerror (final epoch): 0.610414', '\\n', '  validation error (final epoch):\n0.651446', '\\n', 'Initialization scheme: D init=orthogonal, codes init=normal',\n'\\n', '  train error (final epoch): 3.417238', '\\n', '  validation error (final\nepoch): 0.438378', '\\n', 'Initialization scheme: D init=orthogonal, codes\ninit=xavier_uni', '\\n', '  train error (final epoch): 0.603755', '\\n', '\nvalidation error (final epoch): 0.613117', '\\n', 'Initialization scheme: D\ninit=orthogonal, codes init=orthogonal', '\\n', '  train error (final epoch):\n0.578523', '\\n', '  validation error (final epoch): 0.611485', '\\n',\n'Initialization scheme: D init=orthogonal, codes init=zeros', '\\n', '  train\nerror (final epoch): 0.626083', '\\n', '  validation error (final epoch):\n0.644340', '\\n', 'Initialization scheme: D init=zeros, codes init=normal', '\\n',\n'  train error (final epoch): 4.355175', '\\n', '  validation error (final\nepoch): 0.443011', '\\n', 'Initialization scheme: D init=zeros, codes\ninit=xavier_uni', '\\n', '  train error (final epoch): 0.595959', '\\n', '\nvalidation error (final epoch): 0.611067', '\\n', 'Initialization scheme: D\ninit=zeros, codes init=orthogonal', '\\n', '  train error (final epoch):\n0.601275', '\\n', '  validation error (final epoch): 0.641001', '\\n',\n'Initialization scheme: D init=zeros, codes init=zeros', '\\n', '  train error\n(final epoch): 1.000000', '\\n', '  validation error (final epoch): 1.000000',\n'\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', '  Batch size: 80', '\\n', '    train reconstruction\nerror: 15.880869', '\\n', '    validation reconstruction error: 0.250144', '\\n',\n'    train loss: 8.446345', '\\n', '    validation loss: 0.185641', '\\n', '\nBatch size: 40', '\\n', '    train reconstruction error: 10.987655', '\\n', '\nvalidation reconstruction error: 0.293826', '\\n', '    train loss: 4.714045',\n'\\n', '    validation loss: 0.275652', '\\n', '  Batch size: 20', '\\n', '\ntrain reconstruction error: 6.790787', '\\n', '    validation reconstruction\nerror: 0.410859', '\\n', '    train loss: 2.312921', '\\n', '    validation loss:\n0.517551', '\\n', '  Batch size: 10', '\\n', '    train reconstruction error:\n4.518011', '\\n', '    validation reconstruction error: 0.379591', '\\n', '\ntrain loss: 1.212506', '\\n', '    validation loss: 0.370475', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: none', '\\n', '  Final training relative error: 16.123579', '\\n', '\nFinal validation relative error: 0.250150', '\\n', 'Dataset: l1', '\\n', '  Final\ntraining relative error: 16.123459', '\\n', '  Final validation relative error:\n0.250144', '\\n', 'Dataset: l2', '\\n', '  Final training relative error:\n16.123398', '\\n', '  Final validation relative error: 0.250146', '\\n', 'Dataset:\nelasticnet', '\\n', '  Final training relative error: 16.123224', '\\n', '  Final\nvalidation relative error: 0.250141', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Dataset: synthetic', '\\n', '  Lambda2 = 0.0', '\\n', '    Final training\nnormalized error: 16.123459', '\\n', '    Final validation normalized error:\n0.250144', '\\n', '    Final training MSE loss: 8.641868', '\\n', '    Final\nvalidation MSE loss: 0.185641', '\\n', '  Lambda2 = 0.0001', '\\n', '    Final\ntraining normalized error: 16.437529', '\\n', '    Final validation normalized\nerror: 0.238989', '\\n', '    Final training MSE loss: 10.399351', '\\n', '\nFinal validation MSE loss: 0.172300', '\\n', '  Lambda2 = 0.001', '\\n', '\nFinal training normalized error: 16.439890', '\\n', '    Final validation\nnormalized error: 0.238982', '\\n', '    Final training MSE loss: 10.406961',\n'\\n', '    Final validation MSE loss: 0.172267', '\\n', '  Lambda2 = 0.01', '\\n',\n'    Final training normalized error: 16.440117', '\\n', '    Final validation\nnormalized error: 0.238981', '\\n', '    Final training MSE loss: 10.407721',\n'\\n', '    Final validation MSE loss: 0.172263', '\\n', '  Lambda2 = 0.1', '\\n',\n'    Final training normalized error: 16.440140', '\\n', '    Final validation\nnormalized error: 0.238981', '\\n', '    Final training MSE loss: 10.407796',\n'\\n', '    Final validation MSE loss: 0.172263', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "", "['Dataset: bernoulli_gaussian', '\\n', '  Best final training error: 14.575800',\n'\\n', '  Best final validation error: 0.217411\\n', '\\n', 'Dataset:\nbernoulli_laplace', '\\n', '  Best final training error: 9.460361', '\\n', '  Best\nfinal validation error: 0.257739\\n', '\\n', 'Dataset: bernoulli_uniform', '\\n', '\nBest final training error: 9.074814', '\\n', '  Best final validation error:\n0.242588\\n', '\\n', 'Dataset: block_sparse', '\\n', '  Best final training error:\n1.127977', '\\n', '  Best final validation error: 0.203753\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', \"Final training relative reconstruction error for\nloss 'mse' with beta1=0.5: 17.538626\", '\\n', \"Final validation relative\nreconstruction error for loss 'mse' with beta1=0.5: 0.217411\", '\\n', 'Final\ntraining MSE loss: 9.616315', '\\n', 'Final validation MSE loss: 0.137822', '\\n',\n'\\n', \"Final training relative reconstruction error for loss 'mse' with\nbeta1=0.7: 17.263470\", '\\n', \"Final validation relative reconstruction error for\nloss 'mse' with beta1=0.7: 0.222304\", '\\n', 'Final training MSE loss: 9.420985',\n'\\n', 'Final validation MSE loss: 0.144676', '\\n', '\\n', \"Final training\nrelative reconstruction error for loss 'mse' with beta1=0.9: 16.123459\", '\\n',\n\"Final validation relative reconstruction error for loss 'mse' with beta1=0.9:\n0.250144\", '\\n', 'Final training MSE loss: 8.641868', '\\n', 'Final validation\nMSE loss: 0.185641', '\\n', '\\n', \"Final training relative reconstruction error\nfor loss 'mse' with beta1=0.99: 14.575800\", '\\n', \"Final validation relative\nreconstruction error for loss 'mse' with beta1=0.99: 0.332078\", '\\n', 'Final\ntraining MSE loss: 7.621816', '\\n', 'Final validation MSE loss: 0.334691', '\\n',\n'\\n', \"Final training relative reconstruction error for loss 'mae' with\nbeta1=0.5: 15.125069\", '\\n', \"Final validation relative reconstruction error for\nloss 'mae' with beta1=0.5: 0.263467\", '\\n', 'Final training MAE loss: 2.069152',\n'\\n', 'Final validation MAE loss: 0.303676', '\\n', '\\n', \"Final training\nrelative reconstruction error for loss 'mae' with beta1=0.7: 15.024547\", '\\n',\n\"Final validation relative reconstruction error for loss 'mae' with beta1=0.7:\n0.265496\", '\\n', 'Final training MAE loss: 2.061745', '\\n', 'Final validation\nMAE loss: 0.306237', '\\n', '\\n', \"Final training relative reconstruction error\nfor loss 'mae' with beta1=0.9: 14.628970\", '\\n', \"Final validation relative\nreconstruction error for loss 'mae' with beta1=0.9: 0.279651\", '\\n', 'Final\ntraining MAE loss: 2.036294', '\\n', 'Final validation MAE loss: 0.323859', '\\n',\n'\\n', \"Final training relative reconstruction error for loss 'mae' with\nbeta1=0.99: 14.264664\", '\\n', \"Final validation relative reconstruction error\nfor loss 'mae' with beta1=0.99: 0.327714\", '\\n', 'Final training MAE loss:\n2.031380', '\\n', 'Final validation MAE loss: 0.383047', '\\n', '\\n', \"Final\ntraining relative reconstruction error for loss 'huber' with beta1=0.5:\n14.935898\", '\\n', \"Final validation relative reconstruction error for loss\n'huber' with beta1=0.5: 0.279978\", '\\n', 'Final training HUBER loss: 1.631637',\n'\\n', 'Final validation HUBER loss: 0.111086', '\\n', '\\n', \"Final training\nrelative reconstruction error for loss 'huber' with beta1=0.7: 14.838631\", '\\n',\n\"Final validation relative reconstruction error for loss 'huber' with beta1=0.7:\n0.281664\", '\\n', 'Final training HUBER loss: 1.625096', '\\n', 'Final validation\nHUBER loss: 0.112585', '\\n', '\\n', \"Final training relative reconstruction error\nfor loss 'huber' with beta1=0.9: 14.448506\", '\\n', \"Final validation relative\nreconstruction error for loss 'huber' with beta1=0.9: 0.296259\", '\\n', 'Final\ntraining HUBER loss: 1.598281', '\\n', 'Final validation HUBER loss: 0.124017',\n'\\n', '\\n', \"Final training relative reconstruction error for loss 'huber' with\nbeta1=0.99: 14.125102\", '\\n', \"Final validation relative reconstruction error\nfor loss 'huber' with beta1=0.99: 0.349288\", '\\n', 'Final training HUBER loss:\n1.585654', '\\n', 'Final validation HUBER loss: 0.166512', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Final Training Error: 16.1235', '\\n', 'Final\nValidation Error (pseudoinverse): 0.2501', '\\n', 'Final Validation Error (ista):\n0.2501', '\\n', 'Final Validation Error (fista): 0.2501', '\\n', 'Final Validation\nError (omp): 0.3553', '\\n', 'Final Training Loss: 8.641868', '\\n', 'Final\nValidation Loss (pseudoinverse): 0.185641', '\\n', 'Final Validation Loss (ista):\n0.185642', '\\n', 'Final Validation Loss (fista): 0.185641', '\\n', 'Final\nValidation Loss (omp): 0.478490', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Dataset: synthetic', '\\n', '  Schedule: fixed', '\\n', '    Final training\nreconstruction error: 16.123459', '\\n', '    Final validation reconstruction\nerror: 0.250144', '\\n', '    Final training MSE loss: 8.641868', '\\n', '\nFinal validation MSE loss: 0.185641', '\\n', '  Schedule: step_decay', '\\n', '\nFinal training reconstruction error: 27.547682', '\\n', '    Final validation\nreconstruction error: 0.103589', '\\n', '    Final training MSE loss: 20.171721',\n'\\n', '    Final validation MSE loss: 0.027945', '\\n', '  Schedule: exp_decay',\n'\\n', '    Final training reconstruction error: 26.802723', '\\n', '    Final\nvalidation reconstruction error: 0.109744', '\\n', '    Final training MSE loss:\n19.266787', '\\n', '    Final validation MSE loss: 0.031825', '\\n', '  Schedule:\ncosine', '\\n', '    Final training reconstruction error: 23.655577', '\\n', '\nFinal validation reconstruction error: 0.145139', '\\n', '    Final training MSE\nloss: 15.709567', '\\n', '    Final validation MSE loss: 0.058840', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: gaussian', '\\n', '  beta1=0.5 | train reconstruction error:\n17.538626', '\\n', '  beta1=0.5 | validation reconstruction error: 0.217411',\n'\\n', '  beta1=0.7 | train reconstruction error: 17.263470', '\\n', '  beta1=0.7\n| validation reconstruction error: 0.222304', '\\n', '  beta1=0.9 | train\nreconstruction error: 16.123459', '\\n', '  beta1=0.9 | validation reconstruction\nerror: 0.250144', '\\n', '  beta1=0.99 | train reconstruction error: 14.575800',\n'\\n', '  beta1=0.99 | validation reconstruction error: 0.332078', '\\n',\n'Dataset: laplace', '\\n', '  beta1=0.5 | train reconstruction error: 13.217325',\n'\\n', '  beta1=0.5 | validation reconstruction error: 0.220058', '\\n', '\nbeta1=0.7 | train reconstruction error: 13.012154', '\\n', '  beta1=0.7 |\nvalidation reconstruction error: 0.224910', '\\n', '  beta1=0.9 | train\nreconstruction error: 12.162434', '\\n', '  beta1=0.9 | validation reconstruction\nerror: 0.252549', '\\n', '  beta1=0.99 | train reconstruction error: 11.006574',\n'\\n', '  beta1=0.99 | validation reconstruction error: 0.333925', '\\n',\n'Dataset: cauchy', '\\n', '  beta1=0.5 | train reconstruction error: 2.688206',\n'\\n', '  beta1=0.5 | validation reconstruction error: 0.491817', '\\n', '\nbeta1=0.7 | train reconstruction error: 2.654567', '\\n', '  beta1=0.7 |\nvalidation reconstruction error: 0.494093', '\\n', '  beta1=0.9 | train\nreconstruction error: 2.516221', '\\n', '  beta1=0.9 | validation reconstruction\nerror: 0.507512', '\\n', '  beta1=0.99 | train reconstruction error: 2.327018',\n'\\n', '  beta1=0.99 | validation reconstruction error: 0.549536', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: synthetic (dimension = 64)', '\\n', 'Final Training Reconstruction\nLoss: 7.517020', '\\n', 'Final Validation Reconstruction Loss: 1.413808', '\\n',\n'Final Training Relative Error: 19.249327', '\\n', 'Final Validation Relative\nError: 0.679548', '\\n', '\\n', 'Dataset: synthetic (dimension = 256)', '\\n',\n'Final Training Reconstruction Loss: 8.599186', '\\n', 'Final Validation\nReconstruction Loss: 2.704189', '\\n', 'Final Training Relative Error:\n20.850634', '\\n', 'Final Validation Relative Error: 0.928862', '\\n', '\\n',\n'Dataset: synthetic (dimension = 1024)', '\\n', 'Final Training Reconstruction\nLoss: 8.806726', '\\n', 'Final Validation Reconstruction Loss: 2.956825', '\\n',\n'Final Training Relative Error: 21.287039', '\\n', 'Final Validation Relative\nError: 0.978556', '\\n', '\\n', 'Dataset: synthetic (dimension = 4096)', '\\n',\n'Final Training Reconstruction Loss: 8.906409', '\\n', 'Final Validation\nReconstruction Loss: 3.026514', '\\n', 'Final Training Relative Error:\n21.571699', '\\n', 'Final Validation Relative Error: 0.988734', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', '  Schedule (code_updates:dict_updates) = 1:1',\n'\\n', '    Final training mean relative error: 15.878441', '\\n', '    Final\nvalidation mean relative error: 0.250681', '\\n', '    Final training MSE loss:\n8.430651', '\\n', '    Final validation MSE loss: 0.186374', '\\n', '  Schedule\n(code_updates:dict_updates) = 5:1', '\\n', '    Final training mean relative\nerror: 2.927579', '\\n', '    Final validation mean relative error: 0.196716',\n'\\n', '    Final training MSE loss: 1.798701', '\\n', '    Final validation MSE\nloss: 0.114477', '\\n', '  Schedule (code_updates:dict_updates) = 10:1', '\\n', '\nFinal training mean relative error: 0.408293', '\\n', '    Final validation mean\nrelative error: 0.133907', '\\n', '    Final training MSE loss: 0.386549', '\\n',\n'    Final validation MSE loss: 0.050698', '\\n', '  Schedule\n(code_updates:dict_updates) = 1:5', '\\n', '    Final training mean relative\nerror: 7.103316', '\\n', '    Final validation mean relative error: 0.588157',\n'\\n', '    Final training MSE loss: 2.208671', '\\n', '    Final validation MSE\nloss: 1.018526', '\\n', '  Schedule (code_updates:dict_updates) = 1:10', '\\n', '\nFinal training mean relative error: 6.127098', '\\n', '    Final validation mean\nrelative error: 0.414043', '\\n', '    Final training MSE loss: 1.523779', '\\n',\n'    Final validation MSE loss: 0.456286', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', '  Schedule (code_updates:dict_updates) = 1:1',\n'\\n', '    Final training mean relative error: 15.878441', '\\n', '    Final\nvalidation mean relative error: 0.250681', '\\n', '    Final training MSE loss:\n8.430651', '\\n', '    Final validation MSE loss: 0.186374', '\\n', '  Schedule\n(code_updates:dict_updates) = 5:1', '\\n', '    Final training mean relative\nerror: 2.927579', '\\n', '    Final validation mean relative error: 0.196716',\n'\\n', '    Final training MSE loss: 1.798701', '\\n', '    Final validation MSE\nloss: 0.114477', '\\n', '  Schedule (code_updates:dict_updates) = 10:1', '\\n', '\nFinal training mean relative error: 0.408293', '\\n', '    Final validation mean\nrelative error: 0.133907', '\\n', '    Final training MSE loss: 0.386549', '\\n',\n'    Final validation MSE loss: 0.050698', '\\n', '  Schedule\n(code_updates:dict_updates) = 1:5', '\\n', '    Final training mean relative\nerror: 7.103316', '\\n', '    Final validation mean relative error: 0.588157',\n'\\n', '    Final training MSE loss: 2.208671', '\\n', '    Final validation MSE\nloss: 1.018526', '\\n', '  Schedule (code_updates:dict_updates) = 1:10', '\\n', '\nFinal training mean relative error: 6.127098', '\\n', '    Final validation mean\nrelative error: 0.414043', '\\n', '    Final training MSE loss: 1.523779', '\\n',\n'    Final validation MSE loss: 0.456286', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', '  Schedule (code_updates:dict_updates) = 1:1',\n'\\n', '    Final training mean relative error: 15.878441', '\\n', '    Final\nvalidation mean relative error: 0.250681', '\\n', '    Final training MSE loss:\n8.430651', '\\n', '    Final validation MSE loss: 0.186374', '\\n', '  Schedule\n(code_updates:dict_updates) = 5:1', '\\n', '    Final training mean relative\nerror: 2.927579', '\\n', '    Final validation mean relative error: 0.196716',\n'\\n', '    Final training MSE loss: 1.798701', '\\n', '    Final validation MSE\nloss: 0.114477', '\\n', '  Schedule (code_updates:dict_updates) = 10:1', '\\n', '\nFinal training mean relative error: 0.408293', '\\n', '    Final validation mean\nrelative error: 0.133907', '\\n', '    Final training MSE loss: 0.386549', '\\n',\n'    Final validation MSE loss: 0.050698', '\\n', '  Schedule\n(code_updates:dict_updates) = 1:5', '\\n', '    Final training mean relative\nerror: 7.103316', '\\n', '    Final validation mean relative error: 0.588157',\n'\\n', '    Final training MSE loss: 2.208671', '\\n', '    Final validation MSE\nloss: 1.018526', '\\n', '  Schedule (code_updates:dict_updates) = 1:10', '\\n', '\nFinal training mean relative error: 6.127098', '\\n', '    Final validation mean\nrelative error: 0.414043', '\\n', '    Final training MSE loss: 1.523779', '\\n',\n'    Final validation MSE loss: 0.456286', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>

\begin{thebibliography}{13}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aharon et~al.(2006)Aharon, Elad, and Bruckstein]{aharon2006rmka}
M.~Aharon, M.~Elad, and A.~Bruckstein.
\newblock $k$‚Äêsvd: An algorithm for designing overcomplete dictionaries for
  sparse representation.
\newblock \emph{IEEE Transactions on Signal Processing}, 54:\penalty0
  4311--4322, 2006.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017modelagnosticmf}
Chelsea Finn, P.~Abbeel, and S.~Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock \emph{ICML}, 2017.

\bibitem[Gregor \& LeCun(2010)Gregor and LeCun]{gregor2010learningfa}
Karol Gregor and Yann LeCun.
\newblock Learning fast approximations of sparse coding.
\newblock \emph{ICML}, pp.\  399--406, 2010.

\bibitem[Ha et~al.(2016)Ha, Dai, and Le]{ha2016hypernetworks}
David Ha, Andrew~M. Dai, and Quoc~V. Le.
\newblock Hypernetworks.
\newblock \emph{ArXiv}, abs/1609.09106, 2016.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015deeprl}
Kaiming He, X.~Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock \emph{CVPR}, pp.\  770--778, 2015.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, and
  Chen]{hu2021lorala}
J.~E. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{ArXiv}, abs/2106.09685, 2021.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018averagingwl}
Pavel Izmailov, Dmitrii Podoprikhin, T.~Garipov, D.~Vetrov, and A.~Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock \emph{UAI}, pp.\  876--885, 2018.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adamam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{CoRR}, abs/1412.6980, 2014.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learningml}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Mairal et~al.(2009)Mairal, Bach, Ponce, and
  Sapiro]{mairal2009onlinelf}
J.~Mairal, F.~Bach, J.~Ponce, and G.~Sapiro.
\newblock Online learning for matrix factorization and sparse coding.
\newblock \emph{J. Mach. Learn. Res.}, 11:\penalty0 19--60, 2009.

\bibitem[Olshausen \& Field(1997)Olshausen and Field]{olshausen1997sparsecw}
B.~Olshausen and D.~Field.
\newblock Sparse coding with an overcomplete basis set: A strategy employed by
  v1?
\newblock \emph{Vision Research}, 37:\penalty0 3311--3325, 1997.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014verydc}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{ArXiv}, abs/1409.1556, 2014.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Gontijo-Lopes,
  Morcos, Namkoong, Farhadi, Carmon, Kornblith, and
  Schmidt]{wortsman2022modelsa}
Mitchell Wortsman, Gabriel Ilharco, S.~Gadre, R.~Roelofs, Raphael
  Gontijo-Lopes, Ari~S. Morcos, Hongseok Namkoong, Ali Farhadi, Y.~Carmon,
  Simon Kornblith, and Ludwig Schmidt.
\newblock Model soups: averaging weights of multiple fine-tuned models improves
  accuracy without increasing inference time.
\newblock \emph{ArXiv}, abs/2203.05482, 2022.

\end{thebibliography}

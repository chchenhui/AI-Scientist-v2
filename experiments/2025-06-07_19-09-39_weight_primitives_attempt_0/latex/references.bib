%% LaTeX2e file `references.bib'
%% generated by the `filecontents' environment
%% from source `template' on 2025/06/07.
%%
@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT Press}
}
@article{aharon2006rmka,
  author = {M. Aharon and M. Elad and A. Bruckstein},
  journal = {IEEE Transactions on Signal Processing},
  pages = {4311--4322},
  title = {$K$‚ÄêSVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation},
  volume = {54},
  year = {2006}
}
@article{olshausen1997sparsecw,
  author = {B. Olshausen and D. Field},
  journal = {Vision Research},
  pages = {3311--3325},
  title = {Sparse coding with an overcomplete basis set: A strategy employed by V1?},
  volume = {37},
  year = {1997}
}
@article{kingma2014adamam,
  author = {Diederik P. Kingma and Jimmy Ba},
  journal = {CoRR},
  title = {Adam: A Method for Stochastic Optimization},
  volume = {abs/1412.6980},
  year = {2014}
}
@article{ha2016hypernetworks,
  author = {David Ha and Andrew M. Dai and Quoc V. Le},
  journal = {ArXiv},
  title = {HyperNetworks},
  volume = {abs/1609.09106},
  year = {2016}
}
@article{wortsman2022modelsa,
  author = {Mitchell Wortsman and Gabriel Ilharco and S. Gadre and R. Roelofs and Raphael Gontijo-Lopes and Ari S. Morcos and Hongseok Namkoong and Ali Farhadi and Y. Carmon and Simon Kornblith and Ludwig Schmidt},
  journal = {ArXiv},
  title = {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  volume = {abs/2203.05482},
  year = {2022}
}
@article{finn2017modelagnosticmf,
  author = {Chelsea Finn and P. Abbeel and S. Levine},
  journal = {ICML},
  title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  year = {2017}
}
@article{garipov2018losssm,
  author = {T. Garipov and P. Izmailov and D. Podoprikhin and D. Vetrov and A. Wilson},
  journal = {ArXiv},
  title = {Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs},
  volume = {abs/1802.10026},
  year = {2018}
}
@article{mairal2009onlinelf,
  author = {J. Mairal and F. Bach and J. Ponce and G. Sapiro},
  journal = {J. Mach. Learn. Res.},
  pages = {19--60},
  title = {Online Learning for Matrix Factorization and Sparse Coding},
  volume = {11},
  year = {2009}
}
@inproceedings{krizhevsky2009learningml,
  author = {A. Krizhevsky},
  title = {Learning Multiple Layers of Features from Tiny Images},
  year = {2009}
}
@article{he2015deeprl,
  author = {Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal = {CVPR},
  pages = {770--778},
  title = {Deep Residual Learning for Image Recognition},
  year = {2015}
}
@article{simonyan2014verydc,
  author = {K. Simonyan and A. Zisserman},
  journal = {ArXiv},
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  volume = {abs/1409.1556},
  year = {2014}
}
@inproceedings{netzer2011readingdi,
  author = {Yuval Netzer and Tao Wang and Adam Coates and A. Bissacco and Bo Wu and A. Ng},
  title = {Reading Digits in Natural Images with Unsupervised Feature Learning},
  year = {2011}
}
@article{gregor2010learningfa,
  author = {Karol Gregor and Yann LeCun},
  journal = {ICML},
  pages = {399--406},
  title = {Learning Fast Approximations of Sparse Coding},
  year = {2010}
}
@article{izmailov2018averagingwl,
  author = {Pavel Izmailov and Dmitrii Podoprikhin and T. Garipov and D. Vetrov and A. Wilson},
  journal = {UAI},
  pages = {876--885},
  title = {Averaging Weights Leads to Wider Optima and Better Generalization},
  year = {2018}
}
@article{hu2021lorala,
  author = {J. E. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},
  journal = {ArXiv},
  title = {LoRA: Low-Rank Adaptation of Large Language Models},
  volume = {abs/2106.09685},
  year = {2021}
}
@article{li2017visualizingtl,
  author = {Hao Li and Zheng Xu and Gavin Taylor and T. Goldstein},
  journal = {ArXiv},
  title = {Visualizing the Loss Landscape of Neural Nets},
  volume = {abs/1712.09913},
  year = {2017}
}

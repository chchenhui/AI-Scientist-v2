{"edges": [[1, 4], [5, 9], [14, 17], [18, 21], [18, 19], [18, 22], [18, 20]], "layout": [[0.0, 0.0], [0.06060606060606061, 0.0], [0.12121212121212122, 0.0], [0.18181818181818182, 0.0], [0.06060606060606061, 1.0], [0.24242424242424243, 0.0], [0.30303030303030304, 0.0], [0.36363636363636365, 0.0], [0.42424242424242425, 0.0], [0.24242424242424243, 1.0], [0.48484848484848486, 0.0], [0.5454545454545454, 0.0], [0.6060606060606061, 0.0], [0.6666666666666666, 0.0], [0.7272727272727273, 0.0], [0.7878787878787878, 0.0], [0.8484848484848485, 0.0], [0.7272727272727273, 1.0], [0.9090909090909091, 0.0], [0.8181818181818182, 1.0], [0.8787878787878788, 1.0], [0.9393939393939394, 1.0], [1.0, 1.0]], "plan": ["Hyperparam tuning name: learning_rate. I\u2019ll sweep the Adam initial learning rate\nover a log\u2010uniform grid [1e-4, 5e-4, 1e-3, 5e-3, 1e-2], re\u2010initializing models\nat each setting for a fair comparison. For each learning rate I train both AI\nand user MLPs jointly over a fixed number of epochs, recording\ntraining/validation losses and alignments per epoch, and finally collecting\nvalidation predictions and ground truth. All plottable data (metrics, losses,\npredictions, ground truth) are organized under\n`experiment_data['learning_rate']['synthetic']` and saved via `np.save(...,\n'experiment_data.npy')`.", "Ablation name: Fine-Tune-DistilBERT. Below is a solution that runs both the\nfrozen\u2010backbone baseline and the end\u2010to\u2010end fine\u2010tuning ablation.  We loop over\ntwo settings (`baseline` vs. `fine_tune`), three datasets each, and save losses,\naccuracies, alignments, MAI, predictions, and labels into a nested dict which is\nfinally saved to `experiment_data.npy`.", "Ablation name: Linear-Probe Classifier (No Hidden Layer). Below is a script that\nruns both the original two-layer MLP (\u201cbaseline\u201d) and a single\u2010layer linear\nprobe (\u201clinear_probe\u201d) on top of frozen DistilBERT embeddings across three\ndatasets. It collects train/validation losses, accuracies (\u201cmetrics\u201d),\nalignments, MAI, and final predictions, stores everything in a nested dictionary\nkeyed by ablation and dataset, converts lists to NumPy arrays, and saves the\nresult as `experiment_data.npy` in the `working` directory.", "Ablation name: Pooling Mechanism Ablation. We will implement two pooling\nstrategies\u2014CLS token and mean pooling\u2014by looping over both as separate ablation\ntypes. For each, we train and evaluate on AG News, Yelp Polarity, and DBpedia14,\nreplacing the embedding extraction step accordingly. We record per\u2010epoch\ntrain/val losses, accuracies, alignments, and MAI, plus final predictions and\nground truths. All results go into a nested `experiment_data` dict keyed first\nby pooling type then by dataset, and we save it as `experiment_data.npy`.", "We can stabilize fine-tuning by decoupling the learning rates of the backbone\nand the MLP head\u2014using a small lr (e.g. 2e-5) for the DistilBERT parameters and\na higher lr (1e-3) for the classifier head\u2014and by explicitly toggling the\nencoder\u2019s train/eval modes so that dropout is disabled during validation. This\nprevents divergence in the \u201cfine_tune\u201d ablation and ensures reliable evaluation\nmetrics. Below is the revised full script.", "Ablation name: Representation Depth Ablation. We define a set of ablation\nfunctions that extract [CLS] embeddings from different DistilBERT layers (layers\n1, 3, 5) or an average of the last two layers, then tokenize each dataset, build\nDataLoaders, and loop over ablations and datasets to train two MLPs per run. In\neach epoch we compute train/val losses, JS\u2010alignment, and MAI, and after\ntraining save final predictions and ground truth. All results are stored in a\nnested `experiment_data` dict keyed by ablation and dataset, then saved as\n`experiment_data.npy`.", "Ablation name: MLP Dropout Rate Ablation. We will extend the baseline by adding\na Dropout layer in the MLP head parameterized by dropout probability, and sweep\nover the set {0.0, 0.1, 0.3, 0.5}. For each dropout rate and each dataset, we\ntrain ai_model and user_model for 3 epochs on DistilBERT embeddings, record\ntraining/validation losses, JSD\u2010based alignments, and the MAI metric per epoch,\nand finally collect predictions and ground truths. All results are stored in a\nnested `experiment_data` dictionary under `\"mlp_dropout_rate_ablation\"`, keyed\nby dropout rate and dataset name, then saved to `experiment_data.npy`. Below is\nthe complete self-contained Python script.", "Ablation name: Activation Function Ablation. I will parameterize the MLP head to\naccept different activations (ReLU, GELU, Tanh, LeakyReLU) and loop over these\nin our ablation. For each activation and dataset, I train paired AI and user\nmodels as before, recording train/validation losses, JS\u2010based alignment, and\nvalidation MAI. After finishing each run I collect the AI model\u2019s predictions\nand ground truth into a nested dictionary keyed by activation and dataset.\nFinally, I save the complete experiment_data structure to\nworking_dir/experiment_data.npy.", "Ablation name: Alignment Regularization Ablation. Below is a single\u2010file Python\nscript that loads DistilBERT embeddings and trains paired MLP \u201cai\u201d and \u201cuser\u201d\nmodels on three text classification datasets, sweeping the JSD alignment penalty\n\u03bb\u2208{0,0.1,0.5,1.0}.  For each \u03bb and each dataset, it jointly optimizes\ncross\u2010entropy plus \u03bb\u00b7JSD over the models\u2019 soft\u2010predictions, records per\u2010epoch\ntrain/val losses, accuracies, alignment scores and MAI, gathers final\npredictions and ground truth, and stores everything in a nested\n`experiment_data` dict which is saved to `experiment_data.npy`.  Reproducibility\nis ensured by reseeding before each sweep.", "We will compute the Bidirectional Mental Model Alignment Score by measuring two\nnormalized JSD\u2010based similarities: one between the AI\u2019s output and the\nground\u2010truth one\u2010hot label, and one between the user model\u2019s output and the AI\u2019s\noutput.  After normalizing by ln\u20092, we average these two similarity scores to\nobtain the bidirectional alignment metric, log it each epoch for both train and\nvalidation, and print validation loss and the new alignment score per epoch.  We\nensure proper `.to(device)` placement for models and batches, create optimizers\nafter moving models to device, and save all metrics and predictions at the end.", "Ablation name: Loss Function Ablation \u2013 Focal Loss. Below is a sketch of the\napproach followed by the full Python program:  We define a custom FocalLoss\nmodule and then loop over \u03b3\u2208{1,2,5} as separate ablation settings. For each \u03b3\nand each dataset we train both the AI and user MLPs for 3 epochs, replacing\ncross\u2010entropy by focal loss, while tracking training/validation losses,\nJS\u2010divergence alignment, and MAI. Finally we save all collected losses,\nalignments, MAIs, predictions and labels into a single experiment_data.npy.", "Ablation name: Attention Head Ablation. Below is a self\u2010contained Python script\nthat performs attention\u2010head ablation (random vs. importance\u2010ranked) at 12, 8,\n4, and 2 heads per layer on three text classification datasets. It gathers\nlosses, JS\u2010alignment, MAI, predictions, and ground truths into a nested\ndictionary and saves everything as `experiment_data.npy`.", "Ablation name: DistilBERT Pre-training Ablation. Below is the outline and the\nfull implementation.  We loop over two ablation types (\u201cpretrained\u201d vs.\n\u201crandom_init\u201d), initialize/freeze the corresponding DistilBERT encoder, and then\nfor each dataset train two MLPs exactly as before. We record per\u2010epoch losses,\nalignment scores, MAI, and final predictions & ground truths into a nested\ndictionary `experiment_data` keyed by ablation type and dataset, then save it\nall to `experiment_data.npy`.", "Ablation name: Token Dropout Ablation. We load three classification datasets and\ntokenize them to length 128. For each token\u2010dropout rate (0%, 10%, 20%, 30%), we\nrandomly mask tokens (excluding [CLS] and padding) before passing through a\nfrozen DistilBERT, then train two MLP heads to capture classification accuracy\nand JSD\u2010based alignment (plus MAI) over three epochs. We record train/val\nlosses, accuracies, alignments, and val MAI per epoch, along with final\npredictions and ground truths. All results are stored in a nested dictionary\nkeyed by ablation rate and dataset, then saved to `experiment_data.npy`.", "Ablation name: Word-Order Sensitivity Ablation. Below is an implementation that\nruns two conditions\u2014standard and shuffled word\u2010order\u2014on all three datasets,\ntraining separate DistilBERT+MLP pipelines and computing losses, alignment, MAI,\nand predictions. Shuffling randomly permutes non\u2010padding tokens per example\nwhile keeping the [CLS] token fixed. All results are stored under\nexperiment_data[ablation][dataset] and saved as \u201cexperiment_data.npy.\u201d", "Ablation name: Optimizer Choice Ablation. We will loop over three optimizers\n(Adam, SGD with momentum, AdamW) as our ablation variations, and for each\ndataset (AG News, Yelp Polarity, DBpedia) we reinitialize the MLP models, train\nfor three epochs on frozen DistilBERT embeddings, and record training/validation\nlosses, alignment (via JS divergence), user accuracy, and MAI. After training,\nwe collect AI model predictions and ground truths on the validation set. All\nmetrics are stored in a nested experiment_data dictionary keyed by optimizer and\ndataset, then saved to \"experiment_data.npy\" under a \"working\" folder.", "Ablation name: MLP Parameter Sharing Ablation. Here is a sketch of the\nimplementation: we loop over three ablation conditions\u2014independent MLPs, sharing\nonly fc1, and fully shared\u2014creating models and optimizers appropriately, then\nrun the same training/validation procedure while recording losses, alignment,\nMAI, and final predictions. We store everything in a nested `experiment_data`\ndict and save it once at the end.", "We avoid freeing the graph twice by summing the AI and user losses into one\n`total_loss` and performing a single backward pass, so both MLP heads get their\ngradients without needing `retain_graph`. We wrap DistilBERT\u2019s forward in\n`torch.no_grad()` (and freeze its parameters) so only the task-specific heads\nare trained, and we zero both optimizers before calling `total_loss.backward()`\nand then stepping them. We also add the required GPU setup at the top, move all\nmodels and tensors to the device, track the Bidirectional Mental Model Alignment\nScore per epoch, print validation losses, and save the full `experiment_data` at\nthe end.", "Ablation name: Softmax Temperature Ablation. We implement a softmax temperature\nablation study by sweeping T over {0.5, 1.0, 2.0}, training separate MLP pairs\n(ai_model and user_model) on each dataset per temperature. For each temperature\nand dataset, we record per-epoch train/val losses, JS alignment, accuracy, and\nMAI, and save final predictions and ground truths. The distributions P and Q are\ncomputed via softmax(logits / T). Results are stored in a hierarchical dict\nkeyed by ablation setting and saved as \u2018experiment_data.npy\u2019.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Reproducibility\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Synthetic data generation\nN_train, N_val, D, C = 1000, 200, 10, 3\nW_true = np.random.randn(D, C)\nx_train = np.random.randn(N_train, D)\nx_val = np.random.randn(N_val, D)\ny_train = np.argmax(x_train @ W_true + 0.1 * np.random.randn(N_train, C), axis=1)\ny_val = np.argmax(x_val @ W_true + 0.1 * np.random.randn(N_val, C), axis=1)\nmean = x_train.mean(axis=0)\nstd = x_train.std(axis=0) + 1e-8\nx_train = (x_train - mean) / std\nx_val = (x_val - mean) / std\n\n# DataLoaders\ntrain_dataset = TensorDataset(\n    torch.tensor(x_train, dtype=torch.float32),\n    torch.tensor(y_train, dtype=torch.long),\n)\nval_dataset = TensorDataset(\n    torch.tensor(x_val, dtype=torch.float32),\n    torch.tensor(y_val, dtype=torch.long),\n)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64)\n\n\n# Simple MLP definition\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\n# Hyperparameter sweep over learning rates\nlearning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\nnum_epochs = 10\n\n# Initialize experiment_data dict\nexperiment_data = {\n    \"learning_rate\": {\n        \"synthetic\": {\n            \"lrs\": learning_rates,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nloss_fn = nn.CrossEntropyLoss()\n\nfor lr in learning_rates:\n    # Reset model weights for fair comparison\n    torch.manual_seed(0)\n    ai_model = MLP(D, 32, C).to(device)\n    user_model = MLP(D, 32, C).to(device)\n    optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=lr)\n    optimizer_user = torch.optim.Adam(user_model.parameters(), lr=lr)\n\n    # Containers for this learning rate\n    train_losses, val_losses = [], []\n    train_aligns, val_aligns = [], []\n\n    for epoch in range(1, num_epochs + 1):\n        # Training\n        ai_model.train()\n        user_model.train()\n        total_loss, total_align, n_samples = 0.0, 0.0, 0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            # Forward passes\n            logits_ai = ai_model(xb)\n            logits_user = user_model(xb)\n            # Cross-entropy losses\n            loss_ai = loss_fn(logits_ai, yb)\n            loss_user = loss_fn(logits_user, yb)\n            # Backprop AI\n            optimizer_ai.zero_grad()\n            loss_ai.backward()\n            optimizer_ai.step()\n            # Backprop user\n            optimizer_user.zero_grad()\n            loss_user.backward()\n            optimizer_user.step()\n            # Accumulate loss\n            bs = yb.size(0)\n            total_loss += loss_ai.item() * bs\n            # Alignment (1 - JSD)\n            P = F.softmax(logits_ai, dim=1)\n            Q = F.softmax(logits_user, dim=1)\n            M = 0.5 * (P + Q)\n            kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            jsd = 0.5 * (kl1 + kl2)\n            total_align += torch.sum(1 - jsd).item()\n            n_samples += bs\n        train_losses.append(total_loss / len(train_dataset))\n        train_aligns.append(total_align / n_samples)\n\n        # Validation\n        ai_model.eval()\n        user_model.eval()\n        v_loss, v_align, v_samples = 0.0, 0.0, 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits_ai = ai_model(xb)\n                # loss\n                v_loss += loss_fn(logits_ai, yb).item() * yb.size(0)\n                # alignment\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(user_model(xb), dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                v_align += torch.sum(1 - jsd).item()\n                v_samples += yb.size(0)\n        val_losses.append(v_loss / len(val_dataset))\n        val_aligns.append(v_align / v_samples)\n        print(\n            f\"LR {lr:.1e} Epoch {epoch}: val_loss = {val_losses[-1]:.4f}, val_align = {val_aligns[-1]:.4f}\"\n        )\n\n    # Store per\u2010epoch metrics\n    sd = experiment_data[\"learning_rate\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(train_aligns)\n    sd[\"metrics\"][\"val\"].append(val_aligns)\n    sd[\"losses\"][\"train\"].append(train_losses)\n    sd[\"losses\"][\"val\"].append(val_losses)\n\n    # Final validation predictions & ground truth\n    all_preds, all_gts = [], []\n    ai_model.eval()\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            xb = xb.to(device)\n            preds = torch.argmax(ai_model(xb), dim=1).cpu().numpy()\n            all_preds.append(preds)\n            all_gts.append(yb.numpy())\n    preds_arr = np.concatenate(all_preds, axis=0)\n    gts_arr = np.concatenate(all_gts, axis=0)\n    sd[\"predictions\"].append(preds_arr)\n    sd[\"ground_truth\"].append(gts_arr)\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(42)\nnp.random.seed(42)\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n\n\n# simple 2\u2010layer MLP\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\n# store all results\nexperiment_data = {\"baseline\": {}, \"fine_tune\": {}}\n\n# iterate ablations\nfor ablation in [\"baseline\", \"fine_tune\"]:\n    for name in [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]:\n        # init per\u2010dataset dict\n        experiment_data[ablation][name] = {\n            \"metrics\": {\"train\": [], \"val\": []},  # accuracies\n            \"losses\": {\"train\": [], \"val\": []},  # ai loss\n            \"alignments\": {\"train\": [], \"val\": []},  # avg(1\u2010JSD)\n            \"mai\": [],  # mutual AI\u2010user alignment index\n            \"predictions\": None,\n            \"ground_truth\": None,\n        }\n        # load & tokenize\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tok(batch):\n            return tokenizer(\n                batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tok, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tok, batched=True, remove_columns=[text_key])\n        train_ds.set_format(\"torch\", [\"input_ids\", \"attention_mask\", \"label\"])\n        val_ds.set_format(\"torch\", [\"input_ids\", \"attention_mask\", \"label\"])\n        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        num_labels = len(set(train_ds[\"label\"]))\n        # models\n        distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(\n            device\n        )\n        if ablation == \"baseline\":\n            distilbert.eval()\n            for p in distilbert.parameters():\n                p.requires_grad = False\n        else:\n            distilbert.train()\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        # optimizers\n        lr = 1e-3\n        if ablation == \"baseline\":\n            optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=lr)\n        else:\n            optimizer_ai = torch.optim.Adam(\n                list(ai_model.parameters()) + list(distilbert.parameters()), lr=lr\n            )\n        optimizer_user = torch.optim.Adam(user_model.parameters(), lr=lr)\n        loss_fn = nn.CrossEntropyLoss()\n\n        # training\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_loss = tot_align = tot_acc = tot_n = 0.0\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                # encoder forward\n                if ablation == \"baseline\":\n                    with torch.no_grad():\n                        emb = distilbert(\n                            input_ids=batch[\"input_ids\"],\n                            attention_mask=batch[\"attention_mask\"],\n                        ).last_hidden_state[:, 0, :]\n                else:\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                # AI head\n                logits_ai = ai_model(emb)\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                optimizer_ai.zero_grad()\n                loss_ai.backward()\n                optimizer_ai.step()\n                # user head (detach so it doesn't update encoder)\n                emb_user = emb.detach()\n                logits_user = user_model(emb_user)\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                optimizer_user.zero_grad()\n                loss_user.backward()\n                optimizer_user.step()\n                # accumulate metrics\n                bs = batch[\"label\"].size(0)\n                tot_loss += loss_ai.item() * bs\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(logits_user, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_align += torch.sum(1 - jsd).item()\n                tot_acc += (\n                    (torch.argmax(logits_user, dim=1) == batch[\"label\"]).sum().item()\n                )\n                tot_n += bs\n            # record train\n            experiment_data[ablation][name][\"losses\"][\"train\"].append(\n                tot_loss / len(train_ds)\n            )\n            experiment_data[ablation][name][\"alignments\"][\"train\"].append(\n                tot_align / tot_n\n            )\n            experiment_data[ablation][name][\"metrics\"][\"train\"].append(tot_acc / tot_n)\n            # validation\n            ai_model.eval()\n            user_model.eval()\n            v_loss = v_align = v_acc = v_n = 0.0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                    # AI eval\n                    logits_ai = ai_model(emb)\n                    bs = batch[\"label\"].size(0)\n                    v_loss += loss_fn(logits_ai, batch[\"label\"]).item() * bs\n                    # alignment & user accuracy\n                    P = F.softmax(logits_ai, dim=1)\n                    qu = user_model(emb)\n                    Q = F.softmax(qu, dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_align += torch.sum(1 - jsd).item()\n                    v_acc += (torch.argmax(qu, dim=1) == batch[\"label\"]).sum().item()\n                    v_n += bs\n            val_loss = v_loss / len(val_ds)\n            val_align = v_align / v_n\n            val_acc = v_acc / v_n\n            mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n            experiment_data[ablation][name][\"losses\"][\"val\"].append(val_loss)\n            experiment_data[ablation][name][\"alignments\"][\"val\"].append(val_align)\n            experiment_data[ablation][name][\"metrics\"][\"val\"].append(val_acc)\n            experiment_data[ablation][name][\"mai\"].append(mai)\n            print(\n                f\"Ablation {ablation} Dataset {name} Epoch {epoch}: val_loss={val_loss:.4f}, MAI={mai:.4f}\"\n            )\n\n        # save final preds & gts\n        preds, gts = [], []\n        ai_model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                emb = distilbert(\n                    input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n                ).last_hidden_state[:, 0, :]\n                preds.append(torch.argmax(ai_model(emb), dim=1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        experiment_data[ablation][name][\"predictions\"] = np.concatenate(preds)\n        experiment_data[ablation][name][\"ground_truth\"] = np.concatenate(gts)\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# load tokenizer & model\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert.eval()\n\n\n# classifier definitions\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\n# ablations\nablation_types = [\"baseline\", \"linear_probe\"]\nexperiment_data = {}\n\nfor ablation in ablation_types:\n    experiment_data[ablation] = {}\n    for name in [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]:\n        # load & tokenize\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tokenize_fn(batch):\n            return tokenizer(\n                batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        train_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        val_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        num_labels = len(set(train_ds[\"label\"]))\n        in_dim = distilbert.config.hidden_size\n\n        # instantiate models\n        if ablation == \"baseline\":\n            ai_model = MLP(in_dim, 128, num_labels).to(device)\n            user_model = MLP(in_dim, 128, num_labels).to(device)\n        else:\n            ai_model = nn.Linear(in_dim, num_labels).to(device)\n            user_model = nn.Linear(in_dim, num_labels).to(device)\n\n        optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n        optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n        loss_fn = nn.CrossEntropyLoss()\n\n        # storage\n        experiment_data[ablation][name] = {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"alignments\": {\"train\": [], \"val\": []},\n            \"mai\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # training loop\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_loss = tot_align = tot_acc = n = 0\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                with torch.no_grad():\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                optimizer_ai.zero_grad()\n                loss_ai.backward()\n                optimizer_ai.step()\n                optimizer_user.zero_grad()\n                loss_user.backward()\n                optimizer_user.step()\n\n                bs = batch[\"label\"].size(0)\n                tot_loss += loss_ai.item() * bs\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(logits_user, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_align += torch.sum(1 - jsd).item()\n                tot_acc += (\n                    (torch.argmax(logits_user, dim=1) == batch[\"label\"]).sum().item()\n                )\n                n += bs\n\n            train_loss = tot_loss / len(train_ds)\n            train_align = tot_align / n\n            train_acc = tot_acc / n\n            ed = experiment_data[ablation][name]\n            ed[\"losses\"][\"train\"].append(train_loss)\n            ed[\"alignments\"][\"train\"].append(train_align)\n            ed[\"metrics\"][\"train\"].append(train_acc)\n\n            # validation\n            ai_model.eval()\n            user_model.eval()\n            v_loss = v_align = v_acc = v_n = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                    logits_ai = ai_model(emb)\n                    v_loss += loss_fn(logits_ai, batch[\"label\"]).item() * batch[\n                        \"label\"\n                    ].size(0)\n                    P = F.softmax(logits_ai, dim=1)\n                    Q = F.softmax(user_model(emb), dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_align += torch.sum(1 - jsd).item()\n                    v_acc += (\n                        (torch.argmax(user_model(emb), dim=1) == batch[\"label\"])\n                        .sum()\n                        .item()\n                    )\n                    v_n += batch[\"label\"].size(0)\n\n            val_loss = v_loss / len(val_ds)\n            val_align = v_align / v_n\n            val_acc = v_acc / v_n\n            mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n            ed[\"losses\"][\"val\"].append(val_loss)\n            ed[\"alignments\"][\"val\"].append(val_align)\n            ed[\"metrics\"][\"val\"].append(val_acc)\n            ed[\"mai\"].append(mai)\n            print(\n                f\"Ablation {ablation} Dataset {name} Epoch {epoch}: val_loss = {val_loss:.4f}, MAI = {mai:.4f}\"\n            )\n\n        # collect final predictions\n        preds, gts = [], []\n        ai_model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                emb = distilbert(\n                    input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n                ).last_hidden_state[:, 0, :]\n                preds.append(torch.argmax(ai_model(emb), dim=1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        ed[\"predictions\"] = np.concatenate(preds)\n        ed[\"ground_truth\"] = np.concatenate(gts)\n\n# convert lists to numpy arrays\nfor ablation in experiment_data:\n    for ds in experiment_data[ablation]:\n        d = experiment_data[ablation][ds]\n        for key in [\"metrics\", \"losses\", \"alignments\"]:\n            for split in [\"train\", \"val\"]:\n                d[key][split] = np.array(d[key][split])\n        d[\"mai\"] = np.array(d[\"mai\"])\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(42)\nnp.random.seed(42)\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert.eval()\n\n\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\n# pooling functions for ablation\npoolings = {\n    \"cls_pooling\": lambda hs: hs[:, 0, :],\n    \"mean_pooling\": lambda hs: hs.mean(dim=1),\n}\n\nexperiment_data = {p: {} for p in poolings}\n\nfor pooling_name, pool_fn in poolings.items():\n    for name in [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]:\n        # load and tokenize\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tokenize_fn(batch):\n            return tokenizer(\n                batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        train_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        val_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n\n        num_labels = len(set(train_ds[\"label\"]))\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        opt_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n        opt_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n        loss_fn = nn.CrossEntropyLoss()\n\n        # storage for this dataset+pooling\n        data = {\n            \"metrics\": {\"train\": [], \"val\": []},  # accuracy\n            \"losses\": {\"train\": [], \"val\": []},\n            \"alignments\": {\"train\": [], \"val\": []},\n            \"mai\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # training\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_loss = tot_acc = tot_align = n = 0\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                with torch.no_grad():\n                    hs = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state\n                    emb = pool_fn(hs)\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                # update AI\n                opt_ai.zero_grad()\n                loss_ai.backward()\n                opt_ai.step()\n                # update user\n                opt_user.zero_grad()\n                loss_user.backward()\n                opt_user.step()\n                bs = batch[\"label\"].size(0)\n                tot_loss += loss_ai.item() * bs\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(logits_user, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_align += torch.sum(1 - jsd).item()\n                tot_acc += (\n                    (torch.argmax(logits_user, dim=1) == batch[\"label\"]).sum().item()\n                )\n                n += bs\n            train_loss = tot_loss / len(train_ds)\n            train_acc = tot_acc / n\n            train_align = tot_align / n\n            data[\"losses\"][\"train\"].append(train_loss)\n            data[\"metrics\"][\"train\"].append(train_acc)\n            data[\"alignments\"][\"train\"].append(train_align)\n\n            # validation\n            ai_model.eval()\n            user_model.eval()\n            v_loss = v_acc = v_align = v_n = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    hs = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state\n                    emb = pool_fn(hs)\n                    logits_ai = ai_model(emb)\n                    v_loss += loss_fn(logits_ai, batch[\"label\"]).item() * batch[\n                        \"label\"\n                    ].size(0)\n                    P = F.softmax(logits_ai, dim=1)\n                    Q = F.softmax(user_model(emb), dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_align += torch.sum(1 - jsd).item()\n                    v_acc += (\n                        (torch.argmax(user_model(emb), dim=1) == batch[\"label\"])\n                        .sum()\n                        .item()\n                    )\n                    v_n += batch[\"label\"].size(0)\n            val_loss = v_loss / len(val_ds)\n            val_acc = v_acc / v_n\n            val_align = v_align / v_n\n            mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n            data[\"losses\"][\"val\"].append(val_loss)\n            data[\"metrics\"][\"val\"].append(val_acc)\n            data[\"alignments\"][\"val\"].append(val_align)\n            data[\"mai\"].append(mai)\n            print(\n                f\"{pooling_name} | {name} | Epoch {epoch} -> val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}, val_align: {val_align:.4f}, MAI: {mai:.4f}\"\n            )\n\n        # final predictions & gts\n        ai_model.eval()\n        preds, gts = [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                hs = distilbert(\n                    input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n                ).last_hidden_state\n                emb = pool_fn(hs)\n                preds.append(torch.argmax(ai_model(emb), dim=1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        data[\"predictions\"] = np.concatenate(preds)\n        data[\"ground_truth\"] = np.concatenate(gts)\n\n        experiment_data[pooling_name][name] = data\n\n# save results\nnp.save(\"experiment_data.npy\", experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n\n\n# simple 2-layer MLP\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\n# store results\nexperiment_data = {\"baseline\": {}, \"fine_tune\": {}}\n\nfor ablation in [\"baseline\", \"fine_tune\"]:\n    for name in [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]:\n        # init storage\n        experiment_data[ablation][name] = {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"alignments\": {\"train\": [], \"val\": []},\n            \"mai\": [],\n            \"predictions\": None,\n            \"ground_truth\": None,\n        }\n        # load & tokenize\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tok(batch):\n            return tokenizer(\n                batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tok, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tok, batched=True, remove_columns=[text_key])\n        train_ds.set_format(\"torch\", [\"input_ids\", \"attention_mask\", \"label\"])\n        val_ds.set_format(\"torch\", [\"input_ids\", \"attention_mask\", \"label\"])\n        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        num_labels = len(set(train_ds[\"label\"]))\n\n        # models\n        distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(\n            device\n        )\n        if ablation == \"baseline\":\n            distilbert.eval()\n            for p in distilbert.parameters():\n                p.requires_grad = False\n        else:\n            distilbert.train()\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n\n        # optimizers\n        if ablation == \"baseline\":\n            optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n        else:\n            optimizer_ai = torch.optim.Adam(\n                [\n                    {\"params\": distilbert.parameters(), \"lr\": 2e-5},\n                    {\"params\": ai_model.parameters(), \"lr\": 1e-3},\n                ]\n            )\n        optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n        loss_fn = nn.CrossEntropyLoss()\n\n        # training & validation\n        for epoch in range(1, 4):\n            # ---- train ----\n            if ablation == \"fine_tune\":\n                distilbert.train()\n            ai_model.train()\n            user_model.train()\n            tot_loss = tot_align = tot_acc = tot_n = 0.0\n\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                # encode\n                if ablation == \"baseline\":\n                    with torch.no_grad():\n                        emb = distilbert(\n                            input_ids=batch[\"input_ids\"],\n                            attention_mask=batch[\"attention_mask\"],\n                        ).last_hidden_state[:, 0, :]\n                else:\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                # AI head\n                logits_ai = ai_model(emb)\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                optimizer_ai.zero_grad()\n                loss_ai.backward()\n                optimizer_ai.step()\n                # user head\n                emb_user = emb.detach()\n                logits_user = user_model(emb_user)\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                optimizer_user.zero_grad()\n                loss_user.backward()\n                optimizer_user.step()\n                # metrics accumulate\n                bs = batch[\"label\"].size(0)\n                tot_loss += loss_ai.item() * bs\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(logits_user, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_align += torch.sum(1 - jsd).item()\n                tot_acc += (\n                    (torch.argmax(logits_user, dim=1) == batch[\"label\"]).sum().item()\n                )\n                tot_n += bs\n\n            # record train metrics\n            experiment_data[ablation][name][\"losses\"][\"train\"].append(\n                tot_loss / len(train_ds)\n            )\n            experiment_data[ablation][name][\"alignments\"][\"train\"].append(\n                tot_align / tot_n\n            )\n            experiment_data[ablation][name][\"metrics\"][\"train\"].append(tot_acc / tot_n)\n\n            # ---- validation ----\n            distilbert.eval()\n            ai_model.eval()\n            user_model.eval()\n            v_loss = v_align = v_acc = v_n = 0.0\n\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                    logits_ai = ai_model(emb)\n                    bs = batch[\"label\"].size(0)\n                    v_loss += loss_fn(logits_ai, batch[\"label\"]).item() * bs\n                    P = F.softmax(logits_ai, dim=1)\n                    qu = user_model(emb)\n                    Q = F.softmax(qu, dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_align += torch.sum(1 - jsd).item()\n                    v_acc += (torch.argmax(qu, dim=1) == batch[\"label\"]).sum().item()\n                    v_n += bs\n\n            val_loss = v_loss / len(val_ds)\n            val_align = v_align / v_n\n            val_acc = v_acc / v_n\n            experiment_data[ablation][name][\"losses\"][\"val\"].append(val_loss)\n            experiment_data[ablation][name][\"alignments\"][\"val\"].append(val_align)\n            experiment_data[ablation][name][\"metrics\"][\"val\"].append(val_acc)\n            mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n            experiment_data[ablation][name][\"mai\"].append(mai)\n            print(\n                f\"Ablation {ablation} Dataset {name} Epoch {epoch}: validation_loss = {val_loss:.4f}, Bidirectional Alignment = {val_align:.4f}, MAI = {mai:.4f}\"\n            )\n\n        # final predictions & ground truth\n        preds, gts = [], []\n        distilbert.eval()\n        ai_model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                emb = distilbert(\n                    input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n                ).last_hidden_state[:, 0, :]\n                preds.append(torch.argmax(ai_model(emb), dim=1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        experiment_data[ablation][name][\"predictions\"] = np.concatenate(preds)\n        experiment_data[ablation][name][\"ground_truth\"] = np.concatenate(gts)\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# Setup working dir, device, seeds\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Load tokenizer and model\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert.eval()\n\n\n# Simple 2-layer MLP\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\n# Define ablation methods for different representation depths\nablations = {\n    \"layer_1\": lambda hs: hs[1][:, 0, :],\n    \"layer_3\": lambda hs: hs[3][:, 0, :],\n    \"layer_5\": lambda hs: hs[5][:, 0, :],\n    \"avg_last2\": lambda hs: (hs[-1][:, 0, :] + hs[-2][:, 0, :]) / 2.0,\n}\n\n# Dataset names\ndataset_names = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\n\n# Pre-tokenize and create DataLoaders\ndata_loaders = {}\nfor name in dataset_names:\n    raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n    split = raw.train_test_split(test_size=0.2, seed=0)\n    train_ds, val_ds = split[\"train\"], split[\"test\"]\n    text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n    def tokenize_fn(batch):\n        return tokenizer(\n            batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n        )\n\n    train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n    val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n    val_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=32)\n    num_labels = len(set(train_ds[\"label\"]))\n    data_loaders[name] = (train_ds, val_ds, train_loader, val_loader, num_labels)\n\n# Loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Main ablation experiment\nexperiment_data = {}\nfor ablation_name, ablate in ablations.items():\n    experiment_data[ablation_name] = {}\n    for dname in dataset_names:\n        train_ds, val_ds, train_loader, val_loader, num_labels = data_loaders[dname]\n        results = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"alignments\": {\"train\": [], \"val\": []},\n            \"mai\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        # Instantiate fresh models & optimizers\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        opt_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n        opt_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n\n        # Train for 3 epochs\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_loss = tot_align = tot_acc = 0.0\n            n = 0\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                with torch.no_grad():\n                    out = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                        output_hidden_states=True,\n                    )\n                    hs = out.hidden_states\n                    emb = ablate(hs)\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                opt_ai.zero_grad()\n                loss_ai.backward()\n                opt_ai.step()\n                opt_user.zero_grad()\n                loss_user.backward()\n                opt_user.step()\n                bs = batch[\"label\"].size(0)\n                tot_loss += loss_ai.item() * bs\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(logits_user, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_align += torch.sum(1 - jsd).item()\n                tot_acc += (torch.argmax(logits_user, 1) == batch[\"label\"]).sum().item()\n                n += bs\n            results[\"losses\"][\"train\"].append(tot_loss / len(train_ds))\n            results[\"alignments\"][\"train\"].append(tot_align / n)\n\n            # Validation\n            ai_model.eval()\n            user_model.eval()\n            v_loss = v_align = v_acc = 0.0\n            v_n = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    out = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                        output_hidden_states=True,\n                    )\n                    hs = out.hidden_states\n                    emb = ablate(hs)\n                    logits_ai = ai_model(emb)\n                    v_loss += loss_fn(logits_ai, batch[\"label\"]).item() * batch[\n                        \"label\"\n                    ].size(0)\n                    P = F.softmax(logits_ai, dim=1)\n                    Q = F.softmax(user_model(emb), dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_align += torch.sum(1 - jsd).item()\n                    v_acc += (\n                        (torch.argmax(user_model(emb), 1) == batch[\"label\"])\n                        .sum()\n                        .item()\n                    )\n                    v_n += batch[\"label\"].size(0)\n            val_loss = v_loss / len(val_ds)\n            val_align = v_align / v_n\n            val_acc = v_acc / v_n\n            mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n            results[\"losses\"][\"val\"].append(val_loss)\n            results[\"alignments\"][\"val\"].append(val_align)\n            results[\"mai\"].append(mai)\n            print(\n                f\"{ablation_name} | {dname} | Epoch {epoch} | val_loss={val_loss:.4f} MAI={mai:.4f}\"\n            )\n\n        # Final predictions on validation set\n        ai_model.eval()\n        preds, gts = [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                out = distilbert(\n                    input_ids=batch[\"input_ids\"],\n                    attention_mask=batch[\"attention_mask\"],\n                    output_hidden_states=True,\n                )\n                hs = out.hidden_states\n                emb = ablate(hs)\n                preds.append(torch.argmax(ai_model(emb), 1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        results[\"predictions\"] = np.concatenate(preds)\n        results[\"ground_truth\"] = np.concatenate(gts)\n        experiment_data[ablation_name][dname] = results\n\n# Save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# Setup environment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(42)\nnp.random.seed(42)\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load tokenizer and frozen DistilBERT\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert.eval()\n\n\n# MLP head with configurable dropout\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim, drop_p):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.dropout = nn.Dropout(drop_p)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)\n\n\n# Ablation settings\ndropout_probs = [0.0, 0.1, 0.3, 0.5]\ndatasets_list = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\n\n# Container for all results\nexperiment_data = {\"mlp_dropout_rate_ablation\": {}}\n\n# Main loop over dropout rates and datasets\nfor p in dropout_probs:\n    key = f\"drop_{p}\"\n    experiment_data[\"mlp_dropout_rate_ablation\"][key] = {}\n    for name in datasets_list:\n        # Prepare and tokenize\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tokenize_fn(batch):\n            return tokenizer(\n                batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        train_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        val_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n\n        # Initialize models, optimizers, loss\n        num_labels = len(set(train_ds[\"label\"]))\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels, p).to(device)\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels, p).to(device)\n        optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n        optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n        loss_fn = nn.CrossEntropyLoss()\n\n        # Storage for this configuration\n        store = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"alignments\": {\"train\": [], \"val\": []},\n            \"mai\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        experiment_data[\"mlp_dropout_rate_ablation\"][key][name] = store\n\n        # Training loop\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_loss, tot_align, n = 0.0, 0.0, 0\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                with torch.no_grad():\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                optimizer_ai.zero_grad()\n                loss_ai.backward()\n                optimizer_ai.step()\n                optimizer_user.zero_grad()\n                loss_user.backward()\n                optimizer_user.step()\n                bs = batch[\"label\"].size(0)\n                tot_loss += loss_ai.item() * bs\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(logits_user, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_align += torch.sum(1 - jsd).item()\n                n += bs\n            train_loss = tot_loss / len(train_ds)\n            train_align = tot_align / n\n            store[\"losses\"][\"train\"].append(train_loss)\n            store[\"alignments\"][\"train\"].append(train_align)\n\n            # Validation\n            ai_model.eval()\n            user_model.eval()\n            v_loss, v_align, v_acc, v_n = 0.0, 0.0, 0, 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                    logits_ai = ai_model(emb)\n                    logits_user = user_model(emb)\n                    v_loss += loss_fn(logits_ai, batch[\"label\"]).item() * batch[\n                        \"label\"\n                    ].size(0)\n                    P = F.softmax(logits_ai, dim=1)\n                    Q = F.softmax(logits_user, dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_align += torch.sum(1 - jsd).item()\n                    v_acc += (\n                        (torch.argmax(logits_user, dim=1) == batch[\"label\"])\n                        .sum()\n                        .item()\n                    )\n                    v_n += batch[\"label\"].size(0)\n            val_loss = v_loss / len(val_ds)\n            val_align = v_align / v_n\n            val_acc = v_acc / v_n\n            mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n            store[\"losses\"][\"val\"].append(val_loss)\n            store[\"alignments\"][\"val\"].append(val_align)\n            store[\"mai\"].append(mai)\n            print(\n                f\"Dropout {p}, Dataset {name}, Epoch {epoch}: val_loss={val_loss:.4f}, MAI={mai:.4f}\"\n            )\n\n        # Final predictions and ground truth\n        preds, gts = [], []\n        ai_model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                emb = distilbert(\n                    input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n                ).last_hidden_state[:, 0, :]\n                preds.append(torch.argmax(ai_model(emb), dim=1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        store[\"predictions\"] = np.concatenate(preds)\n        store[\"ground_truth\"] = np.concatenate(gts)\n\n# Save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# backbone\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert.eval()\n\n\n# MLP with pluggable activation\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim, act):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n        self.act = act\n\n    def forward(self, x):\n        return self.fc2(self.act(self.fc1(x)))\n\n\n# activations to ablate\nact_map = {\"ReLU\": nn.ReLU, \"GELU\": nn.GELU, \"Tanh\": nn.Tanh, \"LeakyReLU\": nn.LeakyReLU}\ndatasets = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\nexperiment_data = {act: {} for act in act_map}\n\nfor act_name, act_cls in act_map.items():\n    for name in datasets:\n        # load & preprocess\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tokenize_fn(batch):\n            return tokenizer(\n                batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        train_ds.set_format(\"torch\", [\"input_ids\", \"attention_mask\", \"label\"])\n        val_ds.set_format(\"torch\", [\"input_ids\", \"attention_mask\", \"label\"])\n        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        num_labels = len(set(train_ds[\"label\"]))\n        # models, optimizers\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels, act_cls()).to(\n            device\n        )\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels, act_cls()).to(\n            device\n        )\n        opt_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n        opt_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n        loss_fn = nn.CrossEntropyLoss()\n        # storage\n        losses = {\"train\": [], \"val\": []}\n        aligns = {\"train\": [], \"val\": []}\n        mai_list = []\n        # training loop\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_loss = tot_align = tot_n = 0\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                with torch.no_grad():\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                opt_ai.zero_grad()\n                loss_ai.backward()\n                opt_ai.step()\n                opt_user.zero_grad()\n                loss_user.backward()\n                opt_user.step()\n                bs = batch[\"label\"].size(0)\n                tot_loss += loss_ai.item() * bs\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(logits_user, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_align += torch.sum(1 - jsd).item()\n                tot_n += bs\n            losses[\"train\"].append(tot_loss / len(train_ds))\n            aligns[\"train\"].append(tot_align / tot_n)\n            # validation\n            ai_model.eval()\n            user_model.eval()\n            v_loss = v_align = v_acc = v_n = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                    logits_ai = ai_model(emb)\n                    v_loss += loss_fn(logits_ai, batch[\"label\"]).item() * batch[\n                        \"label\"\n                    ].size(0)\n                    P = F.softmax(logits_ai, dim=1)\n                    Q = F.softmax(user_model(emb), dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_align += torch.sum(1 - jsd).item()\n                    v_acc += (\n                        (torch.argmax(user_model(emb), dim=1) == batch[\"label\"])\n                        .sum()\n                        .item()\n                    )\n                    v_n += batch[\"label\"].size(0)\n            val_loss = v_loss / len(val_ds)\n            val_align = v_align / v_n\n            val_acc = v_acc / v_n\n            mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n            losses[\"val\"].append(val_loss)\n            aligns[\"val\"].append(val_align)\n            mai_list.append(mai)\n            print(\n                f\"Activation {act_name} Dataset {name} Epoch {epoch}: validation_loss = {val_loss:.4f}, MAI = {mai:.4f}\"\n            )\n        # collect predictions\n        preds, gts = [], []\n        ai_model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                emb = distilbert(\n                    input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n                ).last_hidden_state[:, 0, :]\n                preds.append(torch.argmax(ai_model(emb), dim=1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        experiment_data[act_name][name] = {\n            \"losses\": losses,\n            \"alignments\": aligns,\n            \"mai\": mai_list,\n            \"predictions\": np.concatenate(preds),\n            \"ground_truth\": np.concatenate(gts),\n        }\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, random\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# set up\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# global seeds\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# tokenizer + frozen encoder\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert.eval()\n\n\n# simple two\u2010layer MLP\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\n# hyperparameters\ndatasets = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\nlambdas = [0.0, 0.1, 0.5, 1.0]\nnum_epochs = 3\nbatch_size = 32\nlr = 1e-3\nloss_fn = nn.CrossEntropyLoss()\n\n# container for everything\nexperiment_data = {}\n\nfor lam in lambdas:\n    # reseed so each lambda run starts from identical MLP init\n    torch.manual_seed(42)\n    np.random.seed(42)\n    random.seed(42)\n    lam_key = f\"lambda_{str(lam).replace('.', '_')}\"\n    experiment_data[lam_key] = {}\n    print(f\"\\n=== Starting ablation {lam_key} ===\")\n\n    for name in datasets:\n        print(f\"\\nDataset {name}, \u03bb={lam}\")\n        # prepare storage\n        experiment_data[lam_key][name] = {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"alignments\": {\"train\": [], \"val\": []},\n            \"mai\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # load and tokenize\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tokenize_fn(batch):\n            return tokenizer(\n                batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        train_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        val_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=batch_size)\n\n        # models + optimizers\n        num_labels = len(set(train_ds[\"label\"]))\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=lr)\n        optimizer_user = torch.optim.Adam(user_model.parameters(), lr=lr)\n\n        # training epochs\n        for epoch in range(1, num_epochs + 1):\n            ai_model.train()\n            user_model.train()\n            tot_loss, tot_align, tot_acc, n = 0.0, 0.0, 0, 0\n\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                # embed\n                with torch.no_grad():\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                # forward\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                # CE losses\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                # JSD penalty\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(logits_user, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                loss_reg = jsd.mean()\n\n                # joint backward + step\n                optimizer_ai.zero_grad()\n                optimizer_user.zero_grad()\n                total_loss = loss_ai + loss_user + lam * loss_reg\n                total_loss.backward()\n                optimizer_ai.step()\n                optimizer_user.step()\n\n                bs = batch[\"label\"].size(0)\n                tot_loss += loss_ai.item() * bs\n                tot_align += torch.sum(1 - jsd).item()\n                tot_acc += (\n                    (torch.argmax(logits_user, dim=1) == batch[\"label\"]).sum().item()\n                )\n                n += bs\n\n            # record train stats\n            experiment_data[lam_key][name][\"losses\"][\"train\"].append(\n                tot_loss / len(train_ds)\n            )\n            experiment_data[lam_key][name][\"alignments\"][\"train\"].append(tot_align / n)\n            experiment_data[lam_key][name][\"metrics\"][\"train\"].append(tot_acc / n)\n\n            # validation\n            ai_model.eval()\n            user_model.eval()\n            v_loss, v_align, v_acc, v_n = 0.0, 0.0, 0, 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                    logits_ai = ai_model(emb)\n                    v_loss += loss_fn(logits_ai, batch[\"label\"]).item() * batch[\n                        \"label\"\n                    ].size(0)\n                    P = F.softmax(logits_ai, dim=1)\n                    Q = F.softmax(user_model(emb), dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_align += torch.sum(1 - jsd).item()\n                    v_acc += (\n                        (torch.argmax(user_model(emb), dim=1) == batch[\"label\"])\n                        .sum()\n                        .item()\n                    )\n                    v_n += batch[\"label\"].size(0)\n            val_loss = v_loss / len(val_ds)\n            val_align = v_align / v_n\n            val_acc = v_acc / v_n\n            mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n\n            # record val stats\n            experiment_data[lam_key][name][\"losses\"][\"val\"].append(val_loss)\n            experiment_data[lam_key][name][\"alignments\"][\"val\"].append(val_align)\n            experiment_data[lam_key][name][\"metrics\"][\"val\"].append(val_acc)\n            experiment_data[lam_key][name][\"mai\"].append(mai)\n\n            print(\n                f\"  Ep{epoch}: train_loss={tot_loss/len(train_ds):.4f}, \"\n                f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, val_align={val_align:.4f}, MAI={mai:.4f}\"\n            )\n\n        # final predictions by ai_model\n        preds, gts = [], []\n        ai_model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                emb = distilbert(\n                    input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n                ).last_hidden_state[:, 0, :]\n                preds.append(torch.argmax(ai_model(emb), dim=1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        experiment_data[lam_key][name][\"predictions\"] = np.concatenate(preds)\n        experiment_data[lam_key][name][\"ground_truth\"] = np.concatenate(gts)\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment_data.npy\")\n", "import os\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# Setup working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Seeds\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Tokenizer and base model\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert.eval()\n\n\n# Simple 2-layer MLP\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\n# Ablation hooks\nablations = {\n    \"layer_1\": lambda hs: hs[1][:, 0, :],\n    \"layer_3\": lambda hs: hs[3][:, 0, :],\n    \"layer_5\": lambda hs: hs[5][:, 0, :],\n    \"avg_last2\": lambda hs: (hs[-1][:, 0, :] + hs[-2][:, 0, :]) / 2.0,\n}\n\n# Datasets\ndataset_names = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\ndata_loaders = {}\nfor name in dataset_names:\n    raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n    split = raw.train_test_split(test_size=0.2, seed=0)\n    train_ds, val_ds = split[\"train\"], split[\"test\"]\n    text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n    def tokenize_fn(batch):\n        return tokenizer(\n            batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n        )\n\n    train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n    val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n    val_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=32)\n    num_labels = len(set(train_ds[\"label\"]))\n    data_loaders[name] = (train_ds, val_ds, train_loader, val_loader, num_labels)\n\n# Loss\nloss_fn = nn.CrossEntropyLoss()\nln2 = math.log(2.0)\n\n# Main experiment\nexperiment_data = {}\nfor ablation_name, ablate in ablations.items():\n    experiment_data[ablation_name] = {}\n    for dname in dataset_names:\n        train_ds, val_ds, train_loader, val_loader, num_labels = data_loaders[dname]\n        # Prepare result containers\n        results = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        # Instantiate models & optimizers\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        opt_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n        opt_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n\n        # Train epochs\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_loss = 0.0\n            tot_align1 = 0.0\n            tot_align2 = 0.0\n            n = 0\n            for batch in train_loader:\n                # move to device\n                batch = {\n                    k: v.to(device)\n                    for k, v in batch.items()\n                    if isinstance(v, torch.Tensor)\n                }\n                with torch.no_grad():\n                    out = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                        output_hidden_states=True,\n                    )\n                    hs = out.hidden_states\n                    emb = ablate(hs)\n                # Forward\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                # Loss and backward\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                opt_ai.zero_grad()\n                loss_ai.backward()\n                opt_ai.step()\n                opt_user.zero_grad()\n                loss_user.backward()\n                opt_user.step()\n                # Metrics\n                bs = batch[\"label\"].size(0)\n                tot_loss += loss_ai.item() * bs\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(logits_user, dim=1)\n                # measure1: AI vs ground-truth one-hot\n                onehot = F.one_hot(batch[\"label\"], num_labels).float()\n                M1 = 0.5 * (P + onehot)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M1 + 1e-8)), dim=1)\n                kl2 = torch.sum(\n                    onehot * (torch.log(onehot + 1e-8) - torch.log(M1 + 1e-8)), dim=1\n                )\n                jsd1 = 0.5 * (kl1 + kl2)\n                sim1 = 1 - (jsd1 / ln2)\n                # measure2: user vs AI\n                M2 = 0.5 * (Q + P)\n                kl1b = torch.sum(\n                    Q * (torch.log(Q + 1e-8) - torch.log(M2 + 1e-8)), dim=1\n                )\n                kl2b = torch.sum(\n                    P * (torch.log(P + 1e-8) - torch.log(M2 + 1e-8)), dim=1\n                )\n                jsd2 = 0.5 * (kl1b + kl2b)\n                sim2 = 1 - (jsd2 / ln2)\n                tot_align1 += sim1.sum().item()\n                tot_align2 += sim2.sum().item()\n                n += bs\n            # Record train stats\n            avg_loss = tot_loss / len(train_ds)\n            avg_sim1 = tot_align1 / n\n            avg_sim2 = tot_align2 / n\n            bmsa_train = 0.5 * (avg_sim1 + avg_sim2)\n            results[\"losses\"][\"train\"].append(avg_loss)\n            results[\"metrics\"][\"train\"].append(bmsa_train)\n\n            # Validation\n            ai_model.eval()\n            user_model.eval()\n            v_loss = 0.0\n            v_a1 = 0.0\n            v_a2 = 0.0\n            v_n = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {\n                        k: v.to(device)\n                        for k, v in batch.items()\n                        if isinstance(v, torch.Tensor)\n                    }\n                    out = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                        output_hidden_states=True,\n                    )\n                    hs = out.hidden_states\n                    emb = ablate(hs)\n                    logits_ai = ai_model(emb)\n                    v_loss += loss_fn(logits_ai, batch[\"label\"]).item() * batch[\n                        \"label\"\n                    ].size(0)\n                    P = F.softmax(logits_ai, dim=1)\n                    Q = F.softmax(user_model(emb), dim=1)\n                    onehot = F.one_hot(batch[\"label\"], num_labels).float()\n                    M1 = 0.5 * (P + onehot)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M1 + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        onehot * (torch.log(onehot + 1e-8) - torch.log(M1 + 1e-8)),\n                        dim=1,\n                    )\n                    sim1 = 1 - (0.5 * (kl1 + kl2) / ln2)\n                    M2 = 0.5 * (Q + P)\n                    kl1b = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M2 + 1e-8)), dim=1\n                    )\n                    kl2b = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M2 + 1e-8)), dim=1\n                    )\n                    sim2 = 1 - (0.5 * (kl1b + kl2b) / ln2)\n                    v_a1 += sim1.sum().item()\n                    v_a2 += sim2.sum().item()\n                    v_n += batch[\"label\"].size(0)\n            val_loss = v_loss / len(val_ds)\n            avg_sim1_v = v_a1 / v_n\n            avg_sim2_v = v_a2 / v_n\n            bmsa_val = 0.5 * (avg_sim1_v + avg_sim2_v)\n            results[\"losses\"][\"val\"].append(val_loss)\n            results[\"metrics\"][\"val\"].append(bmsa_val)\n            print(\n                f\"{ablation_name} | {dname} | Epoch {epoch}: validation_loss = {val_loss:.4f}, Bidirectional Alignment = {bmsa_val:.4f}\"\n            )\n\n        # Final predictions\n        ai_model.eval()\n        preds, gts = [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {\n                    k: v.to(device)\n                    for k, v in batch.items()\n                    if isinstance(v, torch.Tensor)\n                }\n                out = distilbert(\n                    input_ids=batch[\"input_ids\"],\n                    attention_mask=batch[\"attention_mask\"],\n                    output_hidden_states=True,\n                )\n                emb = ablate(out.hidden_states)\n                preds.append(torch.argmax(ai_model(emb), dim=1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        results[\"predictions\"] = np.concatenate(preds)\n        results[\"ground_truth\"] = np.concatenate(gts)\n        experiment_data[ablation_name][dname] = results\n\n# Save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# Setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(42)\nnp.random.seed(42)\nprint(f\"Using device: {device}\")\n\n# Tokenizer & frozen encoder\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert.eval()\n\n\n# Simple MLP head\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\n# Focal Loss\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2, reduction=\"mean\", eps=1e-8):\n        super().__init__()\n        self.gamma = gamma\n        self.reduction = reduction\n        self.eps = eps\n\n    def forward(self, inputs, targets):\n        probs = F.softmax(inputs, dim=1).clamp(min=self.eps)\n        t_one = F.one_hot(targets, inputs.size(1)).float().to(inputs.device)\n        p_t = (probs * t_one).sum(1)\n        loss = -((1 - p_t) ** self.gamma) * torch.log(p_t)\n        if self.reduction == \"mean\":\n            return loss.mean()\n        if self.reduction == \"sum\":\n            return loss.sum()\n        return loss\n\n\n# Ablation settings & data structures\ngammas = [1, 2, 5]\ndatasets = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\nexperiment_data = {}\n\nfor gamma in gammas:\n    ab_key = f\"focal_loss_gamma_{gamma}\"\n    experiment_data[ab_key] = {}\n    loss_fn = FocalLoss(gamma=gamma)\n\n    for name in datasets:\n        # Load & tokenize\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tok(batch):\n            return tokenizer(\n                batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tok, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tok, batched=True, remove_columns=[text_key])\n        cols = [\"input_ids\", \"attention_mask\", \"label\"]\n        train_ds.set_format(type=\"torch\", columns=cols)\n        val_ds.set_format(type=\"torch\", columns=cols)\n        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        num_labels = len(set(train_ds[\"label\"]))\n\n        # Models & optimizers\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        opt_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n        opt_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n\n        # Storage per dataset\n        experiment_data[ab_key][name] = {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"alignments\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # Training loop\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_loss = tot_align = tot_acc = n = 0\n            for b in train_loader:\n                b = {k: v.to(device) for k, v in b.items()}\n                with torch.no_grad():\n                    emb = distilbert(\n                        input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"]\n                    ).last_hidden_state[:, 0, :]\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                loss_ai = loss_fn(logits_ai, b[\"label\"])\n                # AI update\n                opt_ai.zero_grad()\n                loss_ai.backward()\n                opt_ai.step()\n                # User update\n                loss_user = loss_fn(logits_user, b[\"label\"])\n                opt_user.zero_grad()\n                loss_user.backward()\n                opt_user.step()\n\n                bs = b[\"label\"].size(0)\n                tot_loss += loss_ai.item() * bs\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(logits_user, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = (P * (torch.log(P + 1e-8) - torch.log(M + 1e-8))).sum(1)\n                kl2 = (Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8))).sum(1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_align += (1 - jsd).sum().item()\n                tot_acc += (torch.argmax(logits_user, 1) == b[\"label\"]).sum().item()\n                n += bs\n\n            # Compute train stats\n            train_loss = tot_loss / len(train_ds)\n            train_align = tot_align / n\n            train_acc = tot_acc / n\n            train_mai = 2 * (train_align * train_acc) / (train_align + train_acc + 1e-8)\n            experiment_data[ab_key][name][\"losses\"][\"train\"].append(train_loss)\n            experiment_data[ab_key][name][\"alignments\"][\"train\"].append(train_align)\n            experiment_data[ab_key][name][\"metrics\"][\"train\"].append(train_mai)\n\n            # Validation\n            ai_model.eval()\n            user_model.eval()\n            v_loss = v_align = v_acc = v_n = 0\n            with torch.no_grad():\n                for b in val_loader:\n                    b = {k: v.to(device) for k, v in b.items()}\n                    emb = distilbert(\n                        input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"]\n                    ).last_hidden_state[:, 0, :]\n                    logits_ai = ai_model(emb)\n                    v_loss += loss_fn(logits_ai, b[\"label\"]).item() * b[\"label\"].size(0)\n                    P = F.softmax(logits_ai, 1)\n                    Q = F.softmax(user_model(emb), 1)\n                    M = 0.5 * (P + Q)\n                    kl1 = (P * (torch.log(P + 1e-8) - torch.log(M + 1e-8))).sum(1)\n                    kl2 = (Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8))).sum(1)\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_align += (1 - jsd).sum().item()\n                    v_acc += (\n                        (torch.argmax(user_model(emb), 1) == b[\"label\"]).sum().item()\n                    )\n                    v_n += b[\"label\"].size(0)\n\n            val_loss = v_loss / len(val_ds)\n            val_align = v_align / v_n\n            val_acc = v_acc / v_n\n            val_mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n            experiment_data[ab_key][name][\"losses\"][\"val\"].append(val_loss)\n            experiment_data[ab_key][name][\"alignments\"][\"val\"].append(val_align)\n            experiment_data[ab_key][name][\"metrics\"][\"val\"].append(val_mai)\n\n            print(\n                f\"Ablation {ab_key} Dataset {name} Epoch {epoch}: val_loss={val_loss:.4f}, MAI={val_mai:.4f}\"\n            )\n\n        # Final predictions on val\n        preds, gts = [], []\n        ai_model.eval()\n        with torch.no_grad():\n            for b in val_loader:\n                b = {k: v.to(device) for k, v in b.items()}\n                emb = distilbert(\n                    input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"]\n                ).last_hidden_state[:, 0, :]\n                preds.append(torch.argmax(ai_model(emb), 1).cpu().numpy())\n                gts.append(b[\"label\"].cpu().numpy())\n        experiment_data[ab_key][name][\"predictions\"] = np.concatenate(preds)\n        experiment_data[ab_key][name][\"ground_truth\"] = np.concatenate(gts)\n\n# Save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# load model & tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = (\n    DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device).eval()\n)\n\n# compute head importance via L2 norm of output\u2010projection weights\nn_layers = len(distilbert.transformer.layer)\nn_heads = distilbert.transformer.layer[0].attention.n_heads\nhead_dim = distilbert.config.hidden_size // n_heads\nhead_importance = {}\nfor l in range(n_layers):\n    W = distilbert.transformer.layer[l].attention.out_lin.weight.data.cpu()\n    imp = []\n    for h in range(n_heads):\n        block = W[:, h * head_dim : (h + 1) * head_dim]\n        imp.append(torch.norm(block).item())\n    head_importance[l] = np.array(imp)\n\n# prepare head masks\nablation_types = [\"random\", \"importance\"]\nhead_counts = [12, 8, 4, 2]\nhead_masks = {t: {} for t in ablation_types}\nfor t in ablation_types:\n    for hc in head_counts:\n        mask = torch.zeros(n_layers, n_heads)\n        for l in range(n_layers):\n            if t == \"random\":\n                keep = np.random.choice(n_heads, hc, replace=False)\n            else:  # importance\n                keep = np.argsort(-head_importance[l])[:hc]\n            mask[l, keep] = 1.0\n        head_masks[t][hc] = mask.to(device)\n\n# load datasets once\ndataset_names = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\ntrain_loaders, val_loaders, num_labels = {}, {}, {}\nfor name in dataset_names:\n    raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n    split = raw.train_test_split(test_size=0.2, seed=0)\n    train_ds, val_ds = split[\"train\"], split[\"test\"]\n    text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n    def tokenize_fn(batch):\n        return tokenizer(\n            batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n        )\n\n    train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n    val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n    val_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n    train_loaders[name] = DataLoader(train_ds, batch_size=32, shuffle=True)\n    val_loaders[name] = DataLoader(val_ds, batch_size=32)\n    num_labels[name] = len(set(train_ds[\"label\"]))\n\n\n# simple MLP\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\n# init data structure\nexperiment_data = {\n    t: {\n        name: {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"alignments\": {\"train\": [], \"val\": []},\n            \"mai\": [],\n            \"head_counts\": [],\n            \"predictions\": {},\n            \"ground_truth\": None,\n        }\n        for name in dataset_names\n    }\n    for t in ablation_types\n}\n\n# run ablations\nfor t in ablation_types:\n    for hc in head_counts:\n        mask = head_masks[t][hc]\n        for name in dataset_names:\n            train_loader = train_loaders[name]\n            val_loader = val_loaders[name]\n            nl = num_labels[name]\n            ai_model = MLP(distilbert.config.hidden_size, 128, nl).to(device)\n            user_model = MLP(distilbert.config.hidden_size, 128, nl).to(device)\n            opt_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n            opt_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n            loss_fn = nn.CrossEntropyLoss()\n\n            # epochs\n            for epoch in range(1, 4):\n                ai_model.train()\n                user_model.train()\n                tot_loss, tot_align, tot_acc, n = 0.0, 0.0, 0, 0\n                for batch in train_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    with torch.no_grad():\n                        out = distilbert(\n                            input_ids=batch[\"input_ids\"],\n                            attention_mask=batch[\"attention_mask\"],\n                            head_mask=mask,\n                        )\n                        emb = out.last_hidden_state[:, 0, :]\n                    logits_ai = ai_model(emb)\n                    logits_user = user_model(emb)\n                    loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                    loss_user = loss_fn(logits_user, batch[\"label\"])\n                    opt_ai.zero_grad()\n                    loss_ai.backward()\n                    opt_ai.step()\n                    opt_user.zero_grad()\n                    loss_user.backward()\n                    opt_user.step()\n                    bs = batch[\"label\"].size(0)\n                    tot_loss += loss_ai.item() * bs\n                    P = F.softmax(logits_ai, dim=1)\n                    Q = F.softmax(logits_user, dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    tot_align += torch.sum(1 - jsd).item()\n                    tot_acc += (\n                        (torch.argmax(logits_user, dim=1) == batch[\"label\"])\n                        .sum()\n                        .item()\n                    )\n                    n += bs\n                train_loss = tot_loss / len(train_loader.dataset)\n                train_align = tot_align / n\n                experiment_data[t][name][\"losses\"][\"train\"].append(train_loss)\n                experiment_data[t][name][\"alignments\"][\"train\"].append(train_align)\n\n                # validation\n                ai_model.eval()\n                user_model.eval()\n                v_loss, v_align, v_acc, v_n = 0.0, 0.0, 0, 0\n                with torch.no_grad():\n                    for batch in val_loader:\n                        batch = {k: v.to(device) for k, v in batch.items()}\n                        out = distilbert(\n                            input_ids=batch[\"input_ids\"],\n                            attention_mask=batch[\"attention_mask\"],\n                            head_mask=mask,\n                        )\n                        emb = out.last_hidden_state[:, 0, :]\n                        logits_ai = ai_model(emb)\n                        v_loss += loss_fn(logits_ai, batch[\"label\"]).item() * batch[\n                            \"label\"\n                        ].size(0)\n                        P = F.softmax(logits_ai, dim=1)\n                        Q = F.softmax(user_model(emb), dim=1)\n                        M = 0.5 * (P + Q)\n                        kl1 = torch.sum(\n                            P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                        )\n                        kl2 = torch.sum(\n                            Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                        )\n                        jsd = 0.5 * (kl1 + kl2)\n                        v_align += torch.sum(1 - jsd).item()\n                        v_acc += (\n                            (torch.argmax(user_model(emb), dim=1) == batch[\"label\"])\n                            .sum()\n                            .item()\n                        )\n                        v_n += batch[\"label\"].size(0)\n                val_loss = v_loss / len(val_loader.dataset)\n                val_align = v_align / v_n\n                val_acc = v_acc / v_n\n                mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n                experiment_data[t][name][\"losses\"][\"val\"].append(val_loss)\n                experiment_data[t][name][\"alignments\"][\"val\"].append(val_align)\n                experiment_data[t][name][\"mai\"].append(mai)\n                experiment_data[t][name][\"head_counts\"].append(hc)\n                print(\n                    f\"Ablation {t} heads={hc} ds={name} epoch={epoch}: val_loss={val_loss:.4f}, MAI={mai:.4f}\"\n                )\n\n            # collect predictions & ground truth\n            preds, gts = [], []\n            ai_model.eval()\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    out = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                        head_mask=mask,\n                    )\n                    emb = out.last_hidden_state[:, 0, :]\n                    preds.append(torch.argmax(ai_model(emb), dim=1).cpu().numpy())\n                    gts.append(batch[\"label\"].cpu().numpy())\n            preds = np.concatenate(preds)\n            gts = np.concatenate(gts)\n            experiment_data[t][name][\"predictions\"][str(hc)] = preds\n            experiment_data[t][name][\"ground_truth\"] = gts\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel, DistilBertConfig\n\n# setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n\n\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\ndatasets_list = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\nablations = {\n    \"pretrained\": lambda: DistilBertModel.from_pretrained(\"distilbert-base-uncased\"),\n    \"random_init\": lambda: DistilBertModel(\n        DistilBertConfig.from_pretrained(\"distilbert-base-uncased\")\n    ),\n}\n\nexperiment_data = {ab: {} for ab in ablations}\n\nfor ablation_type, model_fn in ablations.items():\n    # reset seed for consistent random-init behavior\n    torch.manual_seed(42)\n    np.random.seed(42)\n    distilbert = model_fn().to(device)\n    distilbert.eval()\n    for p in distilbert.parameters():\n        p.requires_grad = False\n\n    for name in datasets_list:\n        # load and preprocess\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tokenize_fn(batch):\n            return tokenizer(\n                batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        train_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        val_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        num_labels = len(set(train_ds[\"label\"]))\n\n        # models & optimizers\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n        optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n        loss_fn = nn.CrossEntropyLoss()\n\n        # init storage\n        experiment_data[ablation_type][name] = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"alignments\": {\"train\": [], \"val\": []},\n            \"mai\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # training loop\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_loss, tot_align, tot_acc, n = 0.0, 0.0, 0, 0\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                with torch.no_grad():\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                optimizer_ai.zero_grad()\n                loss_ai.backward()\n                optimizer_ai.step()\n                optimizer_user.zero_grad()\n                loss_user.backward()\n                optimizer_user.step()\n                bs = batch[\"label\"].size(0)\n                tot_loss += loss_ai.item() * bs\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(logits_user, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_align += torch.sum(1 - jsd).item()\n                tot_acc += (\n                    (torch.argmax(logits_user, dim=1) == batch[\"label\"]).sum().item()\n                )\n                n += bs\n\n            experiment_data[ablation_type][name][\"losses\"][\"train\"].append(\n                tot_loss / len(train_ds)\n            )\n            experiment_data[ablation_type][name][\"alignments\"][\"train\"].append(\n                tot_align / n\n            )\n\n            # validation\n            ai_model.eval()\n            user_model.eval()\n            v_loss, v_align, v_acc, v_n = 0.0, 0.0, 0, 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                    logits_ai = ai_model(emb)\n                    v_loss += loss_fn(logits_ai, batch[\"label\"]).item() * batch[\n                        \"label\"\n                    ].size(0)\n                    P = F.softmax(logits_ai, dim=1)\n                    Q = F.softmax(user_model(emb), dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_align += torch.sum(1 - jsd).item()\n                    v_acc += (\n                        (torch.argmax(user_model(emb), dim=1) == batch[\"label\"])\n                        .sum()\n                        .item()\n                    )\n                    v_n += batch[\"label\"].size(0)\n\n            val_loss = v_loss / len(val_ds)\n            val_align = v_align / v_n\n            val_acc = v_acc / v_n\n            mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n\n            experiment_data[ablation_type][name][\"losses\"][\"val\"].append(val_loss)\n            experiment_data[ablation_type][name][\"alignments\"][\"val\"].append(val_align)\n            experiment_data[ablation_type][name][\"mai\"].append(mai)\n\n            print(\n                f\"Ablation={ablation_type} Dataset={name} Epoch={epoch} val_loss={val_loss:.4f} MAI={mai:.4f}\"\n            )\n\n        # final predictions\n        preds, gts = [], []\n        ai_model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                emb = distilbert(\n                    input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n                ).last_hidden_state[:, 0, :]\n                preds.append(torch.argmax(ai_model(emb), dim=1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        experiment_data[ablation_type][name][\"predictions\"] = np.concatenate(preds)\n        experiment_data[ablation_type][name][\"ground_truth\"] = np.concatenate(gts)\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# setup working directory and seeds\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# load tokenizer and frozen DistilBERT\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert.eval()\n\n\n# simple MLP head\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\n# token dropout helper (masks tokens with [MASK])\ndef apply_token_dropout(input_ids, attention_mask, drop_rate):\n    if drop_rate <= 0:\n        return input_ids, attention_mask\n    mask = torch.rand(input_ids.shape, device=input_ids.device) < drop_rate\n    mask &= attention_mask.bool()\n    mask[:, 0] = False  # never mask [CLS]\n    dropped = input_ids.clone()\n    dropped[mask] = tokenizer.mask_token_id\n    return dropped, attention_mask\n\n\n# ablation rates and datasets\nablation_rates = [0.0, 0.1, 0.2, 0.3]\ndatasets = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\nexperiment_data = {}\n\nfor drop in ablation_rates:\n    key = f\"token_dropout_{int(drop*100)}\"\n    experiment_data[key] = {}\n    for name in datasets:\n        # load and tokenize\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tokenize_fn(batch):\n            return tokenizer(\n                batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        train_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        val_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n\n        # initialize models and optimizers\n        num_labels = len(set(train_ds[\"label\"]))\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n        optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n        loss_fn = nn.CrossEntropyLoss()\n\n        # storage for this dataset & ablation\n        data = {\n            \"metrics\": {\"train\": [], \"val\": []},  # classification accuracies\n            \"losses\": {\"train\": [], \"val\": []},\n            \"alignments\": {\"train\": [], \"val\": []},\n            \"mai\": [],  # val MAI per epoch\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # training loop\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_loss = tot_align = tot_acc = n_samples = 0.0\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                ids, att = apply_token_dropout(\n                    batch[\"input_ids\"], batch[\"attention_mask\"], drop\n                )\n                with torch.no_grad():\n                    emb = distilbert(\n                        input_ids=ids, attention_mask=att\n                    ).last_hidden_state[:, 0, :]\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                optimizer_ai.zero_grad()\n                loss_ai.backward()\n                optimizer_ai.step()\n                optimizer_user.zero_grad()\n                loss_user.backward()\n                optimizer_user.step()\n\n                bs = batch[\"label\"].size(0)\n                tot_loss += loss_ai.item() * bs\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(logits_user, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_align += torch.sum(1 - jsd).item()\n                tot_acc += (\n                    (torch.argmax(logits_user, dim=1) == batch[\"label\"]).sum().item()\n                )\n                n_samples += bs\n\n            loss_train = tot_loss / len(train_ds)\n            align_train = tot_align / n_samples\n            acc_train = tot_acc / n_samples\n            data[\"losses\"][\"train\"].append(loss_train)\n            data[\"alignments\"][\"train\"].append(align_train)\n            data[\"metrics\"][\"train\"].append(acc_train)\n\n            # validation\n            ai_model.eval()\n            user_model.eval()\n            v_loss = v_align = v_acc = v_n = 0.0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    ids, att = apply_token_dropout(\n                        batch[\"input_ids\"], batch[\"attention_mask\"], drop\n                    )\n                    emb = distilbert(\n                        input_ids=ids, attention_mask=att\n                    ).last_hidden_state[:, 0, :]\n                    logits_ai = ai_model(emb)\n                    v_loss += loss_fn(logits_ai, batch[\"label\"]).item() * batch[\n                        \"label\"\n                    ].size(0)\n                    P = F.softmax(logits_ai, dim=1)\n                    Q = F.softmax(user_model(emb), dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_align += torch.sum(1 - jsd).item()\n                    v_acc += (\n                        (torch.argmax(user_model(emb), dim=1) == batch[\"label\"])\n                        .sum()\n                        .item()\n                    )\n                    v_n += batch[\"label\"].size(0)\n\n            loss_val = v_loss / len(val_ds)\n            align_val = v_align / v_n\n            acc_val = v_acc / v_n\n            mai_val = 2 * (align_val * acc_val) / (align_val + acc_val + 1e-8)\n            data[\"losses\"][\"val\"].append(loss_val)\n            data[\"alignments\"][\"val\"].append(align_val)\n            data[\"metrics\"][\"val\"].append(acc_val)\n            data[\"mai\"].append(mai_val)\n\n            print(\n                f\"{key} {name} Epoch {epoch}: val_loss={loss_val:.4f}, \"\n                f\"val_acc={acc_val:.4f}, val_align={align_val:.4f}, MAI={mai_val:.4f}\"\n            )\n\n        # final predictions & ground truth\n        preds, gts = [], []\n        ai_model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                ids, att = apply_token_dropout(\n                    batch[\"input_ids\"], batch[\"attention_mask\"], drop\n                )\n                emb = distilbert(input_ids=ids, attention_mask=att).last_hidden_state[\n                    :, 0, :\n                ]\n                preds.append(torch.argmax(ai_model(emb), dim=1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        data[\"predictions\"] = np.concatenate(preds)\n        data[\"ground_truth\"] = np.concatenate(gts)\n        experiment_data[key][name] = data\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# Setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(42)\nnp.random.seed(42)\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert.eval()\n\n\n# Simple MLP head\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\n# Utility to shuffle non-pad tokens per sample (keep [CLS] at idx 0)\ndef shuffle_batch(input_ids, attention_mask):\n    ids = input_ids.clone()\n    mask = attention_mask\n    bs, seq_len = ids.shape\n    for i in range(bs):\n        active = mask[i] == 1\n        tokens = ids[i][active]\n        if tokens.size(0) > 1:\n            core = tokens[1:]\n            perm = torch.randperm(core.size(0))\n            new_seq = torch.cat((tokens[:1], core[perm]), dim=0)\n            ids[i, active] = new_seq\n    return ids, mask\n\n\n# Ablations and datasets\nablations = [\"standard\", \"shuffled\"]\ndatasets = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\nexperiment_data = {ab: {} for ab in ablations}\n\nfor ab in ablations:\n    for name in datasets:\n        # Load and preprocess\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tokenize_fn(batch):\n            return tokenizer(\n                batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        train_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        val_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        num_labels = len(set(train_ds[\"label\"]))\n\n        # Models, optimizers, loss\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n        optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n        loss_fn = nn.CrossEntropyLoss()\n\n        # Allocate storage\n        exp = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"alignments\": {\"train\": [], \"val\": []},\n            \"mai\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # Training loop\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_loss, tot_align, n = 0.0, 0.0, 0\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                inp_ids, attn = (batch[\"input_ids\"], batch[\"attention_mask\"])\n                if ab == \"shuffled\":\n                    inp_ids, attn = shuffle_batch(inp_ids, attn)\n                # Get [CLS] embeddings\n                emb = distilbert(\n                    input_ids=inp_ids, attention_mask=attn\n                ).last_hidden_state[:, 0, :]\n                # Forward, loss, backward\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                optimizer_ai.zero_grad()\n                loss_ai.backward()\n                optimizer_ai.step()\n                optimizer_user.zero_grad()\n                loss_user.backward()\n                optimizer_user.step()\n                # Metrics\n                bs = batch[\"label\"].size(0)\n                tot_loss += loss_ai.item() * bs\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(logits_user, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_align += torch.sum(1 - jsd).item()\n                n += bs\n            exp[\"losses\"][\"train\"].append(tot_loss / len(train_ds))\n            exp[\"alignments\"][\"train\"].append(tot_align / n)\n\n            # Validation\n            ai_model.eval()\n            user_model.eval()\n            v_loss, v_align, v_acc, v_n = 0.0, 0.0, 0, 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    inp_ids, attn = (batch[\"input_ids\"], batch[\"attention_mask\"])\n                    if ab == \"shuffled\":\n                        inp_ids, attn = shuffle_batch(inp_ids, attn)\n                    emb = distilbert(\n                        input_ids=inp_ids, attention_mask=attn\n                    ).last_hidden_state[:, 0, :]\n                    logits_ai = ai_model(emb)\n                    v_loss += loss_fn(logits_ai, batch[\"label\"]).item() * batch[\n                        \"label\"\n                    ].size(0)\n                    P = F.softmax(logits_ai, dim=1)\n                    Q = F.softmax(user_model(emb), dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_align += torch.sum(1 - jsd).item()\n                    v_acc += (\n                        (torch.argmax(user_model(emb), dim=1) == batch[\"label\"])\n                        .sum()\n                        .item()\n                    )\n                    v_n += batch[\"label\"].size(0)\n            val_loss = v_loss / len(val_ds)\n            val_align = v_align / v_n\n            val_acc = v_acc / v_n\n            mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n            exp[\"losses\"][\"val\"].append(val_loss)\n            exp[\"alignments\"][\"val\"].append(val_align)\n            exp[\"mai\"].append(mai)\n            print(\n                f\"Ablation {ab} Dataset {name} Epoch {epoch}: validation_loss = {val_loss:.4f}, MAI = {mai:.4f}\"\n            )\n\n        # Final predictions on val\n        preds, gts = [], []\n        ai_model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                inp_ids, attn = (batch[\"input_ids\"], batch[\"attention_mask\"])\n                if ab == \"shuffled\":\n                    inp_ids, attn = shuffle_batch(inp_ids, attn)\n                emb = distilbert(\n                    input_ids=inp_ids, attention_mask=attn\n                ).last_hidden_state[:, 0, :]\n                preds.append(torch.argmax(ai_model(emb), dim=1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        exp[\"predictions\"] = np.concatenate(preds)\n        exp[\"ground_truth\"] = np.concatenate(gts)\n        experiment_data[ab][name] = exp\n\n# Save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# Setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Frozen encoder\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert.eval()\n\n\n# Simple MLP\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\n# Ablation: optimizer choices\noptimizers = {\n    \"adam\": lambda params: torch.optim.Adam(params, lr=1e-3),\n    \"sgd\": lambda params: torch.optim.SGD(params, lr=1e-3, momentum=0.9),\n    \"adamw\": lambda params: torch.optim.AdamW(params, lr=1e-3),\n}\n\ndatasets_list = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\nloss_fn = nn.CrossEntropyLoss()\nexperiment_data = {}\n\nfor opt_name, opt_fn in optimizers.items():\n    experiment_data[opt_name] = {}\n    for name in datasets_list:\n        # Load and preprocess\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tokenize_fn(batch):\n            return tokenizer(\n                batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        train_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        val_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n\n        # Storage\n        experiment_data[opt_name][name] = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"alignments\": {\"train\": [], \"val\": []},\n            \"accuracy\": {\"train\": [], \"val\": []},\n            \"mai\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # Models and optimizers\n        num_labels = len(set(train_ds[\"label\"]))\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        optimizer_ai = opt_fn(ai_model.parameters())\n        optimizer_user = opt_fn(user_model.parameters())\n\n        # Train epochs\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_loss = tot_align = tot_acc = n = 0.0\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                with torch.no_grad():\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                optimizer_ai.zero_grad()\n                loss_ai.backward()\n                optimizer_ai.step()\n                optimizer_user.zero_grad()\n                loss_user.backward()\n                optimizer_user.step()\n                bs = batch[\"label\"].size(0)\n                tot_loss += loss_ai.item() * bs\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(logits_user, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_align += torch.sum(1 - jsd).item()\n                tot_acc += (\n                    (torch.argmax(logits_user, dim=1) == batch[\"label\"]).sum().item()\n                )\n                n += bs\n            train_loss = tot_loss / len(train_ds)\n            train_align = tot_align / n\n            train_acc = tot_acc / n\n            experiment_data[opt_name][name][\"losses\"][\"train\"].append(train_loss)\n            experiment_data[opt_name][name][\"alignments\"][\"train\"].append(train_align)\n            experiment_data[opt_name][name][\"accuracy\"][\"train\"].append(train_acc)\n\n            # Validation\n            ai_model.eval()\n            user_model.eval()\n            v_loss = v_align = v_acc = v_n = 0.0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                    logits_ai = ai_model(emb)\n                    logits_user = user_model(emb)\n                    bs = batch[\"label\"].size(0)\n                    v_loss += loss_fn(logits_ai, batch[\"label\"]).item() * bs\n                    P = F.softmax(logits_ai, dim=1)\n                    Q = F.softmax(logits_user, dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_align += torch.sum(1 - jsd).item()\n                    v_acc += (\n                        (torch.argmax(logits_user, dim=1) == batch[\"label\"])\n                        .sum()\n                        .item()\n                    )\n                    v_n += bs\n            val_loss = v_loss / len(val_ds)\n            val_align = v_align / v_n\n            val_acc = v_acc / v_n\n            mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n            experiment_data[opt_name][name][\"losses\"][\"val\"].append(val_loss)\n            experiment_data[opt_name][name][\"alignments\"][\"val\"].append(val_align)\n            experiment_data[opt_name][name][\"accuracy\"][\"val\"].append(val_acc)\n            experiment_data[opt_name][name][\"mai\"].append(mai)\n            print(\n                f\"Opt {opt_name} | {name} | Epoch {epoch} | val_loss={val_loss:.4f} val_align={val_align:.4f} val_acc={val_acc:.4f} MAI={mai:.4f}\"\n            )\n\n        # Collect predictions\n        preds, gts = [], []\n        ai_model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                emb = distilbert(\n                    input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n                ).last_hidden_state[:, 0, :]\n                preds.append(torch.argmax(ai_model(emb), dim=1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        experiment_data[opt_name][name][\"predictions\"] = np.concatenate(preds)\n        experiment_data[opt_name][name][\"ground_truth\"] = np.concatenate(gts)\n\n# Save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(42)\nnp.random.seed(42)\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert.eval()\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n\n# model classes\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\nclass MLPShared1(nn.Module):\n    def __init__(self, shared_fc1, fc2):\n        super().__init__()\n        self.fc1 = shared_fc1\n        self.fc2 = fc2\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\n# settings\ndatasets = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\nablations = [\"independent\", \"shared_fc1\", \"shared_fc1_fc2\"]\nexperiment_data = {a: {} for a in ablations}\nloss_fn = nn.CrossEntropyLoss()\n\nfor ablation in ablations:\n    for name in datasets:\n        # prepare data\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tok(b):\n            return tokenizer(\n                b[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tok, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tok, batched=True, remove_columns=[text_key])\n        train_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        val_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        num_labels = len(set(train_ds[\"label\"]))\n        # build models & optimizers\n        in_dim, hid_dim, out_dim = distilbert.config.hidden_size, 128, num_labels\n        if ablation == \"independent\":\n            ai_model = MLP(in_dim, hid_dim, out_dim).to(device)\n            user_model = MLP(in_dim, hid_dim, out_dim).to(device)\n            opt_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n            opt_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n            optimizer = None\n        elif ablation == \"shared_fc1\":\n            shared_fc1 = nn.Linear(in_dim, hid_dim).to(device)\n            ai_fc2 = nn.Linear(hid_dim, out_dim).to(device)\n            user_fc2 = nn.Linear(hid_dim, out_dim).to(device)\n            ai_model = MLPShared1(shared_fc1, ai_fc2).to(device)\n            user_model = MLPShared1(shared_fc1, user_fc2).to(device)\n            optimizer = torch.optim.Adam(\n                list(shared_fc1.parameters())\n                + list(ai_fc2.parameters())\n                + list(user_fc2.parameters()),\n                lr=1e-3,\n            )\n        else:  # fully shared\n            shared = MLP(in_dim, hid_dim, out_dim).to(device)\n            ai_model, user_model = shared, shared\n            optimizer = torch.optim.Adam(shared.parameters(), lr=1e-3)\n        # init logs\n        ed = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"alignments\": {\"train\": [], \"val\": []},\n            \"mai\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        # train & validate\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_loss = tot_align = tot_acc = n = 0\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                with torch.no_grad():\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                # update\n                if ablation == \"independent\":\n                    opt_ai.zero_grad()\n                    loss_ai.backward()\n                    opt_ai.step()\n                    opt_user.zero_grad()\n                    loss_user.backward()\n                    opt_user.step()\n                else:\n                    optimizer.zero_grad()\n                    (loss_ai + loss_user).backward()\n                    optimizer.step()\n                # metrics\n                bs = batch[\"label\"].size(0)\n                tot_loss += loss_ai.item() * bs\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(logits_user, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_align += torch.sum(1 - jsd).item()\n                tot_acc += (torch.argmax(logits_user, 1) == batch[\"label\"]).sum().item()\n                n += bs\n            ed[\"losses\"][\"train\"].append(tot_loss / len(train_ds))\n            ed[\"alignments\"][\"train\"].append(tot_align / n)\n            # validation\n            ai_model.eval()\n            user_model.eval()\n            v_loss = v_align = v_acc = v_n = 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                    logits_ai = ai_model(emb)\n                    bs = batch[\"label\"].size(0)\n                    v_loss += loss_fn(logits_ai, batch[\"label\"]).item() * bs\n                    P = F.softmax(logits_ai, dim=1)\n                    Q = F.softmax(user_model(emb), dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_align += torch.sum(1 - jsd).item()\n                    v_acc += (\n                        (torch.argmax(user_model(emb), 1) == batch[\"label\"])\n                        .sum()\n                        .item()\n                    )\n                    v_n += bs\n            val_loss = v_loss / len(val_ds)\n            val_align = v_align / v_n\n            val_acc = v_acc / v_n\n            mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n            ed[\"losses\"][\"val\"].append(val_loss)\n            ed[\"alignments\"][\"val\"].append(val_align)\n            ed[\"mai\"].append(mai)\n            print(\n                f\"Ablation {ablation} Dataset {name} Epoch {epoch}: \"\n                f\"validation_loss={val_loss:.4f}, MAI={mai:.4f}\"\n            )\n        # final preds\n        ai_model.eval()\n        preds, gts = [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                emb = distilbert(\n                    input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n                ).last_hidden_state[:, 0, :]\n                preds.append(torch.argmax(ai_model(emb), 1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        ed[\"predictions\"] = np.concatenate(preds)\n        ed[\"ground_truth\"] = np.concatenate(gts)\n        experiment_data[ablation][name] = ed\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert.eval()\nfor p in distilbert.parameters():\n    p.requires_grad = False\n\n\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\ndef shuffle_batch(input_ids, attention_mask):\n    ids = input_ids.clone()\n    mask = attention_mask\n    bs, _ = ids.shape\n    for i in range(bs):\n        active = mask[i] == 1\n        tokens = ids[i][active]\n        if tokens.size(0) > 1:\n            core = tokens[1:]\n            perm = torch.randperm(core.size(0))\n            new_seq = torch.cat((tokens[:1], core[perm]), dim=0)\n            ids[i, active] = new_seq\n    return ids, mask\n\n\nablations = [\"standard\", \"shuffled\"]\ndatasets_list = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\nexperiment_data = {ab: {} for ab in ablations}\n\nfor ab in ablations:\n    for name in datasets_list:\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tokenize_fn(batch):\n            return tokenizer(\n                batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        train_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        val_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        num_labels = len(set(train_ds[\"label\"]))\n\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n        optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n        loss_fn = nn.CrossEntropyLoss()\n\n        exp = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"metrics\": {\n                \"train\": [],\n                \"val\": [],\n            },  # Bidirectional Mental Model Alignment Score\n            \"mai\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_loss, tot_align, n = 0.0, 0.0, 0\n            for batch in train_loader:\n                batch = {\n                    k: v.to(device)\n                    for k, v in batch.items()\n                    if isinstance(v, torch.Tensor)\n                }\n                inp_ids, attn = batch[\"input_ids\"], batch[\"attention_mask\"]\n                if ab == \"shuffled\":\n                    inp_ids, attn = shuffle_batch(inp_ids, attn)\n                with torch.no_grad():\n                    emb = distilbert(\n                        input_ids=inp_ids, attention_mask=attn\n                    ).last_hidden_state[:, 0, :]\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                optimizer_ai.zero_grad()\n                optimizer_user.zero_grad()\n                total_loss = loss_ai + loss_user\n                total_loss.backward()\n                optimizer_ai.step()\n                optimizer_user.step()\n\n                bs = batch[\"label\"].size(0)\n                tot_loss += loss_ai.item() * bs\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(logits_user, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_align += torch.sum(1 - jsd).item()\n                n += bs\n            train_loss_avg = tot_loss / len(train_ds)\n            train_bm_alignment = tot_align / n\n            exp[\"losses\"][\"train\"].append(train_loss_avg)\n            exp[\"metrics\"][\"train\"].append(train_bm_alignment)\n\n            ai_model.eval()\n            user_model.eval()\n            v_loss, v_align, v_acc, v_n = 0.0, 0.0, 0, 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {\n                        k: v.to(device)\n                        for k, v in batch.items()\n                        if isinstance(v, torch.Tensor)\n                    }\n                    inp_ids, attn = batch[\"input_ids\"], batch[\"attention_mask\"]\n                    if ab == \"shuffled\":\n                        inp_ids, attn = shuffle_batch(inp_ids, attn)\n                    emb = distilbert(\n                        input_ids=inp_ids, attention_mask=attn\n                    ).last_hidden_state[:, 0, :]\n                    logits_ai = ai_model(emb)\n                    v_loss += loss_fn(logits_ai, batch[\"label\"]).item() * batch[\n                        \"label\"\n                    ].size(0)\n                    P = F.softmax(logits_ai, dim=1)\n                    Q = F.softmax(user_model(emb), dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_align += torch.sum(1 - jsd).item()\n                    v_acc += (\n                        (torch.argmax(user_model(emb), dim=1) == batch[\"label\"])\n                        .sum()\n                        .item()\n                    )\n                    v_n += batch[\"label\"].size(0)\n            val_loss = v_loss / len(val_ds)\n            val_bm_alignment = v_align / v_n\n            val_accuracy = v_acc / v_n\n            mai = (\n                2\n                * (val_bm_alignment * val_accuracy)\n                / (val_bm_alignment + val_accuracy + 1e-8)\n            )\n            exp[\"losses\"][\"val\"].append(val_loss)\n            exp[\"metrics\"][\"val\"].append(val_bm_alignment)\n            exp[\"mai\"].append(mai)\n            print(\n                f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, Bidirectional MM Alignment = {val_bm_alignment:.4f}\"\n            )\n\n        preds, gts = [], []\n        ai_model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {\n                    k: v.to(device)\n                    for k, v in batch.items()\n                    if isinstance(v, torch.Tensor)\n                }\n                inp_ids, attn = batch[\"input_ids\"], batch[\"attention_mask\"]\n                if ab == \"shuffled\":\n                    inp_ids, attn = shuffle_batch(inp_ids, attn)\n                emb = distilbert(\n                    input_ids=inp_ids, attention_mask=attn\n                ).last_hidden_state[:, 0, :]\n                preds.append(torch.argmax(ai_model(emb), dim=1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        exp[\"predictions\"] = np.concatenate(preds)\n        exp[\"ground_truth\"] = np.concatenate(gts)\n        experiment_data[ab][name] = exp\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# backbone\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert.eval()\n\n\n# simple classifier\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\n# experiment\ntemps = [0.5, 1.0, 2.0]\ndatasets = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\nexperiment_data = {}\n\nfor T in temps:\n    key = f\"softmax_temperature_{T}\"\n    experiment_data[key] = {}\n    for name in datasets:\n        # prepare data\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tokenize_fn(batch):\n            return tokenizer(\n                batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        train_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        val_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        num_labels = len(set(train_ds[\"label\"]))\n\n        # models & optim\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n        optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n        loss_fn = nn.CrossEntropyLoss()\n\n        # record dict\n        record = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"alignments\": {\"train\": [], \"val\": []},\n            \"accuracy\": {\"train\": [], \"val\": []},\n            \"mai\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        experiment_data[key][name] = record\n\n        # training loop\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_l, tot_a, tot_acc, n = 0.0, 0.0, 0, 0\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                with torch.no_grad():\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                optimizer_ai.zero_grad()\n                loss_ai.backward()\n                optimizer_ai.step()\n                optimizer_user.zero_grad()\n                loss_user.backward()\n                optimizer_user.step()\n                bs = batch[\"label\"].size(0)\n                tot_l += loss_ai.item() * bs\n                P = F.softmax(logits_ai / T, dim=1)\n                Q = F.softmax(logits_user / T, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_a += torch.sum(1 - jsd).item()\n                tot_acc += (torch.argmax(logits_user, 1) == batch[\"label\"]).sum().item()\n                n += bs\n            # append train metrics\n            record[\"losses\"][\"train\"].append(tot_l / len(train_ds))\n            record[\"alignments\"][\"train\"].append(tot_a / n)\n            record[\"accuracy\"][\"train\"].append(tot_acc / n)\n\n            # validation\n            ai_model.eval()\n            user_model.eval()\n            v_l, v_a, v_acc, v_n = 0.0, 0.0, 0, 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                    logits_ai = ai_model(emb)\n                    v_l += loss_fn(logits_ai, batch[\"label\"]).item() * batch[\n                        \"label\"\n                    ].size(0)\n                    P = F.softmax(logits_ai / T, dim=1)\n                    Q = F.softmax(user_model(emb) / T, dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_a += torch.sum(1 - jsd).item()\n                    v_acc += (\n                        (torch.argmax(user_model(emb), 1) == batch[\"label\"])\n                        .sum()\n                        .item()\n                    )\n                    v_n += batch[\"label\"].size(0)\n            val_loss = v_l / len(val_ds)\n            val_align = v_a / v_n\n            val_acc = v_acc / v_n\n            mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n            record[\"losses\"][\"val\"].append(val_loss)\n            record[\"alignments\"][\"val\"].append(val_align)\n            record[\"accuracy\"][\"val\"].append(val_acc)\n            record[\"mai\"].append(mai)\n            print(\n                f\"T={T} {name} Epoch {epoch}: val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, MAI={mai:.4f}\"\n            )\n\n        # final preds & gts\n        preds, gts = [], []\n        ai_model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                emb = distilbert(\n                    input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n                ).last_hidden_state[:, 0, :]\n                preds.append(torch.argmax(ai_model(emb), 1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        record[\"predictions\"] = np.concatenate(preds)\n        record[\"ground_truth\"] = np.concatenate(gts)\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# backbone\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert.eval()\n\n\n# simple classifier\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\n# experiment\ntemps = [0.5, 1.0, 2.0]\ndatasets = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\nexperiment_data = {}\n\nfor T in temps:\n    key = f\"softmax_temperature_{T}\"\n    experiment_data[key] = {}\n    for name in datasets:\n        # prepare data\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tokenize_fn(batch):\n            return tokenizer(\n                batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        train_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        val_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        num_labels = len(set(train_ds[\"label\"]))\n\n        # models & optim\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n        optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n        loss_fn = nn.CrossEntropyLoss()\n\n        # record dict\n        record = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"alignments\": {\"train\": [], \"val\": []},\n            \"accuracy\": {\"train\": [], \"val\": []},\n            \"mai\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        experiment_data[key][name] = record\n\n        # training loop\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_l, tot_a, tot_acc, n = 0.0, 0.0, 0, 0\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                with torch.no_grad():\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                optimizer_ai.zero_grad()\n                loss_ai.backward()\n                optimizer_ai.step()\n                optimizer_user.zero_grad()\n                loss_user.backward()\n                optimizer_user.step()\n                bs = batch[\"label\"].size(0)\n                tot_l += loss_ai.item() * bs\n                P = F.softmax(logits_ai / T, dim=1)\n                Q = F.softmax(logits_user / T, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_a += torch.sum(1 - jsd).item()\n                tot_acc += (torch.argmax(logits_user, 1) == batch[\"label\"]).sum().item()\n                n += bs\n            # append train metrics\n            record[\"losses\"][\"train\"].append(tot_l / len(train_ds))\n            record[\"alignments\"][\"train\"].append(tot_a / n)\n            record[\"accuracy\"][\"train\"].append(tot_acc / n)\n\n            # validation\n            ai_model.eval()\n            user_model.eval()\n            v_l, v_a, v_acc, v_n = 0.0, 0.0, 0, 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                    logits_ai = ai_model(emb)\n                    v_l += loss_fn(logits_ai, batch[\"label\"]).item() * batch[\n                        \"label\"\n                    ].size(0)\n                    P = F.softmax(logits_ai / T, dim=1)\n                    Q = F.softmax(user_model(emb) / T, dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_a += torch.sum(1 - jsd).item()\n                    v_acc += (\n                        (torch.argmax(user_model(emb), 1) == batch[\"label\"])\n                        .sum()\n                        .item()\n                    )\n                    v_n += batch[\"label\"].size(0)\n            val_loss = v_l / len(val_ds)\n            val_align = v_a / v_n\n            val_acc = v_acc / v_n\n            mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n            record[\"losses\"][\"val\"].append(val_loss)\n            record[\"alignments\"][\"val\"].append(val_align)\n            record[\"accuracy\"][\"val\"].append(val_acc)\n            record[\"mai\"].append(mai)\n            print(\n                f\"T={T} {name} Epoch {epoch}: val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, MAI={mai:.4f}\"\n            )\n\n        # final preds & gts\n        preds, gts = [], []\n        ai_model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                emb = distilbert(\n                    input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n                ).last_hidden_state[:, 0, :]\n                preds.append(torch.argmax(ai_model(emb), 1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        record[\"predictions\"] = np.concatenate(preds)\n        record[\"ground_truth\"] = np.concatenate(gts)\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# backbone\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert.eval()\n\n\n# simple classifier\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\n# experiment\ntemps = [0.5, 1.0, 2.0]\ndatasets = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\nexperiment_data = {}\n\nfor T in temps:\n    key = f\"softmax_temperature_{T}\"\n    experiment_data[key] = {}\n    for name in datasets:\n        # prepare data\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tokenize_fn(batch):\n            return tokenizer(\n                batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        train_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        val_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        num_labels = len(set(train_ds[\"label\"]))\n\n        # models & optim\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n        optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n        loss_fn = nn.CrossEntropyLoss()\n\n        # record dict\n        record = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"alignments\": {\"train\": [], \"val\": []},\n            \"accuracy\": {\"train\": [], \"val\": []},\n            \"mai\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        experiment_data[key][name] = record\n\n        # training loop\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_l, tot_a, tot_acc, n = 0.0, 0.0, 0, 0\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                with torch.no_grad():\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                optimizer_ai.zero_grad()\n                loss_ai.backward()\n                optimizer_ai.step()\n                optimizer_user.zero_grad()\n                loss_user.backward()\n                optimizer_user.step()\n                bs = batch[\"label\"].size(0)\n                tot_l += loss_ai.item() * bs\n                P = F.softmax(logits_ai / T, dim=1)\n                Q = F.softmax(logits_user / T, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_a += torch.sum(1 - jsd).item()\n                tot_acc += (torch.argmax(logits_user, 1) == batch[\"label\"]).sum().item()\n                n += bs\n            # append train metrics\n            record[\"losses\"][\"train\"].append(tot_l / len(train_ds))\n            record[\"alignments\"][\"train\"].append(tot_a / n)\n            record[\"accuracy\"][\"train\"].append(tot_acc / n)\n\n            # validation\n            ai_model.eval()\n            user_model.eval()\n            v_l, v_a, v_acc, v_n = 0.0, 0.0, 0, 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                    logits_ai = ai_model(emb)\n                    v_l += loss_fn(logits_ai, batch[\"label\"]).item() * batch[\n                        \"label\"\n                    ].size(0)\n                    P = F.softmax(logits_ai / T, dim=1)\n                    Q = F.softmax(user_model(emb) / T, dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_a += torch.sum(1 - jsd).item()\n                    v_acc += (\n                        (torch.argmax(user_model(emb), 1) == batch[\"label\"])\n                        .sum()\n                        .item()\n                    )\n                    v_n += batch[\"label\"].size(0)\n            val_loss = v_l / len(val_ds)\n            val_align = v_a / v_n\n            val_acc = v_acc / v_n\n            mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n            record[\"losses\"][\"val\"].append(val_loss)\n            record[\"alignments\"][\"val\"].append(val_align)\n            record[\"accuracy\"][\"val\"].append(val_acc)\n            record[\"mai\"].append(mai)\n            print(\n                f\"T={T} {name} Epoch {epoch}: val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, MAI={mai:.4f}\"\n            )\n\n        # final preds & gts\n        preds, gts = [], []\n        ai_model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                emb = distilbert(\n                    input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n                ).last_hidden_state[:, 0, :]\n                preds.append(torch.argmax(ai_model(emb), 1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        record[\"predictions\"] = np.concatenate(preds)\n        record[\"ground_truth\"] = np.concatenate(gts)\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# backbone\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ndistilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\ndistilbert.eval()\n\n\n# simple classifier\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\n# experiment\ntemps = [0.5, 1.0, 2.0]\ndatasets = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\nexperiment_data = {}\n\nfor T in temps:\n    key = f\"softmax_temperature_{T}\"\n    experiment_data[key] = {}\n    for name in datasets:\n        # prepare data\n        raw = load_dataset(name, split=\"train\").shuffle(seed=0).select(range(2500))\n        split = raw.train_test_split(test_size=0.2, seed=0)\n        train_ds, val_ds = split[\"train\"], split[\"test\"]\n        text_key = \"text\" if \"text\" in raw.column_names else \"content\"\n\n        def tokenize_fn(batch):\n            return tokenizer(\n                batch[text_key], padding=\"max_length\", truncation=True, max_length=128\n            )\n\n        train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[text_key])\n        train_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        val_ds.set_format(\n            type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n        )\n        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=32)\n        num_labels = len(set(train_ds[\"label\"]))\n\n        # models & optim\n        ai_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        user_model = MLP(distilbert.config.hidden_size, 128, num_labels).to(device)\n        optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n        optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n        loss_fn = nn.CrossEntropyLoss()\n\n        # record dict\n        record = {\n            \"losses\": {\"train\": [], \"val\": []},\n            \"alignments\": {\"train\": [], \"val\": []},\n            \"accuracy\": {\"train\": [], \"val\": []},\n            \"mai\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        experiment_data[key][name] = record\n\n        # training loop\n        for epoch in range(1, 4):\n            ai_model.train()\n            user_model.train()\n            tot_l, tot_a, tot_acc, n = 0.0, 0.0, 0, 0\n            for batch in train_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                with torch.no_grad():\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                logits_ai = ai_model(emb)\n                logits_user = user_model(emb)\n                loss_ai = loss_fn(logits_ai, batch[\"label\"])\n                loss_user = loss_fn(logits_user, batch[\"label\"])\n                optimizer_ai.zero_grad()\n                loss_ai.backward()\n                optimizer_ai.step()\n                optimizer_user.zero_grad()\n                loss_user.backward()\n                optimizer_user.step()\n                bs = batch[\"label\"].size(0)\n                tot_l += loss_ai.item() * bs\n                P = F.softmax(logits_ai / T, dim=1)\n                Q = F.softmax(logits_user / T, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                tot_a += torch.sum(1 - jsd).item()\n                tot_acc += (torch.argmax(logits_user, 1) == batch[\"label\"]).sum().item()\n                n += bs\n            # append train metrics\n            record[\"losses\"][\"train\"].append(tot_l / len(train_ds))\n            record[\"alignments\"][\"train\"].append(tot_a / n)\n            record[\"accuracy\"][\"train\"].append(tot_acc / n)\n\n            # validation\n            ai_model.eval()\n            user_model.eval()\n            v_l, v_a, v_acc, v_n = 0.0, 0.0, 0, 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    emb = distilbert(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                    ).last_hidden_state[:, 0, :]\n                    logits_ai = ai_model(emb)\n                    v_l += loss_fn(logits_ai, batch[\"label\"]).item() * batch[\n                        \"label\"\n                    ].size(0)\n                    P = F.softmax(logits_ai / T, dim=1)\n                    Q = F.softmax(user_model(emb) / T, dim=1)\n                    M = 0.5 * (P + Q)\n                    kl1 = torch.sum(\n                        P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    kl2 = torch.sum(\n                        Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1\n                    )\n                    jsd = 0.5 * (kl1 + kl2)\n                    v_a += torch.sum(1 - jsd).item()\n                    v_acc += (\n                        (torch.argmax(user_model(emb), 1) == batch[\"label\"])\n                        .sum()\n                        .item()\n                    )\n                    v_n += batch[\"label\"].size(0)\n            val_loss = v_l / len(val_ds)\n            val_align = v_a / v_n\n            val_acc = v_acc / v_n\n            mai = 2 * (val_align * val_acc) / (val_align + val_acc + 1e-8)\n            record[\"losses\"][\"val\"].append(val_loss)\n            record[\"alignments\"][\"val\"].append(val_align)\n            record[\"accuracy\"][\"val\"].append(val_acc)\n            record[\"mai\"].append(mai)\n            print(\n                f\"T={T} {name} Epoch {epoch}: val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, MAI={mai:.4f}\"\n            )\n\n        # final preds & gts\n        preds, gts = [], []\n        ai_model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                emb = distilbert(\n                    input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n                ).last_hidden_state[:, 0, :]\n                preds.append(torch.argmax(ai_model(emb), 1).cpu().numpy())\n                gts.append(batch[\"label\"].cpu().numpy())\n        record[\"predictions\"] = np.concatenate(preds)\n        record[\"ground_truth\"] = np.concatenate(gts)\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'LR 1.0e-04 Epoch 1: val_loss = 1.0651, val_align =\n0.9922', '\\n', 'LR 1.0e-04 Epoch 2: val_loss = 1.0588, val_align = 0.9923',\n'\\n', 'LR 1.0e-04 Epoch 3: val_loss = 1.0526, val_align = 0.9924', '\\n', 'LR\n1.0e-04 Epoch 4: val_loss = 1.0464, val_align = 0.9925', '\\n', 'LR 1.0e-04 Epoch\n5: val_loss = 1.0403, val_align = 0.9926', '\\n', 'LR 1.0e-04 Epoch 6: val_loss =\n1.0343, val_align = 0.9926', '\\n', 'LR 1.0e-04 Epoch 7: val_loss = 1.0283,\nval_align = 0.9927', '\\n', 'LR 1.0e-04 Epoch 8: val_loss = 1.0224, val_align =\n0.9928', '\\n', 'LR 1.0e-04 Epoch 9: val_loss = 1.0165, val_align = 0.9928',\n'\\n', 'LR 1.0e-04 Epoch 10: val_loss = 1.0106, val_align = 0.9929', '\\n', 'LR\n5.0e-04 Epoch 1: val_loss = 1.0400, val_align = 0.9926', '\\n', 'LR 5.0e-04 Epoch\n2: val_loss = 1.0104, val_align = 0.9929', '\\n', 'LR 5.0e-04 Epoch 3: val_loss =\n0.9821, val_align = 0.9931', '\\n', 'LR 5.0e-04 Epoch 4: val_loss = 0.9540,\nval_align = 0.9931', '\\n', 'LR 5.0e-04 Epoch 5: val_loss = 0.9259, val_align =\n0.9930', '\\n', 'LR 5.0e-04 Epoch 6: val_loss = 0.8980, val_align = 0.9927',\n'\\n', 'LR 5.0e-04 Epoch 7: val_loss = 0.8697, val_align = 0.9925', '\\n', 'LR\n5.0e-04 Epoch 8: val_loss = 0.8407, val_align = 0.9922', '\\n', 'LR 5.0e-04 Epoch\n9: val_loss = 0.8115, val_align = 0.9918', '\\n', 'LR 5.0e-04 Epoch 10: val_loss\n= 0.7817, val_align = 0.9915', '\\n', 'LR 1.0e-03 Epoch 1: val_loss = 1.0102,\nval_align = 0.9930', '\\n', 'LR 1.0e-03 Epoch 2: val_loss = 0.9538, val_align =\n0.9932', '\\n', 'LR 1.0e-03 Epoch 3: val_loss = 0.8995, val_align = 0.9928',\n'\\n', 'LR 1.0e-03 Epoch 4: val_loss = 0.8438, val_align = 0.9922', '\\n', 'LR\n1.0e-03 Epoch 5: val_loss = 0.7864, val_align = 0.9916', '\\n', 'LR 1.0e-03 Epoch\n6: val_loss = 0.7292, val_align = 0.9909', '\\n', 'LR 1.0e-03 Epoch 7: val_loss =\n0.6713, val_align = 0.9904', '\\n', 'LR 1.0e-03 Epoch 8: val_loss = 0.6147,\nval_align = 0.9902', '\\n', 'LR 1.0e-03 Epoch 9: val_loss = 0.5608, val_align =\n0.9903', '\\n', 'LR 1.0e-03 Epoch 10: val_loss = 0.5101, val_align = 0.9907',\n'\\n', 'LR 5.0e-03 Epoch 1: val_loss = 0.7999, val_align = 0.9919', '\\n', 'LR\n5.0e-03 Epoch 2: val_loss = 0.5444, val_align = 0.9895', '\\n', 'LR 5.0e-03 Epoch\n3: val_loss = 0.3476, val_align = 0.9923', '\\n', 'LR 5.0e-03 Epoch 4: val_loss =\n0.2409, val_align = 0.9956', '\\n', 'LR 5.0e-03 Epoch 5: val_loss = 0.1822,\nval_align = 0.9971', '\\n', 'LR 5.0e-03 Epoch 6: val_loss = 0.1579, val_align =\n0.9980', '\\n', 'LR 5.0e-03 Epoch 7: val_loss = 0.1343, val_align = 0.9983',\n'\\n', 'LR 5.0e-03 Epoch 8: val_loss = 0.1260, val_align = 0.9986', '\\n', 'LR\n5.0e-03 Epoch 9: val_loss = 0.1144, val_align = 0.9987', '\\n', 'LR 5.0e-03 Epoch\n10: val_loss = 0.1039, val_align = 0.9989', '\\n', 'LR 1.0e-02 Epoch 1: val_loss\n= 0.5694, val_align = 0.9887', '\\n', 'LR 1.0e-02 Epoch 2: val_loss = 0.2476,\nval_align = 0.9946', '\\n', 'LR 1.0e-02 Epoch 3: val_loss = 0.1484, val_align =\n0.9973', '\\n', 'LR 1.0e-02 Epoch 4: val_loss = 0.1135, val_align = 0.9985',\n'\\n', 'LR 1.0e-02 Epoch 5: val_loss = 0.0999, val_align = 0.9986', '\\n', 'LR\n1.0e-02 Epoch 6: val_loss = 0.0867, val_align = 0.9988', '\\n', 'LR 1.0e-02 Epoch\n7: val_loss = 0.0869, val_align = 0.9990', '\\n', 'LR 1.0e-02 Epoch 8: val_loss =\n0.0786, val_align = 0.9988', '\\n', 'LR 1.0e-02 Epoch 9: val_loss = 0.0752,\nval_align = 0.9990', '\\n', 'LR 1.0e-02 Epoch 10: val_loss = 0.0739, val_align =\n0.9986', '\\n', 'Execution time: 3 seconds seconds (time limit is an hour).']", "['\\rMap:   0%|          | 0/2000 [00:00<?, ? examples/s]', '\\rMap:  50%|#####\n| 1000/2000 [00:00<00:00, 7335.35 examples/s]', '', '\\rMap: 100%|##########|\n2000/2000 [00:00<00:00, 8524.64 examples/s]', '\\n', '\\rMap:   0%|          |\n0/500 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500\n[00:00<00:00, 9854.39 examples/s]', '\\n', 'Ablation baseline Dataset ag_news\nEpoch 1: val_loss=0.4909, MAI=0.9305', '\\n', 'Ablation baseline Dataset ag_news\nEpoch 2: val_loss=0.3699, MAI=0.9358', '\\n', 'Ablation baseline Dataset ag_news\nEpoch 3: val_loss=0.3472, MAI=0.9427', '\\n', '\\rMap:   0%|          | 0/2000\n[00:00<?, ? examples/s]', '\\rMap:  50%|#####     | 1000/2000 [00:00<00:00,\n7727.78 examples/s]', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 7926.91\nexamples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 7800.67\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/500 [00:00<?, ? examples/s]', '',\n'\\rMap: 100%|##########| 500/500 [00:00<00:00, 7591.78 examples/s]', '\\n',\n'Ablation baseline Dataset yelp_polarity Epoch 1: val_loss=0.4694, MAI=0.9062',\n'\\n', 'Ablation baseline Dataset yelp_polarity Epoch 2: val_loss=0.3836,\nMAI=0.9094', '\\n', 'Ablation baseline Dataset yelp_polarity Epoch 3:\nval_loss=0.3789, MAI=0.9047', '\\n', '\\rMap:   0%|          | 0/2000 [00:00<?, ?\nexamples/s]', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 11333.75\nexamples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 11133.29\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/500 [00:00<?, ? examples/s]', '',\n'\\rMap: 100%|##########| 500/500 [00:00<00:00, 11636.82 examples/s]', '\\n',\n'Ablation baseline Dataset dbpedia_14 Epoch 1: val_loss=0.7949, MAI=0.9508',\n'\\n', 'Ablation baseline Dataset dbpedia_14 Epoch 2: val_loss=0.2998,\nMAI=0.9797', '\\n', 'Ablation baseline Dataset dbpedia_14 Epoch 3:\nval_loss=0.1728, MAI=0.9858', '\\n', '\\rMap:   0%|          | 0/2000 [00:00<?, ?\nexamples/s]', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 5520.38\nexamples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 5476.79\nexamples/s]', '\\n', 'Ablation fine_tune Dataset ag_news Epoch 1:\nval_loss=4.5556, MAI=0.3564', '\\n', 'Ablation fine_tune Dataset ag_news Epoch 2:\nval_loss=1.4177, MAI=0.3890', '\\n', 'Ablation fine_tune Dataset ag_news Epoch 3:\nval_loss=1.4178, MAI=0.3999', '\\n', 'Ablation fine_tune Dataset yelp_polarity\nEpoch 1: val_loss=0.7229, MAI=0.7013', '\\n', 'Ablation fine_tune Dataset\nyelp_polarity Epoch 2: val_loss=0.7520, MAI=0.6232', '\\n', 'Ablation fine_tune\nDataset yelp_polarity Epoch 3: val_loss=0.6905, MAI=0.7080', '\\n', 'Ablation\nfine_tune Dataset dbpedia_14 Epoch 1: val_loss=1.1999, MAI=0.7728', '\\n',\n'Ablation fine_tune Dataset dbpedia_14 Epoch 2: val_loss=2.7976, MAI=0.1096',\n'\\n', 'Ablation fine_tune Dataset dbpedia_14 Epoch 3: val_loss=2.3812,\nMAI=0.2173', '\\n', 'Execution time: 2 minutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\rMap:   0%|          | 0/500 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00, 6970.85\nexamples/s]', '\\n', 'Ablation baseline Dataset ag_news Epoch 1: val_loss =\n0.4909, MAI = 0.9305', '\\n', 'Ablation baseline Dataset ag_news Epoch 2:\nval_loss = 0.3699, MAI = 0.9358', '\\n', 'Ablation baseline Dataset ag_news Epoch\n3: val_loss = 0.3472, MAI = 0.9427', '\\n', 'Ablation baseline Dataset\nyelp_polarity Epoch 1: val_loss = 0.4694, MAI = 0.9062', '\\n', 'Ablation\nbaseline Dataset yelp_polarity Epoch 2: val_loss = 0.3836, MAI = 0.9094', '\\n',\n'Ablation baseline Dataset yelp_polarity Epoch 3: val_loss = 0.3789, MAI =\n0.9047', '\\n', 'Ablation baseline Dataset dbpedia_14 Epoch 1: val_loss = 0.7949,\nMAI = 0.9508', '\\n', 'Ablation baseline Dataset dbpedia_14 Epoch 2: val_loss =\n0.2998, MAI = 0.9797', '\\n', 'Ablation baseline Dataset dbpedia_14 Epoch 3:\nval_loss = 0.1728, MAI = 0.9858', '\\n', 'Ablation linear_probe Dataset ag_news\nEpoch 1: val_loss = 0.7531, MAI = 0.9252', '\\n', 'Ablation linear_probe Dataset\nag_news Epoch 2: val_loss = 0.5131, MAI = 0.9313', '\\n', 'Ablation linear_probe\nDataset ag_news Epoch 3: val_loss = 0.4337, MAI = 0.9371', '\\n', 'Ablation\nlinear_probe Dataset yelp_polarity Epoch 1: val_loss = 0.5902, MAI = 0.9080',\n'\\n', 'Ablation linear_probe Dataset yelp_polarity Epoch 2: val_loss = 0.4956,\nMAI = 0.9009', '\\n', 'Ablation linear_probe Dataset yelp_polarity Epoch 3:\nval_loss = 0.4337, MAI = 0.9094', '\\n', 'Ablation linear_probe Dataset\ndbpedia_14 Epoch 1: val_loss = 1.1113, MAI = 0.9615', '\\n', 'Ablation\nlinear_probe Dataset dbpedia_14 Epoch 2: val_loss = 0.5383, MAI = 0.9790', '\\n',\n'Ablation linear_probe Dataset dbpedia_14 Epoch 3: val_loss = 0.3517, MAI =\n0.9834', '\\n', 'Execution time: a minute seconds (time limit is an hour).']", "['\\rMap:   0%|          | 0/2000 [00:00<?, ? examples/s]', '\\rMap:  50%|#####\n| 1000/2000 [00:00<00:00, 8759.25 examples/s]', '', '\\rMap: 100%|##########|\n2000/2000 [00:00<00:00, 9684.87 examples/s]', '\\n', '\\rMap:   0%|          |\n0/500 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500\n[00:00<00:00, 11682.78 examples/s]', '\\n', 'cls_pooling | ag_news | Epoch 1 ->\nval_loss: 0.4909, val_acc: 0.8720, val_align: 0.9975, MAI: 0.9305', '\\n',\n'cls_pooling | ag_news | Epoch 2 -> val_loss: 0.3699, val_acc: 0.8800,\nval_align: 0.9992, MAI: 0.9358', '\\n', 'cls_pooling | ag_news | Epoch 3 ->\nval_loss: 0.3472, val_acc: 0.8920, val_align: 0.9995, MAI: 0.9427', '\\n',\n'\\rMap:   0%|          | 0/2000 [00:00<?, ? examples/s]', '\\rMap:  50%|#####\n| 1000/2000 [00:00<00:00, 6937.97 examples/s]', '\\rMap: 100%|##########|\n2000/2000 [00:00<00:00, 7408.09 examples/s]', '', '\\rMap: 100%|##########|\n2000/2000 [00:00<00:00, 7232.29 examples/s]', '\\n', '\\rMap:   0%|          |\n0/500 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500\n[00:00<00:00, 7370.84 examples/s]', '\\n', 'cls_pooling | yelp_polarity | Epoch 1\n-> val_loss: 0.4694, val_acc: 0.8300, val_align: 0.9978, MAI: 0.9062', '\\n',\n'cls_pooling | yelp_polarity | Epoch 2 -> val_loss: 0.3836, val_acc: 0.8340,\nval_align: 0.9998, MAI: 0.9094', '\\n', 'cls_pooling | yelp_polarity | Epoch 3 ->\nval_loss: 0.3789, val_acc: 0.8260, val_align: 0.9999, MAI: 0.9047', '\\n',\n'\\rMap:   0%|          | 0/2000 [00:00<?, ? examples/s]', '\\rMap:\n100%|##########| 2000/2000 [00:00<00:00, 10629.98 examples/s]', '', '\\rMap:\n100%|##########| 2000/2000 [00:00<00:00, 10451.60 examples/s]', '\\n', '\\rMap:\n0%|          | 0/500 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########|\n500/500 [00:00<00:00, 11201.96 examples/s]', '\\n', 'cls_pooling | dbpedia_14 |\nEpoch 1 -> val_loss: 0.7949, val_acc: 0.9140, val_align: 0.9906, MAI: 0.9508',\n'\\n', 'cls_pooling | dbpedia_14 | Epoch 2 -> val_loss: 0.2998, val_acc: 0.9640,\nval_align: 0.9960, MAI: 0.9797', '\\n', 'cls_pooling | dbpedia_14 | Epoch 3 ->\nval_loss: 0.1728, val_acc: 0.9740, val_align: 0.9978, MAI: 0.9858', '\\n',\n'\\rMap:   0%|          | 0/2000 [00:00<?, ? examples/s]', '\\rMap:\n100%|##########| 2000/2000 [00:00<00:00, 11917.97 examples/s]', '', '\\rMap:\n100%|##########| 2000/2000 [00:00<00:00, 11724.91 examples/s]', '\\n',\n'mean_pooling | ag_news | Epoch 1 -> val_loss: 0.4990, val_acc: 0.8640,\nval_align: 0.9980, MAI: 0.9262', '\\n', 'mean_pooling | ag_news | Epoch 2 ->\nval_loss: 0.3696, val_acc: 0.8780, val_align: 0.9993, MAI: 0.9347', '\\n',\n'mean_pooling | ag_news | Epoch 3 -> val_loss: 0.3537, val_acc: 0.8880,\nval_align: 0.9996, MAI: 0.9405', '\\n', 'mean_pooling | yelp_polarity | Epoch 1\n-> val_loss: 0.5121, val_acc: 0.7980, val_align: 0.9982, MAI: 0.8870', '\\n',\n'mean_pooling | yelp_polarity | Epoch 2 -> val_loss: 0.4058, val_acc: 0.8700,\nval_align: 0.9979, MAI: 0.9296', '\\n', 'mean_pooling | yelp_polarity | Epoch 3\n-> val_loss: 0.3617, val_acc: 0.8560, val_align: 0.9979, MAI: 0.9215', '\\n',\n'mean_pooling | dbpedia_14 | Epoch 1 -> val_loss: 0.9669, val_acc: 0.8240,\nval_align: 0.9881, MAI: 0.8986', '\\n', 'mean_pooling | dbpedia_14 | Epoch 2 ->\nval_loss: 0.3804, val_acc: 0.9540, val_align: 0.9945, MAI: 0.9738', '\\n',\n'mean_pooling | dbpedia_14 | Epoch 3 -> val_loss: 0.2339, val_acc: 0.9540,\nval_align: 0.9969, MAI: 0.9750', '\\n', 'Saved experiment_data.npy', '\\n',\n'Execution time: a minute seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\rMap:   0%|          | 0/500 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00, 6616.18\nexamples/s]', '\\n', 'Ablation baseline Dataset ag_news Epoch 1: validation_loss\n= 0.4909, Bidirectional Alignment = 0.9975, MAI = 0.9305', '\\n', 'Ablation\nbaseline Dataset ag_news Epoch 2: validation_loss = 0.3699, Bidirectional\nAlignment = 0.9992, MAI = 0.9358', '\\n', 'Ablation baseline Dataset ag_news\nEpoch 3: validation_loss = 0.3472, Bidirectional Alignment = 0.9995, MAI =\n0.9427', '\\n', 'Ablation baseline Dataset yelp_polarity Epoch 1: validation_loss\n= 0.4694, Bidirectional Alignment = 0.9978, MAI = 0.9062', '\\n', 'Ablation\nbaseline Dataset yelp_polarity Epoch 2: validation_loss = 0.3836, Bidirectional\nAlignment = 0.9998, MAI = 0.9094', '\\n', 'Ablation baseline Dataset\nyelp_polarity Epoch 3: validation_loss = 0.3789, Bidirectional Alignment =\n0.9999, MAI = 0.9047', '\\n', 'Ablation baseline Dataset dbpedia_14 Epoch 1:\nvalidation_loss = 0.7949, Bidirectional Alignment = 0.9906, MAI = 0.9508', '\\n',\n'Ablation baseline Dataset dbpedia_14 Epoch 2: validation_loss = 0.2998,\nBidirectional Alignment = 0.9960, MAI = 0.9797', '\\n', 'Ablation baseline\nDataset dbpedia_14 Epoch 3: validation_loss = 0.1728, Bidirectional Alignment =\n0.9978, MAI = 0.9858', '\\n', 'Ablation fine_tune Dataset ag_news Epoch 1:\nvalidation_loss = 0.3529, Bidirectional Alignment = 0.9984, MAI = 0.9466', '\\n',\n'Ablation fine_tune Dataset ag_news Epoch 2: validation_loss = 0.3221,\nBidirectional Alignment = 0.9995, MAI = 0.9427', '\\n', 'Ablation fine_tune\nDataset ag_news Epoch 3: validation_loss = 0.3631, Bidirectional Alignment =\n0.9998, MAI = 0.9372', '\\n', 'Ablation fine_tune Dataset yelp_polarity Epoch 1:\nvalidation_loss = 0.3069, Bidirectional Alignment = 0.9993, MAI = 0.9221', '\\n',\n'Ablation fine_tune Dataset yelp_polarity Epoch 2: validation_loss = 0.3712,\nBidirectional Alignment = 0.9997, MAI = 0.9200', '\\n', 'Ablation fine_tune\nDataset yelp_polarity Epoch 3: validation_loss = 0.3324, Bidirectional Alignment\n= 0.9998, MAI = 0.9281', '\\n', 'Ablation fine_tune Dataset dbpedia_14 Epoch 1:\nvalidation_loss = 0.0715, Bidirectional Alignment = 0.9988, MAI = 0.9913', '\\n',\n'Ablation fine_tune Dataset dbpedia_14 Epoch 2: validation_loss = 0.0583,\nBidirectional Alignment = 0.9994, MAI = 0.9947', '\\n', 'Ablation fine_tune\nDataset dbpedia_14 Epoch 3: validation_loss = 0.0765, Bidirectional Alignment =\n0.9997, MAI = 0.9928', '\\n', 'Execution time: 2 minutes seconds (time limit is\nan hour).']", "['\\rMap:   0%|          | 0/500 [00:00<?, ? examples/s]', '', '\\rMap:\n100%|##########| 500/500 [00:00<00:00, 8265.26 examples/s]', '\\n', 'layer_1 |\nag_news | Epoch 1 | val_loss=1.3647 MAI=0.4682', '\\n', 'layer_1 | ag_news |\nEpoch 2 | val_loss=1.3388 MAI=0.4984', '\\n', 'layer_1 | ag_news | Epoch 3 |\nval_loss=1.2714 MAI=0.6630', '\\n', 'layer_1 | yelp_polarity | Epoch 1 |\nval_loss=0.6896 MAI=0.7008', '\\n', 'layer_1 | yelp_polarity | Epoch 2 |\nval_loss=0.7090 MAI=0.6298', '\\n', 'layer_1 | yelp_polarity | Epoch 3 |\nval_loss=0.6818 MAI=0.6447', '\\n', 'layer_1 | dbpedia_14 | Epoch 1 |\nval_loss=2.6128 MAI=0.1549', '\\n', 'layer_1 | dbpedia_14 | Epoch 2 |\nval_loss=2.5591 MAI=0.2639', '\\n', 'layer_1 | dbpedia_14 | Epoch 3 |\nval_loss=2.4574 MAI=0.4075', '\\n', 'layer_3 | ag_news | Epoch 1 |\nval_loss=0.6983 MAI=0.9036', '\\n', 'layer_3 | ag_news | Epoch 2 |\nval_loss=0.5403 MAI=0.8923', '\\n', 'layer_3 | ag_news | Epoch 3 |\nval_loss=0.4550 MAI=0.9151', '\\n', 'layer_3 | yelp_polarity | Epoch 1 |\nval_loss=0.6577 MAI=0.7700', '\\n', 'layer_3 | yelp_polarity | Epoch 2 |\nval_loss=0.5765 MAI=0.8329', '\\n', 'layer_3 | yelp_polarity | Epoch 3 |\nval_loss=0.5485 MAI=0.8490', '\\n', 'layer_3 | dbpedia_14 | Epoch 1 |\nval_loss=1.5002 MAI=0.7394', '\\n', 'layer_3 | dbpedia_14 | Epoch 2 |\nval_loss=0.8730 MAI=0.8756', '\\n', 'layer_3 | dbpedia_14 | Epoch 3 |\nval_loss=0.6152 MAI=0.9257', '\\n', 'layer_5 | ag_news | Epoch 1 |\nval_loss=0.3621 MAI=0.9334', '\\n', 'layer_5 | ag_news | Epoch 2 |\nval_loss=0.3419 MAI=0.9348', '\\n', 'layer_5 | ag_news | Epoch 3 |\nval_loss=0.3390 MAI=0.9527', '\\n', 'layer_5 | yelp_polarity | Epoch 1 |\nval_loss=0.4919 MAI=0.8633', '\\n', 'layer_5 | yelp_polarity | Epoch 2 |\nval_loss=0.4475 MAI=0.8836', '\\n', 'layer_5 | yelp_polarity | Epoch 3 |\nval_loss=0.4565 MAI=0.8848', '\\n', 'layer_5 | dbpedia_14 | Epoch 1 |\nval_loss=0.8121 MAI=0.9081', '\\n', 'layer_5 | dbpedia_14 | Epoch 2 |\nval_loss=0.3978 MAI=0.9492', '\\n', 'layer_5 | dbpedia_14 | Epoch 3 |\nval_loss=0.2946 MAI=0.9622', '\\n', 'avg_last2 | ag_news | Epoch 1 |\nval_loss=0.3841 MAI=0.9407', '\\n', 'avg_last2 | ag_news | Epoch 2 |\nval_loss=0.3472 MAI=0.9471', '\\n', 'avg_last2 | ag_news | Epoch 3 |\nval_loss=0.3276 MAI=0.9505', '\\n', 'avg_last2 | yelp_polarity | Epoch 1 |\nval_loss=0.5503 MAI=0.8830', '\\n', 'avg_last2 | yelp_polarity | Epoch 2 |\nval_loss=0.4536 MAI=0.8883', '\\n', 'avg_last2 | yelp_polarity | Epoch 3 |\nval_loss=0.4247 MAI=0.9031', '\\n', 'avg_last2 | dbpedia_14 | Epoch 1 |\nval_loss=0.9453 MAI=0.9059', '\\n', 'avg_last2 | dbpedia_14 | Epoch 2 |\nval_loss=0.4011 MAI=0.9663', '\\n', 'avg_last2 | dbpedia_14 | Epoch 3 |\nval_loss=0.2518 MAI=0.9758', '\\n', 'Execution time: a minute seconds (time limit\nis an hour).']", "['Dropout 0.0, Dataset ag_news, Epoch 1: val_loss=0.4909, MAI=0.9305', '\\n',\n'Dropout 0.0, Dataset ag_news, Epoch 2: val_loss=0.3699, MAI=0.9358', '\\n',\n'Dropout 0.0, Dataset ag_news, Epoch 3: val_loss=0.3472, MAI=0.9427', '\\n',\n'\\rMap:   0%|          | 0/2000 [00:00<?, ? examples/s]', '\\rMap:  50%|#####\n| 1000/2000 [00:00<00:00, 6254.28 examples/s]', '\\rMap: 100%|##########|\n2000/2000 [00:00<00:00, 7176.09 examples/s]', '', '\\rMap: 100%|##########|\n2000/2000 [00:00<00:00, 6936.30 examples/s]', '\\n', 'Dropout 0.0, Dataset\nyelp_polarity, Epoch 1: val_loss=0.4694, MAI=0.9062', '\\n', 'Dropout 0.0,\nDataset yelp_polarity, Epoch 2: val_loss=0.3836, MAI=0.9094', '\\n', 'Dropout\n0.0, Dataset yelp_polarity, Epoch 3: val_loss=0.3789, MAI=0.9047', '\\n',\n'Dropout 0.0, Dataset dbpedia_14, Epoch 1: val_loss=0.7949, MAI=0.9508', '\\n',\n'Dropout 0.0, Dataset dbpedia_14, Epoch 2: val_loss=0.2998, MAI=0.9797', '\\n',\n'Dropout 0.0, Dataset dbpedia_14, Epoch 3: val_loss=0.1728, MAI=0.9858', '\\n',\n'Dropout 0.1, Dataset ag_news, Epoch 1: val_loss=0.4568, MAI=0.9264', '\\n',\n'Dropout 0.1, Dataset ag_news, Epoch 2: val_loss=0.3574, MAI=0.9422', '\\n',\n'Dropout 0.1, Dataset ag_news, Epoch 3: val_loss=0.3426, MAI=0.9416', '\\n',\n'Dropout 0.1, Dataset yelp_polarity, Epoch 1: val_loss=0.4832, MAI=0.9031',\n'\\n', 'Dropout 0.1, Dataset yelp_polarity, Epoch 2: val_loss=0.3800,\nMAI=0.9188', '\\n', 'Dropout 0.1, Dataset yelp_polarity, Epoch 3:\nval_loss=0.3469, MAI=0.9246', '\\n', 'Dropout 0.1, Dataset dbpedia_14, Epoch 1:\nval_loss=0.8433, MAI=0.9292', '\\n', 'Dropout 0.1, Dataset dbpedia_14, Epoch 2:\nval_loss=0.3050, MAI=0.9811', '\\n', 'Dropout 0.1, Dataset dbpedia_14, Epoch 3:\nval_loss=0.1857, MAI=0.9865', '\\n', 'Dropout 0.3, Dataset ag_news, Epoch 1:\nval_loss=0.4460, MAI=0.9263', '\\n', 'Dropout 0.3, Dataset ag_news, Epoch 2:\nval_loss=0.3632, MAI=0.9334', '\\n', 'Dropout 0.3, Dataset ag_news, Epoch 3:\nval_loss=0.3356, MAI=0.9390', '\\n', 'Dropout 0.3, Dataset yelp_polarity, Epoch\n1: val_loss=0.4292, MAI=0.8418', '\\n', 'Dropout 0.3, Dataset yelp_polarity,\nEpoch 2: val_loss=0.3567, MAI=0.9116', '\\n', 'Dropout 0.3, Dataset\nyelp_polarity, Epoch 3: val_loss=0.3402, MAI=0.9253', '\\n', 'Dropout 0.3,\nDataset dbpedia_14, Epoch 1: val_loss=0.9195, MAI=0.9413', '\\n', 'Dropout 0.3,\nDataset dbpedia_14, Epoch 2: val_loss=0.3557, MAI=0.9779', '\\n', 'Dropout 0.3,\nDataset dbpedia_14, Epoch 3: val_loss=0.2077, MAI=0.9827', '\\n', 'Dropout 0.5,\nDataset ag_news, Epoch 1: val_loss=0.4905, MAI=0.9124', '\\n', 'Dropout 0.5,\nDataset ag_news, Epoch 2: val_loss=0.3771, MAI=0.9325', '\\n', 'Dropout 0.5,\nDataset ag_news, Epoch 3: val_loss=0.3501, MAI=0.9361', '\\n', 'Dropout 0.5,\nDataset yelp_polarity, Epoch 1: val_loss=0.5265, MAI=0.9083', '\\n', 'Dropout\n0.5, Dataset yelp_polarity, Epoch 2: val_loss=0.4048, MAI=0.9181', '\\n',\n'Dropout 0.5, Dataset yelp_polarity, Epoch 3: val_loss=0.3614, MAI=0.9298',\n'\\n', 'Dropout 0.5, Dataset dbpedia_14, Epoch 1: val_loss=1.2288, MAI=0.9402',\n'\\n', 'Dropout 0.5, Dataset dbpedia_14, Epoch 2: val_loss=0.5272, MAI=0.9763',\n'\\n', 'Dropout 0.5, Dataset dbpedia_14, Epoch 3: val_loss=0.3048, MAI=0.9813',\n'\\n', 'Execution time: 2 minutes seconds (time limit is an hour).']", "['Activation ReLU Dataset ag_news Epoch 1: validation_loss = 0.4909, MAI =\n0.9305', '\\n', 'Activation ReLU Dataset ag_news Epoch 2: validation_loss =\n0.3699, MAI = 0.9358', '\\n', 'Activation ReLU Dataset ag_news Epoch 3:\nvalidation_loss = 0.3472, MAI = 0.9427', '\\n', '\\rMap:   0%|          | 0/500\n[00:00<?, ? examples/s]', '\\rMap: 100%|##########| 500/500 [00:00<00:00, 4090.59\nexamples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00, 4005.46\nexamples/s]', '\\n', 'Activation ReLU Dataset yelp_polarity Epoch 1:\nvalidation_loss = 0.4694, MAI = 0.9062', '\\n', 'Activation ReLU Dataset\nyelp_polarity Epoch 2: validation_loss = 0.3836, MAI = 0.9094', '\\n',\n'Activation ReLU Dataset yelp_polarity Epoch 3: validation_loss = 0.3789, MAI =\n0.9047', '\\n', 'Activation ReLU Dataset dbpedia_14 Epoch 1: validation_loss =\n0.7949, MAI = 0.9508', '\\n', 'Activation ReLU Dataset dbpedia_14 Epoch 2:\nvalidation_loss = 0.2998, MAI = 0.9797', '\\n', 'Activation ReLU Dataset\ndbpedia_14 Epoch 3: validation_loss = 0.1728, MAI = 0.9858', '\\n', 'Activation\nGELU Dataset ag_news Epoch 1: validation_loss = 0.3879, MAI = 0.9267', '\\n',\n'Activation GELU Dataset ag_news Epoch 2: validation_loss = 0.3500, MAI =\n0.9449', '\\n', 'Activation GELU Dataset ag_news Epoch 3: validation_loss =\n0.3461, MAI = 0.9438', '\\n', 'Activation GELU Dataset yelp_polarity Epoch 1:\nvalidation_loss = 0.4041, MAI = 0.9221', '\\n', 'Activation GELU Dataset\nyelp_polarity Epoch 2: validation_loss = 0.3865, MAI = 0.9069', '\\n',\n'Activation GELU Dataset yelp_polarity Epoch 3: validation_loss = 0.3511, MAI =\n0.9212', '\\n', 'Activation GELU Dataset dbpedia_14 Epoch 1: validation_loss =\n0.4844, MAI = 0.9547', '\\n', 'Activation GELU Dataset dbpedia_14 Epoch 2:\nvalidation_loss = 0.1753, MAI = 0.9856', '\\n', 'Activation GELU Dataset\ndbpedia_14 Epoch 3: validation_loss = 0.1281, MAI = 0.9870', '\\n', 'Activation\nTanh Dataset ag_news Epoch 1: validation_loss = 0.6016, MAI = 0.9251', '\\n',\n'Activation Tanh Dataset ag_news Epoch 2: validation_loss = 0.3948, MAI =\n0.9356', '\\n', 'Activation Tanh Dataset ag_news Epoch 3: validation_loss =\n0.3440, MAI = 0.9437', '\\n', 'Activation Tanh Dataset yelp_polarity Epoch 1:\nvalidation_loss = 0.4896, MAI = 0.8918', '\\n', 'Activation Tanh Dataset\nyelp_polarity Epoch 2: validation_loss = 0.3683, MAI = 0.9195', '\\n',\n'Activation Tanh Dataset yelp_polarity Epoch 3: validation_loss = 0.3478, MAI =\n0.9209', '\\n', 'Activation Tanh Dataset dbpedia_14 Epoch 1: validation_loss =\n1.3182, MAI = 0.9430', '\\n', 'Activation Tanh Dataset dbpedia_14 Epoch 2:\nvalidation_loss = 0.4292, MAI = 0.9799', '\\n', 'Activation Tanh Dataset\ndbpedia_14 Epoch 3: validation_loss = 0.2170, MAI = 0.9866', '\\n', 'Activation\nLeakyReLU Dataset ag_news Epoch 1: validation_loss = 0.4251, MAI = 0.9328',\n'\\n', 'Activation LeakyReLU Dataset ag_news Epoch 2: validation_loss = 0.3587,\nMAI = 0.9392', '\\n', 'Activation LeakyReLU Dataset ag_news Epoch 3:\nvalidation_loss = 0.3372, MAI = 0.9405', '\\n', 'Activation LeakyReLU Dataset\nyelp_polarity Epoch 1: validation_loss = 0.4667, MAI = 0.9055', '\\n',\n'Activation LeakyReLU Dataset yelp_polarity Epoch 2: validation_loss = 0.3712,\nMAI = 0.9069', '\\n', 'Activation LeakyReLU Dataset yelp_polarity Epoch 3:\nvalidation_loss = 0.3485, MAI = 0.9221', '\\n', 'Activation LeakyReLU Dataset\ndbpedia_14 Epoch 1: validation_loss = 0.8374, MAI = 0.9384', '\\n', 'Activation\nLeakyReLU Dataset dbpedia_14 Epoch 2: validation_loss = 0.3014, MAI = 0.9799',\n'\\n', 'Activation LeakyReLU Dataset dbpedia_14 Epoch 3: validation_loss =\n0.1719, MAI = 0.9843', '\\n', 'Execution time: 2 minutes seconds (time limit is\nan hour).']", "['Using device: cuda', '\\n', '\\n=== Starting ablation lambda_0_0 ===', '\\n',\n'\\nDataset ag_news, \u03bb=0.0', '\\n', '  Ep1: train_loss=2.0862, val_loss=0.4909,\nval_acc=0.8720, val_align=0.9975, MAI=0.9305', '\\n', '  Ep2: train_loss=0.4418,\nval_loss=0.3699, val_acc=0.8800, val_align=0.9992, MAI=0.9358', '\\n', '  Ep3:\ntrain_loss=0.3672, val_loss=0.3472, val_acc=0.8920, val_align=0.9995,\nMAI=0.9427', '\\n', '\\nDataset yelp_polarity, \u03bb=0.0', '\\n', '  Ep1:\ntrain_loss=1.8066, val_loss=0.4694, val_acc=0.8300, val_align=0.9978,\nMAI=0.9062', '\\n', '  Ep2: train_loss=0.4048, val_loss=0.3836, val_acc=0.8340,\nval_align=0.9998, MAI=0.9094', '\\n', '  Ep3: train_loss=0.3435, val_loss=0.3789,\nval_acc=0.8260, val_align=0.9999, MAI=0.9047', '\\n', '\\nDataset dbpedia_14,\n\u03bb=0.0', '\\n', '\\rMap:   0%|          | 0/2000 [00:00<?, ? examples/s]', '\\rMap:\n50%|#####     | 1000/2000 [00:00<00:00, 9595.29 examples/s]', '\\rMap:\n100%|##########| 2000/2000 [00:00<00:00, 9472.10 examples/s]', '', '\\rMap:\n100%|##########| 2000/2000 [00:00<00:00, 9343.24 examples/s]', '\\n', '  Ep1:\ntrain_loss=2.6756, val_loss=0.7949, val_acc=0.9140, val_align=0.9906,\nMAI=0.9508', '\\n', '  Ep2: train_loss=0.4782, val_loss=0.2998, val_acc=0.9640,\nval_align=0.9960, MAI=0.9797', '\\n', '  Ep3: train_loss=0.2103, val_loss=0.1728,\nval_acc=0.9740, val_align=0.9978, MAI=0.9858', '\\n', '\\n=== Starting ablation\nlambda_0_1 ===', '\\n', '\\nDataset ag_news, \u03bb=0.1', '\\n', '  Ep1:\ntrain_loss=2.0871, val_loss=0.4919, val_acc=0.8700, val_align=0.9974,\nMAI=0.9293', '\\n', '  Ep2: train_loss=0.4421, val_loss=0.3701, val_acc=0.8800,\nval_align=0.9993, MAI=0.9358', '\\n', '  Ep3: train_loss=0.3674, val_loss=0.3472,\nval_acc=0.8920, val_align=0.9995, MAI=0.9427', '\\n', '\\nDataset yelp_polarity,\n\u03bb=0.1', '\\n', '  Ep1: train_loss=1.8051, val_loss=0.4678, val_acc=0.8300,\nval_align=0.9980, MAI=0.9063', '\\n', '  Ep2: train_loss=0.4043, val_loss=0.3838,\nval_acc=0.8380, val_align=0.9998, MAI=0.9118', '\\n', '  Ep3: train_loss=0.3441,\nval_loss=0.3790, val_acc=0.8280, val_align=0.9999, MAI=0.9059', '\\n', '\\nDataset\ndbpedia_14, \u03bb=0.1', '\\n', '  Ep1: train_loss=2.6754, val_loss=0.7951,\nval_acc=0.9180, val_align=0.9907, MAI=0.9530', '\\n', '  Ep2: train_loss=0.4779,\nval_loss=0.2994, val_acc=0.9640, val_align=0.9961, MAI=0.9798', '\\n', '  Ep3:\ntrain_loss=0.2100, val_loss=0.1726, val_acc=0.9740, val_align=0.9979,\nMAI=0.9858', '\\n', '\\n=== Starting ablation lambda_0_5 ===', '\\n', '\\nDataset\nag_news, \u03bb=0.5', '\\n', '  Ep1: train_loss=2.0899, val_loss=0.4928,\nval_acc=0.8680, val_align=0.9979, MAI=0.9284', '\\n', '  Ep2: train_loss=0.4437,\nval_loss=0.3707, val_acc=0.8800, val_align=0.9994, MAI=0.9359', '\\n', '  Ep3:\ntrain_loss=0.3682, val_loss=0.3472, val_acc=0.8920, val_align=0.9996,\nMAI=0.9427', '\\n', '\\nDataset yelp_polarity, \u03bb=0.5', '\\n', '  Ep1:\ntrain_loss=1.8000, val_loss=0.4686, val_acc=0.8460, val_align=0.9995,\nMAI=0.9164', '\\n', '  Ep2: train_loss=0.4020, val_loss=0.3847, val_acc=0.8420,\nval_align=0.9997, MAI=0.9141', '\\n', '  Ep3: train_loss=0.3442, val_loss=0.3764,\nval_acc=0.8300, val_align=0.9999, MAI=0.9071', '\\n', '\\nDataset dbpedia_14,\n\u03bb=0.5', '\\n', '  Ep1: train_loss=2.6736, val_loss=0.7934, val_acc=0.9220,\nval_align=0.9912, MAI=0.9554', '\\n', '  Ep2: train_loss=0.4761, val_loss=0.2974,\nval_acc=0.9640, val_align=0.9967, MAI=0.9801', '\\n', '  Ep3: train_loss=0.2083,\nval_loss=0.1715, val_acc=0.9740, val_align=0.9982, MAI=0.9860', '\\n', '\\n===\nStarting ablation lambda_1_0 ===', '\\n', '\\nDataset ag_news, \u03bb=1.0', '\\n', '\nEp1: train_loss=2.0925, val_loss=0.4926, val_acc=0.8600, val_align=0.9980,\nMAI=0.9239', '\\n', '  Ep2: train_loss=0.4476, val_loss=0.3724, val_acc=0.8880,\nval_align=0.9994, MAI=0.9404', '\\n', '  Ep3: train_loss=0.3705, val_loss=0.3478,\nval_acc=0.8940, val_align=0.9994, MAI=0.9438', '\\n', '\\nDataset yelp_polarity,\n\u03bb=1.0', '\\n', '  Ep1: train_loss=1.7923, val_loss=0.4777, val_acc=0.8480,\nval_align=0.9959, MAI=0.9160', '\\n', '  Ep2: train_loss=0.3970, val_loss=0.3776,\nval_acc=0.8400, val_align=0.9996, MAI=0.9129', '\\n', '  Ep3: train_loss=0.3378,\nval_loss=0.3697, val_acc=0.8360, val_align=0.9998, MAI=0.9106', '\\n', '\\nDataset\ndbpedia_14, \u03bb=1.0', '\\n', '  Ep1: train_loss=2.6711, val_loss=0.7911,\nval_acc=0.9220, val_align=0.9926, MAI=0.9560', '\\n', '  Ep2: train_loss=0.4749,\nval_loss=0.2962, val_acc=0.9640, val_align=0.9972, MAI=0.9803', '\\n', '  Ep3:\ntrain_loss=0.2076, val_loss=0.1709, val_acc=0.9740, val_align=0.9985,\nMAI=0.9861', '\\n', '\\nSaved experiment_data.npy', '\\n', 'Execution time: 2\nminutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\rMap:   0%|          | 0/2000 [00:00<?, ?\nexamples/s]', '\\rMap:  50%|#####     | 1000/2000 [00:00<00:00, 6774.71\nexamples/s]', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 7377.28\nexamples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 7196.78\nexamples/s]', '\\n', 'layer_1 | ag_news | Epoch 1: validation_loss = 1.3647,\nBidirectional Alignment = 0.7263', '\\n', 'layer_1 | ag_news | Epoch 2:\nvalidation_loss = 1.3388, Bidirectional Alignment = 0.7337', '\\n', 'layer_1 |\nag_news | Epoch 3: validation_loss = 1.2714, Bidirectional Alignment = 0.7436',\n'\\n', 'layer_1 | yelp_polarity | Epoch 1: validation_loss = 0.6896,\nBidirectional Alignment = 0.8447', '\\n', 'layer_1 | yelp_polarity | Epoch 2:\nvalidation_loss = 0.7090, Bidirectional Alignment = 0.8410', '\\n', 'layer_1 |\nyelp_polarity | Epoch 3: validation_loss = 0.6818, Bidirectional Alignment =\n0.8461', '\\n', 'layer_1 | dbpedia_14 | Epoch 1: validation_loss = 2.6128,\nBidirectional Alignment = 0.5959', '\\n', 'layer_1 | dbpedia_14 | Epoch 2:\nvalidation_loss = 2.5591, Bidirectional Alignment = 0.6013', '\\n', 'layer_1 |\ndbpedia_14 | Epoch 3: validation_loss = 2.4574, Bidirectional Alignment =\n0.6088', '\\n', 'layer_3 | ag_news | Epoch 1: validation_loss = 0.6983,\nBidirectional Alignment = 0.8489', '\\n', 'layer_3 | ag_news | Epoch 2:\nvalidation_loss = 0.5403, Bidirectional Alignment = 0.8874', '\\n', 'layer_3 |\nag_news | Epoch 3: validation_loss = 0.4550, Bidirectional Alignment = 0.9078',\n'\\n', 'layer_3 | yelp_polarity | Epoch 1: validation_loss = 0.6577,\nBidirectional Alignment = 0.8528', '\\n', 'layer_3 | yelp_polarity | Epoch 2:\nvalidation_loss = 0.5765, Bidirectional Alignment = 0.8700', '\\n', 'layer_3 |\nyelp_polarity | Epoch 3: validation_loss = 0.5485, Bidirectional Alignment =\n0.8773', '\\n', 'layer_3 | dbpedia_14 | Epoch 1: validation_loss = 1.5002,\nBidirectional Alignment = 0.7202', '\\n', 'layer_3 | dbpedia_14 | Epoch 2:\nvalidation_loss = 0.8730, Bidirectional Alignment = 0.8140', '\\n', 'layer_3 |\ndbpedia_14 | Epoch 3: validation_loss = 0.6152, Bidirectional Alignment =\n0.8674', '\\n', 'layer_5 | ag_news | Epoch 1: validation_loss = 0.3621,\nBidirectional Alignment = 0.9277', '\\n', 'layer_5 | ag_news | Epoch 2:\nvalidation_loss = 0.3419, Bidirectional Alignment = 0.9356', '\\n', 'layer_5 |\nag_news | Epoch 3: validation_loss = 0.3390, Bidirectional Alignment = 0.9405',\n'\\n', 'layer_5 | yelp_polarity | Epoch 1: validation_loss = 0.4919,\nBidirectional Alignment = 0.8893', '\\n', 'layer_5 | yelp_polarity | Epoch 2:\nvalidation_loss = 0.4475, Bidirectional Alignment = 0.9020', '\\n', 'layer_5 |\nyelp_polarity | Epoch 3: validation_loss = 0.4565, Bidirectional Alignment =\n0.9054', '\\n', 'layer_5 | dbpedia_14 | Epoch 1: validation_loss = 0.8121,\nBidirectional Alignment = 0.8301', '\\n', 'layer_5 | dbpedia_14 | Epoch 2:\nvalidation_loss = 0.3978, Bidirectional Alignment = 0.9124', '\\n', 'layer_5 |\ndbpedia_14 | Epoch 3: validation_loss = 0.2946, Bidirectional Alignment =\n0.9383', '\\n', 'avg_last2 | ag_news | Epoch 1: validation_loss = 0.3841,\nBidirectional Alignment = 0.9187', '\\n', 'avg_last2 | ag_news | Epoch 2:\nvalidation_loss = 0.3472, Bidirectional Alignment = 0.9324', '\\n', 'avg_last2 |\nag_news | Epoch 3: validation_loss = 0.3276, Bidirectional Alignment = 0.9399',\n'\\n', 'avg_last2 | yelp_polarity | Epoch 1: validation_loss = 0.5503,\nBidirectional Alignment = 0.8732', '\\n', 'avg_last2 | yelp_polarity | Epoch 2:\nvalidation_loss = 0.4536, Bidirectional Alignment = 0.8982', '\\n', 'avg_last2 |\nyelp_polarity | Epoch 3: validation_loss = 0.4247, Bidirectional Alignment =\n0.9079', '\\n', 'avg_last2 | dbpedia_14 | Epoch 1: validation_loss = 0.9453,\nBidirectional Alignment = 0.7999', '\\n', 'avg_last2 | dbpedia_14 | Epoch 2:\nvalidation_loss = 0.4011, Bidirectional Alignment = 0.9098', '\\n', 'avg_last2 |\ndbpedia_14 | Epoch 3: validation_loss = 0.2518, Bidirectional Alignment =\n0.9426', '\\n', 'Execution time: a minute seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Ablation focal_loss_gamma_1 Dataset ag_news Epoch\n1: val_loss=0.3010, MAI=0.9200', '\\n', 'Ablation focal_loss_gamma_1 Dataset\nag_news Epoch 2: val_loss=0.2316, MAI=0.9380', '\\n', 'Ablation\nfocal_loss_gamma_1 Dataset ag_news Epoch 3: val_loss=0.2147, MAI=0.9448', '\\n',\n'\\rMap:   0%|          | 0/2000 [00:00<?, ? examples/s]', '\\rMap:  50%|#####\n| 1000/2000 [00:00<00:00, 7500.89 examples/s]', '\\rMap: 100%|##########|\n2000/2000 [00:00<00:00, 7879.70 examples/s]', '', '\\rMap: 100%|##########|\n2000/2000 [00:00<00:00, 7725.15 examples/s]', '\\n', 'Ablation focal_loss_gamma_1\nDataset yelp_polarity Epoch 1: val_loss=0.2193, MAI=0.9045', '\\n', 'Ablation\nfocal_loss_gamma_1 Dataset yelp_polarity Epoch 2: val_loss=0.1913, MAI=0.9176',\n'\\n', 'Ablation focal_loss_gamma_1 Dataset yelp_polarity Epoch 3:\nval_loss=0.1958, MAI=0.9010', '\\n', 'Ablation focal_loss_gamma_1 Dataset\ndbpedia_14 Epoch 1: val_loss=0.5096, MAI=0.9578', '\\n', 'Ablation\nfocal_loss_gamma_1 Dataset dbpedia_14 Epoch 2: val_loss=0.1629, MAI=0.9825',\n'\\n', 'Ablation focal_loss_gamma_1 Dataset dbpedia_14 Epoch 3: val_loss=0.0967,\nMAI=0.9854', '\\n', 'Ablation focal_loss_gamma_2 Dataset ag_news Epoch 1:\nval_loss=0.1796, MAI=0.9319', '\\n', 'Ablation focal_loss_gamma_2 Dataset ag_news\nEpoch 2: val_loss=0.1503, MAI=0.9427', '\\n', 'Ablation focal_loss_gamma_2\nDataset ag_news Epoch 3: val_loss=0.1472, MAI=0.9405', '\\n', 'Ablation\nfocal_loss_gamma_2 Dataset yelp_polarity Epoch 1: val_loss=0.1071, MAI=0.9199',\n'\\n', 'Ablation focal_loss_gamma_2 Dataset yelp_polarity Epoch 2:\nval_loss=0.1003, MAI=0.8974', '\\n', 'Ablation focal_loss_gamma_2 Dataset\nyelp_polarity Epoch 3: val_loss=0.0934, MAI=0.9058', '\\n', 'Ablation\nfocal_loss_gamma_2 Dataset dbpedia_14 Epoch 1: val_loss=0.3895, MAI=0.9365',\n'\\n', 'Ablation focal_loss_gamma_2 Dataset dbpedia_14 Epoch 2: val_loss=0.1116,\nMAI=0.9806', '\\n', 'Ablation focal_loss_gamma_2 Dataset dbpedia_14 Epoch 3:\nval_loss=0.0736, MAI=0.9844', '\\n', 'Ablation focal_loss_gamma_5 Dataset ag_news\nEpoch 1: val_loss=0.0602, MAI=0.9286', '\\n', 'Ablation focal_loss_gamma_5\nDataset ag_news Epoch 2: val_loss=0.0491, MAI=0.9277', '\\n', 'Ablation\nfocal_loss_gamma_5 Dataset ag_news Epoch 3: val_loss=0.0496, MAI=0.9458', '\\n',\n'Ablation focal_loss_gamma_5 Dataset yelp_polarity Epoch 1: val_loss=0.0129,\nMAI=0.8607', '\\n', 'Ablation focal_loss_gamma_5 Dataset yelp_polarity Epoch 2:\nval_loss=0.0123, MAI=0.9118', '\\n', 'Ablation focal_loss_gamma_5 Dataset\nyelp_polarity Epoch 3: val_loss=0.0125, MAI=0.9223', '\\n', 'Ablation\nfocal_loss_gamma_5 Dataset dbpedia_14 Epoch 1: val_loss=0.1908, MAI=0.9643',\n'\\n', 'Ablation focal_loss_gamma_5 Dataset dbpedia_14 Epoch 2: val_loss=0.0609,\nMAI=0.9793', '\\n', 'Ablation focal_loss_gamma_5 Dataset dbpedia_14 Epoch 3:\nval_loss=0.0389, MAI=0.9838', '\\n', 'Execution time: a minute seconds (time\nlimit is an hour).']", "['\\rMap:   0%|          | 0/500 [00:00<?, ? examples/s]', '', '\\rMap:\n100%|##########| 500/500 [00:00<00:00, 5123.26 examples/s]', '\\n',\n'DistilBertSdpaAttention is used but\n`torch.nn.functional.scaled_dot_product_attention` does not support\n`output_attentions=True` or `head_mask`. Falling back to the manual attention\nimplementation, but specifying the manual implementation will be required from\nTransformers version v5.0.0 onwards. This warning can be removed using the\nargument `attn_implementation=\"eager\"` when loading the model.\\n', 'Ablation\nrandom heads=12 ds=ag_news epoch=1: val_loss=0.4909, MAI=0.9305', '\\n',\n'Ablation random heads=12 ds=ag_news epoch=2: val_loss=0.3699, MAI=0.9358',\n'\\n', 'Ablation random heads=12 ds=ag_news epoch=3: val_loss=0.3472,\nMAI=0.9427', '\\n', 'Ablation random heads=12 ds=yelp_polarity epoch=1:\nval_loss=0.4694, MAI=0.9062', '\\n', 'Ablation random heads=12 ds=yelp_polarity\nepoch=2: val_loss=0.3836, MAI=0.9094', '\\n', 'Ablation random heads=12\nds=yelp_polarity epoch=3: val_loss=0.3789, MAI=0.9047', '\\n', 'Ablation random\nheads=12 ds=dbpedia_14 epoch=1: val_loss=0.7949, MAI=0.9508', '\\n', 'Ablation\nrandom heads=12 ds=dbpedia_14 epoch=2: val_loss=0.2998, MAI=0.9797', '\\n',\n'Ablation random heads=12 ds=dbpedia_14 epoch=3: val_loss=0.1728, MAI=0.9858',\n'\\n', 'Ablation random heads=8 ds=ag_news epoch=1: val_loss=0.5863, MAI=0.9277',\n'\\n', 'Ablation random heads=8 ds=ag_news epoch=2: val_loss=0.4053, MAI=0.9359',\n'\\n', 'Ablation random heads=8 ds=ag_news epoch=3: val_loss=0.3749, MAI=0.9348',\n'\\n', 'Ablation random heads=8 ds=yelp_polarity epoch=1: val_loss=0.5724,\nMAI=0.8855', '\\n', 'Ablation random heads=8 ds=yelp_polarity epoch=2:\nval_loss=0.4724, MAI=0.9031', '\\n', 'Ablation random heads=8 ds=yelp_polarity\nepoch=3: val_loss=0.4189, MAI=0.9078', '\\n', 'Ablation random heads=8\nds=dbpedia_14 epoch=1: val_loss=1.3080, MAI=0.7788', '\\n', 'Ablation random\nheads=8 ds=dbpedia_14 epoch=2: val_loss=0.5771, MAI=0.9483', '\\n', 'Ablation\nrandom heads=8 ds=dbpedia_14 epoch=3: val_loss=0.3700, MAI=0.9508', '\\n',\n'Ablation random heads=4 ds=ag_news epoch=1: val_loss=0.7241, MAI=0.9123', '\\n',\n'Ablation random heads=4 ds=ag_news epoch=2: val_loss=0.4780, MAI=0.9163', '\\n',\n'Ablation random heads=4 ds=ag_news epoch=3: val_loss=0.4145, MAI=0.9257', '\\n',\n'Ablation random heads=4 ds=yelp_polarity epoch=1: val_loss=0.5591, MAI=0.8658',\n'\\n', 'Ablation random heads=4 ds=yelp_polarity epoch=2: val_loss=0.4713,\nMAI=0.8741', '\\n', 'Ablation random heads=4 ds=yelp_polarity epoch=3:\nval_loss=0.4375, MAI=0.8744', '\\n', 'Ablation random heads=4 ds=dbpedia_14\nepoch=1: val_loss=1.7329, MAI=0.8215', '\\n', 'Ablation random heads=4\nds=dbpedia_14 epoch=2: val_loss=1.0001, MAI=0.8845', '\\n', 'Ablation random\nheads=4 ds=dbpedia_14 epoch=3: val_loss=0.6665, MAI=0.9271', '\\n', 'Ablation\nrandom heads=2 ds=ag_news epoch=1: val_loss=1.2540, MAI=0.7507', '\\n', 'Ablation\nrandom heads=2 ds=ag_news epoch=2: val_loss=1.0397, MAI=0.7693', '\\n', 'Ablation\nrandom heads=2 ds=ag_news epoch=3: val_loss=0.8388, MAI=0.8624', '\\n', 'Ablation\nrandom heads=2 ds=yelp_polarity epoch=1: val_loss=0.6875, MAI=0.7012', '\\n',\n'Ablation random heads=2 ds=yelp_polarity epoch=2: val_loss=0.6862, MAI=0.7893',\n'\\n', 'Ablation random heads=2 ds=yelp_polarity epoch=3: val_loss=0.6924,\nMAI=0.6541', '\\n', 'Ablation random heads=2 ds=dbpedia_14 epoch=1:\nval_loss=2.5695, MAI=0.2816', '\\n', 'Ablation random heads=2 ds=dbpedia_14\nepoch=2: val_loss=2.4080, MAI=0.7041', '\\n', 'Ablation random heads=2\nds=dbpedia_14 epoch=3: val_loss=2.2051, MAI=0.7419', '\\n', 'Ablation importance\nheads=12 ds=ag_news epoch=1: val_loss=0.4742, MAI=0.9304', '\\n', 'Ablation\nimportance heads=12 ds=ag_news epoch=2: val_loss=0.4028, MAI=0.9196', '\\n',\n'Ablation importance heads=12 ds=ag_news epoch=3: val_loss=0.3453, MAI=0.9416',\n'\\n', 'Ablation importance heads=12 ds=yelp_polarity epoch=1: val_loss=0.5017,\nMAI=0.8962', '\\n', 'Ablation importance heads=12 ds=yelp_polarity epoch=2:\nval_loss=0.3813, MAI=0.9281', '\\n', 'Ablation importance heads=12\nds=yelp_polarity epoch=3: val_loss=0.3458, MAI=0.9258', '\\n', 'Ablation\nimportance heads=12 ds=dbpedia_14 epoch=1: val_loss=0.8658, MAI=0.9340', '\\n',\n'Ablation importance heads=12 ds=dbpedia_14 epoch=2: val_loss=0.3182,\nMAI=0.9800', '\\n', 'Ablation importance heads=12 ds=dbpedia_14 epoch=3:\nval_loss=0.1858, MAI=0.9843', '\\n', 'Ablation importance heads=8 ds=ag_news\nepoch=1: val_loss=0.5288, MAI=0.9225', '\\n', 'Ablation importance heads=8\nds=ag_news epoch=2: val_loss=0.3699, MAI=0.9390', '\\n', 'Ablation importance\nheads=8 ds=ag_news epoch=3: val_loss=0.3456, MAI=0.9380', '\\n', 'Ablation\nimportance heads=8 ds=yelp_polarity epoch=1: val_loss=0.4883, MAI=0.8962', '\\n',\n'Ablation importance heads=8 ds=yelp_polarity epoch=2: val_loss=0.4057,\nMAI=0.9200', '\\n', 'Ablation importance heads=8 ds=yelp_polarity epoch=3:\nval_loss=0.3776, MAI=0.9224', '\\n', 'Ablation importance heads=8 ds=dbpedia_14\nepoch=1: val_loss=1.1154, MAI=0.9128', '\\n', 'Ablation importance heads=8\nds=dbpedia_14 epoch=2: val_loss=0.4633, MAI=0.9617', '\\n', 'Ablation importance\nheads=8 ds=dbpedia_14 epoch=3: val_loss=0.2765, MAI=0.9750', '\\n', 'Ablation\nimportance heads=4 ds=ag_news epoch=1: val_loss=0.6024, MAI=0.8937', '\\n',\n'Ablation importance heads=4 ds=ag_news epoch=2: val_loss=0.4210, MAI=0.9257',\n'\\n', 'Ablation importance heads=4 ds=ag_news epoch=3: val_loss=0.4165,\nMAI=0.9200', '\\n', 'Ablation importance heads=4 ds=yelp_polarity epoch=1:\nval_loss=0.6136, MAI=0.7362', '\\n', 'Ablation importance heads=4\nds=yelp_polarity epoch=2: val_loss=0.5811, MAI=0.8009', '\\n', 'Ablation\nimportance heads=4 ds=yelp_polarity epoch=3: val_loss=0.5182, MAI=0.8662', '\\n',\n'Ablation importance heads=4 ds=dbpedia_14 epoch=1: val_loss=1.5968,\nMAI=0.8303', '\\n', 'Ablation importance heads=4 ds=dbpedia_14 epoch=2:\nval_loss=0.7733, MAI=0.9341', '\\n', 'Ablation importance heads=4 ds=dbpedia_14\nepoch=3: val_loss=0.4764, MAI=0.9501', '\\n', 'Ablation importance heads=2\nds=ag_news epoch=1: val_loss=1.1841, MAI=0.8235', '\\n', 'Ablation importance\nheads=2 ds=ag_news epoch=2: val_loss=0.8761, MAI=0.9124', '\\n', 'Ablation\nimportance heads=2 ds=ag_news epoch=3: val_loss=0.6726, MAI=0.9078', '\\n',\n'Ablation importance heads=2 ds=yelp_polarity epoch=1: val_loss=0.6840,\nMAI=0.6376', '\\n', 'Ablation importance heads=2 ds=yelp_polarity epoch=2:\nval_loss=0.6464, MAI=0.7453', '\\n', 'Ablation importance heads=2\nds=yelp_polarity epoch=3: val_loss=0.6216, MAI=0.8221', '\\n', 'Ablation\nimportance heads=2 ds=dbpedia_14 epoch=1: val_loss=2.2167, MAI=0.6063', '\\n',\n'Ablation importance heads=2 ds=dbpedia_14 epoch=2: val_loss=1.5614,\nMAI=0.8512', '\\n', 'Ablation importance heads=2 ds=dbpedia_14 epoch=3:\nval_loss=1.0876, MAI=0.8648', '\\n', 'Execution time: 3 minutes seconds (time\nlimit is an hour).']", "['Using device: cuda', '\\n', 'Ablation=pretrained Dataset=ag_news Epoch=1\nval_loss=0.4909 MAI=0.9305', '\\n', 'Ablation=pretrained Dataset=ag_news Epoch=2\nval_loss=0.3699 MAI=0.9358', '\\n', 'Ablation=pretrained Dataset=ag_news Epoch=3\nval_loss=0.3472 MAI=0.9427', '\\n', 'Ablation=pretrained Dataset=yelp_polarity\nEpoch=1 val_loss=0.4694 MAI=0.9062', '\\n', 'Ablation=pretrained\nDataset=yelp_polarity Epoch=2 val_loss=0.3836 MAI=0.9094', '\\n',\n'Ablation=pretrained Dataset=yelp_polarity Epoch=3 val_loss=0.3789 MAI=0.9047',\n'\\n', '\\rMap:   0%|          | 0/500 [00:00<?, ? examples/s]', '', '\\rMap:\n100%|##########| 500/500 [00:00<00:00, 6220.55 examples/s]', '\\n',\n'Ablation=pretrained Dataset=dbpedia_14 Epoch=1 val_loss=0.7949 MAI=0.9508',\n'\\n', 'Ablation=pretrained Dataset=dbpedia_14 Epoch=2 val_loss=0.2998\nMAI=0.9797', '\\n', 'Ablation=pretrained Dataset=dbpedia_14 Epoch=3\nval_loss=0.1728 MAI=0.9858', '\\n', 'Ablation=random_init Dataset=ag_news Epoch=1\nval_loss=1.4549 MAI=0.4075', '\\n', 'Ablation=random_init Dataset=ag_news Epoch=2\nval_loss=1.3956 MAI=0.4070', '\\n', 'Ablation=random_init Dataset=ag_news Epoch=3\nval_loss=1.4047 MAI=0.3896', '\\n', 'Ablation=random_init Dataset=yelp_polarity\nEpoch=1 val_loss=0.6975 MAI=0.6541', '\\n', 'Ablation=random_init\nDataset=yelp_polarity Epoch=2 val_loss=0.7163 MAI=0.6338', '\\n',\n'Ablation=random_init Dataset=yelp_polarity Epoch=3 val_loss=0.6791 MAI=0.7146',\n'\\n', 'Ablation=random_init Dataset=dbpedia_14 Epoch=1 val_loss=2.6385\nMAI=0.1377', '\\n', 'Ablation=random_init Dataset=dbpedia_14 Epoch=2\nval_loss=2.5803 MAI=0.1343', '\\n', 'Ablation=random_init Dataset=dbpedia_14\nEpoch=3 val_loss=2.5436 MAI=0.3044', '\\n', 'Execution time: a minute seconds\n(time limit is an hour).']", "['token_dropout_0 ag_news Epoch 1: val_loss=0.4909, val_acc=0.8720,\nval_align=0.9975, MAI=0.9305', '\\n', 'token_dropout_0 ag_news Epoch 2:\nval_loss=0.3699, val_acc=0.8800, val_align=0.9992, MAI=0.9358', '\\n',\n'token_dropout_0 ag_news Epoch 3: val_loss=0.3472, val_acc=0.8920,\nval_align=0.9995, MAI=0.9427', '\\n', 'token_dropout_0 yelp_polarity Epoch 1:\nval_loss=0.4694, val_acc=0.8300, val_align=0.9978, MAI=0.9062', '\\n',\n'token_dropout_0 yelp_polarity Epoch 2: val_loss=0.3836, val_acc=0.8340,\nval_align=0.9998, MAI=0.9094', '\\n', 'token_dropout_0 yelp_polarity Epoch 3:\nval_loss=0.3789, val_acc=0.8260, val_align=0.9999, MAI=0.9047', '\\n',\n'token_dropout_0 dbpedia_14 Epoch 1: val_loss=0.7949, val_acc=0.9140,\nval_align=0.9906, MAI=0.9508', '\\n', 'token_dropout_0 dbpedia_14 Epoch 2:\nval_loss=0.2998, val_acc=0.9640, val_align=0.9960, MAI=0.9797', '\\n',\n'token_dropout_0 dbpedia_14 Epoch 3: val_loss=0.1728, val_acc=0.9740,\nval_align=0.9978, MAI=0.9858', '\\n', 'token_dropout_10 ag_news Epoch 1:\nval_loss=0.5189, val_acc=0.8960, val_align=0.9976, MAI=0.9441', '\\n',\n'token_dropout_10 ag_news Epoch 2: val_loss=0.3804, val_acc=0.8940,\nval_align=0.9993, MAI=0.9437', '\\n', 'token_dropout_10 ag_news Epoch 3:\nval_loss=0.3660, val_acc=0.8840, val_align=0.9996, MAI=0.9383', '\\n',\n'token_dropout_10 yelp_polarity Epoch 1: val_loss=0.5193, val_acc=0.8220,\nval_align=0.9996, MAI=0.9021', '\\n', 'token_dropout_10 yelp_polarity Epoch 2:\nval_loss=0.4201, val_acc=0.8260, val_align=0.9995, MAI=0.9045', '\\n',\n'token_dropout_10 yelp_polarity Epoch 3: val_loss=0.3901, val_acc=0.8440,\nval_align=0.9994, MAI=0.9151', '\\n', 'token_dropout_10 dbpedia_14 Epoch 1:\nval_loss=1.1507, val_acc=0.7420, val_align=0.9895, MAI=0.8481', '\\n',\n'token_dropout_10 dbpedia_14 Epoch 2: val_loss=0.4226, val_acc=0.9640,\nval_align=0.9944, MAI=0.9790', '\\n', 'token_dropout_10 dbpedia_14 Epoch 3:\nval_loss=0.2426, val_acc=0.9700, val_align=0.9964, MAI=0.9830', '\\n',\n'token_dropout_20 ag_news Epoch 1: val_loss=0.4870, val_acc=0.8660,\nval_align=0.9967, MAI=0.9268', '\\n', 'token_dropout_20 ag_news Epoch 2:\nval_loss=0.3841, val_acc=0.8740, val_align=0.9990, MAI=0.9323', '\\n',\n'token_dropout_20 ag_news Epoch 3: val_loss=0.3520, val_acc=0.8760,\nval_align=0.9995, MAI=0.9337', '\\n', 'token_dropout_20 yelp_polarity Epoch 1:\nval_loss=0.5239, val_acc=0.6740, val_align=0.9982, MAI=0.8047', '\\n',\n'token_dropout_20 yelp_polarity Epoch 2: val_loss=0.4554, val_acc=0.7800,\nval_align=0.9996, MAI=0.8762', '\\n', 'token_dropout_20 yelp_polarity Epoch 3:\nval_loss=0.4120, val_acc=0.8100, val_align=0.9997, MAI=0.8949', '\\n',\n'token_dropout_20 dbpedia_14 Epoch 1: val_loss=1.1643, val_acc=0.8560,\nval_align=0.9898, MAI=0.9180', '\\n', 'token_dropout_20 dbpedia_14 Epoch 2:\nval_loss=0.4284, val_acc=0.9380, val_align=0.9945, MAI=0.9654', '\\n',\n'token_dropout_20 dbpedia_14 Epoch 3: val_loss=0.2362, val_acc=0.9640,\nval_align=0.9969, MAI=0.9802', '\\n', 'token_dropout_30 ag_news Epoch 1:\nval_loss=0.5246, val_acc=0.8520, val_align=0.9978, MAI=0.9192', '\\n',\n'token_dropout_30 ag_news Epoch 2: val_loss=0.4019, val_acc=0.8740,\nval_align=0.9997, MAI=0.9326', '\\n', 'token_dropout_30 ag_news Epoch 3:\nval_loss=0.3617, val_acc=0.8820, val_align=0.9997, MAI=0.9372', '\\n',\n'token_dropout_30 yelp_polarity Epoch 1: val_loss=0.5692, val_acc=0.7900,\nval_align=0.9971, MAI=0.8815', '\\n', 'token_dropout_30 yelp_polarity Epoch 2:\nval_loss=0.4888, val_acc=0.7660, val_align=0.9997, MAI=0.8674', '\\n',\n'token_dropout_30 yelp_polarity Epoch 3: val_loss=0.4453, val_acc=0.7920,\nval_align=0.9996, MAI=0.8838', '\\n', 'token_dropout_30 dbpedia_14 Epoch 1:\nval_loss=1.2037, val_acc=0.8660, val_align=0.9902, MAI=0.9239', '\\n',\n'token_dropout_30 dbpedia_14 Epoch 2: val_loss=0.4820, val_acc=0.9480,\nval_align=0.9944, MAI=0.9706', '\\n', 'token_dropout_30 dbpedia_14 Epoch 3:\nval_loss=0.2835, val_acc=0.9400, val_align=0.9969, MAI=0.9676', '\\n', 'Execution\ntime: 2 minutes seconds (time limit is an hour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 117, in\n<module>\\n    loss_user.backward()\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/_tensor.py\", line 581, in backward\\n\ntorch.autograd.backward(\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/autograd/__init__.py\", line 347, in backward\\n\n_engine_run_backward(\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/autograd/graph.py\", line 825, in _engine_run_backward\\n    return\nVariable._execution_engine.run_backward(  # Calls into the C++ engine to run the\nbackward pass\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: Trying to backward\nthrough the graph a second time (or directly access saved tensors after they\nhave already been freed). Saved intermediate values of the graph are freed when\nyou call .backward() or autograd.grad(). Specify retain_graph=True if you need\nto backward through the graph a second time or if you need to access saved\ntensors after calling backward.\\n', 'Execution time: 11 seconds seconds (time\nlimit is an hour).']", "['Using device: cuda', '\\n', 'Opt adam | ag_news | Epoch 1 | val_loss=0.4909\nval_align=0.9975 val_acc=0.8720 MAI=0.9305', '\\n', 'Opt adam | ag_news | Epoch 2\n| val_loss=0.3699 val_align=0.9992 val_acc=0.8800 MAI=0.9358', '\\n', 'Opt adam |\nag_news | Epoch 3 | val_loss=0.3472 val_align=0.9995 val_acc=0.8920 MAI=0.9427',\n'\\n', 'Opt adam | yelp_polarity | Epoch 1 | val_loss=0.4694 val_align=0.9978\nval_acc=0.8300 MAI=0.9062', '\\n', 'Opt adam | yelp_polarity | Epoch 2 |\nval_loss=0.3836 val_align=0.9998 val_acc=0.8340 MAI=0.9094', '\\n', 'Opt adam |\nyelp_polarity | Epoch 3 | val_loss=0.3789 val_align=0.9999 val_acc=0.8260\nMAI=0.9047', '\\n', 'Opt adam | dbpedia_14 | Epoch 1 | val_loss=0.7949\nval_align=0.9906 val_acc=0.9140 MAI=0.9508', '\\n', 'Opt adam | dbpedia_14 |\nEpoch 2 | val_loss=0.2998 val_align=0.9960 val_acc=0.9640 MAI=0.9797', '\\n',\n'Opt adam | dbpedia_14 | Epoch 3 | val_loss=0.1728 val_align=0.9978\nval_acc=0.9740 MAI=0.9858', '\\n', 'Opt sgd | ag_news | Epoch 1 | val_loss=1.3424\nval_align=0.9944 val_acc=0.2900 MAI=0.4490', '\\n', 'Opt sgd | ag_news | Epoch 2\n| val_loss=1.1837 val_align=0.9992 val_acc=0.7520 MAI=0.8582', '\\n', 'Opt sgd |\nag_news | Epoch 3 | val_loss=1.0416 val_align=0.9989 val_acc=0.7200 MAI=0.8368',\n'\\n', 'Opt sgd | yelp_polarity | Epoch 1 | val_loss=0.6760 val_align=0.9998\nval_acc=0.5500 MAI=0.7096', '\\n', 'Opt sgd | yelp_polarity | Epoch 2 |\nval_loss=0.6597 val_align=0.9999 val_acc=0.7720 MAI=0.8713', '\\n', 'Opt sgd |\nyelp_polarity | Epoch 3 | val_loss=0.6379 val_align=0.9998 val_acc=0.8100\nMAI=0.8950', '\\n', 'Opt sgd | dbpedia_14 | Epoch 1 | val_loss=5.2848\nval_align=0.9421 val_acc=0.1480 MAI=0.2558', '\\n', 'Opt sgd | dbpedia_14 | Epoch\n2 | val_loss=2.4859 val_align=0.9939 val_acc=0.1600 MAI=0.2756', '\\n', 'Opt sgd\n| dbpedia_14 | Epoch 3 | val_loss=2.3031 val_align=0.9957 val_acc=0.4380\nMAI=0.6084', '\\n', 'Opt adamw | ag_news | Epoch 1 | val_loss=0.4298\nval_align=0.9975 val_acc=0.8660 MAI=0.9271', '\\n', 'Opt adamw | ag_news | Epoch\n2 | val_loss=0.3666 val_align=0.9995 val_acc=0.8720 MAI=0.9314', '\\n', 'Opt\nadamw | ag_news | Epoch 3 | val_loss=0.3395 val_align=0.9996 val_acc=0.9040\nMAI=0.9494', '\\n', 'Opt adamw | yelp_polarity | Epoch 1 | val_loss=0.4244\nval_align=0.9971 val_acc=0.8360 MAI=0.9095', '\\n', 'Opt adamw | yelp_polarity |\nEpoch 2 | val_loss=0.3524 val_align=0.9982 val_acc=0.8520 MAI=0.9193', '\\n',\n'Opt adamw | yelp_polarity | Epoch 3 | val_loss=0.3548 val_align=0.9987\nval_acc=0.8520 MAI=0.9195', '\\n', 'Opt adamw | dbpedia_14 | Epoch 1 |\nval_loss=0.8492 val_align=0.9881 val_acc=0.8780 MAI=0.9298', '\\n', 'Opt adamw |\ndbpedia_14 | Epoch 2 | val_loss=0.3103 val_align=0.9943 val_acc=0.9620\nMAI=0.9779', '\\n', 'Opt adamw | dbpedia_14 | Epoch 3 | val_loss=0.1857\nval_align=0.9965 val_acc=0.9740 MAI=0.9851', '\\n', 'Execution time: a minute\nseconds (time limit is an hour).']", "['\\rMap:   0%|          | 0/2000 [00:00<?, ? examples/s]', '\\rMap:  50%|#####\n| 1000/2000 [00:00<00:00, 8587.88 examples/s]', '', '\\rMap: 100%|##########|\n2000/2000 [00:00<00:00, 9601.19 examples/s]', '\\n', '\\rMap:   0%|          |\n0/500 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500\n[00:00<00:00, 11922.27 examples/s]', '\\n', 'Ablation independent Dataset ag_news\nEpoch 1: validation_loss=0.4909, MAI=0.9305', '\\n', 'Ablation independent\nDataset ag_news Epoch 2: validation_loss=0.3699, MAI=0.9358', '\\n', 'Ablation\nindependent Dataset ag_news Epoch 3: validation_loss=0.3472, MAI=0.9427', '\\n',\n'\\rMap:   0%|          | 0/2000 [00:00<?, ? examples/s]', '\\rMap:  50%|#####\n| 1000/2000 [00:00<00:00, 8230.55 examples/s]', '\\rMap: 100%|##########|\n2000/2000 [00:00<00:00, 4178.20 examples/s]', '', '\\rMap: 100%|##########|\n2000/2000 [00:00<00:00, 4481.19 examples/s]', '\\n', '\\rMap:   0%|          |\n0/500 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 500/500\n[00:00<00:00, 7347.70 examples/s]', '\\n', 'Ablation independent Dataset\nyelp_polarity Epoch 1: validation_loss=0.4694, MAI=0.9062', '\\n', 'Ablation\nindependent Dataset yelp_polarity Epoch 2: validation_loss=0.3836, MAI=0.9094',\n'\\n', 'Ablation independent Dataset yelp_polarity Epoch 3:\nvalidation_loss=0.3789, MAI=0.9047', '\\n', '\\rMap:   0%|          | 0/2000\n[00:00<?, ? examples/s]', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00,\n10890.58 examples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00,\n10710.45 examples/s]', '\\n', '\\rMap:   0%|          | 0/500 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 500/500 [00:00<00:00, 11206.99\nexamples/s]', '\\n', 'Ablation independent Dataset dbpedia_14 Epoch 1:\nvalidation_loss=0.7949, MAI=0.9508', '\\n', 'Ablation independent Dataset\ndbpedia_14 Epoch 2: validation_loss=0.2998, MAI=0.9797', '\\n', 'Ablation\nindependent Dataset dbpedia_14 Epoch 3: validation_loss=0.1728, MAI=0.9858',\n'\\n', '\\rMap:   0%|          | 0/2000 [00:00<?, ? examples/s]', '\\rMap:\n100%|##########| 2000/2000 [00:00<00:00, 11699.90 examples/s]', '', '\\rMap:\n100%|##########| 2000/2000 [00:00<00:00, 11512.71 examples/s]', '\\n', 'Ablation\nshared_fc1 Dataset ag_news Epoch 1: validation_loss=0.5571, MAI=0.9248', '\\n',\n'Ablation shared_fc1 Dataset ag_news Epoch 2: validation_loss=0.3832,\nMAI=0.9365', '\\n', 'Ablation shared_fc1 Dataset ag_news Epoch 3:\nvalidation_loss=0.3568, MAI=0.9403', '\\n', 'Ablation shared_fc1 Dataset\nyelp_polarity Epoch 1: validation_loss=0.5623, MAI=0.9186', '\\n', 'Ablation\nshared_fc1 Dataset yelp_polarity Epoch 2: validation_loss=0.4080, MAI=0.9187',\n'\\n', 'Ablation shared_fc1 Dataset yelp_polarity Epoch 3:\nvalidation_loss=0.3651, MAI=0.9246', '\\n', 'Ablation shared_fc1 Dataset\ndbpedia_14 Epoch 1: validation_loss=1.2018, MAI=0.9140', '\\n', 'Ablation\nshared_fc1 Dataset dbpedia_14 Epoch 2: validation_loss=0.4215, MAI=0.9740',\n'\\n', 'Ablation shared_fc1 Dataset dbpedia_14 Epoch 3: validation_loss=0.2299,\nMAI=0.9835', '\\n', 'Ablation shared_fc1_fc2 Dataset ag_news Epoch 1:\nvalidation_loss=0.4934, MAI=0.9189', '\\n', 'Ablation shared_fc1_fc2 Dataset\nag_news Epoch 2: validation_loss=0.3650, MAI=0.9362', '\\n', 'Ablation\nshared_fc1_fc2 Dataset ag_news Epoch 3: validation_loss=0.3451, MAI=0.9418',\n'\\n', 'Ablation shared_fc1_fc2 Dataset yelp_polarity Epoch 1:\nvalidation_loss=0.4478, MAI=0.9201', '\\n', 'Ablation shared_fc1_fc2 Dataset\nyelp_polarity Epoch 2: validation_loss=0.3710, MAI=0.9177', '\\n', 'Ablation\nshared_fc1_fc2 Dataset yelp_polarity Epoch 3: validation_loss=0.3574,\nMAI=0.9189', '\\n', 'Ablation shared_fc1_fc2 Dataset dbpedia_14 Epoch 1:\nvalidation_loss=0.9065, MAI=0.9362', '\\n', 'Ablation shared_fc1_fc2 Dataset\ndbpedia_14 Epoch 2: validation_loss=0.3391, MAI=0.9785', '\\n', 'Ablation\nshared_fc1_fc2 Dataset dbpedia_14 Epoch 3: validation_loss=0.1876, MAI=0.9858',\n'\\n', 'Execution time: a minute seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 0.4909, Bidirectional\nMM Alignment = 0.9975', '\\n', 'Epoch 2: validation_loss = 0.3699, Bidirectional\nMM Alignment = 0.9992', '\\n', 'Epoch 3: validation_loss = 0.3472, Bidirectional\nMM Alignment = 0.9995', '\\n', 'Epoch 1: validation_loss = 0.4694, Bidirectional\nMM Alignment = 0.9978', '\\n', 'Epoch 2: validation_loss = 0.3836, Bidirectional\nMM Alignment = 0.9998', '\\n', 'Epoch 3: validation_loss = 0.3789, Bidirectional\nMM Alignment = 0.9999', '\\n', 'Epoch 1: validation_loss = 0.7949, Bidirectional\nMM Alignment = 0.9906', '\\n', 'Epoch 2: validation_loss = 0.2998, Bidirectional\nMM Alignment = 0.9960', '\\n', 'Epoch 3: validation_loss = 0.1728, Bidirectional\nMM Alignment = 0.9978', '\\n', 'Epoch 1: validation_loss = 0.5734, Bidirectional\nMM Alignment = 0.9973', '\\n', 'Epoch 2: validation_loss = 0.4172, Bidirectional\nMM Alignment = 0.9994', '\\n', 'Epoch 3: validation_loss = 0.4008, Bidirectional\nMM Alignment = 0.9997', '\\n', 'Epoch 1: validation_loss = 0.5995, Bidirectional\nMM Alignment = 0.9961', '\\n', 'Epoch 2: validation_loss = 0.5516, Bidirectional\nMM Alignment = 0.9986', '\\n', 'Epoch 3: validation_loss = 0.5340, Bidirectional\nMM Alignment = 0.9995', '\\n', 'Epoch 1: validation_loss = 1.4373, Bidirectional\nMM Alignment = 0.9918', '\\n', 'Epoch 2: validation_loss = 0.6929, Bidirectional\nMM Alignment = 0.9935', '\\n', 'Epoch 3: validation_loss = 0.4422, Bidirectional\nMM Alignment = 0.9960', '\\n', 'Execution time: a minute seconds (time limit is\nan hour).']", "['Using device:', ' ', 'cuda', '\\n', 'T=0.5 ag_news Epoch 1: val_loss=0.4909,\nval_acc=0.8720, MAI=0.9301', '\\n', 'T=0.5 ag_news Epoch 2: val_loss=0.3699,\nval_acc=0.8800, MAI=0.9359', '\\n', 'T=0.5 ag_news Epoch 3: val_loss=0.3472,\nval_acc=0.8920, MAI=0.9427', '\\n', 'T=0.5 yelp_polarity Epoch 1:\nval_loss=0.4694, val_acc=0.8300, MAI=0.9046', '\\n', 'T=0.5 yelp_polarity Epoch\n2: val_loss=0.3836, val_acc=0.8340, MAI=0.9094', '\\n', 'T=0.5 yelp_polarity\nEpoch 3: val_loss=0.3789, val_acc=0.8260, MAI=0.9046', '\\n', 'T=0.5 dbpedia_14\nEpoch 1: val_loss=0.7949, val_acc=0.9140, MAI=0.9502', '\\n', 'T=0.5 dbpedia_14\nEpoch 2: val_loss=0.2998, val_acc=0.9640, MAI=0.9807', '\\n', 'T=0.5 dbpedia_14\nEpoch 3: val_loss=0.1728, val_acc=0.9740, MAI=0.9863', '\\n', 'T=1.0 ag_news\nEpoch 1: val_loss=0.4563, val_acc=0.8720, MAI=0.9309', '\\n', 'T=1.0 ag_news\nEpoch 2: val_loss=0.3647, val_acc=0.8940, MAI=0.9438', '\\n', 'T=1.0 ag_news\nEpoch 3: val_loss=0.3498, val_acc=0.8920, MAI=0.9428', '\\n', 'T=1.0\nyelp_polarity Epoch 1: val_loss=0.4816, val_acc=0.8260, MAI=0.9044', '\\n',\n'T=1.0 yelp_polarity Epoch 2: val_loss=0.3761, val_acc=0.8540, MAI=0.9212',\n'\\n', 'T=1.0 yelp_polarity Epoch 3: val_loss=0.3501, val_acc=0.8500,\nMAI=0.9187', '\\n', 'T=1.0 dbpedia_14 Epoch 1: val_loss=0.8254, val_acc=0.8980,\nMAI=0.9418', '\\n', 'T=1.0 dbpedia_14 Epoch 2: val_loss=0.2961, val_acc=0.9760,\nMAI=0.9856', '\\n', 'T=1.0 dbpedia_14 Epoch 3: val_loss=0.1803, val_acc=0.9760,\nMAI=0.9864', '\\n', 'T=2.0 ag_news Epoch 1: val_loss=0.4297, val_acc=0.8660,\nMAI=0.9229', '\\n', 'T=2.0 ag_news Epoch 2: val_loss=0.3665, val_acc=0.8720,\nMAI=0.9272', '\\n', 'T=2.0 ag_news Epoch 3: val_loss=0.3397, val_acc=0.9040,\nMAI=0.9450', '\\n', 'T=2.0 yelp_polarity Epoch 1: val_loss=0.4243,\nval_acc=0.8360, MAI=0.9040', '\\n', 'T=2.0 yelp_polarity Epoch 2:\nval_loss=0.3523, val_acc=0.8520, MAI=0.9142', '\\n', 'T=2.0 yelp_polarity Epoch\n3: val_loss=0.3548, val_acc=0.8520, MAI=0.9147', '\\n', 'T=2.0 dbpedia_14 Epoch\n1: val_loss=0.8485, val_acc=0.8780, MAI=0.9279', '\\n', 'T=2.0 dbpedia_14 Epoch\n2: val_loss=0.3097, val_acc=0.9620, MAI=0.9739', '\\n', 'T=2.0 dbpedia_14 Epoch\n3: val_loss=0.1852, val_acc=0.9740, MAI=0.9810', '\\n', 'Execution time: a minute\nseconds (time limit is an hour).']", "['Using device:', ' ', 'cuda', '\\n', 'T=0.5 ag_news Epoch 1: val_loss=0.4909,\nval_acc=0.8720, MAI=0.9301', '\\n', 'T=0.5 ag_news Epoch 2: val_loss=0.3699,\nval_acc=0.8800, MAI=0.9359', '\\n', 'T=0.5 ag_news Epoch 3: val_loss=0.3472,\nval_acc=0.8920, MAI=0.9427', '\\n', 'T=0.5 yelp_polarity Epoch 1:\nval_loss=0.4694, val_acc=0.8300, MAI=0.9046', '\\n', 'T=0.5 yelp_polarity Epoch\n2: val_loss=0.3836, val_acc=0.8340, MAI=0.9094', '\\n', 'T=0.5 yelp_polarity\nEpoch 3: val_loss=0.3789, val_acc=0.8260, MAI=0.9046', '\\n', 'T=0.5 dbpedia_14\nEpoch 1: val_loss=0.7949, val_acc=0.9140, MAI=0.9502', '\\n', 'T=0.5 dbpedia_14\nEpoch 2: val_loss=0.2998, val_acc=0.9640, MAI=0.9807', '\\n', 'T=0.5 dbpedia_14\nEpoch 3: val_loss=0.1728, val_acc=0.9740, MAI=0.9863', '\\n', 'T=1.0 ag_news\nEpoch 1: val_loss=0.4563, val_acc=0.8720, MAI=0.9309', '\\n', 'T=1.0 ag_news\nEpoch 2: val_loss=0.3647, val_acc=0.8940, MAI=0.9438', '\\n', 'T=1.0 ag_news\nEpoch 3: val_loss=0.3498, val_acc=0.8920, MAI=0.9428', '\\n', 'T=1.0\nyelp_polarity Epoch 1: val_loss=0.4816, val_acc=0.8260, MAI=0.9044', '\\n',\n'T=1.0 yelp_polarity Epoch 2: val_loss=0.3761, val_acc=0.8540, MAI=0.9212',\n'\\n', 'T=1.0 yelp_polarity Epoch 3: val_loss=0.3501, val_acc=0.8500,\nMAI=0.9187', '\\n', 'T=1.0 dbpedia_14 Epoch 1: val_loss=0.8254, val_acc=0.8980,\nMAI=0.9418', '\\n', 'T=1.0 dbpedia_14 Epoch 2: val_loss=0.2961, val_acc=0.9760,\nMAI=0.9856', '\\n', 'T=1.0 dbpedia_14 Epoch 3: val_loss=0.1803, val_acc=0.9760,\nMAI=0.9864', '\\n', 'T=2.0 ag_news Epoch 1: val_loss=0.4297, val_acc=0.8660,\nMAI=0.9229', '\\n', 'T=2.0 ag_news Epoch 2: val_loss=0.3665, val_acc=0.8720,\nMAI=0.9272', '\\n', 'T=2.0 ag_news Epoch 3: val_loss=0.3397, val_acc=0.9040,\nMAI=0.9450', '\\n', 'T=2.0 yelp_polarity Epoch 1: val_loss=0.4243,\nval_acc=0.8360, MAI=0.9040', '\\n', 'T=2.0 yelp_polarity Epoch 2:\nval_loss=0.3523, val_acc=0.8520, MAI=0.9142', '\\n', 'T=2.0 yelp_polarity Epoch\n3: val_loss=0.3548, val_acc=0.8520, MAI=0.9147', '\\n', 'T=2.0 dbpedia_14 Epoch\n1: val_loss=0.8485, val_acc=0.8780, MAI=0.9279', '\\n', 'T=2.0 dbpedia_14 Epoch\n2: val_loss=0.3097, val_acc=0.9620, MAI=0.9739', '\\n', 'T=2.0 dbpedia_14 Epoch\n3: val_loss=0.1852, val_acc=0.9740, MAI=0.9810', '\\n', 'Execution time: a minute\nseconds (time limit is an hour).']", "['Using device:', ' ', 'cuda', '\\n', 'T=0.5 ag_news Epoch 1: val_loss=0.4909,\nval_acc=0.8720, MAI=0.9301', '\\n', 'T=0.5 ag_news Epoch 2: val_loss=0.3699,\nval_acc=0.8800, MAI=0.9359', '\\n', 'T=0.5 ag_news Epoch 3: val_loss=0.3472,\nval_acc=0.8920, MAI=0.9427', '\\n', 'T=0.5 yelp_polarity Epoch 1:\nval_loss=0.4694, val_acc=0.8300, MAI=0.9046', '\\n', 'T=0.5 yelp_polarity Epoch\n2: val_loss=0.3836, val_acc=0.8340, MAI=0.9094', '\\n', 'T=0.5 yelp_polarity\nEpoch 3: val_loss=0.3789, val_acc=0.8260, MAI=0.9046', '\\n', 'T=0.5 dbpedia_14\nEpoch 1: val_loss=0.7949, val_acc=0.9140, MAI=0.9502', '\\n', 'T=0.5 dbpedia_14\nEpoch 2: val_loss=0.2998, val_acc=0.9640, MAI=0.9807', '\\n', 'T=0.5 dbpedia_14\nEpoch 3: val_loss=0.1728, val_acc=0.9740, MAI=0.9863', '\\n', 'T=1.0 ag_news\nEpoch 1: val_loss=0.4563, val_acc=0.8720, MAI=0.9309', '\\n', 'T=1.0 ag_news\nEpoch 2: val_loss=0.3647, val_acc=0.8940, MAI=0.9438', '\\n', 'T=1.0 ag_news\nEpoch 3: val_loss=0.3498, val_acc=0.8920, MAI=0.9428', '\\n', 'T=1.0\nyelp_polarity Epoch 1: val_loss=0.4816, val_acc=0.8260, MAI=0.9044', '\\n',\n'T=1.0 yelp_polarity Epoch 2: val_loss=0.3761, val_acc=0.8540, MAI=0.9212',\n'\\n', 'T=1.0 yelp_polarity Epoch 3: val_loss=0.3501, val_acc=0.8500,\nMAI=0.9187', '\\n', 'T=1.0 dbpedia_14 Epoch 1: val_loss=0.8254, val_acc=0.8980,\nMAI=0.9418', '\\n', 'T=1.0 dbpedia_14 Epoch 2: val_loss=0.2961, val_acc=0.9760,\nMAI=0.9856', '\\n', 'T=1.0 dbpedia_14 Epoch 3: val_loss=0.1803, val_acc=0.9760,\nMAI=0.9864', '\\n', 'T=2.0 ag_news Epoch 1: val_loss=0.4297, val_acc=0.8660,\nMAI=0.9229', '\\n', 'T=2.0 ag_news Epoch 2: val_loss=0.3665, val_acc=0.8720,\nMAI=0.9272', '\\n', 'T=2.0 ag_news Epoch 3: val_loss=0.3397, val_acc=0.9040,\nMAI=0.9450', '\\n', 'T=2.0 yelp_polarity Epoch 1: val_loss=0.4243,\nval_acc=0.8360, MAI=0.9040', '\\n', 'T=2.0 yelp_polarity Epoch 2:\nval_loss=0.3523, val_acc=0.8520, MAI=0.9142', '\\n', 'T=2.0 yelp_polarity Epoch\n3: val_loss=0.3548, val_acc=0.8520, MAI=0.9147', '\\n', 'T=2.0 dbpedia_14 Epoch\n1: val_loss=0.8485, val_acc=0.8780, MAI=0.9279', '\\n', 'T=2.0 dbpedia_14 Epoch\n2: val_loss=0.3097, val_acc=0.9620, MAI=0.9739', '\\n', 'T=2.0 dbpedia_14 Epoch\n3: val_loss=0.1852, val_acc=0.9740, MAI=0.9810', '\\n', 'Execution time: a minute\nseconds (time limit is an hour).']", "['Using device:', ' ', 'cuda', '\\n', 'T=0.5 ag_news Epoch 1: val_loss=0.4909,\nval_acc=0.8720, MAI=0.9301', '\\n', 'T=0.5 ag_news Epoch 2: val_loss=0.3699,\nval_acc=0.8800, MAI=0.9359', '\\n', 'T=0.5 ag_news Epoch 3: val_loss=0.3472,\nval_acc=0.8920, MAI=0.9427', '\\n', 'T=0.5 yelp_polarity Epoch 1:\nval_loss=0.4694, val_acc=0.8300, MAI=0.9046', '\\n', 'T=0.5 yelp_polarity Epoch\n2: val_loss=0.3836, val_acc=0.8340, MAI=0.9094', '\\n', 'T=0.5 yelp_polarity\nEpoch 3: val_loss=0.3789, val_acc=0.8260, MAI=0.9046', '\\n', 'T=0.5 dbpedia_14\nEpoch 1: val_loss=0.7949, val_acc=0.9140, MAI=0.9502', '\\n', 'T=0.5 dbpedia_14\nEpoch 2: val_loss=0.2998, val_acc=0.9640, MAI=0.9807', '\\n', 'T=0.5 dbpedia_14\nEpoch 3: val_loss=0.1728, val_acc=0.9740, MAI=0.9863', '\\n', 'T=1.0 ag_news\nEpoch 1: val_loss=0.4563, val_acc=0.8720, MAI=0.9309', '\\n', 'T=1.0 ag_news\nEpoch 2: val_loss=0.3647, val_acc=0.8940, MAI=0.9438', '\\n', 'T=1.0 ag_news\nEpoch 3: val_loss=0.3498, val_acc=0.8920, MAI=0.9428', '\\n', 'T=1.0\nyelp_polarity Epoch 1: val_loss=0.4816, val_acc=0.8260, MAI=0.9044', '\\n',\n'T=1.0 yelp_polarity Epoch 2: val_loss=0.3761, val_acc=0.8540, MAI=0.9212',\n'\\n', 'T=1.0 yelp_polarity Epoch 3: val_loss=0.3501, val_acc=0.8500,\nMAI=0.9187', '\\n', 'T=1.0 dbpedia_14 Epoch 1: val_loss=0.8254, val_acc=0.8980,\nMAI=0.9418', '\\n', 'T=1.0 dbpedia_14 Epoch 2: val_loss=0.2961, val_acc=0.9760,\nMAI=0.9856', '\\n', 'T=1.0 dbpedia_14 Epoch 3: val_loss=0.1803, val_acc=0.9760,\nMAI=0.9864', '\\n', 'T=2.0 ag_news Epoch 1: val_loss=0.4297, val_acc=0.8660,\nMAI=0.9229', '\\n', 'T=2.0 ag_news Epoch 2: val_loss=0.3665, val_acc=0.8720,\nMAI=0.9272', '\\n', 'T=2.0 ag_news Epoch 3: val_loss=0.3397, val_acc=0.9040,\nMAI=0.9450', '\\n', 'T=2.0 yelp_polarity Epoch 1: val_loss=0.4243,\nval_acc=0.8360, MAI=0.9040', '\\n', 'T=2.0 yelp_polarity Epoch 2:\nval_loss=0.3523, val_acc=0.8520, MAI=0.9142', '\\n', 'T=2.0 yelp_polarity Epoch\n3: val_loss=0.3548, val_acc=0.8520, MAI=0.9147', '\\n', 'T=2.0 dbpedia_14 Epoch\n1: val_loss=0.8485, val_acc=0.8780, MAI=0.9279', '\\n', 'T=2.0 dbpedia_14 Epoch\n2: val_loss=0.3097, val_acc=0.9620, MAI=0.9739', '\\n', 'T=2.0 dbpedia_14 Epoch\n3: val_loss=0.1852, val_acc=0.9740, MAI=0.9810', '\\n', 'Execution time: 2\nminutes seconds (time limit is an hour).']", ""], "analysis": ["The code ran successfully with no runtime errors. Training and validation losses\ndecreased as expected across epochs, and alignment scores behaved reasonably,\npeaking around learning rates 5e-3 and 1e-2. No bugs were detected.", "The fine-tune ablation run appears unstable: on AG News, val_loss jumps to 4.55\nin epoch 1 (vs ~0.5 in baseline) before settling around 1.4, and on DBpedia the\nloss spikes to ~2.8 in epoch 2 with MAI collapsing to 0.11. This suggests\ndivergence when jointly fine-tuning the DistilBERT backbone with a 1e-3 learning\nrate. To fix it, lower the transformer\u2019s learning rate (e.g., 2e-5), use\nseparate optimizers or learning rates for the backbone vs. heads, add a\nscheduler or warm-up, and consider gradient clipping to stabilize training.", "", "The script ran without errors and completed all experiments. Two pooling\nstrategies (CLS pooling and Mean pooling) were evaluated on three datasets (AG\nNews, Yelp Polarity, DBpedia-14) over three epochs. All models converged to high\nalignment scores (near 1.0) as expected since both AI and user models were\ntrained on the same labels. CLS pooling showed slightly better performance on AG\nNews (val_acc up to 0.892) and DBpedia-14 (up to 0.974) while Mean pooling had a\nsmall advantage on Yelp Polarity (up to 0.856). The results were saved\nsuccessfully in `experiment_data.npy`. No runtime bugs were detected.", "", "", "", "", "", "", "", "", "The script executed without any runtime errors on CUDA as expected. It ran 3\nepochs of ablation studies for two model initializations (pretrained vs.\nrandom_init) on three datasets (AG News, Yelp Polarity, DBpedia-14). Results\nshow that using a pretrained DistilBERT backbone yields substantially lower\nvalidation losses and much higher MAI scores compared to a randomly initialized\nbackbone. Specifically, pretrained models achieved val_loss reductions from\n~0.49\u21920.35 (AG News), ~0.47\u21920.38 (Yelp), and ~0.79\u21920.17 (DBpedia) with MAI\nrising to ~0.94, 0.91, and 0.99, respectively. Randomly initialized models\nremained far weaker (val_loss ~1.4\u21921.4 / ~0.7\u21920.68 / ~2.64\u21922.54 with MAI ~0.40,\n0.65, 0.30). These findings confirm the critical role of pretraining for both\ntask performance and alignment metrics.", "", "The training loop calls loss_ai.backward() and then immediately calls\nloss_user.backward() on the same computation graph. By default, PyTorch frees\nthe intermediate activations after the first backward call, so the second\nbackward raises a \"graph freed\" RuntimeError. To fix this, you can either (1)\ncombine the two losses into a single loss (e.g. total_loss = loss_ai +\nloss_user) and call backward() once; (2) call\nloss_ai.backward(retain_graph=True) to preserve the graph for the second\nbackward (though this increases memory use); or (3) detach the shared embedding\nfrom the transformer before the heads (e.g. emb =\ndistilbert(...).last_hidden_state[:,0,:].detach()) or freeze the DistilBERT\nparameters (param.requires_grad = False) so that separate backward passes for\neach head do not traverse the same graph.", "", "", "No runtime errors were observed; however, two logical bugs were identified:  1.\nIn the \u201cshuffled\u201d ablation, shuffle_batch is applied before both AI and user\nembedding extraction, so both models always see the same scrambled input. This\nnullifies the intended ablation (only the user\u2019s mental model input should be\nshuffled).  2. The validation accuracy (v_acc) is computed using the\nuser_model\u2019s predictions instead of the ai_model\u2019s, so the MAI metric does not\nreflect AI performance as intended.  Proposed fixes: - Apply shuffle_batch only\nto the user_model\u2019s input when computing user_model logits.  - Compute\nvalidation accuracy using ai_model predictions (torch.argmax(logits_ai)) rather\nthan user_model\u2019s.", "", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, "RuntimeError", null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, {"args": ["Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."]}, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 117, "<module>", "loss_user.backward()"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/_tensor.py", 581, "backward", "torch.autograd.backward("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/autograd/__init__.py", 347, "backward", "_engine_run_backward("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/autograd/graph.py", 825, "_engine_run_backward", "return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass"]], null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training alignment", "lower_is_better": false, "description": "Final alignment on the training set", "data": [{"dataset_name": "learning rate 1.0e-04", "final_value": 0.9929, "best_value": 0.9929}, {"dataset_name": "learning rate 5.0e-04", "final_value": 0.9921, "best_value": 0.9921}, {"dataset_name": "learning rate 1.0e-03", "final_value": 0.9917, "best_value": 0.9917}, {"dataset_name": "learning rate 5.0e-03", "final_value": 0.9988, "best_value": 0.9988}, {"dataset_name": "learning rate 1.0e-02", "final_value": 0.9991, "best_value": 0.9991}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Final alignment on the validation set", "data": [{"dataset_name": "learning rate 1.0e-04", "final_value": 0.9929, "best_value": 0.9929}, {"dataset_name": "learning rate 5.0e-04", "final_value": 0.9915, "best_value": 0.9915}, {"dataset_name": "learning rate 1.0e-03", "final_value": 0.9907, "best_value": 0.9907}, {"dataset_name": "learning rate 5.0e-03", "final_value": 0.9989, "best_value": 0.9989}, {"dataset_name": "learning rate 1.0e-02", "final_value": 0.9986, "best_value": 0.9986}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Final loss on the training set", "data": [{"dataset_name": "learning rate 1.0e-04", "final_value": 1.0162, "best_value": 1.0162}, {"dataset_name": "learning rate 5.0e-04", "final_value": 0.7943, "best_value": 0.7943}, {"dataset_name": "learning rate 1.0e-03", "final_value": 0.5274, "best_value": 0.5274}, {"dataset_name": "learning rate 5.0e-03", "final_value": 0.0982, "best_value": 0.0982}, {"dataset_name": "learning rate 1.0e-02", "final_value": 0.0588, "best_value": 0.0588}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final loss on the validation set", "data": [{"dataset_name": "learning rate 1.0e-04", "final_value": 1.0106, "best_value": 1.0106}, {"dataset_name": "learning rate 5.0e-04", "final_value": 0.7817, "best_value": 0.7817}, {"dataset_name": "learning rate 1.0e-03", "final_value": 0.5101, "best_value": 0.5101}, {"dataset_name": "learning rate 5.0e-03", "final_value": 0.1039, "best_value": 0.1039}, {"dataset_name": "learning rate 1.0e-02", "final_value": 0.0739, "best_value": 0.0739}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test set", "data": [{"dataset_name": "learning rate 1.0e-04", "final_value": 0.6, "best_value": 0.6}, {"dataset_name": "learning rate 5.0e-04", "final_value": 0.835, "best_value": 0.835}, {"dataset_name": "learning rate 1.0e-03", "final_value": 0.905, "best_value": 0.905}, {"dataset_name": "learning rate 5.0e-03", "final_value": 0.96, "best_value": 0.96}, {"dataset_name": "learning rate 1.0e-02", "final_value": 0.965, "best_value": 0.965}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset.", "data": [{"dataset_name": "ag_news (baseline)", "final_value": 0.878, "best_value": 0.878}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.8635, "best_value": 0.8635}, {"dataset_name": "dbpedia_14 (baseline)", "final_value": 0.9785, "best_value": 0.9785}, {"dataset_name": "ag_news (fine_tune)", "final_value": 0.268, "best_value": 0.268}, {"dataset_name": "yelp_polarity (fine_tune)", "final_value": 0.5055, "best_value": 0.5055}, {"dataset_name": "dbpedia_14 (fine_tune)", "final_value": 0.134, "best_value": 0.134}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset.", "data": [{"dataset_name": "ag_news (baseline)", "final_value": 0.892, "best_value": 0.892}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.826, "best_value": 0.826}, {"dataset_name": "dbpedia_14 (baseline)", "final_value": 0.974, "best_value": 0.974}, {"dataset_name": "ag_news (fine_tune)", "final_value": 0.25, "best_value": 0.25}, {"dataset_name": "yelp_polarity (fine_tune)", "final_value": 0.548, "best_value": 0.548}, {"dataset_name": "dbpedia_14 (fine_tune)", "final_value": 0.122, "best_value": 0.122}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Cross-entropy loss on the training dataset.", "data": [{"dataset_name": "ag_news (baseline)", "final_value": 0.3672, "best_value": 0.3672}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.3435, "best_value": 0.3435}, {"dataset_name": "dbpedia_14 (baseline)", "final_value": 0.2103, "best_value": 0.2103}, {"dataset_name": "ag_news (fine_tune)", "final_value": 1.4203, "best_value": 1.4203}, {"dataset_name": "yelp_polarity (fine_tune)", "final_value": 0.7246, "best_value": 0.7246}, {"dataset_name": "dbpedia_14 (fine_tune)", "final_value": 2.5521, "best_value": 2.5521}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Cross-entropy loss on the validation dataset.", "data": [{"dataset_name": "ag_news (baseline)", "final_value": 0.3472, "best_value": 0.3472}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.3789, "best_value": 0.3789}, {"dataset_name": "dbpedia_14 (baseline)", "final_value": 0.1728, "best_value": 0.1728}, {"dataset_name": "ag_news (fine_tune)", "final_value": 1.4178, "best_value": 1.4178}, {"dataset_name": "yelp_polarity (fine_tune)", "final_value": 0.6905, "best_value": 0.6905}, {"dataset_name": "dbpedia_14 (fine_tune)", "final_value": 2.3812, "best_value": 2.3812}]}, {"metric_name": "training alignment", "lower_is_better": false, "description": "Alignment metric on the training dataset.", "data": [{"dataset_name": "ag_news (baseline)", "final_value": 0.9993, "best_value": 0.9993}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (baseline)", "final_value": 0.9971, "best_value": 0.9971}, {"dataset_name": "ag_news (fine_tune)", "final_value": 0.9938, "best_value": 0.9938}, {"dataset_name": "yelp_polarity (fine_tune)", "final_value": 0.9868, "best_value": 0.9868}, {"dataset_name": "dbpedia_14 (fine_tune)", "final_value": 0.9871, "best_value": 0.9871}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Alignment metric on the validation dataset.", "data": [{"dataset_name": "ag_news (baseline)", "final_value": 0.9995, "best_value": 0.9995}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (baseline)", "final_value": 0.9978, "best_value": 0.9978}, {"dataset_name": "ag_news (fine_tune)", "final_value": 0.9986, "best_value": 0.9986}, {"dataset_name": "yelp_polarity (fine_tune)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (fine_tune)", "final_value": 0.9935, "best_value": 0.9935}]}, {"metric_name": "mutual AI-user alignment index (MAI)", "lower_is_better": false, "description": "Mutual AI-user alignment index measured on each dataset.", "data": [{"dataset_name": "ag_news (baseline)", "final_value": 0.9427, "best_value": 0.9427}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.9047, "best_value": 0.9047}, {"dataset_name": "dbpedia_14 (baseline)", "final_value": 0.9858, "best_value": 0.9858}, {"dataset_name": "ag_news (fine_tune)", "final_value": 0.3999, "best_value": 0.3999}, {"dataset_name": "yelp_polarity (fine_tune)", "final_value": 0.708, "best_value": 0.708}, {"dataset_name": "dbpedia_14 (fine_tune)", "final_value": 0.2173, "best_value": 0.2173}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Proportion of correct predictions on the training set", "data": [{"dataset_name": "ag_news (baseline)", "final_value": 0.878, "best_value": 0.878}, {"dataset_name": "ag_news (linear_probe)", "final_value": 0.873, "best_value": 0.873}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.8635, "best_value": 0.8635}, {"dataset_name": "yelp_polarity (linear_probe)", "final_value": 0.8385, "best_value": 0.8385}, {"dataset_name": "dbpedia_14 (baseline)", "final_value": 0.9785, "best_value": 0.9785}, {"dataset_name": "dbpedia_14 (linear_probe)", "final_value": 0.9685, "best_value": 0.9685}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Proportion of correct predictions on the validation set", "data": [{"dataset_name": "ag_news (baseline)", "final_value": 0.892, "best_value": 0.892}, {"dataset_name": "ag_news (linear_probe)", "final_value": 0.882, "best_value": 0.882}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.826, "best_value": 0.826}, {"dataset_name": "yelp_polarity (linear_probe)", "final_value": 0.834, "best_value": 0.834}, {"dataset_name": "dbpedia_14 (baseline)", "final_value": 0.974, "best_value": 0.974}, {"dataset_name": "dbpedia_14 (linear_probe)", "final_value": 0.968, "best_value": 0.968}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training set", "data": [{"dataset_name": "ag_news (baseline)", "final_value": 0.878, "best_value": 0.878}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.8635, "best_value": 0.8635}, {"dataset_name": "dbpedia_14 (baseline)", "final_value": 0.9785, "best_value": 0.9785}, {"dataset_name": "ag_news (fine_tune)", "final_value": 0.9405, "best_value": 0.9405}, {"dataset_name": "yelp_polarity (fine_tune)", "final_value": 0.9425, "best_value": 0.9425}, {"dataset_name": "dbpedia_14 (fine_tune)", "final_value": 0.9955, "best_value": 0.9955}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "ag_news (baseline)", "final_value": 0.3472, "best_value": 0.3472}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.3789, "best_value": 0.3789}, {"dataset_name": "dbpedia_14 (baseline)", "final_value": 0.1728, "best_value": 0.1728}, {"dataset_name": "ag_news (fine_tune)", "final_value": 0.3631, "best_value": 0.3631}, {"dataset_name": "yelp_polarity (fine_tune)", "final_value": 0.3324, "best_value": 0.3324}, {"dataset_name": "dbpedia_14 (fine_tune)", "final_value": 0.0765, "best_value": 0.0765}]}, {"metric_name": "validation bidirectional alignment", "lower_is_better": false, "description": "Bidirectional alignment on the validation set", "data": [{"dataset_name": "ag_news (baseline)", "final_value": 0.9995, "best_value": 0.9995}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (baseline)", "final_value": 0.9978, "best_value": 0.9978}, {"dataset_name": "ag_news (fine_tune)", "final_value": 0.9998, "best_value": 0.9998}, {"dataset_name": "yelp_polarity (fine_tune)", "final_value": 0.9998, "best_value": 0.9998}, {"dataset_name": "dbpedia_14 (fine_tune)", "final_value": 0.9997, "best_value": 0.9997}]}, {"metric_name": "MAI", "lower_is_better": false, "description": "Mean Alignment Improvement (MAI) metric", "data": [{"dataset_name": "ag_news (baseline)", "final_value": 0.9427, "best_value": 0.9427}, {"dataset_name": "yelp_polarity (baseline)", "final_value": 0.9047, "best_value": 0.9047}, {"dataset_name": "dbpedia_14 (baseline)", "final_value": 0.9858, "best_value": 0.9858}, {"dataset_name": "ag_news (fine_tune)", "final_value": 0.9372, "best_value": 0.9372}, {"dataset_name": "yelp_polarity (fine_tune)", "final_value": 0.9281, "best_value": 0.9281}, {"dataset_name": "dbpedia_14 (fine_tune)", "final_value": 0.9928, "best_value": 0.9928}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Model training loss at final epoch", "data": [{"dataset_name": "ag_news (dropout=0.0)", "final_value": 0.3672, "best_value": 0.3672}, {"dataset_name": "yelp_polarity (dropout=0.0)", "final_value": 0.3435, "best_value": 0.3435}, {"dataset_name": "dbpedia_14 (dropout=0.0)", "final_value": 0.2103, "best_value": 0.2103}, {"dataset_name": "ag_news (dropout=0.1)", "final_value": 0.3793, "best_value": 0.3793}, {"dataset_name": "yelp_polarity (dropout=0.1)", "final_value": 0.3623, "best_value": 0.3623}, {"dataset_name": "dbpedia_14 (dropout=0.1)", "final_value": 0.2447, "best_value": 0.2447}, {"dataset_name": "ag_news (dropout=0.3)", "final_value": 0.4064, "best_value": 0.4064}, {"dataset_name": "yelp_polarity (dropout=0.3)", "final_value": 0.347, "best_value": 0.347}, {"dataset_name": "dbpedia_14 (dropout=0.3)", "final_value": 0.3411, "best_value": 0.3411}, {"dataset_name": "ag_news (dropout=0.5)", "final_value": 0.4706, "best_value": 0.4706}, {"dataset_name": "yelp_polarity (dropout=0.5)", "final_value": 0.4231, "best_value": 0.4231}, {"dataset_name": "dbpedia_14 (dropout=0.5)", "final_value": 0.5765, "best_value": 0.5765}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Model validation loss at final epoch", "data": [{"dataset_name": "ag_news (dropout=0.0)", "final_value": 0.3472, "best_value": 0.3472}, {"dataset_name": "yelp_polarity (dropout=0.0)", "final_value": 0.3789, "best_value": 0.3789}, {"dataset_name": "dbpedia_14 (dropout=0.0)", "final_value": 0.1728, "best_value": 0.1728}, {"dataset_name": "ag_news (dropout=0.1)", "final_value": 0.3426, "best_value": 0.3426}, {"dataset_name": "yelp_polarity (dropout=0.1)", "final_value": 0.3469, "best_value": 0.3469}, {"dataset_name": "dbpedia_14 (dropout=0.1)", "final_value": 0.1857, "best_value": 0.1857}, {"dataset_name": "ag_news (dropout=0.3)", "final_value": 0.3356, "best_value": 0.3356}, {"dataset_name": "yelp_polarity (dropout=0.3)", "final_value": 0.3402, "best_value": 0.3402}, {"dataset_name": "dbpedia_14 (dropout=0.3)", "final_value": 0.2077, "best_value": 0.2077}, {"dataset_name": "ag_news (dropout=0.5)", "final_value": 0.3501, "best_value": 0.3501}, {"dataset_name": "yelp_polarity (dropout=0.5)", "final_value": 0.3614, "best_value": 0.3614}, {"dataset_name": "dbpedia_14 (dropout=0.5)", "final_value": 0.3048, "best_value": 0.3048}]}, {"metric_name": "training alignment", "lower_is_better": false, "description": "Training alignment metric at final epoch", "data": [{"dataset_name": "ag_news (dropout=0.0)", "final_value": 0.9993, "best_value": 0.9993}, {"dataset_name": "yelp_polarity (dropout=0.0)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (dropout=0.0)", "final_value": 0.9971, "best_value": 0.9971}, {"dataset_name": "ag_news (dropout=0.1)", "final_value": 0.9884, "best_value": 0.9884}, {"dataset_name": "yelp_polarity (dropout=0.1)", "final_value": 0.9883, "best_value": 0.9883}, {"dataset_name": "dbpedia_14 (dropout=0.1)", "final_value": 0.9819, "best_value": 0.9819}, {"dataset_name": "ag_news (dropout=0.3)", "final_value": 0.9743, "best_value": 0.9743}, {"dataset_name": "yelp_polarity (dropout=0.3)", "final_value": 0.9792, "best_value": 0.9792}, {"dataset_name": "dbpedia_14 (dropout=0.3)", "final_value": 0.9577, "best_value": 0.9577}, {"dataset_name": "ag_news (dropout=0.5)", "final_value": 0.9528, "best_value": 0.9528}, {"dataset_name": "yelp_polarity (dropout=0.5)", "final_value": 0.9761, "best_value": 0.9761}, {"dataset_name": "dbpedia_14 (dropout=0.5)", "final_value": 0.9086, "best_value": 0.9086}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Validation alignment metric at final epoch", "data": [{"dataset_name": "ag_news (dropout=0.0)", "final_value": 0.9995, "best_value": 0.9995}, {"dataset_name": "yelp_polarity (dropout=0.0)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (dropout=0.0)", "final_value": 0.9978, "best_value": 0.9978}, {"dataset_name": "ag_news (dropout=0.1)", "final_value": 0.9995, "best_value": 0.9995}, {"dataset_name": "yelp_polarity (dropout=0.1)", "final_value": 0.9998, "best_value": 0.9998}, {"dataset_name": "dbpedia_14 (dropout=0.1)", "final_value": 0.9972, "best_value": 0.9972}, {"dataset_name": "ag_news (dropout=0.3)", "final_value": 0.9987, "best_value": 0.9987}, {"dataset_name": "yelp_polarity (dropout=0.3)", "final_value": 0.9958, "best_value": 0.9958}, {"dataset_name": "dbpedia_14 (dropout=0.3)", "final_value": 0.9956, "best_value": 0.9956}, {"dataset_name": "ag_news (dropout=0.5)", "final_value": 0.9974, "best_value": 0.9974}, {"dataset_name": "yelp_polarity (dropout=0.5)", "final_value": 0.9983, "best_value": 0.9983}, {"dataset_name": "dbpedia_14 (dropout=0.5)", "final_value": 0.9929, "best_value": 0.9929}]}, {"metric_name": "MAI", "lower_is_better": false, "description": "Mutual alignment index at final epoch", "data": [{"dataset_name": "ag_news (dropout=0.0)", "final_value": 0.9427, "best_value": 0.9427}, {"dataset_name": "yelp_polarity (dropout=0.0)", "final_value": 0.9047, "best_value": 0.9047}, {"dataset_name": "dbpedia_14 (dropout=0.0)", "final_value": 0.9858, "best_value": 0.9858}, {"dataset_name": "ag_news (dropout=0.1)", "final_value": 0.9416, "best_value": 0.9416}, {"dataset_name": "yelp_polarity (dropout=0.1)", "final_value": 0.9246, "best_value": 0.9246}, {"dataset_name": "dbpedia_14 (dropout=0.1)", "final_value": 0.9865, "best_value": 0.9865}, {"dataset_name": "ag_news (dropout=0.3)", "final_value": 0.939, "best_value": 0.939}, {"dataset_name": "yelp_polarity (dropout=0.3)", "final_value": 0.9253, "best_value": 0.9253}, {"dataset_name": "dbpedia_14 (dropout=0.3)", "final_value": 0.9827, "best_value": 0.9827}, {"dataset_name": "ag_news (dropout=0.5)", "final_value": 0.9361, "best_value": 0.9361}, {"dataset_name": "yelp_polarity (dropout=0.5)", "final_value": 0.9298, "best_value": 0.9298}, {"dataset_name": "dbpedia_14 (dropout=0.5)", "final_value": 0.9813, "best_value": 0.9813}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test set accuracy", "data": [{"dataset_name": "ag_news (dropout=0.0)", "final_value": 0.892, "best_value": 0.892}, {"dataset_name": "yelp_polarity (dropout=0.0)", "final_value": 0.824, "best_value": 0.824}, {"dataset_name": "dbpedia_14 (dropout=0.0)", "final_value": 0.972, "best_value": 0.972}, {"dataset_name": "ag_news (dropout=0.1)", "final_value": 0.888, "best_value": 0.888}, {"dataset_name": "yelp_polarity (dropout=0.1)", "final_value": 0.854, "best_value": 0.854}, {"dataset_name": "dbpedia_14 (dropout=0.1)", "final_value": 0.976, "best_value": 0.976}, {"dataset_name": "ag_news (dropout=0.3)", "final_value": 0.894, "best_value": 0.894}, {"dataset_name": "yelp_polarity (dropout=0.3)", "final_value": 0.864, "best_value": 0.864}, {"dataset_name": "dbpedia_14 (dropout=0.3)", "final_value": 0.974, "best_value": 0.974}, {"dataset_name": "ag_news (dropout=0.5)", "final_value": 0.88, "best_value": 0.88}, {"dataset_name": "yelp_polarity (dropout=0.5)", "final_value": 0.854, "best_value": 0.854}, {"dataset_name": "dbpedia_14 (dropout=0.5)", "final_value": 0.964, "best_value": 0.964}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss (minimum across activations)", "data": [{"dataset_name": "ag_news", "final_value": 0.3362, "best_value": 0.3362}, {"dataset_name": "yelp_polarity", "final_value": 0.3175, "best_value": 0.3175}, {"dataset_name": "dbpedia_14", "final_value": 0.1243, "best_value": 0.1243}]}, {"metric_name": "training alignment", "lower_is_better": false, "description": "Final training alignment (maximum across activations)", "data": [{"dataset_name": "ag_news", "final_value": 0.9996, "best_value": 0.9996}, {"dataset_name": "yelp_polarity", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14", "final_value": 0.9982, "best_value": 0.9982}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss (minimum across activations)", "data": [{"dataset_name": "ag_news", "final_value": 0.3372, "best_value": 0.3372}, {"dataset_name": "yelp_polarity", "final_value": 0.3478, "best_value": 0.3478}, {"dataset_name": "dbpedia_14", "final_value": 0.1281, "best_value": 0.1281}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Validation alignment (maximum across activations)", "data": [{"dataset_name": "ag_news", "final_value": 0.9996, "best_value": 0.9996}, {"dataset_name": "yelp_polarity", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14", "final_value": 0.9982, "best_value": 0.9982}]}, {"metric_name": "MAI", "lower_is_better": false, "description": "Mutual Agreement Index (maximum across activations)", "data": [{"dataset_name": "ag_news", "final_value": 0.9438, "best_value": 0.9438}, {"dataset_name": "yelp_polarity", "final_value": 0.9221, "best_value": 0.9221}, {"dataset_name": "dbpedia_14", "final_value": 0.987, "best_value": 0.987}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training data", "data": [{"dataset_name": "ag_news (lambda=0.0)", "final_value": 0.878, "best_value": 0.878}, {"dataset_name": "yelp_polarity (lambda=0.0)", "final_value": 0.8635, "best_value": 0.8635}, {"dataset_name": "dbpedia_14 (lambda=0.0)", "final_value": 0.9785, "best_value": 0.9785}, {"dataset_name": "ag_news (lambda=0.1)", "final_value": 0.878, "best_value": 0.878}, {"dataset_name": "yelp_polarity (lambda=0.1)", "final_value": 0.8625, "best_value": 0.8625}, {"dataset_name": "dbpedia_14 (lambda=0.1)", "final_value": 0.9785, "best_value": 0.9785}, {"dataset_name": "ag_news (lambda=0.5)", "final_value": 0.8795, "best_value": 0.8795}, {"dataset_name": "yelp_polarity (lambda=0.5)", "final_value": 0.863, "best_value": 0.863}, {"dataset_name": "dbpedia_14 (lambda=0.5)", "final_value": 0.9785, "best_value": 0.9785}, {"dataset_name": "ag_news (lambda=1.0)", "final_value": 0.8805, "best_value": 0.8805}, {"dataset_name": "yelp_polarity (lambda=1.0)", "final_value": 0.861, "best_value": 0.861}, {"dataset_name": "dbpedia_14 (lambda=1.0)", "final_value": 0.9785, "best_value": 0.9785}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation data", "data": [{"dataset_name": "ag_news (lambda=0.0)", "final_value": 0.892, "best_value": 0.892}, {"dataset_name": "yelp_polarity (lambda=0.0)", "final_value": 0.826, "best_value": 0.826}, {"dataset_name": "dbpedia_14 (lambda=0.0)", "final_value": 0.974, "best_value": 0.974}, {"dataset_name": "ag_news (lambda=0.1)", "final_value": 0.892, "best_value": 0.892}, {"dataset_name": "yelp_polarity (lambda=0.1)", "final_value": 0.828, "best_value": 0.828}, {"dataset_name": "dbpedia_14 (lambda=0.1)", "final_value": 0.974, "best_value": 0.974}, {"dataset_name": "ag_news (lambda=0.5)", "final_value": 0.892, "best_value": 0.892}, {"dataset_name": "yelp_polarity (lambda=0.5)", "final_value": 0.83, "best_value": 0.83}, {"dataset_name": "dbpedia_14 (lambda=0.5)", "final_value": 0.974, "best_value": 0.974}, {"dataset_name": "ag_news (lambda=1.0)", "final_value": 0.894, "best_value": 0.894}, {"dataset_name": "yelp_polarity (lambda=1.0)", "final_value": 0.836, "best_value": 0.836}, {"dataset_name": "dbpedia_14 (lambda=1.0)", "final_value": 0.974, "best_value": 0.974}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training data", "data": [{"dataset_name": "ag_news (lambda=0.0)", "final_value": 0.3672, "best_value": 0.3672}, {"dataset_name": "yelp_polarity (lambda=0.0)", "final_value": 0.3435, "best_value": 0.3435}, {"dataset_name": "dbpedia_14 (lambda=0.0)", "final_value": 0.2103, "best_value": 0.2103}, {"dataset_name": "ag_news (lambda=0.1)", "final_value": 0.3674, "best_value": 0.3674}, {"dataset_name": "yelp_polarity (lambda=0.1)", "final_value": 0.3441, "best_value": 0.3441}, {"dataset_name": "dbpedia_14 (lambda=0.1)", "final_value": 0.21, "best_value": 0.21}, {"dataset_name": "ag_news (lambda=0.5)", "final_value": 0.3682, "best_value": 0.3682}, {"dataset_name": "yelp_polarity (lambda=0.5)", "final_value": 0.3442, "best_value": 0.3442}, {"dataset_name": "dbpedia_14 (lambda=0.5)", "final_value": 0.2083, "best_value": 0.2083}, {"dataset_name": "ag_news (lambda=1.0)", "final_value": 0.3705, "best_value": 0.3705}, {"dataset_name": "yelp_polarity (lambda=1.0)", "final_value": 0.3378, "best_value": 0.3378}, {"dataset_name": "dbpedia_14 (lambda=1.0)", "final_value": 0.2076, "best_value": 0.2076}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation data", "data": [{"dataset_name": "ag_news (lambda=0.0)", "final_value": 0.3472, "best_value": 0.3472}, {"dataset_name": "yelp_polarity (lambda=0.0)", "final_value": 0.3789, "best_value": 0.3789}, {"dataset_name": "dbpedia_14 (lambda=0.0)", "final_value": 0.1728, "best_value": 0.1728}, {"dataset_name": "ag_news (lambda=0.1)", "final_value": 0.3472, "best_value": 0.3472}, {"dataset_name": "yelp_polarity (lambda=0.1)", "final_value": 0.379, "best_value": 0.379}, {"dataset_name": "dbpedia_14 (lambda=0.1)", "final_value": 0.1726, "best_value": 0.1726}, {"dataset_name": "ag_news (lambda=0.5)", "final_value": 0.3472, "best_value": 0.3472}, {"dataset_name": "yelp_polarity (lambda=0.5)", "final_value": 0.3764, "best_value": 0.3764}, {"dataset_name": "dbpedia_14 (lambda=0.5)", "final_value": 0.1715, "best_value": 0.1715}, {"dataset_name": "ag_news (lambda=1.0)", "final_value": 0.3478, "best_value": 0.3478}, {"dataset_name": "yelp_polarity (lambda=1.0)", "final_value": 0.3697, "best_value": 0.3697}, {"dataset_name": "dbpedia_14 (lambda=1.0)", "final_value": 0.1709, "best_value": 0.1709}]}, {"metric_name": "train alignment", "lower_is_better": false, "description": "Alignment metric on the training data", "data": [{"dataset_name": "ag_news (lambda=0.0)", "final_value": 0.9993, "best_value": 0.9993}, {"dataset_name": "yelp_polarity (lambda=0.0)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (lambda=0.0)", "final_value": 0.9971, "best_value": 0.9971}, {"dataset_name": "ag_news (lambda=0.1)", "final_value": 0.9993, "best_value": 0.9993}, {"dataset_name": "yelp_polarity (lambda=0.1)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (lambda=0.1)", "final_value": 0.9972, "best_value": 0.9972}, {"dataset_name": "ag_news (lambda=0.5)", "final_value": 0.9994, "best_value": 0.9994}, {"dataset_name": "yelp_polarity (lambda=0.5)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (lambda=0.5)", "final_value": 0.9976, "best_value": 0.9976}, {"dataset_name": "ag_news (lambda=1.0)", "final_value": 0.9994, "best_value": 0.9994}, {"dataset_name": "yelp_polarity (lambda=1.0)", "final_value": 0.9997, "best_value": 0.9997}, {"dataset_name": "dbpedia_14 (lambda=1.0)", "final_value": 0.998, "best_value": 0.998}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Alignment metric on the validation data", "data": [{"dataset_name": "ag_news (lambda=0.0)", "final_value": 0.9995, "best_value": 0.9995}, {"dataset_name": "yelp_polarity (lambda=0.0)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (lambda=0.0)", "final_value": 0.9978, "best_value": 0.9978}, {"dataset_name": "ag_news (lambda=0.1)", "final_value": 0.9995, "best_value": 0.9995}, {"dataset_name": "yelp_polarity (lambda=0.1)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (lambda=0.1)", "final_value": 0.9979, "best_value": 0.9979}, {"dataset_name": "ag_news (lambda=0.5)", "final_value": 0.9996, "best_value": 0.9996}, {"dataset_name": "yelp_polarity (lambda=0.5)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (lambda=0.5)", "final_value": 0.9982, "best_value": 0.9982}, {"dataset_name": "ag_news (lambda=1.0)", "final_value": 0.9994, "best_value": 0.9994}, {"dataset_name": "yelp_polarity (lambda=1.0)", "final_value": 0.9998, "best_value": 0.9998}, {"dataset_name": "dbpedia_14 (lambda=1.0)", "final_value": 0.9985, "best_value": 0.9985}]}, {"metric_name": "MAI", "lower_is_better": false, "description": "Mean Alignment Index", "data": [{"dataset_name": "ag_news (lambda=0.0)", "final_value": 0.9427, "best_value": 0.9427}, {"dataset_name": "yelp_polarity (lambda=0.0)", "final_value": 0.9047, "best_value": 0.9047}, {"dataset_name": "dbpedia_14 (lambda=0.0)", "final_value": 0.9858, "best_value": 0.9858}, {"dataset_name": "ag_news (lambda=0.1)", "final_value": 0.9427, "best_value": 0.9427}, {"dataset_name": "yelp_polarity (lambda=0.1)", "final_value": 0.9059, "best_value": 0.9059}, {"dataset_name": "dbpedia_14 (lambda=0.1)", "final_value": 0.9858, "best_value": 0.9858}, {"dataset_name": "ag_news (lambda=0.5)", "final_value": 0.9427, "best_value": 0.9427}, {"dataset_name": "yelp_polarity (lambda=0.5)", "final_value": 0.9071, "best_value": 0.9071}, {"dataset_name": "dbpedia_14 (lambda=0.5)", "final_value": 0.986, "best_value": 0.986}, {"dataset_name": "ag_news (lambda=1.0)", "final_value": 0.9438, "best_value": 0.9438}, {"dataset_name": "yelp_polarity (lambda=1.0)", "final_value": 0.9106, "best_value": 0.9106}, {"dataset_name": "dbpedia_14 (lambda=1.0)", "final_value": 0.9861, "best_value": 0.9861}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "ag_news (layer_1)", "final_value": 1.3184, "best_value": 1.3184}, {"dataset_name": "yelp_polarity (layer_1)", "final_value": 0.6832, "best_value": 0.6832}, {"dataset_name": "dbpedia_14 (layer_1)", "final_value": 2.5287, "best_value": 2.5287}, {"dataset_name": "ag_news (layer_3)", "final_value": 0.4974, "best_value": 0.4974}, {"dataset_name": "yelp_polarity (layer_3)", "final_value": 0.5621, "best_value": 0.5621}, {"dataset_name": "dbpedia_14 (layer_3)", "final_value": 0.6924, "best_value": 0.6924}, {"dataset_name": "ag_news (layer_5)", "final_value": 0.3225, "best_value": 0.3225}, {"dataset_name": "yelp_polarity (layer_5)", "final_value": 0.4084, "best_value": 0.4084}, {"dataset_name": "dbpedia_14 (layer_5)", "final_value": 0.3142, "best_value": 0.3142}, {"dataset_name": "ag_news (avg_last2)", "final_value": 0.334, "best_value": 0.334}, {"dataset_name": "yelp_polarity (avg_last2)", "final_value": 0.3959, "best_value": 0.3959}, {"dataset_name": "dbpedia_14 (avg_last2)", "final_value": 0.2983, "best_value": 0.2983}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "ag_news (layer_1)", "final_value": 1.2714, "best_value": 1.2714}, {"dataset_name": "yelp_polarity (layer_1)", "final_value": 0.6818, "best_value": 0.6818}, {"dataset_name": "dbpedia_14 (layer_1)", "final_value": 2.4574, "best_value": 2.4574}, {"dataset_name": "ag_news (layer_3)", "final_value": 0.455, "best_value": 0.455}, {"dataset_name": "yelp_polarity (layer_3)", "final_value": 0.5485, "best_value": 0.5485}, {"dataset_name": "dbpedia_14 (layer_3)", "final_value": 0.6152, "best_value": 0.6152}, {"dataset_name": "ag_news (layer_5)", "final_value": 0.339, "best_value": 0.339}, {"dataset_name": "yelp_polarity (layer_5)", "final_value": 0.4565, "best_value": 0.4565}, {"dataset_name": "dbpedia_14 (layer_5)", "final_value": 0.2946, "best_value": 0.2946}, {"dataset_name": "ag_news (avg_last2)", "final_value": 0.3276, "best_value": 0.3276}, {"dataset_name": "yelp_polarity (avg_last2)", "final_value": 0.4247, "best_value": 0.4247}, {"dataset_name": "dbpedia_14 (avg_last2)", "final_value": 0.2518, "best_value": 0.2518}]}, {"metric_name": "train bidirectional alignment", "lower_is_better": false, "description": "Train bidirectional alignment", "data": [{"dataset_name": "ag_news (layer_1)", "final_value": 0.7367, "best_value": 0.7367}, {"dataset_name": "yelp_polarity (layer_1)", "final_value": 0.847, "best_value": 0.847}, {"dataset_name": "dbpedia_14 (layer_1)", "final_value": 0.6037, "best_value": 0.6037}, {"dataset_name": "ag_news (layer_3)", "final_value": 0.8982, "best_value": 0.8982}, {"dataset_name": "yelp_polarity (layer_3)", "final_value": 0.8742, "best_value": 0.8742}, {"dataset_name": "dbpedia_14 (layer_3)", "final_value": 0.8492, "best_value": 0.8492}, {"dataset_name": "ag_news (layer_5)", "final_value": 0.9394, "best_value": 0.9394}, {"dataset_name": "yelp_polarity (layer_5)", "final_value": 0.9112, "best_value": 0.9112}, {"dataset_name": "dbpedia_14 (layer_5)", "final_value": 0.931, "best_value": 0.931}, {"dataset_name": "ag_news (avg_last2)", "final_value": 0.9368, "best_value": 0.9368}, {"dataset_name": "yelp_polarity (avg_last2)", "final_value": 0.9122, "best_value": 0.9122}, {"dataset_name": "dbpedia_14 (avg_last2)", "final_value": 0.9324, "best_value": 0.9324}]}, {"metric_name": "validation bidirectional alignment", "lower_is_better": false, "description": "Validation bidirectional alignment", "data": [{"dataset_name": "ag_news (layer_1)", "final_value": 0.7436, "best_value": 0.7436}, {"dataset_name": "yelp_polarity (layer_1)", "final_value": 0.8461, "best_value": 0.8461}, {"dataset_name": "dbpedia_14 (layer_1)", "final_value": 0.6088, "best_value": 0.6088}, {"dataset_name": "ag_news (layer_3)", "final_value": 0.9078, "best_value": 0.9078}, {"dataset_name": "yelp_polarity (layer_3)", "final_value": 0.8773, "best_value": 0.8773}, {"dataset_name": "dbpedia_14 (layer_3)", "final_value": 0.8674, "best_value": 0.8674}, {"dataset_name": "ag_news (layer_5)", "final_value": 0.9405, "best_value": 0.9405}, {"dataset_name": "yelp_polarity (layer_5)", "final_value": 0.9054, "best_value": 0.9054}, {"dataset_name": "dbpedia_14 (layer_5)", "final_value": 0.9383, "best_value": 0.9383}, {"dataset_name": "ag_news (avg_last2)", "final_value": 0.9399, "best_value": 0.9399}, {"dataset_name": "yelp_polarity (avg_last2)", "final_value": 0.9079, "best_value": 0.9079}, {"dataset_name": "dbpedia_14 (avg_last2)", "final_value": 0.9426, "best_value": 0.9426}]}]}, {"metric_names": [{"metric_name": "train MAI", "lower_is_better": false, "description": "Multi-class accuracy metric on the training set", "data": [{"dataset_name": "ag_news", "final_value": 0.9387, "best_value": 0.9387}, {"dataset_name": "yelp_polarity", "final_value": 0.9267, "best_value": 0.9267}, {"dataset_name": "dbpedia_14", "final_value": 0.9874, "best_value": 0.9874}]}, {"metric_name": "validation MAI", "lower_is_better": false, "description": "Multi-class accuracy metric on the validation set", "data": [{"dataset_name": "ag_news", "final_value": 0.9458, "best_value": 0.9458}, {"dataset_name": "yelp_polarity", "final_value": 0.9223, "best_value": 0.9223}, {"dataset_name": "dbpedia_14", "final_value": 0.9854, "best_value": 0.9854}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "ag_news", "final_value": 0.3635, "best_value": 0.3635}, {"dataset_name": "yelp_polarity", "final_value": 0.3563, "best_value": 0.3435}, {"dataset_name": "dbpedia_14", "final_value": 0.2235, "best_value": 0.2103}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "ag_news", "final_value": 0.3453, "best_value": 0.3453}, {"dataset_name": "yelp_polarity", "final_value": 0.3458, "best_value": 0.3458}, {"dataset_name": "dbpedia_14", "final_value": 0.1858, "best_value": 0.1728}]}, {"metric_name": "train alignment", "lower_is_better": false, "description": "Final training alignment", "data": [{"dataset_name": "ag_news", "final_value": 0.9992, "best_value": 0.9996}, {"dataset_name": "yelp_polarity", "final_value": 0.9997, "best_value": 0.9999}, {"dataset_name": "dbpedia_14", "final_value": 0.996, "best_value": 0.9971}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Final validation alignment", "data": [{"dataset_name": "ag_news", "final_value": 0.9994, "best_value": 0.9997}, {"dataset_name": "yelp_polarity", "final_value": 0.9997, "best_value": 0.9999}, {"dataset_name": "dbpedia_14", "final_value": 0.9968, "best_value": 0.9978}]}, {"metric_name": "validation MAI", "lower_is_better": false, "description": "Final validation mean average similarity (MAI)", "data": [{"dataset_name": "ag_news", "final_value": 0.9416, "best_value": 0.9427}, {"dataset_name": "yelp_polarity", "final_value": 0.9258, "best_value": 0.9258}, {"dataset_name": "dbpedia_14", "final_value": 0.9843, "best_value": 0.9858}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Cross-entropy loss on the training dataset", "data": [{"dataset_name": "ag_news (pretrained)", "final_value": 0.3672, "best_value": 0.3672}, {"dataset_name": "yelp_polarity (pretrained)", "final_value": 0.3435, "best_value": 0.3435}, {"dataset_name": "dbpedia_14 (pretrained)", "final_value": 0.2103, "best_value": 0.2103}, {"dataset_name": "ag_news (random_init)", "final_value": 1.393, "best_value": 1.393}, {"dataset_name": "yelp_polarity (random_init)", "final_value": 0.689, "best_value": 0.689}, {"dataset_name": "dbpedia_14 (random_init)", "final_value": 2.6121, "best_value": 2.6121}]}, {"metric_name": "training alignment", "lower_is_better": false, "description": "Alignment score on the training dataset", "data": [{"dataset_name": "ag_news (pretrained)", "final_value": 0.9993, "best_value": 0.9993}, {"dataset_name": "yelp_polarity (pretrained)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (pretrained)", "final_value": 0.9971, "best_value": 0.9971}, {"dataset_name": "ag_news (random_init)", "final_value": 0.9973, "best_value": 0.9973}, {"dataset_name": "yelp_polarity (random_init)", "final_value": 0.9991, "best_value": 0.9991}, {"dataset_name": "dbpedia_14 (random_init)", "final_value": 0.9889, "best_value": 0.9889}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Cross-entropy loss on the validation dataset", "data": [{"dataset_name": "ag_news (pretrained)", "final_value": 0.3472, "best_value": 0.3472}, {"dataset_name": "yelp_polarity (pretrained)", "final_value": 0.3789, "best_value": 0.3789}, {"dataset_name": "dbpedia_14 (pretrained)", "final_value": 0.1728, "best_value": 0.1728}, {"dataset_name": "ag_news (random_init)", "final_value": 1.4047, "best_value": 1.4047}, {"dataset_name": "yelp_polarity (random_init)", "final_value": 0.6791, "best_value": 0.6791}, {"dataset_name": "dbpedia_14 (random_init)", "final_value": 2.5436, "best_value": 2.5436}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Alignment score on the validation dataset", "data": [{"dataset_name": "ag_news (pretrained)", "final_value": 0.9995, "best_value": 0.9995}, {"dataset_name": "yelp_polarity (pretrained)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (pretrained)", "final_value": 0.9978, "best_value": 0.9978}, {"dataset_name": "ag_news (random_init)", "final_value": 0.9993, "best_value": 0.9993}, {"dataset_name": "yelp_polarity (random_init)", "final_value": 0.9996, "best_value": 0.9996}, {"dataset_name": "dbpedia_14 (random_init)", "final_value": 0.9857, "best_value": 0.9857}]}, {"metric_name": "validation MAI", "lower_is_better": false, "description": "Mean alignment index on the validation dataset", "data": [{"dataset_name": "ag_news (pretrained)", "final_value": 0.9427, "best_value": 0.9427}, {"dataset_name": "yelp_polarity (pretrained)", "final_value": 0.9047, "best_value": 0.9047}, {"dataset_name": "dbpedia_14 (pretrained)", "final_value": 0.9858, "best_value": 0.9858}, {"dataset_name": "ag_news (random_init)", "final_value": 0.3896, "best_value": 0.3896}, {"dataset_name": "yelp_polarity (random_init)", "final_value": 0.7146, "best_value": 0.7146}, {"dataset_name": "dbpedia_14 (random_init)", "final_value": 0.3044, "best_value": 0.3044}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Classification accuracy on the test dataset", "data": [{"dataset_name": "ag_news (pretrained)", "final_value": 0.892, "best_value": 0.892}, {"dataset_name": "yelp_polarity (pretrained)", "final_value": 0.824, "best_value": 0.824}, {"dataset_name": "dbpedia_14 (pretrained)", "final_value": 0.972, "best_value": 0.972}, {"dataset_name": "ag_news (random_init)", "final_value": 0.242, "best_value": 0.242}, {"dataset_name": "yelp_polarity (random_init)", "final_value": 0.542, "best_value": 0.542}, {"dataset_name": "dbpedia_14 (random_init)", "final_value": 0.148, "best_value": 0.148}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on training set", "data": [{"dataset_name": "ag_news (ablation: token_dropout_0)", "final_value": 0.878, "best_value": 0.878}, {"dataset_name": "yelp_polarity (ablation: token_dropout_0)", "final_value": 0.8635, "best_value": 0.8635}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_0)", "final_value": 0.9785, "best_value": 0.9785}, {"dataset_name": "ag_news (ablation: token_dropout_10)", "final_value": 0.8775, "best_value": 0.8775}, {"dataset_name": "yelp_polarity (ablation: token_dropout_10)", "final_value": 0.835, "best_value": 0.835}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_10)", "final_value": 0.962, "best_value": 0.962}, {"dataset_name": "ag_news (ablation: token_dropout_20)", "final_value": 0.874, "best_value": 0.874}, {"dataset_name": "yelp_polarity (ablation: token_dropout_20)", "final_value": 0.826, "best_value": 0.826}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_20)", "final_value": 0.959, "best_value": 0.959}, {"dataset_name": "ag_news (ablation: token_dropout_30)", "final_value": 0.87, "best_value": 0.87}, {"dataset_name": "yelp_polarity (ablation: token_dropout_30)", "final_value": 0.813, "best_value": 0.813}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_30)", "final_value": 0.949, "best_value": 0.949}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on validation set", "data": [{"dataset_name": "ag_news (ablation: token_dropout_0)", "final_value": 0.892, "best_value": 0.892}, {"dataset_name": "yelp_polarity (ablation: token_dropout_0)", "final_value": 0.826, "best_value": 0.826}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_0)", "final_value": 0.974, "best_value": 0.974}, {"dataset_name": "ag_news (ablation: token_dropout_10)", "final_value": 0.884, "best_value": 0.884}, {"dataset_name": "yelp_polarity (ablation: token_dropout_10)", "final_value": 0.844, "best_value": 0.844}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_10)", "final_value": 0.97, "best_value": 0.97}, {"dataset_name": "ag_news (ablation: token_dropout_20)", "final_value": 0.876, "best_value": 0.876}, {"dataset_name": "yelp_polarity (ablation: token_dropout_20)", "final_value": 0.81, "best_value": 0.81}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_20)", "final_value": 0.964, "best_value": 0.964}, {"dataset_name": "ag_news (ablation: token_dropout_30)", "final_value": 0.882, "best_value": 0.882}, {"dataset_name": "yelp_polarity (ablation: token_dropout_30)", "final_value": 0.792, "best_value": 0.792}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_30)", "final_value": 0.94, "best_value": 0.94}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on training set", "data": [{"dataset_name": "ag_news (ablation: token_dropout_0)", "final_value": 0.3672, "best_value": 0.3672}, {"dataset_name": "yelp_polarity (ablation: token_dropout_0)", "final_value": 0.3435, "best_value": 0.3435}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_0)", "final_value": 0.2103, "best_value": 0.2103}, {"dataset_name": "ag_news (ablation: token_dropout_10)", "final_value": 0.3797, "best_value": 0.3797}, {"dataset_name": "yelp_polarity (ablation: token_dropout_10)", "final_value": 0.3817, "best_value": 0.3817}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_10)", "final_value": 0.3125, "best_value": 0.3125}, {"dataset_name": "ag_news (ablation: token_dropout_20)", "final_value": 0.3905, "best_value": 0.3905}, {"dataset_name": "yelp_polarity (ablation: token_dropout_20)", "final_value": 0.4024, "best_value": 0.4024}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_20)", "final_value": 0.3137, "best_value": 0.3137}, {"dataset_name": "ag_news (ablation: token_dropout_30)", "final_value": 0.4055, "best_value": 0.4055}, {"dataset_name": "yelp_polarity (ablation: token_dropout_30)", "final_value": 0.4365, "best_value": 0.4365}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_30)", "final_value": 0.3475, "best_value": 0.3475}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on validation set", "data": [{"dataset_name": "ag_news (ablation: token_dropout_0)", "final_value": 0.3472, "best_value": 0.3472}, {"dataset_name": "yelp_polarity (ablation: token_dropout_0)", "final_value": 0.3789, "best_value": 0.3789}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_0)", "final_value": 0.1728, "best_value": 0.1728}, {"dataset_name": "ag_news (ablation: token_dropout_10)", "final_value": 0.366, "best_value": 0.366}, {"dataset_name": "yelp_polarity (ablation: token_dropout_10)", "final_value": 0.3901, "best_value": 0.3901}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_10)", "final_value": 0.2426, "best_value": 0.2426}, {"dataset_name": "ag_news (ablation: token_dropout_20)", "final_value": 0.352, "best_value": 0.352}, {"dataset_name": "yelp_polarity (ablation: token_dropout_20)", "final_value": 0.412, "best_value": 0.412}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_20)", "final_value": 0.2362, "best_value": 0.2362}, {"dataset_name": "ag_news (ablation: token_dropout_30)", "final_value": 0.3617, "best_value": 0.3617}, {"dataset_name": "yelp_polarity (ablation: token_dropout_30)", "final_value": 0.4453, "best_value": 0.4453}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_30)", "final_value": 0.2835, "best_value": 0.2835}]}, {"metric_name": "train alignment", "lower_is_better": false, "description": "Alignment on training set", "data": [{"dataset_name": "ag_news (ablation: token_dropout_0)", "final_value": 0.9993, "best_value": 0.9993}, {"dataset_name": "yelp_polarity (ablation: token_dropout_0)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_0)", "final_value": 0.9971, "best_value": 0.9971}, {"dataset_name": "ag_news (ablation: token_dropout_10)", "final_value": 0.9995, "best_value": 0.9995}, {"dataset_name": "yelp_polarity (ablation: token_dropout_10)", "final_value": 0.9994, "best_value": 0.9994}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_10)", "final_value": 0.9959, "best_value": 0.9959}, {"dataset_name": "ag_news (ablation: token_dropout_20)", "final_value": 0.9992, "best_value": 0.9992}, {"dataset_name": "yelp_polarity (ablation: token_dropout_20)", "final_value": 0.9997, "best_value": 0.9997}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_20)", "final_value": 0.9959, "best_value": 0.9959}, {"dataset_name": "ag_news (ablation: token_dropout_30)", "final_value": 0.9997, "best_value": 0.9997}, {"dataset_name": "yelp_polarity (ablation: token_dropout_30)", "final_value": 0.9998, "best_value": 0.9998}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_30)", "final_value": 0.9961, "best_value": 0.9961}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Alignment on validation set", "data": [{"dataset_name": "ag_news (ablation: token_dropout_0)", "final_value": 0.9995, "best_value": 0.9995}, {"dataset_name": "yelp_polarity (ablation: token_dropout_0)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_0)", "final_value": 0.9978, "best_value": 0.9978}, {"dataset_name": "ag_news (ablation: token_dropout_10)", "final_value": 0.9996, "best_value": 0.9996}, {"dataset_name": "yelp_polarity (ablation: token_dropout_10)", "final_value": 0.9994, "best_value": 0.9994}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_10)", "final_value": 0.9964, "best_value": 0.9964}, {"dataset_name": "ag_news (ablation: token_dropout_20)", "final_value": 0.9995, "best_value": 0.9995}, {"dataset_name": "yelp_polarity (ablation: token_dropout_20)", "final_value": 0.9997, "best_value": 0.9997}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_20)", "final_value": 0.9969, "best_value": 0.9969}, {"dataset_name": "ag_news (ablation: token_dropout_30)", "final_value": 0.9997, "best_value": 0.9997}, {"dataset_name": "yelp_polarity (ablation: token_dropout_30)", "final_value": 0.9996, "best_value": 0.9996}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_30)", "final_value": 0.9969, "best_value": 0.9969}]}, {"metric_name": "validation MAI", "lower_is_better": false, "description": "MAI on validation set", "data": [{"dataset_name": "ag_news (ablation: token_dropout_0)", "final_value": 0.9427, "best_value": 0.9427}, {"dataset_name": "yelp_polarity (ablation: token_dropout_0)", "final_value": 0.9047, "best_value": 0.9047}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_0)", "final_value": 0.9858, "best_value": 0.9858}, {"dataset_name": "ag_news (ablation: token_dropout_10)", "final_value": 0.9383, "best_value": 0.9383}, {"dataset_name": "yelp_polarity (ablation: token_dropout_10)", "final_value": 0.9151, "best_value": 0.9151}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_10)", "final_value": 0.983, "best_value": 0.983}, {"dataset_name": "ag_news (ablation: token_dropout_20)", "final_value": 0.9337, "best_value": 0.9337}, {"dataset_name": "yelp_polarity (ablation: token_dropout_20)", "final_value": 0.8949, "best_value": 0.8949}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_20)", "final_value": 0.9802, "best_value": 0.9802}, {"dataset_name": "ag_news (ablation: token_dropout_30)", "final_value": 0.9372, "best_value": 0.9372}, {"dataset_name": "yelp_polarity (ablation: token_dropout_30)", "final_value": 0.8838, "best_value": 0.8838}, {"dataset_name": "dbpedia_14 (ablation: token_dropout_30)", "final_value": 0.9676, "best_value": 0.9676}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Training set loss", "data": [{"dataset_name": "ag_news (adam)", "final_value": 0.3672, "best_value": 0.3672}, {"dataset_name": "yelp_polarity (adam)", "final_value": 0.3435, "best_value": 0.3435}, {"dataset_name": "dbpedia_14 (adam)", "final_value": 0.2103, "best_value": 0.2103}, {"dataset_name": "ag_news (sgd)", "final_value": 1.13, "best_value": 1.13}, {"dataset_name": "yelp_polarity (sgd)", "final_value": 0.6519, "best_value": 0.6519}, {"dataset_name": "dbpedia_14 (sgd)", "final_value": 2.3955, "best_value": 2.3955}, {"dataset_name": "ag_news (adamw)", "final_value": 0.3577, "best_value": 0.3577}, {"dataset_name": "yelp_polarity (adamw)", "final_value": 0.3221, "best_value": 0.3221}, {"dataset_name": "dbpedia_14 (adamw)", "final_value": 0.2191, "best_value": 0.2191}]}, {"metric_name": "train alignment", "lower_is_better": false, "description": "Training set alignment metric", "data": [{"dataset_name": "ag_news (adam)", "final_value": 0.9993, "best_value": 0.9993}, {"dataset_name": "yelp_polarity (adam)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (adam)", "final_value": 0.9971, "best_value": 0.9971}, {"dataset_name": "ag_news (sgd)", "final_value": 0.9991, "best_value": 0.9991}, {"dataset_name": "yelp_polarity (sgd)", "final_value": 0.9998, "best_value": 0.9998}, {"dataset_name": "dbpedia_14 (sgd)", "final_value": 0.9953, "best_value": 0.9953}, {"dataset_name": "ag_news (adamw)", "final_value": 0.9994, "best_value": 0.9994}, {"dataset_name": "yelp_polarity (adamw)", "final_value": 0.9989, "best_value": 0.9989}, {"dataset_name": "dbpedia_14 (adamw)", "final_value": 0.9957, "best_value": 0.9957}]}, {"metric_name": "train accuracy", "lower_is_better": false, "description": "Training set accuracy", "data": [{"dataset_name": "ag_news (adam)", "final_value": 0.878, "best_value": 0.878}, {"dataset_name": "yelp_polarity (adam)", "final_value": 0.8635, "best_value": 0.8635}, {"dataset_name": "dbpedia_14 (adam)", "final_value": 0.9785, "best_value": 0.9785}, {"dataset_name": "ag_news (sgd)", "final_value": 0.629, "best_value": 0.629}, {"dataset_name": "yelp_polarity (sgd)", "final_value": 0.668, "best_value": 0.668}, {"dataset_name": "dbpedia_14 (sgd)", "final_value": 0.3, "best_value": 0.3}, {"dataset_name": "ag_news (adamw)", "final_value": 0.88, "best_value": 0.88}, {"dataset_name": "yelp_polarity (adamw)", "final_value": 0.859, "best_value": 0.859}, {"dataset_name": "dbpedia_14 (adamw)", "final_value": 0.978, "best_value": 0.978}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation set loss", "data": [{"dataset_name": "ag_news (adam)", "final_value": 0.3472, "best_value": 0.3472}, {"dataset_name": "yelp_polarity (adam)", "final_value": 0.3789, "best_value": 0.3789}, {"dataset_name": "dbpedia_14 (adam)", "final_value": 0.1728, "best_value": 0.1728}, {"dataset_name": "ag_news (sgd)", "final_value": 1.0416, "best_value": 1.0416}, {"dataset_name": "yelp_polarity (sgd)", "final_value": 0.6379, "best_value": 0.6379}, {"dataset_name": "dbpedia_14 (sgd)", "final_value": 2.3031, "best_value": 2.3031}, {"dataset_name": "ag_news (adamw)", "final_value": 0.3395, "best_value": 0.3395}, {"dataset_name": "yelp_polarity (adamw)", "final_value": 0.3548, "best_value": 0.3548}, {"dataset_name": "dbpedia_14 (adamw)", "final_value": 0.1857, "best_value": 0.1857}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Validation set alignment metric", "data": [{"dataset_name": "ag_news (adam)", "final_value": 0.9995, "best_value": 0.9995}, {"dataset_name": "yelp_polarity (adam)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (adam)", "final_value": 0.9978, "best_value": 0.9978}, {"dataset_name": "ag_news (sgd)", "final_value": 0.9989, "best_value": 0.9989}, {"dataset_name": "yelp_polarity (sgd)", "final_value": 0.9998, "best_value": 0.9998}, {"dataset_name": "dbpedia_14 (sgd)", "final_value": 0.9957, "best_value": 0.9957}, {"dataset_name": "ag_news (adamw)", "final_value": 0.9996, "best_value": 0.9996}, {"dataset_name": "yelp_polarity (adamw)", "final_value": 0.9987, "best_value": 0.9987}, {"dataset_name": "dbpedia_14 (adamw)", "final_value": 0.9965, "best_value": 0.9965}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation set accuracy", "data": [{"dataset_name": "ag_news (adam)", "final_value": 0.892, "best_value": 0.892}, {"dataset_name": "yelp_polarity (adam)", "final_value": 0.826, "best_value": 0.826}, {"dataset_name": "dbpedia_14 (adam)", "final_value": 0.974, "best_value": 0.974}, {"dataset_name": "ag_news (sgd)", "final_value": 0.72, "best_value": 0.72}, {"dataset_name": "yelp_polarity (sgd)", "final_value": 0.81, "best_value": 0.81}, {"dataset_name": "dbpedia_14 (sgd)", "final_value": 0.438, "best_value": 0.438}, {"dataset_name": "ag_news (adamw)", "final_value": 0.904, "best_value": 0.904}, {"dataset_name": "yelp_polarity (adamw)", "final_value": 0.852, "best_value": 0.852}, {"dataset_name": "dbpedia_14 (adamw)", "final_value": 0.974, "best_value": 0.974}]}, {"metric_name": "validation MAI", "lower_is_better": false, "description": "Validation set MAI metric", "data": [{"dataset_name": "ag_news (adam)", "final_value": 0.9427, "best_value": 0.9427}, {"dataset_name": "yelp_polarity (adam)", "final_value": 0.9047, "best_value": 0.9047}, {"dataset_name": "dbpedia_14 (adam)", "final_value": 0.9858, "best_value": 0.9858}, {"dataset_name": "ag_news (sgd)", "final_value": 0.8368, "best_value": 0.8368}, {"dataset_name": "yelp_polarity (sgd)", "final_value": 0.895, "best_value": 0.895}, {"dataset_name": "dbpedia_14 (sgd)", "final_value": 0.6084, "best_value": 0.6084}, {"dataset_name": "ag_news (adamw)", "final_value": 0.9494, "best_value": 0.9494}, {"dataset_name": "yelp_polarity (adamw)", "final_value": 0.9195, "best_value": 0.9195}, {"dataset_name": "dbpedia_14 (adamw)", "final_value": 0.9851, "best_value": 0.9851}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "ag_news (independent)", "final_value": 0.3672, "best_value": 0.3672}, {"dataset_name": "yelp_polarity (independent)", "final_value": 0.3435, "best_value": 0.3435}, {"dataset_name": "dbpedia_14 (independent)", "final_value": 0.2103, "best_value": 0.2103}, {"dataset_name": "ag_news (shared_fc1)", "final_value": 0.3825, "best_value": 0.3825}, {"dataset_name": "yelp_polarity (shared_fc1)", "final_value": 0.3648, "best_value": 0.3648}, {"dataset_name": "dbpedia_14 (shared_fc1)", "final_value": 0.2793, "best_value": 0.2793}, {"dataset_name": "ag_news (shared_fc1_fc2)", "final_value": 0.3595, "best_value": 0.3595}, {"dataset_name": "yelp_polarity (shared_fc1_fc2)", "final_value": 0.3505, "best_value": 0.3505}, {"dataset_name": "dbpedia_14 (shared_fc1_fc2)", "final_value": 0.2265, "best_value": 0.2265}]}, {"metric_name": "training alignment", "lower_is_better": false, "description": "Final training alignment", "data": [{"dataset_name": "ag_news (independent)", "final_value": 0.9993, "best_value": 0.9993}, {"dataset_name": "yelp_polarity (independent)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (independent)", "final_value": 0.9971, "best_value": 0.9971}, {"dataset_name": "ag_news (shared_fc1)", "final_value": 0.9987, "best_value": 0.9987}, {"dataset_name": "yelp_polarity (shared_fc1)", "final_value": 0.9996, "best_value": 0.9996}, {"dataset_name": "dbpedia_14 (shared_fc1)", "final_value": 0.9965, "best_value": 0.9965}, {"dataset_name": "ag_news (shared_fc1_fc2)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "yelp_polarity (shared_fc1_fc2)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "dbpedia_14 (shared_fc1_fc2)", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "ag_news (independent)", "final_value": 0.3472, "best_value": 0.3472}, {"dataset_name": "yelp_polarity (independent)", "final_value": 0.3789, "best_value": 0.3789}, {"dataset_name": "dbpedia_14 (independent)", "final_value": 0.1728, "best_value": 0.1728}, {"dataset_name": "ag_news (shared_fc1)", "final_value": 0.3568, "best_value": 0.3568}, {"dataset_name": "yelp_polarity (shared_fc1)", "final_value": 0.3651, "best_value": 0.3651}, {"dataset_name": "dbpedia_14 (shared_fc1)", "final_value": 0.2299, "best_value": 0.2299}, {"dataset_name": "ag_news (shared_fc1_fc2)", "final_value": 0.3451, "best_value": 0.3451}, {"dataset_name": "yelp_polarity (shared_fc1_fc2)", "final_value": 0.3574, "best_value": 0.3574}, {"dataset_name": "dbpedia_14 (shared_fc1_fc2)", "final_value": 0.1876, "best_value": 0.1876}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Final validation alignment", "data": [{"dataset_name": "ag_news (independent)", "final_value": 0.9995, "best_value": 0.9995}, {"dataset_name": "yelp_polarity (independent)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (independent)", "final_value": 0.9978, "best_value": 0.9978}, {"dataset_name": "ag_news (shared_fc1)", "final_value": 0.9991, "best_value": 0.9991}, {"dataset_name": "yelp_polarity (shared_fc1)", "final_value": 0.9998, "best_value": 0.9998}, {"dataset_name": "dbpedia_14 (shared_fc1)", "final_value": 0.9975, "best_value": 0.9975}, {"dataset_name": "ag_news (shared_fc1_fc2)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "yelp_polarity (shared_fc1_fc2)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "dbpedia_14 (shared_fc1_fc2)", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "MAI", "lower_is_better": false, "description": "Final MAI", "data": [{"dataset_name": "ag_news (independent)", "final_value": 0.9427, "best_value": 0.9427}, {"dataset_name": "yelp_polarity (independent)", "final_value": 0.9047, "best_value": 0.9047}, {"dataset_name": "dbpedia_14 (independent)", "final_value": 0.9858, "best_value": 0.9858}, {"dataset_name": "ag_news (shared_fc1)", "final_value": 0.9403, "best_value": 0.9403}, {"dataset_name": "yelp_polarity (shared_fc1)", "final_value": 0.9246, "best_value": 0.9246}, {"dataset_name": "dbpedia_14 (shared_fc1)", "final_value": 0.9835, "best_value": 0.9835}, {"dataset_name": "ag_news (shared_fc1_fc2)", "final_value": 0.9418, "best_value": 0.9418}, {"dataset_name": "yelp_polarity (shared_fc1_fc2)", "final_value": 0.9189, "best_value": 0.9189}, {"dataset_name": "dbpedia_14 (shared_fc1_fc2)", "final_value": 0.9858, "best_value": 0.9858}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Final test accuracy", "data": [{"dataset_name": "ag_news (independent)", "final_value": 0.892, "best_value": 0.892}, {"dataset_name": "yelp_polarity (independent)", "final_value": 0.824, "best_value": 0.824}, {"dataset_name": "dbpedia_14 (independent)", "final_value": 0.972, "best_value": 0.972}, {"dataset_name": "ag_news (shared_fc1)", "final_value": 0.882, "best_value": 0.882}, {"dataset_name": "yelp_polarity (shared_fc1)", "final_value": 0.852, "best_value": 0.852}, {"dataset_name": "dbpedia_14 (shared_fc1)", "final_value": 0.966, "best_value": 0.966}, {"dataset_name": "ag_news (shared_fc1_fc2)", "final_value": 0.89, "best_value": 0.89}, {"dataset_name": "yelp_polarity (shared_fc1_fc2)", "final_value": 0.85, "best_value": 0.85}, {"dataset_name": "dbpedia_14 (shared_fc1_fc2)", "final_value": 0.972, "best_value": 0.972}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "ag_news (standard)", "final_value": 0.3672, "best_value": 0.3672}, {"dataset_name": "yelp_polarity (standard)", "final_value": 0.3435, "best_value": 0.3435}, {"dataset_name": "dbpedia_14 (standard)", "final_value": 0.2103, "best_value": 0.2103}, {"dataset_name": "ag_news (shuffled)", "final_value": 0.4161, "best_value": 0.4161}, {"dataset_name": "yelp_polarity (shuffled)", "final_value": 0.5037, "best_value": 0.5037}, {"dataset_name": "dbpedia_14 (shuffled)", "final_value": 0.5639, "best_value": 0.5639}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "ag_news (standard)", "final_value": 0.3472, "best_value": 0.3472}, {"dataset_name": "yelp_polarity (standard)", "final_value": 0.3789, "best_value": 0.3789}, {"dataset_name": "dbpedia_14 (standard)", "final_value": 0.1728, "best_value": 0.1728}, {"dataset_name": "ag_news (shuffled)", "final_value": 0.4008, "best_value": 0.4008}, {"dataset_name": "yelp_polarity (shuffled)", "final_value": 0.534, "best_value": 0.534}, {"dataset_name": "dbpedia_14 (shuffled)", "final_value": 0.4422, "best_value": 0.4422}]}, {"metric_name": "training alignment score", "lower_is_better": false, "description": "Final training alignment score", "data": [{"dataset_name": "ag_news (standard)", "final_value": 0.9993, "best_value": 0.9993}, {"dataset_name": "yelp_polarity (standard)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (standard)", "final_value": 0.9971, "best_value": 0.9971}, {"dataset_name": "ag_news (shuffled)", "final_value": 0.9995, "best_value": 0.9995}, {"dataset_name": "yelp_polarity (shuffled)", "final_value": 0.9988, "best_value": 0.9988}, {"dataset_name": "dbpedia_14 (shuffled)", "final_value": 0.9947, "best_value": 0.9947}]}, {"metric_name": "validation alignment score", "lower_is_better": false, "description": "Final validation alignment score", "data": [{"dataset_name": "ag_news (standard)", "final_value": 0.9995, "best_value": 0.9995}, {"dataset_name": "yelp_polarity (standard)", "final_value": 0.9999, "best_value": 0.9999}, {"dataset_name": "dbpedia_14 (standard)", "final_value": 0.9978, "best_value": 0.9978}, {"dataset_name": "ag_news (shuffled)", "final_value": 0.9997, "best_value": 0.9997}, {"dataset_name": "yelp_polarity (shuffled)", "final_value": 0.9995, "best_value": 0.9995}, {"dataset_name": "dbpedia_14 (shuffled)", "final_value": 0.996, "best_value": 0.996}]}, {"metric_name": "MAI score", "lower_is_better": false, "description": "Final MAI score", "data": [{"dataset_name": "ag_news (standard)", "final_value": 0.9427, "best_value": 0.9427}, {"dataset_name": "yelp_polarity (standard)", "final_value": 0.9047, "best_value": 0.9047}, {"dataset_name": "dbpedia_14 (standard)", "final_value": 0.9858, "best_value": 0.9858}, {"dataset_name": "ag_news (shuffled)", "final_value": 0.9326, "best_value": 0.9326}, {"dataset_name": "yelp_polarity (shuffled)", "final_value": 0.8357, "best_value": 0.8357}, {"dataset_name": "dbpedia_14 (shuffled)", "final_value": 0.9367, "best_value": 0.9367}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "ag_news (standard)", "final_value": 0.892, "best_value": 0.892}, {"dataset_name": "yelp_polarity (standard)", "final_value": 0.824, "best_value": 0.824}, {"dataset_name": "dbpedia_14 (standard)", "final_value": 0.972, "best_value": 0.972}, {"dataset_name": "ag_news (shuffled)", "final_value": 0.874, "best_value": 0.874}, {"dataset_name": "yelp_polarity (shuffled)", "final_value": 0.716, "best_value": 0.716}, {"dataset_name": "dbpedia_14 (shuffled)", "final_value": 0.898, "best_value": 0.898}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Cross-entropy loss on the training set.", "data": [{"dataset_name": "ag_news", "final_value": 0.3576, "best_value": 0.3576}, {"dataset_name": "yelp_polarity", "final_value": 0.322, "best_value": 0.322}, {"dataset_name": "dbpedia_14", "final_value": 0.2186, "best_value": 0.2082}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Cross-entropy loss on the validation set.", "data": [{"dataset_name": "ag_news", "final_value": 0.3397, "best_value": 0.3397}, {"dataset_name": "yelp_polarity", "final_value": 0.3548, "best_value": 0.3501}, {"dataset_name": "dbpedia_14", "final_value": 0.1852, "best_value": 0.1728}]}, {"metric_name": "train alignment", "lower_is_better": false, "description": "Alignment score on the training set.", "data": [{"dataset_name": "ag_news", "final_value": 0.9897, "best_value": 0.9996}, {"dataset_name": "yelp_polarity", "final_value": 0.9869, "best_value": 0.9998}, {"dataset_name": "dbpedia_14", "final_value": 0.9871, "best_value": 0.9986}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Alignment score on the validation set.", "data": [{"dataset_name": "ag_news", "final_value": 0.9899, "best_value": 0.9998}, {"dataset_name": "yelp_polarity", "final_value": 0.9874, "best_value": 0.9998}, {"dataset_name": "dbpedia_14", "final_value": 0.9881, "best_value": 0.9989}]}, {"metric_name": "train accuracy", "lower_is_better": false, "description": "Classification accuracy on the training set.", "data": [{"dataset_name": "ag_news", "final_value": 0.8805, "best_value": 0.8825}, {"dataset_name": "yelp_polarity", "final_value": 0.859, "best_value": 0.8635}, {"dataset_name": "dbpedia_14", "final_value": 0.978, "best_value": 0.9785}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Classification accuracy on the validation set.", "data": [{"dataset_name": "ag_news", "final_value": 0.904, "best_value": 0.904}, {"dataset_name": "yelp_polarity", "final_value": 0.852, "best_value": 0.852}, {"dataset_name": "dbpedia_14", "final_value": 0.974, "best_value": 0.976}]}, {"metric_name": "validation MAI", "lower_is_better": false, "description": "Mutual agreement index on the validation set.", "data": [{"dataset_name": "ag_news", "final_value": 0.945, "best_value": 0.945}, {"dataset_name": "yelp_polarity", "final_value": 0.9147, "best_value": 0.9187}, {"dataset_name": "dbpedia_14", "final_value": 0.981, "best_value": 0.9864}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Cross-entropy loss on the training set", "data": [{"dataset_name": "ag_news", "final_value": 0.3576, "best_value": 0.3576}, {"dataset_name": "yelp_polarity", "final_value": 0.322, "best_value": 0.322}, {"dataset_name": "dbpedia_14", "final_value": 0.2186, "best_value": 0.2186}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Cross-entropy loss on the validation set", "data": [{"dataset_name": "ag_news", "final_value": 0.3397, "best_value": 0.3397}, {"dataset_name": "yelp_polarity", "final_value": 0.3548, "best_value": 0.3548}, {"dataset_name": "dbpedia_14", "final_value": 0.1852, "best_value": 0.1852}]}, {"metric_name": "train alignment", "lower_is_better": false, "description": "Alignment metric on the training set", "data": [{"dataset_name": "ag_news", "final_value": 0.9897, "best_value": 0.9897}, {"dataset_name": "yelp_polarity", "final_value": 0.9869, "best_value": 0.9869}, {"dataset_name": "dbpedia_14", "final_value": 0.9871, "best_value": 0.9871}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Alignment metric on the validation set", "data": [{"dataset_name": "ag_news", "final_value": 0.9899, "best_value": 0.9899}, {"dataset_name": "yelp_polarity", "final_value": 0.9874, "best_value": 0.9874}, {"dataset_name": "dbpedia_14", "final_value": 0.9881, "best_value": 0.9881}]}, {"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training set", "data": [{"dataset_name": "ag_news", "final_value": 0.8805, "best_value": 0.8805}, {"dataset_name": "yelp_polarity", "final_value": 0.859, "best_value": 0.859}, {"dataset_name": "dbpedia_14", "final_value": 0.978, "best_value": 0.978}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "ag_news", "final_value": 0.904, "best_value": 0.904}, {"dataset_name": "yelp_polarity", "final_value": 0.852, "best_value": 0.852}, {"dataset_name": "dbpedia_14", "final_value": 0.974, "best_value": 0.974}]}, {"metric_name": "validation MAI", "lower_is_better": false, "description": "MAI metric on the validation set", "data": [{"dataset_name": "ag_news", "final_value": 0.945, "best_value": 0.945}, {"dataset_name": "yelp_polarity", "final_value": 0.9147, "best_value": 0.9147}, {"dataset_name": "dbpedia_14", "final_value": 0.981, "best_value": 0.981}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Cross-entropy loss on the training set", "data": [{"dataset_name": "ag_news", "final_value": 0.3576, "best_value": 0.3576}, {"dataset_name": "yelp_polarity", "final_value": 0.322, "best_value": 0.322}, {"dataset_name": "dbpedia_14", "final_value": 0.2186, "best_value": 0.2082}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Cross-entropy loss on the validation set", "data": [{"dataset_name": "ag_news", "final_value": 0.3397, "best_value": 0.3397}, {"dataset_name": "yelp_polarity", "final_value": 0.3548, "best_value": 0.3501}, {"dataset_name": "dbpedia_14", "final_value": 0.1852, "best_value": 0.1728}]}, {"metric_name": "train alignment", "lower_is_better": false, "description": "Alignment score on the training set", "data": [{"dataset_name": "ag_news", "final_value": 0.9897, "best_value": 0.9996}, {"dataset_name": "yelp_polarity", "final_value": 0.9869, "best_value": 0.9998}, {"dataset_name": "dbpedia_14", "final_value": 0.9871, "best_value": 0.9986}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Alignment score on the validation set", "data": [{"dataset_name": "ag_news", "final_value": 0.9899, "best_value": 0.9998}, {"dataset_name": "yelp_polarity", "final_value": 0.9874, "best_value": 0.9998}, {"dataset_name": "dbpedia_14", "final_value": 0.9881, "best_value": 0.9989}]}, {"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training set", "data": [{"dataset_name": "ag_news", "final_value": 0.8805, "best_value": 0.8825}, {"dataset_name": "yelp_polarity", "final_value": 0.859, "best_value": 0.8635}, {"dataset_name": "dbpedia_14", "final_value": 0.978, "best_value": 0.9785}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "ag_news", "final_value": 0.904, "best_value": 0.904}, {"dataset_name": "yelp_polarity", "final_value": 0.852, "best_value": 0.852}, {"dataset_name": "dbpedia_14", "final_value": 0.974, "best_value": 0.976}]}, {"metric_name": "validation MAI", "lower_is_better": false, "description": "MAI metric on the validation set", "data": [{"dataset_name": "ag_news", "final_value": 0.945, "best_value": 0.945}, {"dataset_name": "yelp_polarity", "final_value": 0.9147, "best_value": 0.9187}, {"dataset_name": "dbpedia_14", "final_value": 0.981, "best_value": 0.9864}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Training cross-entropy loss", "data": [{"dataset_name": "ag_news", "final_value": 0.3576, "best_value": 0.3576}, {"dataset_name": "yelp_polarity", "final_value": 0.322, "best_value": 0.322}, {"dataset_name": "dbpedia_14", "final_value": 0.2082, "best_value": 0.2082}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation cross-entropy loss", "data": [{"dataset_name": "ag_news", "final_value": 0.3397, "best_value": 0.3397}, {"dataset_name": "yelp_polarity", "final_value": 0.3501, "best_value": 0.3501}, {"dataset_name": "dbpedia_14", "final_value": 0.1728, "best_value": 0.1728}]}, {"metric_name": "train alignment", "lower_is_better": false, "description": "Training alignment score", "data": [{"dataset_name": "ag_news", "final_value": 0.9996, "best_value": 0.9996}, {"dataset_name": "yelp_polarity", "final_value": 0.9998, "best_value": 0.9998}, {"dataset_name": "dbpedia_14", "final_value": 0.9986, "best_value": 0.9986}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Validation alignment score", "data": [{"dataset_name": "ag_news", "final_value": 0.9998, "best_value": 0.9998}, {"dataset_name": "yelp_polarity", "final_value": 0.9998, "best_value": 0.9998}, {"dataset_name": "dbpedia_14", "final_value": 0.9989, "best_value": 0.9989}]}, {"metric_name": "train accuracy", "lower_is_better": false, "description": "Training classification accuracy", "data": [{"dataset_name": "ag_news", "final_value": 0.8825, "best_value": 0.8825}, {"dataset_name": "yelp_polarity", "final_value": 0.8635, "best_value": 0.8635}, {"dataset_name": "dbpedia_14", "final_value": 0.9785, "best_value": 0.9785}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation classification accuracy", "data": [{"dataset_name": "ag_news", "final_value": 0.904, "best_value": 0.904}, {"dataset_name": "yelp_polarity", "final_value": 0.852, "best_value": 0.852}, {"dataset_name": "dbpedia_14", "final_value": 0.976, "best_value": 0.976}]}, {"metric_name": "validation MAI", "lower_is_better": false, "description": "Validation MAI score", "data": [{"dataset_name": "ag_news", "final_value": 0.945, "best_value": 0.945}, {"dataset_name": "yelp_polarity", "final_value": 0.9187, "best_value": 0.9187}, {"dataset_name": "dbpedia_14", "final_value": 0.9864, "best_value": 0.9864}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_7d04e00cb67f4c5b995391df64eb1749_proc_4007053/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_7d04e00cb67f4c5b995391df64eb1749_proc_4007053/synthetic_alignment_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_9529c89532744f2795767a760cae81c1_proc_4081279/yelp_polarity_acc_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9529c89532744f2795767a760cae81c1_proc_4081279/ag_news_acc_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9529c89532744f2795767a760cae81c1_proc_4081279/dbpedia_14_acc_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9529c89532744f2795767a760cae81c1_proc_4081279/final_val_accuracy_bar.png"], [], ["../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_baseline_mai_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_baseline_alignment_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_baseline_alignment_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_fine_tune_alignment_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_baseline_alignment_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_fine_tune_mai_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_baseline_loss_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_baseline_mai_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_baseline_loss_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_fine_tune_alignment_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_fine_tune_alignment_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_fine_tune_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_baseline_mai_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_baseline_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_baseline_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_baseline_loss_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_fine_tune_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_fine_tune_mai_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_baseline_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_fine_tune_mai_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_fine_tune_loss_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_fine_tune_loss_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_fine_tune_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_fine_tune_loss_curve.png"], [], ["../../logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/dbpedia_14_loss_curves.png", "../../logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/ag_news_loss_curves.png", "../../logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/yelp_polarity_alignment_curves.png", "../../logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/dbpedia_14_alignment_curves.png", "../../logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/dbpedia_14_mai_curves.png", "../../logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/yelp_polarity_mai_curves.png", "../../logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/ag_news_mai_curves.png", "../../logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/ag_news_alignment_curves.png", "../../logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/yelp_polarity_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_31b90be82e6447e180577bc2ef7e6136_proc_4081278/yelp_polarity_metrics.png", "../../logs/0-run/experiment_results/experiment_31b90be82e6447e180577bc2ef7e6136_proc_4081278/dbpedia_14_metrics.png", "../../logs/0-run/experiment_results/experiment_31b90be82e6447e180577bc2ef7e6136_proc_4081278/ag_news_metrics.png"], ["../../logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/yelp_polarity_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/dbpedia_14_loss_curves.png", "../../logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/ag_news_loss_curves.png", "../../logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/dbpedia_14_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/yelp_polarity_loss_curves.png", "../../logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/ag_news_accuracy_curves.png"], ["../../logs/0-run/experiment_results/experiment_8dcaca62f83e486a85260559d83fa8d8_proc_4081279/dbpedia_14_loss_alignment.png", "../../logs/0-run/experiment_results/experiment_8dcaca62f83e486a85260559d83fa8d8_proc_4081279/ag_news_loss_alignment.png", "../../logs/0-run/experiment_results/experiment_8dcaca62f83e486a85260559d83fa8d8_proc_4081279/yelp_polarity_loss_alignment.png"], ["../../logs/0-run/experiment_results/experiment_5423d385102944c4a70669fbe1229642_proc_4081278/avg_loss_gamma_comparison.png", "../../logs/0-run/experiment_results/experiment_5423d385102944c4a70669fbe1229642_proc_4081278/avg_alignment_gamma_comparison.png", "../../logs/0-run/experiment_results/experiment_5423d385102944c4a70669fbe1229642_proc_4081278/avg_MAI_gamma_comparison.png"], ["../../logs/0-run/experiment_results/experiment_00b9d440cc414af5af98e9b2d351e02a_proc_4081279/dbpedia_14_MAI_vs_heads.png", "../../logs/0-run/experiment_results/experiment_00b9d440cc414af5af98e9b2d351e02a_proc_4081279/ag_news_random_loss_curves.png", "../../logs/0-run/experiment_results/experiment_00b9d440cc414af5af98e9b2d351e02a_proc_4081279/ag_news_importance_loss_curves.png", "../../logs/0-run/experiment_results/experiment_00b9d440cc414af5af98e9b2d351e02a_proc_4081279/ag_news_MAI_vs_heads.png", "../../logs/0-run/experiment_results/experiment_00b9d440cc414af5af98e9b2d351e02a_proc_4081279/yelp_polarity_MAI_vs_heads.png"], ["../../logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/dbpedia_14_alignment_curve.png", "../../logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/ag_news_loss_curve.png", "../../logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/yelp_polarity_alignment_curve.png", "../../logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/yelp_polarity_mai_curve.png", "../../logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/yelp_polarity_loss_curve.png", "../../logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/dbpedia_14_loss_curve.png", "../../logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/dbpedia_14_mai_curve.png", "../../logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/ag_news_mai_curve.png", "../../logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/ag_news_alignment_curve.png"], ["../../logs/0-run/experiment_results/experiment_ec7d598449024b57a51ff39af928139d_proc_4081278/yelp_polarity_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_ec7d598449024b57a51ff39af928139d_proc_4081278/dbpedia_14_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_ec7d598449024b57a51ff39af928139d_proc_4081278/ag_news_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_ec7d598449024b57a51ff39af928139d_proc_4081278/mai_vs_dropout_rate.png"], [], ["../../logs/0-run/experiment_results/experiment_8b1e5415f46b4fd4a24eeaf1ff52ebec_proc_4081279/accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_8b1e5415f46b4fd4a24eeaf1ff52ebec_proc_4081279/mai_curves.png", "../../logs/0-run/experiment_results/experiment_8b1e5415f46b4fd4a24eeaf1ff52ebec_proc_4081279/alignment_curves.png", "../../logs/0-run/experiment_results/experiment_8b1e5415f46b4fd4a24eeaf1ff52ebec_proc_4081279/loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_shared_fc1_alignment_curves.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_shared_fc1_mai.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_independent_alignment_curves.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_independent_alignment_curves.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_independent_mai.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_shared_fc1_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_shared_fc1_fc2_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_shared_fc1_fc2_alignment_curves.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_independent_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_shared_fc1_mai.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_shared_fc1_fc2_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_independent_mai.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_shared_fc1_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_shared_fc1_fc2_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_shared_fc1_fc2_alignment_curves.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_independent_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_shared_fc1_fc2_mai.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_shared_fc1_fc2_alignment_curves.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_independent_mai.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_shared_fc1_mai.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_shared_fc1_fc2_mai.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_shared_fc1_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_independent_alignment_curves.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_shared_fc1_fc2_mai.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_shared_fc1_alignment_curves.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_shared_fc1_alignment_curves.png", "../../logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_independent_loss_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_5adec4951f8740a090873130a88a7445_proc_4081279/yelp_polarity_metrics.png", "../../logs/0-run/experiment_results/experiment_5adec4951f8740a090873130a88a7445_proc_4081279/dbpedia_14_metrics.png", "../../logs/0-run/experiment_results/experiment_5adec4951f8740a090873130a88a7445_proc_4081279/ag_news_metrics.png", "../../logs/0-run/experiment_results/experiment_5adec4951f8740a090873130a88a7445_proc_4081279/mai_vs_temperature.png"], ["../../logs/0-run/experiment_results/experiment_a01c5371854b472d9d698de9c013b3bb_proc_4081278/yelp_polarity_metrics.png", "../../logs/0-run/experiment_results/experiment_a01c5371854b472d9d698de9c013b3bb_proc_4081278/dbpedia_14_metrics.png", "../../logs/0-run/experiment_results/experiment_a01c5371854b472d9d698de9c013b3bb_proc_4081278/ag_news_metrics.png", "../../logs/0-run/experiment_results/experiment_a01c5371854b472d9d698de9c013b3bb_proc_4081278/mai_vs_temperature.png"], ["../../logs/0-run/experiment_results/experiment_c54e9756eab849f398468a9dee659963_proc_4081280/yelp_polarity_metrics.png", "../../logs/0-run/experiment_results/experiment_c54e9756eab849f398468a9dee659963_proc_4081280/dbpedia_14_metrics.png", "../../logs/0-run/experiment_results/experiment_c54e9756eab849f398468a9dee659963_proc_4081280/ag_news_metrics.png", "../../logs/0-run/experiment_results/experiment_c54e9756eab849f398468a9dee659963_proc_4081280/mai_vs_temperature.png"], ["../../logs/0-run/experiment_results/experiment_85f8d185ab7644bb8c36bce01d477867_proc_4081279/yelp_polarity_metrics.png", "../../logs/0-run/experiment_results/experiment_85f8d185ab7644bb8c36bce01d477867_proc_4081279/dbpedia_14_metrics.png", "../../logs/0-run/experiment_results/experiment_85f8d185ab7644bb8c36bce01d477867_proc_4081279/ag_news_metrics.png", "../../logs/0-run/experiment_results/experiment_85f8d185ab7644bb8c36bce01d477867_proc_4081279/mai_vs_temperature.png"], ["../../logs/0-run/experiment_results/seed_aggregation_a7a0de092f2948be8b6a36d0fd46af26/ag_news_aggregated_metrics.png", "../../logs/0-run/experiment_results/seed_aggregation_a7a0de092f2948be8b6a36d0fd46af26/yelp_polarity_aggregated_metrics.png", "../../logs/0-run/experiment_results/seed_aggregation_a7a0de092f2948be8b6a36d0fd46af26/dbpedia_14_aggregated_metrics.png", "../../logs/0-run/experiment_results/seed_aggregation_a7a0de092f2948be8b6a36d0fd46af26/aggregated_mai_vs_temperature.png"]], "plot_paths": [["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_7d04e00cb67f4c5b995391df64eb1749_proc_4007053/synthetic_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_7d04e00cb67f4c5b995391df64eb1749_proc_4007053/synthetic_alignment_curves.png"], [], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9529c89532744f2795767a760cae81c1_proc_4081279/yelp_polarity_acc_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9529c89532744f2795767a760cae81c1_proc_4081279/ag_news_acc_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9529c89532744f2795767a760cae81c1_proc_4081279/dbpedia_14_acc_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9529c89532744f2795767a760cae81c1_proc_4081279/final_val_accuracy_bar.png"], [], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_baseline_mai_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_baseline_alignment_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_baseline_alignment_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_fine_tune_alignment_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_baseline_alignment_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_fine_tune_mai_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_baseline_loss_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_baseline_mai_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_baseline_loss_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_fine_tune_alignment_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_fine_tune_alignment_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_fine_tune_accuracy_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_baseline_mai_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_baseline_accuracy_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_baseline_accuracy_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_baseline_loss_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_fine_tune_accuracy_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_fine_tune_mai_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_baseline_accuracy_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_fine_tune_mai_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_fine_tune_loss_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_fine_tune_loss_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_fine_tune_accuracy_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_fine_tune_loss_curve.png"], [], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/dbpedia_14_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/ag_news_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/yelp_polarity_alignment_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/dbpedia_14_alignment_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/dbpedia_14_mai_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/yelp_polarity_mai_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/ag_news_mai_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/ag_news_alignment_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/yelp_polarity_loss_curves.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_31b90be82e6447e180577bc2ef7e6136_proc_4081278/yelp_polarity_metrics.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_31b90be82e6447e180577bc2ef7e6136_proc_4081278/dbpedia_14_metrics.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_31b90be82e6447e180577bc2ef7e6136_proc_4081278/ag_news_metrics.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/yelp_polarity_accuracy_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/dbpedia_14_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/ag_news_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/dbpedia_14_accuracy_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/yelp_polarity_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/ag_news_accuracy_curves.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_8dcaca62f83e486a85260559d83fa8d8_proc_4081279/dbpedia_14_loss_alignment.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_8dcaca62f83e486a85260559d83fa8d8_proc_4081279/ag_news_loss_alignment.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_8dcaca62f83e486a85260559d83fa8d8_proc_4081279/yelp_polarity_loss_alignment.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_5423d385102944c4a70669fbe1229642_proc_4081278/avg_loss_gamma_comparison.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_5423d385102944c4a70669fbe1229642_proc_4081278/avg_alignment_gamma_comparison.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_5423d385102944c4a70669fbe1229642_proc_4081278/avg_MAI_gamma_comparison.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_00b9d440cc414af5af98e9b2d351e02a_proc_4081279/dbpedia_14_MAI_vs_heads.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_00b9d440cc414af5af98e9b2d351e02a_proc_4081279/ag_news_random_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_00b9d440cc414af5af98e9b2d351e02a_proc_4081279/ag_news_importance_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_00b9d440cc414af5af98e9b2d351e02a_proc_4081279/ag_news_MAI_vs_heads.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_00b9d440cc414af5af98e9b2d351e02a_proc_4081279/yelp_polarity_MAI_vs_heads.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/dbpedia_14_alignment_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/ag_news_loss_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/yelp_polarity_alignment_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/yelp_polarity_mai_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/yelp_polarity_loss_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/dbpedia_14_loss_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/dbpedia_14_mai_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/ag_news_mai_curve.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/ag_news_alignment_curve.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_ec7d598449024b57a51ff39af928139d_proc_4081278/yelp_polarity_accuracy_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_ec7d598449024b57a51ff39af928139d_proc_4081278/dbpedia_14_accuracy_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_ec7d598449024b57a51ff39af928139d_proc_4081278/ag_news_accuracy_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_ec7d598449024b57a51ff39af928139d_proc_4081278/mai_vs_dropout_rate.png"], [], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_8b1e5415f46b4fd4a24eeaf1ff52ebec_proc_4081279/accuracy_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_8b1e5415f46b4fd4a24eeaf1ff52ebec_proc_4081279/mai_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_8b1e5415f46b4fd4a24eeaf1ff52ebec_proc_4081279/alignment_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_8b1e5415f46b4fd4a24eeaf1ff52ebec_proc_4081279/loss_curves.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_shared_fc1_alignment_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_shared_fc1_mai.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_independent_alignment_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_independent_alignment_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_independent_mai.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_shared_fc1_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_shared_fc1_fc2_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_shared_fc1_fc2_alignment_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_independent_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_shared_fc1_mai.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_shared_fc1_fc2_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_independent_mai.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_shared_fc1_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_shared_fc1_fc2_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_shared_fc1_fc2_alignment_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_independent_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_shared_fc1_fc2_mai.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_shared_fc1_fc2_alignment_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_independent_mai.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_shared_fc1_mai.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_shared_fc1_fc2_mai.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_shared_fc1_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_independent_alignment_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_shared_fc1_fc2_mai.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_shared_fc1_alignment_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_shared_fc1_alignment_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_independent_loss_curves.png"], [], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_5adec4951f8740a090873130a88a7445_proc_4081279/yelp_polarity_metrics.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_5adec4951f8740a090873130a88a7445_proc_4081279/dbpedia_14_metrics.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_5adec4951f8740a090873130a88a7445_proc_4081279/ag_news_metrics.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_5adec4951f8740a090873130a88a7445_proc_4081279/mai_vs_temperature.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_a01c5371854b472d9d698de9c013b3bb_proc_4081278/yelp_polarity_metrics.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_a01c5371854b472d9d698de9c013b3bb_proc_4081278/dbpedia_14_metrics.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_a01c5371854b472d9d698de9c013b3bb_proc_4081278/ag_news_metrics.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_a01c5371854b472d9d698de9c013b3bb_proc_4081278/mai_vs_temperature.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c54e9756eab849f398468a9dee659963_proc_4081280/yelp_polarity_metrics.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c54e9756eab849f398468a9dee659963_proc_4081280/dbpedia_14_metrics.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c54e9756eab849f398468a9dee659963_proc_4081280/ag_news_metrics.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c54e9756eab849f398468a9dee659963_proc_4081280/mai_vs_temperature.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_85f8d185ab7644bb8c36bce01d477867_proc_4081279/yelp_polarity_metrics.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_85f8d185ab7644bb8c36bce01d477867_proc_4081279/dbpedia_14_metrics.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_85f8d185ab7644bb8c36bce01d477867_proc_4081279/ag_news_metrics.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_85f8d185ab7644bb8c36bce01d477867_proc_4081279/mai_vs_temperature.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_a7a0de092f2948be8b6a36d0fd46af26/ag_news_aggregated_metrics.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_a7a0de092f2948be8b6a36d0fd46af26/yelp_polarity_aggregated_metrics.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_a7a0de092f2948be8b6a36d0fd46af26/dbpedia_14_aggregated_metrics.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_a7a0de092f2948be8b6a36d0fd46af26/aggregated_mai_vs_temperature.png"]], "plot_analyses": [[], [], [{"analysis": "Dataset: yelp_polarity - Baseline fine-tuning achieves faster loss reduction and higher training accuracy (from ~0.63 to ~0.86) compared to the linear probe (from ~0.71 to ~0.84). Validation accuracy for the linear probe starts slightly above the baseline at epoch 1 (~0.84 vs ~0.83) and remains marginally higher at epoch 3 (~0.83 vs ~0.82), but both converge by epoch 2 with only ~1% difference. Baseline validation loss is consistently lower, indicating stronger generalization despite the slightly lower early validation accuracy.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9529c89532744f2795767a760cae81c1_proc_4081279/yelp_polarity_acc_loss_curves.png"}, {"analysis": "Dataset: ag_news - Both methods converge by epoch 2 with validation accuracy near 0.88\u20130.89. Baseline fine-tuning yields lower training and validation losses throughout (e.g., val loss dropping from ~0.48 to ~0.33) and marginally higher validation accuracy at epochs 1 and 2 (~0.87 vs ~0.86; ~0.88 vs ~0.87). The linear probe catches up by epoch 3, ending with nearly identical validation accuracy (~0.89) but still retains somewhat higher loss values, signalling slower adaptation.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9529c89532744f2795767a760cae81c1_proc_4081279/ag_news_acc_loss_curves.png"}, {"analysis": "Dataset: dbpedia_14 - Validation accuracy for the linear probe is slightly higher in early epochs (~0.93 vs ~0.915 at epoch 1; ~0.96 vs ~0.955 at epoch 2) but baseline overtakes by epoch 3 (~0.97 vs ~0.967). Training and validation losses are consistently lower for the baseline (train loss from ~2.7\u21920.22 vs ~2.6\u21920.40; val loss from ~0.78\u21920.17 vs ~1.1\u21920.35), showing more efficient convergence and stronger fine-tuning on this larger classification task.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9529c89532744f2795767a760cae81c1_proc_4081279/dbpedia_14_acc_loss_curves.png"}, {"analysis": "Final Validation Accuracy by Dataset and Ablation - ag_news: linear probe edges baseline by ~1% (0.88 vs 0.87); yelp_polarity: linear probe higher by ~1% (0.84 vs 0.83); dbpedia_14: baseline higher by ~1% (0.98 vs 0.97). These differences are within 1% across all tasks, indicating that the lighter-weight linear probe remains competitive, while full fine-tuning consistently yields more robust loss reduction and slightly better performance on the largest dataset.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9529c89532744f2795767a760cae81c1_proc_4081279/final_val_accuracy_bar.png"}], [], [{"analysis": "baseline yelp_polarity alignment curve shows train alignment rising sharply from ~0.956 at epoch 1 to ~0.999 by epoch 2, while validation alignment starts high (~0.997) and nearly saturates at ~0.9999 by epoch 3. Minimal train\u2013val gap indicates strong generalization and quick convergence, suggesting further epochs offer diminishing returns for alignment score.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_baseline_mai_curve.png"}, {"analysis": "fine_tune yelp_polarity alignment curve begins with a higher train alignment (~0.975) and validation alignment (~0.999) at epoch 1 compared to baseline. Both metrics reach near-ceiling (~0.9997 train, ~0.9997 val) by epoch 2, with slight fluctuations at epoch 3. Fine-tuning yields better early-stage alignment but a marginally flatter improvement towards the end.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_baseline_alignment_curve.png"}, {"analysis": "baseline ag_news alignment curve exhibits a similar pattern: train alignment climbs from ~0.968 at epoch 1 to ~0.998 by epoch 2 and ~0.999 by epoch 3. Validation alignment steadily increases from ~0.9977 to ~0.9993. The small and diminishing gap underscores rapid convergence and low overfitting risk under the static baseline.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_baseline_alignment_curve.png"}, {"analysis": "fine_tune ag_news alignment curve starts at a higher initial train alignment (~0.975) and validation alignment (~0.998) than baseline. Both metrics hit ~0.999 by epoch 2 and show slight gains into epoch 3 (~0.9998 train, ~0.9996 val), indicating that fine-tuning accelerates convergence and marginally boosts final alignment.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_fine_tune_alignment_curve.png"}, {"analysis": "baseline dbpedia_14 alignment curve shows a wider initial train\u2013val gap (train ~0.970 vs. val ~0.9905 at epoch 1) that narrows over epochs, reaching train ~0.997 and val ~0.9972 by epoch 3. The monotonic rise highlights strong learning but suggests more room for early improvement versus other datasets.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_baseline_alignment_curve.png"}, {"analysis": "fine_tune dbpedia_14 alignment curve has a slightly lower start for train (~0.967) but leaps to ~0.9997 by epoch 2 and ~0.9998 by epoch 3, with validation alignment rising from ~0.9987 to ~0.99975. Fine-tuning closes the train\u2013val gap faster and achieves the highest overall alignment, especially in early epochs.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_fine_tune_mai_curve.png"}, {"analysis": "baseline yelp_polarity MAI curve increases from ~0.9062 at epoch 1 to ~0.9094 at epoch 2 before dipping to ~0.9047 at epoch 3. This peak at epoch 2 suggests optimal alignment improvement occurs mid-training, with slight degradation if training continues without adaptation.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/dbpedia_14_baseline_loss_curve.png"}, {"analysis": "fine_tune yelp_polarity MAI curve starts at ~0.922, dips to ~0.920 at epoch 2, then rises to ~0.928 at epoch 3. Overall higher MAI values than baseline and a recovery in the final epoch point to more stable and enhanced alignment adjustment through fine-tuning.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_baseline_mai_curve.png"}, {"analysis": "baseline dbpedia_14 MAI curve shows steady gains from ~0.9504 at epoch 1 to ~0.9798 at epoch 2 and ~0.9856 at epoch 3, reflecting consistent improvement in mutual alignment over training without any saturation within three epochs.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/yelp_polarity_baseline_loss_curve.png"}, {"analysis": "fine_tune dbpedia_14 MAI curve surges from ~0.9913 at epoch 1 to ~0.9947 at epoch 2 before a slight drop to ~0.9927 at epoch 3. Near-ceiling values from the start and peak at epoch 2 indicate rapid mutual alignment benefits from fine-tuning, with marginal returns for deeper training.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_9272ea401a914b37ad61657f71e135c1_proc_4081280/ag_news_fine_tune_alignment_curve.png"}], [], [{"analysis": "Loss Curves on dbpedia_14 (Training & Validation Loss): All models show rapid loss reduction by epoch 2 and further improvement into epoch 3. Zero and light (0.1) dropout achieve the lowest training and validation losses, with nearly identical curves. Moderate dropout (0.3) slows convergence slightly and yields higher losses at each epoch. Heavy dropout (0.5) exhibits the slowest convergence and highest losses, particularly noticeable at epoch 1 (training loss ~3.4, validation ~1.23) and remains above other settings through epoch 3. Little to no overfitting is observed, but heavy dropout underperforms across both splits.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/dbpedia_14_loss_curves.png"}, {"analysis": "Loss Curves on ag_news (Training & Validation Loss): All variants follow a steep decline from epoch 1 to 2, then plateau. Dropout 0.1 yields the lowest training loss at epoch 3, slightly below no-dropout, while dropout 0.5 again shows the highest training loss. On validation, dropout 0.3 reaches the lowest loss by epoch 3 (~0.35), outperforming both no-dropout and light dropout. Heavy dropout remains suboptimal. These patterns suggest moderate dropout can improve generalization for ag_news.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/ag_news_loss_curves.png"}, {"analysis": "Alignment Curves on yelp_polarity (Training & Validation Alignment): Training alignment climbs for all settings, with no-dropout reaching near-perfect alignment fastest. Increasing dropout consistently lowers training alignment at each epoch. In validation, alignment is uniformly high (>0.99). No-dropout and 0.1 are essentially tied at the top; dropout 0.3 lags behind and dropout 0.5 recovers somewhat but still trails light settings. Overall, dropout reduces alignment effectiveness, with heavier rates having a stronger negative effect.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/yelp_polarity_alignment_curves.png"}, {"analysis": "Alignment Curves on dbpedia_14 (Training & Validation Alignment): Zero dropout yields the highest training alignment growth (from ~0.97 to ~0.997), followed by 0.1, 0.3, and 0.5 in descending order. Validation alignment similarly ranks settings: no-dropout highest, then 0.1, then 0.3, then 0.5. All settings improve over epochs, but alignment capacity degrades progressively as dropout rate increases, indicating dropout undermines the model\u2019s ability to align predictions with the inferred user model.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/dbpedia_14_alignment_curves.png"}, {"analysis": "MAI Curves on dbpedia_14: Mutual alignment index improves sharply from epoch 1 to 2 for all settings, then plateaus. No-dropout achieves the highest MAI at each epoch. Light and moderate dropout show slightly lower MAI, with dropout 0.5 worst at epoch 2 but catching up by epoch 3. Differences narrow over time but remain: heavier dropout yields marginally lower final MAI, suggesting some adverse impact on bidirectional alignment quality.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/dbpedia_14_mai_curves.png"}, {"analysis": "MAI Curves on yelp_polarity: No-dropout starts higher but barely improves after epoch 2, actually dipping by epoch 3. Dropout 0.1, 0.3, and 0.5 all show stronger gains through epoch 3, with dropout 0.5 achieving the highest MAI (~0.93), followed by 0.3 and 0.1. Moderate-to-heavy dropout enhances MAI progression on yelp_polarity at later epochs, indicating beneficial regularization in this dataset.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/yelp_polarity_mai_curves.png"}, {"analysis": "MAI Curves on ag_news: All settings improve. No-dropout and 0.1 dropouts lead at epoch 2, but by epoch 3 no-dropout edges ahead (~0.943), with 0.1 slightly below and 0.3/0.5 trailing. Dropout accelerates initial MAI growth but heavier rates slow final convergence relative to no-dropout. Overall, minimal or no dropout yields the best MAI on ag_news.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/ag_news_mai_curves.png"}, {"analysis": "Alignment Curves on ag_news (Training & Validation Alignment): Training alignment: no-dropout quickly reaches near-perfect (>0.999) by epoch 3. Dropout 0.1 plateaus slightly lower (~0.989), dropout 0.3 and 0.5 lag significantly. Validation alignment is uniformly high (>0.993), with no-dropout and 0.1 best, moderate dropout trailing and heavy dropout lowest. Again, dropout consistently trades off alignment quality for robustness.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/ag_news_alignment_curves.png"}, {"analysis": "Loss Curves on yelp_polarity (Training & Validation Loss): Training losses drop steeply to epoch 2 for all settings then decrease mildly. No-dropout attains the lowest final training loss (~0.35), followed by dropout 0.1, 0.3, and 0.5. Validation shows dropout 0.3 yields the lowest loss at epoch 3 (~0.34), light dropout next, with no-dropout and heavy dropout slightly higher. Moderate dropout improves generalization, but heavy dropout underperforms.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c01d46ac1e9c48da88cc5010da1fc00e_proc_4081279/yelp_polarity_loss_curves.png"}], [{"analysis": "yelp_polarity_metrics: Training losses for all activations plummet from ~1.8\u20131.6 at epoch 1 to ~0.35 by epoch 2 and ~0.30 by epoch 3. Validation losses follow suit, leveling off around ~0.38 with negligible overfitting. Among activations, Tanh and LeakyReLU have marginally lower training loss at epoch 3, but all validations converge similarly. MAI: Tanh exhibits the largest early jump (0.892\u21920.919), while GELU and LeakyReLU achieve the highest MAI (~0.922) by epoch 3, edging out ReLU which plateaus and dips slightly. This suggests strong early alignment from Tanh and top\u2010end alignment from GELU/LeakyReLU.", "},{": "},{", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_31b90be82e6447e180577bc2ef7e6136_proc_4081278/yelp_polarity_metrics.png"}], [{"analysis": "Training accuracy curves for yelp_polarity overlap almost perfectly across all four \u03bb configurations, rising from ~0.63 at epoch 1 to ~0.87 by epoch 3. Validation accuracy peaks around epoch 2 for \u03bb=0.1 (0.838) and \u03bb=0.5 (0.842) then dips by epoch 3, while \u03bb=1.0 achieves the highest initial validation accuracy (0.848) and remains marginally above the others at epoch 3. Overall differences are under 1%, indicating rapid convergence and only slight generalization gains for the highest \u03bb early on.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/yelp_polarity_accuracy_curves.png"}, {"analysis": "For dbpedia_14, training loss decreases identically from ~2.7 to ~0.2 over three epochs under every \u03bb. Validation loss follows the same pattern, falling from ~0.80 to ~0.17 with no visible separation between \u03bb settings. This uniformity implies that the \u03bb hyperparameter has negligible impact on convergence speed or final loss on this dataset.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/dbpedia_14_loss_curves.png"}, {"analysis": "On ag_news, training loss uniformly declines from ~2.1 to ~0.36 across all \u03bb values. Validation loss also drops in lockstep from ~0.495 to ~0.345. No \u03bb configuration stands out, demonstrating that this ablation has no measurable effect on loss reduction or overfitting for ag_news.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/ag_news_loss_curves.png"}, {"analysis": "Training accuracy for dbpedia_14 climbs from ~0.67 to ~0.98 by epoch 3 under every \u03bb. Validation accuracy shows a minor initial spread (\u03bb=0.0 at ~0.915 vs. \u03bb=1.0 at ~0.923 at epoch 1) but converges to ~0.975 for all \u03bb by epoch 3. The tiny early advantage of \u03bb=1.0 vanishes after epoch 2, indicating minimal sensitivity to this hyperparameter in final performance.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/dbpedia_14_accuracy_curves.png"}, {"analysis": "Yelp_polarity training loss declines identically (~1.8\u21920.35) across \u03bb settings. Validation loss reveals a slight edge for \u03bb=1.0: at epoch 2 it is lowest (0.377 vs. 0.379\u20130.383) and this margin persists into epoch 3 (0.370 vs. 0.377\u20130.380). The other \u03bb curves remain essentially indistinguishable, suggesting only a modest generalization benefit for the highest \u03bb.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/yelp_polarity_loss_curves.png"}, {"analysis": "Validation accuracy on ag_news shows the clearest \u03bb-dependent effect: \u03bb=1.0 rises from ~0.860 at epoch 1 to ~0.894 by epoch 3, outperforming \u03bb=0.0 (0.872\u21920.887) and \u03bb=0.5 (0.868\u21920.892). Training accuracy is nearly identical across all \u03bb. Thus, only on ag_news does \u03bb=1.0 yield a modest but consistent generalization improvement.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_e1adbc0569d4459b888faf7a298452ab_proc_4081280/ag_news_accuracy_curves.png"}], [{"analysis": "dbpedia_14 dataset: Loss curves show that shallow representations (layer_1) start with the highest training loss (~4.0) and plateau around 2.5 by epoch 3, with validation loss roughly mirroring this behavior (~2.6\u21922.5). Intermediate layer (layer_3) quickly drops from ~2.8 to ~1.0 by epoch 2 and then to ~0.7 on train (val ~1.5\u21920.6). Deepest layer (layer_5) converges fastest, falling to ~0.2 train loss by epoch 3 (val ~0.9\u21920.2). The ensemble of the last two layers (avg_last2) yields a training loss ~0.4 and validation ~0.3 by epoch 3, closely matching layer_5. Bidirectional alignment curves rise steadily for all components: layer_1 climbs from ~0.53\u21920.60, layer_3 from 0.60\u21920.85 (val 0.75\u21920.82), layer_5 from 0.80\u21920.90 (val 0.83\u21920.92), and avg_last2 from 0.65\u21920.90 (val 0.82\u21920.92). Deeper layers and their ensemble achieve both lower loss and higher alignment, and improvements flatten between epochs 2\u20133.", "dataset": "dbpedia_14", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_8dcaca62f83e486a85260559d83fa8d8_proc_4081279/dbpedia_14_loss_alignment.png"}, {"analysis": "ag_news dataset: Loss for layer_1 drops from ~2.8 to ~1.3 by epoch 2 and remains flat on both train/val (~1.3/1.4). Layer_3 falls more sharply to ~0.6 train (val ~0.7) by epoch 2 and further to ~0.5 train/0.45 val by epoch 3. Layer_5 achieves ~0.5\u21920.3 train and ~0.35\u21920.3 val over the same epochs. avg_last2 tracks layer_5 closely (~0.45\u21920.35 train, ~0.35\u21920.33 val). Alignment increases markedly: layer_1 from ~0.63\u21920.75 (val ~0.73\u21920.74), layer_3 from ~0.70\u21920.90 (val ~0.85\u21920.90), layer_5 from ~0.79\u21920.94 (val ~0.92\u21920.94), avg_last2 nearly identical to layer_5. Again, deeper layers and their combination yield superior alignment and loss profiles with diminishing gains after epoch 2.", "dataset": "ag_news", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_8dcaca62f83e486a85260559d83fa8d8_proc_4081279/ag_news_loss_alignment.png"}, {"analysis": "yelp_polarity dataset: Layer_1 loss declines to ~0.67 train/~0.70 val by epoch 2, then plateaus. Layer_3 reaches ~0.68 train/~0.66 val, while layer_5 drops to ~0.45 train/~0.48 val by epoch 3. avg_last2 settles at ~0.50 train/~0.50 val. Alignment curves follow the familiar pattern: layer_1 from ~0.72\u21920.85 (val ~0.75\u21920.84), layer_3 from ~0.76\u21920.87 (val ~0.85\u21920.88), layer_5 from ~0.78\u21920.92 (val ~0.89\u21920.90), and avg_last2 from ~0.80\u21920.91 (val ~0.88\u21920.91). Consistently, the deepest layers (and their ensemble) outperform in alignment and converge more rapidly in loss, with most gains already realized by the second epoch.", "dataset": "yelp_polarity", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_8dcaca62f83e486a85260559d83fa8d8_proc_4081279/yelp_polarity_loss_alignment.png"}], [{"analysis": "Avg Loss vs Epoch shows a dramatic drop in both training and validation loss between epoch 1 and epoch 2, followed by a marginal decline by epoch 3. At epoch 1, \u03b3 = 5 yields the lowest losses (\u22481.55 train, \u22480.08 val), followed by \u03b3 = 2 (\u22481.85 train, \u22480.22 val) and \u03b3 = 1 (\u22481.97 train, \u22480.35 val). By epoch 2, losses converge to roughly 0.23/0.17 (train/val) for \u03b3 = 1, 0.18/0.12 for \u03b3 = 2, and 0.05/0.01 for \u03b3 = 5, and by epoch 3 these flatten to around 0.17/0.17, 0.14/0.14, and 0.03/0.02 respectively. The validation curves closely track the training curves without any sign of overfitting, suggesting good generalization. Higher \u03b3 accelerates convergence and yields lower final loss, indicating that the corresponding component weight is critical for loss optimization.", "valid_plots_received": true, "vlm_feedback_summary": "Loss is steeply reduced in the first epoch and nearly converges by the second. Larger \u03b3 values improve both convergence speed and final loss. No significant overfitting observed; validation curves mirror training.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_5423d385102944c4a70669fbe1229642_proc_4081278/avg_loss_gamma_comparison.png"}, {"analysis": "Avg Alignment vs Epoch rises sharply from epoch 1 to epoch 2, then edges toward saturation by epoch 3. Initial training alignment at epoch 1 is around 0.965 (\u03b3 = 1), 0.968 (\u03b3 = 2), and 0.969 (\u03b3 = 5), while validation alignment starts at 0.996, 0.996, and 0.995 respectively. At epoch 2 these all jump to approximately 0.996\u20130.997 for training and 0.998\u20130.999 for validation, and by epoch 3 they reach \u22480.998\u20130.999 (train) and \u22480.999 (val) across all \u03b3 settings. Differences between \u03b3 values are negligible after epoch 2, and validation alignment consistently exceeds training alignment by a small margin, indicating robust and generalizable mutual\u2010model alignment.", "valid_plots_received": true, "vlm_feedback_summary": "Alignment improves nearly to ceiling by the second epoch regardless of \u03b3. Validation alignment slightly outperforms training, and all models converge to ~0.998\u20130.999 alignment by epoch 3.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_5423d385102944c4a70669fbe1229642_proc_4081278/avg_alignment_gamma_comparison.png"}, {"analysis": "Avg MAI vs Epoch mirrors the alignment trend, with a steep jump between epoch 1 and epoch 2, then a mild increase to epoch 3. At epoch 1, training MAI is about 0.77 (\u03b3 = 1), 0.77 (\u03b3 = 2), 0.74 (\u03b3 = 5), and validation MAI is 0.92, 0.93, and 0.92 respectively. By epoch 2, all settings reach \u22480.94\u20130.945 (train) and \u22480.94\u20130.945 (val). Final values at epoch 3 sit near 0.948\u20130.95 for training and 0.948\u20130.949 for validation, with \u03b3 = 2 yielding the highest MAI. The close alignment of training and validation metrics again signals minimal overfitting. This rapid convergence suggests two epochs suffice to achieve near\u2010optimal mutual attention and interpretability interaction scores.", "valid_plots_received": true, "vlm_feedback_summary": "MAI jumps from ~0.75 to ~0.94 in one epoch and saturates by the second. \u03b3 = 2 gives the highest final MAI but differences among \u03b3 settings are small. Good generalization is observed.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_5423d385102944c4a70669fbe1229642_proc_4081278/avg_MAI_gamma_comparison.png"}], [{"analysis": "MAI improves as head count increases for both ablation methods, with importance-based ablation consistently outperforming random pruning at lower head counts. At 2 heads, importance (0.867) significantly exceeds random (0.742). Gains diminish at higher head counts, and both methods converge near 0.985 MAI at 12 heads, suggesting redundancy across many heads.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_00b9d440cc414af5af98e9b2d351e02a_proc_4081279/dbpedia_14_MAI_vs_heads.png"}, {"analysis": "Random-ablation model on ag_news with 12 heads shows rapid loss reduction: training loss drops from ~2.08 to ~0.36 by epoch 3, while validation loss decreases more modestly from ~0.49 to ~0.35. The gap narrows across epochs, indicating stable generalization and limited overfitting under random pruning.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_00b9d440cc414af5af98e9b2d351e02a_proc_4081279/ag_news_random_loss_curves.png"}, {"analysis": "Importance-based ablation on ag_news (12 heads) also yields fast convergence: train loss decreases from ~1.95 to ~0.36 by epoch 3, with validation loss falling from ~0.47 to ~0.34. Compared to random, importance pruning achieves slightly lower initial training loss and marginally reduced final validation loss, indicating more efficient head selection.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_00b9d440cc414af5af98e9b2d351e02a_proc_4081279/ag_news_importance_loss_curves.png"}, {"analysis": "On ag_news MAI vs. head count, both methods benefit from additional heads, but importance ablation leads at low (2 heads: 0.908 vs. 0.862) and mid counts (8 heads: 0.938 vs. 0.935). Random briefly surpasses at 4 heads (0.926 vs. 0.920). Convergence near 0.943\u20130.942 at 12 heads indicates diminishing returns from head importance ranking as capacity increases.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_00b9d440cc414af5af98e9b2d351e02a_proc_4081279/ag_news_MAI_vs_heads.png"}, {"analysis": "Yelp-polarity MAI shows a strong advantage for importance pruning at low capacity (2 heads: 0.818 vs. 0.652). At 4 heads, random edges out slightly (0.874 vs. 0.867). At higher counts (8 and 12 heads), importance regains lead (0.922 vs. 0.908 and 0.925 vs. 0.903), demonstrating that head importance selection yields more robust alignment gains when model capacity is constrained.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_00b9d440cc414af5af98e9b2d351e02a_proc_4081279/yelp_polarity_MAI_vs_heads.png"}], [{"analysis": "Alignment Curves for dbpedia_14 Dataset: pretrained initialization yields high alignment from the start (~0.97 train, ~0.99 val) and further improves to ~0.997 train and ~0.998 val by epoch 3. Random initialization starts significantly lower (~0.94 train, ~0.99 val), only catching up on the train curve by epoch 2 but showing a slight validation dip at epoch 3 (~0.985). This indicates that pretrained components drive stable, high-quality alignment, while random models learn more slowly and generalize less consistently.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/dbpedia_14_alignment_curve.png"}, {"analysis": "Loss Curves for ag_news Dataset: pretrained model loss plummets from ~2.1 to ~0.44 on train and ~0.49 to ~0.35 on validation by epoch 2, continuing to decrease to ~0.35. Random initialization stays around 1.9\u21921.4 train loss and 1.45\u21921.40 validation, with minimal improvement. Pretrained setup converges rapidly and achieves significantly lower loss than random, highlighting sample efficiency and better generalization.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/ag_news_loss_curve.png"}, {"analysis": "Alignment Curves for yelp_polarity Dataset: both pretrained and random initializations reach near-perfect alignment (>0.997) on train and validation by epoch 2. Pretrained starts slightly lower on train at epoch 1 (~0.957 vs. ~0.964) but aligns fully by epoch 2. Validation alignment is already very high (>0.997) for both. Indicates that this dataset is easily aligned and both setups can achieve saturation, though pretrained still demonstrates marginal stability.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/yelp_polarity_alignment_curve.png"}, {"analysis": "Validation MAI for yelp_polarity Dataset: pretrained MAI holds around ~0.90\u20130.91 across all epochs, whereas random init starts at ~0.655, dips to ~0.63 at epoch 2, and rises to ~0.715 by epoch 3. The pretrained system maintains calibrated inference, while random initialization struggles to produce reliable alignment signals and only partly recovers by the end.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/yelp_polarity_mai_curve.png"}, {"analysis": "Loss Curves for yelp_polarity Dataset: pretrained train loss drops sharply from ~1.8 to ~0.40 by epoch 2 and further to ~0.35, with validation loss falling from ~0.47 to ~0.37 then ~0.38. Random init starts at ~1.48, reduces to ~0.72\u21920.67 on train, and ~0.70\u21920.68 on validation, still well above pretrained. Confirms pretrained model\u2019s superior efficiency and generalization on this task.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/yelp_polarity_loss_curve.png"}, {"analysis": "Loss Curves for dbpedia_14 Dataset: pretrained train loss decreases from ~2.65 to ~0.47\u2192~0.20 over three epochs, and validation from ~0.79 to ~0.30\u2192~0.15. Random init shows minimal change (~3.40\u21922.60 train, ~2.65\u21922.55 val), indicating failure to learn effectively. Pretrained initialization is critical for meaningful training progress in this domain.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/dbpedia_14_loss_curve.png"}, {"analysis": "Validation MAI for dbpedia_14 Dataset: pretrained MAI climbs from ~0.95 to ~0.98\u2192~0.99, whereas random init remains flat near ~0.14\u2192~0.13 before a modest rise to ~0.30 in epoch 3. Pretrained model delivers well-calibrated mental alignment, random init yields poor calibration until very late and still far below.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/dbpedia_14_mai_curve.png"}, {"analysis": "Validation MAI for ag_news Dataset: pretrained MAI steadily improves from ~0.93\u2192~0.94 over epochs, while random init hovers around ~0.41 and even dips slightly by epoch 3. Reinforces that pretrained architectures maintain reliable calibration and trust metrics, random frameworks fail to align users effectively.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/ag_news_mai_curve.png"}, {"analysis": "Alignment Curves for ag_news Dataset: pretrained train alignment grows from ~0.968 to ~0.998 and validation from ~0.998 to ~0.999. Random init rises from ~0.945\u2192~0.997 train and ~0.998\u2192~0.999 val, but shows a slight val dip at epoch 2. Both reach high final alignment, yet pretrained is more stable and faster converging.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_33b1f7cfac554a38be76dd24c08c5ae6_proc_4081280/ag_news_alignment_curve.png"}], [{"analysis": "Yelp Polarity Classification\nTrain accuracy for models with 0% and 10% dropout jumps sharply between epoch 1 and 2, reaching ~0.85 and ~0.82 respectively by epoch 2, then modestly increasing by epoch 3. Validation accuracy for 0% and 10% dropout remains around 0.82\u20130.83 throughout, indicating fast convergence but a small generalization gap. Higher dropout rates (20% and 30%) show slower training (starting below 0.60 at epoch 1) and lower final validation (\u22480.81 and \u22480.79), though they close the gap by epoch 3. Overall, minimal or no token dropout yields the best validation performance and fastest convergence; heavier dropout hurts both training speed and final accuracy on this sentiment task.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_ec7d598449024b57a51ff39af928139d_proc_4081278/yelp_polarity_accuracy_curves.png"}, {"analysis": "Dbpedia 14 Classification\nAt epoch 1, models with no dropout already achieve ~0.68 train vs ~0.92 val, while 10% dropout underfits heavily (0.40 train / 0.75 val). By epoch 2, all variants converge to high performance: no dropout and 10% reach ~0.96 train & val, 20% about 0.92/0.94, 30% about 0.90/0.93. Epoch 3 yields marginal gains. This suggests that for this large taxonomy task, even moderate dropout slows early learning but by epoch 2 the network recovers. No or low dropout is optimal for peak accuracy; higher rates incur a slight persistent penalty on both train and val.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_ec7d598449024b57a51ff39af928139d_proc_4081278/dbpedia_14_accuracy_curves.png"}, {"analysis": "Ag News Classification\nAll models start with moderate underfitting at epoch 1 (train 0.62\u20130.67, val 0.86\u20130.89). 10% dropout leads to the highest initial validation (~0.89). By epoch 2 and 3, 0% and 10% dropout both converge to ~0.87\u20130.88 validation, with training above 0.86. Higher dropout (20%, 30%) starts lower and remains marginally behind through epoch 3. The validation curves are relatively flat after epoch 1, indicating early convergence. A small amount of token dropout can speed generalization early, but beyond 10% it offers no added benefit and slightly degrades final accuracy.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_ec7d598449024b57a51ff39af928139d_proc_4081278/ag_news_accuracy_curves.png"}, {"analysis": "MAI vs Token Dropout Rate \u2013 Final Epoch MAI\nAlignment Index (MAI) decreases as token dropout increases. For ag_news, MAI falls from ~0.943 (0%) to ~0.933 (20%) then slightly recovers to ~0.937 (30%). Yelp polarity peaks at 10% dropout (0.915) before declining to 0.883 at 30%. Dbpedia declines steadily from ~0.985 (0%) to ~0.967 (30%). These trends show that injecting token-level noise generally disrupts the learned alignment between user and model representations. A small dropout rate (\u224810%) can occasionally offer negligible MAI improvement in smaller tasks, but higher rates consistently reduce bidirectional alignment.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_ec7d598449024b57a51ff39af928139d_proc_4081278/mai_vs_dropout_rate.png"}], [], [{"analysis": "Loss trajectories reinforce these findings: on ag_news and yelp_polarity, Adam and AdamW drop loss from ~2.0 and ~1.9 to ~0.3\u20130.4 in two epochs, while SGD only falls from ~5.2 and ~4.0 to ~1.2 and ~0.7. On dbpedia_14, Adam/AdamW reduce loss from ~2.7\u20132.8 to ~0.2\u20130.4, whereas SGD remains above ~2.8. The marginally lower validation loss of AdamW vs. Adam at epoch 3 underscores the regularizing influence of weight decay.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_8b1e5415f46b4fd4a24eeaf1ff52ebec_proc_4081279/accuracy_curves.png"}], [{"analysis": "Dataset ag_news \u2014 Ablation independent: Alignment curves show that train alignment starts at about 0.968 in epoch 1, jumps to 0.998 by epoch 2, and plateaus near 0.999 at epoch 3. Validation alignment begins high (\u22480.997) and steadily climbs to 0.999 by epoch 3. The small gap between train and validation at epoch 1 closes rapidly, indicating fast convergence and minimal overfitting in this independent\u2010component setup.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_shared_fc1_alignment_curves.png"}, {"analysis": "Dataset yelp_polarity \u2014 Ablation independent: Train alignment rises from roughly 0.956 at epoch 1 to virtually 1.000 by epoch 2, and remains flat thereafter. Validation alignment moves from 0.998 at epoch 1 to 0.9999+ by epoch 3. This mirrors the ag_news behavior: an initial gap at epoch 1 that disappears by epoch 2, confirming that the independent configuration achieves almost perfect alignment within two epochs.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_shared_fc1_mai.png"}, {"analysis": "Dataset dbpedia_14 \u2014 Ablation independent: Training alignment climbs from about 0.970 at epoch 1 to 0.994 at epoch 2 and 0.997 at epoch 3. Validation follows a similar trajectory (0.990 \u2192 0.996 \u2192 0.998). Again, the independent variant exhibits rapid improvement and strong generalization, with train and validation alignment nearly indistinguishable after epoch 2.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_independent_alignment_curves.png"}, {"analysis": "Dataset ag_news \u2014 Ablation shared_fc1_fc2: Both train and validation alignment curves are flat at exactly 1.000 across all three epochs. Such saturation suggests that sharing the two final fully\u2010connected layers forces the alignment metric to max out immediately, preventing any observable dynamics in this measurement.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_independent_alignment_curves.png"}, {"analysis": "Dataset yelp_polarity \u2014 Ablation shared_fc1_fc2: As with ag_news, train and validation alignment remain at 1.000 from epoch 1 onward. The lack of variance indicates that the shared\u2010layer design collapses the alignment score to its upper bound, masking any meaningful progression.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_independent_mai.png"}, {"analysis": "Dataset dbpedia_14 \u2014 Ablation shared_fc1_fc2: Alignment is constant at 1.000 for both train and validation through epochs 1\u20133. This uniformity confirms that the shared_fc1_fc2 ablation universally saturates the alignment metric across datasets.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/dbpedia_14_shared_fc1_loss_curves.png"}, {"analysis": "Dataset ag_news \u2014 Ablation independent: Loss curves show train loss plummeting from 2.08 at epoch 1 to 0.45 at epoch 2 and 0.36 at epoch 3. Validation loss decreases more gradually (0.50 \u2192 0.35 \u2192 0.34). The smooth, parallel declines and small train/validation gap indicate stable learning without overfitting.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_shared_fc1_fc2_loss_curves.png"}, {"analysis": "Dataset yelp_polarity \u2014 Ablation independent: Train loss drops from 1.81 to 0.40 and then 0.34 over three epochs. Validation loss decreases from 0.47 to 0.38 to 0.37. The consistent downward trends again reflect effective training dynamics and robust generalization in the independent\u2010component variant.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_shared_fc1_fc2_alignment_curves.png"}, {"analysis": "Dataset dbpedia_14 \u2014 Ablation independent: Starting at 2.70, train loss falls to 0.46 and then 0.19. Validation loss descends from 0.80 to 0.30 to 0.17. These curves confirm that the independent ablation reliably drives loss reduction on both splits, with no signs of divergence.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/ag_news_independent_loss_curves.png"}, {"analysis": "Dataset ag_news \u2014 Ablation shared_fc1_fc2: Validation MAI (mean alignment index) increases steadily from 0.919 at epoch 1 to 0.936 at epoch 2 and 0.942 at epoch 3. Despite the alignment metric saturation seen earlier, MAI reveals ongoing improvement in the shared\u2010layer model\u2019s performance over time.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4e9f1eeb3d094f9096032491721f8311_proc_4081280/yelp_polarity_shared_fc1_mai.png"}], [], [{"analysis": "yelp_polarity metrics across temperatures\n- Loss curves drop sharply by epoch 2 for all temperatures, with T=2.0 yielding the lowest validation loss (\u22480.30 at epoch 3) compared to T=1.0 (\u22480.35) and T=0.5 (\u22480.37).\n- Accuracy curves show strong gains by epoch 2; T=2.0 achieves the highest validation accuracy at epoch 3 (\u22480.86), while T=1.0 and T=0.5 trail slightly (\u22480.85 and 0.82 respectively).\n- Alignment curves converge to near\u2010perfect train alignment by epoch 2 at all temperatures. Validation alignment is perfect (1.0) across epochs for T=1.0, slightly lower but stable for T=2.0 (\u22480.987), and very consistent for T=0.5 (\u22480.99).\n- Trade\u2010off: T=2.0 offers best loss and accuracy, T=1.0 delivers perfect alignment without significant performance hit, T=0.5 under\u2010performs in validation accuracy and loss.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_5adec4951f8740a090873130a88a7445_proc_4081279/yelp_polarity_metrics.png"}, {"analysis": "dbpedia_14 metrics across temperatures\n- Loss curves again plummet by epoch 2; T=0.5 yields the lowest final validation loss (\u22480.16), followed by T=1.0 (\u22480.17) and T=2.0 (\u22480.19).\n- Accuracy curves reach \u22480.95\u20130.98 on validation at epoch 2; T=1.0 peaks highest at epoch 3 (\u22480.98), T=0.5 close behind (\u22480.97), T=2.0 slightly lower (\u22480.97).\n- Alignment curves for T=0.5 and T=1.0 both attain nearly perfect train and validation alignment by epoch 2 (\u22480.99\u20131.0). T=2.0 lags marginally (\u22480.985).\n- Trade\u2010off: Lower temperatures (0.5 and 1.0) help alignment and loss; T=1.0 balances highest accuracy with perfect alignment; T=2.0 under\u2010performs alignment and slightly accuracy.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_5adec4951f8740a090873130a88a7445_proc_4081279/dbpedia_14_metrics.png"}, {"analysis": "ag_news metrics across temperatures\n- Loss curves show similar convergence across temperatures with epoch 2; T=1.0 achieves slightly lowest validation loss at epoch 3 (\u22480.36), T=0.5 and T=2.0 around 0.37.\n- Validation accuracy by epoch 3 is highest for T=2.0 (\u22480.90), with T=1.0 and T=0.5 just below (\u22480.89).\n- Alignment saturates quickly: T=1.0 yields the highest alignment on both train (\u22480.995) and val (\u22480.999), T=0.5 and T=2.0 slightly lower (\u22480.99).\n- Trade\u2010off: T=2.0 gives marginal accuracy gains at the cost of slight alignment drop; T=1.0 offers the best overall alignment with near\u2010peak accuracy and loss behavior.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_5adec4951f8740a090873130a88a7445_proc_4081279/ag_news_metrics.png"}], [{"analysis": "yelp_polarity: All temperature settings show a rapid drop in training and validation loss after epoch 1, reaching a plateau by epoch 3. T=2.0 yields the lowest final validation loss (~0.325) but starts with a slightly higher training loss. Validation accuracy peaks at epoch 2 for T=0.5 and T=1.0 before a small drop, while T=2.0 continues to improve into epoch 3, ending with the highest validation accuracy (~0.865). Alignment converges to near 1.0 across all temperatures; T=0.5 and T=1.0 reach perfect alignment by epoch 2, whereas T=2.0 lags slightly but still exceeds 0.99 by epoch 3.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_a01c5371854b472d9d698de9c013b3bb_proc_4081278/yelp_polarity_metrics.png"}, {"analysis": "dbpedia_14: Loss curves show a steep decline by epoch 2, with T=0.5 producing the lowest final validation loss (~0.18). T=1.0 closely matches T=0.5, while T=2.0 underperforms slightly in both training and validation loss. Accuracies mirror this: T=1.0 achieves the highest final validation accuracy (~0.98), T=0.5 is a close second, and T=2.0 trails. Alignment scores for T=0.5 and T=1.0 converge to perfect 1.0 on validation data, whereas T=2.0 plateaus around 0.987 by epoch 3, indicating slightly less consistency in mental-model matching.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_a01c5371854b472d9d698de9c013b3bb_proc_4081278/dbpedia_14_metrics.png"}, {"analysis": "ag_news: Training and validation losses decline quickly; T=2.0 attains the best final validation loss (~0.36). Validation accuracy grows steadily, with T=2.0 edging out others at epoch 3 (~0.91), T=1.0 and T=0.5 finish around 0.89. Alignment for T=0.5 and T=1.0 reaches 1.0 by epoch 3 on validation, while T=2.0 remains slightly lower (~0.992), suggesting minor trade-offs between diversity (higher temperature) and perfect alignment.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_a01c5371854b472d9d698de9c013b3bb_proc_4081278/ag_news_metrics.png"}, {"analysis": "Final MAI vs Temperature: ag_news (blue) slightly improves from 0.942 at T=0.5 and T=1.0 to 0.945 at T=2.0. yelp_polarity (orange) peaks at 0.919 at T=1.0, dipping to 0.914 at T=2.0. dbpedia_14 (green) holds at 0.987 for T=0.5 and T=1.0 before dropping to 0.981 at T=2.0. Overall, T=1.0 delivers the best aggregate alignment\u2013accuracy trade-off across datasets, balancing low loss, high accuracy, and near-perfect alignment.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_a01c5371854b472d9d698de9c013b3bb_proc_4081278/mai_vs_temperature.png"}], [{"analysis": "yelp_polarity metrics across temperatures show that all three temperature values (0.5, 1.0, 2.0) achieve rapid convergence in both training and validation loss by epoch 2. At epoch 1, higher temperatures produce slightly lower initial loss (T = 2.0 best, T = 1.0 worst), but by epoch 3 losses are within a narrow band (~0.32\u20130.38). Accuracy curves mirror this pattern: T = 1.0 gives the strongest jump from epoch 1 to epoch 2 on validation accuracy (~0.83\u21920.85), and all settings plateau near ~0.86 by epoch 3, though T = 2.0 is marginally ahead on validation at epoch 2. Alignment scores begin lower for T = 0.5 (training ~0.89, val ~0.995 at epoch 1) but all temperatures saturate to near-perfect alignment (~0.999\u20131.0) by epoch 2, with T = 1.0/2.0 slightly edging out T = 0.5 early on. Overall, temperature has modest impact after epoch 2, but T = 1.0 balances initial convergence speed in loss and accuracy with rapid alignment.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c54e9756eab849f398468a9dee659963_proc_4081280/yelp_polarity_metrics.png"}, {"analysis": "dbpedia_14 metrics across temperatures indicate slightly higher initial training loss at T = 2.0 (epoch 1) compared with T = 0.5/1.0, but all temperatures collapse losses by epoch 2 (< 0.50 train, ~0.30 val) and reach very low values (~0.18 train, ~0.17 val) by epoch 3 with negligible differences. Validation accuracy at epoch 1 is highest for T = 1.0 (~0.92) despite its lower training accuracy (~0.52), suggesting better early generalization. By epoch 2, all settings achieve ~0.97\u20130.98 on train and ~0.97\u20130.98 on validation, and they remain there at epoch 3. Alignment follows a similar rapid rise: epoch 1 scores range from ~0.925 (T = 0.5 train) to ~0.977 (T = 1.0 val), then exceed ~0.995 across both splits by epoch 2. Temperature choice matters little beyond ensuring this fast alignment\u2014T = 1.0 slightly edges others in early val performance.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c54e9756eab849f398468a9dee659963_proc_4081280/dbpedia_14_metrics.png"}, {"analysis": "ag_news metrics across temperatures reveal that higher temperature again lowers epoch 1 training loss (T = 2.0: ~1.89 vs T = 0.5: ~2.08) and validation loss (~0.46 vs ~0.49). By epoch 2 all losses converge (~0.42 train, ~0.38 val) and further drop to ~0.36/~0.39 at epoch 3. Validation accuracy starts around ~0.87 for all temperatures, jumps to ~0.89\u20130.90 at epoch 2, and reaches ~0.90\u20130.91 by epoch 3, with only minor differences: T = 2.0 has a slight advantage at epoch 3. Alignment curves show initial spread (epoch 1 train: 0.929\u20130.976; val: 0.988\u20130.997) but saturate near 1.0 by epoch 2 for all. T = 1.0 gives the strongest early rise, but differences vanish by epoch 3. Overall, ag_news follows the same trend: moderate temperature accelerates initial learning, but all settings converge quickly.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c54e9756eab849f398468a9dee659963_proc_4081280/ag_news_metrics.png"}, {"analysis": "Final Model Alignment-Accuracy Index (MAI) vs Temperature shows that the sweet spot across datasets is around T = 1.0. yelp_polarity\u2019s MAI climbs from ~0.904 (T = 0.5) to ~0.919 (T = 1.0) then dips slightly to ~0.914 (T = 2.0). dbpedia_14 peaks at ~0.9876 (T = 1.0) from ~0.9870 at T = 0.5 and drops to ~0.9811 at T = 2.0. ag_news shows only a small MAI increase from ~0.943 (T = 0.5) to ~0.945 (T = 2.0), with T = 1.0 in between (~0.943). Together, these results suggest that a moderate softmax temperature (around 1.0) provides the best trade-off between accuracy and alignment; very low temperatures slow initial convergence and very high temperatures yield diminishing or slightly negative returns on model alignment-accuracy index.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c54e9756eab849f398468a9dee659963_proc_4081280/mai_vs_temperature.png"}], [{"analysis": "Losses for yelp_polarity drop sharply in the first two epochs across all temperatures, with T=1.0 achieving the lowest final training and validation loss. Validation accuracy peaks at Epoch 3 for T=0.5 and T=2.0 around 0.87\u20130.86, but T=1.0 attains the highest validation accuracy at Epoch 2 (\u22480.855) and retains strong performance at Epoch 3 (\u22480.85). Alignment scores climb rapidly; T=1.0 hits perfect alignment by Epoch 3 on both train and val, while T=0.5 and T=2.0 saturate slightly later. Overall, T=1.0 provides the best trade-off between fast convergence, high accuracy, and perfect alignment on this dataset.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_85f8d185ab7644bb8c36bce01d477867_proc_4081279/yelp_polarity_metrics.png"}, {"analysis": "In dbpedia_14, losses again plummet by Epoch 2. T=0.5 yields the lowest final validation loss (~0.18) and fastest loss reduction, with T=1.0 close behind and T=2.0 slightly higher. All temperatures achieve high validation accuracy (~0.98) by Epoch 3, though T=0.5 reaches it fastest (Epoch 2). Alignment trajectories show T=0.5 and T=1.0 converging to perfect alignment by Epoch 3; T=2.0 lags slightly but still exceeds 0.987. The T=0.5 model edges out others in early convergence and final loss while matching accuracy and alignment.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_85f8d185ab7644bb8c36bce01d477867_proc_4081279/dbpedia_14_metrics.png"}, {"analysis": "For ag_news, loss curves and accuracy improvements mirror the other tasks, with rapid learning by Epoch 2. T=2.0 attains the highest validation accuracy by Epoch 3 (~0.90), compared to ~0.89 for the lower temperatures. Alignment scores for T=2.0 are slightly lower initially but converge to ~0.99\u20131.0 by Epoch 3, closely matching T=0.5 and T=1.0. The T=2.0 setting trades a marginally slower alignment ramp-up for the best classification accuracy, indicating a different optimum for this dataset.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_85f8d185ab7644bb8c36bce01d477867_proc_4081279/ag_news_metrics.png"}, {"analysis": "The final MAI vs. temperature plot consolidates these observations: yelp_polarity peaks at T=1.0, dbpedia_14 at T=0.5 (with T=1.0 equally strong), and ag_news at T=2.0. No single temperature is universally optimal; each task benefits from dataset-specific tuning.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_85f8d185ab7644bb8c36bce01d477867_proc_4081279/mai_vs_temperature.png"}], []], "vlm_feedback_summary": ["[]", "[]", "Ablation between full fine-tuning and a linear probe shows that both reach\nsimilar final validation accuracy within a 1% margin across three text\nclassification datasets. Full fine-tuning yields faster and stronger loss\nreduction and higher training accuracy, whereas the linear probe occasionally\noutperforms in early validation accuracy on two datasets. All curves saturate by\nepoch 2, suggesting rapid convergence for both approaches.", "[]", "Alignment curves across all datasets reach near-perfect scores by epoch 2 for\nboth baseline and fine-tune, suggesting the chosen alignment metric saturates\nquickly. Fine-tuning consistently improves early training alignment and MAI\nstability, achieving higher initial values and faster convergence. MAI curves\nreveal optimal mutual alignment improvement around epoch 2, after which further\ntraining yields diminishing or slightly negative returns. These results\nrecommend employing early-stop or adaptive epoch scheduling to maximize co-\nadaptive alignment efficiency.", "[]", "Across tasks, dropout has mixed effects: light dropout (0.1) closely matches no-\ndropout on loss and alignment, offering slight regularization benefits. Moderate\ndropout (0.3) can improve generalization (validation loss, MAI) on sentiment\ntasks, but consistently undermines alignment metrics, especially on dbpedia_14\nand ag_news. Heavy dropout (0.5) slows convergence, raises losses, and degrades\nalignment and MAI except on yelp_polarity where it boosts late-stage MAI. For\nCAMMA\u2019s bidirectional alignment, minimal or no dropout best preserves alignment\nquality, while moderate dropout may be selectively beneficial for generalization\nin text classification benchmarks.", "Across all three datasets, activation functions converge rapidly, with GELU\noften achieving the lowest loss and strongest MAI trajectory. Tanh drives early\nMAI gains on smaller datasets like yelp_polarity, but GELU and LeakyReLU match\nor surpass it by saturation. On large datasets (dbpedia_14), GELU consistently\nleads in both loss minimization and MAI. ag_news shows minimal differences after\nsufficient training, suggesting activation choice is less critical once\nconvergence is reached.", "Across six complementary plots, training curves are virtually invariant to \u03bb\nchoice, indicating rapid convergence independent of this hyperparameter.\nValidation metrics show only minor sensitivity: \u03bb=1.0 consistently edges out\nother settings on yelp_polarity and ag_news, but has no effect on dbpedia_14.\nGiven the minimal impact, further ablations should explore additional components\nor more challenging tasks to tease apart the contributions of the co-adaptive\nframework.", "Across all three datasets, deeper representations (layer_5) and the ensemble of\nthe last two layers (avg_last2) consistently achieve the lowest training and\nvalidation losses alongside the highest bidirectional alignment scores. The bulk\nof improvements occur by epoch 2, after which gains diminish. Shallow layers\n(layer_1) improve more slowly and plateau at lower alignment. Generalization\ngaps remain small, indicating stable validation performance. These findings\nhighlight the importance of leveraging deeper features and simple ensembling for\nfast, high-quality co-adaptive alignment in CAMMA.", "All three metrics (loss, alignment, MAI) exhibit sharp gains between epoch 1 and\n2 and plateau by epoch 3. Higher \u03b3 speeds up loss reduction and yields lower\nfinal loss, though alignment and MAI saturate rapidly with minimal \u03b3\u2010dependent\ndifferences beyond epoch 2. Validation curves match training curves closely,\nindicating strong generalization and little overfitting.", "Across datasets, importance-based head ablation consistently yields modestly\nbetter alignment (MAI) and slightly lower validation losses compared to random\npruning, especially at low head counts. Differences shrink as head count\nincreases, and both methods converge at high capacity. Loss curves on ag_news\ndemonstrate that importance pruning can achieve marginally faster convergence\nand reduced final validation loss, suggesting more efficient utilization of\nattention heads.", "Across tasks, pretrained initialization consistently enables rapid convergence,\nlower loss, stable high alignment, and superior MAI compared to random\ninitialization. Randomly initialized models take longer to align, yield higher\nlosses, and produce poorly calibrated MAI, especially on complex datasets like\ndbpedia_14 and ag_news. On easier datasets such as yelp_polarity, both\neventually reach high alignment but pretrained still offers better calibration\nand efficiency.", "Low or zero token dropout consistently yields the fastest convergence, highest\nclassification accuracy, and strongest mental alignment (MAI) across all three\ndatasets. Small dropout rates (around 10%) may offer slight early regularization\nbut do not improve final metrics and can harm bidirectional mental-model\nalignment when applied beyond minimal levels. The ablation underscores that\ntoken dropout as a regularizer must be used sparingly in a co-adaptive alignment\nframework to avoid degrading both performance and mutual understanding.", "[]", "Adaptive optimizers (Adam, AdamW) consistently enable faster convergence, higher\naccuracy, stronger mental\u2010model alignment, and lower loss than SGD across all\ndatasets. Incorporating weight decay (AdamW) yields slight but consistent\nimprovements in validation metrics and generalization, underscoring its value in\nco\u2010adaptive human\u2013AI alignment systems.", "Independent\u2010component ablations produce realistic, rapidly converging alignment\nand loss curves with minimal train/validation gaps, demonstrating both effective\nlearning and generalization. By contrast, shared_fc1_fc2 ablation causes the\nalignment metric to saturate immediately at its maximum, obscuring the model\u2019s\ntrue alignment dynamics. However, the MAI score still rises over epochs under\nthe shared configuration, suggesting that the alignment metric lacks sufficient\ndynamic range in that setting. Revisiting the alignment scoring function or its\nscaling when layers are shared is recommended to capture more nuanced\nprogression.", "[]", "Across all three datasets, temperature=1.0 consistently provides the best\noverall trade\u2010off between task accuracy and model\u2013user alignment: it achieves\nperfect or near\u2010perfect alignment, top\u2010tier validation accuracy, and competitive\nloss. Lower temperature (0.5) under\u2010performs in accuracy for some tasks, while\nhigher temperature (2.0) sometimes yields minor accuracy gains but at the\nexpense of alignment. The final MAI index confirms that 1.0 is the sweet spot\nacross yelp_polarity and dbpedia_14, and remains highly competitive for ag_news.", "Across all three tasks, a temperature of 1.0 consistently provides the best\nbalance between loss minimization, accuracy gains, and alignment convergence.\nLower temperature (0.5) occasionally underfits recent trends, while higher\ntemperature (2.0) increases perplexity and slows alignment. Hence, T=1.0 emerges\nas the optimal setting in the ablation study.", "", "All three datasets show rapid convergence of loss, accuracy, and alignment\nwithin three epochs, demonstrating the effectiveness of the co-adaptive\nalignment components under all temperatures. Temperature influences the\nalignment-accuracy trade-off: moderate softmax (T=1.0) is best for\nyelp_polarity, lower softness (T=0.5) suits dbpedia_14, and higher softness\n(T=2.0) benefits ag_news. These ablations confirm that CAMMA\u2019s bidirectional\nupdates yield robust alignment and performance, but fine-tuning temperature per\ntask yields additional gains.", "[]"], "exec_time": [3.7596991062164307, 122.11243510246277, 81.58281016349792, 93.31499361991882, 121.99025058746338, 108.02708530426025, 167.8486933708191, 162.29687666893005, 153.04155564308167, 103.40292286872864, 119.24169516563416, 188.0740044116974, 85.21640753746033, 149.76728677749634, 11.389106750488281, 117.59280967712402, 114.41107654571533, 89.37219715118408, 115.16400384902954, 117.67117381095886, 119.86024856567383, 120.15317583084106, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [[], [], ["[yelp_polarity", "ag_news", "dbpedia_14]"], [], ["[yelp_polarity", "ag_news", "dbpedia_14]"], [], ["['dbpedia_14'", "'ag_news'", "'yelp_polarity']"], ["yelp_polarity", "dbpedia_14", "ag_news"], ["['yelp_polarity'", "'ag_news']"], ["[dbpedia_14", "ag_news", "yelp_polarity]"], ["[\u201call_datasets\u201d]"], ["[ag_news", "yelp_polarity", "dbpedia_14]"], ["[dbpedia_14", "ag_news", "yelp_polarity]"], ["[\"yelp_polarity\"", "\"dbpedia_14\"", "\"ag_news\"]"], [], ["ag_news", "yelp_polarity", "dbpedia_14"], ["[ag_news", "yelp_polarity", "dbpedia_14]"], [], ["[\"yelp_polarity\"", "\"dbpedia_14\"", "\"ag_news\"]"], ["[yelp_polarity", "dbpedia_14", "ag_news]"], ["[yelp_polarity", "dbpedia_14", "ag_news]"], ["yelp_polarity", "dbpedia_14", "ag_news"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = {}\n\nsd = data.get(\"learning_rate\", {}).get(\"synthetic\", {})\n\n# Plot 1: Loss curves\ntry:\n    lrs = sd.get(\"lrs\", [])\n    train_losses = sd.get(\"losses\", {}).get(\"train\", [])\n    val_losses = sd.get(\"losses\", {}).get(\"val\", [])\n    epochs = range(1, len(train_losses[0]) + 1)\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for lr, tr in zip(lrs, train_losses):\n        axes[0].plot(epochs, tr, label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[0].legend()\n    for lr, vl in zip(lrs, val_losses):\n        axes[1].plot(epochs, vl, label=f\"lr={lr}\")\n    axes[1].set_title(\"Validation Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    axes[1].legend()\n    fig.suptitle(\n        \"Synthetic Dataset - Loss Curves\\nLeft: Training Loss, Right: Validation Loss\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\n# Plot 2: Alignment curves\ntry:\n    train_align = sd.get(\"metrics\", {}).get(\"train\", [])\n    val_align = sd.get(\"metrics\", {}).get(\"val\", [])\n    epochs = range(1, len(train_align[0]) + 1)\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for lr, ta in zip(lrs, train_align):\n        axes[0].plot(epochs, ta, label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Alignment\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Alignment (1-JSD)\")\n    axes[0].legend()\n    for lr, va in zip(lrs, val_align):\n        axes[1].plot(epochs, va, label=f\"lr={lr}\")\n    axes[1].set_title(\"Validation Alignment\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Alignment (1-JSD)\")\n    axes[1].legend()\n    fig.suptitle(\n        \"Synthetic Dataset - Alignment Curves\\nLeft: Training Alignment, Right: Validation Alignment\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_alignment_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot combined accuracy and loss curves for each dataset\nfor ds in experiment_data.get(\"baseline\", {}):\n    try:\n        epochs = np.arange(\n            1, len(experiment_data[\"baseline\"][ds][\"metrics\"][\"train\"]) + 1\n        )\n        plt.figure(figsize=(10, 5))\n        # Accuracy subplot\n        plt.subplot(1, 2, 1)\n        for ab in experiment_data:\n            tr = experiment_data[ab][ds][\"metrics\"][\"train\"]\n            val = experiment_data[ab][ds][\"metrics\"][\"val\"]\n            plt.plot(epochs, tr, marker=\"o\", label=f\"{ab} Train\")\n            plt.plot(epochs, val, marker=\"x\", linestyle=\"--\", label=f\"{ab} Val\")\n        plt.title(f\"{ds} Accuracy Curves\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        # Loss subplot\n        plt.subplot(1, 2, 2)\n        for ab in experiment_data:\n            tr = experiment_data[ab][ds][\"losses\"][\"train\"]\n            val = experiment_data[ab][ds][\"losses\"][\"val\"]\n            plt.plot(epochs, tr, marker=\"o\", label=f\"{ab} Train\")\n            plt.plot(epochs, val, marker=\"x\", linestyle=\"--\", label=f\"{ab} Val\")\n        plt.title(f\"{ds} Loss Curves\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.suptitle(f\"Dataset: {ds} - Baseline vs Linear Probe\")\n        fname = os.path.join(working_dir, f\"{ds}_acc_loss_curves.png\")\n        plt.savefig(fname)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot_{ds}_curves: {e}\")\n        plt.close()\n\n# Plot bar chart of final validation accuracy by ablation and dataset\ntry:\n    ds_list = list(experiment_data.get(\"baseline\", {}))\n    x = np.arange(len(ds_list))\n    width = 0.35\n    base_vals = [experiment_data[\"baseline\"][d][\"metrics\"][\"val\"][-1] for d in ds_list]\n    lin_vals = [\n        experiment_data[\"linear_probe\"][d][\"metrics\"][\"val\"][-1] for d in ds_list\n    ]\n    plt.figure()\n    plt.bar(x - width / 2, base_vals, width, label=\"Baseline\")\n    plt.bar(x + width / 2, lin_vals, width, label=\"Linear Probe\")\n    plt.xticks(x, ds_list)\n    plt.ylabel(\"Final Validation Accuracy\")\n    plt.title(\"Final Validation Accuracy by Dataset and Ablation\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"final_val_accuracy_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final_val_accuracy_bar plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    # Print final evaluation metrics\n    print(\"Final evaluation metrics:\")\n    for pool, ds_dict in experiment_data.items():\n        for ds_name, data in ds_dict.items():\n            try:\n                acc = data[\"metrics\"][\"val\"][-1]\n                mai = data[\"mai\"][-1]\n                print(f\"{pool} | {ds_name} | val_acc: {acc:.4f}, MAI: {mai:.4f}\")\n            except:\n                pass\n\n    # Generate plots\n    for pool, ds_dict in experiment_data.items():\n        for ds_name, data in ds_dict.items():\n            epochs = np.arange(1, len(data[\"losses\"][\"train\"]) + 1)\n            # Loss & Accuracy curves\n            try:\n                plt.figure()\n                plt.subplot(1, 2, 1)\n                plt.plot(epochs, data[\"losses\"][\"train\"], label=\"Train Loss\")\n                plt.plot(epochs, data[\"losses\"][\"val\"], label=\"Val Loss\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Loss\")\n                plt.title(f\"{pool} | {ds_name} Loss\")\n                plt.legend()\n                plt.subplot(1, 2, 2)\n                plt.plot(epochs, data[\"metrics\"][\"train\"], label=\"Train Acc\")\n                plt.plot(epochs, data[\"metrics\"][\"val\"], label=\"Val Acc\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Accuracy\")\n                plt.title(f\"{pool} | {ds_name} Accuracy\")\n                plt.legend()\n                plt.suptitle(\"Loss (Left) vs Accuracy (Right)\")\n                plt.tight_layout()\n                plt.savefig(os.path.join(working_dir, f\"{pool}_{ds_name}_loss_acc.png\"))\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating loss/acc plot for {pool}-{ds_name}: {e}\")\n                plt.close()\n\n            # Alignment & MAI curves\n            try:\n                plt.figure()\n                plt.plot(epochs, data[\"alignments\"][\"train\"], label=\"Train Align\")\n                plt.plot(epochs, data[\"alignments\"][\"val\"], label=\"Val Align\")\n                plt.plot(epochs, data[\"mai\"], label=\"MAI\", linestyle=\"--\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Value\")\n                plt.title(f\"{pool} | {ds_name} Alignment & MAI\")\n                plt.legend()\n                plt.suptitle(\"Alignment & MAI over Epochs\")\n                plt.tight_layout()\n                plt.savefig(\n                    os.path.join(working_dir, f\"{pool}_{ds_name}_align_mai.png\")\n                )\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating alignment/MAI plot for {pool}-{ds_name}: {e}\")\n                plt.close()\n\n            # Predictions vs Ground Truth bar chart\n            try:\n                gt = data[\"ground_truth\"]\n                preds = data[\"predictions\"]\n                n_classes = max(gt.max(), preds.max()) + 1\n                gt_counts = np.bincount(gt, minlength=n_classes)\n                pred_counts = np.bincount(preds, minlength=n_classes)\n                x = np.arange(n_classes)\n                width = 0.35\n                fig, ax = plt.subplots()\n                ax.bar(x - width / 2, gt_counts, width, label=\"Ground Truth\")\n                ax.bar(x + width / 2, pred_counts, width, label=\"Predictions\")\n                ax.set_xlabel(\"Class\")\n                ax.set_ylabel(\"Count\")\n                ax.set_xticks(x)\n                ax.set_title(f\"{pool} | {ds_name} Preds vs GT\")\n                fig.suptitle(\"Left: Ground Truth, Right: Predictions\")\n                ax.legend()\n                plt.tight_layout()\n                plt.savefig(\n                    os.path.join(working_dir, f\"{pool}_{ds_name}_preds_vs_gt.png\")\n                )\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating preds vs gt plot for {pool}-{ds_name}: {e}\")\n                plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nfor ablation, datasets in experiment_data.items():\n    for name, data in datasets.items():\n        epochs = list(range(1, len(data[\"losses\"][\"train\"]) + 1))\n        # Loss curve\n        try:\n            plt.figure()\n            plt.plot(epochs, data[\"losses\"][\"train\"], label=\"Train Loss\")\n            plt.plot(epochs, data[\"losses\"][\"val\"], label=\"Val Loss\")\n            plt.title(f\"{ablation} - {name} Loss Curve\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{name}_{ablation}_loss_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {ablation} {name}: {e}\")\n            plt.close()\n        # Accuracy curve\n        try:\n            plt.figure()\n            plt.plot(epochs, data[\"metrics\"][\"train\"], label=\"Train Acc\")\n            plt.plot(epochs, data[\"metrics\"][\"val\"], label=\"Val Acc\")\n            plt.title(f\"{ablation} - {name} Accuracy Curve\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{name}_{ablation}_accuracy_curve.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating accuracy plot for {ablation} {name}: {e}\")\n            plt.close()\n        # Alignment curve\n        try:\n            plt.figure()\n            plt.plot(epochs, data[\"alignments\"][\"train\"], label=\"Train Alignment\")\n            plt.plot(epochs, data[\"alignments\"][\"val\"], label=\"Val Alignment\")\n            plt.title(f\"{ablation} - {name} Alignment Curve\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Alignment\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{name}_{ablation}_alignment_curve.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating alignment plot for {ablation} {name}: {e}\")\n            plt.close()\n        # MAI curve\n        try:\n            plt.figure()\n            plt.plot(epochs, data[\"mai\"], label=\"MAI\")\n            plt.title(f\"{ablation} - {name} MAI Curve\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"MAI\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{name}_{ablation}_mai_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating MAI plot for {ablation} {name}: {e}\")\n            plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\ndrops = list(experiment_data.get(\"mlp_dropout_rate_ablation\", {}).keys())\nif drops:\n    ds_names = list(experiment_data[\"mlp_dropout_rate_ablation\"][drops[0]].keys())\n    for ds in ds_names:\n        # Plot loss curves\n        try:\n            fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n            for dk in drops:\n                p = dk.split(\"_\", 1)[1]\n                train = experiment_data[\"mlp_dropout_rate_ablation\"][dk][ds][\"losses\"][\n                    \"train\"\n                ]\n                val = experiment_data[\"mlp_dropout_rate_ablation\"][dk][ds][\"losses\"][\n                    \"val\"\n                ]\n                axs[0].plot(range(1, len(train) + 1), train, label=f\"drop {p}\")\n                axs[1].plot(range(1, len(val) + 1), val, label=f\"drop {p}\")\n            fig.suptitle(\n                f\"Loss Curves on {ds}\\nLeft: Training Loss, Right: Validation Loss\"\n            )\n            axs[0].set_title(\"Training Loss\")\n            axs[1].set_title(\"Validation Loss\")\n            axs[0].set_xlabel(\"Epoch\")\n            axs[1].set_xlabel(\"Epoch\")\n            axs[0].set_ylabel(\"Loss\")\n            axs[1].set_ylabel(\"Loss\")\n            axs[0].legend()\n            axs[1].legend()\n            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n            plt.savefig(os.path.join(working_dir, f\"{ds}_loss_curves.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {ds}: {e}\")\n            plt.close()\n        # Plot alignment curves\n        try:\n            fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n            for dk in drops:\n                p = dk.split(\"_\", 1)[1]\n                tr = experiment_data[\"mlp_dropout_rate_ablation\"][dk][ds][\"alignments\"][\n                    \"train\"\n                ]\n                va = experiment_data[\"mlp_dropout_rate_ablation\"][dk][ds][\"alignments\"][\n                    \"val\"\n                ]\n                axs[0].plot(range(1, len(tr) + 1), tr, label=f\"drop {p}\")\n                axs[1].plot(range(1, len(va) + 1), va, label=f\"drop {p}\")\n            fig.suptitle(\n                f\"Alignment Curves on {ds}\\nLeft: Training Alignment, Right: Validation Alignment\"\n            )\n            axs[0].set_title(\"Training Alignment\")\n            axs[1].set_title(\"Validation Alignment\")\n            axs[0].set_xlabel(\"Epoch\")\n            axs[1].set_xlabel(\"Epoch\")\n            axs[0].set_ylabel(\"Alignment\")\n            axs[1].set_ylabel(\"Alignment\")\n            axs[0].legend()\n            axs[1].legend()\n            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n            plt.savefig(os.path.join(working_dir, f\"{ds}_alignment_curves.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating alignment plot for {ds}: {e}\")\n            plt.close()\n        # Plot MAI curves\n        try:\n            plt.figure(figsize=(6, 4))\n            for dk in drops:\n                p = dk.split(\"_\", 1)[1]\n                mai = experiment_data[\"mlp_dropout_rate_ablation\"][dk][ds][\"mai\"]\n                plt.plot(range(1, len(mai) + 1), mai, label=f\"drop {p}\")\n            plt.title(f\"MAI Curves on {ds}\\nMAI across Epochs\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"MAI\")\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f\"{ds}_mai_curves.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating MAI plot for {ds}: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nif \"experiment_data\" in locals():\n    datasets = list(next(iter(experiment_data.values())).keys())\n    activations = list(experiment_data.keys())\nelse:\n    datasets, activations = [], []\n\nfor dataset in datasets:\n    try:\n        plt.figure(figsize=(10, 5))\n        # Left: Loss Curves\n        ax1 = plt.subplot(1, 2, 1)\n        for act in activations:\n            losses = experiment_data[act][dataset][\"losses\"]\n            epochs = range(1, len(losses[\"train\"]) + 1)\n            ax1.plot(epochs, losses[\"train\"], label=f\"{act} Train\")\n            ax1.plot(epochs, losses[\"val\"], linestyle=\"--\", label=f\"{act} Val\")\n        ax1.set_title(\"Training vs Validation Loss\")\n        ax1.set_xlabel(\"Epoch\")\n        ax1.set_ylabel(\"Loss\")\n        ax1.legend()\n\n        # Right: MAI Curves\n        ax2 = plt.subplot(1, 2, 2)\n        for act in activations:\n            mai = experiment_data[act][dataset][\"mai\"]\n            ax2.plot(range(1, len(mai) + 1), mai, label=act)\n        ax2.set_title(\"MAI over Epochs\")\n        ax2.set_xlabel(\"Epoch\")\n        ax2.set_ylabel(\"MAI\")\n        ax2.legend()\n\n        plt.suptitle(\n            f\"{dataset} Metrics (Left: Loss Curves, Right: MAI Curves)\", fontsize=14\n        )\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        save_path = os.path.join(working_dir, f\"{dataset}_metrics.png\")\n        plt.savefig(save_path)\n        plt.close()\n\n        # Print final MAI values\n        final_mai = {\n            act: experiment_data[act][dataset][\"mai\"][-1] for act in activations\n        }\n        print(f\"{dataset} Final MAI: {final_mai}\")\n    except Exception as e:\n        print(f\"Error creating plot for {dataset}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\nif exp:\n    datasets = list(next(iter(exp.values())).keys())\n    lam_keys = sorted(\n        exp.keys(), key=lambda x: float(x[len(\"lambda_\") :].replace(\"_\", \".\"))\n    )\n\n    for name in datasets:\n        try:\n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n            for lam in lam_keys:\n                data = exp[lam][name]\n                epochs = range(1, len(data[\"losses\"][\"train\"]) + 1)\n                ax1.plot(epochs, data[\"losses\"][\"train\"], label=lam)\n                ax2.plot(epochs, data[\"losses\"][\"val\"], label=lam)\n            fig.suptitle(f\"Loss Curves for {name}\")\n            ax1.set_title(\"Left: Training Loss\")\n            ax2.set_title(\"Right: Validation Loss\")\n            ax1.set_xlabel(\"Epoch\")\n            ax2.set_xlabel(\"Epoch\")\n            ax1.set_ylabel(\"Loss\")\n            ax2.set_ylabel(\"Loss\")\n            ax1.legend()\n            ax2.legend()\n            plt.savefig(os.path.join(working_dir, f\"{name}_loss_curves.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {name}: {e}\")\n            plt.close()\n\n    for name in datasets:\n        try:\n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n            for lam in lam_keys:\n                data = exp[lam][name]\n                epochs = range(1, len(data[\"metrics\"][\"train\"]) + 1)\n                ax1.plot(epochs, data[\"metrics\"][\"train\"], label=lam)\n                ax2.plot(epochs, data[\"metrics\"][\"val\"], label=lam)\n            fig.suptitle(f\"Accuracy Curves for {name}\")\n            ax1.set_title(\"Left: Training Accuracy\")\n            ax2.set_title(\"Right: Validation Accuracy\")\n            ax1.set_xlabel(\"Epoch\")\n            ax2.set_xlabel(\"Epoch\")\n            ax1.set_ylabel(\"Accuracy\")\n            ax2.set_ylabel(\"Accuracy\")\n            ax1.legend()\n            ax2.legend()\n            plt.savefig(os.path.join(working_dir, f\"{name}_accuracy_curves.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating accuracy plot for {name}: {e}\")\n            plt.close()\n\n    for name in datasets:\n        print(f\"Dataset {name}:\")\n        for lam in lam_keys:\n            d = exp[lam][name]\n            print(\n                f\" {lam}: final val_acc={d['metrics']['val'][-1]:.4f}, final MAI={d['mai'][-1]:.4f}\"\n            )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Determine datasets and ablations\nablations = list(experiment_data.keys())\ndatasets = list(next(iter(experiment_data.values()), {}).keys())\n\n# Plot for each dataset\nfor dname in datasets:\n    try:\n        fig = plt.figure(figsize=(10, 4))\n        ax1 = fig.add_subplot(1, 2, 1)\n        ax2 = fig.add_subplot(1, 2, 2)\n        # Epochs\n        n_epochs = len(experiment_data[ablations[0]][dname][\"losses\"][\"train\"])\n        epochs = np.arange(1, n_epochs + 1)\n        # Plot each ablation\n        for abl in ablations:\n            losses = experiment_data[abl][dname][\"losses\"]\n            metrics = experiment_data[abl][dname][\"metrics\"]\n            ax1.plot(epochs, losses[\"train\"], label=f\"{abl} train\")\n            ax1.plot(epochs, losses[\"val\"], \"--\", label=f\"{abl} val\")\n            ax2.plot(epochs, metrics[\"train\"], label=f\"{abl} train\")\n            ax2.plot(epochs, metrics[\"val\"], \"--\", label=f\"{abl} val\")\n        # Titles and labels\n        fig.suptitle(\n            f\"{dname} dataset\\nLeft: Loss curves, Right: Bidirectional Alignment curves\"\n        )\n        ax1.set_title(\"Loss\")\n        ax1.set_xlabel(\"Epoch\")\n        ax1.set_ylabel(\"Loss\")\n        ax2.set_title(\"Bidirectional Alignment\")\n        ax2.set_xlabel(\"Epoch\")\n        ax2.set_ylabel(\"Alignment\")\n        ax1.legend(fontsize=\"small\")\n        ax2.legend(fontsize=\"small\")\n        # Save and close\n        plt.tight_layout(rect=[0, 0, 1, 0.90])\n        plt.savefig(os.path.join(working_dir, f\"{dname}_loss_alignment.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {dname}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Prepare averaged metrics across datasets\nab_keys = sorted(\n    experiment_data.keys(), key=lambda k: int(k.split(\"_\")[-1])\n)  # sort by gamma\nloss_avg = {}\nalign_avg = {}\nmai_avg = {}\n\nfor ab in ab_keys:\n    ds_data = experiment_data[ab]\n    # stack per-dataset lists\n    train_losses = np.array([ds_data[ds][\"losses\"][\"train\"] for ds in ds_data])\n    val_losses = np.array([ds_data[ds][\"losses\"][\"val\"] for ds in ds_data])\n    train_align = np.array([ds_data[ds][\"alignments\"][\"train\"] for ds in ds_data])\n    val_align = np.array([ds_data[ds][\"alignments\"][\"val\"] for ds in ds_data])\n    train_mai = np.array([ds_data[ds][\"metrics\"][\"train\"] for ds in ds_data])\n    val_mai = np.array([ds_data[ds][\"metrics\"][\"val\"] for ds in ds_data])\n    # average over datasets\n    loss_avg[ab] = {\"train\": train_losses.mean(0), \"val\": val_losses.mean(0)}\n    align_avg[ab] = {\"train\": train_align.mean(0), \"val\": val_align.mean(0)}\n    mai_avg[ab] = {\"train\": train_mai.mean(0), \"val\": val_mai.mean(0)}\n\n# Define epoch axis\nif ab_keys:\n    epochs = np.arange(1, len(loss_avg[ab_keys[0]][\"train\"]) + 1)\n\n# Plot average loss\ntry:\n    plt.figure()\n    for ab in ab_keys:\n        gamma = ab.split(\"_\")[-1]\n        plt.plot(epochs, loss_avg[ab][\"train\"], \"-o\", label=f\"\u03b3={gamma} train\")\n        plt.plot(epochs, loss_avg[ab][\"val\"], \"--s\", label=f\"\u03b3={gamma} val\")\n    plt.title(\"Avg Loss vs Epoch\\nSubtitle: Averaged Across Datasets\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"avg_loss_gamma_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Plot average alignment\ntry:\n    plt.figure()\n    for ab in ab_keys:\n        gamma = ab.split(\"_\")[-1]\n        plt.plot(epochs, align_avg[ab][\"train\"], \"-o\", label=f\"\u03b3={gamma} train\")\n        plt.plot(epochs, align_avg[ab][\"val\"], \"--s\", label=f\"\u03b3={gamma} val\")\n    plt.title(\"Avg Alignment vs Epoch\\nSubtitle: Averaged Across Datasets\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Alignment\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"avg_alignment_gamma_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating alignment plot: {e}\")\n    plt.close()\n\n# Plot average MAI\ntry:\n    plt.figure()\n    for ab in ab_keys:\n        gamma = ab.split(\"_\")[-1]\n        plt.plot(epochs, mai_avg[ab][\"train\"], \"-o\", label=f\"\u03b3={gamma} train\")\n        plt.plot(epochs, mai_avg[ab][\"val\"], \"--s\", label=f\"\u03b3={gamma} val\")\n    plt.title(\"Avg MAI vs Epoch\\nSubtitle: Averaged Across Datasets\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"MAI\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"avg_MAI_gamma_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MAI plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndatasets = [\"ag_news\", \"yelp_polarity\", \"dbpedia_14\"]\nablation_types = [\"random\", \"importance\"]\nhead_counts_list = [2, 4, 8, 12]\n\n# 1\u20133: MAI vs head count for each dataset\nfor name in datasets:\n    try:\n        plt.figure()\n        for t in ablation_types:\n            heads = []\n            mais = []\n            data = experiment_data.get(t, {}).get(name, {})\n            hs = data.get(\"head_counts\", [])\n            m = data.get(\"mai\", [])\n            # compute final MAI per head count\n            for h in sorted(set(hs)):\n                idxs = [i for i, hh in enumerate(hs) if hh == h]\n                if idxs:\n                    heads.append(h)\n                    mais.append(m[max(idxs)])\n            # sort pairs\n            paired = sorted(zip(heads, mais))\n            x, y = zip(*paired)\n            plt.plot(x, y, marker=\"o\", label=t.capitalize())\n        plt.xlabel(\"Number of Heads\")\n        plt.ylabel(\"MAI\")\n        plt.title(f\"{name} MAI vs Head Count\\nRandom vs Importance Ablation\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{name}_MAI_vs_heads.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating MAI plot for {name}: {e}\")\n        plt.close()\n\n# 4\u20135: Loss curves for ag_news at head_count=12\nfor t in ablation_types:\n    try:\n        plt.figure()\n        data = experiment_data.get(t, {}).get(\"ag_news\", {})\n        hs = data.get(\"head_counts\", [])\n        train_losses = data.get(\"losses\", {}).get(\"train\", [])\n        val_losses = data.get(\"losses\", {}).get(\"val\", [])\n        # filter for head_count = 12\n        idxs = [i for i, hh in enumerate(hs) if hh == 12]\n        epochs = list(range(1, len(idxs) + 1))\n        tr = [train_losses[i] for i in idxs]\n        vl = [val_losses[i] for i in idxs]\n        plt.plot(epochs, tr, marker=\"o\", label=\"Train Loss\")\n        plt.plot(epochs, vl, marker=\"o\", linestyle=\"--\", label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"ag_news Loss Curves for {t.capitalize()} Ablation\\nHead Count = 12\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"ag_news_{t}_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves for ag_news ({t}): {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Print best MAI per dataset and ablation\nfor ab in data:\n    for ds in data[ab]:\n        mai_list = data[ab][ds].get(\"mai\", [])\n        if mai_list:\n            print(f\"Dataset: {ds}, Ablation: {ab}, Best MAI: {max(mai_list):.4f}\")\n\n# Generate plots per dataset\ndatasets = next(iter(data.values())).keys()\nfor ds in datasets:\n    # Loss curves\n    try:\n        plt.figure()\n        for ab in data:\n            losses = data[ab][ds][\"losses\"]\n            epochs = np.arange(1, len(losses[\"train\"]) + 1)\n            plt.plot(epochs, losses[\"train\"], label=f\"{ab} train\")\n            plt.plot(epochs, losses[\"val\"], label=f\"{ab} val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"Loss Curves for {ds} Dataset\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ds}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds}: {e}\")\n        plt.close()\n\n    # Alignment curves\n    try:\n        plt.figure()\n        for ab in data:\n            align = data[ab][ds][\"alignments\"]\n            epochs = np.arange(1, len(align[\"train\"]) + 1)\n            plt.plot(epochs, align[\"train\"], label=f\"{ab} train\")\n            plt.plot(epochs, align[\"val\"], label=f\"{ab} val\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Alignment\")\n        plt.title(f\"Alignment Curves for {ds} Dataset\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ds}_alignment_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating alignment plot for {ds}: {e}\")\n        plt.close()\n\n    # Validation MAI curves\n    try:\n        plt.figure()\n        for ab in data:\n            mai_list = data[ab][ds][\"mai\"]\n            epochs = np.arange(1, len(mai_list) + 1)\n            plt.plot(epochs, mai_list, label=f\"{ab} val MAI\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"MAI\")\n        plt.title(f\"Validation MAI for {ds} Dataset\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ds}_mai_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating MAI plot for {ds}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# get sorted dropout keys and dataset names\ndrop_keys = sorted(experiment_data.keys(), key=lambda x: int(x.split(\"_\")[-1]))\ndataset_names = list(experiment_data[drop_keys[0]].keys()) if drop_keys else []\n\n# plot accuracy curves per dataset\nfor ds in dataset_names:\n    try:\n        plt.figure()\n        for key in drop_keys:\n            rate = int(key.split(\"_\")[-1])\n            acc_tr = experiment_data[key][ds][\"metrics\"][\"train\"]\n            acc_val = experiment_data[key][ds][\"metrics\"][\"val\"]\n            epochs = np.arange(1, len(acc_tr) + 1)\n            plt.plot(epochs, acc_tr, label=f\"Train {rate}% dropout\")\n            plt.plot(epochs, acc_val, \"--\", label=f\"Val {rate}% dropout\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\n            f'{ds.replace(\"_\",\" \").title()} Classification\\nTrain (solid) vs Val (dashed) Accuracy'\n        )\n        plt.legend()\n        fname = f\"{ds}_accuracy_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot for {ds}: {e}\")\n        plt.close()\n\n# plot MAI vs dropout rate for all datasets\ntry:\n    plt.figure()\n    rates = [int(k.split(\"_\")[-1]) for k in drop_keys]\n    for ds in dataset_names:\n        mai_vals = [experiment_data[k][ds][\"mai\"][-1] for k in drop_keys]\n        plt.plot(rates, mai_vals, marker=\"o\", label=ds)\n    plt.xlabel(\"Token Dropout Rate (%)\")\n    plt.ylabel(\"Final Epoch MAI\")\n    plt.title(\"MAI vs Token Dropout Rate\\nFinal Epoch MAI for Each Dataset\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"mai_vs_dropout_rate.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MAI vs dropout plot: {e}\")\n    plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\noptimizers = list(experiment_data.keys())\ndatasets = list(experiment_data[optimizers[0]].keys())\n\n# Plot loss curves\ntry:\n    plt.figure(figsize=(12, 4))\n    for i, ds in enumerate(datasets):\n        ax = plt.subplot(1, len(datasets), i + 1)\n        for opt in optimizers:\n            epochs = np.arange(1, len(experiment_data[opt][ds][\"losses\"][\"train\"]) + 1)\n            ax.plot(\n                epochs,\n                experiment_data[opt][ds][\"losses\"][\"train\"],\n                linestyle=\"-\",\n                label=f\"{opt} Train\",\n            )\n            ax.plot(\n                epochs,\n                experiment_data[opt][ds][\"losses\"][\"val\"],\n                linestyle=\"--\",\n                label=f\"{opt} Val\",\n            )\n        ax.set_title(f\"{ds} Dataset\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n        ax.legend()\n    plt.suptitle(\"Loss Curves Across Datasets\\nSolid: Train, Dashed: Val\")\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(os.path.join(working_dir, \"loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# Plot accuracy curves\ntry:\n    plt.figure(figsize=(12, 4))\n    for i, ds in enumerate(datasets):\n        ax = plt.subplot(1, len(datasets), i + 1)\n        for opt in optimizers:\n            epochs = np.arange(\n                1, len(experiment_data[opt][ds][\"accuracy\"][\"train\"]) + 1\n            )\n            ax.plot(\n                epochs,\n                experiment_data[opt][ds][\"accuracy\"][\"train\"],\n                linestyle=\"-\",\n                label=f\"{opt} Train\",\n            )\n            ax.plot(\n                epochs,\n                experiment_data[opt][ds][\"accuracy\"][\"val\"],\n                linestyle=\"--\",\n                label=f\"{opt} Val\",\n            )\n        ax.set_title(f\"{ds} Dataset\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Accuracy\")\n        ax.legend()\n    plt.suptitle(\"Accuracy Curves Across Datasets\\nSolid: Train, Dashed: Val\")\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(os.path.join(working_dir, \"accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves: {e}\")\n    plt.close()\n\n# Plot alignment curves\ntry:\n    plt.figure(figsize=(12, 4))\n    for i, ds in enumerate(datasets):\n        ax = plt.subplot(1, len(datasets), i + 1)\n        for opt in optimizers:\n            epochs = np.arange(\n                1, len(experiment_data[opt][ds][\"alignments\"][\"train\"]) + 1\n            )\n            ax.plot(\n                epochs,\n                experiment_data[opt][ds][\"alignments\"][\"train\"],\n                linestyle=\"-\",\n                label=f\"{opt} Train\",\n            )\n            ax.plot(\n                epochs,\n                experiment_data[opt][ds][\"alignments\"][\"val\"],\n                linestyle=\"--\",\n                label=f\"{opt} Val\",\n            )\n        ax.set_title(f\"{ds} Dataset\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Alignment\")\n        ax.legend()\n    plt.suptitle(\"Alignment Curves Across Datasets\\nSolid: Train, Dashed: Val\")\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(os.path.join(working_dir, \"alignment_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating alignment curves: {e}\")\n    plt.close()\n\n# Plot MAI curves\ntry:\n    plt.figure(figsize=(12, 4))\n    for i, ds in enumerate(datasets):\n        ax = plt.subplot(1, len(datasets), i + 1)\n        for opt in optimizers:\n            epochs = np.arange(1, len(experiment_data[opt][ds][\"mai\"]) + 1)\n            ax.plot(epochs, experiment_data[opt][ds][\"mai\"], label=f\"{opt}\")\n        ax.set_title(f\"{ds} Dataset\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"MAI\")\n        ax.legend()\n    plt.suptitle(\"MAI Across Datasets\")\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(os.path.join(working_dir, \"mai_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MAI curves: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    for ablation, ds_dict in experiment_data.items():\n        for dataset_name, ed in ds_dict.items():\n            epochs = list(range(1, len(ed[\"losses\"][\"train\"]) + 1))\n            try:\n                plt.figure()\n                plt.plot(epochs, ed[\"losses\"][\"train\"], label=\"Train Loss\")\n                plt.plot(epochs, ed[\"losses\"][\"val\"], label=\"Validation Loss\")\n                plt.title(\n                    f\"Dataset {dataset_name} - Ablation {ablation}: Loss Curves\\n\"\n                    \"Train vs Validation Loss\"\n                )\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Loss\")\n                plt.legend()\n                fname = f\"{dataset_name}_{ablation}_loss_curves.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating loss plot for {dataset_name} {ablation}: {e}\")\n                plt.close()\n            try:\n                plt.figure()\n                plt.plot(epochs, ed[\"alignments\"][\"train\"], label=\"Train Alignment\")\n                plt.plot(epochs, ed[\"alignments\"][\"val\"], label=\"Validation Alignment\")\n                plt.title(\n                    f\"Dataset {dataset_name} - Ablation {ablation}: Alignment Curves\\n\"\n                    \"Train vs Validation Alignment\"\n                )\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Alignment\")\n                plt.legend()\n                fname = f\"{dataset_name}_{ablation}_alignment_curves.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n                plt.close()\n            except Exception as e:\n                print(\n                    f\"Error creating alignment plot for {dataset_name} {ablation}: {e}\"\n                )\n                plt.close()\n            try:\n                plt.figure()\n                plt.plot(epochs, ed[\"mai\"], marker=\"o\")\n                plt.title(\n                    f\"Dataset {dataset_name} - Ablation {ablation}: MAI over Epochs\\n\"\n                    \"Validation MAI\"\n                )\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"MAI\")\n                fname = f\"{dataset_name}_{ablation}_mai.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating MAI plot for {dataset_name} {ablation}: {e}\")\n                plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Prepare temperature keys and dataset list\ntemp_items = sorted([(float(k.split(\"_\")[-1]), k) for k in experiment_data.keys()])\nfirst_key = temp_items[0][1]\ndatasets = list(experiment_data[first_key].keys())\n\n# Plot per\u2010dataset metrics\nfor dataset in datasets:\n    try:\n        fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n        for idx, metric in enumerate([\"losses\", \"accuracy\", \"alignments\"]):\n            ax = axs[idx]\n            for temp, key in temp_items:\n                rec = experiment_data[key][dataset]\n                if metric == \"losses\":\n                    y_tr, y_val = rec[\"losses\"][\"train\"], rec[\"losses\"][\"val\"]\n                    ax.plot(range(1, len(y_tr) + 1), y_tr, label=f\"T={temp} train\")\n                    ax.plot(\n                        range(1, len(y_val) + 1),\n                        y_val,\n                        linestyle=\"--\",\n                        label=f\"T={temp} val\",\n                    )\n                    ax.set_ylabel(\"Loss\")\n                elif metric == \"accuracy\":\n                    y_tr, y_val = rec[\"accuracy\"][\"train\"], rec[\"accuracy\"][\"val\"]\n                    ax.plot(range(1, len(y_tr) + 1), y_tr, label=f\"T={temp} train\")\n                    ax.plot(\n                        range(1, len(y_val) + 1),\n                        y_val,\n                        linestyle=\"--\",\n                        label=f\"T={temp} val\",\n                    )\n                    ax.set_ylabel(\"Accuracy\")\n                else:  # alignments\n                    y_tr, y_val = rec[\"alignments\"][\"train\"], rec[\"alignments\"][\"val\"]\n                    ax.plot(range(1, len(y_tr) + 1), y_tr, label=f\"T={temp} train\")\n                    ax.plot(\n                        range(1, len(y_val) + 1),\n                        y_val,\n                        linestyle=\"--\",\n                        label=f\"T={temp} val\",\n                    )\n                    ax.set_ylabel(\"Alignment\")\n                ax.set_xlabel(\"Epoch\")\n                ax.set_title(f\"{metric.capitalize()} Curves\")\n        fig.suptitle(f\"{dataset} Metrics across Temperatures\")\n        fig.legend(loc=\"upper right\")\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        fname = f\"{dataset}_metrics.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {dataset}: {e}\")\n        plt.close()\n\n# Summary plot of final MAI vs temperature\ntry:\n    plt.figure()\n    for dataset in datasets:\n        temps, mai_vals = [], []\n        for temp, key in temp_items:\n            rec = experiment_data[key][dataset]\n            temps.append(temp)\n            mai_vals.append(rec[\"mai\"][-1])\n        plt.plot(temps, mai_vals, marker=\"o\", label=dataset)\n    plt.xlabel(\"Softmax Temperature\")\n    plt.ylabel(\"Final MAI\")\n    plt.title(\"Final Model Alignment-Accuracy Index vs Temperature\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"mai_vs_temperature.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MAI summary plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Prepare temperature keys and dataset list\ntemp_items = sorted([(float(k.split(\"_\")[-1]), k) for k in experiment_data.keys()])\nfirst_key = temp_items[0][1]\ndatasets = list(experiment_data[first_key].keys())\n\n# Plot per\u2010dataset metrics\nfor dataset in datasets:\n    try:\n        fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n        for idx, metric in enumerate([\"losses\", \"accuracy\", \"alignments\"]):\n            ax = axs[idx]\n            for temp, key in temp_items:\n                rec = experiment_data[key][dataset]\n                if metric == \"losses\":\n                    y_tr, y_val = rec[\"losses\"][\"train\"], rec[\"losses\"][\"val\"]\n                    ax.plot(range(1, len(y_tr) + 1), y_tr, label=f\"T={temp} train\")\n                    ax.plot(\n                        range(1, len(y_val) + 1),\n                        y_val,\n                        linestyle=\"--\",\n                        label=f\"T={temp} val\",\n                    )\n                    ax.set_ylabel(\"Loss\")\n                elif metric == \"accuracy\":\n                    y_tr, y_val = rec[\"accuracy\"][\"train\"], rec[\"accuracy\"][\"val\"]\n                    ax.plot(range(1, len(y_tr) + 1), y_tr, label=f\"T={temp} train\")\n                    ax.plot(\n                        range(1, len(y_val) + 1),\n                        y_val,\n                        linestyle=\"--\",\n                        label=f\"T={temp} val\",\n                    )\n                    ax.set_ylabel(\"Accuracy\")\n                else:  # alignments\n                    y_tr, y_val = rec[\"alignments\"][\"train\"], rec[\"alignments\"][\"val\"]\n                    ax.plot(range(1, len(y_tr) + 1), y_tr, label=f\"T={temp} train\")\n                    ax.plot(\n                        range(1, len(y_val) + 1),\n                        y_val,\n                        linestyle=\"--\",\n                        label=f\"T={temp} val\",\n                    )\n                    ax.set_ylabel(\"Alignment\")\n                ax.set_xlabel(\"Epoch\")\n                ax.set_title(f\"{metric.capitalize()} Curves\")\n        fig.suptitle(f\"{dataset} Metrics across Temperatures\")\n        fig.legend(loc=\"upper right\")\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        fname = f\"{dataset}_metrics.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {dataset}: {e}\")\n        plt.close()\n\n# Summary plot of final MAI vs temperature\ntry:\n    plt.figure()\n    for dataset in datasets:\n        temps, mai_vals = [], []\n        for temp, key in temp_items:\n            rec = experiment_data[key][dataset]\n            temps.append(temp)\n            mai_vals.append(rec[\"mai\"][-1])\n        plt.plot(temps, mai_vals, marker=\"o\", label=dataset)\n    plt.xlabel(\"Softmax Temperature\")\n    plt.ylabel(\"Final MAI\")\n    plt.title(\"Final Model Alignment-Accuracy Index vs Temperature\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"mai_vs_temperature.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MAI summary plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Prepare temperature keys and dataset list\ntemp_items = sorted([(float(k.split(\"_\")[-1]), k) for k in experiment_data.keys()])\nfirst_key = temp_items[0][1]\ndatasets = list(experiment_data[first_key].keys())\n\n# Plot per\u2010dataset metrics\nfor dataset in datasets:\n    try:\n        fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n        for idx, metric in enumerate([\"losses\", \"accuracy\", \"alignments\"]):\n            ax = axs[idx]\n            for temp, key in temp_items:\n                rec = experiment_data[key][dataset]\n                if metric == \"losses\":\n                    y_tr, y_val = rec[\"losses\"][\"train\"], rec[\"losses\"][\"val\"]\n                    ax.plot(range(1, len(y_tr) + 1), y_tr, label=f\"T={temp} train\")\n                    ax.plot(\n                        range(1, len(y_val) + 1),\n                        y_val,\n                        linestyle=\"--\",\n                        label=f\"T={temp} val\",\n                    )\n                    ax.set_ylabel(\"Loss\")\n                elif metric == \"accuracy\":\n                    y_tr, y_val = rec[\"accuracy\"][\"train\"], rec[\"accuracy\"][\"val\"]\n                    ax.plot(range(1, len(y_tr) + 1), y_tr, label=f\"T={temp} train\")\n                    ax.plot(\n                        range(1, len(y_val) + 1),\n                        y_val,\n                        linestyle=\"--\",\n                        label=f\"T={temp} val\",\n                    )\n                    ax.set_ylabel(\"Accuracy\")\n                else:  # alignments\n                    y_tr, y_val = rec[\"alignments\"][\"train\"], rec[\"alignments\"][\"val\"]\n                    ax.plot(range(1, len(y_tr) + 1), y_tr, label=f\"T={temp} train\")\n                    ax.plot(\n                        range(1, len(y_val) + 1),\n                        y_val,\n                        linestyle=\"--\",\n                        label=f\"T={temp} val\",\n                    )\n                    ax.set_ylabel(\"Alignment\")\n                ax.set_xlabel(\"Epoch\")\n                ax.set_title(f\"{metric.capitalize()} Curves\")\n        fig.suptitle(f\"{dataset} Metrics across Temperatures\")\n        fig.legend(loc=\"upper right\")\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        fname = f\"{dataset}_metrics.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {dataset}: {e}\")\n        plt.close()\n\n# Summary plot of final MAI vs temperature\ntry:\n    plt.figure()\n    for dataset in datasets:\n        temps, mai_vals = [], []\n        for temp, key in temp_items:\n            rec = experiment_data[key][dataset]\n            temps.append(temp)\n            mai_vals.append(rec[\"mai\"][-1])\n        plt.plot(temps, mai_vals, marker=\"o\", label=dataset)\n    plt.xlabel(\"Softmax Temperature\")\n    plt.ylabel(\"Final MAI\")\n    plt.title(\"Final Model Alignment-Accuracy Index vs Temperature\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"mai_vs_temperature.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MAI summary plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Prepare temperature keys and dataset list\ntemp_items = sorted([(float(k.split(\"_\")[-1]), k) for k in experiment_data.keys()])\nfirst_key = temp_items[0][1]\ndatasets = list(experiment_data[first_key].keys())\n\n# Plot per\u2010dataset metrics\nfor dataset in datasets:\n    try:\n        fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n        for idx, metric in enumerate([\"losses\", \"accuracy\", \"alignments\"]):\n            ax = axs[idx]\n            for temp, key in temp_items:\n                rec = experiment_data[key][dataset]\n                if metric == \"losses\":\n                    y_tr, y_val = rec[\"losses\"][\"train\"], rec[\"losses\"][\"val\"]\n                    ax.plot(range(1, len(y_tr) + 1), y_tr, label=f\"T={temp} train\")\n                    ax.plot(\n                        range(1, len(y_val) + 1),\n                        y_val,\n                        linestyle=\"--\",\n                        label=f\"T={temp} val\",\n                    )\n                    ax.set_ylabel(\"Loss\")\n                elif metric == \"accuracy\":\n                    y_tr, y_val = rec[\"accuracy\"][\"train\"], rec[\"accuracy\"][\"val\"]\n                    ax.plot(range(1, len(y_tr) + 1), y_tr, label=f\"T={temp} train\")\n                    ax.plot(\n                        range(1, len(y_val) + 1),\n                        y_val,\n                        linestyle=\"--\",\n                        label=f\"T={temp} val\",\n                    )\n                    ax.set_ylabel(\"Accuracy\")\n                else:  # alignments\n                    y_tr, y_val = rec[\"alignments\"][\"train\"], rec[\"alignments\"][\"val\"]\n                    ax.plot(range(1, len(y_tr) + 1), y_tr, label=f\"T={temp} train\")\n                    ax.plot(\n                        range(1, len(y_val) + 1),\n                        y_val,\n                        linestyle=\"--\",\n                        label=f\"T={temp} val\",\n                    )\n                    ax.set_ylabel(\"Alignment\")\n                ax.set_xlabel(\"Epoch\")\n                ax.set_title(f\"{metric.capitalize()} Curves\")\n        fig.suptitle(f\"{dataset} Metrics across Temperatures\")\n        fig.legend(loc=\"upper right\")\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        fname = f\"{dataset}_metrics.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {dataset}: {e}\")\n        plt.close()\n\n# Summary plot of final MAI vs temperature\ntry:\n    plt.figure()\n    for dataset in datasets:\n        temps, mai_vals = [], []\n        for temp, key in temp_items:\n            rec = experiment_data[key][dataset]\n            temps.append(temp)\n            mai_vals.append(rec[\"mai\"][-1])\n        plt.plot(temps, mai_vals, marker=\"o\", label=dataset)\n    plt.xlabel(\"Softmax Temperature\")\n    plt.ylabel(\"Final MAI\")\n    plt.title(\"Final Model Alignment-Accuracy Index vs Temperature\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"mai_vs_temperature.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating MAI summary plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load all experiment data\ntry:\n    experiment_data_path_list = [\n        \"experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_a01c5371854b472d9d698de9c013b3bb_proc_4081278/experiment_data.npy\",\n        \"experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_c54e9756eab849f398468a9dee659963_proc_4081280/experiment_data.npy\",\n        \"experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_85f8d185ab7644bb8c36bce01d477867_proc_4081279/experiment_data.npy\",\n    ]\n    all_experiment_data = []\n    for rel_path in experiment_data_path_list:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\"), rel_path)\n        data = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(data)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Prepare temperature keys and dataset list\ntemp_items = sorted(\n    [(float(k.split(\"_\")[-1]), k) for k in all_experiment_data[0].keys()]\n)\nfirst_key = temp_items[0][1]\ndatasets = list(all_experiment_data[0][first_key].keys())\n\n# Plot aggregated per\u2010dataset metrics\nfor dataset in datasets:\n    try:\n        fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n        for idx, metric in enumerate([\"losses\", \"accuracy\", \"alignments\"]):\n            ax = axs[idx]\n            for temp, key in temp_items:\n                # collect replicates\n                y_tr_list = [\n                    exp[key][dataset][metric][\"train\"] for exp in all_experiment_data\n                ]\n                y_val_list = [\n                    exp[key][dataset][metric][\"val\"] for exp in all_experiment_data\n                ]\n                # truncate to shortest run\n                min_epochs = min(len(y) for y in y_tr_list)\n                y_tr_arr = np.array([y[:min_epochs] for y in y_tr_list])\n                y_val_arr = np.array([y[:min_epochs] for y in y_val_list])\n                # compute mean & SEM\n                mean_tr = np.mean(y_tr_arr, axis=0)\n                sem_tr = np.std(y_tr_arr, axis=0, ddof=1) / np.sqrt(len(y_tr_arr))\n                mean_val = np.mean(y_val_arr, axis=0)\n                sem_val = np.std(y_val_arr, axis=0, ddof=1) / np.sqrt(len(y_val_arr))\n                epochs = np.arange(1, min_epochs + 1)\n                # plot curves and SEM\n                ax.plot(epochs, mean_tr, label=f\"T={temp} train\")\n                ax.fill_between(epochs, mean_tr - sem_tr, mean_tr + sem_tr, alpha=0.2)\n                ax.plot(epochs, mean_val, linestyle=\"--\", label=f\"T={temp} val\")\n                ax.fill_between(\n                    epochs, mean_val - sem_val, mean_val + sem_val, alpha=0.2\n                )\n                # labels\n                if metric == \"losses\":\n                    ax.set_ylabel(\"Loss\")\n                elif metric == \"accuracy\":\n                    ax.set_ylabel(\"Accuracy\")\n                else:\n                    ax.set_ylabel(\"Alignment\")\n                ax.set_xlabel(\"Epoch\")\n                ax.set_title(f\"{metric.capitalize()} Curves\")\n            ax.legend()\n        fig.suptitle(f\"{dataset} Aggregated Metrics across Temperatures\")\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        fname = f\"{dataset}_aggregated_metrics.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating aggregated plot for {dataset}: {e}\")\n        plt.close()\n\n# Summary plot of aggregated final MAI vs temperature\ntry:\n    plt.figure()\n    for dataset in datasets:\n        temps = []\n        mean_mais = []\n        sem_mais = []\n        for temp, key in temp_items:\n            mai_vals = [exp[key][dataset][\"mai\"][-1] for exp in all_experiment_data]\n            mean_mai = np.mean(mai_vals)\n            sem_mai = np.std(mai_vals, ddof=1) / np.sqrt(len(mai_vals))\n            temps.append(temp)\n            mean_mais.append(mean_mai)\n            sem_mais.append(sem_mai)\n        plt.errorbar(\n            temps, mean_mais, yerr=sem_mais, marker=\"o\", linestyle=\"-\", label=dataset\n        )\n    plt.xlabel(\"Softmax Temperature\")\n    plt.ylabel(\"Final MAI\")\n    plt.title(\"Aggregated Final Model Alignment-Accuracy Index vs Temperature\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"aggregated_mai_vs_temperature.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated MAI summary plot: {e}\")\n    plt.close()\n\n# Print out aggregated MAI summary\ntry:\n    print(\"Aggregated final MAI per dataset (mean \u00b1 SEM):\")\n    for dataset in datasets:\n        temps = []\n        means = []\n        sems = []\n        for temp, key in temp_items:\n            vals = [exp[key][dataset][\"mai\"][-1] for exp in all_experiment_data]\n            temps.append(temp)\n            means.append(np.mean(vals))\n            sems.append(np.std(vals, ddof=1) / np.sqrt(len(vals)))\n        print(f\"{dataset}: Temps {temps}\")\n        print(f\"  Means: {np.round(means,4)}, SEM: {np.round(sems,4)}\")\nexcept Exception as e:\n    print(f\"Error printing MAI summary: {e}\")\n"], "plot_plan": ["First, load the experiment data and extract the synthetic results under the learning rate sweep. Then use two separate try-except blocks to create and save figures: one for the training/validation loss curves and another for the training/validation alignment curves. Each figure will have two subplots\u2014left for training metrics and right for validation\u2014with clear titles, axis labels, legends for each learning rate, and a suptitle specifying \u201cSynthetic Dataset\u201d and the left/right designations. We iterate over the stored learning rates to plot each curve, save the figures to `working_dir` with descriptive filenames, and close them. If any error occurs in plotting or data loading, we print an error message and ensure the figure is closed.", null, "I will load the saved experiment data from the working directory, then iterate over each dataset (ag_news, yelp_polarity, dbpedia_14) to plot side-by-side subplots of training vs validation accuracy and loss, overlaying both ablation types (baseline and linear_probe). Each figure is wrapped in a try-except block to catch and report errors, and explicitly closed after saving to avoid resource leaks. Finally, I will assemble a bar chart comparing the final validation accuracy of both ablations across all datasets in another try-except block. All plots are saved with descriptive filenames into the working directory.", "I will load the saved experiment data from the working directory and print out the final validation accuracy and MAI for each pooling and dataset. Then I will iterate over each pooling method and dataset to generate three standard plots: (1) training and validation loss/accuracy curves, (2) training and validation alignment and MAI curves, and (3) a bar chart comparing ground truth and predicted class distributions. Each plotting block is enclosed in a try\u2013except to safely continue on errors, figures are closed after saving, and each plot uses descriptive titles including pooling type and dataset name with a clear subtitle. All images are saved into the working directory using filenames that specify the pooling, dataset, and plot type.", "The script loads the saved experiment data and then iterates over both ablation settings and datasets. For each combination, it extracts per\u2010epoch losses, accuracy, alignment, and MAI values. It then generates separate plots for training vs. validation loss, accuracy vs. epoch, alignment vs. epoch, and MAI vs. epoch, each inside its own try\u2010except block to catch and log any errors. Every figure is titled with the ablation setting and dataset name, labeled axes, and a legend where appropriate. The plots are saved to the working directory with descriptive filenames and closed after saving to free resources. Any errors encountered while loading or plotting are printed to the console.", null, "We load the saved experiment data, extract the dropout keys and dataset names, and then iterate over each dataset. For each dataset we create three plots wrapped in independent try-except blocks: (1) loss curves with left subplot showing training loss and right showing validation loss, (2) alignment curves similarly split, and (3) MAI curves across epochs. Each figure\u2019s title includes the dataset name and a clear subtitle describing its subplots, and legends differentiate dropout rates. We save each figure to the working directory with descriptive filenames containing both the dataset and plot type. After each save or on error, we close the figure to free resources. Any loading or plotting error is printed to the console.", "First, we load the saved experiment data from the working directory and extract the list of datasets and activations. For each dataset, we create a figure with two side-by-side panels: on the left we plot training and validation loss curves across epochs for each activation function, and on the right we plot MAI curves across epochs. Each subplot is titled and labeled, and a succinct suptitle indicates \u201cLeft: Loss Curves, Right: MAI Curves\u201d along with the dataset name. We wrap each dataset\u2019s plotting routine in a try-except block to catch errors, ensure the figure is always closed, and save the image to the working folder with a descriptive filename. After saving, we compute and print the final MAI values for all activations on that dataset to summarize performance.", "I will load the saved experiment data and extract losses and accuracies for each lambda and dataset. For each dataset I will make two plots: one showing training vs validation loss curves across epochs, and another showing the corresponding accuracy curves. Each figure uses two subplots (left for training, right for validation) and includes all lambdas in the legend. Every plotting step is wrapped in a try-except to log errors and always close the figure afterward. Filenames include the dataset name and plot type for clarity. Finally, I will print a summary of the final validation accuracy and MAI for each lambda and dataset. This ensures we visualize convergence behavior and compare ablations succinctly.", "We will load the saved experiment data dictionary from the working directory and extract the list of datasets and ablation conditions. For each dataset, we will create a single figure containing two subplots side-by-side: on the left the training and validation loss curves for each ablation, and on the right the corresponding bidirectional alignment metrics. We will label axes, add legends, and include a main figure title indicating the dataset name and a subtitle \u201cLeft: Loss curves, Right: Bidirectional Alignment curves.\u201d Each figure is saved to the working directory under a descriptive filename, and the plotting commands are wrapped in try-except blocks to ensure errors are caught and figures are always closed. This produces clear, comparable plots for all datasets in at most three figures.", "We will load the saved experiment_data dictionary and extract the three focal_loss_gamma ablation keys. For each gamma, we will gather the per-epoch train/val losses, alignments, and MAI metrics across all datasets and compute their averages. We define epochs simply as [1,\u2026,n_epochs] based on the recorded lists. We then create three separate plots\u2014average loss, average alignment, and average MAI\u2014each wrapped in its own try\u2013except block to ensure errors are caught without aborting. Each plot includes curves for train and val for all gamma settings, a clear title with a subtitle indicating that the data is averaged across datasets, labeled axes, and a legend. Filenames incorporate the metric name and note that this is a gamma comparison across datasets. Figures are saved into the working directory and closed after saving to manage resources.", "We start by loading the saved `experiment_data.npy` from the working directory and handle any loading errors gracefully. For each of the three datasets, we compute the final MAI at each head\u2010count by selecting the last value among the three epochs per head\u2010count, then plot MAI vs. number of heads with separate curves for random and importance\u2013based ablations. Each plot is wrapped in a try/except to catch errors, and we save with a descriptive filename like `ag_news_MAI_vs_heads.png`. Then, focusing on `ag_news`, we extract train and validation losses for head\u2010count 12 across the three epochs for each ablation type, plot these curves in separate figures, and again wrap each in try/except. We label axes clearly and include titles that specify the dataset, plot type, and ablation method. All figures are saved to the `working` directory and figures are closed after saving. This yields five plots in total: three MAI\u2010vs\u2010head\u2010count plots and two loss\u2010curve plots.", "First, the script loads the experiment_data.npy file from the working directory. It then iterates over each dataset recorded under both 'pretrained' and 'random_init' ablations. For each dataset, it computes and prints the best validation MAI per ablation. Next, it creates three standard plots per dataset: a loss curve (training vs validation) combining both ablations; an alignment curve (training vs validation) combining both ablations; and a validation MAI curve for both ablations. Each plot is wrapped in its own try-except block, uses basic matplotlib calls, includes a descriptive title and saves to working_dir with a clear filename. Figures are always closed after saving or on exception. No extra styling or simulated data is used; only values extracted from experiment_data are plotted.", "The solution first loads the saved experiment data from the working directory. It then identifies all token-dropout variants and dataset names. For each dataset, it plots training versus validation accuracy curves across epochs for every dropout rate, using solid lines for training and dashed for validation. Each plot is wrapped in try-except to handle errors and always closes the figure. Finally, it generates a combined MAI versus dropout rate plot across all datasets using the final epoch\u2019s MAI, again with error handling and figure cleanup. All figures are titled with dataset information and saved under descriptive filenames in the working directory.", null, "Here\u2019s a sketch of the solution:  \n1. Load the saved `experiment_data.npy` from the working directory.  \n2. Extract the list of optimizers and datasets contained in the data.  \n3. In separate try\u2013except blocks, create and save a figure of loss curves: a 1\u00d73 grid of subplots (one per dataset), plotting solid lines for training loss and dashed for validation loss, with subplot titles naming each dataset and a clear suptitle.  \n4. Repeat similarly for accuracy and alignment curves, saving each as its own PNG.  \n5. Finally, plot MAI across epochs in a 1\u00d73 subplot figure, with each optimizer\u2019s MAI curve per dataset, appropriate legends, titles, and save the figure.  \n6. Always close figures after saving and catch any exceptions to avoid crashing.", "Our solution will load the saved experiment_data.npy from the working directory. Then we loop over each ablation setting and dataset, extracting the logged losses, alignments, and MAI metrics. We will create three standard plots per combination: loss curves (train vs validation), alignment curves (train vs validation), and the MAI over epochs. Each plot will be generated in its own try-except block to catch errors and ensure the figure closes properly even on exceptions. We will use basic matplotlib plotting without custom styles, labeling axes, legends, and using a two-line title to specify dataset, ablation, and subtitle (e.g., 'Train vs Validation Loss'). Saved file names will include the dataset name, ablation, and plot type for clarity (e.g., ag_news_independent_loss.png). All plots will be saved in the working directory. Finally, we ensure no more than five similar figures are generated per category (which is met since we have only three epochs).", null, "We will load the saved experiment data from the working directory and extract the list of datasets and softmax temperatures. For each dataset, we generate a combined three\u2010panel figure showing training vs validation curves for losses, accuracy, and alignment metrics across epochs, with separate line styles for each temperature. Each dataset figure is wrapped in its own try\u2010except block to catch errors, and figures are closed after saving. After plotting per\u2010dataset metrics, we create a summary figure of the final MAI (Model Alignment\u2010Accuracy Index) versus temperature for all datasets in a single plot. This summary plot is also enclosed in its own try\u2010except block. All saved filenames include the dataset name and metric type, and each figure has clear titles and subplot subtitles. We ensure no more than five figures are created in total and only plot existing data. All code uses basic matplotlib calls, numpy for data handling, and standard OS paths.", null, null, null, null], "ablation_name": [null, "Fine-Tune-DistilBERT", "Linear-Probe Classifier (No Hidden Layer)", "Pooling Mechanism Ablation", null, "Representation Depth Ablation", "MLP Dropout Rate Ablation", "Activation Function Ablation", "Alignment Regularization Ablation", null, "Loss Function Ablation \u2013 Focal Loss", "Attention Head Ablation", "DistilBERT Pre-training Ablation", "Token Dropout Ablation", "Word-Order Sensitivity Ablation", "Optimizer Choice Ablation", "MLP Parameter Sharing Ablation", null, "Softmax Temperature Ablation", null, null, null, null], "hyperparam_name": ["learning_rate", null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["Here\u2019s a script that loads the saved experiment data, iterates over each dataset\nand learning rate, extracts the final training/validation alignments and losses,\ncomputes test accuracy from the stored predictions, and prints each metric with\nclear labels at the global scope:", "Here\u2019s a script that loads the stored `experiment_data.npy` from the working\ndirectory, iterates over each ablation and dataset, and prints the final epoch\u2019s\ntrain and validation accuracy, loss, alignment, and MAI with clear labels. The\ndataset name (including ablation) is printed before its metrics, and no\nentry\u2010point guard is used so it executes immediately.", "The script first constructs the working directory path and loads the saved\nexperiment data from the numpy file. It then identifies all datasets and\nablation types present in the data. For each dataset and ablation, it extracts\nthe final values of the train and validation accuracy from the `metrics` arrays.\nFinally, it prints the dataset name followed by each ablation\u2019s train accuracy\nand validation accuracy with clear metric labels.", "", "The following script sets the working directory, loads the saved NumPy file, and\nthen iterates over each ablation setting and dataset to extract the final\nepoch\u2019s training accuracy, validation loss, validation bidirectional alignment,\nand MAI values. It prints each dataset name (including the ablation type)\nfollowed by clearly labeled metric values.", "I will load the saved `experiment_data.npy` dictionary from the working\ndirectory, then iterate over each ablation method and dataset in it. For each\ndataset I will pull out the final training loss and alignment, the best\nvalidation loss (minimum), the final validation alignment and MAI, and compute\ntest\u2010set accuracy and macro F1 from the saved predictions and ground truths. I\nwill then print the dataset name followed by each metric with a clear label.", "I will load the saved `experiment_data.npy` from the working directory, access\nthe `\"mlp_dropout_rate_ablation\"` section, and then iterate over each dropout\nsetting and dataset. For each, I extract the last (final epoch) values of\ntraining/validation loss, training/validation alignment, and the MAI score, and\ncompute test accuracy from the stored predictions. Finally, I print the dataset\nname (including its dropout rate) and each metric with clear labels.", "I will load the saved NumPy file from the working directory, then iterate\nthrough each dataset and activation to pull out the final epoch values for\ntraining loss, training alignment, validation loss, validation alignment, and\nMAI. For each dataset, I will print its name once, and then clearly label each\nmetric under each activation. The script will execute immediately without any\n`if __name__ == \"__main__\":` guard.", "The script loads the saved `experiment_data.npy` from the `working` directory as\na nested dictionary. It then loops over each lambda key and each dataset,\nextracting the final epoch\u2019s metrics for training/validation accuracy, loss,\nalignment, and MAI. For every dataset (annotated with its lambda value), it\nprints out each metric with clear, descriptive labels. The code runs immediately\nin the global scope without any `__main__` guard.", "I will load the saved `experiment_data.npy` file from the working directory\nusing `numpy.load` and unpack the nested dictionary structure of ablations and\ndatasets. For each ablation and dataset pair, I will extract the final epoch\nvalues of training loss, validation loss, training bidirectional alignment, and\nvalidation bidirectional alignment. I will then print the dataset (and ablation)\nname followed by each metric with clear, descriptive labels. The script runs\nimmediately at global scope and requires no special entry point.", "The following script locates the working directory, loads the saved experiment\ndata, and derives the list of datasets from the dictionary. It then loops over\neach dataset and each focal\u2010loss gamma setting, extracting the final epoch\u2019s\ntrain and validation MAI values. Each dataset\u2019s name is printed before its\nmetrics, and metric names (\u2018train MAI\u2019, \u2018validation MAI\u2019) are clearly labeled.\nThe script runs immediately from the global scope without any special entry\npoint.", "Below is a simple script that loads the saved results, then for each dataset\nprints its name once and iterates through each ablation type and head\u2010count\nconfiguration.  For each configuration it finds the final (last) epoch data and\nclearly labels \u201cfinal train loss,\u201d \u201cfinal validation loss,\u201d \u201cfinal train\nalignment,\u201d \u201cfinal validation alignment,\u201d and \u201cfinal validation MAI.\u201d  All code\nruns immediately without needing an entry\u2010point guard.", "The script loads the `experiment_data.npy` file from the working directory and\niterates through each ablation setting and dataset. For each dataset it\nretrieves the last values of training/validation losses and alignments, the\nfinal MAI, and computes test accuracy from the stored predictions and ground\ntruth. It then prints the dataset name (including the ablation type) followed by\nclearly labeled metrics such as train loss, training alignment, validation loss,\nvalidation alignment, validation MAI, and test accuracy.", "I will load the saved experiment data from the working directory, iterate\nthrough each token\u2010dropout setting and dataset, and extract the final epoch\nvalues for accuracy, loss, alignment, and MAI.  For each dataset (and its\ncorresponding ablation key), I will print a header with the dataset name and\ndropout setting, followed by each metric with precise labels like \u201ctrain\naccuracy,\u201d \u201cvalidation loss,\u201d and \u201cvalidation MAI.\u201d  This script runs\nimmediately at the global scope without requiring a special entry point.  No\nplotting or conditional main guards are used.", "", "The following script locates the `experiment_data.npy` file in the `working`\ndirectory, loads it as a nested dictionary, and then iterates over each\noptimizer and dataset entry. For each dataset, it pulls the final epoch\u2019s\nmetrics\u2014train loss, train alignment, train accuracy, validation loss, validation\nalignment, validation accuracy, and MAI\u2014and prints them with precise labels. The\nscript runs immediately at import without any `if __name__ == \"__main__\":` guard\nand does not create any plots.", "I will load the saved numpy file from the working directory, iterate through\neach ablation setting and dataset, and extract the final (third\u2010epoch) values\nfor training/validation loss and alignment plus the final MAI. I\u2019ll then compute\nthe test accuracy from the stored predictions and ground truth, and print each\nmetric with a clear label immediately, at the top level of the script.", "Below is a script that constructs the working directory path, loads the saved\nNumPy experiment data, and iterates through each ablation and dataset. For each\ncombination, it pulls the final epoch\u2019s training and validation losses,\nalignment scores, and MAI, computes validation accuracy from the stored\npredictions, and prints them with clear, descriptive labels. This runs\nimmediately without any entry\u2010point guard.", "The script below immediately loads the NumPy file from the `working` directory,\niterates through each temperature setting and dataset, and extracts the final\nepoch values for training and validation loss, alignment, accuracy, and\nvalidation MAI. It then prints the name of each dataset followed by clearly\nlabeled metrics. All code is at global scope and runs as soon as the script is\ninvoked.", "The script below immediately loads the NumPy file from the `working` directory,\niterates through each temperature setting and dataset, and extracts the final\nepoch values for training and validation loss, alignment, accuracy, and\nvalidation MAI. It then prints the name of each dataset followed by clearly\nlabeled metrics. All code is at global scope and runs as soon as the script is\ninvoked.", "The script below immediately loads the NumPy file from the `working` directory,\niterates through each temperature setting and dataset, and extracts the final\nepoch values for training and validation loss, alignment, accuracy, and\nvalidation MAI. It then prints the name of each dataset followed by clearly\nlabeled metrics. All code is at global scope and runs as soon as the script is\ninvoked.", "The script below immediately loads the NumPy file from the `working` directory,\niterates through each temperature setting and dataset, and extracts the final\nepoch values for training and validation loss, alignment, accuracy, and\nvalidation MAI. It then prints the name of each dataset followed by clearly\nlabeled metrics. All code is at global scope and runs as soon as the script is\ninvoked.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Define working directory and load the data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset under the learning_rate key\nfor dataset_name, ds in experiment_data[\"learning_rate\"].items():\n    print(f\"{dataset_name.capitalize()} dataset:\")\n    lrs = ds[\"lrs\"]\n    # Loop over all learning rates\n    for idx, lr in enumerate(lrs):\n        train_aligns = ds[\"metrics\"][\"train\"][idx]\n        val_aligns = ds[\"metrics\"][\"val\"][idx]\n        train_losses = ds[\"losses\"][\"train\"][idx]\n        val_losses = ds[\"losses\"][\"val\"][idx]\n        preds = ds[\"predictions\"][idx]\n        gts = ds[\"ground_truth\"][idx]\n        # Extract final epoch values\n        final_train_alignment = train_aligns[-1]\n        final_val_alignment = val_aligns[-1]\n        final_train_loss = train_losses[-1]\n        final_val_loss = val_losses[-1]\n        # Compute test accuracy\n        test_accuracy = np.mean(preds == gts)\n        # Print metrics with clear labels\n        print(f\"  Learning rate {lr:.1e}:\")\n        print(f\"    Final training alignment: {final_train_alignment:.4f}\")\n        print(f\"    Final validation alignment: {final_val_alignment:.4f}\")\n        print(f\"    Final training loss: {final_train_loss:.4f}\")\n        print(f\"    Final validation loss: {final_val_loss:.4f}\")\n        print(f\"    Test accuracy: {test_accuracy:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Define working directory and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over ablations and datasets to extract and print final metrics\nfor ablation, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        # Fetch final values for each metric\n        final_train_acc = data[\"metrics\"][\"train\"][-1]\n        final_val_acc = data[\"metrics\"][\"val\"][-1]\n        final_train_loss = data[\"losses\"][\"train\"][-1]\n        final_val_loss = data[\"losses\"][\"val\"][-1]\n        final_train_align = data[\"alignments\"][\"train\"][-1]\n        final_val_align = data[\"alignments\"][\"val\"][-1]\n        final_mai = data[\"mai\"][-1]\n\n        # Print dataset and metrics with clear labels\n        print(f\"Dataset: {dataset_name} ({ablation})\")\n        print(f\"train accuracy: {final_train_acc:.4f}\")\n        print(f\"validation accuracy: {final_val_acc:.4f}\")\n        print(f\"training loss: {final_train_loss:.4f}\")\n        print(f\"validation loss: {final_val_loss:.4f}\")\n        print(f\"training alignment: {final_train_align:.4f}\")\n        print(f\"validation alignment: {final_val_align:.4f}\")\n        print(f\"mutual AI-user alignment index (MAI): {final_mai:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# 1. Define the working directory and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# 2. Identify datasets and ablation types\nablation_types = list(experiment_data.keys())\n# assume all ablations have the same dataset keys\ndatasets = list(experiment_data[ablation_types[0]].keys())\n\n# 3. Iterate through each dataset and ablation, print final metrics\nfor dataset in datasets:\n    print(f\"Dataset: {dataset}\")\n    for ablation in ablation_types:\n        ed = experiment_data[ablation][dataset]\n        final_train_acc = ed[\"metrics\"][\"train\"][-1]\n        final_val_acc = ed[\"metrics\"][\"val\"][-1]\n        print(f\"  Ablation: {ablation}\")\n        print(f\"    Train accuracy: {final_train_acc:.4f}\")\n        print(f\"    Validation accuracy: {final_val_acc:.4f}\")\n    print()\n", "", "import os\nimport numpy as np\n\n# Define working directory and load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each ablation and dataset, printing the final metrics\nfor ablation, datasets in experiment_data.items():\n    for name, data in datasets.items():\n        train_acc = data[\"metrics\"][\"train\"][-1]\n        val_loss = data[\"losses\"][\"val\"][-1]\n        val_align = data[\"alignments\"][\"val\"][-1]\n        mai = data[\"mai\"][-1]\n\n        print(f\"Dataset: {name} ({ablation})\")\n        print(f\"train accuracy: {train_acc:.4f}\")\n        print(f\"validation loss: {val_loss:.4f}\")\n        print(f\"validation bidirectional alignment: {val_align:.4f}\")\n        print(f\"MAI: {mai:.4f}\\n\")\n", "import os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over ablation methods and datasets\nfor ablation_name, datasets in experiment_data.items():\n    print(f\"Ablation method: {ablation_name}\")\n    for dataset_name, results in datasets.items():\n        # Extract final and best metrics\n        train_loss_final = results[\"losses\"][\"train\"][-1]\n        val_loss_best = min(results[\"losses\"][\"val\"])\n        train_align_final = results[\"alignments\"][\"train\"][-1]\n        val_align_final = results[\"alignments\"][\"val\"][-1]\n        mai_final = results[\"mai\"][-1]\n        # Compute test accuracy and F1 from saved predictions\n        preds = results[\"predictions\"]\n        gts = results[\"ground_truth\"]\n        test_acc = accuracy_score(gts, preds)\n        test_f1 = f1_score(gts, preds, average=\"macro\")\n\n        # Print dataset name and all metrics with clear labels\n        print(f\"Dataset: {dataset_name}\")\n        print(f\"  Training loss: {train_loss_final:.4f}\")\n        print(f\"  Validation loss: {val_loss_best:.4f}\")\n        print(f\"  Training alignment: {train_align_final:.4f}\")\n        print(f\"  Validation alignment: {val_align_final:.4f}\")\n        print(f\"  MAI: {mai_final:.4f}\")\n        print(f\"  Test accuracy: {test_acc:.4f}\")\n        print(f\"  Test F1 score: {test_f1:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Extract the ablation results\nablation_results = experiment_data.get(\"mlp_dropout_rate_ablation\", {})\n\n# Iterate through each dropout setting and dataset\nfor drop_key, datasets in ablation_results.items():\n    dropout_rate = drop_key.replace(\"drop_\", \"\")\n    for dataset_name, store in datasets.items():\n        # Final epoch metrics\n        final_train_loss = store[\"losses\"][\"train\"][-1]\n        final_validation_loss = store[\"losses\"][\"val\"][-1]\n        final_train_alignment = store[\"alignments\"][\"train\"][-1]\n        final_validation_alignment = store[\"alignments\"][\"val\"][-1]\n        final_mai = store[\"mai\"][-1]\n        # Compute test accuracy\n        preds = store[\"predictions\"]\n        gts = store[\"ground_truth\"]\n        test_accuracy = (preds == gts).sum() / len(gts)\n\n        # Print results\n        print(f\"Dataset: {dataset_name} (dropout rate: {dropout_rate})\")\n        print(f\"  training loss: {final_train_loss:.4f}\")\n        print(f\"  validation loss: {final_validation_loss:.4f}\")\n        print(f\"  training alignment: {final_train_alignment:.4f}\")\n        print(f\"  validation alignment: {final_validation_alignment:.4f}\")\n        print(f\"  MAI: {final_mai:.4f}\")\n        print(f\"  test accuracy: {test_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\ndata_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Determine dataset names from the first activation\ndataset_names = next(iter(experiment_data.values())).keys()\n\n# Loop over each dataset\nfor dataset_name in dataset_names:\n    print(f\"Dataset: {dataset_name}\")\n    # Loop over each activation function\n    for act_name, ds_dict in experiment_data.items():\n        metrics = ds_dict[dataset_name]\n        final_training_loss = metrics[\"losses\"][\"train\"][-1]\n        final_training_alignment = metrics[\"alignments\"][\"train\"][-1]\n        final_validation_loss = metrics[\"losses\"][\"val\"][-1]\n        final_validation_alignment = metrics[\"alignments\"][\"val\"][-1]\n        final_mai = metrics[\"mai\"][-1]\n\n        print(f\"  Activation: {act_name}\")\n        print(f\"    training loss: {final_training_loss:.4f}\")\n        print(f\"    training alignment: {final_training_alignment:.4f}\")\n        print(f\"    validation loss: {final_validation_loss:.4f}\")\n        print(f\"    validation alignment: {final_validation_alignment:.4f}\")\n        print(f\"    MAI: {final_mai:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each lambda setting and each dataset\nfor lam_key, datasets in experiment_data.items():\n    # Recover the numeric lambda value from the key\n    lam_value = lam_key.split(\"_\", 1)[1].replace(\"_\", \".\")\n    for dataset_name, results in datasets.items():\n        # Print dataset name (with lambda) before metrics\n        print(f\"Dataset: {dataset_name} (lambda={lam_value})\")\n\n        # Extract final (last epoch) values for each metric\n        train_acc = results[\"metrics\"][\"train\"][-1]\n        val_acc = results[\"metrics\"][\"val\"][-1]\n        train_loss = results[\"losses\"][\"train\"][-1]\n        val_loss = results[\"losses\"][\"val\"][-1]\n        train_align = results[\"alignments\"][\"train\"][-1]\n        val_align = results[\"alignments\"][\"val\"][-1]\n        mai = results[\"mai\"][-1]\n\n        # Print each metric with a clear label\n        print(f\"train accuracy: {train_acc:.4f}\")\n        print(f\"validation accuracy: {val_acc:.4f}\")\n        print(f\"train loss: {train_loss:.4f}\")\n        print(f\"validation loss: {val_loss:.4f}\")\n        print(f\"train alignment: {train_align:.4f}\")\n        print(f\"validation alignment: {val_align:.4f}\")\n        print(f\"MAI: {mai:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Locate and load the experiment data file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through ablations and datasets, printing final metrics\nfor ablation_name, datasets in experiment_data.items():\n    for dataset_name, results in datasets.items():\n        print(f\"Dataset: {dataset_name} (Ablation: {ablation_name})\")\n        # Extract final epoch metrics\n        final_train_loss = results[\"losses\"][\"train\"][-1]\n        final_validation_loss = results[\"losses\"][\"val\"][-1]\n        final_train_alignment = results[\"metrics\"][\"train\"][-1]\n        final_validation_alignment = results[\"metrics\"][\"val\"][-1]\n        # Print with clear metric names\n        print(f\"  train loss: {final_train_loss:.4f}\")\n        print(f\"  validation loss: {final_validation_loss:.4f}\")\n        print(f\"  train bidirectional alignment: {final_train_alignment:.4f}\")\n        print(f\"  validation bidirectional alignment: {final_validation_alignment:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# 0. Locate working directory and load saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# 1. Determine dataset names from one of the ablation entries\ndatasets = list(next(iter(experiment_data.values())).keys())\n\n# 2. Iterate over datasets and extract final train/validation MAI for each gamma\nfor dataset_name in datasets:\n    print(f\"Dataset: {dataset_name}\")\n    for ablation_key, ds_results in experiment_data.items():\n        # Extract gamma value from key\n        gamma = ablation_key.split(\"_\")[-1]\n        # Final MAI values are the last entries in the metrics lists\n        train_mai_final = ds_results[dataset_name][\"metrics\"][\"train\"][-1]\n        val_mai_final = ds_results[dataset_name][\"metrics\"][\"val\"][-1]\n        # 3. Print settings and metrics with clear labels\n        print(f\"Focal loss gamma={gamma}\")\n        print(f\"train MAI: {train_mai_final:.4f}\")\n        print(f\"validation MAI: {val_mai_final:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# 1. Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# 2. Iterate over datasets and print final metrics for each config\ndataset_names = next(iter(experiment_data.values())).keys()\nfor dataset in dataset_names:\n    print(f\"Dataset: {dataset}\")\n    # for each ablation type (random, importance)\n    for ablation_type, ds_dict in experiment_data.items():\n        data = ds_dict[dataset]\n        head_counts = data[\"head_counts\"]\n        train_losses = data[\"losses\"][\"train\"]\n        val_losses = data[\"losses\"][\"val\"]\n        train_aligns = data[\"alignments\"][\"train\"]\n        val_aligns = data[\"alignments\"][\"val\"]\n        mai_values = data[\"mai\"]\n        # get each unique head count and print its final\u2010epoch metrics\n        for hc in sorted(set(head_counts)):\n            # find the last index where this head count was used\n            idxs = [i for i, h in enumerate(head_counts) if h == hc]\n            idx = idxs[-1]\n            print(f\"  Configuration: ablation_type={ablation_type}, head_count={hc}\")\n            print(f\"    final train loss: {train_losses[idx]:.4f}\")\n            print(f\"    final validation loss: {val_losses[idx]:.4f}\")\n            print(f\"    final train alignment: {train_aligns[idx]:.4f}\")\n            print(f\"    final validation alignment: {val_aligns[idx]:.4f}\")\n            print(f\"    final validation MAI: {mai_values[idx]:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each ablation type and dataset\nfor ablation, datasets in experiment_data.items():\n    for dataset_name, metrics in datasets.items():\n        # Extract stored metrics\n        train_losses = metrics[\"losses\"][\"train\"]\n        val_losses = metrics[\"losses\"][\"val\"]\n        train_aligns = metrics[\"alignments\"][\"train\"]\n        val_aligns = metrics[\"alignments\"][\"val\"]\n        mai_values = metrics[\"mai\"]\n        predictions = metrics[\"predictions\"]\n        ground_truth = metrics[\"ground_truth\"]\n\n        # Compute final metrics\n        final_train_loss = train_losses[-1]\n        final_train_align = train_aligns[-1]\n        final_val_loss = val_losses[-1]\n        final_val_align = val_aligns[-1]\n        final_mai = mai_values[-1]\n        test_accuracy = np.mean(predictions == ground_truth)\n\n        # Print results\n        print(f\"Dataset: {dataset_name} (Ablation: {ablation})\")\n        print(f\"train loss: {final_train_loss:.4f}\")\n        print(f\"training alignment: {final_train_align:.4f}\")\n        print(f\"validation loss: {final_val_loss:.4f}\")\n        print(f\"validation alignment: {final_val_align:.4f}\")\n        print(f\"validation MAI: {final_mai:.4f}\")\n        print(f\"test accuracy: {test_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each ablation setting and dataset\nfor ablation_key, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        # Extract final epoch metrics\n        train_acc = data[\"metrics\"][\"train\"][-1]\n        val_acc = data[\"metrics\"][\"val\"][-1]\n        train_loss = data[\"losses\"][\"train\"][-1]\n        val_loss = data[\"losses\"][\"val\"][-1]\n        train_align = data[\"alignments\"][\"train\"][-1]\n        val_align = data[\"alignments\"][\"val\"][-1]\n        val_mai = data[\"mai\"][-1]\n\n        # Print dataset header and metrics\n        print(f\"Dataset: {dataset_name} (ablation: {ablation_key})\")\n        print(f\"train accuracy: {train_acc:.4f}\")\n        print(f\"validation accuracy: {val_acc:.4f}\")\n        print(f\"train loss: {train_loss:.4f}\")\n        print(f\"validation loss: {val_loss:.4f}\")\n        print(f\"train alignment: {train_align:.4f}\")\n        print(f\"validation alignment: {val_align:.4f}\")\n        print(f\"validation MAI: {val_mai:.4f}\\n\")\n", "", "import os\nimport numpy as np\n\n# Determine the working directory and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each optimizer and dataset, printing the final metrics\nfor optimizer_name, datasets in experiment_data.items():\n    for dataset_name, metrics in datasets.items():\n        # Extract final epoch values for each metric\n        train_loss = metrics[\"losses\"][\"train\"][-1]\n        train_alignment = metrics[\"alignments\"][\"train\"][-1]\n        train_accuracy = metrics[\"accuracy\"][\"train\"][-1]\n        val_loss = metrics[\"losses\"][\"val\"][-1]\n        val_alignment = metrics[\"alignments\"][\"val\"][-1]\n        val_accuracy = metrics[\"accuracy\"][\"val\"][-1]\n        val_mai = metrics[\"mai\"][-1]\n\n        # Print dataset and optimizer context, followed by labeled metrics\n        print(f\"Dataset: {dataset_name} (Optimizer: {optimizer_name})\")\n        print(f\"  train loss: {train_loss:.4f}\")\n        print(f\"  train alignment: {train_alignment:.4f}\")\n        print(f\"  train accuracy: {train_accuracy:.4f}\")\n        print(f\"  validation loss: {val_loss:.4f}\")\n        print(f\"  validation alignment: {val_alignment:.4f}\")\n        print(f\"  validation accuracy: {val_accuracy:.4f}\")\n        print(f\"  validation MAI: {val_mai:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through ablation settings and datasets\nfor ablation, datasets in experiment_data.items():\n    for dataset_name, metrics in datasets.items():\n        print(f\"Dataset: {dataset_name} (Ablation: {ablation})\")\n\n        # Extract final epoch metrics\n        train_losses = metrics[\"losses\"][\"train\"]\n        train_aligns = metrics[\"alignments\"][\"train\"]\n        val_losses = metrics[\"losses\"][\"val\"]\n        val_aligns = metrics[\"alignments\"][\"val\"]\n        mais = metrics[\"mai\"]\n\n        final_train_loss = train_losses[-1]\n        final_train_alignment = train_aligns[-1]\n        final_validation_loss = val_losses[-1]\n        final_validation_alignment = val_aligns[-1]\n        final_mai = mais[-1]\n\n        # Compute test accuracy\n        preds = metrics[\"predictions\"]\n        gts = metrics[\"ground_truth\"]\n        test_accuracy = (preds == gts).mean()\n\n        # Print clearly labeled metrics\n        print(f\"Final training loss: {final_train_loss:.4f}\")\n        print(f\"Final training alignment: {final_train_alignment:.4f}\")\n        print(f\"Final validation loss: {final_validation_loss:.4f}\")\n        print(f\"Final validation alignment: {final_validation_alignment:.4f}\")\n        print(f\"Final MAI: {final_mai:.4f}\")\n        print(f\"Test accuracy: {test_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Define working directory and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each ablation and dataset to print final metrics\nfor ablation, datasets in experiment_data.items():\n    for dataset_name, exp in datasets.items():\n        # Extract final epoch values\n        final_train_loss = exp[\"losses\"][\"train\"][-1]\n        final_validation_loss = exp[\"losses\"][\"val\"][-1]\n        final_train_alignment = exp[\"metrics\"][\"train\"][-1]\n        final_validation_alignment = exp[\"metrics\"][\"val\"][-1]\n        final_mai = exp[\"mai\"][-1]\n\n        # Compute validation accuracy from stored predictions\n        preds = exp[\"predictions\"]\n        ground_truth = exp[\"ground_truth\"]\n        validation_accuracy = np.mean(preds == ground_truth)\n\n        # Print metrics with clear labels\n        print(f\"Dataset: {dataset_name} (Ablation: {ablation})\")\n        print(f\"Final training loss: {final_train_loss:.4f}\")\n        print(f\"Final validation loss: {final_validation_loss:.4f}\")\n        print(f\"Final training alignment score: {final_train_alignment:.4f}\")\n        print(f\"Final validation alignment score: {final_validation_alignment:.4f}\")\n        print(f\"Final MAI score: {final_mai:.4f}\")\n        print(f\"Validation accuracy: {validation_accuracy:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each temperature and dataset\nfor temp_key, datasets in experiment_data.items():\n    # Extract numeric temperature for clarity\n    temp_value = temp_key.split(\"_\")[-1]\n    print(f\"Softmax Temperature: {temp_value}\")\n    for dataset_name, record in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        # Final epoch metrics\n        train_loss = record[\"losses\"][\"train\"][-1]\n        val_loss = record[\"losses\"][\"val\"][-1]\n        train_alignment = record[\"alignments\"][\"train\"][-1]\n        val_alignment = record[\"alignments\"][\"val\"][-1]\n        train_accuracy = record[\"accuracy\"][\"train\"][-1]\n        val_accuracy = record[\"accuracy\"][\"val\"][-1]\n        val_mai = record[\"mai\"][-1]\n\n        # Print metrics with clear labels\n        print(f\"train loss: {train_loss:.4f}\")\n        print(f\"validation loss: {val_loss:.4f}\")\n        print(f\"train alignment: {train_alignment:.4f}\")\n        print(f\"validation alignment: {val_alignment:.4f}\")\n        print(f\"train accuracy: {train_accuracy:.4f}\")\n        print(f\"validation accuracy: {val_accuracy:.4f}\")\n        print(f\"validation MAI: {val_mai:.4f}\")\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each temperature and dataset\nfor temp_key, datasets in experiment_data.items():\n    # Extract numeric temperature for clarity\n    temp_value = temp_key.split(\"_\")[-1]\n    print(f\"Softmax Temperature: {temp_value}\")\n    for dataset_name, record in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        # Final epoch metrics\n        train_loss = record[\"losses\"][\"train\"][-1]\n        val_loss = record[\"losses\"][\"val\"][-1]\n        train_alignment = record[\"alignments\"][\"train\"][-1]\n        val_alignment = record[\"alignments\"][\"val\"][-1]\n        train_accuracy = record[\"accuracy\"][\"train\"][-1]\n        val_accuracy = record[\"accuracy\"][\"val\"][-1]\n        val_mai = record[\"mai\"][-1]\n\n        # Print metrics with clear labels\n        print(f\"train loss: {train_loss:.4f}\")\n        print(f\"validation loss: {val_loss:.4f}\")\n        print(f\"train alignment: {train_alignment:.4f}\")\n        print(f\"validation alignment: {val_alignment:.4f}\")\n        print(f\"train accuracy: {train_accuracy:.4f}\")\n        print(f\"validation accuracy: {val_accuracy:.4f}\")\n        print(f\"validation MAI: {val_mai:.4f}\")\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each temperature and dataset\nfor temp_key, datasets in experiment_data.items():\n    # Extract numeric temperature for clarity\n    temp_value = temp_key.split(\"_\")[-1]\n    print(f\"Softmax Temperature: {temp_value}\")\n    for dataset_name, record in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        # Final epoch metrics\n        train_loss = record[\"losses\"][\"train\"][-1]\n        val_loss = record[\"losses\"][\"val\"][-1]\n        train_alignment = record[\"alignments\"][\"train\"][-1]\n        val_alignment = record[\"alignments\"][\"val\"][-1]\n        train_accuracy = record[\"accuracy\"][\"train\"][-1]\n        val_accuracy = record[\"accuracy\"][\"val\"][-1]\n        val_mai = record[\"mai\"][-1]\n\n        # Print metrics with clear labels\n        print(f\"train loss: {train_loss:.4f}\")\n        print(f\"validation loss: {val_loss:.4f}\")\n        print(f\"train alignment: {train_alignment:.4f}\")\n        print(f\"validation alignment: {val_alignment:.4f}\")\n        print(f\"train accuracy: {train_accuracy:.4f}\")\n        print(f\"validation accuracy: {val_accuracy:.4f}\")\n        print(f\"validation MAI: {val_mai:.4f}\")\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each temperature and dataset\nfor temp_key, datasets in experiment_data.items():\n    # Extract numeric temperature for clarity\n    temp_value = temp_key.split(\"_\")[-1]\n    print(f\"Softmax Temperature: {temp_value}\")\n    for dataset_name, record in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        # Final epoch metrics\n        train_loss = record[\"losses\"][\"train\"][-1]\n        val_loss = record[\"losses\"][\"val\"][-1]\n        train_alignment = record[\"alignments\"][\"train\"][-1]\n        val_alignment = record[\"alignments\"][\"val\"][-1]\n        train_accuracy = record[\"accuracy\"][\"train\"][-1]\n        val_accuracy = record[\"accuracy\"][\"val\"][-1]\n        val_mai = record[\"mai\"][-1]\n\n        # Print metrics with clear labels\n        print(f\"train loss: {train_loss:.4f}\")\n        print(f\"validation loss: {val_loss:.4f}\")\n        print(f\"train alignment: {train_alignment:.4f}\")\n        print(f\"validation alignment: {val_alignment:.4f}\")\n        print(f\"train accuracy: {train_accuracy:.4f}\")\n        print(f\"validation accuracy: {val_accuracy:.4f}\")\n        print(f\"validation MAI: {val_mai:.4f}\")\n        print()  # blank line between datasets\n", ""], "parse_term_out": ["['Synthetic dataset:', '\\n', '  Learning rate 1.0e-04:', '\\n', '    Final\ntraining alignment: 0.9929', '\\n', '    Final validation alignment: 0.9929',\n'\\n', '    Final training loss: 1.0162', '\\n', '    Final validation loss:\n1.0106', '\\n', '    Test accuracy: 0.6000', '\\n', '  Learning rate 5.0e-04:',\n'\\n', '    Final training alignment: 0.9921', '\\n', '    Final validation\nalignment: 0.9915', '\\n', '    Final training loss: 0.7943', '\\n', '    Final\nvalidation loss: 0.7817', '\\n', '    Test accuracy: 0.8350', '\\n', '  Learning\nrate 1.0e-03:', '\\n', '    Final training alignment: 0.9917', '\\n', '    Final\nvalidation alignment: 0.9907', '\\n', '    Final training loss: 0.5274', '\\n', '\nFinal validation loss: 0.5101', '\\n', '    Test accuracy: 0.9050', '\\n', '\nLearning rate 5.0e-03:', '\\n', '    Final training alignment: 0.9988', '\\n', '\nFinal validation alignment: 0.9989', '\\n', '    Final training loss: 0.0982',\n'\\n', '    Final validation loss: 0.1039', '\\n', '    Test accuracy: 0.9600',\n'\\n', '  Learning rate 1.0e-02:', '\\n', '    Final training alignment: 0.9991',\n'\\n', '    Final validation alignment: 0.9986', '\\n', '    Final training loss:\n0.0588', '\\n', '    Final validation loss: 0.0739', '\\n', '    Test accuracy:\n0.9650', '\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Dataset: ag_news (baseline)', '\\n', 'train accuracy: 0.8780', '\\n',\n'validation accuracy: 0.8920', '\\n', 'training loss: 0.3672', '\\n', 'validation\nloss: 0.3472', '\\n', 'training alignment: 0.9993', '\\n', 'validation alignment:\n0.9995', '\\n', 'mutual AI-user alignment index (MAI): 0.9427\\n', '\\n', 'Dataset:\nyelp_polarity (baseline)', '\\n', 'train accuracy: 0.8635', '\\n', 'validation\naccuracy: 0.8260', '\\n', 'training loss: 0.3435', '\\n', 'validation loss:\n0.3789', '\\n', 'training alignment: 0.9999', '\\n', 'validation alignment:\n0.9999', '\\n', 'mutual AI-user alignment index (MAI): 0.9047\\n', '\\n', 'Dataset:\ndbpedia_14 (baseline)', '\\n', 'train accuracy: 0.9785', '\\n', 'validation\naccuracy: 0.9740', '\\n', 'training loss: 0.2103', '\\n', 'validation loss:\n0.1728', '\\n', 'training alignment: 0.9971', '\\n', 'validation alignment:\n0.9978', '\\n', 'mutual AI-user alignment index (MAI): 0.9858\\n', '\\n', 'Dataset:\nag_news (fine_tune)', '\\n', 'train accuracy: 0.2680', '\\n', 'validation\naccuracy: 0.2500', '\\n', 'training loss: 1.4203', '\\n', 'validation loss:\n1.4178', '\\n', 'training alignment: 0.9938', '\\n', 'validation alignment:\n0.9986', '\\n', 'mutual AI-user alignment index (MAI): 0.3999\\n', '\\n', 'Dataset:\nyelp_polarity (fine_tune)', '\\n', 'train accuracy: 0.5055', '\\n', 'validation\naccuracy: 0.5480', '\\n', 'training loss: 0.7246', '\\n', 'validation loss:\n0.6905', '\\n', 'training alignment: 0.9868', '\\n', 'validation alignment:\n0.9999', '\\n', 'mutual AI-user alignment index (MAI): 0.7080\\n', '\\n', 'Dataset:\ndbpedia_14 (fine_tune)', '\\n', 'train accuracy: 0.1340', '\\n', 'validation\naccuracy: 0.1220', '\\n', 'training loss: 2.5521', '\\n', 'validation loss:\n2.3812', '\\n', 'training alignment: 0.9871', '\\n', 'validation alignment:\n0.9935', '\\n', 'mutual AI-user alignment index (MAI): 0.2173\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: ag_news', '\\n', '  Ablation: baseline', '\\n', '    Train accuracy:\n0.8780', '\\n', '    Validation accuracy: 0.8920', '\\n', '  Ablation:\nlinear_probe', '\\n', '    Train accuracy: 0.8730', '\\n', '    Validation\naccuracy: 0.8820', '\\n', '\\n', 'Dataset: yelp_polarity', '\\n', '  Ablation:\nbaseline', '\\n', '    Train accuracy: 0.8635', '\\n', '    Validation accuracy:\n0.8260', '\\n', '  Ablation: linear_probe', '\\n', '    Train accuracy: 0.8385',\n'\\n', '    Validation accuracy: 0.8340', '\\n', '\\n', 'Dataset: dbpedia_14',\n'\\n', '  Ablation: baseline', '\\n', '    Train accuracy: 0.9785', '\\n', '\nValidation accuracy: 0.9740', '\\n', '  Ablation: linear_probe', '\\n', '    Train\naccuracy: 0.9685', '\\n', '    Validation accuracy: 0.9680', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "", "['Dataset: ag_news (baseline)', '\\n', 'train accuracy: 0.8780', '\\n',\n'validation loss: 0.3472', '\\n', 'validation bidirectional alignment: 0.9995',\n'\\n', 'MAI: 0.9427\\n', '\\n', 'Dataset: yelp_polarity (baseline)', '\\n', 'train\naccuracy: 0.8635', '\\n', 'validation loss: 0.3789', '\\n', 'validation\nbidirectional alignment: 0.9999', '\\n', 'MAI: 0.9047\\n', '\\n', 'Dataset:\ndbpedia_14 (baseline)', '\\n', 'train accuracy: 0.9785', '\\n', 'validation loss:\n0.1728', '\\n', 'validation bidirectional alignment: 0.9978', '\\n', 'MAI:\n0.9858\\n', '\\n', 'Dataset: ag_news (fine_tune)', '\\n', 'train accuracy: 0.9405',\n'\\n', 'validation loss: 0.3631', '\\n', 'validation bidirectional alignment:\n0.9998', '\\n', 'MAI: 0.9372\\n', '\\n', 'Dataset: yelp_polarity (fine_tune)',\n'\\n', 'train accuracy: 0.9425', '\\n', 'validation loss: 0.3324', '\\n',\n'validation bidirectional alignment: 0.9998', '\\n', 'MAI: 0.9281\\n', '\\n',\n'Dataset: dbpedia_14 (fine_tune)', '\\n', 'train accuracy: 0.9955', '\\n',\n'validation loss: 0.0765', '\\n', 'validation bidirectional alignment: 0.9997',\n'\\n', 'MAI: 0.9928\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 3, in <module>\\n\nfrom sklearn.metrics import accuracy_score, f1_score\\nModuleNotFoundError: No\nmodule named \\'sklearn\\'\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Dataset: ag_news (dropout rate: 0.0)', '\\n', '  training loss: 0.3672', '\\n',\n'  validation loss: 0.3472', '\\n', '  training alignment: 0.9993', '\\n', '\nvalidation alignment: 0.9995', '\\n', '  MAI: 0.9427', '\\n', '  test accuracy:\n0.8920\\n', '\\n', 'Dataset: yelp_polarity (dropout rate: 0.0)', '\\n', '  training\nloss: 0.3435', '\\n', '  validation loss: 0.3789', '\\n', '  training alignment:\n0.9999', '\\n', '  validation alignment: 0.9999', '\\n', '  MAI: 0.9047', '\\n', '\ntest accuracy: 0.8240\\n', '\\n', 'Dataset: dbpedia_14 (dropout rate: 0.0)', '\\n',\n'  training loss: 0.2103', '\\n', '  validation loss: 0.1728', '\\n', '  training\nalignment: 0.9971', '\\n', '  validation alignment: 0.9978', '\\n', '  MAI:\n0.9858', '\\n', '  test accuracy: 0.9720\\n', '\\n', 'Dataset: ag_news (dropout\nrate: 0.1)', '\\n', '  training loss: 0.3793', '\\n', '  validation loss: 0.3426',\n'\\n', '  training alignment: 0.9884', '\\n', '  validation alignment: 0.9995',\n'\\n', '  MAI: 0.9416', '\\n', '  test accuracy: 0.8880\\n', '\\n', 'Dataset:\nyelp_polarity (dropout rate: 0.1)', '\\n', '  training loss: 0.3623', '\\n', '\nvalidation loss: 0.3469', '\\n', '  training alignment: 0.9883', '\\n', '\nvalidation alignment: 0.9998', '\\n', '  MAI: 0.9246', '\\n', '  test accuracy:\n0.8540\\n', '\\n', 'Dataset: dbpedia_14 (dropout rate: 0.1)', '\\n', '  training\nloss: 0.2447', '\\n', '  validation loss: 0.1857', '\\n', '  training alignment:\n0.9819', '\\n', '  validation alignment: 0.9972', '\\n', '  MAI: 0.9865', '\\n', '\ntest accuracy: 0.9760\\n', '\\n', 'Dataset: ag_news (dropout rate: 0.3)', '\\n', '\ntraining loss: 0.4064', '\\n', '  validation loss: 0.3356', '\\n', '  training\nalignment: 0.9743', '\\n', '  validation alignment: 0.9987', '\\n', '  MAI:\n0.9390', '\\n', '  test accuracy: 0.8940\\n', '\\n', 'Dataset: yelp_polarity\n(dropout rate: 0.3)', '\\n', '  training loss: 0.3470', '\\n', '  validation loss:\n0.3402', '\\n', '  training alignment: 0.9792', '\\n', '  validation alignment:\n0.9958', '\\n', '  MAI: 0.9253', '\\n', '  test accuracy: 0.8640\\n', '\\n',\n'Dataset: dbpedia_14 (dropout rate: 0.3)', '\\n', '  training loss: 0.3411',\n'\\n', '  validation loss: 0.2077', '\\n', '  training alignment: 0.9577', '\\n', '\nvalidation alignment: 0.9956', '\\n', '  MAI: 0.9827', '\\n', '  test accuracy:\n0.9740\\n', '\\n', 'Dataset: ag_news (dropout rate: 0.5)', '\\n', '  training loss:\n0.4706', '\\n', '  validation loss: 0.3501', '\\n', '  training alignment:\n0.9528', '\\n', '  validation alignment: 0.9974', '\\n', '  MAI: 0.9361', '\\n', '\ntest accuracy: 0.8800\\n', '\\n', 'Dataset: yelp_polarity (dropout rate: 0.5)',\n'\\n', '  training loss: 0.4231', '\\n', '  validation loss: 0.3614', '\\n', '\ntraining alignment: 0.9761', '\\n', '  validation alignment: 0.9983', '\\n', '\nMAI: 0.9298', '\\n', '  test accuracy: 0.8540\\n', '\\n', 'Dataset: dbpedia_14\n(dropout rate: 0.5)', '\\n', '  training loss: 0.5765', '\\n', '  validation loss:\n0.3048', '\\n', '  training alignment: 0.9086', '\\n', '  validation alignment:\n0.9929', '\\n', '  MAI: 0.9813', '\\n', '  test accuracy: 0.9640\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: ag_news', '\\n', '  Activation: ReLU', '\\n', '    training loss:\n0.3672', '\\n', '    training alignment: 0.9993', '\\n', '    validation loss:\n0.3472', '\\n', '    validation alignment: 0.9995', '\\n', '    MAI: 0.9427',\n'\\n', '  Activation: GELU', '\\n', '    training loss: 0.3362', '\\n', '\ntraining alignment: 0.9995', '\\n', '    validation loss: 0.3461', '\\n', '\nvalidation alignment: 0.9996', '\\n', '    MAI: 0.9438', '\\n', '  Activation:\nTanh', '\\n', '    training loss: 0.3781', '\\n', '    training alignment:\n0.9991', '\\n', '    validation loss: 0.3440', '\\n', '    validation alignment:\n0.9993', '\\n', '    MAI: 0.9437', '\\n', '  Activation: LeakyReLU', '\\n', '\ntraining loss: 0.3546', '\\n', '    training alignment: 0.9996', '\\n', '\nvalidation loss: 0.3372', '\\n', '    validation alignment: 0.9996', '\\n', '\nMAI: 0.9405', '\\n', '\\n', 'Dataset: yelp_polarity', '\\n', '  Activation: ReLU',\n'\\n', '    training loss: 0.3435', '\\n', '    training alignment: 0.9999', '\\n',\n'    validation loss: 0.3789', '\\n', '    validation alignment: 0.9999', '\\n', '\nMAI: 0.9047', '\\n', '  Activation: GELU', '\\n', '    training loss: 0.3175',\n'\\n', '    training alignment: 0.9995', '\\n', '    validation loss: 0.3511',\n'\\n', '    validation alignment: 0.9998', '\\n', '    MAI: 0.9212', '\\n', '\nActivation: Tanh', '\\n', '    training loss: 0.3344', '\\n', '    training\nalignment: 0.9989', '\\n', '    validation loss: 0.3478', '\\n', '    validation\nalignment: 0.9991', '\\n', '    MAI: 0.9209', '\\n', '  Activation: LeakyReLU',\n'\\n', '    training loss: 0.3379', '\\n', '    training alignment: 0.9981', '\\n',\n'    validation loss: 0.3485', '\\n', '    validation alignment: 0.9992', '\\n', '\nMAI: 0.9221', '\\n', '\\n', 'Dataset: dbpedia_14', '\\n', '  Activation: ReLU',\n'\\n', '    training loss: 0.2103', '\\n', '    training alignment: 0.9971', '\\n',\n'    validation loss: 0.1728', '\\n', '    validation alignment: 0.9978', '\\n', '\nMAI: 0.9858', '\\n', '  Activation: GELU', '\\n', '    training loss: 0.1243',\n'\\n', '    training alignment: 0.9982', '\\n', '    validation loss: 0.1281',\n'\\n', '    validation alignment: 0.9982', '\\n', '    MAI: 0.9870', '\\n', '\nActivation: Tanh', '\\n', '    training loss: 0.2813', '\\n', '    training\nalignment: 0.9944', '\\n', '    validation loss: 0.2170', '\\n', '    validation\nalignment: 0.9954', '\\n', '    MAI: 0.9866', '\\n', '  Activation: LeakyReLU',\n'\\n', '    training loss: 0.2080', '\\n', '    training alignment: 0.9960', '\\n',\n'    validation loss: 0.1719', '\\n', '    validation alignment: 0.9969', '\\n', '\nMAI: 0.9843', '\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Dataset: ag_news (lambda=0.0)', '\\n', 'train accuracy: 0.8780', '\\n',\n'validation accuracy: 0.8920', '\\n', 'train loss: 0.3672', '\\n', 'validation\nloss: 0.3472', '\\n', 'train alignment: 0.9993', '\\n', 'validation alignment:\n0.9995', '\\n', 'MAI: 0.9427\\n', '\\n', 'Dataset: yelp_polarity (lambda=0.0)',\n'\\n', 'train accuracy: 0.8635', '\\n', 'validation accuracy: 0.8260', '\\n',\n'train loss: 0.3435', '\\n', 'validation loss: 0.3789', '\\n', 'train alignment:\n0.9999', '\\n', 'validation alignment: 0.9999', '\\n', 'MAI: 0.9047\\n', '\\n',\n'Dataset: dbpedia_14 (lambda=0.0)', '\\n', 'train accuracy: 0.9785', '\\n',\n'validation accuracy: 0.9740', '\\n', 'train loss: 0.2103', '\\n', 'validation\nloss: 0.1728', '\\n', 'train alignment: 0.9971', '\\n', 'validation alignment:\n0.9978', '\\n', 'MAI: 0.9858\\n', '\\n', 'Dataset: ag_news (lambda=0.1)', '\\n',\n'train accuracy: 0.8780', '\\n', 'validation accuracy: 0.8920', '\\n', 'train\nloss: 0.3674', '\\n', 'validation loss: 0.3472', '\\n', 'train alignment: 0.9993',\n'\\n', 'validation alignment: 0.9995', '\\n', 'MAI: 0.9427\\n', '\\n', 'Dataset:\nyelp_polarity (lambda=0.1)', '\\n', 'train accuracy: 0.8625', '\\n', 'validation\naccuracy: 0.8280', '\\n', 'train loss: 0.3441', '\\n', 'validation loss: 0.3790',\n'\\n', 'train alignment: 0.9999', '\\n', 'validation alignment: 0.9999', '\\n',\n'MAI: 0.9059\\n', '\\n', 'Dataset: dbpedia_14 (lambda=0.1)', '\\n', 'train\naccuracy: 0.9785', '\\n', 'validation accuracy: 0.9740', '\\n', 'train loss:\n0.2100', '\\n', 'validation loss: 0.1726', '\\n', 'train alignment: 0.9972', '\\n',\n'validation alignment: 0.9979', '\\n', 'MAI: 0.9858\\n', '\\n', 'Dataset: ag_news\n(lambda=0.5)', '\\n', 'train accuracy: 0.8795', '\\n', 'validation accuracy:\n0.8920', '\\n', 'train loss: 0.3682', '\\n', 'validation loss: 0.3472', '\\n',\n'train alignment: 0.9994', '\\n', 'validation alignment: 0.9996', '\\n', 'MAI:\n0.9427\\n', '\\n', 'Dataset: yelp_polarity (lambda=0.5)', '\\n', 'train accuracy:\n0.8630', '\\n', 'validation accuracy: 0.8300', '\\n', 'train loss: 0.3442', '\\n',\n'validation loss: 0.3764', '\\n', 'train alignment: 0.9999', '\\n', 'validation\nalignment: 0.9999', '\\n', 'MAI: 0.9071\\n', '\\n', 'Dataset: dbpedia_14\n(lambda=0.5)', '\\n', 'train accuracy: 0.9785', '\\n', 'validation accuracy:\n0.9740', '\\n', 'train loss: 0.2083', '\\n', 'validation loss: 0.1715', '\\n',\n'train alignment: 0.9976', '\\n', 'validation alignment: 0.9982', '\\n', 'MAI:\n0.9860\\n', '\\n', 'Dataset: ag_news (lambda=1.0)', '\\n', 'train accuracy:\n0.8805', '\\n', 'validation accuracy: 0.8940', '\\n', 'train loss: 0.3705', '\\n',\n'validation loss: 0.3478', '\\n', 'train alignment: 0.9994', '\\n', 'validation\nalignment: 0.9994', '\\n', 'MAI: 0.9438\\n', '\\n', 'Dataset: yelp_polarity\n(lambda=1.0)', '\\n', 'train accuracy: 0.8610', '\\n', 'validation accuracy:\n0.8360', '\\n', 'train loss: 0.3378', '\\n', 'validation loss: 0.3697', '\\n',\n'train alignment: 0.9997', '\\n', 'validation alignment: 0.9998', '\\n', 'MAI:\n0.9106\\n', '\\n', 'Dataset: dbpedia_14 (lambda=1.0)', '\\n', 'train accuracy:\n0.9785', '\\n', 'validation accuracy: 0.9740', '\\n', 'train loss: 0.2076', '\\n',\n'validation loss: 0.1709', '\\n', 'train alignment: 0.9980', '\\n', 'validation\nalignment: 0.9985', '\\n', 'MAI: 0.9861\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: ag_news (Ablation: layer_1)', '\\n', '  train loss: 1.3184', '\\n', '\nvalidation loss: 1.2714', '\\n', '  train bidirectional alignment: 0.7367', '\\n',\n'  validation bidirectional alignment: 0.7436', '\\n', '\\n', 'Dataset:\nyelp_polarity (Ablation: layer_1)', '\\n', '  train loss: 0.6832', '\\n', '\nvalidation loss: 0.6818', '\\n', '  train bidirectional alignment: 0.8470', '\\n',\n'  validation bidirectional alignment: 0.8461', '\\n', '\\n', 'Dataset: dbpedia_14\n(Ablation: layer_1)', '\\n', '  train loss: 2.5287', '\\n', '  validation loss:\n2.4574', '\\n', '  train bidirectional alignment: 0.6037', '\\n', '  validation\nbidirectional alignment: 0.6088', '\\n', '\\n', 'Dataset: ag_news (Ablation:\nlayer_3)', '\\n', '  train loss: 0.4974', '\\n', '  validation loss: 0.4550',\n'\\n', '  train bidirectional alignment: 0.8982', '\\n', '  validation\nbidirectional alignment: 0.9078', '\\n', '\\n', 'Dataset: yelp_polarity (Ablation:\nlayer_3)', '\\n', '  train loss: 0.5621', '\\n', '  validation loss: 0.5485',\n'\\n', '  train bidirectional alignment: 0.8742', '\\n', '  validation\nbidirectional alignment: 0.8773', '\\n', '\\n', 'Dataset: dbpedia_14 (Ablation:\nlayer_3)', '\\n', '  train loss: 0.6924', '\\n', '  validation loss: 0.6152',\n'\\n', '  train bidirectional alignment: 0.8492', '\\n', '  validation\nbidirectional alignment: 0.8674', '\\n', '\\n', 'Dataset: ag_news (Ablation:\nlayer_5)', '\\n', '  train loss: 0.3225', '\\n', '  validation loss: 0.3390',\n'\\n', '  train bidirectional alignment: 0.9394', '\\n', '  validation\nbidirectional alignment: 0.9405', '\\n', '\\n', 'Dataset: yelp_polarity (Ablation:\nlayer_5)', '\\n', '  train loss: 0.4084', '\\n', '  validation loss: 0.4565',\n'\\n', '  train bidirectional alignment: 0.9112', '\\n', '  validation\nbidirectional alignment: 0.9054', '\\n', '\\n', 'Dataset: dbpedia_14 (Ablation:\nlayer_5)', '\\n', '  train loss: 0.3142', '\\n', '  validation loss: 0.2946',\n'\\n', '  train bidirectional alignment: 0.9310', '\\n', '  validation\nbidirectional alignment: 0.9383', '\\n', '\\n', 'Dataset: ag_news (Ablation:\navg_last2)', '\\n', '  train loss: 0.3340', '\\n', '  validation loss: 0.3276',\n'\\n', '  train bidirectional alignment: 0.9368', '\\n', '  validation\nbidirectional alignment: 0.9399', '\\n', '\\n', 'Dataset: yelp_polarity (Ablation:\navg_last2)', '\\n', '  train loss: 0.3959', '\\n', '  validation loss: 0.4247',\n'\\n', '  train bidirectional alignment: 0.9122', '\\n', '  validation\nbidirectional alignment: 0.9079', '\\n', '\\n', 'Dataset: dbpedia_14 (Ablation:\navg_last2)', '\\n', '  train loss: 0.2983', '\\n', '  validation loss: 0.2518',\n'\\n', '  train bidirectional alignment: 0.9324', '\\n', '  validation\nbidirectional alignment: 0.9426', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Dataset: ag_news', '\\n', 'Focal loss gamma=1', '\\n', 'train MAI: 0.9368',\n'\\n', 'validation MAI: 0.9448', '\\n', 'Focal loss gamma=2', '\\n', 'train MAI:\n0.9387', '\\n', 'validation MAI: 0.9405', '\\n', 'Focal loss gamma=5', '\\n',\n'train MAI: 0.9321', '\\n', 'validation MAI: 0.9458', '\\n', '\\n', 'Dataset:\nyelp_polarity', '\\n', 'Focal loss gamma=1', '\\n', 'train MAI: 0.9267', '\\n',\n'validation MAI: 0.9010', '\\n', 'Focal loss gamma=2', '\\n', 'train MAI: 0.9235',\n'\\n', 'validation MAI: 0.9058', '\\n', 'Focal loss gamma=5', '\\n', 'train MAI:\n0.9214', '\\n', 'validation MAI: 0.9223', '\\n', '\\n', 'Dataset: dbpedia_14',\n'\\n', 'Focal loss gamma=1', '\\n', 'train MAI: 0.9874', '\\n', 'validation MAI:\n0.9854', '\\n', 'Focal loss gamma=2', '\\n', 'train MAI: 0.9849', '\\n',\n'validation MAI: 0.9844', '\\n', 'Focal loss gamma=5', '\\n', 'train MAI: 0.9862',\n'\\n', 'validation MAI: 0.9838', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Dataset: ag_news', '\\n', '  Configuration: ablation_type=random,\nhead_count=2', '\\n', '    final train loss: 0.9384', '\\n', '    final validation\nloss: 0.8388', '\\n', '    final train alignment: 0.9874', '\\n', '    final\nvalidation alignment: 0.9834', '\\n', '    final validation MAI: 0.8624', '\\n', '\nConfiguration: ablation_type=random, head_count=4', '\\n', '    final train loss:\n0.4505', '\\n', '    final validation loss: 0.4145', '\\n', '    final train\nalignment: 0.9995', '\\n', '    final validation alignment: 0.9997', '\\n', '\nfinal validation MAI: 0.9257', '\\n', '  Configuration: ablation_type=random,\nhead_count=8', '\\n', '    final train loss: 0.4025', '\\n', '    final validation\nloss: 0.3749', '\\n', '    final train alignment: 0.9994', '\\n', '    final\nvalidation alignment: 0.9995', '\\n', '    final validation MAI: 0.9348', '\\n', '\nConfiguration: ablation_type=random, head_count=12', '\\n', '    final train\nloss: 0.3672', '\\n', '    final validation loss: 0.3472', '\\n', '    final train\nalignment: 0.9993', '\\n', '    final validation alignment: 0.9995', '\\n', '\nfinal validation MAI: 0.9427', '\\n', '  Configuration: ablation_type=importance,\nhead_count=2', '\\n', '    final train loss: 0.7638', '\\n', '    final validation\nloss: 0.6726', '\\n', '    final train alignment: 0.9889', '\\n', '    final\nvalidation alignment: 0.9903', '\\n', '    final validation MAI: 0.9078', '\\n', '\nConfiguration: ablation_type=importance, head_count=4', '\\n', '    final train\nloss: 0.3959', '\\n', '    final validation loss: 0.4165', '\\n', '    final train\nalignment: 0.9996', '\\n', '    final validation alignment: 0.9997', '\\n', '\nfinal validation MAI: 0.9200', '\\n', '  Configuration: ablation_type=importance,\nhead_count=8', '\\n', '    final train loss: 0.3733', '\\n', '    final validation\nloss: 0.3456', '\\n', '    final train alignment: 0.9989', '\\n', '    final\nvalidation alignment: 0.9991', '\\n', '    final validation MAI: 0.9380', '\\n', '\nConfiguration: ablation_type=importance, head_count=12', '\\n', '    final train\nloss: 0.3635', '\\n', '    final validation loss: 0.3453', '\\n', '    final train\nalignment: 0.9992', '\\n', '    final validation alignment: 0.9994', '\\n', '\nfinal validation MAI: 0.9416', '\\n', '\\n', 'Dataset: yelp_polarity', '\\n', '\nConfiguration: ablation_type=random, head_count=2', '\\n', '    final train loss:\n0.6865', '\\n', '    final validation loss: 0.6924', '\\n', '    final train\nalignment: 0.9998', '\\n', '    final validation alignment: 0.9998', '\\n', '\nfinal validation MAI: 0.6541', '\\n', '  Configuration: ablation_type=random,\nhead_count=4', '\\n', '    final train loss: 0.4338', '\\n', '    final validation\nloss: 0.4375', '\\n', '    final train alignment: 0.9971', '\\n', '    final\nvalidation alignment: 0.9980', '\\n', '    final validation MAI: 0.8744', '\\n', '\nConfiguration: ablation_type=random, head_count=8', '\\n', '    final train loss:\n0.4258', '\\n', '    final validation loss: 0.4189', '\\n', '    final train\nalignment: 0.9991', '\\n', '    final validation alignment: 0.9987', '\\n', '\nfinal validation MAI: 0.9078', '\\n', '  Configuration: ablation_type=random,\nhead_count=12', '\\n', '    final train loss: 0.3435', '\\n', '    final\nvalidation loss: 0.3789', '\\n', '    final train alignment: 0.9999', '\\n', '\nfinal validation alignment: 0.9999', '\\n', '    final validation MAI: 0.9047',\n'\\n', '  Configuration: ablation_type=importance, head_count=2', '\\n', '\nfinal train loss: 0.6374', '\\n', '    final validation loss: 0.6216', '\\n', '\nfinal train alignment: 0.9998', '\\n', '    final validation alignment: 0.9999',\n'\\n', '    final validation MAI: 0.8221', '\\n', '  Configuration:\nablation_type=importance, head_count=4', '\\n', '    final train loss: 0.5305',\n'\\n', '    final validation loss: 0.5182', '\\n', '    final train alignment:\n0.9998', '\\n', '    final validation alignment: 0.9999', '\\n', '    final\nvalidation MAI: 0.8662', '\\n', '  Configuration: ablation_type=importance,\nhead_count=8', '\\n', '    final train loss: 0.3837', '\\n', '    final validation\nloss: 0.3776', '\\n', '    final train alignment: 0.9999', '\\n', '    final\nvalidation alignment: 0.9999', '\\n', '    final validation MAI: 0.9224', '\\n', '\nConfiguration: ablation_type=importance, head_count=12', '\\n', '    final train\nloss: 0.3563', '\\n', '    final validation loss: 0.3458', '\\n', '    final train\nalignment: 0.9997', '\\n', '    final validation alignment: 0.9997', '\\n', '\nfinal validation MAI: 0.9258', '\\n', '\\n', 'Dataset: dbpedia_14', '\\n', '\nConfiguration: ablation_type=random, head_count=2', '\\n', '    final train loss:\n2.3269', '\\n', '    final validation loss: 2.2051', '\\n', '    final train\nalignment: 0.9964', '\\n', '    final validation alignment: 0.9934', '\\n', '\nfinal validation MAI: 0.7419', '\\n', '  Configuration: ablation_type=random,\nhead_count=4', '\\n', '    final train loss: 0.8115', '\\n', '    final validation\nloss: 0.6665', '\\n', '    final train alignment: 0.9909', '\\n', '    final\nvalidation alignment: 0.9923', '\\n', '    final validation MAI: 0.9271', '\\n', '\nConfiguration: ablation_type=random, head_count=8', '\\n', '    final train loss:\n0.4434', '\\n', '    final validation loss: 0.3700', '\\n', '    final train\nalignment: 0.9943', '\\n', '    final validation alignment: 0.9955', '\\n', '\nfinal validation MAI: 0.9508', '\\n', '  Configuration: ablation_type=random,\nhead_count=12', '\\n', '    final train loss: 0.2103', '\\n', '    final\nvalidation loss: 0.1728', '\\n', '    final train alignment: 0.9971', '\\n', '\nfinal validation alignment: 0.9978', '\\n', '    final validation MAI: 0.9858',\n'\\n', '  Configuration: ablation_type=importance, head_count=2', '\\n', '\nfinal train loss: 1.3085', '\\n', '    final validation loss: 1.0876', '\\n', '\nfinal train alignment: 0.9895', '\\n', '    final validation alignment: 0.9894',\n'\\n', '    final validation MAI: 0.8648', '\\n', '  Configuration:\nablation_type=importance, head_count=4', '\\n', '    final train loss: 0.6135',\n'\\n', '    final validation loss: 0.4764', '\\n', '    final train alignment:\n0.9920', '\\n', '    final validation alignment: 0.9940', '\\n', '    final\nvalidation MAI: 0.9501', '\\n', '  Configuration: ablation_type=importance,\nhead_count=8', '\\n', '    final train loss: 0.3551', '\\n', '    final validation\nloss: 0.2765', '\\n', '    final train alignment: 0.9962', '\\n', '    final\nvalidation alignment: 0.9970', '\\n', '    final validation MAI: 0.9750', '\\n', '\nConfiguration: ablation_type=importance, head_count=12', '\\n', '    final train\nloss: 0.2235', '\\n', '    final validation loss: 0.1858', '\\n', '    final train\nalignment: 0.9960', '\\n', '    final validation alignment: 0.9968', '\\n', '\nfinal validation MAI: 0.9843', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Dataset: ag_news (Ablation: pretrained)', '\\n', 'train loss: 0.3672', '\\n',\n'training alignment: 0.9993', '\\n', 'validation loss: 0.3472', '\\n', 'validation\nalignment: 0.9995', '\\n', 'validation MAI: 0.9427', '\\n', 'test accuracy:\n0.8920\\n', '\\n', 'Dataset: yelp_polarity (Ablation: pretrained)', '\\n', 'train\nloss: 0.3435', '\\n', 'training alignment: 0.9999', '\\n', 'validation loss:\n0.3789', '\\n', 'validation alignment: 0.9999', '\\n', 'validation MAI: 0.9047',\n'\\n', 'test accuracy: 0.8240\\n', '\\n', 'Dataset: dbpedia_14 (Ablation:\npretrained)', '\\n', 'train loss: 0.2103', '\\n', 'training alignment: 0.9971',\n'\\n', 'validation loss: 0.1728', '\\n', 'validation alignment: 0.9978', '\\n',\n'validation MAI: 0.9858', '\\n', 'test accuracy: 0.9720\\n', '\\n', 'Dataset:\nag_news (Ablation: random_init)', '\\n', 'train loss: 1.3930', '\\n', 'training\nalignment: 0.9973', '\\n', 'validation loss: 1.4047', '\\n', 'validation\nalignment: 0.9993', '\\n', 'validation MAI: 0.3896', '\\n', 'test accuracy:\n0.2420\\n', '\\n', 'Dataset: yelp_polarity (Ablation: random_init)', '\\n', 'train\nloss: 0.6890', '\\n', 'training alignment: 0.9991', '\\n', 'validation loss:\n0.6791', '\\n', 'validation alignment: 0.9996', '\\n', 'validation MAI: 0.7146',\n'\\n', 'test accuracy: 0.5420\\n', '\\n', 'Dataset: dbpedia_14 (Ablation:\nrandom_init)', '\\n', 'train loss: 2.6121', '\\n', 'training alignment: 0.9889',\n'\\n', 'validation loss: 2.5436', '\\n', 'validation alignment: 0.9857', '\\n',\n'validation MAI: 0.3044', '\\n', 'test accuracy: 0.1480\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: ag_news (ablation: token_dropout_0)', '\\n', 'train accuracy: 0.8780',\n'\\n', 'validation accuracy: 0.8920', '\\n', 'train loss: 0.3672', '\\n',\n'validation loss: 0.3472', '\\n', 'train alignment: 0.9993', '\\n', 'validation\nalignment: 0.9995', '\\n', 'validation MAI: 0.9427\\n', '\\n', 'Dataset:\nyelp_polarity (ablation: token_dropout_0)', '\\n', 'train accuracy: 0.8635',\n'\\n', 'validation accuracy: 0.8260', '\\n', 'train loss: 0.3435', '\\n',\n'validation loss: 0.3789', '\\n', 'train alignment: 0.9999', '\\n', 'validation\nalignment: 0.9999', '\\n', 'validation MAI: 0.9047\\n', '\\n', 'Dataset: dbpedia_14\n(ablation: token_dropout_0)', '\\n', 'train accuracy: 0.9785', '\\n', 'validation\naccuracy: 0.9740', '\\n', 'train loss: 0.2103', '\\n', 'validation loss: 0.1728',\n'\\n', 'train alignment: 0.9971', '\\n', 'validation alignment: 0.9978', '\\n',\n'validation MAI: 0.9858\\n', '\\n', 'Dataset: ag_news (ablation:\ntoken_dropout_10)', '\\n', 'train accuracy: 0.8775', '\\n', 'validation accuracy:\n0.8840', '\\n', 'train loss: 0.3797', '\\n', 'validation loss: 0.3660', '\\n',\n'train alignment: 0.9995', '\\n', 'validation alignment: 0.9996', '\\n',\n'validation MAI: 0.9383\\n', '\\n', 'Dataset: yelp_polarity (ablation:\ntoken_dropout_10)', '\\n', 'train accuracy: 0.8350', '\\n', 'validation accuracy:\n0.8440', '\\n', 'train loss: 0.3817', '\\n', 'validation loss: 0.3901', '\\n',\n'train alignment: 0.9994', '\\n', 'validation alignment: 0.9994', '\\n',\n'validation MAI: 0.9151\\n', '\\n', 'Dataset: dbpedia_14 (ablation:\ntoken_dropout_10)', '\\n', 'train accuracy: 0.9620', '\\n', 'validation accuracy:\n0.9700', '\\n', 'train loss: 0.3125', '\\n', 'validation loss: 0.2426', '\\n',\n'train alignment: 0.9959', '\\n', 'validation alignment: 0.9964', '\\n',\n'validation MAI: 0.9830\\n', '\\n', 'Dataset: ag_news (ablation:\ntoken_dropout_20)', '\\n', 'train accuracy: 0.8740', '\\n', 'validation accuracy:\n0.8760', '\\n', 'train loss: 0.3905', '\\n', 'validation loss: 0.3520', '\\n',\n'train alignment: 0.9992', '\\n', 'validation alignment: 0.9995', '\\n',\n'validation MAI: 0.9337\\n', '\\n', 'Dataset: yelp_polarity (ablation:\ntoken_dropout_20)', '\\n', 'train accuracy: 0.8260', '\\n', 'validation accuracy:\n0.8100', '\\n', 'train loss: 0.4024', '\\n', 'validation loss: 0.4120', '\\n',\n'train alignment: 0.9997', '\\n', 'validation alignment: 0.9997', '\\n',\n'validation MAI: 0.8949\\n', '\\n', 'Dataset: dbpedia_14 (ablation:\ntoken_dropout_20)', '\\n', 'train accuracy: 0.9590', '\\n', 'validation accuracy:\n0.9640', '\\n', 'train loss: 0.3137', '\\n', 'validation loss: 0.2362', '\\n',\n'train alignment: 0.9959', '\\n', 'validation alignment: 0.9969', '\\n',\n'validation MAI: 0.9802\\n', '\\n', 'Dataset: ag_news (ablation:\ntoken_dropout_30)', '\\n', 'train accuracy: 0.8700', '\\n', 'validation accuracy:\n0.8820', '\\n', 'train loss: 0.4055', '\\n', 'validation loss: 0.3617', '\\n',\n'train alignment: 0.9997', '\\n', 'validation alignment: 0.9997', '\\n',\n'validation MAI: 0.9372\\n', '\\n', 'Dataset: yelp_polarity (ablation:\ntoken_dropout_30)', '\\n', 'train accuracy: 0.8130', '\\n', 'validation accuracy:\n0.7920', '\\n', 'train loss: 0.4365', '\\n', 'validation loss: 0.4453', '\\n',\n'train alignment: 0.9998', '\\n', 'validation alignment: 0.9996', '\\n',\n'validation MAI: 0.8838\\n', '\\n', 'Dataset: dbpedia_14 (ablation:\ntoken_dropout_30)', '\\n', 'train accuracy: 0.9490', '\\n', 'validation accuracy:\n0.9400', '\\n', 'train loss: 0.3475', '\\n', 'validation loss: 0.2835', '\\n',\n'train alignment: 0.9961', '\\n', 'validation alignment: 0.9969', '\\n',\n'validation MAI: 0.9676\\n', '\\n', 'Execution time: a moment seconds (time limit\nis an hour).']", "", "['Dataset: ag_news (Optimizer: adam)', '\\n', '  train loss: 0.3672', '\\n', '\ntrain alignment: 0.9993', '\\n', '  train accuracy: 0.8780', '\\n', '  validation\nloss: 0.3472', '\\n', '  validation alignment: 0.9995', '\\n', '  validation\naccuracy: 0.8920', '\\n', '  validation MAI: 0.9427\\n', '\\n', 'Dataset:\nyelp_polarity (Optimizer: adam)', '\\n', '  train loss: 0.3435', '\\n', '  train\nalignment: 0.9999', '\\n', '  train accuracy: 0.8635', '\\n', '  validation loss:\n0.3789', '\\n', '  validation alignment: 0.9999', '\\n', '  validation accuracy:\n0.8260', '\\n', '  validation MAI: 0.9047\\n', '\\n', 'Dataset: dbpedia_14\n(Optimizer: adam)', '\\n', '  train loss: 0.2103', '\\n', '  train alignment:\n0.9971', '\\n', '  train accuracy: 0.9785', '\\n', '  validation loss: 0.1728',\n'\\n', '  validation alignment: 0.9978', '\\n', '  validation accuracy: 0.9740',\n'\\n', '  validation MAI: 0.9858\\n', '\\n', 'Dataset: ag_news (Optimizer: sgd)',\n'\\n', '  train loss: 1.1300', '\\n', '  train alignment: 0.9991', '\\n', '  train\naccuracy: 0.6290', '\\n', '  validation loss: 1.0416', '\\n', '  validation\nalignment: 0.9989', '\\n', '  validation accuracy: 0.7200', '\\n', '  validation\nMAI: 0.8368\\n', '\\n', 'Dataset: yelp_polarity (Optimizer: sgd)', '\\n', '  train\nloss: 0.6519', '\\n', '  train alignment: 0.9998', '\\n', '  train accuracy:\n0.6680', '\\n', '  validation loss: 0.6379', '\\n', '  validation alignment:\n0.9998', '\\n', '  validation accuracy: 0.8100', '\\n', '  validation MAI:\n0.8950\\n', '\\n', 'Dataset: dbpedia_14 (Optimizer: sgd)', '\\n', '  train loss:\n2.3955', '\\n', '  train alignment: 0.9953', '\\n', '  train accuracy: 0.3000',\n'\\n', '  validation loss: 2.3031', '\\n', '  validation alignment: 0.9957', '\\n',\n'  validation accuracy: 0.4380', '\\n', '  validation MAI: 0.6084\\n', '\\n',\n'Dataset: ag_news (Optimizer: adamw)', '\\n', '  train loss: 0.3577', '\\n', '\ntrain alignment: 0.9994', '\\n', '  train accuracy: 0.8800', '\\n', '  validation\nloss: 0.3395', '\\n', '  validation alignment: 0.9996', '\\n', '  validation\naccuracy: 0.9040', '\\n', '  validation MAI: 0.9494\\n', '\\n', 'Dataset:\nyelp_polarity (Optimizer: adamw)', '\\n', '  train loss: 0.3221', '\\n', '  train\nalignment: 0.9989', '\\n', '  train accuracy: 0.8590', '\\n', '  validation loss:\n0.3548', '\\n', '  validation alignment: 0.9987', '\\n', '  validation accuracy:\n0.8520', '\\n', '  validation MAI: 0.9195\\n', '\\n', 'Dataset: dbpedia_14\n(Optimizer: adamw)', '\\n', '  train loss: 0.2191', '\\n', '  train alignment:\n0.9957', '\\n', '  train accuracy: 0.9780', '\\n', '  validation loss: 0.1857',\n'\\n', '  validation alignment: 0.9965', '\\n', '  validation accuracy: 0.9740',\n'\\n', '  validation MAI: 0.9851\\n', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Dataset: ag_news (Ablation: independent)', '\\n', 'Final training loss:\n0.3672', '\\n', 'Final training alignment: 0.9993', '\\n', 'Final validation loss:\n0.3472', '\\n', 'Final validation alignment: 0.9995', '\\n', 'Final MAI: 0.9427',\n'\\n', 'Test accuracy: 0.8920\\n', '\\n', 'Dataset: yelp_polarity (Ablation:\nindependent)', '\\n', 'Final training loss: 0.3435', '\\n', 'Final training\nalignment: 0.9999', '\\n', 'Final validation loss: 0.3789', '\\n', 'Final\nvalidation alignment: 0.9999', '\\n', 'Final MAI: 0.9047', '\\n', 'Test accuracy:\n0.8240\\n', '\\n', 'Dataset: dbpedia_14 (Ablation: independent)', '\\n', 'Final\ntraining loss: 0.2103', '\\n', 'Final training alignment: 0.9971', '\\n', 'Final\nvalidation loss: 0.1728', '\\n', 'Final validation alignment: 0.9978', '\\n',\n'Final MAI: 0.9858', '\\n', 'Test accuracy: 0.9720\\n', '\\n', 'Dataset: ag_news\n(Ablation: shared_fc1)', '\\n', 'Final training loss: 0.3825', '\\n', 'Final\ntraining alignment: 0.9987', '\\n', 'Final validation loss: 0.3568', '\\n', 'Final\nvalidation alignment: 0.9991', '\\n', 'Final MAI: 0.9403', '\\n', 'Test accuracy:\n0.8820\\n', '\\n', 'Dataset: yelp_polarity (Ablation: shared_fc1)', '\\n', 'Final\ntraining loss: 0.3648', '\\n', 'Final training alignment: 0.9996', '\\n', 'Final\nvalidation loss: 0.3651', '\\n', 'Final validation alignment: 0.9998', '\\n',\n'Final MAI: 0.9246', '\\n', 'Test accuracy: 0.8520\\n', '\\n', 'Dataset: dbpedia_14\n(Ablation: shared_fc1)', '\\n', 'Final training loss: 0.2793', '\\n', 'Final\ntraining alignment: 0.9965', '\\n', 'Final validation loss: 0.2299', '\\n', 'Final\nvalidation alignment: 0.9975', '\\n', 'Final MAI: 0.9835', '\\n', 'Test accuracy:\n0.9660\\n', '\\n', 'Dataset: ag_news (Ablation: shared_fc1_fc2)', '\\n', 'Final\ntraining loss: 0.3595', '\\n', 'Final training alignment: 1.0000', '\\n', 'Final\nvalidation loss: 0.3451', '\\n', 'Final validation alignment: 1.0000', '\\n',\n'Final MAI: 0.9418', '\\n', 'Test accuracy: 0.8900\\n', '\\n', 'Dataset:\nyelp_polarity (Ablation: shared_fc1_fc2)', '\\n', 'Final training loss: 0.3505',\n'\\n', 'Final training alignment: 1.0000', '\\n', 'Final validation loss: 0.3574',\n'\\n', 'Final validation alignment: 1.0000', '\\n', 'Final MAI: 0.9189', '\\n',\n'Test accuracy: 0.8500\\n', '\\n', 'Dataset: dbpedia_14 (Ablation:\nshared_fc1_fc2)', '\\n', 'Final training loss: 0.2265', '\\n', 'Final training\nalignment: 1.0000', '\\n', 'Final validation loss: 0.1876', '\\n', 'Final\nvalidation alignment: 1.0000', '\\n', 'Final MAI: 0.9858', '\\n', 'Test accuracy:\n0.9720\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: ag_news (Ablation: standard)', '\\n', 'Final training loss: 0.3672',\n'\\n', 'Final validation loss: 0.3472', '\\n', 'Final training alignment score:\n0.9993', '\\n', 'Final validation alignment score: 0.9995', '\\n', 'Final MAI\nscore: 0.9427', '\\n', 'Validation accuracy: 0.8920\\n', '\\n', 'Dataset:\nyelp_polarity (Ablation: standard)', '\\n', 'Final training loss: 0.3435', '\\n',\n'Final validation loss: 0.3789', '\\n', 'Final training alignment score: 0.9999',\n'\\n', 'Final validation alignment score: 0.9999', '\\n', 'Final MAI score:\n0.9047', '\\n', 'Validation accuracy: 0.8240\\n', '\\n', 'Dataset: dbpedia_14\n(Ablation: standard)', '\\n', 'Final training loss: 0.2103', '\\n', 'Final\nvalidation loss: 0.1728', '\\n', 'Final training alignment score: 0.9971', '\\n',\n'Final validation alignment score: 0.9978', '\\n', 'Final MAI score: 0.9858',\n'\\n', 'Validation accuracy: 0.9720\\n', '\\n', 'Dataset: ag_news (Ablation:\nshuffled)', '\\n', 'Final training loss: 0.4161', '\\n', 'Final validation loss:\n0.4008', '\\n', 'Final training alignment score: 0.9995', '\\n', 'Final validation\nalignment score: 0.9997', '\\n', 'Final MAI score: 0.9326', '\\n', 'Validation\naccuracy: 0.8740\\n', '\\n', 'Dataset: yelp_polarity (Ablation: shuffled)', '\\n',\n'Final training loss: 0.5037', '\\n', 'Final validation loss: 0.5340', '\\n',\n'Final training alignment score: 0.9988', '\\n', 'Final validation alignment\nscore: 0.9995', '\\n', 'Final MAI score: 0.8357', '\\n', 'Validation accuracy:\n0.7160\\n', '\\n', 'Dataset: dbpedia_14 (Ablation: shuffled)', '\\n', 'Final\ntraining loss: 0.5639', '\\n', 'Final validation loss: 0.4422', '\\n', 'Final\ntraining alignment score: 0.9947', '\\n', 'Final validation alignment score:\n0.9960', '\\n', 'Final MAI score: 0.9367', '\\n', 'Validation accuracy: 0.8980\\n',\n'\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Softmax Temperature: 0.5', '\\n', 'Dataset: ag_news', '\\n', 'train loss:\n0.3672', '\\n', 'validation loss: 0.3472', '\\n', 'train alignment: 0.9993', '\\n',\n'validation alignment: 0.9995', '\\n', 'train accuracy: 0.8780', '\\n',\n'validation accuracy: 0.8920', '\\n', 'validation MAI: 0.9427', '\\n', '\\n',\n'Dataset: yelp_polarity', '\\n', 'train loss: 0.3435', '\\n', 'validation loss:\n0.3789', '\\n', 'train alignment: 0.9998', '\\n', 'validation alignment: 0.9998',\n'\\n', 'train accuracy: 0.8635', '\\n', 'validation accuracy: 0.8260', '\\n',\n'validation MAI: 0.9046', '\\n', '\\n', 'Dataset: dbpedia_14', '\\n', 'train loss:\n0.2103', '\\n', 'validation loss: 0.1728', '\\n', 'train alignment: 0.9986', '\\n',\n'validation alignment: 0.9989', '\\n', 'train accuracy: 0.9785', '\\n',\n'validation accuracy: 0.9740', '\\n', 'validation MAI: 0.9863', '\\n', '\\n',\n'Softmax Temperature: 1.0', '\\n', 'Dataset: ag_news', '\\n', 'train loss:\n0.3630', '\\n', 'validation loss: 0.3498', '\\n', 'train alignment: 0.9996', '\\n',\n'validation alignment: 0.9998', '\\n', 'train accuracy: 0.8825', '\\n',\n'validation accuracy: 0.8920', '\\n', 'validation MAI: 0.9428', '\\n', '\\n',\n'Dataset: yelp_polarity', '\\n', 'train loss: 0.3477', '\\n', 'validation loss:\n0.3501', '\\n', 'train alignment: 0.9997', '\\n', 'validation alignment: 0.9995',\n'\\n', 'train accuracy: 0.8575', '\\n', 'validation accuracy: 0.8500', '\\n',\n'validation MAI: 0.9187', '\\n', '\\n', 'Dataset: dbpedia_14', '\\n', 'train loss:\n0.2082', '\\n', 'validation loss: 0.1803', '\\n', 'train alignment: 0.9968', '\\n',\n'validation alignment: 0.9971', '\\n', 'train accuracy: 0.9745', '\\n',\n'validation accuracy: 0.9760', '\\n', 'validation MAI: 0.9864', '\\n', '\\n',\n'Softmax Temperature: 2.0', '\\n', 'Dataset: ag_news', '\\n', 'train loss:\n0.3576', '\\n', 'validation loss: 0.3397', '\\n', 'train alignment: 0.9897', '\\n',\n'validation alignment: 0.9899', '\\n', 'train accuracy: 0.8805', '\\n',\n'validation accuracy: 0.9040', '\\n', 'validation MAI: 0.9450', '\\n', '\\n',\n'Dataset: yelp_polarity', '\\n', 'train loss: 0.3220', '\\n', 'validation loss:\n0.3548', '\\n', 'train alignment: 0.9869', '\\n', 'validation alignment: 0.9874',\n'\\n', 'train accuracy: 0.8590', '\\n', 'validation accuracy: 0.8520', '\\n',\n'validation MAI: 0.9147', '\\n', '\\n', 'Dataset: dbpedia_14', '\\n', 'train loss:\n0.2186', '\\n', 'validation loss: 0.1852', '\\n', 'train alignment: 0.9871', '\\n',\n'validation alignment: 0.9881', '\\n', 'train accuracy: 0.9780', '\\n',\n'validation accuracy: 0.9740', '\\n', 'validation MAI: 0.9810', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Softmax Temperature: 0.5', '\\n', 'Dataset: ag_news', '\\n', 'train loss:\n0.3672', '\\n', 'validation loss: 0.3472', '\\n', 'train alignment: 0.9993', '\\n',\n'validation alignment: 0.9995', '\\n', 'train accuracy: 0.8780', '\\n',\n'validation accuracy: 0.8920', '\\n', 'validation MAI: 0.9427', '\\n', '\\n',\n'Dataset: yelp_polarity', '\\n', 'train loss: 0.3435', '\\n', 'validation loss:\n0.3789', '\\n', 'train alignment: 0.9998', '\\n', 'validation alignment: 0.9998',\n'\\n', 'train accuracy: 0.8635', '\\n', 'validation accuracy: 0.8260', '\\n',\n'validation MAI: 0.9046', '\\n', '\\n', 'Dataset: dbpedia_14', '\\n', 'train loss:\n0.2103', '\\n', 'validation loss: 0.1728', '\\n', 'train alignment: 0.9986', '\\n',\n'validation alignment: 0.9989', '\\n', 'train accuracy: 0.9785', '\\n',\n'validation accuracy: 0.9740', '\\n', 'validation MAI: 0.9863', '\\n', '\\n',\n'Softmax Temperature: 1.0', '\\n', 'Dataset: ag_news', '\\n', 'train loss:\n0.3630', '\\n', 'validation loss: 0.3498', '\\n', 'train alignment: 0.9996', '\\n',\n'validation alignment: 0.9998', '\\n', 'train accuracy: 0.8825', '\\n',\n'validation accuracy: 0.8920', '\\n', 'validation MAI: 0.9428', '\\n', '\\n',\n'Dataset: yelp_polarity', '\\n', 'train loss: 0.3477', '\\n', 'validation loss:\n0.3501', '\\n', 'train alignment: 0.9997', '\\n', 'validation alignment: 0.9995',\n'\\n', 'train accuracy: 0.8575', '\\n', 'validation accuracy: 0.8500', '\\n',\n'validation MAI: 0.9187', '\\n', '\\n', 'Dataset: dbpedia_14', '\\n', 'train loss:\n0.2082', '\\n', 'validation loss: 0.1803', '\\n', 'train alignment: 0.9968', '\\n',\n'validation alignment: 0.9971', '\\n', 'train accuracy: 0.9745', '\\n',\n'validation accuracy: 0.9760', '\\n', 'validation MAI: 0.9864', '\\n', '\\n',\n'Softmax Temperature: 2.0', '\\n', 'Dataset: ag_news', '\\n', 'train loss:\n0.3576', '\\n', 'validation loss: 0.3397', '\\n', 'train alignment: 0.9897', '\\n',\n'validation alignment: 0.9899', '\\n', 'train accuracy: 0.8805', '\\n',\n'validation accuracy: 0.9040', '\\n', 'validation MAI: 0.9450', '\\n', '\\n',\n'Dataset: yelp_polarity', '\\n', 'train loss: 0.3220', '\\n', 'validation loss:\n0.3548', '\\n', 'train alignment: 0.9869', '\\n', 'validation alignment: 0.9874',\n'\\n', 'train accuracy: 0.8590', '\\n', 'validation accuracy: 0.8520', '\\n',\n'validation MAI: 0.9147', '\\n', '\\n', 'Dataset: dbpedia_14', '\\n', 'train loss:\n0.2186', '\\n', 'validation loss: 0.1852', '\\n', 'train alignment: 0.9871', '\\n',\n'validation alignment: 0.9881', '\\n', 'train accuracy: 0.9780', '\\n',\n'validation accuracy: 0.9740', '\\n', 'validation MAI: 0.9810', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Softmax Temperature: 0.5', '\\n', 'Dataset: ag_news', '\\n', 'train loss:\n0.3672', '\\n', 'validation loss: 0.3472', '\\n', 'train alignment: 0.9993', '\\n',\n'validation alignment: 0.9995', '\\n', 'train accuracy: 0.8780', '\\n',\n'validation accuracy: 0.8920', '\\n', 'validation MAI: 0.9427', '\\n', '\\n',\n'Dataset: yelp_polarity', '\\n', 'train loss: 0.3435', '\\n', 'validation loss:\n0.3789', '\\n', 'train alignment: 0.9998', '\\n', 'validation alignment: 0.9998',\n'\\n', 'train accuracy: 0.8635', '\\n', 'validation accuracy: 0.8260', '\\n',\n'validation MAI: 0.9046', '\\n', '\\n', 'Dataset: dbpedia_14', '\\n', 'train loss:\n0.2103', '\\n', 'validation loss: 0.1728', '\\n', 'train alignment: 0.9986', '\\n',\n'validation alignment: 0.9989', '\\n', 'train accuracy: 0.9785', '\\n',\n'validation accuracy: 0.9740', '\\n', 'validation MAI: 0.9863', '\\n', '\\n',\n'Softmax Temperature: 1.0', '\\n', 'Dataset: ag_news', '\\n', 'train loss:\n0.3630', '\\n', 'validation loss: 0.3498', '\\n', 'train alignment: 0.9996', '\\n',\n'validation alignment: 0.9998', '\\n', 'train accuracy: 0.8825', '\\n',\n'validation accuracy: 0.8920', '\\n', 'validation MAI: 0.9428', '\\n', '\\n',\n'Dataset: yelp_polarity', '\\n', 'train loss: 0.3477', '\\n', 'validation loss:\n0.3501', '\\n', 'train alignment: 0.9997', '\\n', 'validation alignment: 0.9995',\n'\\n', 'train accuracy: 0.8575', '\\n', 'validation accuracy: 0.8500', '\\n',\n'validation MAI: 0.9187', '\\n', '\\n', 'Dataset: dbpedia_14', '\\n', 'train loss:\n0.2082', '\\n', 'validation loss: 0.1803', '\\n', 'train alignment: 0.9968', '\\n',\n'validation alignment: 0.9971', '\\n', 'train accuracy: 0.9745', '\\n',\n'validation accuracy: 0.9760', '\\n', 'validation MAI: 0.9864', '\\n', '\\n',\n'Softmax Temperature: 2.0', '\\n', 'Dataset: ag_news', '\\n', 'train loss:\n0.3576', '\\n', 'validation loss: 0.3397', '\\n', 'train alignment: 0.9897', '\\n',\n'validation alignment: 0.9899', '\\n', 'train accuracy: 0.8805', '\\n',\n'validation accuracy: 0.9040', '\\n', 'validation MAI: 0.9450', '\\n', '\\n',\n'Dataset: yelp_polarity', '\\n', 'train loss: 0.3220', '\\n', 'validation loss:\n0.3548', '\\n', 'train alignment: 0.9869', '\\n', 'validation alignment: 0.9874',\n'\\n', 'train accuracy: 0.8590', '\\n', 'validation accuracy: 0.8520', '\\n',\n'validation MAI: 0.9147', '\\n', '\\n', 'Dataset: dbpedia_14', '\\n', 'train loss:\n0.2186', '\\n', 'validation loss: 0.1852', '\\n', 'train alignment: 0.9871', '\\n',\n'validation alignment: 0.9881', '\\n', 'train accuracy: 0.9780', '\\n',\n'validation accuracy: 0.9740', '\\n', 'validation MAI: 0.9810', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Softmax Temperature: 0.5', '\\n', 'Dataset: ag_news', '\\n', 'train loss:\n0.3672', '\\n', 'validation loss: 0.3472', '\\n', 'train alignment: 0.9993', '\\n',\n'validation alignment: 0.9995', '\\n', 'train accuracy: 0.8780', '\\n',\n'validation accuracy: 0.8920', '\\n', 'validation MAI: 0.9427', '\\n', '\\n',\n'Dataset: yelp_polarity', '\\n', 'train loss: 0.3435', '\\n', 'validation loss:\n0.3789', '\\n', 'train alignment: 0.9998', '\\n', 'validation alignment: 0.9998',\n'\\n', 'train accuracy: 0.8635', '\\n', 'validation accuracy: 0.8260', '\\n',\n'validation MAI: 0.9046', '\\n', '\\n', 'Dataset: dbpedia_14', '\\n', 'train loss:\n0.2103', '\\n', 'validation loss: 0.1728', '\\n', 'train alignment: 0.9986', '\\n',\n'validation alignment: 0.9989', '\\n', 'train accuracy: 0.9785', '\\n',\n'validation accuracy: 0.9740', '\\n', 'validation MAI: 0.9863', '\\n', '\\n',\n'Softmax Temperature: 1.0', '\\n', 'Dataset: ag_news', '\\n', 'train loss:\n0.3630', '\\n', 'validation loss: 0.3498', '\\n', 'train alignment: 0.9996', '\\n',\n'validation alignment: 0.9998', '\\n', 'train accuracy: 0.8825', '\\n',\n'validation accuracy: 0.8920', '\\n', 'validation MAI: 0.9428', '\\n', '\\n',\n'Dataset: yelp_polarity', '\\n', 'train loss: 0.3477', '\\n', 'validation loss:\n0.3501', '\\n', 'train alignment: 0.9997', '\\n', 'validation alignment: 0.9995',\n'\\n', 'train accuracy: 0.8575', '\\n', 'validation accuracy: 0.8500', '\\n',\n'validation MAI: 0.9187', '\\n', '\\n', 'Dataset: dbpedia_14', '\\n', 'train loss:\n0.2082', '\\n', 'validation loss: 0.1803', '\\n', 'train alignment: 0.9968', '\\n',\n'validation alignment: 0.9971', '\\n', 'train accuracy: 0.9745', '\\n',\n'validation accuracy: 0.9760', '\\n', 'validation MAI: 0.9864', '\\n', '\\n',\n'Softmax Temperature: 2.0', '\\n', 'Dataset: ag_news', '\\n', 'train loss:\n0.3576', '\\n', 'validation loss: 0.3397', '\\n', 'train alignment: 0.9897', '\\n',\n'validation alignment: 0.9899', '\\n', 'train accuracy: 0.8805', '\\n',\n'validation accuracy: 0.9040', '\\n', 'validation MAI: 0.9450', '\\n', '\\n',\n'Dataset: yelp_polarity', '\\n', 'train loss: 0.3220', '\\n', 'validation loss:\n0.3548', '\\n', 'train alignment: 0.9869', '\\n', 'validation alignment: 0.9874',\n'\\n', 'train accuracy: 0.8590', '\\n', 'validation accuracy: 0.8520', '\\n',\n'validation MAI: 0.9147', '\\n', '\\n', 'Dataset: dbpedia_14', '\\n', 'train loss:\n0.2186', '\\n', 'validation loss: 0.1852', '\\n', 'train alignment: 0.9871', '\\n',\n'validation alignment: 0.9881', '\\n', 'train accuracy: 0.9780', '\\n',\n'validation accuracy: 0.9740', '\\n', 'validation MAI: 0.9810', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, "ModuleNotFoundError", null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, {"args": ["No module named 'sklearn'"], "name": "sklearn", "msg": "No module named 'sklearn'"}, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 3, "<module>", "from sklearn.metrics import accuracy_score, f1_score"]], null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"]}
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 4], [0, 7], [0, 6], [0, 11], [0, 9], [0, 5], [0, 1], [0, 12], [0, 3], [0, 10], [0, 2], [0, 8], [1, 13], [1, 16], [1, 15], [1, 14]], "layout": [[0.56, 0.0], [0.12, 0.5], [0.2, 0.5], [0.28, 0.5], [0.36, 0.5], [0.44, 0.5], [0.52, 0.5], [0.6, 0.5], [0.68, 0.5], [0.76, 0.5], [0.84, 0.5], [0.92, 0.5], [1.0, 0.5], [0.0, 1.0], [0.08, 1.0], [0.16, 1.0], [0.24, 1.0]], "plan": ["We generate a small synthetic classification dataset via a random linear mapping\nand normalize the inputs. Two simple MLPs represent the AI system and a proxy\nuser model, each mapping 10\u2010dimensional inputs to three class logits. We jointly\ntrain both networks on the ground\u2010truth labels using cross\u2010entropy loss. During\ntraining, we compute the Mutual Model Alignment Score (MMAS) as one minus the\nJensen\u2013Shannon divergence between their softmax outputs on each batch. We log\nthe average training and validation loss (for the AI model) along with MMAS at\neach epoch. After training, we collect the AI model\u2019s predictions on the\nvalidation split and save them together with the true labels. All metrics,\nlosses, predictions, ground truth, and epoch indices are stored in an\nexperiment_data dictionary. Finally, we save this dictionary as a numpy file\nunder the working directory for later analysis.", "Hyperparam tuning name: learning_rate. I\u2019ll sweep the Adam initial learning rate\nover a log\u2010uniform grid [1e-4, 5e-4, 1e-3, 5e-3, 1e-2], re\u2010initializing models\nat each setting for a fair comparison. For each learning rate I train both AI\nand user MLPs jointly over a fixed number of epochs, recording\ntraining/validation losses and alignments per epoch, and finally collecting\nvalidation predictions and ground truth. All plottable data (metrics, losses,\npredictions, ground truth) are organized under\n`experiment_data['learning_rate']['synthetic']` and saved via `np.save(...,\n'experiment_data.npy')`.", "Hyperparam tuning name: batch_size. The solution wraps the existing training and\nevaluation loops in a for\u2010loop over the candidate batch sizes [16, 32, 128,\n256].  For each batch size it reinitializes the models and optimizers, trains\nfor ten epochs while recording train/validation losses and alignment metrics per\nepoch, evaluates on the validation set to collect final predictions, and appends\nall of these into a nested `experiment_data` dict under the key `\"batch_size\" \u2192\n\"synthetic\"`.  We also store the list of batch sizes tested, the fixed epoch\nindices, and the ground\u2010truth labels once.  Finally, we save the full dictionary\nas `\"experiment_data.npy\"` in the working directory.", "Hyperparam tuning name: weight_decay. I will loop over a set of weight_decay\nvalues, retrain fresh models for each, collect epoch\u2010level train/val losses and\nalignment metrics, and record final validation predictions. All results\n(metrics, losses, predictions, ground truth, and the weight_decay values) are\nstored under experiment_data['weight_decay']['synthetic'], then converted to\nnumpy arrays and saved to \u201cexperiment_data.npy\u201d for easy plotting.", "Hyperparam tuning name: hidden_dim. We loop over candidate hidden_dim values,\ninstantiating fresh AI and user MLPs for each setting and training them for a\nfixed number of epochs while recording training/validation alignment (1-JSD) and\nloss per epoch. After each sweep we compute validation predictions and store\nthem alongside the ground-truth labels. All data\u2014hyperparameter list, epoch\nindices, metrics, losses, predictions, and ground truth\u2014is organized under a\nnested dictionary keyed by the tuning type and dataset. Finally, the entire\nstructure is saved via np.save to `experiment_data.npy`.", "Hyperparam tuning name: dropout_rate. We extend the MLP with a dropout layer\nafter the ReLU and loop over a set of dropout rates (0.0, 0.1, 0.2, 0.5). For\neach rate, we instantiate fresh ai and user models, train them for a fixed\nnumber of epochs, and record per-epoch train/val losses and alignment metrics.\nAfter training we also collect final validation predictions and ground truth,\nstoring everything under a single `experiment_data` dict keyed by \u201cdropout_rate\u201d\nand saved as `experiment_data.npy`.", "Hyperparam tuning name: label_smoothing. Here\u2019s a self\u2010contained script that\nsweeps over \u03b1\u2208[0.0,0.05,0.1,0.2] by rebuilding the models and loss each time\n(with `nn.CrossEntropyLoss(label_smoothing=\u03b1)`), resets the seeds before every\nrun for reproducibility, records training/validation losses and alignments per\nepoch, gathers final predictions and ground truth, and saves everything into\n`working/experiment_data.npy`:", "Hyperparam tuning name: adam_beta1. We perform a simple grid search over \u03b2\u2081\nvalues [0.5, 0.7, 0.9, 0.99], retraining the two-layer MLP for each setting\nwhile keeping \u03b2\u2082 fixed at 0.999. For each \u03b2\u2081 we reinitialize the models (with\nthe same random seed for fairness), train for 10 epochs, and record per-epoch\ntraining/validation losses and alignment metrics. After training, we compute\nfinal validation predictions and ground truth, storing all data under\n`experiment_data['adam_beta1']['synthetic']` including the list of \u03b2\u2081 values.\nFinally, we save the full `experiment_data` dictionary as a numpy file for later\nanalysis.", "Hyperparam tuning name: adam_beta2. Here\u2019s the plan: we loop over a list of \u03b2\u2082\nvalues, and for each we re-init separate AI and user models with Adam using that\n\u03b2\u2082; we train both, collect per-epoch train/validation losses and JS alignment\nmetrics, and finally record validation predictions. We aggregate these results\n(including the \u03b2\u2082 sweep and epoch indices) into a single `experiment_data` dict\nunder the key `\"adam_beta2\"\u2192\"synthetic\"`. At the end we save the entire dict as\n`experiment_data.npy` in the working directory.", "Hyperparam tuning name: lr_scheduler_gamma. I will loop over a set of decay\nfactors \u03b3 (0.1, 0.5, 0.9), attach a StepLR scheduler (step_size=5) to both the\nAI and user optimizers, and retrain separate MLP instances for each \u03b3. During\ntraining, I record per\u2010epoch training/validation losses and alignment metrics,\nthen at the end compute and store final validation predictions and ground truth.\nAll results are organized in a nested `experiment_data` dict under\n`'lr_scheduler_gamma' -> 'synthetic' -> 'gamma_<value>'`, and saved to\n`experiment_data.npy`. The code remains a single, self\u2010contained Python file.", "Hyperparam tuning name: max_grad_norm. We will run experiments over a set of\ngradient\u2010norm clipping thresholds, resetting model weights and seeds before each\nrun to ensure comparability. In each training loop, after calling\n`loss.backward()`, we apply `torch.nn.utils.clip_grad_norm_` with the current\n`max_grad_norm` before stepping the optimizer. For each threshold we collect\ntraining/validation losses and alignments per epoch, along with final validation\npredictions and ground truth, and store them under\n`experiment_data['max_grad_norm'][str(threshold)]['synthetic']`. Finally, all\nresults are saved in a single `experiment_data.npy` file.", "Hyperparam tuning name: init_std. We will loop over a grid of init_std values,\nand for each we will re\u2010initialize both MLPs\u2019 linear weights via a zero\u2010mean\nGaussian with that standard deviation (and zero biases), then train for a fixed\nnumber of epochs while logging train/val losses and alignment metrics. After\ntraining each setting, we\u2019ll run final validation predictions and gather ground\ntruths. All per\u2010epoch metrics, losses, predictions, and epochs will be stored\nunder a nested `experiment_data['init_std'][str(std)]` entry, converted to NumPy\narrays, and finally saved to `working/experiment_data.npy`. This single\u2010file\nscript is self\u2010contained and fully executable.", "Hyperparam tuning name: JSD_ALIGNMENT_WEIGHT. We\u2019ll wrap the original training\nloop inside a simple grid-search over \u03bb values, re\u2010initializing both MLPs for\neach setting.  At each batch we compute CrossEntropy for both models, then\nJensen\u2013Shannon divergence between their softmax outputs, form a combined loss =\nCE_ai + CE_user + \u03bb\u00b7mean(JSD), and backpropagate to update both networks via\ntheir Adam optimizers.  We record per\u2010epoch train/val loss and alignment\nmetrics, plus final val predictions and ground\u2010truth, storing everything in a\nnested dict under \u201cjsd_alignment_weight\u201d and then save it all with\nnp.save(\"experiment_data.npy\") at the end.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Reproducibility\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Synthetic data generation\nN_train, N_val, D, C = 1000, 200, 10, 3\nW_true = np.random.randn(D, C)\nx_train = np.random.randn(N_train, D)\nx_val = np.random.randn(N_val, D)\ny_train = np.argmax(x_train @ W_true + 0.1 * np.random.randn(N_train, C), axis=1)\ny_val = np.argmax(x_val @ W_true + 0.1 * np.random.randn(N_val, C), axis=1)\n# Normalize features\nmean = x_train.mean(axis=0)\nstd = x_train.std(axis=0) + 1e-8\nx_train = (x_train - mean) / std\nx_val = (x_val - mean) / std\n\n# Create DataLoaders\ntrain_dataset = TensorDataset(\n    torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n)\nval_dataset = TensorDataset(\n    torch.tensor(x_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long)\n)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64)\n\n\n# Define simple MLP\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\nai_model = MLP(D, 32, C).to(device)\nuser_model = MLP(D, 32, C).to(device)\noptimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\noptimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\nloss_fn = nn.CrossEntropyLoss()\n\n# Initialize experiment data\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"epochs\": [],\n    }\n}\n\nnum_epochs = 10\nfor epoch in range(1, num_epochs + 1):\n    ai_model.train()\n    user_model.train()\n    train_loss_sum = 0.0\n    train_jsd_sum = 0.0\n    train_samples = 0\n\n    for x_batch, y_batch in train_loader:\n        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n        # Forward\n        ai_logits = ai_model(x_batch)\n        user_logits = user_model(x_batch)\n        # Losses and updates\n        loss_ai = loss_fn(ai_logits, y_batch)\n        loss_user = loss_fn(user_logits, y_batch)\n        optimizer_ai.zero_grad()\n        loss_ai.backward()\n        optimizer_ai.step()\n        optimizer_user.zero_grad()\n        loss_user.backward()\n        optimizer_user.step()\n        bs = y_batch.size(0)\n        train_loss_sum += loss_ai.item() * bs\n        # Compute alignment (MMAS)\n        P = F.softmax(ai_logits, dim=1)\n        Q = F.softmax(user_logits, dim=1)\n        M = 0.5 * (P + Q)\n        kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n        kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n        jsd = 0.5 * (kl1 + kl2)\n        train_jsd_sum += torch.sum(1 - jsd).item()\n        train_samples += bs\n\n    train_loss = train_loss_sum / len(train_dataset)\n    train_align = train_jsd_sum / train_samples\n\n    ai_model.eval()\n    user_model.eval()\n    val_loss_sum = 0.0\n    val_jsd_sum = 0.0\n    val_samples = 0\n    with torch.no_grad():\n        for x_batch, y_batch in val_loader:\n            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n            ai_logits = ai_model(x_batch)\n            bs = y_batch.size(0)\n            val_loss_sum += loss_fn(ai_logits, y_batch).item() * bs\n            P = F.softmax(ai_logits, dim=1)\n            Q = F.softmax(user_model(x_batch), dim=1)\n            M = 0.5 * (P + Q)\n            kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            jsd = 0.5 * (kl1 + kl2)\n            val_jsd_sum += torch.sum(1 - jsd).item()\n            val_samples += bs\n\n    val_loss = val_loss_sum / len(val_dataset)\n    val_align = val_jsd_sum / val_samples\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(train_align)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(val_align)\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"synthetic\"][\"epochs\"].append(epoch)\n\n# Final validation predictions and ground truth\nall_preds, all_gts = [], []\nwith torch.no_grad():\n    for x_batch, y_batch in val_loader:\n        x_batch = x_batch.to(device)\n        preds = torch.argmax(ai_model(x_batch), dim=1).cpu().numpy()\n        all_preds.append(preds)\n        all_gts.append(y_batch.numpy())\nall_preds = np.concatenate(all_preds, axis=0)\nall_gts = np.concatenate(all_gts, axis=0)\nexperiment_data[\"synthetic\"][\"predictions\"] = all_preds\nexperiment_data[\"synthetic\"][\"ground_truth\"] = all_gts\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Reproducibility\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Synthetic data generation\nN_train, N_val, D, C = 1000, 200, 10, 3\nW_true = np.random.randn(D, C)\nx_train = np.random.randn(N_train, D)\nx_val = np.random.randn(N_val, D)\ny_train = np.argmax(x_train @ W_true + 0.1 * np.random.randn(N_train, C), axis=1)\ny_val = np.argmax(x_val @ W_true + 0.1 * np.random.randn(N_val, C), axis=1)\nmean = x_train.mean(axis=0)\nstd = x_train.std(axis=0) + 1e-8\nx_train = (x_train - mean) / std\nx_val = (x_val - mean) / std\n\n# DataLoaders\ntrain_dataset = TensorDataset(\n    torch.tensor(x_train, dtype=torch.float32),\n    torch.tensor(y_train, dtype=torch.long),\n)\nval_dataset = TensorDataset(\n    torch.tensor(x_val, dtype=torch.float32),\n    torch.tensor(y_val, dtype=torch.long),\n)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64)\n\n\n# Simple MLP definition\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\n# Hyperparameter sweep over learning rates\nlearning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\nnum_epochs = 10\n\n# Initialize experiment_data dict\nexperiment_data = {\n    \"learning_rate\": {\n        \"synthetic\": {\n            \"lrs\": learning_rates,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nloss_fn = nn.CrossEntropyLoss()\n\nfor lr in learning_rates:\n    # Reset model weights for fair comparison\n    torch.manual_seed(0)\n    ai_model = MLP(D, 32, C).to(device)\n    user_model = MLP(D, 32, C).to(device)\n    optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=lr)\n    optimizer_user = torch.optim.Adam(user_model.parameters(), lr=lr)\n\n    # Containers for this learning rate\n    train_losses, val_losses = [], []\n    train_aligns, val_aligns = [], []\n\n    for epoch in range(1, num_epochs + 1):\n        # Training\n        ai_model.train()\n        user_model.train()\n        total_loss, total_align, n_samples = 0.0, 0.0, 0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            # Forward passes\n            logits_ai = ai_model(xb)\n            logits_user = user_model(xb)\n            # Cross-entropy losses\n            loss_ai = loss_fn(logits_ai, yb)\n            loss_user = loss_fn(logits_user, yb)\n            # Backprop AI\n            optimizer_ai.zero_grad()\n            loss_ai.backward()\n            optimizer_ai.step()\n            # Backprop user\n            optimizer_user.zero_grad()\n            loss_user.backward()\n            optimizer_user.step()\n            # Accumulate loss\n            bs = yb.size(0)\n            total_loss += loss_ai.item() * bs\n            # Alignment (1 - JSD)\n            P = F.softmax(logits_ai, dim=1)\n            Q = F.softmax(logits_user, dim=1)\n            M = 0.5 * (P + Q)\n            kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            jsd = 0.5 * (kl1 + kl2)\n            total_align += torch.sum(1 - jsd).item()\n            n_samples += bs\n        train_losses.append(total_loss / len(train_dataset))\n        train_aligns.append(total_align / n_samples)\n\n        # Validation\n        ai_model.eval()\n        user_model.eval()\n        v_loss, v_align, v_samples = 0.0, 0.0, 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits_ai = ai_model(xb)\n                # loss\n                v_loss += loss_fn(logits_ai, yb).item() * yb.size(0)\n                # alignment\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(user_model(xb), dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                v_align += torch.sum(1 - jsd).item()\n                v_samples += yb.size(0)\n        val_losses.append(v_loss / len(val_dataset))\n        val_aligns.append(v_align / v_samples)\n        print(\n            f\"LR {lr:.1e} Epoch {epoch}: val_loss = {val_losses[-1]:.4f}, val_align = {val_aligns[-1]:.4f}\"\n        )\n\n    # Store per\u2010epoch metrics\n    sd = experiment_data[\"learning_rate\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(train_aligns)\n    sd[\"metrics\"][\"val\"].append(val_aligns)\n    sd[\"losses\"][\"train\"].append(train_losses)\n    sd[\"losses\"][\"val\"].append(val_losses)\n\n    # Final validation predictions & ground truth\n    all_preds, all_gts = [], []\n    ai_model.eval()\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            xb = xb.to(device)\n            preds = torch.argmax(ai_model(xb), dim=1).cpu().numpy()\n            all_preds.append(preds)\n            all_gts.append(yb.numpy())\n    preds_arr = np.concatenate(all_preds, axis=0)\n    gts_arr = np.concatenate(all_gts, axis=0)\n    sd[\"predictions\"].append(preds_arr)\n    sd[\"ground_truth\"].append(gts_arr)\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# prepare working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# reproducibility\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# synthetic data\nN_train, N_val, D, C = 1000, 200, 10, 3\nW_true = np.random.randn(D, C)\nx_train = np.random.randn(N_train, D)\nx_val = np.random.randn(N_val, D)\ny_train = np.argmax(x_train @ W_true + 0.1 * np.random.randn(N_train, C), axis=1)\ny_val = np.argmax(x_val @ W_true + 0.1 * np.random.randn(N_val, C), axis=1)\nmean, std = x_train.mean(0), x_train.std(0) + 1e-8\nx_train = (x_train - mean) / std\nx_val = (x_val - mean) / std\n\ntrain_dataset = TensorDataset(\n    torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n)\nval_dataset = TensorDataset(\n    torch.tensor(x_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long)\n)\n\n\n# simple MLP\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\nloss_fn = nn.CrossEntropyLoss()\nnum_epochs = 10\nbatch_sizes = [16, 32, 128, 256]\n\n# initialize experiment data container\nexperiment_data = {\n    \"batch_size\": {\n        \"synthetic\": {\n            \"batch_sizes\": [],\n            \"epochs\": list(range(1, num_epochs + 1)),\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": y_val,\n        }\n    }\n}\n\nfor bs in batch_sizes:\n    print(f\"Starting run with batch size = {bs}\")\n    experiment_data[\"batch_size\"][\"synthetic\"][\"batch_sizes\"].append(bs)\n    # re-seed for reproducibility across runs\n    torch.manual_seed(0)\n    np.random.seed(0)\n    # dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=bs)\n    # fresh models and optimizers\n    ai_model = MLP(D, 32, C).to(device)\n    user_model = MLP(D, 32, C).to(device)\n    optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n    optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n\n    train_alignments, val_alignments = [], []\n    train_losses, val_losses = [], []\n\n    for epoch in range(1, num_epochs + 1):\n        ai_model.train()\n        user_model.train()\n        t_loss_sum = 0.0\n        t_jsd_sum = 0.0\n        t_samples = 0\n\n        for x_b, y_b in train_loader:\n            x_b, y_b = x_b.to(device), y_b.to(device)\n            ai_logits = ai_model(x_b)\n            user_logits = user_model(x_b)\n            loss_ai = loss_fn(ai_logits, y_b)\n            loss_user = loss_fn(user_logits, y_b)\n\n            optimizer_ai.zero_grad()\n            loss_ai.backward()\n            optimizer_ai.step()\n            optimizer_user.zero_grad()\n            loss_user.backward()\n            optimizer_user.step()\n\n            bs_cur = y_b.size(0)\n            t_loss_sum += loss_ai.item() * bs_cur\n            P = F.softmax(ai_logits, dim=1)\n            Q = F.softmax(user_logits, dim=1)\n            M = 0.5 * (P + Q)\n            kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            jsd = 0.5 * (kl1 + kl2)\n            t_jsd_sum += torch.sum(1 - jsd).item()\n            t_samples += bs_cur\n\n        train_loss = t_loss_sum / len(train_dataset)\n        train_align = t_jsd_sum / t_samples\n\n        # validation\n        ai_model.eval()\n        user_model.eval()\n        v_loss_sum = 0.0\n        v_jsd_sum = 0.0\n        v_samples = 0\n        with torch.no_grad():\n            for x_b, y_b in val_loader:\n                x_b, y_b = x_b.to(device), y_b.to(device)\n                ai_logits = ai_model(x_b)\n                bs_cur = y_b.size(0)\n                v_loss_sum += loss_fn(ai_logits, y_b).item() * bs_cur\n                P = F.softmax(ai_logits, dim=1)\n                Q = F.softmax(user_model(x_b), dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                v_jsd_sum += torch.sum(1 - jsd).item()\n                v_samples += bs_cur\n\n        val_loss = v_loss_sum / len(val_dataset)\n        val_align = v_jsd_sum / v_samples\n        print(\n            f\"[bs={bs}] Epoch {epoch}: val_loss={val_loss:.4f}, val_align={val_align:.4f}\"\n        )\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        train_alignments.append(train_align)\n        val_alignments.append(val_align)\n\n    # record curves\n    experiment_data[\"batch_size\"][\"synthetic\"][\"metrics\"][\"train\"].append(\n        train_alignments\n    )\n    experiment_data[\"batch_size\"][\"synthetic\"][\"metrics\"][\"val\"].append(val_alignments)\n    experiment_data[\"batch_size\"][\"synthetic\"][\"losses\"][\"train\"].append(train_losses)\n    experiment_data[\"batch_size\"][\"synthetic\"][\"losses\"][\"val\"].append(val_losses)\n\n    # final predictions\n    preds = []\n    with torch.no_grad():\n        for x_b, _ in val_loader:\n            x_b = x_b.to(device)\n            p = torch.argmax(ai_model(x_b), dim=1).cpu().numpy()\n            preds.append(p)\n    preds = np.concatenate(preds, axis=0)\n    experiment_data[\"batch_size\"][\"synthetic\"][\"predictions\"].append(preds)\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# set up working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device and reproducibility\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# synthetic data\nN_train, N_val, D, C = 1000, 200, 10, 3\nW_true = np.random.randn(D, C)\nx_train = np.random.randn(N_train, D)\nx_val = np.random.randn(N_val, D)\ny_train = np.argmax(x_train @ W_true + 0.1 * np.random.randn(N_train, C), axis=1)\ny_val = np.argmax(x_val @ W_true + 0.1 * np.random.randn(N_val, C), axis=1)\nmean, std = x_train.mean(0), x_train.std(0) + 1e-8\nx_train = (x_train - mean) / std\nx_val = (x_val - mean) / std\n\n# dataloaders\ntrain_ds = TensorDataset(\n    torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n)\nval_ds = TensorDataset(\n    torch.tensor(x_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long)\n)\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=64)\n\n\n# model definition\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\nloss_fn = nn.CrossEntropyLoss()\nnum_epochs = 10\nweight_decays = [1e-5, 1e-4, 1e-3, 1e-2]\n\n# prepare experiment_data\nexperiment_data = {\n    \"weight_decay\": {\n        \"synthetic\": {\n            \"weight_decay_values\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": y_val.copy(),\n        }\n    }\n}\n\n# hyperparameter sweep\nfor wd in weight_decays:\n    print(f\"=== Training with weight_decay={wd} ===\")\n    experiment_data[\"weight_decay\"][\"synthetic\"][\"weight_decay_values\"].append(wd)\n    ai_model = MLP(D, 32, C).to(device)\n    user_model = MLP(D, 32, C).to(device)\n    optim_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3, weight_decay=wd)\n    optim_usr = torch.optim.Adam(user_model.parameters(), lr=1e-3, weight_decay=wd)\n\n    tr_metrics, vl_metrics = [], []\n    tr_losses, vl_losses = [], []\n\n    for epoch in range(1, num_epochs + 1):\n        ai_model.train()\n        user_model.train()\n        train_loss_sum = train_jsd_sum = train_n = 0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits_ai = ai_model(xb)\n            logits_usr = user_model(xb)\n            loss_ai = loss_fn(logits_ai, yb)\n            loss_usr = loss_fn(logits_usr, yb)\n            optim_ai.zero_grad()\n            loss_ai.backward()\n            optim_ai.step()\n            optim_usr.zero_grad()\n            loss_usr.backward()\n            optim_usr.step()\n            bs = yb.size(0)\n            train_loss_sum += loss_ai.item() * bs\n            P = F.softmax(logits_ai, dim=1)\n            Q = F.softmax(logits_usr, dim=1)\n            M = 0.5 * (P + Q)\n            kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            jsd = 0.5 * (kl1 + kl2)\n            train_jsd_sum += (1 - jsd).sum().item()\n            train_n += bs\n        tr_loss = train_loss_sum / len(train_ds)\n        tr_align = train_jsd_sum / train_n\n\n        ai_model.eval()\n        user_model.eval()\n        val_loss_sum = val_jsd_sum = val_n = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits_ai = ai_model(xb)\n                bs = yb.size(0)\n                val_loss_sum += loss_fn(logits_ai, yb).item() * bs\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(user_model(xb), dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                val_jsd_sum += (1 - jsd).sum().item()\n                val_n += bs\n        vl_loss = val_loss_sum / len(val_ds)\n        vl_align = val_jsd_sum / val_n\n        print(f\"wd={wd} epoch={epoch} val_loss={vl_loss:.4f}\")\n\n        tr_metrics.append(tr_align)\n        vl_metrics.append(vl_align)\n        tr_losses.append(tr_loss)\n        vl_losses.append(vl_loss)\n\n    # store results\n    experiment_data[\"weight_decay\"][\"synthetic\"][\"metrics\"][\"train\"].append(tr_metrics)\n    experiment_data[\"weight_decay\"][\"synthetic\"][\"metrics\"][\"val\"].append(vl_metrics)\n    experiment_data[\"weight_decay\"][\"synthetic\"][\"losses\"][\"train\"].append(tr_losses)\n    experiment_data[\"weight_decay\"][\"synthetic\"][\"losses\"][\"val\"].append(vl_losses)\n\n    preds = []\n    with torch.no_grad():\n        for xb, _ in val_loader:\n            xb = xb.to(device)\n            p = torch.argmax(ai_model(xb), dim=1).cpu().numpy()\n            preds.append(p)\n    preds = np.concatenate(preds, axis=0)\n    experiment_data[\"weight_decay\"][\"synthetic\"][\"predictions\"].append(preds)\n\n# convert lists to np arrays for plotting\nsd = experiment_data[\"weight_decay\"][\"synthetic\"]\nsd[\"metrics\"][\"train\"] = np.array(sd[\"metrics\"][\"train\"])\nsd[\"metrics\"][\"val\"] = np.array(sd[\"metrics\"][\"val\"])\nsd[\"losses\"][\"train\"] = np.array(sd[\"losses\"][\"train\"])\nsd[\"losses\"][\"val\"] = np.array(sd[\"losses\"][\"val\"])\nsd[\"predictions\"] = np.stack(sd[\"predictions\"], axis=0)\nsd[\"ground_truth\"] = np.array(sd[\"ground_truth\"])\nsd[\"weight_decay_values\"] = np.array(sd[\"weight_decay_values\"])\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Reproducibility\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Synthetic data\nN_train, N_val, D, C = 1000, 200, 10, 3\nW_true = np.random.randn(D, C)\nx_train = np.random.randn(N_train, D)\nx_val = np.random.randn(N_val, D)\ny_train = np.argmax(x_train @ W_true + 0.1 * np.random.randn(N_train, C), axis=1)\ny_val = np.argmax(x_val @ W_true + 0.1 * np.random.randn(N_val, C), axis=1)\n# Normalize\nmean = x_train.mean(axis=0)\nstd = x_train.std(axis=0) + 1e-8\nx_train = (x_train - mean) / std\nx_val = (x_val - mean) / std\n\n# DataLoaders\ntrain_dataset = TensorDataset(\n    torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n)\nval_dataset = TensorDataset(\n    torch.tensor(x_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long)\n)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64)\n\n\n# Model definition\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\n# Hyperparameter tuning setup\nhidden_dims = [16, 32, 64, 128]\nnum_epochs = 10\nloss_fn = nn.CrossEntropyLoss()\nexperiment_data = {\n    \"hidden_dim_tuning\": {\n        \"synthetic\": {\n            \"hidden_dims\": hidden_dims,\n            \"epochs\": list(range(1, num_epochs + 1)),\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Sweep over hidden_dims\nfor hd in hidden_dims:\n    print(f\"Running hidden_dim = {hd}\")\n    ai_model = MLP(D, hd, C).to(device)\n    user_model = MLP(D, hd, C).to(device)\n    optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n    optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n\n    train_align_list, val_align_list = [], []\n    train_loss_list, val_loss_list = [], []\n\n    for epoch in range(1, num_epochs + 1):\n        ai_model.train()\n        user_model.train()\n        t_loss_sum = t_jsd_sum = t_samples = 0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            ai_logits = ai_model(xb)\n            user_logits = user_model(xb)\n            loss_ai = loss_fn(ai_logits, yb)\n            loss_user = loss_fn(user_logits, yb)\n            optimizer_ai.zero_grad()\n            loss_ai.backward()\n            optimizer_ai.step()\n            optimizer_user.zero_grad()\n            loss_user.backward()\n            optimizer_user.step()\n            bs = yb.size(0)\n            t_loss_sum += loss_ai.item() * bs\n            P = F.softmax(ai_logits, dim=1)\n            Q = F.softmax(user_logits, dim=1)\n            M = 0.5 * (P + Q)\n            kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            jsd = 0.5 * (kl1 + kl2)\n            t_jsd_sum += torch.sum(1 - jsd).item()\n            t_samples += bs\n        train_loss = t_loss_sum / len(train_dataset)\n        train_align = t_jsd_sum / t_samples\n\n        ai_model.eval()\n        user_model.eval()\n        v_loss_sum = v_jsd_sum = v_samples = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                ai_logits = ai_model(xb)\n                loss = loss_fn(ai_logits, yb)\n                v_loss_sum += loss.item() * yb.size(0)\n                P = F.softmax(ai_logits, dim=1)\n                Q = F.softmax(user_model(xb), dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                v_jsd_sum += torch.sum(1 - jsd).item()\n                v_samples += yb.size(0)\n        val_loss = v_loss_sum / len(val_dataset)\n        val_align = v_jsd_sum / v_samples\n\n        print(f\"hidden_dim {hd} Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n        train_align_list.append(train_align)\n        val_align_list.append(val_align)\n        train_loss_list.append(train_loss)\n        val_loss_list.append(val_loss)\n\n    # Collect predictions\n    ai_model.eval()\n    preds = []\n    with torch.no_grad():\n        for xb, _ in val_loader:\n            xb = xb.to(device)\n            p = torch.argmax(ai_model(xb), dim=1).cpu().numpy()\n            preds.append(p)\n    preds = np.concatenate(preds, axis=0)\n\n    experiment_data[\"hidden_dim_tuning\"][\"synthetic\"][\"metrics\"][\"train\"].append(\n        train_align_list\n    )\n    experiment_data[\"hidden_dim_tuning\"][\"synthetic\"][\"metrics\"][\"val\"].append(\n        val_align_list\n    )\n    experiment_data[\"hidden_dim_tuning\"][\"synthetic\"][\"losses\"][\"train\"].append(\n        train_loss_list\n    )\n    experiment_data[\"hidden_dim_tuning\"][\"synthetic\"][\"losses\"][\"val\"].append(\n        val_loss_list\n    )\n    experiment_data[\"hidden_dim_tuning\"][\"synthetic\"][\"predictions\"].append(preds)\n\n# Save ground truth once\nexperiment_data[\"hidden_dim_tuning\"][\"synthetic\"][\"ground_truth\"] = y_val\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# prepare working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# reproducibility\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# synthetic data\nN_train, N_val, D, C = 1000, 200, 10, 3\nW_true = np.random.randn(D, C)\nx_train = np.random.randn(N_train, D)\nx_val = np.random.randn(N_val, D)\ny_train = np.argmax(x_train @ W_true + 0.1 * np.random.randn(N_train, C), axis=1)\ny_val = np.argmax(x_val @ W_true + 0.1 * np.random.randn(N_val, C), axis=1)\nmean = x_train.mean(axis=0)\nstd = x_train.std(axis=0) + 1e-8\nx_train = (x_train - mean) / std\nx_val = (x_val - mean) / std\n\n# DataLoaders\ntrain_dataset = TensorDataset(\n    torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n)\nval_dataset = TensorDataset(\n    torch.tensor(x_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long)\n)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64)\n\n\n# MLP with dropout\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim, dropout_rate):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)\n\n\n# hyperparameters\ndropout_rates = [0.0, 0.1, 0.2, 0.5]\nnum_epochs = 10\n\n# initialize experiment data\nexperiment_data = {\n    \"dropout_rate\": {\n        \"synthetic\": {\n            \"hyperparams\": dropout_rates,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# training loop over dropout rates\nfor dr in dropout_rates:\n    # models, optimizer, loss\n    ai_model = MLP(D, 32, C, dr).to(device)\n    user_model = MLP(D, 32, C, dr).to(device)\n    optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n    optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n    loss_fn = nn.CrossEntropyLoss()\n    # storage per epoch\n    train_metrics, val_metrics = [], []\n    train_losses, val_losses = [], []\n    # epochs\n    for epoch in range(1, num_epochs + 1):\n        ai_model.train()\n        user_model.train()\n        train_loss_sum = 0.0\n        train_jsd_sum = 0.0\n        train_samples = 0\n        for x_batch, y_batch in train_loader:\n            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n            # forward\n            ai_logits = ai_model(x_batch)\n            user_logits = user_model(x_batch)\n            # losses\n            loss_ai = loss_fn(ai_logits, y_batch)\n            # backward ai\n            optimizer_ai.zero_grad()\n            loss_ai.backward()\n            optimizer_ai.step()\n            # backward user\n            optimizer_user.zero_grad()\n            loss_user = loss_fn(user_logits, y_batch)\n            loss_user.backward()\n            optimizer_user.step()\n            # accumulate\n            bs = y_batch.size(0)\n            train_loss_sum += loss_ai.item() * bs\n            P = F.softmax(ai_logits, dim=1)\n            Q = F.softmax(user_logits, dim=1)\n            M = 0.5 * (P + Q)\n            kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            jsd = 0.5 * (kl1 + kl2)\n            train_jsd_sum += torch.sum(1 - jsd).item()\n            train_samples += bs\n        train_loss = train_loss_sum / len(train_dataset)\n        train_align = train_jsd_sum / train_samples\n        train_losses.append(train_loss)\n        train_metrics.append(train_align)\n        # validation\n        ai_model.eval()\n        user_model.eval()\n        val_loss_sum = 0.0\n        val_jsd_sum = 0.0\n        val_samples = 0\n        with torch.no_grad():\n            for x_batch, y_batch in val_loader:\n                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n                ai_logits = ai_model(x_batch)\n                user_logits = user_model(x_batch)\n                bs = y_batch.size(0)\n                val_loss_sum += loss_fn(ai_logits, y_batch).item() * bs\n                P = F.softmax(ai_logits, dim=1)\n                Q = F.softmax(user_logits, dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                val_jsd_sum += torch.sum(1 - jsd).item()\n                val_samples += bs\n        val_loss = val_loss_sum / len(val_dataset)\n        val_align = val_jsd_sum / val_samples\n        val_losses.append(val_loss)\n        val_metrics.append(val_align)\n        print(f\"dropout={dr} epoch={epoch} val_loss={val_loss:.4f}\")\n    # final predictions\n    all_preds, all_gts = [], []\n    with torch.no_grad():\n        for x_batch, y_batch in val_loader:\n            x_batch = x_batch.to(device)\n            preds = torch.argmax(ai_model(x_batch), dim=1).cpu().numpy()\n            all_preds.append(preds)\n            all_gts.append(y_batch.numpy())\n    all_preds = np.concatenate(all_preds, axis=0)\n    all_gts = np.concatenate(all_gts, axis=0)\n    # save run results\n    exp = experiment_data[\"dropout_rate\"][\"synthetic\"]\n    exp[\"metrics\"][\"train\"].append(train_metrics)\n    exp[\"metrics\"][\"val\"].append(val_metrics)\n    exp[\"losses\"][\"train\"].append(train_losses)\n    exp[\"losses\"][\"val\"].append(val_losses)\n    exp[\"predictions\"].append(all_preds)\n    exp[\"ground_truth\"].append(all_gts)\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Hyperparameters\nalphas = [0.0, 0.05, 0.1, 0.2]\nnum_epochs = 10\nbatch_size = 64\nlr = 1e-3\n\n# Reproducibility for data\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Synthetic data generation\nN_train, N_val, D, C = 1000, 200, 10, 3\nW_true = np.random.randn(D, C)\nx_train = np.random.randn(N_train, D)\nx_val = np.random.randn(N_val, D)\ny_train = np.argmax(x_train @ W_true + 0.1 * np.random.randn(N_train, C), axis=1)\ny_val = np.argmax(x_val @ W_true + 0.1 * np.random.randn(N_val, C), axis=1)\nmean = x_train.mean(axis=0)\nstd = x_train.std(axis=0) + 1e-8\nx_train = (x_train - mean) / std\nx_val = (x_val - mean) / std\n\n# TensorDatasets & Loaders (shuffle will be controlled by the seed)\ntrain_dataset = TensorDataset(\n    torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n)\nval_dataset = TensorDataset(\n    torch.tensor(x_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long)\n)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n\n# Simple MLP definition\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\n# Prepare experiment_data\nexperiment_data = {\n    \"label_smoothing\": {\n        \"synthetic\": {\n            \"alphas\": alphas,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"epochs\": list(range(1, num_epochs + 1)),\n        }\n    }\n}\n\n# Hyperparameter sweep\nfor alpha in alphas:\n    # reset seeds for reproducibility of weights & shuffles\n    torch.manual_seed(0)\n    np.random.seed(0)\n    # models & optimizers\n    ai_model = MLP(D, 32, C).to(device)\n    user_model = MLP(D, 32, C).to(device)\n    optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=lr)\n    optimizer_user = torch.optim.Adam(user_model.parameters(), lr=lr)\n    # loss with smoothing\n    loss_fn = nn.CrossEntropyLoss(label_smoothing=alpha)\n\n    train_aligns, val_aligns = [], []\n    train_losses, val_losses = [], []\n\n    for epoch in range(1, num_epochs + 1):\n        ai_model.train()\n        user_model.train()\n        train_loss_sum = 0.0\n        train_jsd_sum = 0.0\n        train_samples = 0\n\n        for x_batch, y_batch in train_loader:\n            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n            ai_logits = ai_model(x_batch)\n            user_logits = user_model(x_batch)\n\n            loss_ai = loss_fn(ai_logits, y_batch)\n            loss_user = loss_fn(user_logits, y_batch)\n            optimizer_ai.zero_grad()\n            loss_ai.backward()\n            optimizer_ai.step()\n            optimizer_user.zero_grad()\n            loss_user.backward()\n            optimizer_user.step()\n\n            bs = y_batch.size(0)\n            train_loss_sum += loss_ai.item() * bs\n            P = F.softmax(ai_logits, dim=1)\n            Q = F.softmax(user_logits, dim=1)\n            M = 0.5 * (P + Q)\n            kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            jsd = 0.5 * (kl1 + kl2)\n            train_jsd_sum += torch.sum(1 - jsd).item()\n            train_samples += bs\n\n        train_loss = train_loss_sum / len(train_dataset)\n        train_align = train_jsd_sum / train_samples\n        train_losses.append(train_loss)\n        train_aligns.append(train_align)\n\n        ai_model.eval()\n        user_model.eval()\n        val_loss_sum = 0.0\n        val_jsd_sum = 0.0\n        val_samples = 0\n\n        with torch.no_grad():\n            for x_batch, y_batch in val_loader:\n                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n                ai_logits = ai_model(x_batch)\n                bs = y_batch.size(0)\n                val_loss_sum += loss_fn(ai_logits, y_batch).item() * bs\n\n                P = F.softmax(ai_logits, dim=1)\n                Q = F.softmax(user_model(x_batch), dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                val_jsd_sum += torch.sum(1 - jsd).item()\n                val_samples += bs\n\n        val_loss = val_loss_sum / len(val_dataset)\n        val_align = val_jsd_sum / val_samples\n        val_losses.append(val_loss)\n        val_aligns.append(val_align)\n\n        print(\n            f\"alpha={alpha} | Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\"\n        )\n\n    # final predictions & ground truth\n    all_preds, all_gts = [], []\n    with torch.no_grad():\n        for x_batch, y_batch in val_loader:\n            x_batch = x_batch.to(device)\n            preds = torch.argmax(ai_model(x_batch), dim=1).cpu().numpy()\n            all_preds.append(preds)\n            all_gts.append(y_batch.numpy())\n    all_preds = np.concatenate(all_preds, axis=0)\n    all_gts = np.concatenate(all_gts, axis=0)\n\n    sd = experiment_data[\"label_smoothing\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(train_aligns)\n    sd[\"metrics\"][\"val\"].append(val_aligns)\n    sd[\"losses\"][\"train\"].append(train_losses)\n    sd[\"losses\"][\"val\"].append(val_losses)\n    sd[\"predictions\"].append(all_preds)\n    sd[\"ground_truth\"].append(all_gts)\n\n# Save experiment_data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and reproducibility\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Synthetic data generation\nN_train, N_val, D, C = 1000, 200, 10, 3\nW_true = np.random.randn(D, C)\nx_train = np.random.randn(N_train, D)\nx_val = np.random.randn(N_val, D)\ny_train = np.argmax(x_train @ W_true + 0.1 * np.random.randn(N_train, C), axis=1)\ny_val = np.argmax(x_val @ W_true + 0.1 * np.random.randn(N_val, C), axis=1)\n# Normalize\nmean = x_train.mean(0)\nstd = x_train.std(0) + 1e-8\nx_train = (x_train - mean) / std\nx_val = (x_val - mean) / std\n\n# DataLoaders\ntrain_ds = TensorDataset(torch.tensor(x_train, float), torch.tensor(y_train, long))\nval_ds = TensorDataset(torch.tensor(x_val, float), torch.tensor(y_val, long))\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=64)\n\n\n# Model definition\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\n# Hyperparameter grid\nbeta1_list = [0.5, 0.7, 0.9, 0.99]\nnum_epochs = 10\nlr = 1e-3\n\n# Prepare experiment data structure\nexperiment_data = {\n    \"adam_beta1\": {\n        \"beta1_values\": beta1_list,\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        },\n    }\n}\n\nloss_fn = nn.CrossEntropyLoss()\n\n# Grid search over beta1\nfor b1 in beta1_list:\n    # Reproducibility per run\n    torch.manual_seed(0)\n    np.random.seed(0)\n    # Init models and optimizers\n    ai_model = MLP(D, 32, C).to(device)\n    user_model = MLP(D, 32, C).to(device)\n    opt_ai = torch.optim.Adam(ai_model.parameters(), lr=lr, betas=(b1, 0.999))\n    opt_user = torch.optim.Adam(user_model.parameters(), lr=lr, betas=(b1, 0.999))\n\n    train_aligns, val_aligns = [], []\n    train_losses, val_losses = [], []\n\n    for epoch in range(1, num_epochs + 1):\n        ai_model.train()\n        user_model.train()\n        tl_sum = 0.0\n        ta_sum = 0.0\n        n_train = 0\n        for x_b, y_b in train_loader:\n            x_b, y_b = x_b.to(device), y_b.to(device)\n            # Forward\n            logits_ai = ai_model(x_b)\n            logits_u = user_model(x_b)\n            # Loss and update\n            loss_ai = loss_fn(logits_ai, y_b)\n            loss_u = loss_fn(logits_u, y_b)\n            opt_ai.zero_grad()\n            loss_ai.backward()\n            opt_ai.step()\n            opt_user.zero_grad()\n            loss_u.backward()\n            opt_user.step()\n            bs = y_b.size(0)\n            tl_sum += loss_ai.item() * bs\n            # Alignment\n            P = F.softmax(logits_ai, dim=1)\n            Q = F.softmax(logits_u, dim=1)\n            M = 0.5 * (P + Q)\n            kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), 1)\n            kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), 1)\n            jsd = 0.5 * (kl1 + kl2)\n            ta_sum += torch.sum(1 - jsd).item()\n            n_train += bs\n        tr_loss = tl_sum / len(train_ds)\n        tr_align = ta_sum / n_train\n        train_losses.append(tr_loss)\n        train_aligns.append(tr_align)\n\n        # Validation\n        ai_model.eval()\n        user_model.eval()\n        vl_sum = 0.0\n        va_sum = 0.0\n        n_val = 0\n        with torch.no_grad():\n            for x_b, y_b in val_loader:\n                x_b, y_b = x_b.to(device), y_b.to(device)\n                logits_ai = ai_model(x_b)\n                bs = y_b.size(0)\n                vl_sum += loss_fn(logits_ai, y_b).item() * bs\n                P = F.softmax(logits_ai, 1)\n                Q = F.softmax(user_model(x_b), 1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), 1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), 1)\n                jsd = 0.5 * (kl1 + kl2)\n                va_sum += torch.sum(1 - jsd).item()\n                n_val += bs\n        vl = vl_sum / len(val_ds)\n        va = va_sum / n_val\n        val_losses.append(vl)\n        val_aligns.append(va)\n        print(f\"beta1={b1:.2f} epoch={epoch} val_loss={vl:.4f}\")\n\n    # Store per-run data\n    d = experiment_data[\"adam_beta1\"][\"synthetic\"]\n    d[\"metrics\"][\"train\"].append(train_aligns)\n    d[\"metrics\"][\"val\"].append(val_aligns)\n    d[\"losses\"][\"train\"].append(train_losses)\n    d[\"losses\"][\"val\"].append(val_losses)\n    # Final predictions\n    preds_all, gts_all = [], []\n    with torch.no_grad():\n        for x_b, y_b in val_loader:\n            x_b = x_b.to(device)\n            preds = torch.argmax(ai_model(x_b), 1).cpu().numpy()\n            preds_all.append(preds)\n            gts_all.append(y_b.numpy())\n    preds_all = np.concatenate(preds_all)\n    gts_all = np.concatenate(gts_all)\n    d[\"predictions\"].append(preds_all)\n    d[\"ground_truth\"].append(gts_all)\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device and reproducibility\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(0)\nnp.random.seed(0)\nprint(f\"Using device: {device}\")\n\n# Synthetic data\nN_train, N_val, D, C = 1000, 200, 10, 3\nW_true = np.random.randn(D, C)\nx_train = np.random.randn(N_train, D)\nx_val = np.random.randn(N_val, D)\ny_train = np.argmax(x_train @ W_true + 0.1 * np.random.randn(N_train, C), axis=1)\ny_val = np.argmax(x_val @ W_true + 0.1 * np.random.randn(N_val, C), axis=1)\nmean, std = x_train.mean(0), x_train.std(0) + 1e-8\nx_train = (x_train - mean) / std\nx_val = (x_val - mean) / std\n\ntrain_dataset = TensorDataset(torch.tensor(x_train, float), torch.tensor(y_train, long))\nval_dataset = TensorDataset(torch.tensor(x_val, float), torch.tensor(y_val, long))\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64)\n\n\n# Simple MLP\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\n# Hyperparameter sweep\nbeta2_list = [0.9, 0.99, 0.999]\nnum_epochs = 10\n\nmetrics_train_runs = []\nmetrics_val_runs = []\nlosses_train_runs = []\nlosses_val_runs = []\npreds_runs = []\n\nfor beta2 in beta2_list:\n    # re-init models & optimizers\n    ai_model = MLP(D, 32, C).to(device)\n    user_model = MLP(D, 32, C).to(device)\n    opt_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3, betas=(0.9, beta2))\n    opt_user = torch.optim.Adam(user_model.parameters(), lr=1e-3, betas=(0.9, beta2))\n    loss_fn = nn.CrossEntropyLoss()\n\n    m_tr, m_val = [], []\n    l_tr, l_val = [], []\n\n    for epoch in range(1, num_epochs + 1):\n        ai_model.train()\n        user_model.train()\n        tloss_sum = 0.0\n        tjsd_sum = 0.0\n        ntr = 0\n        # train\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits_ai = ai_model(xb)\n            logits_user = user_model(xb)\n            loss_ai = loss_fn(logits_ai, yb)\n            loss_user = loss_fn(logits_user, yb)\n            opt_ai.zero_grad()\n            loss_ai.backward()\n            opt_ai.step()\n            opt_user.zero_grad()\n            loss_user.backward()\n            opt_user.step()\n            bs = yb.size(0)\n            tloss_sum += loss_ai.item() * bs\n            P = F.softmax(logits_ai, 1)\n            Q = F.softmax(logits_user, 1)\n            M = 0.5 * (P + Q)\n            kl1 = (P * (torch.log(P + 1e-8) - torch.log(M + 1e-8))).sum(1)\n            kl2 = (Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8))).sum(1)\n            jsd = 0.5 * (kl1 + kl2)\n            tjsd_sum += (1 - jsd).sum().item()\n            ntr += bs\n        train_loss = tloss_sum / len(train_dataset)\n        train_align = tjsd_sum / ntr\n\n        # val\n        ai_model.eval()\n        user_model.eval()\n        vloss_sum = 0.0\n        vjsd_sum = 0.0\n        nval = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits_ai = ai_model(xb)\n                bs = yb.size(0)\n                vloss_sum += loss_fn(logits_ai, yb).item() * bs\n                P = F.softmax(logits_ai, 1)\n                Q = F.softmax(user_model(xb), 1)\n                M = 0.5 * (P + Q)\n                kl1 = (P * (torch.log(P + 1e-8) - torch.log(M + 1e-8))).sum(1)\n                kl2 = (Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8))).sum(1)\n                jsd = 0.5 * (kl1 + kl2)\n                vjsd_sum += (1 - jsd).sum().item()\n                nval += bs\n        val_loss = vloss_sum / len(val_dataset)\n        val_align = vjsd_sum / nval\n\n        print(f\"beta2={beta2} Epoch {epoch}: val_loss={val_loss:.4f}\")\n        m_tr.append(train_align)\n        m_val.append(val_align)\n        l_tr.append(train_loss)\n        l_val.append(val_loss)\n\n    # record final preds\n    all_preds = []\n    with torch.no_grad():\n        for xb, _ in val_loader:\n            xb = xb.to(device)\n            preds = torch.argmax(ai_model(xb), 1).cpu().numpy()\n            all_preds.append(preds)\n    preds_runs.append(np.concatenate(all_preds, 0))\n    metrics_train_runs.append(m_tr)\n    metrics_val_runs.append(m_val)\n    losses_train_runs.append(l_tr)\n    losses_val_runs.append(l_val)\n\n# assemble experiment_data\nexperiment_data = {\n    \"adam_beta2\": {\n        \"synthetic\": {\n            \"beta2\": np.array(beta2_list),\n            \"epochs\": np.arange(1, num_epochs + 1),\n            \"metrics\": {\n                \"train\": np.array(metrics_train_runs),\n                \"val\": np.array(metrics_val_runs),\n            },\n            \"losses\": {\n                \"train\": np.array(losses_train_runs),\n                \"val\": np.array(losses_val_runs),\n            },\n            \"predictions\": np.array(preds_runs),\n            \"ground_truth\": y_val,\n        }\n    }\n}\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# prepare working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device & seeds\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# synthetic data\nN_train, N_val, D, C = 1000, 200, 10, 3\nW_true = np.random.randn(D, C)\nx_train = np.random.randn(N_train, D)\nx_val = np.random.randn(N_val, D)\ny_train = np.argmax(x_train @ W_true + 0.1 * np.random.randn(N_train, C), axis=1)\ny_val = np.argmax(x_val @ W_true + 0.1 * np.random.randn(N_val, C), axis=1)\nmean = x_train.mean(axis=0)\nstd = x_train.std(axis=0) + 1e-8\nx_train = (x_train - mean) / std\nx_val = (x_val - mean) / std\n\n# dataloaders\nbatch_size = 64\ntrain_loader = DataLoader(\n    TensorDataset(\n        torch.tensor(x_train, dtype=torch.float32),\n        torch.tensor(y_train, dtype=torch.long),\n    ),\n    batch_size=batch_size,\n    shuffle=True,\n)\nval_loader = DataLoader(\n    TensorDataset(\n        torch.tensor(x_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long)\n    ),\n    batch_size=batch_size,\n)\n\n\n# simple MLP\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\n# hyperparameter tuning: StepLR gammas\ngammas = [0.1, 0.5, 0.9]\nstep_size = 5\nnum_epochs = 10\nloss_fn = nn.CrossEntropyLoss()\n\nexperiment_data = {\"lr_scheduler_gamma\": {\"synthetic\": {}}}\n\nfor gamma in gammas:\n    key = f\"gamma_{gamma}\"\n    # init storage for this gamma\n    experiment_data[\"lr_scheduler_gamma\"][\"synthetic\"][key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"epochs\": list(range(1, num_epochs + 1)),\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # fresh models & optimizers & schedulers\n    ai_model = MLP(D, 32, C).to(device)\n    user_model = MLP(D, 32, C).to(device)\n    optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n    optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n    scheduler_ai = torch.optim.lr_scheduler.StepLR(\n        optimizer_ai, step_size=step_size, gamma=gamma\n    )\n    scheduler_user = torch.optim.lr_scheduler.StepLR(\n        optimizer_user, step_size=step_size, gamma=gamma\n    )\n\n    for epoch in range(1, num_epochs + 1):\n        # training\n        ai_model.train()\n        user_model.train()\n        train_loss_sum = 0.0\n        train_jsd_sum = 0.0\n        train_samples = 0\n        for x_b, y_b in train_loader:\n            x_b, y_b = x_b.to(device), y_b.to(device)\n            ai_logits = ai_model(x_b)\n            user_logits = user_model(x_b)\n            loss_ai = loss_fn(ai_logits, y_b)\n            loss_user = loss_fn(user_logits, y_b)\n            optimizer_ai.zero_grad()\n            loss_ai.backward()\n            optimizer_ai.step()\n            optimizer_user.zero_grad()\n            loss_user.backward()\n            optimizer_user.step()\n            bs = y_b.size(0)\n            train_loss_sum += loss_ai.item() * bs\n            P = F.softmax(ai_logits, dim=1)\n            Q = F.softmax(user_logits, dim=1)\n            M = 0.5 * (P + Q)\n            kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            jsd = 0.5 * (kl1 + kl2)\n            train_jsd_sum += torch.sum(1 - jsd).item()\n            train_samples += bs\n        train_loss = train_loss_sum / len(train_loader.dataset)\n        train_align = train_jsd_sum / train_samples\n\n        # validation\n        ai_model.eval()\n        user_model.eval()\n        val_loss_sum = 0.0\n        val_jsd_sum = 0.0\n        val_samples = 0\n        with torch.no_grad():\n            for x_b, y_b in val_loader:\n                x_b, y_b = x_b.to(device), y_b.to(device)\n                ai_logits = ai_model(x_b)\n                bs = y_b.size(0)\n                val_loss_sum += loss_fn(ai_logits, y_b).item() * bs\n                P = F.softmax(ai_logits, dim=1)\n                Q = F.softmax(user_model(x_b), dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                val_jsd_sum += torch.sum(1 - jsd).item()\n                val_samples += bs\n        val_loss = val_loss_sum / len(val_loader.dataset)\n        val_align = val_jsd_sum / val_samples\n        print(f\"Gamma {gamma} Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n        # record\n        exp = experiment_data[\"lr_scheduler_gamma\"][\"synthetic\"][key]\n        exp[\"metrics\"][\"train\"].append(train_align)\n        exp[\"metrics\"][\"val\"].append(val_align)\n        exp[\"losses\"][\"train\"].append(train_loss)\n        exp[\"losses\"][\"val\"].append(val_loss)\n\n        # step schedulers\n        scheduler_ai.step()\n        scheduler_user.step()\n\n    # final predictions\n    all_preds, all_gts = [], []\n    ai_model.eval()\n    with torch.no_grad():\n        for x_b, y_b in val_loader:\n            x_b = x_b.to(device)\n            preds = torch.argmax(ai_model(x_b), dim=1).cpu().numpy()\n            all_preds.append(preds)\n            all_gts.append(y_b.numpy())\n    all_preds = np.concatenate(all_preds, axis=0)\n    all_gts = np.concatenate(all_gts, axis=0)\n    experiment_data[\"lr_scheduler_gamma\"][\"synthetic\"][key][\"predictions\"] = all_preds\n    experiment_data[\"lr_scheduler_gamma\"][\"synthetic\"][key][\"ground_truth\"] = all_gts\n\n# save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.nn.utils import clip_grad_norm_\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Synthetic data generation\ntorch.manual_seed(0)\nnp.random.seed(0)\nN_train, N_val, D, C = 1000, 200, 10, 3\nW_true = np.random.randn(D, C)\nx_train = np.random.randn(N_train, D)\nx_val = np.random.randn(N_val, D)\ny_train = np.argmax(x_train @ W_true + 0.1 * np.random.randn(N_train, C), axis=1)\ny_val = np.argmax(x_val @ W_true + 0.1 * np.random.randn(N_val, C), axis=1)\nmean, std = x_train.mean(0), x_train.std(0) + 1e-8\nx_train = (x_train - mean) / std\nx_val = (x_val - mean) / std\ntrain_ds = TensorDataset(\n    torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n)\nval_ds = TensorDataset(\n    torch.tensor(x_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long)\n)\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n\n\n# Model definition\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nloss_fn = nn.CrossEntropyLoss()\nthresholds = [0.5, 1.0, 5.0, 10.0]\nnum_epochs = 10\n\nexperiment_data = {\"max_grad_norm\": {}}\n\nfor max_norm in thresholds:\n    # Reproducibility per run\n    torch.manual_seed(0)\n    np.random.seed(0)\n    # (re)initialize models & optimizers\n    ai_model = MLP(D, 32, C).to(device)\n    user_model = MLP(D, 32, C).to(device)\n    optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n    optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n    # Storage\n    metrics = {\"train\": [], \"val\": []}\n    losses = {\"train\": [], \"val\": []}\n    epochs_list = []\n    # Training loop\n    for epoch in range(1, num_epochs + 1):\n        ai_model.train()\n        user_model.train()\n        train_loss_sum, train_jsd_sum, train_samples = 0.0, 0.0, 0\n        for x_batch, y_batch in train_loader:\n            x, y = x_batch.to(device), y_batch.to(device)\n            # AI update\n            optimizer_ai.zero_grad()\n            logits_ai = ai_model(x)\n            l_ai = loss_fn(logits_ai, y)\n            l_ai.backward()\n            clip_grad_norm_(ai_model.parameters(), max_norm)\n            optimizer_ai.step()\n            # User update\n            optimizer_user.zero_grad()\n            logits_u = user_model(x)\n            l_u = loss_fn(logits_u, y)\n            l_u.backward()\n            clip_grad_norm_(user_model.parameters(), max_norm)\n            optimizer_user.step()\n            bs = y.size(0)\n            train_loss_sum += l_ai.item() * bs\n            P = F.softmax(logits_ai, dim=1)\n            Q = F.softmax(logits_u, dim=1)\n            M = 0.5 * (P + Q)\n            kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            jsd = 0.5 * (kl1 + kl2)\n            train_jsd_sum += torch.sum(1 - jsd).item()\n            train_samples += bs\n        train_loss = train_loss_sum / len(train_loader.dataset)\n        train_align = train_jsd_sum / train_samples\n        # Validation\n        ai_model.eval()\n        user_model.eval()\n        val_loss_sum, val_jsd_sum, val_samples = 0.0, 0.0, 0\n        with torch.no_grad():\n            for x_batch, y_batch in val_loader:\n                x, y = x_batch.to(device), y_batch.to(device)\n                logits_ai = ai_model(x)\n                bs = y.size(0)\n                val_loss_sum += loss_fn(logits_ai, y).item() * bs\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(user_model(x), dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                val_jsd_sum += torch.sum(1 - jsd).item()\n                val_samples += bs\n        val_loss = val_loss_sum / len(val_loader.dataset)\n        val_align = val_jsd_sum / val_samples\n        # Record\n        metrics[\"train\"].append(train_align)\n        metrics[\"val\"].append(val_align)\n        losses[\"train\"].append(train_loss)\n        losses[\"val\"].append(val_loss)\n        epochs_list.append(epoch)\n        print(f\"max_grad_norm={max_norm} Epoch {epoch}: val_loss={val_loss:.4f}\")\n    # Final predictions\n    all_preds, all_gts = [], []\n    with torch.no_grad():\n        for x_batch, y_batch in val_loader:\n            x = x_batch.to(device)\n            preds = torch.argmax(ai_model(x), dim=1).cpu().numpy()\n            all_preds.append(preds)\n            all_gts.append(y_batch.numpy())\n    all_preds = np.concatenate(all_preds)\n    all_gts = np.concatenate(all_gts)\n    # Store per threshold\n    experiment_data[\"max_grad_norm\"][str(max_norm)] = {\n        \"synthetic\": {\n            \"metrics\": metrics,\n            \"losses\": losses,\n            \"epochs\": epochs_list,\n            \"predictions\": all_preds,\n            \"ground_truth\": all_gts,\n        }\n    }\n\n# Save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup working directory and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Reproducibility\nseed = 0\ntorch.manual_seed(seed)\nnp.random.seed(seed)\n\n# Synthetic data generation\nN_train, N_val, D, C = 1000, 200, 10, 3\nW_true = np.random.randn(D, C)\nx_train = np.random.randn(N_train, D)\nx_val = np.random.randn(N_val, D)\ny_train = np.argmax(x_train @ W_true + 0.1 * np.random.randn(N_train, C), axis=1)\ny_val = np.argmax(x_val @ W_true + 0.1 * np.random.randn(N_val, C), axis=1)\n# Normalize features\nmean = x_train.mean(axis=0)\nstd = x_train.std(axis=0) + 1e-8\nx_train = (x_train - mean) / std\nx_val = (x_val - mean) / std\n\n# Create DataLoaders\ntrain_dataset = TensorDataset(\n    torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n)\nval_dataset = TensorDataset(\n    torch.tensor(x_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long)\n)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64)\n\n\n# Define simple MLP\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\n# Hyperparameter sweep over init_std\ninit_stds = [0.01, 0.1, 0.5, 1.0]\nnum_epochs = 10\nexperiment_data = {\"init_std\": {}}\n\nfor init_std in init_stds:\n    # Prepare storage\n    exp = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"epochs\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # Instantiate and initialize models\n    ai_model = MLP(D, 32, C).to(device)\n    user_model = MLP(D, 32, C).to(device)\n    for model in (ai_model, user_model):\n        for m in model.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, mean=0.0, std=init_std)\n                nn.init.zeros_(m.bias)\n    optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n    optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n    loss_fn = nn.CrossEntropyLoss()\n\n    # Training loop\n    for epoch in range(1, num_epochs + 1):\n        ai_model.train()\n        user_model.train()\n        train_loss_sum, train_jsd_sum, train_samples = 0.0, 0.0, 0\n        for x_batch, y_batch in train_loader:\n            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n            # Forward & backward for AI model\n            ai_logits = ai_model(x_batch)\n            loss_ai = loss_fn(ai_logits, y_batch)\n            optimizer_ai.zero_grad()\n            loss_ai.backward()\n            optimizer_ai.step()\n            # Forward & backward for User model\n            user_logits = user_model(x_batch)\n            loss_user = loss_fn(user_logits, y_batch)\n            optimizer_user.zero_grad()\n            loss_user.backward()\n            optimizer_user.step()\n            # Accumulate loss and alignment\n            bs = y_batch.size(0)\n            train_loss_sum += loss_ai.item() * bs\n            P = F.softmax(ai_logits, dim=1)\n            Q = F.softmax(user_logits, dim=1)\n            M = 0.5 * (P + Q)\n            kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            jsd = 0.5 * (kl1 + kl2)\n            train_jsd_sum += torch.sum(1 - jsd).item()\n            train_samples += bs\n        train_loss = train_loss_sum / len(train_dataset)\n        train_align = train_jsd_sum / train_samples\n\n        # Validation\n        ai_model.eval()\n        user_model.eval()\n        val_loss_sum, val_jsd_sum, val_samples = 0.0, 0.0, 0\n        with torch.no_grad():\n            for x_batch, y_batch in val_loader:\n                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n                ai_logits = ai_model(x_batch)\n                loss_val = loss_fn(ai_logits, y_batch)\n                bs = y_batch.size(0)\n                val_loss_sum += loss_val.item() * bs\n                P = F.softmax(ai_logits, dim=1)\n                Q = F.softmax(user_model(x_batch), dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                val_jsd_sum += torch.sum(1 - jsd).item()\n                val_samples += bs\n        val_loss = val_loss_sum / len(val_dataset)\n        val_align = val_jsd_sum / val_samples\n\n        # Record\n        exp[\"metrics\"][\"train\"].append(train_align)\n        exp[\"metrics\"][\"val\"].append(val_align)\n        exp[\"losses\"][\"train\"].append(train_loss)\n        exp[\"losses\"][\"val\"].append(val_loss)\n        exp[\"epochs\"].append(epoch)\n        print(f\"init_std={init_std} epoch={epoch} val_loss={val_loss:.4f}\")\n\n    # Final validation predictions\n    all_preds, all_gts = [], []\n    with torch.no_grad():\n        for x_batch, y_batch in val_loader:\n            x_batch = x_batch.to(device)\n            preds = torch.argmax(ai_model(x_batch), dim=1).cpu().numpy()\n            all_preds.append(preds)\n            all_gts.append(y_batch.numpy())\n    exp[\"predictions\"] = np.concatenate(all_preds, axis=0)\n    exp[\"ground_truth\"] = np.concatenate(all_gts, axis=0)\n\n    experiment_data[\"init_std\"][str(init_std)] = exp\n\n# Convert all lists to numpy arrays\nfor std_key, exp in experiment_data[\"init_std\"].items():\n    exp[\"metrics\"][\"train\"] = np.array(exp[\"metrics\"][\"train\"])\n    exp[\"metrics\"][\"val\"] = np.array(exp[\"metrics\"][\"val\"])\n    exp[\"losses\"][\"train\"] = np.array(exp[\"losses\"][\"train\"])\n    exp[\"losses\"][\"val\"] = np.array(exp[\"losses\"][\"val\"])\n    exp[\"epochs\"] = np.array(exp[\"epochs\"])\n    exp[\"predictions\"] = np.array(exp[\"predictions\"])\n    exp[\"ground_truth\"] = np.array(exp[\"ground_truth\"])\n\n# Save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Reproducibility\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Synthetic data gen\nN_train, N_val, D, C = 1000, 200, 10, 3\nW_true = np.random.randn(D, C)\nx_train = np.random.randn(N_train, D)\nx_val = np.random.randn(N_val, D)\ny_train = np.argmax(x_train @ W_true + 0.1 * np.random.randn(N_train, C), axis=1)\ny_val = np.argmax(x_val @ W_true + 0.1 * np.random.randn(N_val, C), axis=1)\nmean, std = x_train.mean(0), x_train.std(0) + 1e-8\nx_train = (x_train - mean) / std\nx_val = (x_val - mean) / std\n\ntrain_dataset = TensorDataset(\n    torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n)\nval_dataset = TensorDataset(\n    torch.tensor(x_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long)\n)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64)\n\n\n# Simple MLP\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\nloss_fn = nn.CrossEntropyLoss()\nnum_epochs = 10\nlambda_list = [0.0, 0.1, 0.5, 1.0]\n\n# Experiment data structure\nexperiment_data = {\n    \"jsd_alignment_weight\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"lambdas\": [],\n        }\n    }\n}\n\nfor lam in lambda_list:\n    # reset for reproducibility\n    torch.manual_seed(0)\n    np.random.seed(0)\n    ai_model = MLP(D, 32, C).to(device)\n    user_model = MLP(D, 32, C).to(device)\n    optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=1e-3)\n    optimizer_user = torch.optim.Adam(user_model.parameters(), lr=1e-3)\n\n    train_loss_hist, val_loss_hist = [], []\n    train_align_hist, val_align_hist = [], []\n\n    for epoch in range(1, num_epochs + 1):\n        ai_model.train()\n        user_model.train()\n        train_loss_sum = 0.0\n        train_jsd_sum = 0.0\n        n_samples = 0\n\n        for x_batch, y_batch in train_loader:\n            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n            ai_logits = ai_model(x_batch)\n            user_logits = user_model(x_batch)\n\n            loss_ai = loss_fn(ai_logits, y_batch)\n            loss_user = loss_fn(user_logits, y_batch)\n\n            P = F.softmax(ai_logits, dim=1)\n            Q = F.softmax(user_logits, dim=1)\n            M = 0.5 * (P + Q)\n            kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            jsd = 0.5 * (kl1 + kl2)\n            jsd_mean = jsd.mean()\n\n            total_loss = loss_ai + loss_user + lam * jsd_mean\n            optimizer_ai.zero_grad()\n            optimizer_user.zero_grad()\n            total_loss.backward()\n            optimizer_ai.step()\n            optimizer_user.step()\n\n            bs = y_batch.size(0)\n            train_loss_sum += loss_ai.item() * bs\n            train_jsd_sum += torch.sum(1 - jsd).item()\n            n_samples += bs\n\n        train_loss = train_loss_sum / len(train_dataset)\n        train_align = train_jsd_sum / n_samples\n        train_loss_hist.append(train_loss)\n        train_align_hist.append(train_align)\n\n        # validation\n        ai_model.eval()\n        val_loss_sum = 0.0\n        val_jsd_sum = 0.0\n        v_samples = 0\n        with torch.no_grad():\n            for x_batch, y_batch in val_loader:\n                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n                ai_logits = ai_model(x_batch)\n                loss_val = loss_fn(ai_logits, y_batch)\n\n                P = F.softmax(ai_logits, dim=1)\n                Q = F.softmax(user_model(x_batch), dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n\n                bs = y_batch.size(0)\n                val_loss_sum += loss_val.item() * bs\n                val_jsd_sum += torch.sum(1 - jsd).item()\n                v_samples += bs\n\n        val_loss = val_loss_sum / len(val_dataset)\n        val_align = val_jsd_sum / v_samples\n        val_loss_hist.append(val_loss)\n        val_align_hist.append(val_align)\n        print(\n            f\"\u03bb={lam} Epoch {epoch}: val_loss={val_loss:.4f}, val_align={val_align:.4f}\"\n        )\n\n    # final predictions\n    preds_list, gts_list = [], []\n    with torch.no_grad():\n        for x_batch, y_batch in val_loader:\n            x_batch = x_batch.to(device)\n            preds = torch.argmax(ai_model(x_batch), dim=1).cpu().numpy()\n            preds_list.append(preds)\n            gts_list.append(y_batch.numpy())\n    preds_arr = np.concatenate(preds_list, axis=0)\n    gts_arr = np.concatenate(gts_list, axis=0)\n\n    # save into experiment_data\n    sd = experiment_data[\"jsd_alignment_weight\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(train_align_hist)\n    sd[\"metrics\"][\"val\"].append(val_align_hist)\n    sd[\"losses\"][\"train\"].append(train_loss_hist)\n    sd[\"losses\"][\"val\"].append(val_loss_hist)\n    sd[\"predictions\"].append(preds_arr)\n    sd[\"ground_truth\"].append(gts_arr)\n    sd[\"lambdas\"].append(lam)\n\n# final save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Reproducibility\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Synthetic data generation\nN_train, N_val, D, C = 1000, 200, 10, 3\nW_true = np.random.randn(D, C)\nx_train = np.random.randn(N_train, D)\nx_val = np.random.randn(N_val, D)\ny_train = np.argmax(x_train @ W_true + 0.1 * np.random.randn(N_train, C), axis=1)\ny_val = np.argmax(x_val @ W_true + 0.1 * np.random.randn(N_val, C), axis=1)\nmean = x_train.mean(axis=0)\nstd = x_train.std(axis=0) + 1e-8\nx_train = (x_train - mean) / std\nx_val = (x_val - mean) / std\n\n# DataLoaders\ntrain_dataset = TensorDataset(\n    torch.tensor(x_train, dtype=torch.float32),\n    torch.tensor(y_train, dtype=torch.long),\n)\nval_dataset = TensorDataset(\n    torch.tensor(x_val, dtype=torch.float32),\n    torch.tensor(y_val, dtype=torch.long),\n)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64)\n\n\n# Simple MLP definition\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\n# Hyperparameter sweep over learning rates\nlearning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\nnum_epochs = 10\n\n# Initialize experiment_data dict\nexperiment_data = {\n    \"learning_rate\": {\n        \"synthetic\": {\n            \"lrs\": learning_rates,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nloss_fn = nn.CrossEntropyLoss()\n\nfor lr in learning_rates:\n    # Reset model weights for fair comparison\n    torch.manual_seed(0)\n    ai_model = MLP(D, 32, C).to(device)\n    user_model = MLP(D, 32, C).to(device)\n    optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=lr)\n    optimizer_user = torch.optim.Adam(user_model.parameters(), lr=lr)\n\n    # Containers for this learning rate\n    train_losses, val_losses = [], []\n    train_aligns, val_aligns = [], []\n\n    for epoch in range(1, num_epochs + 1):\n        # Training\n        ai_model.train()\n        user_model.train()\n        total_loss, total_align, n_samples = 0.0, 0.0, 0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            # Forward passes\n            logits_ai = ai_model(xb)\n            logits_user = user_model(xb)\n            # Cross-entropy losses\n            loss_ai = loss_fn(logits_ai, yb)\n            loss_user = loss_fn(logits_user, yb)\n            # Backprop AI\n            optimizer_ai.zero_grad()\n            loss_ai.backward()\n            optimizer_ai.step()\n            # Backprop user\n            optimizer_user.zero_grad()\n            loss_user.backward()\n            optimizer_user.step()\n            # Accumulate loss\n            bs = yb.size(0)\n            total_loss += loss_ai.item() * bs\n            # Alignment (1 - JSD)\n            P = F.softmax(logits_ai, dim=1)\n            Q = F.softmax(logits_user, dim=1)\n            M = 0.5 * (P + Q)\n            kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            jsd = 0.5 * (kl1 + kl2)\n            total_align += torch.sum(1 - jsd).item()\n            n_samples += bs\n        train_losses.append(total_loss / len(train_dataset))\n        train_aligns.append(total_align / n_samples)\n\n        # Validation\n        ai_model.eval()\n        user_model.eval()\n        v_loss, v_align, v_samples = 0.0, 0.0, 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits_ai = ai_model(xb)\n                # loss\n                v_loss += loss_fn(logits_ai, yb).item() * yb.size(0)\n                # alignment\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(user_model(xb), dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                v_align += torch.sum(1 - jsd).item()\n                v_samples += yb.size(0)\n        val_losses.append(v_loss / len(val_dataset))\n        val_aligns.append(v_align / v_samples)\n        print(\n            f\"LR {lr:.1e} Epoch {epoch}: val_loss = {val_losses[-1]:.4f}, val_align = {val_aligns[-1]:.4f}\"\n        )\n\n    # Store per\u2010epoch metrics\n    sd = experiment_data[\"learning_rate\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(train_aligns)\n    sd[\"metrics\"][\"val\"].append(val_aligns)\n    sd[\"losses\"][\"train\"].append(train_losses)\n    sd[\"losses\"][\"val\"].append(val_losses)\n\n    # Final validation predictions & ground truth\n    all_preds, all_gts = [], []\n    ai_model.eval()\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            xb = xb.to(device)\n            preds = torch.argmax(ai_model(xb), dim=1).cpu().numpy()\n            all_preds.append(preds)\n            all_gts.append(yb.numpy())\n    preds_arr = np.concatenate(all_preds, axis=0)\n    gts_arr = np.concatenate(all_gts, axis=0)\n    sd[\"predictions\"].append(preds_arr)\n    sd[\"ground_truth\"].append(gts_arr)\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Reproducibility\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Synthetic data generation\nN_train, N_val, D, C = 1000, 200, 10, 3\nW_true = np.random.randn(D, C)\nx_train = np.random.randn(N_train, D)\nx_val = np.random.randn(N_val, D)\ny_train = np.argmax(x_train @ W_true + 0.1 * np.random.randn(N_train, C), axis=1)\ny_val = np.argmax(x_val @ W_true + 0.1 * np.random.randn(N_val, C), axis=1)\nmean = x_train.mean(axis=0)\nstd = x_train.std(axis=0) + 1e-8\nx_train = (x_train - mean) / std\nx_val = (x_val - mean) / std\n\n# DataLoaders\ntrain_dataset = TensorDataset(\n    torch.tensor(x_train, dtype=torch.float32),\n    torch.tensor(y_train, dtype=torch.long),\n)\nval_dataset = TensorDataset(\n    torch.tensor(x_val, dtype=torch.float32),\n    torch.tensor(y_val, dtype=torch.long),\n)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64)\n\n\n# Simple MLP definition\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\n# Hyperparameter sweep over learning rates\nlearning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\nnum_epochs = 10\n\n# Initialize experiment_data dict\nexperiment_data = {\n    \"learning_rate\": {\n        \"synthetic\": {\n            \"lrs\": learning_rates,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nloss_fn = nn.CrossEntropyLoss()\n\nfor lr in learning_rates:\n    # Reset model weights for fair comparison\n    torch.manual_seed(0)\n    ai_model = MLP(D, 32, C).to(device)\n    user_model = MLP(D, 32, C).to(device)\n    optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=lr)\n    optimizer_user = torch.optim.Adam(user_model.parameters(), lr=lr)\n\n    # Containers for this learning rate\n    train_losses, val_losses = [], []\n    train_aligns, val_aligns = [], []\n\n    for epoch in range(1, num_epochs + 1):\n        # Training\n        ai_model.train()\n        user_model.train()\n        total_loss, total_align, n_samples = 0.0, 0.0, 0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            # Forward passes\n            logits_ai = ai_model(xb)\n            logits_user = user_model(xb)\n            # Cross-entropy losses\n            loss_ai = loss_fn(logits_ai, yb)\n            loss_user = loss_fn(logits_user, yb)\n            # Backprop AI\n            optimizer_ai.zero_grad()\n            loss_ai.backward()\n            optimizer_ai.step()\n            # Backprop user\n            optimizer_user.zero_grad()\n            loss_user.backward()\n            optimizer_user.step()\n            # Accumulate loss\n            bs = yb.size(0)\n            total_loss += loss_ai.item() * bs\n            # Alignment (1 - JSD)\n            P = F.softmax(logits_ai, dim=1)\n            Q = F.softmax(logits_user, dim=1)\n            M = 0.5 * (P + Q)\n            kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            jsd = 0.5 * (kl1 + kl2)\n            total_align += torch.sum(1 - jsd).item()\n            n_samples += bs\n        train_losses.append(total_loss / len(train_dataset))\n        train_aligns.append(total_align / n_samples)\n\n        # Validation\n        ai_model.eval()\n        user_model.eval()\n        v_loss, v_align, v_samples = 0.0, 0.0, 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits_ai = ai_model(xb)\n                # loss\n                v_loss += loss_fn(logits_ai, yb).item() * yb.size(0)\n                # alignment\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(user_model(xb), dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                v_align += torch.sum(1 - jsd).item()\n                v_samples += yb.size(0)\n        val_losses.append(v_loss / len(val_dataset))\n        val_aligns.append(v_align / v_samples)\n        print(\n            f\"LR {lr:.1e} Epoch {epoch}: val_loss = {val_losses[-1]:.4f}, val_align = {val_aligns[-1]:.4f}\"\n        )\n\n    # Store per\u2010epoch metrics\n    sd = experiment_data[\"learning_rate\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(train_aligns)\n    sd[\"metrics\"][\"val\"].append(val_aligns)\n    sd[\"losses\"][\"train\"].append(train_losses)\n    sd[\"losses\"][\"val\"].append(val_losses)\n\n    # Final validation predictions & ground truth\n    all_preds, all_gts = [], []\n    ai_model.eval()\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            xb = xb.to(device)\n            preds = torch.argmax(ai_model(xb), dim=1).cpu().numpy()\n            all_preds.append(preds)\n            all_gts.append(yb.numpy())\n    preds_arr = np.concatenate(all_preds, axis=0)\n    gts_arr = np.concatenate(all_gts, axis=0)\n    sd[\"predictions\"].append(preds_arr)\n    sd[\"ground_truth\"].append(gts_arr)\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Reproducibility\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Synthetic data generation\nN_train, N_val, D, C = 1000, 200, 10, 3\nW_true = np.random.randn(D, C)\nx_train = np.random.randn(N_train, D)\nx_val = np.random.randn(N_val, D)\ny_train = np.argmax(x_train @ W_true + 0.1 * np.random.randn(N_train, C), axis=1)\ny_val = np.argmax(x_val @ W_true + 0.1 * np.random.randn(N_val, C), axis=1)\nmean = x_train.mean(axis=0)\nstd = x_train.std(axis=0) + 1e-8\nx_train = (x_train - mean) / std\nx_val = (x_val - mean) / std\n\n# DataLoaders\ntrain_dataset = TensorDataset(\n    torch.tensor(x_train, dtype=torch.float32),\n    torch.tensor(y_train, dtype=torch.long),\n)\nval_dataset = TensorDataset(\n    torch.tensor(x_val, dtype=torch.float32),\n    torch.tensor(y_val, dtype=torch.long),\n)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64)\n\n\n# Simple MLP definition\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hid_dim)\n        self.fc2 = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\n# Hyperparameter sweep over learning rates\nlearning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\nnum_epochs = 10\n\n# Initialize experiment_data dict\nexperiment_data = {\n    \"learning_rate\": {\n        \"synthetic\": {\n            \"lrs\": learning_rates,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nloss_fn = nn.CrossEntropyLoss()\n\nfor lr in learning_rates:\n    # Reset model weights for fair comparison\n    torch.manual_seed(0)\n    ai_model = MLP(D, 32, C).to(device)\n    user_model = MLP(D, 32, C).to(device)\n    optimizer_ai = torch.optim.Adam(ai_model.parameters(), lr=lr)\n    optimizer_user = torch.optim.Adam(user_model.parameters(), lr=lr)\n\n    # Containers for this learning rate\n    train_losses, val_losses = [], []\n    train_aligns, val_aligns = [], []\n\n    for epoch in range(1, num_epochs + 1):\n        # Training\n        ai_model.train()\n        user_model.train()\n        total_loss, total_align, n_samples = 0.0, 0.0, 0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            # Forward passes\n            logits_ai = ai_model(xb)\n            logits_user = user_model(xb)\n            # Cross-entropy losses\n            loss_ai = loss_fn(logits_ai, yb)\n            loss_user = loss_fn(logits_user, yb)\n            # Backprop AI\n            optimizer_ai.zero_grad()\n            loss_ai.backward()\n            optimizer_ai.step()\n            # Backprop user\n            optimizer_user.zero_grad()\n            loss_user.backward()\n            optimizer_user.step()\n            # Accumulate loss\n            bs = yb.size(0)\n            total_loss += loss_ai.item() * bs\n            # Alignment (1 - JSD)\n            P = F.softmax(logits_ai, dim=1)\n            Q = F.softmax(logits_user, dim=1)\n            M = 0.5 * (P + Q)\n            kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n            jsd = 0.5 * (kl1 + kl2)\n            total_align += torch.sum(1 - jsd).item()\n            n_samples += bs\n        train_losses.append(total_loss / len(train_dataset))\n        train_aligns.append(total_align / n_samples)\n\n        # Validation\n        ai_model.eval()\n        user_model.eval()\n        v_loss, v_align, v_samples = 0.0, 0.0, 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits_ai = ai_model(xb)\n                # loss\n                v_loss += loss_fn(logits_ai, yb).item() * yb.size(0)\n                # alignment\n                P = F.softmax(logits_ai, dim=1)\n                Q = F.softmax(user_model(xb), dim=1)\n                M = 0.5 * (P + Q)\n                kl1 = torch.sum(P * (torch.log(P + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                kl2 = torch.sum(Q * (torch.log(Q + 1e-8) - torch.log(M + 1e-8)), dim=1)\n                jsd = 0.5 * (kl1 + kl2)\n                v_align += torch.sum(1 - jsd).item()\n                v_samples += yb.size(0)\n        val_losses.append(v_loss / len(val_dataset))\n        val_aligns.append(v_align / v_samples)\n        print(\n            f\"LR {lr:.1e} Epoch {epoch}: val_loss = {val_losses[-1]:.4f}, val_align = {val_aligns[-1]:.4f}\"\n        )\n\n    # Store per\u2010epoch metrics\n    sd = experiment_data[\"learning_rate\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(train_aligns)\n    sd[\"metrics\"][\"val\"].append(val_aligns)\n    sd[\"losses\"][\"train\"].append(train_losses)\n    sd[\"losses\"][\"val\"].append(val_losses)\n\n    # Final validation predictions & ground truth\n    all_preds, all_gts = [], []\n    ai_model.eval()\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            xb = xb.to(device)\n            preds = torch.argmax(ai_model(xb), dim=1).cpu().numpy()\n            all_preds.append(preds)\n            all_gts.append(yb.numpy())\n    preds_arr = np.concatenate(all_preds, axis=0)\n    gts_arr = np.concatenate(all_gts, axis=0)\n    sd[\"predictions\"].append(preds_arr)\n    sd[\"ground_truth\"].append(gts_arr)\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 1.0102', '\\n', 'Epoch\n2: validation_loss = 0.9538', '\\n', 'Epoch 3: validation_loss = 0.8995', '\\n',\n'Epoch 4: validation_loss = 0.8438', '\\n', 'Epoch 5: validation_loss = 0.7864',\n'\\n', 'Epoch 6: validation_loss = 0.7292', '\\n', 'Epoch 7: validation_loss =\n0.6713', '\\n', 'Epoch 8: validation_loss = 0.6147', '\\n', 'Epoch 9:\nvalidation_loss = 0.5608', '\\n', 'Epoch 10: validation_loss = 0.5101', '\\n',\n'Execution time: a second seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'LR 1.0e-04 Epoch 1: val_loss = 1.0651, val_align =\n0.9922', '\\n', 'LR 1.0e-04 Epoch 2: val_loss = 1.0588, val_align = 0.9923',\n'\\n', 'LR 1.0e-04 Epoch 3: val_loss = 1.0526, val_align = 0.9924', '\\n', 'LR\n1.0e-04 Epoch 4: val_loss = 1.0464, val_align = 0.9925', '\\n', 'LR 1.0e-04 Epoch\n5: val_loss = 1.0403, val_align = 0.9926', '\\n', 'LR 1.0e-04 Epoch 6: val_loss =\n1.0343, val_align = 0.9926', '\\n', 'LR 1.0e-04 Epoch 7: val_loss = 1.0283,\nval_align = 0.9927', '\\n', 'LR 1.0e-04 Epoch 8: val_loss = 1.0224, val_align =\n0.9928', '\\n', 'LR 1.0e-04 Epoch 9: val_loss = 1.0165, val_align = 0.9928',\n'\\n', 'LR 1.0e-04 Epoch 10: val_loss = 1.0106, val_align = 0.9929', '\\n', 'LR\n5.0e-04 Epoch 1: val_loss = 1.0400, val_align = 0.9926', '\\n', 'LR 5.0e-04 Epoch\n2: val_loss = 1.0104, val_align = 0.9929', '\\n', 'LR 5.0e-04 Epoch 3: val_loss =\n0.9821, val_align = 0.9931', '\\n', 'LR 5.0e-04 Epoch 4: val_loss = 0.9540,\nval_align = 0.9931', '\\n', 'LR 5.0e-04 Epoch 5: val_loss = 0.9259, val_align =\n0.9930', '\\n', 'LR 5.0e-04 Epoch 6: val_loss = 0.8980, val_align = 0.9927',\n'\\n', 'LR 5.0e-04 Epoch 7: val_loss = 0.8697, val_align = 0.9925', '\\n', 'LR\n5.0e-04 Epoch 8: val_loss = 0.8407, val_align = 0.9922', '\\n', 'LR 5.0e-04 Epoch\n9: val_loss = 0.8115, val_align = 0.9918', '\\n', 'LR 5.0e-04 Epoch 10: val_loss\n= 0.7817, val_align = 0.9915', '\\n', 'LR 1.0e-03 Epoch 1: val_loss = 1.0102,\nval_align = 0.9930', '\\n', 'LR 1.0e-03 Epoch 2: val_loss = 0.9538, val_align =\n0.9932', '\\n', 'LR 1.0e-03 Epoch 3: val_loss = 0.8995, val_align = 0.9928',\n'\\n', 'LR 1.0e-03 Epoch 4: val_loss = 0.8438, val_align = 0.9922', '\\n', 'LR\n1.0e-03 Epoch 5: val_loss = 0.7864, val_align = 0.9916', '\\n', 'LR 1.0e-03 Epoch\n6: val_loss = 0.7292, val_align = 0.9909', '\\n', 'LR 1.0e-03 Epoch 7: val_loss =\n0.6713, val_align = 0.9904', '\\n', 'LR 1.0e-03 Epoch 8: val_loss = 0.6147,\nval_align = 0.9902', '\\n', 'LR 1.0e-03 Epoch 9: val_loss = 0.5608, val_align =\n0.9903', '\\n', 'LR 1.0e-03 Epoch 10: val_loss = 0.5101, val_align = 0.9907',\n'\\n', 'LR 5.0e-03 Epoch 1: val_loss = 0.7999, val_align = 0.9919', '\\n', 'LR\n5.0e-03 Epoch 2: val_loss = 0.5444, val_align = 0.9895', '\\n', 'LR 5.0e-03 Epoch\n3: val_loss = 0.3476, val_align = 0.9923', '\\n', 'LR 5.0e-03 Epoch 4: val_loss =\n0.2409, val_align = 0.9956', '\\n', 'LR 5.0e-03 Epoch 5: val_loss = 0.1822,\nval_align = 0.9971', '\\n', 'LR 5.0e-03 Epoch 6: val_loss = 0.1579, val_align =\n0.9980', '\\n', 'LR 5.0e-03 Epoch 7: val_loss = 0.1343, val_align = 0.9983',\n'\\n', 'LR 5.0e-03 Epoch 8: val_loss = 0.1260, val_align = 0.9986', '\\n', 'LR\n5.0e-03 Epoch 9: val_loss = 0.1144, val_align = 0.9987', '\\n', 'LR 5.0e-03 Epoch\n10: val_loss = 0.1039, val_align = 0.9989', '\\n', 'LR 1.0e-02 Epoch 1: val_loss\n= 0.5694, val_align = 0.9887', '\\n', 'LR 1.0e-02 Epoch 2: val_loss = 0.2476,\nval_align = 0.9946', '\\n', 'LR 1.0e-02 Epoch 3: val_loss = 0.1484, val_align =\n0.9973', '\\n', 'LR 1.0e-02 Epoch 4: val_loss = 0.1135, val_align = 0.9985',\n'\\n', 'LR 1.0e-02 Epoch 5: val_loss = 0.0999, val_align = 0.9986', '\\n', 'LR\n1.0e-02 Epoch 6: val_loss = 0.0867, val_align = 0.9988', '\\n', 'LR 1.0e-02 Epoch\n7: val_loss = 0.0869, val_align = 0.9990', '\\n', 'LR 1.0e-02 Epoch 8: val_loss =\n0.0786, val_align = 0.9988', '\\n', 'LR 1.0e-02 Epoch 9: val_loss = 0.0752,\nval_align = 0.9990', '\\n', 'LR 1.0e-02 Epoch 10: val_loss = 0.0739, val_align =\n0.9986', '\\n', 'Execution time: 3 seconds seconds (time limit is an hour).']", "['Starting run with batch size = 16', '\\n', '[bs=16] Epoch 1: val_loss=0.9026,\nval_align=0.9928', '\\n', '[bs=16] Epoch 2: val_loss=0.7377, val_align=0.9910',\n'\\n', '[bs=16] Epoch 3: val_loss=0.5779, val_align=0.9903', '\\n', '[bs=16] Epoch\n4: val_loss=0.4478, val_align=0.9919', '\\n', '[bs=16] Epoch 5: val_loss=0.3553,\nval_align=0.9940', '\\n', '[bs=16] Epoch 6: val_loss=0.2916, val_align=0.9956',\n'\\n', '[bs=16] Epoch 7: val_loss=0.2463, val_align=0.9966', '\\n', '[bs=16] Epoch\n8: val_loss=0.2177, val_align=0.9973', '\\n', '[bs=16] Epoch 9: val_loss=0.1941,\nval_align=0.9977', '\\n', '[bs=16] Epoch 10: val_loss=0.1767, val_align=0.9980',\n'\\n', 'Starting run with batch size = 32', '\\n', '[bs=32] Epoch 1:\nval_loss=0.9659, val_align=0.9931', '\\n', '[bs=32] Epoch 2: val_loss=0.8680,\nval_align=0.9925', '\\n', '[bs=32] Epoch 3: val_loss=0.7701, val_align=0.9913',\n'\\n', '[bs=32] Epoch 4: val_loss=0.6704, val_align=0.9905', '\\n', '[bs=32] Epoch\n5: val_loss=0.5752, val_align=0.9904', '\\n', '[bs=32] Epoch 6: val_loss=0.4929,\nval_align=0.9911', '\\n', '[bs=32] Epoch 7: val_loss=0.4222, val_align=0.9922',\n'\\n', '[bs=32] Epoch 8: val_loss=0.3672, val_align=0.9935', '\\n', '[bs=32] Epoch\n9: val_loss=0.3225, val_align=0.9947', '\\n', '[bs=32] Epoch 10: val_loss=0.2876,\nval_align=0.9955', '\\n', 'Starting run with batch size = 128', '\\n', '[bs=128]\nEpoch 1: val_loss=1.0374, val_align=0.9927', '\\n', '[bs=128] Epoch 2:\nval_loss=1.0051, val_align=0.9931', '\\n', '[bs=128] Epoch 3: val_loss=0.9743,\nval_align=0.9932', '\\n', '[bs=128] Epoch 4: val_loss=0.9439, val_align=0.9932',\n'\\n', '[bs=128] Epoch 5: val_loss=0.9138, val_align=0.9931', '\\n', '[bs=128]\nEpoch 6: val_loss=0.8836, val_align=0.9928', '\\n', '[bs=128] Epoch 7:\nval_loss=0.8531, val_align=0.9924', '\\n', '[bs=128] Epoch 8: val_loss=0.8218,\nval_align=0.9921', '\\n', '[bs=128] Epoch 9: val_loss=0.7900, val_align=0.9917',\n'\\n', '[bs=128] Epoch 10: val_loss=0.7577, val_align=0.9913', '\\n', 'Starting\nrun with batch size = 256', '\\n', '[bs=256] Epoch 1: val_loss=1.0531,\nval_align=0.9925', '\\n', '[bs=256] Epoch 2: val_loss=1.0351, val_align=0.9928',\n'\\n', '[bs=256] Epoch 3: val_loss=1.0178, val_align=0.9930', '\\n', '[bs=256]\nEpoch 4: val_loss=1.0007, val_align=0.9932', '\\n', '[bs=256] Epoch 5:\nval_loss=0.9840, val_align=0.9933', '\\n', '[bs=256] Epoch 6: val_loss=0.9675,\nval_align=0.9933', '\\n', '[bs=256] Epoch 7: val_loss=0.9512, val_align=0.9933',\n'\\n', '[bs=256] Epoch 8: val_loss=0.9350, val_align=0.9932', '\\n', '[bs=256]\nEpoch 9: val_loss=0.9189, val_align=0.9931', '\\n', '[bs=256] Epoch 10:\nval_loss=0.9026, val_align=0.9930', '\\n', 'Execution time: 4 seconds seconds\n(time limit is an hour).']", "['Using device: cuda', '\\n', '=== Training with weight_decay=1e-05 ===', '\\n',\n'wd=1e-05 epoch=1 val_loss=1.0102', '\\n', 'wd=1e-05 epoch=2 val_loss=0.9538',\n'\\n', 'wd=1e-05 epoch=3 val_loss=0.8995', '\\n', 'wd=1e-05 epoch=4\nval_loss=0.8438', '\\n', 'wd=1e-05 epoch=5 val_loss=0.7864', '\\n', 'wd=1e-05\nepoch=6 val_loss=0.7292', '\\n', 'wd=1e-05 epoch=7 val_loss=0.6713', '\\n',\n'wd=1e-05 epoch=8 val_loss=0.6147', '\\n', 'wd=1e-05 epoch=9 val_loss=0.5608',\n'\\n', 'wd=1e-05 epoch=10 val_loss=0.5101', '\\n', '=== Training with\nweight_decay=0.0001 ===', '\\n', 'wd=0.0001 epoch=1 val_loss=1.1056', '\\n',\n'wd=0.0001 epoch=2 val_loss=1.0487', '\\n', 'wd=0.0001 epoch=3 val_loss=0.9977',\n'\\n', 'wd=0.0001 epoch=4 val_loss=0.9476', '\\n', 'wd=0.0001 epoch=5\nval_loss=0.8968', '\\n', 'wd=0.0001 epoch=6 val_loss=0.8440', '\\n', 'wd=0.0001\nepoch=7 val_loss=0.7894', '\\n', 'wd=0.0001 epoch=8 val_loss=0.7328', '\\n',\n'wd=0.0001 epoch=9 val_loss=0.6766', '\\n', 'wd=0.0001 epoch=10 val_loss=0.6219',\n'\\n', '=== Training with weight_decay=0.001 ===', '\\n', 'wd=0.001 epoch=1\nval_loss=1.0694', '\\n', 'wd=0.001 epoch=2 val_loss=1.0182', '\\n', 'wd=0.001\nepoch=3 val_loss=0.9675', '\\n', 'wd=0.001 epoch=4 val_loss=0.9142', '\\n',\n'wd=0.001 epoch=5 val_loss=0.8568', '\\n', 'wd=0.001 epoch=6 val_loss=0.7957',\n'\\n', 'wd=0.001 epoch=7 val_loss=0.7327', '\\n', 'wd=0.001 epoch=8\nval_loss=0.6682', '\\n', 'wd=0.001 epoch=9 val_loss=0.6070', '\\n', 'wd=0.001\nepoch=10 val_loss=0.5505', '\\n', '=== Training with weight_decay=0.01 ===',\n'\\n', 'wd=0.01 epoch=1 val_loss=1.0579', '\\n', 'wd=0.01 epoch=2\nval_loss=0.9923', '\\n', 'wd=0.01 epoch=3 val_loss=0.9308', '\\n', 'wd=0.01\nepoch=4 val_loss=0.8729', '\\n', 'wd=0.01 epoch=5 val_loss=0.8140', '\\n',\n'wd=0.01 epoch=6 val_loss=0.7545', '\\n', 'wd=0.01 epoch=7 val_loss=0.6949',\n'\\n', 'wd=0.01 epoch=8 val_loss=0.6377', '\\n', 'wd=0.01 epoch=9\nval_loss=0.5832', '\\n', 'wd=0.01 epoch=10 val_loss=0.5316', '\\n', 'Execution\ntime: 3 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Running hidden_dim = 16', '\\n', 'hidden_dim 16\nEpoch 1: validation_loss = 1.0734', '\\n', 'hidden_dim 16 Epoch 2:\nvalidation_loss = 1.0423', '\\n', 'hidden_dim 16 Epoch 3: validation_loss =\n1.0099', '\\n', 'hidden_dim 16 Epoch 4: validation_loss = 0.9752', '\\n',\n'hidden_dim 16 Epoch 5: validation_loss = 0.9389', '\\n', 'hidden_dim 16 Epoch 6:\nvalidation_loss = 0.8981', '\\n', 'hidden_dim 16 Epoch 7: validation_loss =\n0.8549', '\\n', 'hidden_dim 16 Epoch 8: validation_loss = 0.8082', '\\n',\n'hidden_dim 16 Epoch 9: validation_loss = 0.7604', '\\n', 'hidden_dim 16 Epoch\n10: validation_loss = 0.7131', '\\n', 'Running hidden_dim = 32', '\\n',\n'hidden_dim 32 Epoch 1: validation_loss = 1.0369', '\\n', 'hidden_dim 32 Epoch 2:\nvalidation_loss = 0.9760', '\\n', 'hidden_dim 32 Epoch 3: validation_loss =\n0.9203', '\\n', 'hidden_dim 32 Epoch 4: validation_loss = 0.8664', '\\n',\n'hidden_dim 32 Epoch 5: validation_loss = 0.8122', '\\n', 'hidden_dim 32 Epoch 6:\nvalidation_loss = 0.7576', '\\n', 'hidden_dim 32 Epoch 7: validation_loss =\n0.7029', '\\n', 'hidden_dim 32 Epoch 8: validation_loss = 0.6495', '\\n',\n'hidden_dim 32 Epoch 9: validation_loss = 0.5988', '\\n', 'hidden_dim 32 Epoch\n10: validation_loss = 0.5510', '\\n', 'Running hidden_dim = 64', '\\n',\n'hidden_dim 64 Epoch 1: validation_loss = 1.0549', '\\n', 'hidden_dim 64 Epoch 2:\nvalidation_loss = 0.9658', '\\n', 'hidden_dim 64 Epoch 3: validation_loss =\n0.8812', '\\n', 'hidden_dim 64 Epoch 4: validation_loss = 0.7984', '\\n',\n'hidden_dim 64 Epoch 5: validation_loss = 0.7165', '\\n', 'hidden_dim 64 Epoch 6:\nvalidation_loss = 0.6388', '\\n', 'hidden_dim 64 Epoch 7: validation_loss =\n0.5670', '\\n', 'hidden_dim 64 Epoch 8: validation_loss = 0.5029', '\\n',\n'hidden_dim 64 Epoch 9: validation_loss = 0.4487', '\\n', 'hidden_dim 64 Epoch\n10: validation_loss = 0.4033', '\\n', 'Running hidden_dim = 128', '\\n',\n'hidden_dim 128 Epoch 1: validation_loss = 0.9458', '\\n', 'hidden_dim 128 Epoch\n2: validation_loss = 0.8099', '\\n', 'hidden_dim 128 Epoch 3: validation_loss =\n0.6926', '\\n', 'hidden_dim 128 Epoch 4: validation_loss = 0.5906', '\\n',\n'hidden_dim 128 Epoch 5: validation_loss = 0.5063', '\\n', 'hidden_dim 128 Epoch\n6: validation_loss = 0.4363', '\\n', 'hidden_dim 128 Epoch 7: validation_loss =\n0.3827', '\\n', 'hidden_dim 128 Epoch 8: validation_loss = 0.3402', '\\n',\n'hidden_dim 128 Epoch 9: validation_loss = 0.3062', '\\n', 'hidden_dim 128 Epoch\n10: validation_loss = 0.2784', '\\n', 'Execution time: 3 seconds seconds (time\nlimit is an hour).']", "['dropout=0.0 epoch=1 val_loss=1.0102', '\\n', 'dropout=0.0 epoch=2\nval_loss=0.9538', '\\n', 'dropout=0.0 epoch=3 val_loss=0.8995', '\\n',\n'dropout=0.0 epoch=4 val_loss=0.8438', '\\n', 'dropout=0.0 epoch=5\nval_loss=0.7864', '\\n', 'dropout=0.0 epoch=6 val_loss=0.7292', '\\n',\n'dropout=0.0 epoch=7 val_loss=0.6713', '\\n', 'dropout=0.0 epoch=8\nval_loss=0.6147', '\\n', 'dropout=0.0 epoch=9 val_loss=0.5608', '\\n',\n'dropout=0.0 epoch=10 val_loss=0.5101', '\\n', 'dropout=0.1 epoch=1\nval_loss=1.1067', '\\n', 'dropout=0.1 epoch=2 val_loss=1.0507', '\\n',\n'dropout=0.1 epoch=3 val_loss=1.0004', '\\n', 'dropout=0.1 epoch=4\nval_loss=0.9515', '\\n', 'dropout=0.1 epoch=5 val_loss=0.9021', '\\n',\n'dropout=0.1 epoch=6 val_loss=0.8508', '\\n', 'dropout=0.1 epoch=7\nval_loss=0.7976', '\\n', 'dropout=0.1 epoch=8 val_loss=0.7429', '\\n',\n'dropout=0.1 epoch=9 val_loss=0.6884', '\\n', 'dropout=0.1 epoch=10\nval_loss=0.6350', '\\n', 'dropout=0.2 epoch=1 val_loss=1.0711', '\\n',\n'dropout=0.2 epoch=2 val_loss=1.0215', '\\n', 'dropout=0.2 epoch=3\nval_loss=0.9728', '\\n', 'dropout=0.2 epoch=4 val_loss=0.9217', '\\n',\n'dropout=0.2 epoch=5 val_loss=0.8678', '\\n', 'dropout=0.2 epoch=6\nval_loss=0.8111', '\\n', 'dropout=0.2 epoch=7 val_loss=0.7523', '\\n',\n'dropout=0.2 epoch=8 val_loss=0.6924', '\\n', 'dropout=0.2 epoch=9\nval_loss=0.6339', '\\n', 'dropout=0.2 epoch=10 val_loss=0.5791', '\\n',\n'dropout=0.5 epoch=1 val_loss=1.0655', '\\n', 'dropout=0.5 epoch=2\nval_loss=1.0069', '\\n', 'dropout=0.5 epoch=3 val_loss=0.9527', '\\n',\n'dropout=0.5 epoch=4 val_loss=0.9026', '\\n', 'dropout=0.5 epoch=5\nval_loss=0.8522', '\\n', 'dropout=0.5 epoch=6 val_loss=0.8022', '\\n',\n'dropout=0.5 epoch=7 val_loss=0.7525', '\\n', 'dropout=0.5 epoch=8\nval_loss=0.7041', '\\n', 'dropout=0.5 epoch=9 val_loss=0.6567', '\\n',\n'dropout=0.5 epoch=10 val_loss=0.6109', '\\n', 'Execution time: 3 seconds seconds\n(time limit is an hour).']", "['Using device: cuda', '\\n', 'alpha=0.0 | Epoch 1: train_loss=1.0467,\nval_loss=1.0102', '\\n', 'alpha=0.0 | Epoch 2: train_loss=0.9866,\nval_loss=0.9538', '\\n', 'alpha=0.0 | Epoch 3: train_loss=0.9301,\nval_loss=0.8995', '\\n', 'alpha=0.0 | Epoch 4: train_loss=0.8732,\nval_loss=0.8438', '\\n', 'alpha=0.0 | Epoch 5: train_loss=0.8151,\nval_loss=0.7864', '\\n', 'alpha=0.0 | Epoch 6: train_loss=0.7560,\nval_loss=0.7292', '\\n', 'alpha=0.0 | Epoch 7: train_loss=0.6954,\nval_loss=0.6713', '\\n', 'alpha=0.0 | Epoch 8: train_loss=0.6368,\nval_loss=0.6147', '\\n', 'alpha=0.0 | Epoch 9: train_loss=0.5800,\nval_loss=0.5608', '\\n', 'alpha=0.0 | Epoch 10: train_loss=0.5274,\nval_loss=0.5101', '\\n', 'alpha=0.05 | Epoch 1: train_loss=1.0502,\nval_loss=1.0156', '\\n', 'alpha=0.05 | Epoch 2: train_loss=0.9935,\nval_loss=0.9626', '\\n', 'alpha=0.05 | Epoch 3: train_loss=0.9405,\nval_loss=0.9120', '\\n', 'alpha=0.05 | Epoch 4: train_loss=0.8875,\nval_loss=0.8604', '\\n', 'alpha=0.05 | Epoch 5: train_loss=0.8340,\nval_loss=0.8077', '\\n', 'alpha=0.05 | Epoch 6: train_loss=0.7799,\nval_loss=0.7558', '\\n', 'alpha=0.05 | Epoch 7: train_loss=0.7252,\nval_loss=0.7039', '\\n', 'alpha=0.05 | Epoch 8: train_loss=0.6730,\nval_loss=0.6538', '\\n', 'alpha=0.05 | Epoch 9: train_loss=0.6231,\nval_loss=0.6070', '\\n', 'alpha=0.05 | Epoch 10: train_loss=0.5778,\nval_loss=0.5638', '\\n', 'alpha=0.1 | Epoch 1: train_loss=1.0537,\nval_loss=1.0210', '\\n', 'alpha=0.1 | Epoch 2: train_loss=1.0003,\nval_loss=0.9714', '\\n', 'alpha=0.1 | Epoch 3: train_loss=0.9508,\nval_loss=0.9243', '\\n', 'alpha=0.1 | Epoch 4: train_loss=0.9018,\nval_loss=0.8769', '\\n', 'alpha=0.1 | Epoch 5: train_loss=0.8527,\nval_loss=0.8288', '\\n', 'alpha=0.1 | Epoch 6: train_loss=0.8036,\nval_loss=0.7821', '\\n', 'alpha=0.1 | Epoch 7: train_loss=0.7545,\nval_loss=0.7359', '\\n', 'alpha=0.1 | Epoch 8: train_loss=0.7085,\nval_loss=0.6922', '\\n', 'alpha=0.1 | Epoch 9: train_loss=0.6653,\nval_loss=0.6521', '\\n', 'alpha=0.1 | Epoch 10: train_loss=0.6269,\nval_loss=0.6159', '\\n', 'alpha=0.2 | Epoch 1: train_loss=1.0608,\nval_loss=1.0319', '\\n', 'alpha=0.2 | Epoch 2: train_loss=1.0139,\nval_loss=0.9889', '\\n', 'alpha=0.2 | Epoch 3: train_loss=0.9715,\nval_loss=0.9489', '\\n', 'alpha=0.2 | Epoch 4: train_loss=0.9301,\nval_loss=0.9096', '\\n', 'alpha=0.2 | Epoch 5: train_loss=0.8897,\nval_loss=0.8706', '\\n', 'alpha=0.2 | Epoch 6: train_loss=0.8504,\nval_loss=0.8339', '\\n', 'alpha=0.2 | Epoch 7: train_loss=0.8122,\nval_loss=0.7986', '\\n', 'alpha=0.2 | Epoch 8: train_loss=0.7777,\nval_loss=0.7667', '\\n', 'alpha=0.2 | Epoch 9: train_loss=0.7467,\nval_loss=0.7387', '\\n', 'alpha=0.2 | Epoch 10: train_loss=0.7205,\nval_loss=0.7146', '\\n', 'Execution time: 4 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 32, in <module>\\n    train_ds =\nTensorDataset(torch.tensor(x_train, float), torch.tensor(y_train, long))\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nTypeError: tensor() takes 1 positional argument\nbut 2 were given\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 29, in <module>\\n    train_dataset =\nTensorDataset(torch.tensor(x_train, float), torch.tensor(y_train, long))\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nTypeError: tensor() takes 1 positional argument\nbut 2 were given\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Gamma 0.1 Epoch 1: validation_loss = 1.0102',\n'\\n', 'Gamma 0.1 Epoch 2: validation_loss = 0.9538', '\\n', 'Gamma 0.1 Epoch 3:\nvalidation_loss = 0.8995', '\\n', 'Gamma 0.1 Epoch 4: validation_loss = 0.8438',\n'\\n', 'Gamma 0.1 Epoch 5: validation_loss = 0.7864', '\\n', 'Gamma 0.1 Epoch 6:\nvalidation_loss = 0.7807', '\\n', 'Gamma 0.1 Epoch 7: validation_loss = 0.7750',\n'\\n', 'Gamma 0.1 Epoch 8: validation_loss = 0.7692', '\\n', 'Gamma 0.1 Epoch 9:\nvalidation_loss = 0.7635', '\\n', 'Gamma 0.1 Epoch 10: validation_loss = 0.7578',\n'\\n', 'Gamma 0.5 Epoch 1: validation_loss = 1.1056', '\\n', 'Gamma 0.5 Epoch 2:\nvalidation_loss = 1.0487', '\\n', 'Gamma 0.5 Epoch 3: validation_loss = 0.9977',\n'\\n', 'Gamma 0.5 Epoch 4: validation_loss = 0.9475', '\\n', 'Gamma 0.5 Epoch 5:\nvalidation_loss = 0.8968', '\\n', 'Gamma 0.5 Epoch 6: validation_loss = 0.8706',\n'\\n', 'Gamma 0.5 Epoch 7: validation_loss = 0.8438', '\\n', 'Gamma 0.5 Epoch 8:\nvalidation_loss = 0.8165', '\\n', 'Gamma 0.5 Epoch 9: validation_loss = 0.7890',\n'\\n', 'Gamma 0.5 Epoch 10: validation_loss = 0.7613', '\\n', 'Gamma 0.9 Epoch 1:\nvalidation_loss = 1.0693', '\\n', 'Gamma 0.9 Epoch 2: validation_loss = 1.0180',\n'\\n', 'Gamma 0.9 Epoch 3: validation_loss = 0.9673', '\\n', 'Gamma 0.9 Epoch 4:\nvalidation_loss = 0.9138', '\\n', 'Gamma 0.9 Epoch 5: validation_loss = 0.8562',\n'\\n', 'Gamma 0.9 Epoch 6: validation_loss = 0.8014', '\\n', 'Gamma 0.9 Epoch 7:\nvalidation_loss = 0.7448', '\\n', 'Gamma 0.9 Epoch 8: validation_loss = 0.6868',\n'\\n', 'Gamma 0.9 Epoch 9: validation_loss = 0.6311', '\\n', 'Gamma 0.9 Epoch 10:\nvalidation_loss = 0.5787', '\\n', 'Execution time: 2 seconds seconds (time limit\nis an hour).']", "['max_grad_norm=0.5 Epoch 1: val_loss=1.0099', '\\n', 'max_grad_norm=0.5 Epoch 2:\nval_loss=0.9527', '\\n', 'max_grad_norm=0.5 Epoch 3: val_loss=0.8977', '\\n',\n'max_grad_norm=0.5 Epoch 4: val_loss=0.8412', '\\n', 'max_grad_norm=0.5 Epoch 5:\nval_loss=0.7832', '\\n', 'max_grad_norm=0.5 Epoch 6: val_loss=0.7257', '\\n',\n'max_grad_norm=0.5 Epoch 7: val_loss=0.6677', '\\n', 'max_grad_norm=0.5 Epoch 8:\nval_loss=0.6109', '\\n', 'max_grad_norm=0.5 Epoch 9: val_loss=0.5569', '\\n',\n'max_grad_norm=0.5 Epoch 10: val_loss=0.5062', '\\n', 'max_grad_norm=1.0 Epoch 1:\nval_loss=1.0102', '\\n', 'max_grad_norm=1.0 Epoch 2: val_loss=0.9538', '\\n',\n'max_grad_norm=1.0 Epoch 3: val_loss=0.8995', '\\n', 'max_grad_norm=1.0 Epoch 4:\nval_loss=0.8438', '\\n', 'max_grad_norm=1.0 Epoch 5: val_loss=0.7864', '\\n',\n'max_grad_norm=1.0 Epoch 6: val_loss=0.7292', '\\n', 'max_grad_norm=1.0 Epoch 7:\nval_loss=0.6713', '\\n', 'max_grad_norm=1.0 Epoch 8: val_loss=0.6147', '\\n',\n'max_grad_norm=1.0 Epoch 9: val_loss=0.5608', '\\n', 'max_grad_norm=1.0 Epoch 10:\nval_loss=0.5101', '\\n', 'max_grad_norm=5.0 Epoch 1: val_loss=1.0102', '\\n',\n'max_grad_norm=5.0 Epoch 2: val_loss=0.9538', '\\n', 'max_grad_norm=5.0 Epoch 3:\nval_loss=0.8995', '\\n', 'max_grad_norm=5.0 Epoch 4: val_loss=0.8438', '\\n',\n'max_grad_norm=5.0 Epoch 5: val_loss=0.7864', '\\n', 'max_grad_norm=5.0 Epoch 6:\nval_loss=0.7292', '\\n', 'max_grad_norm=5.0 Epoch 7: val_loss=0.6713', '\\n',\n'max_grad_norm=5.0 Epoch 8: val_loss=0.6147', '\\n', 'max_grad_norm=5.0 Epoch 9:\nval_loss=0.5608', '\\n', 'max_grad_norm=5.0 Epoch 10: val_loss=0.5101', '\\n',\n'max_grad_norm=10.0 Epoch 1: val_loss=1.0102', '\\n', 'max_grad_norm=10.0 Epoch\n2: val_loss=0.9538', '\\n', 'max_grad_norm=10.0 Epoch 3: val_loss=0.8995', '\\n',\n'max_grad_norm=10.0 Epoch 4: val_loss=0.8438', '\\n', 'max_grad_norm=10.0 Epoch\n5: val_loss=0.7864', '\\n', 'max_grad_norm=10.0 Epoch 6: val_loss=0.7292', '\\n',\n'max_grad_norm=10.0 Epoch 7: val_loss=0.6713', '\\n', 'max_grad_norm=10.0 Epoch\n8: val_loss=0.6147', '\\n', 'max_grad_norm=10.0 Epoch 9: val_loss=0.5608', '\\n',\n'max_grad_norm=10.0 Epoch 10: val_loss=0.5101', '\\n', 'Execution time: 3 seconds\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'init_std=0.01 epoch=1 val_loss=1.0900', '\\n',\n'init_std=0.01 epoch=2 val_loss=1.0623', '\\n', 'init_std=0.01 epoch=3\nval_loss=1.0063', '\\n', 'init_std=0.01 epoch=4 val_loss=0.9217', '\\n',\n'init_std=0.01 epoch=5 val_loss=0.8197', '\\n', 'init_std=0.01 epoch=6\nval_loss=0.7155', '\\n', 'init_std=0.01 epoch=7 val_loss=0.6182', '\\n',\n'init_std=0.01 epoch=8 val_loss=0.5351', '\\n', 'init_std=0.01 epoch=9\nval_loss=0.4666', '\\n', 'init_std=0.01 epoch=10 val_loss=0.4101', '\\n',\n'init_std=0.1 epoch=1 val_loss=1.0751', '\\n', 'init_std=0.1 epoch=2\nval_loss=1.0298', '\\n', 'init_std=0.1 epoch=3 val_loss=0.9805', '\\n',\n'init_std=0.1 epoch=4 val_loss=0.9226', '\\n', 'init_std=0.1 epoch=5\nval_loss=0.8548', '\\n', 'init_std=0.1 epoch=6 val_loss=0.7792', '\\n',\n'init_std=0.1 epoch=7 val_loss=0.7000', '\\n', 'init_std=0.1 epoch=8\nval_loss=0.6213', '\\n', 'init_std=0.1 epoch=9 val_loss=0.5492', '\\n',\n'init_std=0.1 epoch=10 val_loss=0.4864', '\\n', 'init_std=0.5 epoch=1\nval_loss=2.5555', '\\n', 'init_std=0.5 epoch=2 val_loss=2.2225', '\\n',\n'init_std=0.5 epoch=3 val_loss=1.9360', '\\n', 'init_std=0.5 epoch=4\nval_loss=1.6836', '\\n', 'init_std=0.5 epoch=5 val_loss=1.4711', '\\n',\n'init_std=0.5 epoch=6 val_loss=1.2938', '\\n', 'init_std=0.5 epoch=7\nval_loss=1.1433', '\\n', 'init_std=0.5 epoch=8 val_loss=1.0134', '\\n',\n'init_std=0.5 epoch=9 val_loss=0.9075', '\\n', 'init_std=0.5 epoch=10\nval_loss=0.8220', '\\n', 'init_std=1.0 epoch=1 val_loss=11.5857', '\\n',\n'init_std=1.0 epoch=2 val_loss=10.7028', '\\n', 'init_std=1.0 epoch=3\nval_loss=9.8652', '\\n', 'init_std=1.0 epoch=4 val_loss=9.0974', '\\n',\n'init_std=1.0 epoch=5 val_loss=8.3806', '\\n', 'init_std=1.0 epoch=6\nval_loss=7.7339', '\\n', 'init_std=1.0 epoch=7 val_loss=7.1501', '\\n',\n'init_std=1.0 epoch=8 val_loss=6.6107', '\\n', 'init_std=1.0 epoch=9\nval_loss=6.1259', '\\n', 'init_std=1.0 epoch=10 val_loss=5.6873', '\\n',\n'Execution time: 3 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\u03bb=0.0 Epoch 1: val_loss=1.0102, val_align=0.9930',\n'\\n', '\u03bb=0.0 Epoch 2: val_loss=0.9538, val_align=0.9932', '\\n', '\u03bb=0.0 Epoch 3:\nval_loss=0.8995, val_align=0.9928', '\\n', '\u03bb=0.0 Epoch 4: val_loss=0.8438,\nval_align=0.9922', '\\n', '\u03bb=0.0 Epoch 5: val_loss=0.7864, val_align=0.9916',\n'\\n', '\u03bb=0.0 Epoch 6: val_loss=0.7292, val_align=0.9909', '\\n', '\u03bb=0.0 Epoch 7:\nval_loss=0.6713, val_align=0.9904', '\\n', '\u03bb=0.0 Epoch 8: val_loss=0.6147,\nval_align=0.9902', '\\n', '\u03bb=0.0 Epoch 9: val_loss=0.5608, val_align=0.9903',\n'\\n', '\u03bb=0.0 Epoch 10: val_loss=0.5101, val_align=0.9907', '\\n', '\u03bb=0.1 Epoch 1:\nval_loss=1.0102, val_align=0.9930', '\\n', '\u03bb=0.1 Epoch 2: val_loss=0.9539,\nval_align=0.9932', '\\n', '\u03bb=0.1 Epoch 3: val_loss=0.8996, val_align=0.9929',\n'\\n', '\u03bb=0.1 Epoch 4: val_loss=0.8440, val_align=0.9924', '\\n', '\u03bb=0.1 Epoch 5:\nval_loss=0.7866, val_align=0.9917', '\\n', '\u03bb=0.1 Epoch 6: val_loss=0.7295,\nval_align=0.9911', '\\n', '\u03bb=0.1 Epoch 7: val_loss=0.6716, val_align=0.9907',\n'\\n', '\u03bb=0.1 Epoch 8: val_loss=0.6149, val_align=0.9905', '\\n', '\u03bb=0.1 Epoch 9:\nval_loss=0.5610, val_align=0.9907', '\\n', '\u03bb=0.1 Epoch 10: val_loss=0.5103,\nval_align=0.9910', '\\n', '\u03bb=0.5 Epoch 1: val_loss=1.0102, val_align=0.9932',\n'\\n', '\u03bb=0.5 Epoch 2: val_loss=0.9541, val_align=0.9935', '\\n', '\u03bb=0.5 Epoch 3:\nval_loss=0.9000, val_align=0.9933', '\\n', '\u03bb=0.5 Epoch 4: val_loss=0.8446,\nval_align=0.9928', '\\n', '\u03bb=0.5 Epoch 5: val_loss=0.7875, val_align=0.9924',\n'\\n', '\u03bb=0.5 Epoch 6: val_loss=0.7304, val_align=0.9919', '\\n', '\u03bb=0.5 Epoch 7:\nval_loss=0.6726, val_align=0.9916', '\\n', '\u03bb=0.5 Epoch 8: val_loss=0.6160,\nval_align=0.9916', '\\n', '\u03bb=0.5 Epoch 9: val_loss=0.5621, val_align=0.9918',\n'\\n', '\u03bb=0.5 Epoch 10: val_loss=0.5113, val_align=0.9922', '\\n', '\u03bb=1.0 Epoch 1:\nval_loss=1.0104, val_align=0.9933', '\\n', '\u03bb=1.0 Epoch 2: val_loss=0.9545,\nval_align=0.9937', '\\n', '\u03bb=1.0 Epoch 3: val_loss=0.9006, val_align=0.9937',\n'\\n', '\u03bb=1.0 Epoch 4: val_loss=0.8455, val_align=0.9934', '\\n', '\u03bb=1.0 Epoch 5:\nval_loss=0.7886, val_align=0.9931', '\\n', '\u03bb=1.0 Epoch 6: val_loss=0.7317,\nval_align=0.9927', '\\n', '\u03bb=1.0 Epoch 7: val_loss=0.6740, val_align=0.9926',\n'\\n', '\u03bb=1.0 Epoch 8: val_loss=0.6174, val_align=0.9926', '\\n', '\u03bb=1.0 Epoch 9:\nval_loss=0.5635, val_align=0.9929', '\\n', '\u03bb=1.0 Epoch 10: val_loss=0.5126,\nval_align=0.9933', '\\n', 'Execution time: 4 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'LR 1.0e-04 Epoch 1: val_loss = 1.0651, val_align =\n0.9922', '\\n', 'LR 1.0e-04 Epoch 2: val_loss = 1.0588, val_align = 0.9923',\n'\\n', 'LR 1.0e-04 Epoch 3: val_loss = 1.0526, val_align = 0.9924', '\\n', 'LR\n1.0e-04 Epoch 4: val_loss = 1.0464, val_align = 0.9925', '\\n', 'LR 1.0e-04 Epoch\n5: val_loss = 1.0403, val_align = 0.9926', '\\n', 'LR 1.0e-04 Epoch 6: val_loss =\n1.0343, val_align = 0.9926', '\\n', 'LR 1.0e-04 Epoch 7: val_loss = 1.0283,\nval_align = 0.9927', '\\n', 'LR 1.0e-04 Epoch 8: val_loss = 1.0224, val_align =\n0.9928', '\\n', 'LR 1.0e-04 Epoch 9: val_loss = 1.0165, val_align = 0.9928',\n'\\n', 'LR 1.0e-04 Epoch 10: val_loss = 1.0106, val_align = 0.9929', '\\n', 'LR\n5.0e-04 Epoch 1: val_loss = 1.0400, val_align = 0.9926', '\\n', 'LR 5.0e-04 Epoch\n2: val_loss = 1.0104, val_align = 0.9929', '\\n', 'LR 5.0e-04 Epoch 3: val_loss =\n0.9821, val_align = 0.9931', '\\n', 'LR 5.0e-04 Epoch 4: val_loss = 0.9540,\nval_align = 0.9931', '\\n', 'LR 5.0e-04 Epoch 5: val_loss = 0.9259, val_align =\n0.9930', '\\n', 'LR 5.0e-04 Epoch 6: val_loss = 0.8980, val_align = 0.9927',\n'\\n', 'LR 5.0e-04 Epoch 7: val_loss = 0.8697, val_align = 0.9925', '\\n', 'LR\n5.0e-04 Epoch 8: val_loss = 0.8407, val_align = 0.9922', '\\n', 'LR 5.0e-04 Epoch\n9: val_loss = 0.8115, val_align = 0.9918', '\\n', 'LR 5.0e-04 Epoch 10: val_loss\n= 0.7817, val_align = 0.9915', '\\n', 'LR 1.0e-03 Epoch 1: val_loss = 1.0102,\nval_align = 0.9930', '\\n', 'LR 1.0e-03 Epoch 2: val_loss = 0.9538, val_align =\n0.9932', '\\n', 'LR 1.0e-03 Epoch 3: val_loss = 0.8995, val_align = 0.9928',\n'\\n', 'LR 1.0e-03 Epoch 4: val_loss = 0.8438, val_align = 0.9922', '\\n', 'LR\n1.0e-03 Epoch 5: val_loss = 0.7864, val_align = 0.9916', '\\n', 'LR 1.0e-03 Epoch\n6: val_loss = 0.7292, val_align = 0.9909', '\\n', 'LR 1.0e-03 Epoch 7: val_loss =\n0.6713, val_align = 0.9904', '\\n', 'LR 1.0e-03 Epoch 8: val_loss = 0.6147,\nval_align = 0.9902', '\\n', 'LR 1.0e-03 Epoch 9: val_loss = 0.5608, val_align =\n0.9903', '\\n', 'LR 1.0e-03 Epoch 10: val_loss = 0.5101, val_align = 0.9907',\n'\\n', 'LR 5.0e-03 Epoch 1: val_loss = 0.7999, val_align = 0.9919', '\\n', 'LR\n5.0e-03 Epoch 2: val_loss = 0.5444, val_align = 0.9895', '\\n', 'LR 5.0e-03 Epoch\n3: val_loss = 0.3476, val_align = 0.9923', '\\n', 'LR 5.0e-03 Epoch 4: val_loss =\n0.2409, val_align = 0.9956', '\\n', 'LR 5.0e-03 Epoch 5: val_loss = 0.1822,\nval_align = 0.9971', '\\n', 'LR 5.0e-03 Epoch 6: val_loss = 0.1579, val_align =\n0.9980', '\\n', 'LR 5.0e-03 Epoch 7: val_loss = 0.1343, val_align = 0.9983',\n'\\n', 'LR 5.0e-03 Epoch 8: val_loss = 0.1260, val_align = 0.9986', '\\n', 'LR\n5.0e-03 Epoch 9: val_loss = 0.1144, val_align = 0.9987', '\\n', 'LR 5.0e-03 Epoch\n10: val_loss = 0.1039, val_align = 0.9989', '\\n', 'LR 1.0e-02 Epoch 1: val_loss\n= 0.5694, val_align = 0.9887', '\\n', 'LR 1.0e-02 Epoch 2: val_loss = 0.2476,\nval_align = 0.9946', '\\n', 'LR 1.0e-02 Epoch 3: val_loss = 0.1484, val_align =\n0.9973', '\\n', 'LR 1.0e-02 Epoch 4: val_loss = 0.1135, val_align = 0.9985',\n'\\n', 'LR 1.0e-02 Epoch 5: val_loss = 0.0999, val_align = 0.9986', '\\n', 'LR\n1.0e-02 Epoch 6: val_loss = 0.0867, val_align = 0.9988', '\\n', 'LR 1.0e-02 Epoch\n7: val_loss = 0.0869, val_align = 0.9990', '\\n', 'LR 1.0e-02 Epoch 8: val_loss =\n0.0786, val_align = 0.9988', '\\n', 'LR 1.0e-02 Epoch 9: val_loss = 0.0752,\nval_align = 0.9990', '\\n', 'LR 1.0e-02 Epoch 10: val_loss = 0.0739, val_align =\n0.9986', '\\n', 'Execution time: 4 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'LR 1.0e-04 Epoch 1: val_loss = 1.0651, val_align =\n0.9922', '\\n', 'LR 1.0e-04 Epoch 2: val_loss = 1.0588, val_align = 0.9923',\n'\\n', 'LR 1.0e-04 Epoch 3: val_loss = 1.0526, val_align = 0.9924', '\\n', 'LR\n1.0e-04 Epoch 4: val_loss = 1.0464, val_align = 0.9925', '\\n', 'LR 1.0e-04 Epoch\n5: val_loss = 1.0403, val_align = 0.9926', '\\n', 'LR 1.0e-04 Epoch 6: val_loss =\n1.0343, val_align = 0.9926', '\\n', 'LR 1.0e-04 Epoch 7: val_loss = 1.0283,\nval_align = 0.9927', '\\n', 'LR 1.0e-04 Epoch 8: val_loss = 1.0224, val_align =\n0.9928', '\\n', 'LR 1.0e-04 Epoch 9: val_loss = 1.0165, val_align = 0.9928',\n'\\n', 'LR 1.0e-04 Epoch 10: val_loss = 1.0106, val_align = 0.9929', '\\n', 'LR\n5.0e-04 Epoch 1: val_loss = 1.0400, val_align = 0.9926', '\\n', 'LR 5.0e-04 Epoch\n2: val_loss = 1.0104, val_align = 0.9929', '\\n', 'LR 5.0e-04 Epoch 3: val_loss =\n0.9821, val_align = 0.9931', '\\n', 'LR 5.0e-04 Epoch 4: val_loss = 0.9540,\nval_align = 0.9931', '\\n', 'LR 5.0e-04 Epoch 5: val_loss = 0.9259, val_align =\n0.9930', '\\n', 'LR 5.0e-04 Epoch 6: val_loss = 0.8980, val_align = 0.9927',\n'\\n', 'LR 5.0e-04 Epoch 7: val_loss = 0.8697, val_align = 0.9925', '\\n', 'LR\n5.0e-04 Epoch 8: val_loss = 0.8407, val_align = 0.9922', '\\n', 'LR 5.0e-04 Epoch\n9: val_loss = 0.8115, val_align = 0.9918', '\\n', 'LR 5.0e-04 Epoch 10: val_loss\n= 0.7817, val_align = 0.9915', '\\n', 'LR 1.0e-03 Epoch 1: val_loss = 1.0102,\nval_align = 0.9930', '\\n', 'LR 1.0e-03 Epoch 2: val_loss = 0.9538, val_align =\n0.9932', '\\n', 'LR 1.0e-03 Epoch 3: val_loss = 0.8995, val_align = 0.9928',\n'\\n', 'LR 1.0e-03 Epoch 4: val_loss = 0.8438, val_align = 0.9922', '\\n', 'LR\n1.0e-03 Epoch 5: val_loss = 0.7864, val_align = 0.9916', '\\n', 'LR 1.0e-03 Epoch\n6: val_loss = 0.7292, val_align = 0.9909', '\\n', 'LR 1.0e-03 Epoch 7: val_loss =\n0.6713, val_align = 0.9904', '\\n', 'LR 1.0e-03 Epoch 8: val_loss = 0.6147,\nval_align = 0.9902', '\\n', 'LR 1.0e-03 Epoch 9: val_loss = 0.5608, val_align =\n0.9903', '\\n', 'LR 1.0e-03 Epoch 10: val_loss = 0.5101, val_align = 0.9907',\n'\\n', 'LR 5.0e-03 Epoch 1: val_loss = 0.7999, val_align = 0.9919', '\\n', 'LR\n5.0e-03 Epoch 2: val_loss = 0.5444, val_align = 0.9895', '\\n', 'LR 5.0e-03 Epoch\n3: val_loss = 0.3476, val_align = 0.9923', '\\n', 'LR 5.0e-03 Epoch 4: val_loss =\n0.2409, val_align = 0.9956', '\\n', 'LR 5.0e-03 Epoch 5: val_loss = 0.1822,\nval_align = 0.9971', '\\n', 'LR 5.0e-03 Epoch 6: val_loss = 0.1579, val_align =\n0.9980', '\\n', 'LR 5.0e-03 Epoch 7: val_loss = 0.1343, val_align = 0.9983',\n'\\n', 'LR 5.0e-03 Epoch 8: val_loss = 0.1260, val_align = 0.9986', '\\n', 'LR\n5.0e-03 Epoch 9: val_loss = 0.1144, val_align = 0.9987', '\\n', 'LR 5.0e-03 Epoch\n10: val_loss = 0.1039, val_align = 0.9989', '\\n', 'LR 1.0e-02 Epoch 1: val_loss\n= 0.5694, val_align = 0.9887', '\\n', 'LR 1.0e-02 Epoch 2: val_loss = 0.2476,\nval_align = 0.9946', '\\n', 'LR 1.0e-02 Epoch 3: val_loss = 0.1484, val_align =\n0.9973', '\\n', 'LR 1.0e-02 Epoch 4: val_loss = 0.1135, val_align = 0.9985',\n'\\n', 'LR 1.0e-02 Epoch 5: val_loss = 0.0999, val_align = 0.9986', '\\n', 'LR\n1.0e-02 Epoch 6: val_loss = 0.0867, val_align = 0.9988', '\\n', 'LR 1.0e-02 Epoch\n7: val_loss = 0.0869, val_align = 0.9990', '\\n', 'LR 1.0e-02 Epoch 8: val_loss =\n0.0786, val_align = 0.9988', '\\n', 'LR 1.0e-02 Epoch 9: val_loss = 0.0752,\nval_align = 0.9990', '\\n', 'LR 1.0e-02 Epoch 10: val_loss = 0.0739, val_align =\n0.9986', '\\n', 'Execution time: 3 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'LR 1.0e-04 Epoch 1: val_loss = 1.0651, val_align =\n0.9922', '\\n', 'LR 1.0e-04 Epoch 2: val_loss = 1.0588, val_align = 0.9923',\n'\\n', 'LR 1.0e-04 Epoch 3: val_loss = 1.0526, val_align = 0.9924', '\\n', 'LR\n1.0e-04 Epoch 4: val_loss = 1.0464, val_align = 0.9925', '\\n', 'LR 1.0e-04 Epoch\n5: val_loss = 1.0403, val_align = 0.9926', '\\n', 'LR 1.0e-04 Epoch 6: val_loss =\n1.0343, val_align = 0.9926', '\\n', 'LR 1.0e-04 Epoch 7: val_loss = 1.0283,\nval_align = 0.9927', '\\n', 'LR 1.0e-04 Epoch 8: val_loss = 1.0224, val_align =\n0.9928', '\\n', 'LR 1.0e-04 Epoch 9: val_loss = 1.0165, val_align = 0.9928',\n'\\n', 'LR 1.0e-04 Epoch 10: val_loss = 1.0106, val_align = 0.9929', '\\n', 'LR\n5.0e-04 Epoch 1: val_loss = 1.0400, val_align = 0.9926', '\\n', 'LR 5.0e-04 Epoch\n2: val_loss = 1.0104, val_align = 0.9929', '\\n', 'LR 5.0e-04 Epoch 3: val_loss =\n0.9821, val_align = 0.9931', '\\n', 'LR 5.0e-04 Epoch 4: val_loss = 0.9540,\nval_align = 0.9931', '\\n', 'LR 5.0e-04 Epoch 5: val_loss = 0.9259, val_align =\n0.9930', '\\n', 'LR 5.0e-04 Epoch 6: val_loss = 0.8980, val_align = 0.9927',\n'\\n', 'LR 5.0e-04 Epoch 7: val_loss = 0.8697, val_align = 0.9925', '\\n', 'LR\n5.0e-04 Epoch 8: val_loss = 0.8407, val_align = 0.9922', '\\n', 'LR 5.0e-04 Epoch\n9: val_loss = 0.8115, val_align = 0.9918', '\\n', 'LR 5.0e-04 Epoch 10: val_loss\n= 0.7817, val_align = 0.9915', '\\n', 'LR 1.0e-03 Epoch 1: val_loss = 1.0102,\nval_align = 0.9930', '\\n', 'LR 1.0e-03 Epoch 2: val_loss = 0.9538, val_align =\n0.9932', '\\n', 'LR 1.0e-03 Epoch 3: val_loss = 0.8995, val_align = 0.9928',\n'\\n', 'LR 1.0e-03 Epoch 4: val_loss = 0.8438, val_align = 0.9922', '\\n', 'LR\n1.0e-03 Epoch 5: val_loss = 0.7864, val_align = 0.9916', '\\n', 'LR 1.0e-03 Epoch\n6: val_loss = 0.7292, val_align = 0.9909', '\\n', 'LR 1.0e-03 Epoch 7: val_loss =\n0.6713, val_align = 0.9904', '\\n', 'LR 1.0e-03 Epoch 8: val_loss = 0.6147,\nval_align = 0.9902', '\\n', 'LR 1.0e-03 Epoch 9: val_loss = 0.5608, val_align =\n0.9903', '\\n', 'LR 1.0e-03 Epoch 10: val_loss = 0.5101, val_align = 0.9907',\n'\\n', 'LR 5.0e-03 Epoch 1: val_loss = 0.7999, val_align = 0.9919', '\\n', 'LR\n5.0e-03 Epoch 2: val_loss = 0.5444, val_align = 0.9895', '\\n', 'LR 5.0e-03 Epoch\n3: val_loss = 0.3476, val_align = 0.9923', '\\n', 'LR 5.0e-03 Epoch 4: val_loss =\n0.2409, val_align = 0.9956', '\\n', 'LR 5.0e-03 Epoch 5: val_loss = 0.1822,\nval_align = 0.9971', '\\n', 'LR 5.0e-03 Epoch 6: val_loss = 0.1579, val_align =\n0.9980', '\\n', 'LR 5.0e-03 Epoch 7: val_loss = 0.1343, val_align = 0.9983',\n'\\n', 'LR 5.0e-03 Epoch 8: val_loss = 0.1260, val_align = 0.9986', '\\n', 'LR\n5.0e-03 Epoch 9: val_loss = 0.1144, val_align = 0.9987', '\\n', 'LR 5.0e-03 Epoch\n10: val_loss = 0.1039, val_align = 0.9989', '\\n', 'LR 1.0e-02 Epoch 1: val_loss\n= 0.5694, val_align = 0.9887', '\\n', 'LR 1.0e-02 Epoch 2: val_loss = 0.2476,\nval_align = 0.9946', '\\n', 'LR 1.0e-02 Epoch 3: val_loss = 0.1484, val_align =\n0.9973', '\\n', 'LR 1.0e-02 Epoch 4: val_loss = 0.1135, val_align = 0.9985',\n'\\n', 'LR 1.0e-02 Epoch 5: val_loss = 0.0999, val_align = 0.9986', '\\n', 'LR\n1.0e-02 Epoch 6: val_loss = 0.0867, val_align = 0.9988', '\\n', 'LR 1.0e-02 Epoch\n7: val_loss = 0.0869, val_align = 0.9990', '\\n', 'LR 1.0e-02 Epoch 8: val_loss =\n0.0786, val_align = 0.9988', '\\n', 'LR 1.0e-02 Epoch 9: val_loss = 0.0752,\nval_align = 0.9990', '\\n', 'LR 1.0e-02 Epoch 10: val_loss = 0.0739, val_align =\n0.9986', '\\n', 'Execution time: 3 seconds seconds (time limit is an hour).']", ""], "analysis": ["", "The code ran successfully with no runtime errors. Training and validation losses\ndecreased as expected across epochs, and alignment scores behaved reasonably,\npeaking around learning rates 5e-3 and 1e-2. No bugs were detected.", "", "", "The script ran without errors and produced smoothly decreasing validation losses\nacross hidden dimensions. However, it did not include any HuggingFace datasets\nas required by the sub-stage goals. To fix this, add at least two new datasets\nfrom HuggingFace (e.g., 'ag_news' and 'yelp_review_full' or 'imdb') for testing,\nloading them via the datasets library or HuggingFace API, and integrate them\ninto the evaluation loop alongside the synthetic data.", "", "", "The error occurs because torch.tensor expects only one positional argument; the\ndtype must be specified using the keyword argument. Currently\ntorch.tensor(x_train, float) and torch.tensor(y_train, long) are passed\nincorrectly, causing a TypeError. To fix, change to torch.tensor(x_train,\ndtype=torch.float) and torch.tensor(y_train, dtype=torch.long).", "The script fails at creating TensorDatasets due to incorrect use of\ntorch.tensor. The function signature expects only the data as a positional\nargument; dtype must be provided as a keyword. Use torch.tensor(x_train,\ndtype=torch.float) and torch.tensor(y_train, dtype=torch.long) (or\ntorch.from_numpy) to fix the error.", "", "The implementation runs successfully on the synthetic data but does not\nincorporate any HuggingFace datasets as required by the sub-stage goals. To fix\nthis, import and preprocess at least two appropriate HuggingFace datasets (e.g.,\n'imdb' for sentiment classification and 'ag_news' for topic classification),\ncreate DataLoaders for them, and integrate them into the training/validation\nloops alongside the synthetic data to properly test and benchmark the model.", "", "The script executed without errors and val_loss steadily decreased from ~1.01 to\n~0.51, but the alignment metric (1\u2013JSD) remained \u22480.99 across all \u03bb\nvalues\u2014likely due to identical model initializations and only synthetic data, so\nno real alignment variance. Crucially, the implementation did not load or\nevaluate the two additional HuggingFace datasets as required. To fix, integrate\ntwo real-world HuggingFace classification datasets (e.g., AG News, SST-2), adapt\nthe preprocessing and model input accordingly, and then rerun evaluations.\nAdditionally, perform true hyperparameter tuning on learning rate, batch size,\nand epochs as specified.", "The training script executed successfully with no runtime errors or crashes. For\neach learning rate, validation loss steadily decreased over epochs, and\nmodel\u2013user alignment (1 \u2013 JSD) increased, demonstrating convergence. The 5e-3\nand 1e-2 learning rates yielded the fastest loss reduction and highest alignment\nscores. Overall, behavior matches expectations on synthetic data and\nexperiment_data.npy was saved. No code-level bugs detected.", "", "", ""], "exc_type": [null, null, null, null, null, null, null, "TypeError", "TypeError", null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, {"args": ["tensor() takes 1 positional argument but 2 were given"]}, {"args": ["tensor() takes 1 positional argument but 2 were given"]}, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 32, "<module>", "train_ds = TensorDataset(torch.tensor(x_train, float), torch.tensor(y_train, long))"]], [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 29, "<module>", "train_dataset = TensorDataset(torch.tensor(x_train, float), torch.tensor(y_train, long))"]], null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train alignment", "lower_is_better": false, "description": "Alignment on the training split of the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9917, "best_value": 0.9917}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Alignment on the validation split of the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9907, "best_value": 0.9907}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training split of the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.5274, "best_value": 0.5274}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation split of the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.5101, "best_value": 0.5101}]}]}, {"metric_names": [{"metric_name": "training alignment", "lower_is_better": false, "description": "Final alignment on the training set", "data": [{"dataset_name": "learning rate 1.0e-04", "final_value": 0.9929, "best_value": 0.9929}, {"dataset_name": "learning rate 5.0e-04", "final_value": 0.9921, "best_value": 0.9921}, {"dataset_name": "learning rate 1.0e-03", "final_value": 0.9917, "best_value": 0.9917}, {"dataset_name": "learning rate 5.0e-03", "final_value": 0.9988, "best_value": 0.9988}, {"dataset_name": "learning rate 1.0e-02", "final_value": 0.9991, "best_value": 0.9991}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Final alignment on the validation set", "data": [{"dataset_name": "learning rate 1.0e-04", "final_value": 0.9929, "best_value": 0.9929}, {"dataset_name": "learning rate 5.0e-04", "final_value": 0.9915, "best_value": 0.9915}, {"dataset_name": "learning rate 1.0e-03", "final_value": 0.9907, "best_value": 0.9907}, {"dataset_name": "learning rate 5.0e-03", "final_value": 0.9989, "best_value": 0.9989}, {"dataset_name": "learning rate 1.0e-02", "final_value": 0.9986, "best_value": 0.9986}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Final loss on the training set", "data": [{"dataset_name": "learning rate 1.0e-04", "final_value": 1.0162, "best_value": 1.0162}, {"dataset_name": "learning rate 5.0e-04", "final_value": 0.7943, "best_value": 0.7943}, {"dataset_name": "learning rate 1.0e-03", "final_value": 0.5274, "best_value": 0.5274}, {"dataset_name": "learning rate 5.0e-03", "final_value": 0.0982, "best_value": 0.0982}, {"dataset_name": "learning rate 1.0e-02", "final_value": 0.0588, "best_value": 0.0588}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final loss on the validation set", "data": [{"dataset_name": "learning rate 1.0e-04", "final_value": 1.0106, "best_value": 1.0106}, {"dataset_name": "learning rate 5.0e-04", "final_value": 0.7817, "best_value": 0.7817}, {"dataset_name": "learning rate 1.0e-03", "final_value": 0.5101, "best_value": 0.5101}, {"dataset_name": "learning rate 5.0e-03", "final_value": 0.1039, "best_value": 0.1039}, {"dataset_name": "learning rate 1.0e-02", "final_value": 0.0739, "best_value": 0.0739}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test set", "data": [{"dataset_name": "learning rate 1.0e-04", "final_value": 0.6, "best_value": 0.6}, {"dataset_name": "learning rate 5.0e-04", "final_value": 0.835, "best_value": 0.835}, {"dataset_name": "learning rate 1.0e-03", "final_value": 0.905, "best_value": 0.905}, {"dataset_name": "learning rate 5.0e-03", "final_value": 0.96, "best_value": 0.96}, {"dataset_name": "learning rate 1.0e-02", "final_value": 0.965, "best_value": 0.965}]}]}, {"metric_names": [{"metric_name": "train alignment", "lower_is_better": false, "description": "Alignment score on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.998, "best_value": 0.998}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Alignment score on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.998, "best_value": 0.998}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.1756, "best_value": 0.1756}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.1767, "best_value": 0.1767}]}]}, {"metric_names": [{"metric_name": "training alignment", "lower_is_better": false, "description": "Alignment on the synthetic training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9927, "best_value": 0.9942}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Alignment on the synthetic validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9926, "best_value": 0.9946}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the synthetic training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.5502, "best_value": 0.5274}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the synthetic validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.5316, "best_value": 0.5101}]}]}, {"metric_names": [{"metric_name": "train alignment", "lower_is_better": false, "description": "Alignment score on the training split of dataset synthetic", "data": [{"dataset_name": "synthetic", "final_value": 0.9977, "best_value": 0.9977}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Alignment score on the validation split of dataset synthetic", "data": [{"dataset_name": "synthetic", "final_value": 0.9973, "best_value": 0.9973}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training split of dataset synthetic", "data": [{"dataset_name": "synthetic", "final_value": 0.2821, "best_value": 0.2821}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation split of dataset synthetic", "data": [{"dataset_name": "synthetic", "final_value": 0.2784, "best_value": 0.2784}]}]}, {"metric_names": [{"metric_name": "training alignment", "lower_is_better": false, "description": "Alignment metric on the training data", "data": [{"dataset_name": "synthetic", "final_value": 0.9917, "best_value": 0.9917}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Alignment metric on the validation data", "data": [{"dataset_name": "synthetic", "final_value": 0.9946, "best_value": 0.9946}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training data", "data": [{"dataset_name": "synthetic", "final_value": 0.5274, "best_value": 0.5274}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation data", "data": [{"dataset_name": "synthetic", "final_value": 0.5101, "best_value": 0.5101}]}]}, {"metric_names": [{"metric_name": "train alignment", "lower_is_better": false, "description": "Model's alignment score on the training set", "data": [{"dataset_name": "synthetic alpha 0.0", "final_value": 0.9917, "best_value": 0.9917}, {"dataset_name": "synthetic alpha 0.05", "final_value": 0.9922, "best_value": 0.9922}, {"dataset_name": "synthetic alpha 0.1", "final_value": 0.9927, "best_value": 0.9927}, {"dataset_name": "synthetic alpha 0.2", "final_value": 0.9938, "best_value": 0.9938}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Model's alignment score on the validation set", "data": [{"dataset_name": "synthetic alpha 0.0", "final_value": 0.9907, "best_value": 0.9907}, {"dataset_name": "synthetic alpha 0.05", "final_value": 0.9913, "best_value": 0.9913}, {"dataset_name": "synthetic alpha 0.1", "final_value": 0.9919, "best_value": 0.9919}, {"dataset_name": "synthetic alpha 0.2", "final_value": 0.9931, "best_value": 0.9931}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Model's loss on the training set", "data": [{"dataset_name": "synthetic alpha 0.0", "final_value": 0.5274, "best_value": 0.5274}, {"dataset_name": "synthetic alpha 0.05", "final_value": 0.5778, "best_value": 0.5778}, {"dataset_name": "synthetic alpha 0.1", "final_value": 0.6269, "best_value": 0.6269}, {"dataset_name": "synthetic alpha 0.2", "final_value": 0.7205, "best_value": 0.7205}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Model's loss on the validation set", "data": [{"dataset_name": "synthetic alpha 0.0", "final_value": 0.5101, "best_value": 0.5101}, {"dataset_name": "synthetic alpha 0.05", "final_value": 0.5638, "best_value": 0.5638}, {"dataset_name": "synthetic alpha 0.1", "final_value": 0.6159, "best_value": 0.6159}, {"dataset_name": "synthetic alpha 0.2", "final_value": 0.7146, "best_value": 0.7146}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train alignment", "lower_is_better": false, "description": "Alignment measure on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9939481010437011, "best_value": 0.9939481010437011}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Alignment measure on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9943271970748901, "best_value": 0.9943271970748901}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.597569676399231, "best_value": 0.597569676399231}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.5787173199653626, "best_value": 0.5787173199653626}]}]}, {"metric_names": [{"metric_name": "training alignment", "lower_is_better": false, "description": "Final alignment on the training dataset", "data": [{"dataset_name": "synthetic (max_grad_norm=0.5)", "final_value": 0.991731689453125, "best_value": 0.991731689453125}, {"dataset_name": "synthetic (max_grad_norm=1.0)", "final_value": 0.9916952400207519, "best_value": 0.9916952400207519}, {"dataset_name": "synthetic (max_grad_norm=5.0)", "final_value": 0.9916952400207519, "best_value": 0.9916952400207519}, {"dataset_name": "synthetic (max_grad_norm=10.0)", "final_value": 0.9916952400207519, "best_value": 0.9916952400207519}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Final alignment on the validation dataset", "data": [{"dataset_name": "synthetic (max_grad_norm=0.5)", "final_value": 0.9907101154327392, "best_value": 0.9907101154327392}, {"dataset_name": "synthetic (max_grad_norm=1.0)", "final_value": 0.9906968212127686, "best_value": 0.9906968212127686}, {"dataset_name": "synthetic (max_grad_norm=5.0)", "final_value": 0.9906968212127686, "best_value": 0.9906968212127686}, {"dataset_name": "synthetic (max_grad_norm=10.0)", "final_value": 0.9906968212127686, "best_value": 0.9906968212127686}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Final loss on the training dataset", "data": [{"dataset_name": "synthetic (max_grad_norm=0.5)", "final_value": 0.5233471393585205, "best_value": 0.5233471393585205}, {"dataset_name": "synthetic (max_grad_norm=1.0)", "final_value": 0.5274013090133667, "best_value": 0.5274013090133667}, {"dataset_name": "synthetic (max_grad_norm=5.0)", "final_value": 0.5274013090133667, "best_value": 0.5274013090133667}, {"dataset_name": "synthetic (max_grad_norm=10.0)", "final_value": 0.5274013090133667, "best_value": 0.5274013090133667}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final loss on the validation dataset", "data": [{"dataset_name": "synthetic (max_grad_norm=0.5)", "final_value": 0.5061547112464905, "best_value": 0.5061547112464905}, {"dataset_name": "synthetic (max_grad_norm=1.0)", "final_value": 0.51006591796875, "best_value": 0.51006591796875}, {"dataset_name": "synthetic (max_grad_norm=5.0)", "final_value": 0.51006591796875, "best_value": 0.51006591796875}, {"dataset_name": "synthetic (max_grad_norm=10.0)", "final_value": 0.51006591796875, "best_value": 0.51006591796875}]}]}, {"metric_names": [{"metric_name": "training alignment", "lower_is_better": false, "description": "Alignment metric on the development data", "data": [{"dataset_name": "development", "final_value": 0.6308, "best_value": 0.9994}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the development data", "data": [{"dataset_name": "development", "final_value": 5.4055, "best_value": 0.4384}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Alignment metric on the holdout data", "data": [{"dataset_name": "holdout", "final_value": 0.6712, "best_value": 0.9995}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the holdout data", "data": [{"dataset_name": "holdout", "final_value": 5.6873, "best_value": 0.4101}]}]}, {"metric_names": [{"metric_name": "training alignment", "lower_is_better": false, "description": "Alignment score on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.994, "best_value": 0.994}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Alignment score on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.9933, "best_value": 0.9933}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.53, "best_value": 0.5274}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.5126, "best_value": 0.5101}]}]}, {"metric_names": [{"metric_name": "training alignment", "lower_is_better": false, "description": "Alignment metric on the synthetic training dataset", "data": [{"dataset_name": "synthetic-lr1e-04", "final_value": 0.9929, "best_value": 0.9929}, {"dataset_name": "synthetic-lr5e-04", "final_value": 0.9921, "best_value": 0.9921}, {"dataset_name": "synthetic-lr1e-03", "final_value": 0.9917, "best_value": 0.9917}, {"dataset_name": "synthetic-lr5e-03", "final_value": 0.9988, "best_value": 0.9988}, {"dataset_name": "synthetic-lr1e-02", "final_value": 0.9991, "best_value": 0.9991}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Alignment metric on the synthetic validation dataset", "data": [{"dataset_name": "synthetic-lr1e-04", "final_value": 0.9929, "best_value": 0.9929}, {"dataset_name": "synthetic-lr5e-04", "final_value": 0.9915, "best_value": 0.9915}, {"dataset_name": "synthetic-lr1e-03", "final_value": 0.9907, "best_value": 0.9907}, {"dataset_name": "synthetic-lr5e-03", "final_value": 0.9989, "best_value": 0.9989}, {"dataset_name": "synthetic-lr1e-02", "final_value": 0.9986, "best_value": 0.9986}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the synthetic training dataset", "data": [{"dataset_name": "synthetic-lr1e-04", "final_value": 1.0162, "best_value": 1.0162}, {"dataset_name": "synthetic-lr5e-04", "final_value": 0.7943, "best_value": 0.7943}, {"dataset_name": "synthetic-lr1e-03", "final_value": 0.5274, "best_value": 0.5274}, {"dataset_name": "synthetic-lr5e-03", "final_value": 0.0982, "best_value": 0.0982}, {"dataset_name": "synthetic-lr1e-02", "final_value": 0.0588, "best_value": 0.0588}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the synthetic validation dataset", "data": [{"dataset_name": "synthetic-lr1e-04", "final_value": 1.0106, "best_value": 1.0106}, {"dataset_name": "synthetic-lr5e-04", "final_value": 0.7817, "best_value": 0.7817}, {"dataset_name": "synthetic-lr1e-03", "final_value": 0.5101, "best_value": 0.5101}, {"dataset_name": "synthetic-lr5e-03", "final_value": 0.1039, "best_value": 0.1039}, {"dataset_name": "synthetic-lr1e-02", "final_value": 0.0739, "best_value": 0.0739}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the synthetic test dataset", "data": [{"dataset_name": "synthetic-lr1e-04", "final_value": 0.6, "best_value": 0.6}, {"dataset_name": "synthetic-lr5e-04", "final_value": 0.835, "best_value": 0.835}, {"dataset_name": "synthetic-lr1e-03", "final_value": 0.905, "best_value": 0.905}, {"dataset_name": "synthetic-lr5e-03", "final_value": 0.96, "best_value": 0.96}, {"dataset_name": "synthetic-lr1e-02", "final_value": 0.965, "best_value": 0.965}]}]}, {"metric_names": [{"metric_name": "training alignment", "lower_is_better": false, "description": "alignment metric on the synthetic dataset's training split", "data": [{"dataset_name": "synthetic dataset", "final_value": 0.9991, "best_value": 0.9991}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "alignment metric on the synthetic dataset's validation split", "data": [{"dataset_name": "synthetic dataset", "final_value": 0.9989, "best_value": 0.9989}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "loss metric on the synthetic dataset's training split", "data": [{"dataset_name": "synthetic dataset", "final_value": 0.0588, "best_value": 0.0588}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "loss metric on the synthetic dataset's validation split", "data": [{"dataset_name": "synthetic dataset", "final_value": 0.0739, "best_value": 0.0739}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "accuracy metric on the synthetic dataset's test split", "data": [{"dataset_name": "synthetic dataset", "final_value": 0.965, "best_value": 0.965}]}]}, {"metric_names": [{"metric_name": "training alignment", "lower_is_better": false, "description": "Alignment on the training set; higher values indicate better alignment.", "data": [{"dataset_name": "synthetic (lr=1.0e-04)", "final_value": 0.9929, "best_value": 0.9929}, {"dataset_name": "synthetic (lr=5.0e-04)", "final_value": 0.9921, "best_value": 0.9921}, {"dataset_name": "synthetic (lr=1.0e-03)", "final_value": 0.9917, "best_value": 0.9917}, {"dataset_name": "synthetic (lr=5.0e-03)", "final_value": 0.9988, "best_value": 0.9988}, {"dataset_name": "synthetic (lr=1.0e-02)", "final_value": 0.9991, "best_value": 0.9991}]}, {"metric_name": "validation alignment", "lower_is_better": false, "description": "Alignment on the validation set; higher values indicate better alignment.", "data": [{"dataset_name": "synthetic (lr=1.0e-04)", "final_value": 0.9929, "best_value": 0.9929}, {"dataset_name": "synthetic (lr=5.0e-04)", "final_value": 0.9915, "best_value": 0.9915}, {"dataset_name": "synthetic (lr=1.0e-03)", "final_value": 0.9907, "best_value": 0.9907}, {"dataset_name": "synthetic (lr=5.0e-03)", "final_value": 0.9989, "best_value": 0.9989}, {"dataset_name": "synthetic (lr=1.0e-02)", "final_value": 0.9986, "best_value": 0.9986}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training set; lower values indicate better performance.", "data": [{"dataset_name": "synthetic (lr=1.0e-04)", "final_value": 1.0162, "best_value": 1.0162}, {"dataset_name": "synthetic (lr=5.0e-04)", "final_value": 0.7943, "best_value": 0.7943}, {"dataset_name": "synthetic (lr=1.0e-03)", "final_value": 0.5274, "best_value": 0.5274}, {"dataset_name": "synthetic (lr=5.0e-03)", "final_value": 0.0982, "best_value": 0.0982}, {"dataset_name": "synthetic (lr=1.0e-02)", "final_value": 0.0588, "best_value": 0.0588}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set; lower values indicate better performance.", "data": [{"dataset_name": "synthetic (lr=1.0e-04)", "final_value": 1.0106, "best_value": 1.0106}, {"dataset_name": "synthetic (lr=5.0e-04)", "final_value": 0.7817, "best_value": 0.7817}, {"dataset_name": "synthetic (lr=1.0e-03)", "final_value": 0.5101, "best_value": 0.5101}, {"dataset_name": "synthetic (lr=5.0e-03)", "final_value": 0.1039, "best_value": 0.1039}, {"dataset_name": "synthetic (lr=1.0e-02)", "final_value": 0.0739, "best_value": 0.0739}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test set; higher values indicate better performance.", "data": [{"dataset_name": "synthetic (lr=1.0e-04)", "final_value": 0.6, "best_value": 0.6}, {"dataset_name": "synthetic (lr=5.0e-04)", "final_value": 0.835, "best_value": 0.835}, {"dataset_name": "synthetic (lr=1.0e-03)", "final_value": 0.905, "best_value": 0.905}, {"dataset_name": "synthetic (lr=5.0e-03)", "final_value": 0.96, "best_value": 0.96}, {"dataset_name": "synthetic (lr=1.0e-02)", "final_value": 0.965, "best_value": 0.965}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_3d63da353c8b40799be938360c6975a0_proc_4003350/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_3d63da353c8b40799be938360c6975a0_proc_4003350/synthetic_alignment_curves.png"], ["../../logs/0-run/experiment_results/experiment_7d04e00cb67f4c5b995391df64eb1749_proc_4007053/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_7d04e00cb67f4c5b995391df64eb1749_proc_4007053/synthetic_alignment_curves.png"], ["../../logs/0-run/experiment_results/experiment_f73f6bf2b2154674990fe83951097b64_proc_4007054/synthetic_alignment_val.png", "../../logs/0-run/experiment_results/experiment_f73f6bf2b2154674990fe83951097b64_proc_4007054/synthetic_alignment_train.png", "../../logs/0-run/experiment_results/experiment_f73f6bf2b2154674990fe83951097b64_proc_4007054/synthetic_loss_val.png", "../../logs/0-run/experiment_results/experiment_f73f6bf2b2154674990fe83951097b64_proc_4007054/synthetic_loss_train.png", "../../logs/0-run/experiment_results/experiment_f73f6bf2b2154674990fe83951097b64_proc_4007054/synthetic_accuracy_batch_size.png"], ["../../logs/0-run/experiment_results/experiment_a3902b8d33ea4c21b93d2b6f3c73bc62_proc_4007055/synthetic_pred_vs_gt_wd_0.001.png", "../../logs/0-run/experiment_results/experiment_a3902b8d33ea4c21b93d2b6f3c73bc62_proc_4007055/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_a3902b8d33ea4c21b93d2b6f3c73bc62_proc_4007055/synthetic_alignment_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_4a93baf581a54615b35f3637d22e31aa_proc_4007055/synthetic_final_val_accuracy.png", "../../logs/0-run/experiment_results/experiment_4a93baf581a54615b35f3637d22e31aa_proc_4007055/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_4a93baf581a54615b35f3637d22e31aa_proc_4007055/synthetic_alignment_curves.png"], ["../../logs/0-run/experiment_results/experiment_ce50ae1ad15e4e10a98054e163982731_proc_4007054/synthetic_final_val_alignment.png", "../../logs/0-run/experiment_results/experiment_ce50ae1ad15e4e10a98054e163982731_proc_4007054/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_ce50ae1ad15e4e10a98054e163982731_proc_4007054/synthetic_alignment_curves.png"], [], [], ["../../logs/0-run/experiment_results/experiment_d477207a92c74770973c15cd9256d999_proc_4007054/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_d477207a92c74770973c15cd9256d999_proc_4007054/synthetic_alignment_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_6b277ccfe00a4c08826c431b8ac7d6fd_proc_4007055/synthetic_final_val_loss_bar.png", "../../logs/0-run/experiment_results/experiment_6b277ccfe00a4c08826c431b8ac7d6fd_proc_4007055/synthetic_final_val_alignment_bar.png", "../../logs/0-run/experiment_results/experiment_6b277ccfe00a4c08826c431b8ac7d6fd_proc_4007055/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_6b277ccfe00a4c08826c431b8ac7d6fd_proc_4007055/synthetic_alignment_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_88ad674defc54627a13ed4a86fb95390_proc_4007053/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_88ad674defc54627a13ed4a86fb95390_proc_4007053/synthetic_alignment_curves.png"], ["../../logs/0-run/experiment_results/experiment_7eeac2a50f9146109aca34e1a7661420_proc_4007054/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_7eeac2a50f9146109aca34e1a7661420_proc_4007054/synthetic_alignment_curves.png"], ["../../logs/0-run/experiment_results/experiment_922c3b95d2b7449a9c9eb92fb78f420b_proc_4007055/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_922c3b95d2b7449a9c9eb92fb78f420b_proc_4007055/synthetic_alignment_curves.png"], ["../../logs/0-run/experiment_results/seed_aggregation_86d5007f41624d1e901dee1d4496d233/synthetic_loss_curves_mean_se.png", "../../logs/0-run/experiment_results/seed_aggregation_86d5007f41624d1e901dee1d4496d233/synthetic_alignment_curves_mean_se.png"]], "plot_paths": [["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_3d63da353c8b40799be938360c6975a0_proc_4003350/synthetic_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_3d63da353c8b40799be938360c6975a0_proc_4003350/synthetic_alignment_curves.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_7d04e00cb67f4c5b995391df64eb1749_proc_4007053/synthetic_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_7d04e00cb67f4c5b995391df64eb1749_proc_4007053/synthetic_alignment_curves.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_f73f6bf2b2154674990fe83951097b64_proc_4007054/synthetic_alignment_val.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_f73f6bf2b2154674990fe83951097b64_proc_4007054/synthetic_alignment_train.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_f73f6bf2b2154674990fe83951097b64_proc_4007054/synthetic_loss_val.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_f73f6bf2b2154674990fe83951097b64_proc_4007054/synthetic_loss_train.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_f73f6bf2b2154674990fe83951097b64_proc_4007054/synthetic_accuracy_batch_size.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_a3902b8d33ea4c21b93d2b6f3c73bc62_proc_4007055/synthetic_pred_vs_gt_wd_0.001.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_a3902b8d33ea4c21b93d2b6f3c73bc62_proc_4007055/synthetic_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_a3902b8d33ea4c21b93d2b6f3c73bc62_proc_4007055/synthetic_alignment_curves.png"], [], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4a93baf581a54615b35f3637d22e31aa_proc_4007055/synthetic_final_val_accuracy.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4a93baf581a54615b35f3637d22e31aa_proc_4007055/synthetic_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4a93baf581a54615b35f3637d22e31aa_proc_4007055/synthetic_alignment_curves.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_ce50ae1ad15e4e10a98054e163982731_proc_4007054/synthetic_final_val_alignment.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_ce50ae1ad15e4e10a98054e163982731_proc_4007054/synthetic_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_ce50ae1ad15e4e10a98054e163982731_proc_4007054/synthetic_alignment_curves.png"], [], [], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_d477207a92c74770973c15cd9256d999_proc_4007054/synthetic_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_d477207a92c74770973c15cd9256d999_proc_4007054/synthetic_alignment_curves.png"], [], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_6b277ccfe00a4c08826c431b8ac7d6fd_proc_4007055/synthetic_final_val_loss_bar.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_6b277ccfe00a4c08826c431b8ac7d6fd_proc_4007055/synthetic_final_val_alignment_bar.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_6b277ccfe00a4c08826c431b8ac7d6fd_proc_4007055/synthetic_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_6b277ccfe00a4c08826c431b8ac7d6fd_proc_4007055/synthetic_alignment_curves.png"], [], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_88ad674defc54627a13ed4a86fb95390_proc_4007053/synthetic_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_88ad674defc54627a13ed4a86fb95390_proc_4007053/synthetic_alignment_curves.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_7eeac2a50f9146109aca34e1a7661420_proc_4007054/synthetic_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_7eeac2a50f9146109aca34e1a7661420_proc_4007054/synthetic_alignment_curves.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_922c3b95d2b7449a9c9eb92fb78f420b_proc_4007055/synthetic_loss_curves.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_922c3b95d2b7449a9c9eb92fb78f420b_proc_4007055/synthetic_alignment_curves.png"], ["experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_86d5007f41624d1e901dee1d4496d233/synthetic_loss_curves_mean_se.png", "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/seed_aggregation_86d5007f41624d1e901dee1d4496d233/synthetic_alignment_curves_mean_se.png"]], "plot_analyses": [[{"analysis": "Loss steadily decreases on both training and validation sets from epoch 1 through epoch 10, moving from approximately 1.05 down to 0.53 on training and from about 1.01 down to 0.51 on validation. The gap between train and validation loss remains small (around 0.03\u20130.04 early on, narrowing to roughly 0.02 toward the end), indicating good generalization with no clear signs of overfitting. The curves show diminishing returns after epoch 5 but continue to improve, suggesting the basic implementation is learning effectively on the synthetic data.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_3d63da353c8b40799be938360c6975a0_proc_4003350/synthetic_loss_curves.png"}, {"analysis": "Alignment metric (1\u2013JSD) peaks in early epochs (around 0.9932 at epoch 2\u20133) and then gradually declines until epoch 8 (down to about 0.9912 train, 0.9902 validation), with a slight uptick in the final two epochs. This downward trend after epoch 3 suggests a tension between minimizing reconstruction/classification loss and preserving distributional alignment. The small but consistent train\u2013validation gap (on the order of 10^{-4}) indicates stable but suboptimal alignment over longer training. Early stopping or incorporating an explicit alignment objective could preserve higher alignment scores.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_3d63da353c8b40799be938360c6975a0_proc_4003350/synthetic_alignment_curves.png"}], [], [{"analysis": "Validation alignment curves show that the smallest batch size (16) dips slightly in the first few epochs before climbing steadily to the highest alignment (~0.998) by epoch 10. Batch size 32 follows a similar but more gradual trend, plateauing around ~0.9955. Larger batch sizes (128 and 256) reach lower peak alignment (~0.993\u20130.9933) early and either stagnate or slightly decline, indicating slower or limited adaptation on validation.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_f73f6bf2b2154674990fe83951097b64_proc_4007054/synthetic_alignment_val.png"}, {"analysis": "Training alignment exhibits nearly the same behavior as validation: batch size 16 recovers from an early dip (~0.991) to reach ~0.998, batch size 32 bottoms at ~0.9913 before rising to ~0.9959, while batch sizes 128 and 256 show marginal gains that plateau (bs128 peaks ~0.9934 then drifts down; bs256 peaks ~0.9935). This close match suggests limited overfitting across sizes.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_f73f6bf2b2154674990fe83951097b64_proc_4007054/synthetic_alignment_train.png"}, {"analysis": "Validation loss curves confirm faster convergence for smaller batches: bs16 drops from ~0.9 to ~0.17, bs32 to ~0.28, while bs128 and bs256 only reduce to ~0.75 and ~0.90. There is no sign of validation loss increasing at later epochs, ruling out overfitting, but larger batches converge much more slowly.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_f73f6bf2b2154674990fe83951097b64_proc_4007054/synthetic_loss_val.png"}, {"analysis": "Training loss trends mirror validation loss, with bs16 falling from ~1.0 to ~0.17, bs32 to ~0.29, bs128 to ~0.77, and bs256 to ~0.91 over 10 epochs. The parallel decrease on training and validation indicates stable learning dynamics and again highlights the benefit of smaller batch sizes.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_f73f6bf2b2154674990fe83951097b64_proc_4007054/synthetic_loss_train.png"}, {"analysis": "Final accuracy decreases monotonically as batch size grows: bs16 and bs32 achieve ~97\u201398% accuracy, whereas bs128 falls to ~85% and bs256 to ~74%. This confirms that smaller batches not only improve alignment and loss but also translate into higher end-task performance.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_f73f6bf2b2154674990fe83951097b64_proc_4007054/synthetic_accuracy_batch_size.png"}], [{"analysis": "Data points for ground truth and predictions are tightly clustered at their respective class levels across all sample indices, indicating very high classification accuracy. Mismatches between predicted markers and ground-truth markers are negligible, suggesting that the model with a weight decay of 0.001 produces almost perfect predictions on this synthetic dataset.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_a3902b8d33ea4c21b93d2b6f3c73bc62_proc_4007055/synthetic_pred_vs_gt_wd_0.001.png"}, {"analysis": "All training and validation curves decrease steadily over epochs without any sign of overfitting, as dashed validation curves track their solid counterparts closely. A weight decay of 1e-05 yields the lowest loss throughout, followed by 0.001, 0.0001, and 0.01. Higher weight decay values slow convergence and result in higher final losses.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_a3902b8d33ea4c21b93d2b6f3c73bc62_proc_4007055/synthetic_loss_curves.png"}, {"analysis": "Alignment scores rise quickly in early epochs and plateau near optimal values for all weight decays. The setting wd=0.001 achieves the highest peak alignment (around 0.995) on both train and validation, while wd=1e-05 and wd=0.0001 converge slightly lower and decline marginally after epoch 4. The wd=0.01 curve is moderate, stabilizing around 0.993.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_a3902b8d33ea4c21b93d2b6f3c73bc62_proc_4007055/synthetic_alignment_curves.png"}], [], [{"analysis": "Validation accuracy peaks at no dropout (0.0) and high dropout (0.5) around 0.90\u20130.91, while moderate dropout rates (0.1 and 0.2) yield lower accuracy (~0.84\u20130.85). This suggests that a small amount of regularization may hurt generalization on this synthetic task, but heavy regularization recovers performance, possibly by preventing over-reliance on spurious features.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4a93baf581a54615b35f3637d22e31aa_proc_4007055/synthetic_final_val_accuracy.png"}, {"analysis": "Training and validation loss curves for dropout rates from 0.0 to 0.5 show smooth, monotonic decreases over 10 epochs without signs of divergence or severe overfitting. Higher dropout slows training convergence (higher initial losses and gentler slope), whereas no-dropout converges fastest. The tight gap between train and validation losses indicates good generalization but also that moderate dropout trades off convergence speed with regularization.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4a93baf581a54615b35f3637d22e31aa_proc_4007055/synthetic_loss_curves.png"}, {"analysis": "Alignment metric (1\u2009\u2013\u2009JSD) remains high (>0.99) across all dropout settings, but validation alignment is highest for dropout=0.2, peaking near 0.995. Lower dropout settings show slight declines in alignment after epoch 4, and dropout=0.5 exhibits a steady drop from epoch 5 onward. This indicates that moderate regularization best preserves or enhances alignment between models, while too little or too much harms it.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_4a93baf581a54615b35f3637d22e31aa_proc_4007055/synthetic_alignment_curves.png"}], [{"analysis": "Final validation alignment stays uniformly high across all label smoothing values \u03b1 \u2208 {0, 0.05, 0.1, 0.2}, with only a marginal upward trend at larger \u03b1. This indicates that in this synthetic setting, label smoothing has minimal impact on the ultimate alignment metric, though there is a slight benefit in pushing the score even closer to 1.0 at \u03b1=0.2.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_ce50ae1ad15e4e10a98054e163982731_proc_4007054/synthetic_final_val_alignment.png"}, {"analysis": "Training and validation loss curves decrease nearly linearly over epochs for all \u03b1. The run without smoothing (\u03b1=0) achieves the lowest final train and val losses and converges fastest. As \u03b1 increases, convergence slows and final losses rise, though the gap between train and val loss narrows slightly, suggesting that label smoothing mitigates overfitting at the cost of slower loss reduction.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_ce50ae1ad15e4e10a98054e163982731_proc_4007054/synthetic_loss_curves.png"}, {"analysis": "Alignment (1\u2013JSD) curves exhibit an early peak around epoch 2 followed by a dip to a minimum near epochs 7\u20138, then a recovery by epoch 10. Higher smoothing values, particularly \u03b1=0.2, maintain overall higher alignment throughout and exhibit a stronger rebound in later epochs. This pattern suggests that label smoothing stabilizes alignment during mid-training and improves alignment consistency at convergence.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_ce50ae1ad15e4e10a98054e163982731_proc_4007054/synthetic_alignment_curves.png"}], [], [], [{"analysis": "All three gamma settings show steadily decreasing training and validation loss across epochs 1 to 10. Gamma=0.1 reaches lowest validation loss around 0.76 by epoch 6 and then plateaus, with a very small train\u2013validation gap indicating strong generalization and minimal overfitting. Gamma=0.5 follows a similar trend but starts from a slightly higher initial loss and converges to approximately 0.75 by epoch 10. Gamma=0.9 delivers the fastest training loss reduction (dropping to about 0.60 by epoch 10), but its validation loss only declines to around 0.75\u2014comparable to the other settings. The widening gap between training and validation losses for gamma=0.9 after epoch 6 suggests an increased risk of overfitting under that schedule.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_d477207a92c74770973c15cd9256d999_proc_4007054/synthetic_loss_curves.png"}, {"analysis": "Alignment scores for both training and validation rise quickly in the first few epochs for all gamma values and then level off. Gamma=0.1 peaks near 0.993 by epoch 2, then gradually declines to about 0.992 by epoch 10, showing minor drift. Gamma=0.5 lags early but reaches around 0.993 by epoch 4 before dipping slightly to roughly 0.991 by the end. Gamma=0.9 achieves the highest alignment, climbing to approximately 0.995 on validation by epoch 4 and maintaining around 0.994\u20130.995 thereafter. This indicates the strongest and most stable alignment under gamma=0.9, albeit with marginal gains past epoch 4.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_d477207a92c74770973c15cd9256d999_proc_4007054/synthetic_alignment_curves.png"}], [], [{"analysis": "Final validation loss decreases dramatically when the initial weight standard deviation is very small (0.01 and 0.1 yield around 0.4\u20130.5) but rises sharply at larger scales (0.5 gives ~0.8, and 1.0 shoots up past 5). This suggests that tighter initialization supports stable optimization and effective convergence, whereas larger random scales introduce too much variance and wreck both convergence speed and solution quality.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_6b277ccfe00a4c08826c431b8ac7d6fd_proc_4007055/synthetic_final_val_loss_bar.png"}, {"analysis": "Final validation alignment is nearly perfect (\u22481.0) at init std of 0.01 and 0.1, then degrades to \u22480.86 at 0.5 and \u22480.67 at 1.0. Alignment follows the same trend as loss: small initial weights help the model learn representations that align well with the synthetic user\u2013AI mental\u2010model objective, while large spreads impede that learning.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_6b277ccfe00a4c08826c431b8ac7d6fd_proc_4007055/synthetic_final_val_alignment_bar.png"}, {"analysis": "Loss curves over training epochs show that settings with init std of 0.01 and 0.1 both achieve rapid decay from ~1.1 down to ~0.5 on both train and val, with minimal generalization gap. The 0.5 curve starts around 2.4\u21920.8 by epoch 10, still reasonable but slower. The 1.0 curve barely budges from ~11 down to ~5.5, indicating the optimizer struggles under high\u2010variance initialization. Stability and generalization both favor the smaller scales.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_6b277ccfe00a4c08826c431b8ac7d6fd_proc_4007055/synthetic_loss_curves.png"}, {"analysis": "Alignment curves reveal that 0.01 and 0.1 begin near perfect alignment (>0.99) and remain flat throughout training on both train and val splits. The 0.5 curve starts near 0.62 and climbs steadily to ~0.88 (train) and ~0.85 (val), showing the model can recover alignment if not too badly initialized. The 1.0 run barely improves, going from ~0.58 to ~0.63 (train) and ~0.67 (val) after 10 epochs, confirming that overly large initialization blunts the co\u2010adaptive alignment loop.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_6b277ccfe00a4c08826c431b8ac7d6fd_proc_4007055/synthetic_alignment_curves.png"}], [], [{"analysis": "Training and validation loss curves show a clear speed-quality trade-off across learning rates. At lr=0.01 and lr=0.005, both training and validation losses decrease rapidly, reaching below 0.1 by epoch 10. lr=0.01 converges fastest, followed by lr=0.005. lr=0.001 achieves moderate progress but plateaus around 0.5 on validation loss. The smallest lrs (0.0005, 0.0001) converge very slowly, with final validation losses near 0.78 and 1.01, respectively. No signs of divergence or severe overfitting are observed even at high lr, but validation curves flatten beyond epoch 8, suggesting potential diminishing returns beyond epoch 10 for the highest rates.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_88ad674defc54627a13ed4a86fb95390_proc_4007053/synthetic_loss_curves.png"}, {"analysis": "Alignment curves (1 \u2013 JSD) reflect the same lr hierarchy. Training alignment at lr=0.01 jumps above 0.998 by epoch 4 and plateaus near 0.999; lr=0.005 follows closely, reaching ~0.999 by epoch 10. Lower lrs show marginal alignment gains (<0.993 for 0.0001/0.0005, dipping to ~0.991 for 0.001). Validation alignment mirrors this pattern: lr=0.01 rapidly climbs above 0.998 by epoch 6 (with a slight plateau/dip afterward), and lr=0.005 steadily increases to ~0.999 by epoch 10. This indicates that higher lrs not only speed up loss minimization but also accelerate model-user alignment improvement without notable instability.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_88ad674defc54627a13ed4a86fb95390_proc_4007053/synthetic_alignment_curves.png"}], [{"analysis": "Loss Curves\n- lr=0.01 yields the steepest drop in both training and validation loss, approaching ~0.05 on train and ~0.06 on validation by epoch 10. Convergence is very rapid, and the small train\u2013val gap suggests good generalization up to this point.\n- lr=0.005 also converges quickly, with training loss ~0.10 and validation loss ~0.12 at epoch 10. The loss trajectories track closely across epochs, indicating a healthy trade-off between speed and stability.\n- lr=0.001 shows moderate progress: training loss reaches ~0.53, validation ~0.51 by epoch 10. Both curves decline smoothly but more slowly, hinting at under-utilization of capacity.\n- lr=0.0005 and lr=0.0001 progress sluggishly, with training losses of ~0.80 and ~1.02 (and similar validation values) at epoch 10. These low rates may require many more epochs to achieve comparable performance.\n- Overall, higher rates (0.005\u20130.01) offer much faster convergence. lr=0.01 edges out lr=0.005 in pure speed but carries a slight risk of over-sharpening if training continues beyond epoch 10.\n\nAlignment Curves\n- lr=0.01 boosts training alignment to ~0.9993 by epoch 7 and validation alignment to ~0.9990 by epoch 5, then plateaus. Very tight train\u2013val alignment indicates the model quickly aligns its mental model with the synthetic ground truth.\n- lr=0.005 attains ~0.9990 train alignment and ~0.9989 validation alignment by epoch 10, tracking closely with lr=0.01 but requiring a few extra epochs.\n- lr=0.001 improves until epoch 2 then dips slightly, settling around ~0.991 by epoch 10 on both sets, suggesting this rate is too low to refine subtle alignment discrepancies.\n- lr=0.0005 peaks at ~0.9932 around epoch 3, then gradually drifts to ~0.992 by epoch 10; the behavior is stable but limited in ultimate alignment quality.\n- lr=0.0001 yields a nearly linear uptick from ~0.9922 to ~0.9930 by epoch 10, showing very slow but consistent alignment gains.\n- In summary, lr=0.005 balances speed and reliability, while lr=0.01 maximizes alignment fast but should be monitored for overfitting if training goes much longer.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_7eeac2a50f9146109aca34e1a7661420_proc_4007054/synthetic_loss_curves.png"}], [{"analysis": "Training loss curves show that the highest learning rate (0.01) yields the fastest decrease and reaches the lowest loss by epoch 10, followed by lr=0.005, lr=0.001, lr=0.0005, and lr=0.0001. Validation loss mirrors this trend closely, indicating that higher rates drive quicker convergence without significant overfitting; lr=0.01 achieves the best validation loss, while lr=0.0001 barely improves over ten epochs.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_922c3b95d2b7449a9c9eb92fb78f420b_proc_4007055/synthetic_loss_curves.png"}, {"analysis": "Training alignment (1\u2013JSD) rises gradually for lr=0.0001 and lr=0.0005 but plateaus below 0.993. lr=0.001 peaks early then drifts downward slightly. lr=0.005 and lr=0.01 both dip briefly at the start then climb sharply, reaching near-perfect alignment (~0.999) by epoch 10. Validation alignment follows suit: lr=0.005 offers a strong, stable increase without downturn, while lr=0.01 peaks fastest around epoch 6 then shows a minor decline, suggesting a hint of over-tuning. Mid-range lr=0.005 balances speed and stability best.", "plot_path": "experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_922c3b95d2b7449a9c9eb92fb78f420b_proc_4007055/synthetic_alignment_curves.png"}], []], "vlm_feedback_summary": ["Loss curves show effective training and strong generalization with no\noverfitting. Alignment metric peaks early and degrades with continued training,\nindicating a tradeoff between loss minimization and distributional alignment;\nconsider early stopping or targeted alignment objectives to maintain high\nalignment.", "[]", "Batch size 16 delivers the best overall performance in terms of alignment, loss\nreduction, and final accuracy. Batch size 32 is a close second, while larger\nbatches (128 and 256) converge more slowly and underperform. For continued\nbaseline tuning, keep batch size at 16 or 32 and explore adjustments to learning\nrate or epoch count to further refine convergence. To test generalization across\ndiverse text classification tasks, introduce additional Hugging Face datasets\nsuch as \u201cag_news\u201d for news-topic classification and \u201cdbpedia_14\u201d for multi-class\ndomain categorization.", "Model predictions nearly match ground truth with minimal errors. Lower weight\ndecay yields faster loss reduction and best final losses, but a moderate decay\n(0.001) provides the best alignment performance. No overfitting observed.", "[]", "Moderate dropout (0.1\u20130.2) degrades predictive accuracy compared to extremes,\nbut dropout=0.2 yields the best alignment score on validation. No dropout\nconverges fastest but risks slight alignment decay. Heavy dropout slows training\nand alignment over time. There is a clear trade-off between accuracy,\nconvergence speed, and alignment robustness. To test generalization, evaluate on\ntwo additional HuggingFace text-classification datasets such as AG News (topic\nclassification) and IMDb (sentiment analysis).", "Although final validation alignment is almost invariant across smoothing levels,\nloss trajectories reveal that unsmoothed labels converge faster with lower loss,\nwhile smoothing (\u03b1>0) reduces the train\u2013val gap and yields more stable mid- and\nlate-stage alignment. To further validate generalization on real-world tasks, we\nrecommend testing on HuggingFace datasets such as 'ag_news' for news topic\nclassification and 'yelp_polarity' for sentiment analysis.", "[]", "[]", "Loss analysis suggests gamma=0.9 facilitates the fastest fitting but introduces\noverfitting risk, while gamma=0.1 offers the best balance between convergence\nspeed and generalization. Alignment analysis confirms gamma=0.9 yields the\nhighest and most stable bidirectional alignment, with diminishing returns after\nepoch 4. To assess robustness and generalizability, we recommend evaluating the\ntuned models on at least two additional HuggingFace text classification\nbenchmarks\u2014AG News and Amazon Polarity\u2014to further validate performance and\nalignment across diverse real-world datasets.", "[]", "Initialization std plays a critical role: small stds (0.01\u20130.1) yield fast\nconvergence, low loss, and near\u2010perfect alignment, while large stds (0.5\u20131.0)\ncause slow learning, high loss, and poor alignment. Recommend using init std\u22640.1\nmoving forward.", "[]", "High learning rates (0.005\u20130.01) deliver the best trade-off between convergence\nspeed and final performance in both loss minimization and alignment score.\nlr=0.01 is fastest but shows early plateau; lr=0.005 offers slightly smoother\ngains and is recommended for further tuning. Next steps: extend training to\n15\u201320 epochs at lr=0.005, keep batch size constant, and introduce two new\nHuggingFace text-classification benchmarks\u2014AG News and DBpedia_14\u2014to evaluate\ngeneralization.", "High learning rates (0.005\u20130.01) drive rapid loss reduction and near-perfect\nalignment within 10 epochs; lower rates underperform on both metrics. lr=0.005\nis recommended for a compromise between convergence speed and robustness.", "Higher learning rates (especially 0.005 and 0.01) deliver rapid, effective\nconvergence in both loss reduction and alignment improvement, with lr=0.005\noffering the most stable performance across training and validation. Lower rates\nconverge too slowly for practical use. Next steps: adopt lr=0.005 for baseline\ntuning and extend evaluation to two diverse HuggingFace benchmarks\u2014AG News for\ngeneral news classification and Banking77 for fine-grained domain-specific\nintent classification\u2014to assess robustness across real-world text tasks.", "[]"], "exec_time": [1.9232463836669922, 3.7596991062164307, 4.377376556396484, 3.2182772159576416, 3.247035264968872, 3.335667848587036, 4.373184442520142, 0.05927562713623047, 0.05782175064086914, 2.7336087226867676, 3.5467801094055176, 3.301076889038086, 4.862944841384888, 4.312450647354126, 3.7469756603240967, 3.773467779159546, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"synthetic\"]"], [], ["['synthetic']"], ["['synthetic']"], [], ["['synthetic']"], ["['synthetic']"], [], [], ["[\"synthetic\"]"], [], ["['Synthetic Data']"], [], ["[\"synthetic\"]"], ["['synthetic']"], ["[\"synthetic\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    exp = experiment_data[\"synthetic\"]\n    epochs = exp[\"epochs\"]\n    losses_train = exp[\"losses\"][\"train\"]\n    losses_val = exp[\"losses\"][\"val\"]\n    align_train = exp[\"metrics\"][\"train\"]\n    align_val = exp[\"metrics\"][\"val\"]\n    preds = exp[\"predictions\"]\n    gts = exp[\"ground_truth\"]\n\n    # Plot loss curves\n    try:\n        plt.figure()\n        plt.plot(epochs, losses_train, marker=\"o\", label=\"Train Loss\")\n        plt.plot(epochs, losses_val, marker=\"s\", label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"Synthetic Dataset Loss Curves\\nTrain vs Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot: {e}\")\n        plt.close()\n\n    # Plot alignment curves\n    try:\n        plt.figure()\n        plt.plot(epochs, align_train, marker=\"o\", label=\"Train Alignment\")\n        plt.plot(epochs, align_val, marker=\"s\", label=\"Val Alignment\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Alignment (1 - JSD)\")\n        plt.title(\"Synthetic Dataset Alignment Metric\\nTrain vs Validation Alignment\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_alignment_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating alignment plot: {e}\")\n        plt.close()\n\n    # Compute and print final accuracy\n    accuracy = np.mean(preds == gts)\n    print(f\"Final accuracy on synthetic dataset: {accuracy:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = {}\n\nsd = data.get(\"learning_rate\", {}).get(\"synthetic\", {})\n\n# Plot 1: Loss curves\ntry:\n    lrs = sd.get(\"lrs\", [])\n    train_losses = sd.get(\"losses\", {}).get(\"train\", [])\n    val_losses = sd.get(\"losses\", {}).get(\"val\", [])\n    epochs = range(1, len(train_losses[0]) + 1)\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for lr, tr in zip(lrs, train_losses):\n        axes[0].plot(epochs, tr, label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[0].legend()\n    for lr, vl in zip(lrs, val_losses):\n        axes[1].plot(epochs, vl, label=f\"lr={lr}\")\n    axes[1].set_title(\"Validation Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    axes[1].legend()\n    fig.suptitle(\n        \"Synthetic Dataset - Loss Curves\\nLeft: Training Loss, Right: Validation Loss\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\n# Plot 2: Alignment curves\ntry:\n    train_align = sd.get(\"metrics\", {}).get(\"train\", [])\n    val_align = sd.get(\"metrics\", {}).get(\"val\", [])\n    epochs = range(1, len(train_align[0]) + 1)\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for lr, ta in zip(lrs, train_align):\n        axes[0].plot(epochs, ta, label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Alignment\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Alignment (1-JSD)\")\n    axes[0].legend()\n    for lr, va in zip(lrs, val_align):\n        axes[1].plot(epochs, va, label=f\"lr={lr}\")\n    axes[1].set_title(\"Validation Alignment\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Alignment (1-JSD)\")\n    axes[1].legend()\n    fig.suptitle(\n        \"Synthetic Dataset - Alignment Curves\\nLeft: Training Alignment, Right: Validation Alignment\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_alignment_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    ed = experiment_data[\"batch_size\"][\"synthetic\"]\n    batch_sizes = ed[\"batch_sizes\"]\n    epochs = ed[\"epochs\"]\n    metrics = ed[\"metrics\"]\n    losses = ed[\"losses\"]\n    preds_list = ed[\"predictions\"]\n    gt = ed[\"ground_truth\"]\n\n    # Training Alignment vs Epochs\n    try:\n        fig, ax = plt.subplots()\n        fig.suptitle(\"Synthetic dataset\")\n        for bs, vals in zip(batch_sizes, metrics[\"train\"]):\n            ax.plot(epochs, vals, label=f\"bs={bs}\")\n        ax.set_title(\"Training Alignment vs Epochs\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Alignment\")\n        ax.legend()\n        fig.savefig(os.path.join(working_dir, \"synthetic_alignment_train.png\"))\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating alignment train plot: {e}\")\n        plt.close()\n\n    # Validation Alignment vs Epochs\n    try:\n        fig, ax = plt.subplots()\n        fig.suptitle(\"Synthetic dataset\")\n        for bs, vals in zip(batch_sizes, metrics[\"val\"]):\n            ax.plot(epochs, vals, label=f\"bs={bs}\")\n        ax.set_title(\"Validation Alignment vs Epochs\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Alignment\")\n        ax.legend()\n        fig.savefig(os.path.join(working_dir, \"synthetic_alignment_val.png\"))\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating alignment val plot: {e}\")\n        plt.close()\n\n    # Training Loss vs Epochs\n    try:\n        fig, ax = plt.subplots()\n        fig.suptitle(\"Synthetic dataset\")\n        for bs, vals in zip(batch_sizes, losses[\"train\"]):\n            ax.plot(epochs, vals, label=f\"bs={bs}\")\n        ax.set_title(\"Training Loss vs Epochs\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n        ax.legend()\n        fig.savefig(os.path.join(working_dir, \"synthetic_loss_train.png\"))\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating loss train plot: {e}\")\n        plt.close()\n\n    # Validation Loss vs Epochs\n    try:\n        fig, ax = plt.subplots()\n        fig.suptitle(\"Synthetic dataset\")\n        for bs, vals in zip(batch_sizes, losses[\"val\"]):\n            ax.plot(epochs, vals, label=f\"bs={bs}\")\n        ax.set_title(\"Validation Loss vs Epochs\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n        ax.legend()\n        fig.savefig(os.path.join(working_dir, \"synthetic_loss_val.png\"))\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating loss val plot: {e}\")\n        plt.close()\n\n    # Final Accuracy vs Batch Size\n    acc_list = [np.mean(preds == gt) for preds in preds_list]\n    print(f\"Final accuracies per batch size: {dict(zip(batch_sizes, acc_list))}\")\n    try:\n        fig, ax = plt.subplots()\n        fig.suptitle(\"Synthetic dataset\")\n        ax.bar([str(bs) for bs in batch_sizes], acc_list)\n        ax.set_title(\"Final Accuracy vs Batch Size\")\n        ax.set_xlabel(\"Batch Size\")\n        ax.set_ylabel(\"Accuracy\")\n        fig.savefig(os.path.join(working_dir, \"synthetic_accuracy_batch_size.png\"))\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating accuracy bar plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    synth = experiment_data[\"weight_decay\"][\"synthetic\"]\n    wds = synth[\"weight_decay_values\"]\n    tr_losses = synth[\"losses\"][\"train\"]\n    vl_losses = synth[\"losses\"][\"val\"]\n    tr_align = synth[\"metrics\"][\"train\"]\n    vl_align = synth[\"metrics\"][\"val\"]\n    preds = synth[\"predictions\"]\n    gt = synth[\"ground_truth\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    tr_losses = vl_losses = tr_align = vl_align = preds = gt = wds = None\n\n# Print final validation metrics\nif wds is not None:\n    final_vl = vl_losses[:, -1]\n    final_va = vl_align[:, -1]\n    print(\"Weight decays:\", wds)\n    print(\"Final val loss:\", final_vl)\n    print(\"Final val alignment:\", final_va)\n\n# Plot loss curves\ntry:\n    plt.figure()\n    epochs = np.arange(1, tr_losses.shape[1] + 1)\n    for i, wd in enumerate(wds):\n        plt.plot(epochs, tr_losses[i], label=f\"train wd={wd}\")\n        plt.plot(epochs, vl_losses[i], \"--\", label=f\"val wd={wd}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss Curves - Synthetic dataset\\nSolid: train, Dashed: val\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# Plot alignment curves\ntry:\n    plt.figure()\n    for i, wd in enumerate(wds):\n        plt.plot(epochs, tr_align[i], label=f\"train wd={wd}\")\n        plt.plot(epochs, vl_align[i], \"--\", label=f\"val wd={wd}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Alignment\")\n    plt.title(\"Alignment Curves - Synthetic dataset\\nSolid: train, Dashed: val\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_alignment_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating alignment curves: {e}\")\n    plt.close()\n\n# Plot predictions vs ground truth for best model\nif preds is not None:\n    best_idx = int(np.argmax(vl_align[:, -1]))\n    try:\n        plt.figure(figsize=(10, 4))\n        plt.plot(gt, \"o\", label=\"Ground Truth\")\n        plt.plot(preds[best_idx], \"^\", label=\"Predicted\")\n        plt.xlabel(\"Sample Index\")\n        plt.ylabel(\"Class\")\n        plt.title(\n            f\"Prediction vs Ground Truth - Synthetic dataset (wd={wds[best_idx]})\\nSymbols: o=GT, ^=Pred\"\n        )\n        plt.legend()\n        fname = f\"synthetic_pred_vs_gt_wd_{wds[best_idx]}.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating prediction vs ground truth plot: {e}\")\n        plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    data_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(data_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Extract synthetic dataset results\nexp = experiment_data[\"dropout_rate\"][\"synthetic\"]\ndropout_rates = exp[\"hyperparams\"]\ntrain_losses = exp[\"losses\"][\"train\"]\nval_losses = exp[\"losses\"][\"val\"]\ntrain_metrics = exp[\"metrics\"][\"train\"]\nval_metrics = exp[\"metrics\"][\"val\"]\npreds_list = exp[\"predictions\"]\ngts_list = exp[\"ground_truth\"]\n\n# Plot loss curves\ntry:\n    plt.figure()\n    epochs = range(1, len(train_losses[0]) + 1)\n    for dr, t_loss, v_loss in zip(dropout_rates, train_losses, val_losses):\n        plt.plot(epochs, t_loss, label=f\"Train dr={dr}\")\n        plt.plot(epochs, v_loss, \"--\", label=f\"Val dr={dr}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Synthetic Dataset Loss Curves\\nTraining (solid) vs Validation (dashed)\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Plot alignment metric curves\ntry:\n    plt.figure()\n    for dr, t_met, v_met in zip(dropout_rates, train_metrics, val_metrics):\n        plt.plot(epochs, t_met, label=f\"Train dr={dr}\")\n        plt.plot(epochs, v_met, \"--\", label=f\"Val dr={dr}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Alignment Metric\")\n    plt.title(\"Synthetic Dataset Alignment Curves\\nAlignment metric (1 - JSD)\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_alignment_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating alignment curves plot: {e}\")\n    plt.close()\n\n# Plot final validation accuracy\ntry:\n    accs = [np.mean(preds == gts) for preds, gts in zip(preds_list, gts_list)]\n    plt.figure()\n    idx = np.arange(len(dropout_rates))\n    plt.bar(idx, accs)\n    plt.xticks(idx, dropout_rates)\n    plt.xlabel(\"Dropout Rate\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Synthetic Dataset Final Validation Accuracy\\nAccuracy per Dropout Rate\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_final_val_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final accuracy plot: {e}\")\n    plt.close()\n\n# Print final accuracies\nfor dr, acc in zip(dropout_rates, accs):\n    print(f\"Dropout={dr}: Final Val Accuracy={acc:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    exp_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = exp_data[\"label_smoothing\"][\"synthetic\"]\n    alphas = data[\"alphas\"]\n    epochs = data[\"epochs\"]\n    loss_train = data[\"losses\"][\"train\"]\n    loss_val = data[\"losses\"][\"val\"]\n    align_train = data[\"metrics\"][\"train\"]\n    align_val = data[\"metrics\"][\"val\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for i, alpha in enumerate(alphas):\n        plt.plot(epochs, loss_train[i], label=f\"\u03b1={alpha} Train\")\n        plt.plot(epochs, loss_val[i], \"--\", label=f\"\u03b1={alpha} Val\")\n    plt.title(\"Synthetic Dataset: Loss Curves (Train vs Val)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# Plot alignment curves\ntry:\n    plt.figure()\n    for i, alpha in enumerate(alphas):\n        plt.plot(epochs, align_train[i], label=f\"\u03b1={alpha} Train\")\n        plt.plot(epochs, align_val[i], \"--\", label=f\"\u03b1={alpha} Val\")\n    plt.title(\"Synthetic Dataset: Alignment Curves (Train vs Val)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Alignment (1\u2212JSD)\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_alignment_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating alignment curves: {e}\")\n    plt.close()\n\n# Plot final validation alignment vs alpha\ntry:\n    final_val_align = [lst[-1] for lst in align_val]\n    plt.figure()\n    plt.bar([str(a) for a in alphas], final_val_align)\n    plt.title(\"Synthetic Dataset: Final Validation Alignment vs \u03b1\")\n    plt.xlabel(\"Label Smoothing \u03b1\")\n    plt.ylabel(\"Final Validation Alignment (1\u2212JSD)\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_final_val_alignment.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final alignment bar chart: {e}\")\n    plt.close()\n\n# Print out final metrics\nprint(\"Final validation alignment for each alpha:\")\nfor alpha, val in zip(alphas, final_val_align):\n    print(f\"\u03b1={alpha}: {val:.4f}\")\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    exp_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    synth = exp_data[\"lr_scheduler_gamma\"][\"synthetic\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    synth = {}\n\n# Compute and print accuracies\nfor key, d in synth.items():\n    preds = d.get(\"predictions\", [])\n    gts = d.get(\"ground_truth\", [])\n    if len(preds) and len(gts):\n        acc = np.mean(preds == gts)\n        print(f\"{key} accuracy: {acc:.4f}\")\n\n# Plot Loss Curves\ntry:\n    plt.figure()\n    for key, d in synth.items():\n        epochs = d[\"epochs\"]\n        train_losses = d[\"losses\"][\"train\"]\n        val_losses = d[\"losses\"][\"val\"]\n        plt.plot(epochs, train_losses, label=f\"{key} Train\")\n        plt.plot(epochs, val_losses, \"--\", label=f\"{key} Val\")\n    plt.title(\"Synthetic Dataset\\nTrain vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Plot Alignment Curves\ntry:\n    plt.figure()\n    for key, d in synth.items():\n        epochs = d[\"epochs\"]\n        train_align = d[\"metrics\"][\"train\"]\n        val_align = d[\"metrics\"][\"val\"]\n        plt.plot(epochs, train_align, label=f\"{key} Train\")\n        plt.plot(epochs, val_align, \"--\", label=f\"{key} Val\")\n    plt.title(\"Synthetic Dataset\\nTrain vs Validation Alignment\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Alignment\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_alignment_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating alignment plot: {e}\")\n    plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    inits = sorted(exp_data[\"init_std\"].keys(), key=float)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    inits = []\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    for std in inits:\n        exp = exp_data[\"init_std\"][std]\n        epochs = exp[\"epochs\"]\n        plt.plot(epochs, exp[\"losses\"][\"train\"], label=f\"train {std}\")\n        plt.plot(epochs, exp[\"losses\"][\"val\"], \"--\", label=f\"val {std}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Synthetic Data Loss Curves\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# 2) Alignment curves\ntry:\n    plt.figure()\n    for std in inits:\n        exp = exp_data[\"init_std\"][std]\n        epochs = exp[\"epochs\"]\n        plt.plot(epochs, exp[\"metrics\"][\"train\"], label=f\"train align {std}\")\n        plt.plot(epochs, exp[\"metrics\"][\"val\"], \"--\", label=f\"val align {std}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Alignment\")\n    plt.title(\"Synthetic Data Alignment Curves\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_alignment_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating alignment curves: {e}\")\n    plt.close()\n\n# 3) Final validation loss bar chart\ntry:\n    finals = [exp_data[\"init_std\"][s][\"losses\"][\"val\"][-1] for s in inits]\n    plt.figure()\n    plt.bar(inits, finals)\n    plt.xlabel(\"Init Std\")\n    plt.ylabel(\"Final Val Loss\")\n    plt.title(\"Synthetic Data Final Validation Loss by Init Std\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_final_val_loss_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final val loss bar: {e}\")\n    plt.close()\n\n# 4) Final validation alignment bar chart\ntry:\n    finals_align = [exp_data[\"init_std\"][s][\"metrics\"][\"val\"][-1] for s in inits]\n    plt.figure()\n    plt.bar(inits, finals_align)\n    plt.xlabel(\"Init Std\")\n    plt.ylabel(\"Final Val Alignment\")\n    plt.title(\"Synthetic Data Final Validation Alignment by Init Std\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_final_val_alignment_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final val alignment bar: {e}\")\n    plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = {}\n\nsd = data.get(\"learning_rate\", {}).get(\"synthetic\", {})\n\n# Plot 1: Loss curves\ntry:\n    lrs = sd.get(\"lrs\", [])\n    train_losses = sd.get(\"losses\", {}).get(\"train\", [])\n    val_losses = sd.get(\"losses\", {}).get(\"val\", [])\n    epochs = range(1, len(train_losses[0]) + 1)\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for lr, tr in zip(lrs, train_losses):\n        axes[0].plot(epochs, tr, label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[0].legend()\n    for lr, vl in zip(lrs, val_losses):\n        axes[1].plot(epochs, vl, label=f\"lr={lr}\")\n    axes[1].set_title(\"Validation Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    axes[1].legend()\n    fig.suptitle(\n        \"Synthetic Dataset - Loss Curves\\nLeft: Training Loss, Right: Validation Loss\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\n# Plot 2: Alignment curves\ntry:\n    train_align = sd.get(\"metrics\", {}).get(\"train\", [])\n    val_align = sd.get(\"metrics\", {}).get(\"val\", [])\n    epochs = range(1, len(train_align[0]) + 1)\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for lr, ta in zip(lrs, train_align):\n        axes[0].plot(epochs, ta, label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Alignment\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Alignment (1-JSD)\")\n    axes[0].legend()\n    for lr, va in zip(lrs, val_align):\n        axes[1].plot(epochs, va, label=f\"lr={lr}\")\n    axes[1].set_title(\"Validation Alignment\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Alignment (1-JSD)\")\n    axes[1].legend()\n    fig.suptitle(\n        \"Synthetic Dataset - Alignment Curves\\nLeft: Training Alignment, Right: Validation Alignment\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_alignment_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = {}\n\nsd = data.get(\"learning_rate\", {}).get(\"synthetic\", {})\n\n# Plot 1: Loss curves\ntry:\n    lrs = sd.get(\"lrs\", [])\n    train_losses = sd.get(\"losses\", {}).get(\"train\", [])\n    val_losses = sd.get(\"losses\", {}).get(\"val\", [])\n    epochs = range(1, len(train_losses[0]) + 1)\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for lr, tr in zip(lrs, train_losses):\n        axes[0].plot(epochs, tr, label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[0].legend()\n    for lr, vl in zip(lrs, val_losses):\n        axes[1].plot(epochs, vl, label=f\"lr={lr}\")\n    axes[1].set_title(\"Validation Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    axes[1].legend()\n    fig.suptitle(\n        \"Synthetic Dataset - Loss Curves\\nLeft: Training Loss, Right: Validation Loss\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\n# Plot 2: Alignment curves\ntry:\n    train_align = sd.get(\"metrics\", {}).get(\"train\", [])\n    val_align = sd.get(\"metrics\", {}).get(\"val\", [])\n    epochs = range(1, len(train_align[0]) + 1)\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for lr, ta in zip(lrs, train_align):\n        axes[0].plot(epochs, ta, label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Alignment\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Alignment (1-JSD)\")\n    axes[0].legend()\n    for lr, va in zip(lrs, val_align):\n        axes[1].plot(epochs, va, label=f\"lr={lr}\")\n    axes[1].set_title(\"Validation Alignment\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Alignment (1-JSD)\")\n    axes[1].legend()\n    fig.suptitle(\n        \"Synthetic Dataset - Alignment Curves\\nLeft: Training Alignment, Right: Validation Alignment\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_alignment_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = {}\n\nsd = data.get(\"learning_rate\", {}).get(\"synthetic\", {})\n\n# Plot 1: Loss curves\ntry:\n    lrs = sd.get(\"lrs\", [])\n    train_losses = sd.get(\"losses\", {}).get(\"train\", [])\n    val_losses = sd.get(\"losses\", {}).get(\"val\", [])\n    epochs = range(1, len(train_losses[0]) + 1)\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for lr, tr in zip(lrs, train_losses):\n        axes[0].plot(epochs, tr, label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[0].legend()\n    for lr, vl in zip(lrs, val_losses):\n        axes[1].plot(epochs, vl, label=f\"lr={lr}\")\n    axes[1].set_title(\"Validation Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    axes[1].legend()\n    fig.suptitle(\n        \"Synthetic Dataset - Loss Curves\\nLeft: Training Loss, Right: Validation Loss\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\n# Plot 2: Alignment curves\ntry:\n    train_align = sd.get(\"metrics\", {}).get(\"train\", [])\n    val_align = sd.get(\"metrics\", {}).get(\"val\", [])\n    epochs = range(1, len(train_align[0]) + 1)\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for lr, ta in zip(lrs, train_align):\n        axes[0].plot(epochs, ta, label=f\"lr={lr}\")\n    axes[0].set_title(\"Training Alignment\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Alignment (1-JSD)\")\n    axes[0].legend()\n    for lr, va in zip(lrs, val_align):\n        axes[1].plot(epochs, va, label=f\"lr={lr}\")\n    axes[1].set_title(\"Validation Alignment\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Alignment (1-JSD)\")\n    axes[1].legend()\n    fig.suptitle(\n        \"Synthetic Dataset - Alignment Curves\\nLeft: Training Alignment, Right: Validation Alignment\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_alignment_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load and aggregate data across runs\ntry:\n    experiment_data_path_list = [\n        \"experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_88ad674defc54627a13ed4a86fb95390_proc_4007053/experiment_data.npy\",\n        \"experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_7eeac2a50f9146109aca34e1a7661420_proc_4007054/experiment_data.npy\",\n        \"experiments/2025-05-21_18-26-09_bidirectional_mental_model_alignment_attempt_0/logs/0-run/experiment_results/experiment_922c3b95d2b7449a9c9eb92fb78f420b_proc_4007055/experiment_data.npy\",\n    ]\n    all_experiment_data = []\n    for rel_path in experiment_data_path_list:\n        data = np.load(\n            os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\"), rel_path), allow_pickle=True\n        ).item()\n        all_experiment_data.append(data)\n    dataset = \"synthetic\"\n    # Assume all runs share the same lrs\n    sd0 = all_experiment_data[0].get(\"learning_rate\", {}).get(dataset, {})\n    lrs = sd0.get(\"lrs\", [])\n    n_runs = len(all_experiment_data)\n    # Collect per-run metrics\n    train_losses_list = []\n    val_losses_list = []\n    train_align_list = []\n    val_align_list = []\n    for d in all_experiment_data:\n        sd = d.get(\"learning_rate\", {}).get(dataset, {})\n        train_losses_list.append(sd.get(\"losses\", {}).get(\"train\", []))\n        val_losses_list.append(sd.get(\"losses\", {}).get(\"val\", []))\n        train_align_list.append(sd.get(\"metrics\", {}).get(\"train\", []))\n        val_align_list.append(sd.get(\"metrics\", {}).get(\"val\", []))\n    # Convert to arrays (shape: n_runs x n_lrs x n_epochs)\n    train_losses_arr = np.array(train_losses_list, dtype=float)\n    val_losses_arr = np.array(val_losses_list, dtype=float)\n    train_align_arr = np.array(train_align_list, dtype=float)\n    val_align_arr = np.array(val_align_list, dtype=float)\n    # Compute mean and standard error across runs\n    mean_train_losses = np.mean(train_losses_arr, axis=0)\n    se_train_losses = np.std(train_losses_arr, axis=0, ddof=1) / np.sqrt(n_runs)\n    mean_val_losses = np.mean(val_losses_arr, axis=0)\n    se_val_losses = np.std(val_losses_arr, axis=0, ddof=1) / np.sqrt(n_runs)\n    mean_train_align = np.mean(train_align_arr, axis=0)\n    se_train_align = np.std(train_align_arr, axis=0, ddof=1) / np.sqrt(n_runs)\n    mean_val_align = np.mean(val_align_arr, axis=0)\n    se_val_align = np.std(val_align_arr, axis=0, ddof=1) / np.sqrt(n_runs)\n    # Print final validation metrics\n    for idx, lr in enumerate(lrs):\n        print(\n            f\"LR={lr} Final Val Loss: {mean_val_losses[idx,-1]:.4f} \u00b1 {se_val_losses[idx,-1]:.4f}\"\n        )\n        print(\n            f\"LR={lr} Final Val Alignment: {mean_val_align[idx,-1]:.4f} \u00b1 {se_val_align[idx,-1]:.4f}\"\n        )\nexcept Exception as e:\n    print(f\"Error loading or aggregating data: {e}\")\n\n# Plot 1: Loss curves with mean \u00b1 SE\ntry:\n    if \"mean_train_losses\" not in locals():\n        raise ValueError(\"Aggregated data unavailable for losses\")\n    epochs = np.arange(1, mean_train_losses.shape[1] + 1)\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, mean_train_losses[i], label=f\"lr={lr}\")\n        axes[0].fill_between(\n            epochs,\n            mean_train_losses[i] - se_train_losses[i],\n            mean_train_losses[i] + se_train_losses[i],\n            alpha=0.2,\n        )\n    axes[0].set_title(\"Training Loss\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[0].legend()\n    for i, lr in enumerate(lrs):\n        axes[1].plot(epochs, mean_val_losses[i], label=f\"lr={lr}\")\n        axes[1].fill_between(\n            epochs,\n            mean_val_losses[i] - se_val_losses[i],\n            mean_val_losses[i] + se_val_losses[i],\n            alpha=0.2,\n        )\n    axes[1].set_title(\"Validation Loss\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    axes[1].legend()\n    fig.suptitle(\n        \"Synthetic Dataset - Loss Curves\\nLeft: Training Loss, Right: Validation Loss\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves_mean_se.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Plot 2: Alignment curves with mean \u00b1 SE\ntry:\n    if \"mean_train_align\" not in locals():\n        raise ValueError(\"Aggregated data unavailable for alignment\")\n    epochs = np.arange(1, mean_train_align.shape[1] + 1)\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for i, lr in enumerate(lrs):\n        axes[0].plot(epochs, mean_train_align[i], label=f\"lr={lr}\")\n        axes[0].fill_between(\n            epochs,\n            mean_train_align[i] - se_train_align[i],\n            mean_train_align[i] + se_train_align[i],\n            alpha=0.2,\n        )\n    axes[0].set_title(\"Training Alignment\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Alignment (1-JSD)\")\n    axes[0].legend()\n    for i, lr in enumerate(lrs):\n        axes[1].plot(epochs, mean_val_align[i], label=f\"lr={lr}\")\n        axes[1].fill_between(\n            epochs,\n            mean_val_align[i] - se_val_align[i],\n            mean_val_align[i] + se_val_align[i],\n            alpha=0.2,\n        )\n    axes[1].set_title(\"Validation Alignment\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Alignment (1-JSD)\")\n    axes[1].legend()\n    fig.suptitle(\n        \"Synthetic Dataset - Alignment Curves\\nLeft: Training Alignment, Right: Validation Alignment\"\n    )\n    plt.savefig(os.path.join(working_dir, \"synthetic_alignment_curves_mean_se.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating alignment curves plot: {e}\")\n    plt.close()\n"], "plot_plan": ["We first set up the working directory and load the saved experiment data. Then we extract epochs, training/validation losses, alignment metrics, and final predictions with ground truth. In separate try-except blocks, we plot the loss curves and alignment curves over epochs, each with clear titles and legends, and save them to the working directory. After plotting, we always close the figures to free resources and report any errors in plotting. Finally, we compute the overall accuracy on the synthetic validation set from the saved predictions and print it out. This provides a quick visual and quantitative summary of the model\u2019s performance.", "First, load the experiment data and extract the synthetic results under the learning rate sweep. Then use two separate try-except blocks to create and save figures: one for the training/validation loss curves and another for the training/validation alignment curves. Each figure will have two subplots\u2014left for training metrics and right for validation\u2014with clear titles, axis labels, legends for each learning rate, and a suptitle specifying \u201cSynthetic Dataset\u201d and the left/right designations. We iterate over the stored learning rates to plot each curve, save the figures to `working_dir` with descriptive filenames, and close them. If any error occurs in plotting or data loading, we print an error message and ensure the figure is closed.", "We begin by loading the saved experiment data, then extract the synthetic dataset\u2019s batch sizes, epochs, alignment metrics, losses, predictions, and ground truth.  We then create separate line plots for training and validation alignment over epochs, each in its own try-excet block, with a suptitle specifying \u201cSynthetic dataset\u201d and a subtitle naming the metric. Similarly, we plot training and validation loss curves. Next, we compute the final accuracy for each batch size by comparing predictions to ground truth, print these accuracies, and plot them as a bar chart. Each figure is saved under a descriptive filename in working/, and every figure is closed after saving. Any errors loading or creating plots are caught and printed to avoid interrupting the script.", "We will load the saved experiment_data.npy and extract the synthetic dataset\u2019s weight decay sweep results. Then we'll compute and print the final validation losses and alignment metrics for each weight decay value. Next, we will produce two line-plots: one showing training and validation loss curves across epochs, and another showing the alignment metric across epochs, using solid lines for training and dashed for validation. We'll identify the best-performing weight decay model by its highest final validation alignment and plot its predicted versus ground-truth labels on the validation set. Each figure will be drawn in a separate try-except block, saved under descriptive filenames including the dataset name and plot type, and closed afterward. Plot titles will specify the dataset and include clear subtitles (e.g., \u201cSolid: train, Dashed: val\u201d). Throughout, we only use data loaded from experiment_data.npy and basic matplotlib. Metrics are printed but no synthetic data is generated or assumed beyond what was saved.", null, "I will load the saved experiment data and extract dropout rates, losses, alignment metrics, predictions and ground truth for the synthetic dataset. I\u2019ll generate three figures: loss curves, alignment metric curves, and a bar chart of final validation accuracy per dropout rate. Each plotting block will be wrapped in a try\u2013except to ensure errors are caught and figures are always closed. The loss and alignment plots will overlay training (solid) and validation (dashed) lines for all dropout rates and include clear titles and subtitles. The accuracy bar chart will compute and display final accuracy for each dropout. All figures will be saved to `working_dir` with descriptive filenames. Finally, the code will print out the final validation accuracies for each dropout rate.", "We will load the saved experiment data from the working directory and extract label\u2010smoothing alphas, epochs, losses, and alignment metrics. Then we wrap each plotting routine in a try\u2010except block to ensure errors are caught and figures are closed. First, we plot training and validation loss curves across epochs for each alpha, saving as \u201csynthetic_loss_curves.png.\u201d Next, we plot training and validation alignment (1\u2013JSD) curves, saving as \u201csynthetic_alignment_curves.png.\u201d We then create a bar chart of final validation alignment versus alpha, saving as \u201csynthetic_final_val_alignment.png.\u201d Each figure includes a clear title specifying the synthetic dataset and subtitle-like parenthetical explanation. Finally, we print the final validation alignment values for each alpha. All plots are saved into the working directory with descriptive names and closed after saving.", null, null, "We start by loading the saved experiment data and isolating the synthetic dataset results under different learning\u2010rate scheduler gammas. Next, we compute the final classification accuracy for each gamma by comparing saved predictions against ground truth labels. We then prepare two combined line plots\u2014one for train vs. validation loss and another for train vs. validation alignment metrics\u2014where each curve corresponds to a different gamma. Each plotting block is wrapped in a try\u2010except to ensure that errors in one plot do not block subsequent ones, and all figures are closed after saving. Titles clearly indicate the dataset (\u201cSynthetic Dataset\u201d) and include informative subtitles showing \u201cTrain vs Validation Loss\u201d or \u201cTrain vs Validation Alignment.\u201d We save figures into the working directory with descriptive file names. Finally, we print out the computed accuracy metrics for each gamma to summarize model performance. All of these steps use only the existing data from experiment_data.npy without generating any synthetic or simulated values.", null, "We\u2019ll load the saved experiment data, extract the available initialization standards, and produce four core visualizations. First, we plot combined training and validation loss curves over epochs for all init_stds on a single figure. Next, we do the same for the alignment metric (1\u2212JSD). Then, we summarize the final validation losses across init_stds as a bar chart, followed by a bar chart for final validation alignment. Each plot is wrapped in its own try-except to handle errors gracefully, uses clear titles/subtitles specifying \u201cSynthetic Data,\u201d and saves figures in the working directory with descriptive filenames. We close each figure after saving to free resources.", null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "learning_rate", "batch_size", "weight_decay", "hidden_dim", "dropout_rate", "label_smoothing", "adam_beta1", "adam_beta2", "lr_scheduler_gamma", "max_grad_norm", "init_std", "JSD_ALIGNMENT_WEIGHT", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The following script constructs the path to the working directory, loads the\n\u201cexperiment_data.npy\u201d file containing recorded experiment metrics, and iterates\nover each dataset to extract and print the final training and validation\nalignment and loss values. For each dataset, it clearly labels and outputs the\ndataset name, final epoch, train alignment, validation alignment, train loss,\nand validation loss. The code runs immediately at the global scope without\nrequiring any special entry point or plots.", "Here\u2019s a script that loads the saved experiment data, iterates over each dataset\nand learning rate, extracts the final training/validation alignments and losses,\ncomputes test accuracy from the stored predictions, and prints each metric with\nclear labels at the global scope:", "The script locates the working directory and loads the saved NumPy dictionary.\nIt navigates through the outer hyperparameter sweep (\u201cbatch_size\u201d) and then each\ndataset (\u201csynthetic\u201d), pulling out the final epoch metrics for alignment and\nloss. For each batch\u2010size run, it prints the batch size followed by the final\ntraining alignment, validation alignment, training loss, and validation loss\nwith clear metric labels. All code executes at global scope, without any\nentry\u2010point guard.", "The script below loads the saved experiment data from the specified working\ndirectory, iterates over each hyperparameter sweep and dataset, and prints the\ndataset name followed by the final epoch values of training alignment,\nvalidation alignment, training loss, and validation loss for each weight\u2010decay\nsetting. All metric names are fully spelled out for clarity.", "The following script loads the experiment data from the working directory,\niterates over each dataset under `hidden_dim_tuning`, and prints the final epoch\nvalues of train alignment, validation alignment, train loss, and validation loss\nfor each hidden dimension. It labels all metrics clearly and runs immediately\nwhen executed without any `if __name__ == \"__main__\":` guard.", "Below is a script that loads the saved experiment data, extracts the synthetic\ndataset\u2019s per\u2010dropout metrics, and prints the final training/validation\nalignment and loss for each dropout rate. It clearly labels the dataset and each\nmetric and requires no special entry point.", "I will load the saved NumPy file from the `working` directory, locate the\n\u201csynthetic\u201d dataset within the `label_smoothing` experiment, and then for each\nsmoothing value (`alpha`) extract the final epoch\u2019s training alignment,\nvalidation alignment, training loss, and validation loss. These values will be\nprinted with explicit labels, and the dataset name will precede all metrics. The\nscript runs immediately upon execution and contains no `if __name__ ==\n\"__main__\":` guard.", "The script below loads the saved experimental data from the \u201cworking\u201d directory,\nextracts the per\u2010epoch alignment and loss for both training and validation on\nthe synthetic dataset, then iterates over each \u03b2\u2081 value to print out the final\nepoch\u2019s training alignment, validation alignment, training loss, and validation\nloss with clear labels.", "", "The script first constructs the path to the working directory and loads the\nsaved numpy file into a dictionary. It then iterates over the\n`lr_scheduler_gamma` namespace to handle each dataset (e.g., `\"synthetic\"`),\nprinting the dataset name. Within each dataset, it loops over each\nhyperparameter setting (`gamma_x`), extracts the final values of both the\nalignment metrics and the losses for training and validation, and prints them\nwith clear labels. All code resides at the global scope so that it runs\nimmediately when the script is executed.", "I will load the saved NumPy dictionary, iterate over each gradient\u2010clipping\nthreshold and its contained datasets (e.g. the \"synthetic\" set), and extract the\nfinal epoch's training and validation alignment and loss.  The script prints the\ndataset name followed by clearly labeled metrics for training alignment,\nvalidation alignment, training loss, and validation loss.  All code runs at the\nglobal scope without any `if __name__ == \"__main__\":` guard.", "This script constructs the path to the \u2018working\u2019 directory and loads the saved\nnumpy file containing our `experiment_data` dictionary. It then iterates through\neach initialization standard deviation, accessing the final alignment metrics\nand losses recorded for both the training and validation sets. For each\ninitialization, it prints the name of the dataset (\u2018Training\u2019 or \u2018Validation\u2019)\nbefore listing the final alignment and loss values with clear metric labels. We\navoid any plotting or special entry points so the code runs immediately at the\nglobal scope. The output provides a concise summary of how initialization scale\nimpacts alignment and loss performance.", "Below is a script that loads the saved experiment data from the `working`\ndirectory and iterates through each dataset in the loaded structure. For every\ndataset and \u03bb setting, it prints the dataset name once, then shows the final\ntraining and validation alignment as well as final training and validation loss\nfor that \u03bb. Each metric is clearly labeled (e.g., \"Training loss\", \"Validation\nalignment\") and only the final epoch values are displayed. The code is executed\nat the global scope without an entry\u2010point guard.", "Here\u2019s a script that loads the saved experiment data, iterates over each dataset\nand learning rate, extracts the final training/validation alignments and losses,\ncomputes test accuracy from the stored predictions, and prints each metric with\nclear labels at the global scope:", "Here\u2019s a script that loads the saved experiment data, iterates over each dataset\nand learning rate, extracts the final training/validation alignments and losses,\ncomputes test accuracy from the stored predictions, and prints each metric with\nclear labels at the global scope:", "Here\u2019s a script that loads the saved experiment data, iterates over each dataset\nand learning rate, extracts the final training/validation alignments and losses,\ncomputes test accuracy from the stored predictions, and prints each metric with\nclear labels at the global scope:", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Define the working directory and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over each dataset and print the final metrics\nfor dataset_name, dataset_dict in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Print final epoch\n    epochs = dataset_dict.get(\"epochs\", [])\n    if epochs:\n        final_epoch = epochs[-1]\n        print(f\"Final epoch: {final_epoch}\")\n\n    # Extract and print the final train and validation alignment\n    metrics = dataset_dict.get(\"metrics\", {})\n    train_align_list = metrics.get(\"train\", [])\n    val_align_list = metrics.get(\"val\", [])\n    if train_align_list:\n        print(f\"Train alignment: {train_align_list[-1]:.4f}\")\n    if val_align_list:\n        print(f\"Validation alignment: {val_align_list[-1]:.4f}\")\n\n    # Extract and print the final train and validation loss\n    losses = dataset_dict.get(\"losses\", {})\n    train_loss_list = losses.get(\"train\", [])\n    val_loss_list = losses.get(\"val\", [])\n    if train_loss_list:\n        print(f\"Train loss: {train_loss_list[-1]:.4f}\")\n    if val_loss_list:\n        print(f\"Validation loss: {val_loss_list[-1]:.4f}\")\n\n    print()  # Blank line for readability between datasets\n", "import os\nimport numpy as np\n\n# Define working directory and load the data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset under the learning_rate key\nfor dataset_name, ds in experiment_data[\"learning_rate\"].items():\n    print(f\"{dataset_name.capitalize()} dataset:\")\n    lrs = ds[\"lrs\"]\n    # Loop over all learning rates\n    for idx, lr in enumerate(lrs):\n        train_aligns = ds[\"metrics\"][\"train\"][idx]\n        val_aligns = ds[\"metrics\"][\"val\"][idx]\n        train_losses = ds[\"losses\"][\"train\"][idx]\n        val_losses = ds[\"losses\"][\"val\"][idx]\n        preds = ds[\"predictions\"][idx]\n        gts = ds[\"ground_truth\"][idx]\n        # Extract final epoch values\n        final_train_alignment = train_aligns[-1]\n        final_val_alignment = val_aligns[-1]\n        final_train_loss = train_losses[-1]\n        final_val_loss = val_losses[-1]\n        # Compute test accuracy\n        test_accuracy = np.mean(preds == gts)\n        # Print metrics with clear labels\n        print(f\"  Learning rate {lr:.1e}:\")\n        print(f\"    Final training alignment: {final_train_alignment:.4f}\")\n        print(f\"    Final validation alignment: {final_val_alignment:.4f}\")\n        print(f\"    Final training loss: {final_train_loss:.4f}\")\n        print(f\"    Final validation loss: {final_val_loss:.4f}\")\n        print(f\"    Test accuracy: {test_accuracy:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Locate and load the experimental results\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through hyperparameter settings and datasets\nfor hyperparam, datasets in experiment_data.items():\n    for dataset_name, info in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        batch_sizes = info[\"batch_sizes\"]\n        train_metrics = info[\"metrics\"][\"train\"]\n        val_metrics = info[\"metrics\"][\"val\"]\n        train_losses = info[\"losses\"][\"train\"]\n        val_losses = info[\"losses\"][\"val\"]\n\n        # Print final metric values for each batch size\n        for idx, bs in enumerate(batch_sizes):\n            final_train_alignment = train_metrics[idx][-1]\n            final_val_alignment = val_metrics[idx][-1]\n            final_train_loss = train_losses[idx][-1]\n            final_val_loss = val_losses[idx][-1]\n\n            print(f\"Batch size = {bs}\")\n            print(f\"train alignment: {final_train_alignment:.4f}\")\n            print(f\"validation alignment: {final_val_alignment:.4f}\")\n            print(f\"train loss: {final_train_loss:.4f}\")\n            print(f\"validation loss: {final_val_loss:.4f}\")\n            print()\n", "import os\nimport numpy as np\n\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each hyperparameter category and dataset\nfor hyperparam, hp_data in experiment_data.items():\n    for dataset_name, data in hp_data.items():\n        print(f\"Dataset: {dataset_name}\")\n        # Extract arrays of weight\u2010decay values and metrics\n        weight_decays = data[\"weight_decay_values\"]\n        train_align = data[\"metrics\"][\"train\"]\n        val_align = data[\"metrics\"][\"val\"]\n        train_loss = data[\"losses\"][\"train\"]\n        val_loss = data[\"losses\"][\"val\"]\n\n        # Print the final epoch values for each weight\u2010decay setting\n        for idx, wd in enumerate(weight_decays):\n            print(f\"Hyperparameter {hyperparam} = {wd}\")\n            print(f\"final training alignment: {train_align[idx, -1]:.4f}\")\n            print(f\"final validation alignment: {val_align[idx, -1]:.4f}\")\n            print(f\"final training loss: {train_loss[idx, -1]:.4f}\")\n            print(f\"final validation loss: {val_loss[idx, -1]:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through tuning configurations (e.g., hidden_dim_tuning)\nfor tuning_name, tuning_section in experiment_data.items():\n    # For each dataset under this tuning (e.g., 'synthetic')\n    for dataset_name, dataset_info in tuning_section.items():\n        print(f\"Dataset: {dataset_name}\")\n        hidden_dims = dataset_info[\"hidden_dims\"]\n        metrics = dataset_info[\"metrics\"]\n        losses = dataset_info[\"losses\"]\n        # Print final epoch metrics for each hidden dimension\n        for idx, hd in enumerate(hidden_dims):\n            final_train_alignment = metrics[\"train\"][idx][-1]\n            final_val_alignment = metrics[\"val\"][idx][-1]\n            final_train_loss = losses[\"train\"][idx][-1]\n            final_val_loss = losses[\"val\"][idx][-1]\n            print(f\"  Hidden dimension: {hd}\")\n            print(f\"    Train alignment: {final_train_alignment:.4f}\")\n            print(f\"    Validation alignment: {final_val_alignment:.4f}\")\n            print(f\"    Train loss: {final_train_loss:.4f}\")\n            print(f\"    Validation loss: {final_val_loss:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# extract synthetic results under dropout_rate\nexp = experiment_data[\"dropout_rate\"][\"synthetic\"]\ndropout_rates = exp[\"hyperparams\"]\ntrain_aligns = exp[\"metrics\"][\"train\"]\nval_aligns = exp[\"metrics\"][\"val\"]\ntrain_losses = exp[\"losses\"][\"train\"]\nval_losses = exp[\"losses\"][\"val\"]\n\n# print results\nprint(\"Dataset: synthetic\")\nfor dr, tr_align, vl_align, tr_loss, vl_loss in zip(\n    dropout_rates, train_aligns, val_aligns, train_losses, val_losses\n):\n    print(f\"\\nDropout rate: {dr}\")\n    print(f\"Final training alignment: {tr_align[-1]:.4f}\")\n    print(f\"Final validation alignment: {vl_align[-1]:.4f}\")\n    print(f\"Final training loss: {tr_loss[-1]:.4f}\")\n    print(f\"Final validation loss: {vl_loss[-1]:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through experiments and datasets\nfor exp_name, datasets in experiment_data.items():\n    for dataset_name, dataset_content in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        alphas = dataset_content[\"alphas\"]\n        train_aligns = dataset_content[\"metrics\"][\"train\"]\n        val_aligns = dataset_content[\"metrics\"][\"val\"]\n        train_losses = dataset_content[\"losses\"][\"train\"]\n        val_losses = dataset_content[\"losses\"][\"val\"]\n\n        # Print final epoch metrics for each alpha\n        for idx, alpha in enumerate(alphas):\n            final_train_alignment = train_aligns[idx][-1]\n            final_val_alignment = val_aligns[idx][-1]\n            final_train_loss = train_losses[idx][-1]\n            final_val_loss = val_losses[idx][-1]\n\n            print(f\"  Label smoothing alpha = {alpha}\")\n            print(f\"    Train alignment: {final_train_alignment:.4f}\")\n            print(f\"    Validation alignment: {final_val_alignment:.4f}\")\n            print(f\"    Train loss: {final_train_loss:.4f}\")\n            print(f\"    Validation loss: {final_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Extract beta1 grid and synthetic dataset results\nbeta1_values = experiment_data[\"adam_beta1\"][\"beta1_values\"]\nsynthetic_data = experiment_data[\"adam_beta1\"][\"synthetic\"]\ntrain_aligns = synthetic_data[\"metrics\"][\"train\"]\nval_aligns = synthetic_data[\"metrics\"][\"val\"]\ntrain_losses = synthetic_data[\"losses\"][\"train\"]\nval_losses = synthetic_data[\"losses\"][\"val\"]\n\n# Print final values for each beta1 run\nfor b1, ta, va, tl, vl in zip(\n    beta1_values, train_aligns, val_aligns, train_losses, val_losses\n):\n    print(f\"Dataset: synthetic (beta1={b1:.2f})\")\n    print(f\"Training alignment: {ta[-1]:.4f}\")\n    print(f\"Validation alignment: {va[-1]:.4f}\")\n    print(f\"Training loss: {tl[-1]:.4f}\")\n    print(f\"Validation loss: {vl[-1]:.4f}\")\n    print()\n", "", "import os\nimport numpy as np\n\n# Define and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Parse and print final metrics for each dataset and gamma setting\nfor dataset_name, gamma_dict in experiment_data.get(\"lr_scheduler_gamma\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    for gamma_key, exp in gamma_dict.items():\n        # Extract final metric and loss values\n        final_train_alignment = exp[\"metrics\"][\"train\"][-1]\n        final_validation_alignment = exp[\"metrics\"][\"val\"][-1]\n        final_train_loss = exp[\"losses\"][\"train\"][-1]\n        final_validation_loss = exp[\"losses\"][\"val\"][-1]\n\n        print(f\"  Hyperparameter setting {gamma_key}:\")\n        print(f\"    train alignment: {final_train_alignment}\")\n        print(f\"    validation alignment: {final_validation_alignment}\")\n        print(f\"    train loss: {final_train_loss}\")\n        print(f\"    validation loss: {final_validation_loss}\")\n", "import os\nimport numpy as np\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\nfor max_norm, experiments in experiment_data.get(\"max_grad_norm\", {}).items():\n    for dataset_name, dataset_info in experiments.items():\n        metrics = dataset_info.get(\"metrics\", {})\n        losses = dataset_info.get(\"losses\", {})\n        # Final epoch metrics\n        final_train_alignment = metrics.get(\"train\", [])[-1]\n        final_validation_alignment = metrics.get(\"val\", [])[-1]\n        final_train_loss = losses.get(\"train\", [])[-1]\n        final_validation_loss = losses.get(\"val\", [])[-1]\n\n        print(f\"Dataset: {dataset_name} (max_grad_norm={max_norm})\")\n        print(f\"Final training alignment: {final_train_alignment}\")\n        print(f\"Final validation alignment: {final_validation_alignment}\")\n        print(f\"Final training loss: {final_train_loss}\")\n        print(f\"Final validation loss: {final_validation_loss}\\n\")\n", "import os\nimport numpy as np\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\nfor std_key in sorted(experiment_data[\"init_std\"], key=float):\n    exp = experiment_data[\"init_std\"][std_key]\n    final_train_alignment = exp[\"metrics\"][\"train\"][-1]\n    final_val_alignment = exp[\"metrics\"][\"val\"][-1]\n    final_train_loss = exp[\"losses\"][\"train\"][-1]\n    final_val_loss = exp[\"losses\"][\"val\"][-1]\n\n    print(f\"Initialization standard deviation: {std_key}\")\n    print(\"Dataset: Training\")\n    print(f\"training alignment: {final_train_alignment:.4f}\")\n    print(f\"training loss: {final_train_loss:.4f}\")\n    print(\"Dataset: Validation\")\n    print(f\"validation alignment: {final_val_alignment:.4f}\")\n    print(f\"validation loss: {final_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\nfor method_name, datasets in experiment_data.items():\n    for dataset_name, ds in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        lambdas = ds[\"lambdas\"]\n        train_aligns = ds[\"metrics\"][\"train\"]\n        val_aligns = ds[\"metrics\"][\"val\"]\n        train_losses = ds[\"losses\"][\"train\"]\n        val_losses = ds[\"losses\"][\"val\"]\n        for lam, t_align_hist, v_align_hist, t_loss_hist, v_loss_hist in zip(\n            lambdas, train_aligns, val_aligns, train_losses, val_losses\n        ):\n            print(f\"Lambda: {lam}\")\n            print(f\"Training alignment: {t_align_hist[-1]:.4f}\")\n            print(f\"Validation alignment: {v_align_hist[-1]:.4f}\")\n            print(f\"Training loss: {t_loss_hist[-1]:.4f}\")\n            print(f\"Validation loss: {v_loss_hist[-1]:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Define working directory and load the data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset under the learning_rate key\nfor dataset_name, ds in experiment_data[\"learning_rate\"].items():\n    print(f\"{dataset_name.capitalize()} dataset:\")\n    lrs = ds[\"lrs\"]\n    # Loop over all learning rates\n    for idx, lr in enumerate(lrs):\n        train_aligns = ds[\"metrics\"][\"train\"][idx]\n        val_aligns = ds[\"metrics\"][\"val\"][idx]\n        train_losses = ds[\"losses\"][\"train\"][idx]\n        val_losses = ds[\"losses\"][\"val\"][idx]\n        preds = ds[\"predictions\"][idx]\n        gts = ds[\"ground_truth\"][idx]\n        # Extract final epoch values\n        final_train_alignment = train_aligns[-1]\n        final_val_alignment = val_aligns[-1]\n        final_train_loss = train_losses[-1]\n        final_val_loss = val_losses[-1]\n        # Compute test accuracy\n        test_accuracy = np.mean(preds == gts)\n        # Print metrics with clear labels\n        print(f\"  Learning rate {lr:.1e}:\")\n        print(f\"    Final training alignment: {final_train_alignment:.4f}\")\n        print(f\"    Final validation alignment: {final_val_alignment:.4f}\")\n        print(f\"    Final training loss: {final_train_loss:.4f}\")\n        print(f\"    Final validation loss: {final_val_loss:.4f}\")\n        print(f\"    Test accuracy: {test_accuracy:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Define working directory and load the data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset under the learning_rate key\nfor dataset_name, ds in experiment_data[\"learning_rate\"].items():\n    print(f\"{dataset_name.capitalize()} dataset:\")\n    lrs = ds[\"lrs\"]\n    # Loop over all learning rates\n    for idx, lr in enumerate(lrs):\n        train_aligns = ds[\"metrics\"][\"train\"][idx]\n        val_aligns = ds[\"metrics\"][\"val\"][idx]\n        train_losses = ds[\"losses\"][\"train\"][idx]\n        val_losses = ds[\"losses\"][\"val\"][idx]\n        preds = ds[\"predictions\"][idx]\n        gts = ds[\"ground_truth\"][idx]\n        # Extract final epoch values\n        final_train_alignment = train_aligns[-1]\n        final_val_alignment = val_aligns[-1]\n        final_train_loss = train_losses[-1]\n        final_val_loss = val_losses[-1]\n        # Compute test accuracy\n        test_accuracy = np.mean(preds == gts)\n        # Print metrics with clear labels\n        print(f\"  Learning rate {lr:.1e}:\")\n        print(f\"    Final training alignment: {final_train_alignment:.4f}\")\n        print(f\"    Final validation alignment: {final_val_alignment:.4f}\")\n        print(f\"    Final training loss: {final_train_loss:.4f}\")\n        print(f\"    Final validation loss: {final_val_loss:.4f}\")\n        print(f\"    Test accuracy: {test_accuracy:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Define working directory and load the data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset under the learning_rate key\nfor dataset_name, ds in experiment_data[\"learning_rate\"].items():\n    print(f\"{dataset_name.capitalize()} dataset:\")\n    lrs = ds[\"lrs\"]\n    # Loop over all learning rates\n    for idx, lr in enumerate(lrs):\n        train_aligns = ds[\"metrics\"][\"train\"][idx]\n        val_aligns = ds[\"metrics\"][\"val\"][idx]\n        train_losses = ds[\"losses\"][\"train\"][idx]\n        val_losses = ds[\"losses\"][\"val\"][idx]\n        preds = ds[\"predictions\"][idx]\n        gts = ds[\"ground_truth\"][idx]\n        # Extract final epoch values\n        final_train_alignment = train_aligns[-1]\n        final_val_alignment = val_aligns[-1]\n        final_train_loss = train_losses[-1]\n        final_val_loss = val_losses[-1]\n        # Compute test accuracy\n        test_accuracy = np.mean(preds == gts)\n        # Print metrics with clear labels\n        print(f\"  Learning rate {lr:.1e}:\")\n        print(f\"    Final training alignment: {final_train_alignment:.4f}\")\n        print(f\"    Final validation alignment: {final_val_alignment:.4f}\")\n        print(f\"    Final training loss: {final_train_loss:.4f}\")\n        print(f\"    Final validation loss: {final_val_loss:.4f}\")\n        print(f\"    Test accuracy: {test_accuracy:.4f}\")\n    print()\n", ""], "parse_term_out": ["['Dataset: synthetic', '\\n', 'Final epoch: 10', '\\n', 'Train alignment: 0.9917',\n'\\n', 'Validation alignment: 0.9907', '\\n', 'Train loss: 0.5274', '\\n',\n'Validation loss: 0.5101', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "['Synthetic dataset:', '\\n', '  Learning rate 1.0e-04:', '\\n', '    Final\ntraining alignment: 0.9929', '\\n', '    Final validation alignment: 0.9929',\n'\\n', '    Final training loss: 1.0162', '\\n', '    Final validation loss:\n1.0106', '\\n', '    Test accuracy: 0.6000', '\\n', '  Learning rate 5.0e-04:',\n'\\n', '    Final training alignment: 0.9921', '\\n', '    Final validation\nalignment: 0.9915', '\\n', '    Final training loss: 0.7943', '\\n', '    Final\nvalidation loss: 0.7817', '\\n', '    Test accuracy: 0.8350', '\\n', '  Learning\nrate 1.0e-03:', '\\n', '    Final training alignment: 0.9917', '\\n', '    Final\nvalidation alignment: 0.9907', '\\n', '    Final training loss: 0.5274', '\\n', '\nFinal validation loss: 0.5101', '\\n', '    Test accuracy: 0.9050', '\\n', '\nLearning rate 5.0e-03:', '\\n', '    Final training alignment: 0.9988', '\\n', '\nFinal validation alignment: 0.9989', '\\n', '    Final training loss: 0.0982',\n'\\n', '    Final validation loss: 0.1039', '\\n', '    Test accuracy: 0.9600',\n'\\n', '  Learning rate 1.0e-02:', '\\n', '    Final training alignment: 0.9991',\n'\\n', '    Final validation alignment: 0.9986', '\\n', '    Final training loss:\n0.0588', '\\n', '    Final validation loss: 0.0739', '\\n', '    Test accuracy:\n0.9650', '\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Dataset: synthetic', '\\n', 'Batch size = 16', '\\n', 'train alignment: 0.9980',\n'\\n', 'validation alignment: 0.9980', '\\n', 'train loss: 0.1756', '\\n',\n'validation loss: 0.1767', '\\n', '\\n', 'Batch size = 32', '\\n', 'train\nalignment: 0.9959', '\\n', 'validation alignment: 0.9955', '\\n', 'train loss:\n0.2951', '\\n', 'validation loss: 0.2876', '\\n', '\\n', 'Batch size = 128', '\\n',\n'train alignment: 0.9920', '\\n', 'validation alignment: 0.9913', '\\n', 'train\nloss: 0.7706', '\\n', 'validation loss: 0.7577', '\\n', '\\n', 'Batch size = 256',\n'\\n', 'train alignment: 0.9933', '\\n', 'validation alignment: 0.9930', '\\n',\n'train loss: 0.9120', '\\n', 'validation loss: 0.9026', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Hyperparameter weight_decay = 1e-05', '\\n', 'final\ntraining alignment: 0.9917', '\\n', 'final validation alignment: 0.9907', '\\n',\n'final training loss: 0.5274', '\\n', 'final validation loss: 0.5101', '\\n',\n'Hyperparameter weight_decay = 0.0001', '\\n', 'final training alignment:\n0.9915', '\\n', 'final validation alignment: 0.9908', '\\n', 'final training loss:\n0.6368', '\\n', 'final validation loss: 0.6219', '\\n', 'Hyperparameter\nweight_decay = 0.001', '\\n', 'final training alignment: 0.9942', '\\n', 'final\nvalidation alignment: 0.9946', '\\n', 'final training loss: 0.5715', '\\n', 'final\nvalidation loss: 0.5505', '\\n', 'Hyperparameter weight_decay = 0.01', '\\n',\n'final training alignment: 0.9927', '\\n', 'final validation alignment: 0.9926',\n'\\n', 'final training loss: 0.5502', '\\n', 'final validation loss: 0.5316',\n'\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', '  Hidden dimension: 16', '\\n', '    Train\nalignment: 0.9905', '\\n', '    Validation alignment: 0.9908', '\\n', '    Train\nloss: 0.7504', '\\n', '    Validation loss: 0.7131', '\\n', '  Hidden dimension:\n32', '\\n', '    Train alignment: 0.9889', '\\n', '    Validation alignment:\n0.9886', '\\n', '    Train loss: 0.5591', '\\n', '    Validation loss: 0.5510',\n'\\n', '  Hidden dimension: 64', '\\n', '    Train alignment: 0.9938', '\\n', '\nValidation alignment: 0.9928', '\\n', '    Train loss: 0.4079', '\\n', '\nValidation loss: 0.4033', '\\n', '  Hidden dimension: 128', '\\n', '    Train\nalignment: 0.9977', '\\n', '    Validation alignment: 0.9973', '\\n', '    Train\nloss: 0.2821', '\\n', '    Validation loss: 0.2784', '\\n', '\\n', 'Execution time:\na moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', '\\nDropout rate: 0.0', '\\n', 'Final training\nalignment: 0.9917', '\\n', 'Final validation alignment: 0.9907', '\\n', 'Final\ntraining loss: 0.5274', '\\n', 'Final validation loss: 0.5101', '\\n', '\\nDropout\nrate: 0.1', '\\n', 'Final training alignment: 0.9885', '\\n', 'Final validation\nalignment: 0.9910', '\\n', 'Final training loss: 0.6557', '\\n', 'Final validation\nloss: 0.6350', '\\n', '\\nDropout rate: 0.2', '\\n', 'Final training alignment:\n0.9876', '\\n', 'Final validation alignment: 0.9946', '\\n', 'Final training loss:\n0.6184', '\\n', 'Final validation loss: 0.5791', '\\n', '\\nDropout rate: 0.5',\n'\\n', 'Final training alignment: 0.9694', '\\n', 'Final validation alignment:\n0.9923', '\\n', 'Final training loss: 0.6877', '\\n', 'Final validation loss:\n0.6109', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', '  Label smoothing alpha = 0.0', '\\n', '    Train\nalignment: 0.9917', '\\n', '    Validation alignment: 0.9907', '\\n', '    Train\nloss: 0.5274', '\\n', '    Validation loss: 0.5101', '\\n', '  Label smoothing\nalpha = 0.05', '\\n', '    Train alignment: 0.9922', '\\n', '    Validation\nalignment: 0.9913', '\\n', '    Train loss: 0.5778', '\\n', '    Validation loss:\n0.5638', '\\n', '  Label smoothing alpha = 0.1', '\\n', '    Train alignment:\n0.9927', '\\n', '    Validation alignment: 0.9919', '\\n', '    Train loss:\n0.6269', '\\n', '    Validation loss: 0.6159', '\\n', '  Label smoothing alpha =\n0.2', '\\n', '    Train alignment: 0.9938', '\\n', '    Validation alignment:\n0.9931', '\\n', '    Train loss: 0.7205', '\\n', '    Validation loss: 0.7146',\n'\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 10, in\n<module>\\n    beta1_values = experiment_data[\"adam_beta1\"][\"beta1_values\"]\\n\n~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\\nKeyError: \\'adam_beta1\\'\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "", "['Dataset: synthetic', '\\n', '  Hyperparameter setting gamma_0.1:', '\\n', '\ntrain alignment: 0.9918032341003418', '\\n', '    validation alignment:\n0.9912157917022705', '\\n', '    train loss: 0.7558426809310913', '\\n', '\nvalidation loss: 0.7577569818496704', '\\n', '  Hyperparameter setting\ngamma_0.5:', '\\n', '    train alignment: 0.991660945892334', '\\n', '\nvalidation alignment: 0.9911333847045899', '\\n', '    train loss:\n0.7635355634689331', '\\n', '    validation loss: 0.7612556004524231', '\\n', '\nHyperparameter setting gamma_0.9:', '\\n', '    train alignment:\n0.9939481010437011', '\\n', '    validation alignment: 0.9943271970748901', '\\n',\n'    train loss: 0.597569676399231', '\\n', '    validation loss:\n0.5787173199653626', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Dataset: synthetic (max_grad_norm=0.5)', '\\n', 'Final training alignment:\n0.991731689453125', '\\n', 'Final validation alignment: 0.9907101154327392',\n'\\n', 'Final training loss: 0.5233471393585205', '\\n', 'Final validation loss:\n0.5061547112464905\\n', '\\n', 'Dataset: synthetic (max_grad_norm=1.0)', '\\n',\n'Final training alignment: 0.9916952400207519', '\\n', 'Final validation\nalignment: 0.9906968212127686', '\\n', 'Final training loss: 0.5274013090133667',\n'\\n', 'Final validation loss: 0.51006591796875\\n', '\\n', 'Dataset: synthetic\n(max_grad_norm=5.0)', '\\n', 'Final training alignment: 0.9916952400207519',\n'\\n', 'Final validation alignment: 0.9906968212127686', '\\n', 'Final training\nloss: 0.5274013090133667', '\\n', 'Final validation loss: 0.51006591796875\\n',\n'\\n', 'Dataset: synthetic (max_grad_norm=10.0)', '\\n', 'Final training\nalignment: 0.9916952400207519', '\\n', 'Final validation alignment:\n0.9906968212127686', '\\n', 'Final training loss: 0.5274013090133667', '\\n',\n'Final validation loss: 0.51006591796875\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Initialization standard deviation: 0.01', '\\n', 'Dataset: Training', '\\n',\n'training alignment: 0.9994', '\\n', 'training loss: 0.4384', '\\n', 'Dataset:\nValidation', '\\n', 'validation alignment: 0.9995', '\\n', 'validation loss:\n0.4101', '\\n', 'Initialization standard deviation: 0.1', '\\n', 'Dataset:\nTraining', '\\n', 'training alignment: 0.9952', '\\n', 'training loss: 0.5228',\n'\\n', 'Dataset: Validation', '\\n', 'validation alignment: 0.9951', '\\n',\n'validation loss: 0.4864', '\\n', 'Initialization standard deviation: 0.5', '\\n',\n'Dataset: Training', '\\n', 'training alignment: 0.8848', '\\n', 'training loss:\n0.6865', '\\n', 'Dataset: Validation', '\\n', 'validation alignment: 0.8638',\n'\\n', 'validation loss: 0.8220', '\\n', 'Initialization standard deviation: 1.0',\n'\\n', 'Dataset: Training', '\\n', 'training alignment: 0.6308', '\\n', 'training\nloss: 5.4055', '\\n', 'Dataset: Validation', '\\n', 'validation alignment:\n0.6712', '\\n', 'validation loss: 5.6873', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Lambda: 0.0', '\\n', 'Training alignment: 0.9917',\n'\\n', 'Validation alignment: 0.9907', '\\n', 'Training loss: 0.5274', '\\n',\n'Validation loss: 0.5101', '\\n', 'Lambda: 0.1', '\\n', 'Training alignment:\n0.9920', '\\n', 'Validation alignment: 0.9910', '\\n', 'Training loss: 0.5276',\n'\\n', 'Validation loss: 0.5103', '\\n', 'Lambda: 0.5', '\\n', 'Training alignment:\n0.9930', '\\n', 'Validation alignment: 0.9922', '\\n', 'Training loss: 0.5286',\n'\\n', 'Validation loss: 0.5113', '\\n', 'Lambda: 1.0', '\\n', 'Training alignment:\n0.9940', '\\n', 'Validation alignment: 0.9933', '\\n', 'Training loss: 0.5300',\n'\\n', 'Validation loss: 0.5126', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Synthetic dataset:', '\\n', '  Learning rate 1.0e-04:', '\\n', '    Final\ntraining alignment: 0.9929', '\\n', '    Final validation alignment: 0.9929',\n'\\n', '    Final training loss: 1.0162', '\\n', '    Final validation loss:\n1.0106', '\\n', '    Test accuracy: 0.6000', '\\n', '  Learning rate 5.0e-04:',\n'\\n', '    Final training alignment: 0.9921', '\\n', '    Final validation\nalignment: 0.9915', '\\n', '    Final training loss: 0.7943', '\\n', '    Final\nvalidation loss: 0.7817', '\\n', '    Test accuracy: 0.8350', '\\n', '  Learning\nrate 1.0e-03:', '\\n', '    Final training alignment: 0.9917', '\\n', '    Final\nvalidation alignment: 0.9907', '\\n', '    Final training loss: 0.5274', '\\n', '\nFinal validation loss: 0.5101', '\\n', '    Test accuracy: 0.9050', '\\n', '\nLearning rate 5.0e-03:', '\\n', '    Final training alignment: 0.9988', '\\n', '\nFinal validation alignment: 0.9989', '\\n', '    Final training loss: 0.0982',\n'\\n', '    Final validation loss: 0.1039', '\\n', '    Test accuracy: 0.9600',\n'\\n', '  Learning rate 1.0e-02:', '\\n', '    Final training alignment: 0.9991',\n'\\n', '    Final validation alignment: 0.9986', '\\n', '    Final training loss:\n0.0588', '\\n', '    Final validation loss: 0.0739', '\\n', '    Test accuracy:\n0.9650', '\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Synthetic dataset:', '\\n', '  Learning rate 1.0e-04:', '\\n', '    Final\ntraining alignment: 0.9929', '\\n', '    Final validation alignment: 0.9929',\n'\\n', '    Final training loss: 1.0162', '\\n', '    Final validation loss:\n1.0106', '\\n', '    Test accuracy: 0.6000', '\\n', '  Learning rate 5.0e-04:',\n'\\n', '    Final training alignment: 0.9921', '\\n', '    Final validation\nalignment: 0.9915', '\\n', '    Final training loss: 0.7943', '\\n', '    Final\nvalidation loss: 0.7817', '\\n', '    Test accuracy: 0.8350', '\\n', '  Learning\nrate 1.0e-03:', '\\n', '    Final training alignment: 0.9917', '\\n', '    Final\nvalidation alignment: 0.9907', '\\n', '    Final training loss: 0.5274', '\\n', '\nFinal validation loss: 0.5101', '\\n', '    Test accuracy: 0.9050', '\\n', '\nLearning rate 5.0e-03:', '\\n', '    Final training alignment: 0.9988', '\\n', '\nFinal validation alignment: 0.9989', '\\n', '    Final training loss: 0.0982',\n'\\n', '    Final validation loss: 0.1039', '\\n', '    Test accuracy: 0.9600',\n'\\n', '  Learning rate 1.0e-02:', '\\n', '    Final training alignment: 0.9991',\n'\\n', '    Final validation alignment: 0.9986', '\\n', '    Final training loss:\n0.0588', '\\n', '    Final validation loss: 0.0739', '\\n', '    Test accuracy:\n0.9650', '\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Synthetic dataset:', '\\n', '  Learning rate 1.0e-04:', '\\n', '    Final\ntraining alignment: 0.9929', '\\n', '    Final validation alignment: 0.9929',\n'\\n', '    Final training loss: 1.0162', '\\n', '    Final validation loss:\n1.0106', '\\n', '    Test accuracy: 0.6000', '\\n', '  Learning rate 5.0e-04:',\n'\\n', '    Final training alignment: 0.9921', '\\n', '    Final validation\nalignment: 0.9915', '\\n', '    Final training loss: 0.7943', '\\n', '    Final\nvalidation loss: 0.7817', '\\n', '    Test accuracy: 0.8350', '\\n', '  Learning\nrate 1.0e-03:', '\\n', '    Final training alignment: 0.9917', '\\n', '    Final\nvalidation alignment: 0.9907', '\\n', '    Final training loss: 0.5274', '\\n', '\nFinal validation loss: 0.5101', '\\n', '    Test accuracy: 0.9050', '\\n', '\nLearning rate 5.0e-03:', '\\n', '    Final training alignment: 0.9988', '\\n', '\nFinal validation alignment: 0.9989', '\\n', '    Final training loss: 0.0982',\n'\\n', '    Final validation loss: 0.1039', '\\n', '    Test accuracy: 0.9600',\n'\\n', '  Learning rate 1.0e-02:', '\\n', '    Final training alignment: 0.9991',\n'\\n', '    Final validation alignment: 0.9986', '\\n', '    Final training loss:\n0.0588', '\\n', '    Final validation loss: 0.0739', '\\n', '    Test accuracy:\n0.9650', '\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, "KeyError", null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, {"args": ["adam_beta1"]}, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 10, "<module>", "beta1_values = experiment_data[\"adam_beta1\"][\"beta1_values\"]"]], null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>

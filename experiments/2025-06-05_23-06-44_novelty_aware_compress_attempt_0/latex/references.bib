%% LaTeX2e file `references.bib'
%% generated by the `filecontents' environment
%% from source `template' on 2025/06/06.
%%
@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT Press}
}
@article{rae2019compressivetf,
 author = {Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and T. Lillicrap},
 journal = {ArXiv},
 title = {Compressive Transformers for Long-Range Sequence Modelling},
 volume = {abs/1911.05507},
 year = {2019}
}
@article{dai2019transformerxlal,
 author = {Zihang Dai and Zhilin Yang and Yiming Yang and J. Carbonell and Quoc V. Le and R. Salakhutdinov},
 journal = {ArXiv},
 title = {Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
 volume = {abs/1901.02860},
 year = {2019}
}
@article{poli2023hyenaht,
 author = {Michael Poli and Stefano Massaroli and Eric Q. Nguyen and Daniel Y. Fu and Tri Dao and S. Baccus and Y. Bengio and Stefano Ermon and Christopher Ré},
 booktitle = {International Conference on Machine Learning},
 pages = {28043-28078},
 title = {Hyena Hierarchy: Towards Larger Convolutional Language Models},
 year = {2023}
}
@article{lewis2020retrievalaugmentedgf,
 author = {Patrick Lewis and Ethan Perez and Aleksandara Piktus and F. Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Kuttler and M. Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
 journal = {ArXiv},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 volume = {abs/2005.11401},
 year = {2020}
}
@article{sukhbaatar2019adaptiveas,
 author = {Sainbayar Sukhbaatar and Edouard Grave and Piotr Bojanowski and Armand Joulin},
 journal = {ArXiv},
 title = {Adaptive Attention Span in Transformers},
 volume = {abs/1905.07799},
 year = {2019}
}
@article{vaswani2017attentionia,
 author = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
 booktitle = {Neural Information Processing Systems},
 pages = {5998-6008},
 title = {Attention is All you Need},
 year = {2017}
}
@article{beltagy2020longformertl,
 author = {Iz Beltagy and Matthew E. Peters and Arman Cohan},
 journal = {ArXiv},
 title = {Longformer: The Long-Document Transformer},
 volume = {abs/2004.05150},
 year = {2020}
}
@article{sun2021doll,
 author = {Simeng Sun and Kalpesh Krishna and Andrew Mattarella-Micke and Mohit Iyyer},
 journal = {ArXiv},
 title = {Do Long-Range Language Models Actually Use Long-Range Context?},
 volume = {abs/2109.09115},
 year = {2021}
}
@inproceedings{shannon2021amt,
 author = {C. Shannon},
 pages = {121-134},
 title = {A Mathematical Theory of Communication (1948)},
 year = {2021}
}

{"edges": [[0, 15], [0, 14], [0, 1], [0, 11], [0, 2], [0, 17], [0, 16], [0, 7], [0, 5], [0, 12], [0, 10], [0, 4], [0, 3], [0, 6], [2, 8], [8, 9], [10, 13], [15, 18], [17, 20], [17, 19], [17, 21], [17, 22]], "layout": [[0.4357142857142857, 0.0], [0.0, 0.33333333333333337], [0.06666666666666667, 0.33333333333333337], [0.13333333333333333, 0.33333333333333337], [0.2, 0.33333333333333337], [0.26666666666666666, 0.33333333333333337], [0.3333333333333333, 0.33333333333333337], [0.4, 0.33333333333333337], [0.06666666666666667, 0.6666666666666667], [0.06666666666666667, 1.0], [0.4666666666666667, 0.33333333333333337], [0.5333333333333333, 0.33333333333333337], [0.6, 0.33333333333333337], [0.4666666666666667, 0.6666666666666667], [0.6666666666666666, 0.33333333333333337], [0.7333333333333333, 0.33333333333333337], [0.8, 0.33333333333333337], [0.9, 0.33333333333333337], [0.7333333333333333, 0.6666666666666667], [0.8, 0.6666666666666667], [0.8666666666666667, 0.6666666666666667], [0.9333333333333333, 0.6666666666666667], [1.0, 0.6666666666666667]], "plan": ["We can avoid the long download and mapping delays by using the datasets\nlibrary\u2019s streaming mode to fetch only a small subset of each dataset (e.g., 200\ntraining and 100 validation examples). We then manually encode these samples,\nbuild small TensorDatasets, and use DataLoader as before. This streaming\napproach prevents full dataset downloads and heavy mapping operations, ensuring\nthe script completes quickly while still testing on three HuggingFace\ncollections.", "Ablation name: Recency-Based Memory Retention Ablation. Here is the outline of\nthe solution: we define two transformer\u2010XL style models\u2014one using the original\nentropy\u2010based top\u2010k memory selection and one using the recency FIFO ablation. We\nloop over both ablations and over each dataset, train for two epochs, record\nper\u2010epoch train/validation losses and metrics, and on the final epoch record\nvalidation predictions and ground truth. All results are saved into a single\n`experiment_data.npy` file.", "Ablation name: Memory Gradient Flow Ablation. We introduce a `detach_mem` flag\nto the memory layer so it either detaches or retains gradients for old memory\nslots, and run two variants (\u201cbaseline\u201d with detach, and \u201cmemory_grad_flow\u201d\nwithout). We loop over both ablations and each dataset, training for two epochs\nand collecting losses, the two metrics, plus final-epoch validation predictions\nand ground truth. At the end we save `experiment_data` as a numpy file with the\nprescribed nested structure.", "Ablation name: Random Memory Retention Ablation. The solution defines a unified\nmemory\u2010transformer layer that can switch between entropy\u2010based and random\nretention, then runs experiments under both settings across several datasets. We\nrecord training and validation losses, memory ratios, entropy\u2010weighted\nefficiencies, plus model predictions and corresponding ground truth tokens for\neach validation sample. All results are stored in a nested dictionary keyed by\nablation type and dataset, and finally saved via numpy.", "Ablation name: Feedforward Sub-layer Removal Ablation. The code below extends\nthe original script to run both the full (\u201cbaseline\u201d) model and a\nfeedforward\u2010removed ablation (\u201cffn_removed\u201d) over the same datasets. We define\nan AblatedMemoryTransformerLayer that replaces the FFN with an identity mapping,\nthen loop over the two ablations and three datasets, training and validating\neach, while collecting losses, memory metrics, predictions, and ground truths.\nAll results are stored in a single `experiment_data` dict keyed first by\nablation type and then by dataset, and finally saved as `experiment_data.npy` in\nthe `working` directory.", "Ablation name: Layer Normalization Ablation. Here\u2019s the implementation: we\ndefine a new `MemoryTransformerLayerNoLN` without any LayerNorms, wrap it into\nan `AblatedTransformerXLModel`, and run the same train/val loops while recording\nlosses, both memory metrics, and per\u2010sample predictions & ground\u2010truths on the\nfinal epoch. All results are stored under\n`experiment_data['no_layernorm'][dataset]` and saved as `experiment_data.npy`.", "Ablation name: No Memory Attention Ablation. I propose to run both the original\nmemory\u2010augmented model and a no\u2010memory ablation in one script, nesting them\nunder an `ablation_types` loop. When `disable_memory` is True, we reset `mem_x,\nmem_ent` to None at each chunk so attention only sees the current context and we\nnever propagate memory. We record the same losses and memory metrics across\nepochs, and for the final validation pass we also save per\u2010sequence predictions\nand ground truths. Everything is stored into a nested `experiment_data` dict and\nsaved via `np.save('experiment_data.npy', experiment_data)` for downstream\nplotting.", "Ablation name: Norm-Based Memory Retention Ablation. The proposed solution\nrefactors the memory layer to take a retention_mode argument (\u201centropy\u201d or\n\u201cnorm\u201d) and selects the top\u2010K memory entries either by entropy or by L2 norm. We\nthen loop over both ablation types and each dataset, training and evaluating the\nmodel while logging Memory Retention Ratio, Entropy\u2010Weighted Memory Efficiency,\ntraining/validation loss, and per\u2010token predictions vs ground truth. All results\nare stored in a nested experiment_data dict and saved as a single NumPy file.", "To avoid the HTTP streaming errors when slicing large datasets like PG19, I\u2019ve\nswitched from `streaming=True` and `itertools.islice` to Hugging Face\u2019s built-in\nsplit slicing (e.g. `split='train[:200]'`), which loads only the required subset\ninto memory. This ensures reliable, local dataset loading without connection\nresets. The rest of the training loop and model definitions remain the same,\nwith all tensors and models moved to the proper device. Below is the complete\nfixed implementation.", "We can avoid repeated full-dataset downloads by using the Hugging Face datasets\nstreaming API with split slicing (e.g., `split=\"train[:200]\"` and\n`streaming=True`), so only the small subset we need is fetched. We also preload\neach dataset split a single time into tensors and then reuse those for both\nablation variants, cutting the number of `load_dataset` calls in half. This\nstreaming approach and caching of encoded tensors ensures we stay within time\nlimits while preserving the rest of the training and evaluation logic unchanged.", "Ablation name: Attention-Usage\u2013Based Memory Retention Ablation. We introduce a\nUsageMemoryTransformerLayer that accumulates per\u2010slot attention mass and prunes\nthe lowest\u2010usage slots once mem_size is exceeded, computing a retention ratio\nbased on total kept usage. We wrap this in a UsageTransformerXLModel and iterate\nover the same three datasets, chunking each sequence, tracking per\u2010epoch\nretention ratios and final\u2010epoch token predictions vs. ground truth. All losses,\nretention ratios, predictions, and ground truths are stored under\n`experiment_data['usage_based'][dataset]` and saved as `experiment_data.npy`.", "Ablation name: Feedforward Activation Ablation. Here\u2019s the plan: we wrap the\noriginal transformer code so that the feedforward ReLU can be toggled off for\nthe \u201cfeedforward_identity\u201d ablation, and we iterate over both baseline and\nablated variants for each dataset. We record per\u2010epoch train/val losses and the\ntwo memory metrics, plus final\u2010epoch validation predictions and ground truth.\nAll results are stored in a nested `experiment_data` dict and saved via\n`np.save(\"experiment_data.npy\")`.", "Ablation name: Memory Reset Policy Ablation. Below is a one\u2010file script that\nruns both the baseline (memory reset each sample) and the continuous\u2010memory\nablation (memory only reset at epoch boundaries), collects all losses, retention\nratios, entropy\u2010weighted efficiencies, plus final validation predictions and\ntargets, and saves everything as `experiment_data.npy`.", "We correct the retention\u2010ratio bug by switching from a usage\u2010sum metric\u2014which\ncollapses to 1 whenever pruned slots have zero usage\u2014to a slot\u2010fraction metric.\nSpecifically, after concatenating old and new memory slots, we compute     ratio\n= mem_x_new.size(0) / mem_x_concat.size(0)   so that once the memory is full and\npruning occurs, the ratio falls below 1 in proportion to the slots kept. The\nrest of the streaming, training, and evaluation loop remains unchanged.", "Ablation name: Head Aggregation Function Ablation. We extend the memory layer to\naccept a \u201chead_agg\u201d argument (max or mean) and compute ent_tok accordingly. We\nthen loop over two ablation types (\u201cmax\u201d vs. \u201cmean\u201d) and three datasets, running\ntraining/validation as before while logging per\u2010epoch losses and metrics, and\ncollecting final validation predictions and ground truth. All results are stored\nin a nested experiment_data dict and saved to \u201cexperiment_data.npy\u201d at the end.", "Ablation name: Memory Key-Only Ablation. We will extend the script so that it\nruns both the original \u201cfull\u2010memory\u201d Transformer\u2010XL style model and a \u201ckey\u2010only\u201d\nablation version.  A new `KeyOnlyMemoryTransformerLayer` class concatenates past\nmemory only into the attention keys (not values).  We loop over both ablation\nconditions and each dataset, train for a couple epochs, and collect per\u2010epoch\nlosses, two memory metrics, plus validation predictions and ground\u2010truth.  All\nresults go into a nested `experiment_data` dict which is saved as\n`experiment_data.npy` at the end.", "Ablation name: Residual Connection Ablation. Here we define a \u201cno\u2010residual\u201d\nvariant of the ImprovedMemoryTransformerLayer by simply omitting the x+attn and\nx2+ff(x2) skip additions, then wrap this in a model and re\u2010run the same\ntraining/validation loops across our three datasets. We record per\u2010epoch\ntrain/val losses and memory metrics, and during validation we also collect\nnext\u2010token predictions alongside the ground truth. All results are stored in a\nsingle `experiment_data['residual_connection_ablation']` dict keyed by dataset\nand finally saved to `experiment_data.npy`.", "Ablation name: Gradient-Based Memory Retention Ablation. Here\u2019s the plan: we\ndefine two transformer\u2010xl variants, one with the original entropy\u2010based memory\npruning and one with gradient\u2010based pruning. We loop over both ablation types\nand datasets, training for a few epochs, and inside the gradient case we do\nper\u2010chunk forward+backward, capture the hidden\u2010state grads to score tokens,\nprune to mem_size, and carry mem_x forward. We log train/val losses,\nmemory\u2010retention ratio, score\u2010weighted efficiency, and collect val\npredictions/ground truth. Finally we save `experiment_data.npy`.", "We pad the value tensor with zeros to match the key tensor length in the\nkey\u2010only ablation, so that MultiheadAttention receives matching key/value\nshapes. Specifically, we replace `v = x` with `v = torch.cat([zeros_like(mem),\nx], dim=1)` when memory exists. Additionally, we rename and consistently track\nthe output ratio as \"Entropy Retention Ratio\" and save all metrics at the end.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\nimport itertools\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Hyperparameters\nvocab_size = 256\nmax_len = 128\nembed_dim = 32\nnum_heads = 2\nmem_size = 50\nchunk_size = 32\nnum_epochs = 2\nlr = 1e-3\n\n\n# Define improved memory layer\nclass ImprovedMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)  # B, heads, T\n        ent_tok = ent_h[0].max(dim=0)[0]  # take max across heads, shape T\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n        if mem_x_new.size(0) > self.mem_size:\n            total_ent = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass ImprovedTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = ImprovedMemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# Prepare experiment_data container\nexperiment_data = {}\n\n# Datasets to use\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n\n# Text\u2192token encoding\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * (max_len + 1 - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\nfor ds_name, cfg in configs:\n    key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n    print(f\"\\n=== Dataset: {key} ===\")\n    # stream and sample\n    train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n    train_samples = list(itertools.islice(train_stream, 200))\n    train_enc = [encode_fn(x) for x in train_samples]\n    train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n    train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n    train_loader = DataLoader(\n        TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n    )\n\n    val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n    val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n    val_samples = list(itertools.islice(val_stream, 100))\n    val_enc = [encode_fn(x) for x in val_samples]\n    val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n    val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n    val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n\n    # init model & optimizer\n    model = ImprovedTransformerXLModel(vocab_size, embed_dim, num_heads, mem_size).to(\n        device\n    )\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    # init storage\n    experiment_data[key] = {\n        \"metrics\": {\n            \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n            \"Entropy-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n        },\n        \"losses\": {\"train\": [], \"val\": []},\n    }\n\n    # training & validation\n    for epoch in range(num_epochs):\n        # train\n        model.train()\n        train_loss, train_ratios, train_eme = 0.0, [], []\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in zip([\"input\", \"target\"], batch)}\n            inp, tgt = batch[\"input\"], batch[\"target\"]\n            mem_x = mem_ent = None\n            optimizer.zero_grad()\n            acc_loss = 0.0\n            for i in range(0, inp.size(1), chunk_size):\n                ic = inp[:, i : i + chunk_size]\n                tc = tgt[:, i : i + chunk_size]\n                logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                acc_loss += loss\n                train_ratios.append(ratio)\n                eme = mem_ent.sum().item() / mem_ent.numel()\n                train_eme.append(eme)\n            acc_loss.backward()\n            optimizer.step()\n            train_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n        avg_tr_loss = train_loss / len(train_loader)\n        avg_tr_ratio = sum(train_ratios) / len(train_ratios)\n        avg_tr_eme = sum(train_eme) / len(train_eme)\n        experiment_data[key][\"losses\"][\"train\"].append(avg_tr_loss)\n        experiment_data[key][\"metrics\"][\"Memory Retention Ratio\"][\"train\"].append(\n            avg_tr_ratio\n        )\n        experiment_data[key][\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\n            \"train\"\n        ].append(avg_tr_eme)\n\n        # val\n        model.eval()\n        val_loss, val_ratios, val_eme = 0.0, [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in zip([\"input\", \"target\"], batch)}\n                inp, tgt = batch[\"input\"], batch[\"target\"]\n                mem_x = mem_ent = None\n                acc_loss = 0.0\n                for i in range(0, inp.size(1), chunk_size):\n                    ic = inp[:, i : i + chunk_size]\n                    tc = tgt[:, i : i + chunk_size]\n                    logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                    acc_loss += criterion(logits.view(-1, vocab_size), tc.view(-1))\n                    val_ratios.append(ratio)\n                    eme = mem_ent.sum().item() / mem_ent.numel()\n                    val_eme.append(eme)\n                val_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n        avg_val_loss = val_loss / len(val_loader)\n        avg_val_ratio = sum(val_ratios) / len(val_ratios)\n        avg_val_eme = sum(val_eme) / len(val_eme)\n        experiment_data[key][\"losses\"][\"val\"].append(avg_val_loss)\n        experiment_data[key][\"metrics\"][\"Memory Retention Ratio\"][\"val\"].append(\n            avg_val_ratio\n        )\n        experiment_data[key][\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\n            \"val\"\n        ].append(avg_val_eme)\n        print(f\"Epoch {epoch}: validation_loss = {avg_val_loss:.4f}\")\n\n# save all metrics\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\nimport itertools\nimport numpy as np\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# hyperparams\nvocab_size = 256\nmax_len = 128\nembed_dim = 32\nnum_heads = 2\nmem_size = 50\nchunk_size = 32\nnum_epochs = 2\nlr = 1e-3\n\n\n# Original entropy\u2010based memory layer\nclass OriginalMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)  # B,heads,T\n        ent_tok = ent_h[0].max(dim=0)[0]  # T\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det.clone()\n            mem_ent_new = ent_tok.clone()\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n        if mem_x_new.size(0) > self.mem_size:\n            total = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept = mem_ent_new[idx].sum().item()\n            ratio = kept / total\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\n# Recency\u2010only memory layer\nclass RecencyMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)\n        ent_tok = ent_h[0].max(dim=0)[0]\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det.clone()\n            mem_ent_new = ent_tok.clone()\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n        if mem_x_new.size(0) > self.mem_size:\n            L = mem_x_new.size(0)\n            mem_x_new = mem_x_new[-self.mem_size :]\n            mem_ent_new = mem_ent_new[-self.mem_size :]\n            ratio = self.mem_size / L\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\n# Models\nclass OriginalTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = OriginalMemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mx, me, r = self.mem_layer(emb, mem_x, mem_ent)\n        return self.out(out), mx, me, r\n\n\nclass RecencyTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = RecencyMemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mx, me, r = self.mem_layer(emb, mem_x, mem_ent)\n        return self.out(out), mx, me, r\n\n\n# datasets/configs\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * ((max_len + 1) - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# run ablations\nexperiment_data = {\"original\": {}, \"recency\": {}}\nmapping = {\"original\": OriginalTransformerXLModel, \"recency\": RecencyTransformerXLModel}\nfor ab_type, ModelClass in mapping.items():\n    for ds_name, cfg in configs:\n        ds_key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n        print(f\"Running {ab_type} on {ds_key}\")\n        # load train\n        train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n        train_samples = list(itertools.islice(train_stream, 200))\n        train_enc = [encode_fn(x) for x in train_samples]\n        train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n        train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n        train_loader = DataLoader(\n            TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n        )\n        # load val\n        vs = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n        val_stream = load_dataset(ds_name, cfg, split=vs, streaming=True)\n        val_samples = list(itertools.islice(val_stream, 100))\n        val_enc = [encode_fn(x) for x in val_samples]\n        val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n        val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n        val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n        # init model, stats\n        model = ModelClass(vocab_size, embed_dim, num_heads, mem_size).to(device)\n        opt = optim.Adam(model.parameters(), lr=lr)\n        crit = nn.CrossEntropyLoss()\n        ed = {\n            \"metrics\": {\n                \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n                \"Entropy-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        experiment_data[ab_type][ds_key] = ed\n        # train/val loop\n        for epoch in range(num_epochs):\n            # train\n            model.train()\n            tr_loss, tr_ratios, tr_eme = 0.0, [], []\n            for inp, tgt in train_loader:\n                inp, tgt = inp.to(device), tgt.to(device)\n                mem_x = mem_ent = None\n                opt.zero_grad()\n                batch_loss = 0\n                for i in range(0, inp.size(1), chunk_size):\n                    ic, tc = inp[:, i : i + chunk_size], tgt[:, i : i + chunk_size]\n                    logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                    loss = crit(logits.view(-1, vocab_size), tc.view(-1))\n                    batch_loss += loss\n                    tr_ratios.append(ratio)\n                    tr_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                batch_loss.backward()\n                opt.step()\n                tr_loss += batch_loss.item() / (inp.size(1) / chunk_size)\n            avg_tr_loss = tr_loss / len(train_loader)\n            avg_tr_ratio = sum(tr_ratios) / len(tr_ratios)\n            avg_tr_eme = sum(tr_eme) / len(tr_eme)\n            ed[\"losses\"][\"train\"].append(avg_tr_loss)\n            ed[\"metrics\"][\"Memory Retention Ratio\"][\"train\"].append(avg_tr_ratio)\n            ed[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"].append(\n                avg_tr_eme\n            )\n            # val\n            model.eval()\n            val_loss, val_ratios, val_eme = 0.0, [], []\n            with torch.no_grad():\n                for bi, (inp, tgt) in enumerate(val_loader):\n                    inp, tgt = inp.to(device), tgt.to(device)\n                    mem_x = mem_ent = None\n                    sample_loss = 0\n                    sample_pred, sample_gt = [], []\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic, tc = inp[:, i : i + chunk_size], tgt[:, i : i + chunk_size]\n                        logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                        sample_loss += crit(logits.view(-1, vocab_size), tc.view(-1))\n                        val_ratios.append(ratio)\n                        val_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                        preds = logits.argmax(-1).squeeze(0).cpu().tolist()\n                        sample_pred += preds\n                        sample_gt += tc.squeeze(0).cpu().tolist()\n                    val_loss += sample_loss.item() / (inp.size(1) / chunk_size)\n                    if epoch == num_epochs - 1:\n                        ed[\"predictions\"].append(sample_pred)\n                        ed[\"ground_truth\"].append(sample_gt)\n            avg_val_loss = val_loss / len(val_loader)\n            avg_val_ratio = sum(val_ratios) / len(val_ratios)\n            avg_val_eme = sum(val_eme) / len(val_eme)\n            ed[\"losses\"][\"val\"].append(avg_val_loss)\n            ed[\"metrics\"][\"Memory Retention Ratio\"][\"val\"].append(avg_val_ratio)\n            ed[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"].append(\n                avg_val_eme\n            )\n            print(f\"{ab_type} {ds_key} Epoch {epoch} val_loss={avg_val_loss:.4f}\")\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport itertools\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\n\n# working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# hyperparams\nvocab_size = 256\nmax_len = 128\nembed_dim = 32\nnum_heads = 2\nmem_size = 50\nchunk_size = 32\nnum_epochs = 2\nlr = 1e-3\n\n\n# memory\u2010aware transformer layer with optional detach\nclass ImprovedMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size, detach_mem=True):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n        self.detach_mem = detach_mem\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)\n        ent_tok = ent_h[0].max(dim=0)[0]\n\n        # choose whether to detach\n        if self.detach_mem:\n            mem_update = x.detach()[0]\n        else:\n            mem_update = x[0]\n\n        if mem_x is None:\n            mem_x_new = mem_update\n            mem_ent_new = ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, mem_update], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n\n        # prune to mem_size\n        if mem_x_new.size(0) > self.mem_size:\n            total_ent = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n\n        return out, mem_x_new, mem_ent_new, ratio\n\n\n# full model\nclass ImprovedTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size, detach_mem=True):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = ImprovedMemoryTransformerLayer(\n            embed_dim, num_heads, mem_size, detach_mem\n        )\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# simple encoder\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * (max_len + 1 - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# datasets\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n# prepare storage\nexperiment_data = {}\nablation_types = [\"baseline\", \"memory_grad_flow\"]\n\nfor ablation in ablation_types:\n    experiment_data[ablation] = {}\n    detach_flag = True if ablation == \"baseline\" else False\n\n    for ds_name, cfg in configs:\n        key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n        print(f\"\\n=== Ablation: {ablation} | Dataset: {key} ===\")\n\n        # load train\n        train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n        train_samples = list(itertools.islice(train_stream, 200))\n        train_enc = [encode_fn(x) for x in train_samples]\n        train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n        train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n        train_loader = DataLoader(\n            TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n        )\n\n        # load val\n        val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n        val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n        val_samples = list(itertools.islice(val_stream, 100))\n        val_enc = [encode_fn(x) for x in val_samples]\n        val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n        val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n        val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n\n        # model, opt, loss\n        model = ImprovedTransformerXLModel(\n            vocab_size, embed_dim, num_heads, mem_size, detach_flag\n        ).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        criterion = nn.CrossEntropyLoss()\n\n        # per\u2010dataset storage\n        dataset_data = {\n            \"metrics\": {\n                \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n                \"Entropy-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # train & val\n        for epoch in range(num_epochs):\n            # train\n            model.train()\n            train_loss = 0.0\n            tr_ratios, tr_eme = [], []\n            for inputs, targets in train_loader:\n                inp = inputs.to(device)\n                tgt = targets.to(device)\n                mem_x = mem_ent = None\n                optimizer.zero_grad()\n                acc_loss = 0.0\n                for i in range(0, inp.size(1), chunk_size):\n                    ic = inp[:, i : i + chunk_size]\n                    tc = tgt[:, i : i + chunk_size]\n                    logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                    loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                    acc_loss += loss\n                    tr_ratios.append(ratio)\n                    tr_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                acc_loss.backward()\n                optimizer.step()\n                train_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n            avg_tr_loss = train_loss / len(train_loader)\n            avg_tr_ratio = sum(tr_ratios) / len(tr_ratios)\n            avg_tr_eme = sum(tr_eme) / len(tr_eme)\n            dataset_data[\"losses\"][\"train\"].append(avg_tr_loss)\n            dataset_data[\"metrics\"][\"Memory Retention Ratio\"][\"train\"].append(\n                avg_tr_ratio\n            )\n            dataset_data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\n                \"train\"\n            ].append(avg_tr_eme)\n\n            # val\n            model.eval()\n            val_loss = 0.0\n            vl_ratios, vl_eme = [], []\n            if epoch == num_epochs - 1:\n                preds_all = []\n                gts_all = []\n            with torch.no_grad():\n                for inputs, targets in val_loader:\n                    inp = inputs.to(device)\n                    tgt = targets.to(device)\n                    if epoch == num_epochs - 1:\n                        gt_seq = tgt.squeeze(0).tolist()\n                        pred_seq = []\n                    mem_x = mem_ent = None\n                    acc_loss = 0.0\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                        acc_loss += criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        vl_ratios.append(ratio)\n                        vl_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                        if epoch == num_epochs - 1:\n                            chunk_preds = logits.argmax(-1).squeeze(0).tolist()\n                            pred_seq.extend(chunk_preds)\n                    val_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n                    if epoch == num_epochs - 1:\n                        preds_all.append(pred_seq)\n                        gts_all.append(gt_seq)\n            avg_val_loss = val_loss / len(val_loader)\n            avg_val_ratio = sum(vl_ratios) / len(vl_ratios)\n            avg_val_eme = sum(vl_eme) / len(vl_eme)\n            dataset_data[\"losses\"][\"val\"].append(avg_val_loss)\n            dataset_data[\"metrics\"][\"Memory Retention Ratio\"][\"val\"].append(\n                avg_val_ratio\n            )\n            dataset_data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"].append(\n                avg_val_eme\n            )\n            print(f\"{ablation}-{key} Epoch {epoch}: val_loss={avg_val_loss:.4f}\")\n            if epoch == num_epochs - 1:\n                dataset_data[\"predictions\"] = preds_all\n                dataset_data[\"ground_truth\"] = gts_all\n\n        experiment_data[ablation][key] = dataset_data\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\nimport itertools\nimport numpy as np\n\n# Reproducibility\ntorch.manual_seed(0)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(0)\n\n# Setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hyperparameters\nvocab_size, max_len, embed_dim = 256, 128, 32\nnum_heads, mem_size, chunk_size = 2, 50, 32\nnum_epochs, lr = 2, 1e-3\n\n\n# Memory\u2010transformer with switchable retention\nclass ImprovedMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size, random_retain=False):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n        self.random_retain = random_retain\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)\n        ent_tok = ent_h[0].max(dim=0)[0]\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det.clone()\n            mem_ent_new = ent_tok.clone()\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n        if mem_x_new.size(0) > self.mem_size:\n            total_ent = mem_ent_new.sum().item() + eps\n            if self.random_retain:\n                perm = torch.randperm(mem_ent_new.size(0), device=mem_ent_new.device)\n                idx = perm[: self.mem_size]\n            else:\n                _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass TransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size, random_retain=False):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = ImprovedMemoryTransformerLayer(\n            embed_dim, num_heads, mem_size, random_retain\n        )\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# Datasets and encoding\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * (max_len + 1 - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# Experiment storage\nablation_types = [\"entropy_based\", \"random_retention\"]\nexperiment_data = {ab: {} for ab in ablation_types}\n\nfor ablation in ablation_types:\n    random_retain = ablation == \"random_retention\"\n    for ds_name, cfg in configs:\n        dataset_key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n        print(f\"\\n=== Ablation: {ablation}, Dataset: {dataset_key} ===\")\n\n        # Prepare data\n        train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n        train_samples = list(itertools.islice(train_stream, 200))\n        train_enc = [encode_fn(x) for x in train_samples]\n        train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n        train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n        train_loader = DataLoader(\n            TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n        )\n\n        val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n        val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n        val_samples = list(itertools.islice(val_stream, 100))\n        val_enc = [encode_fn(x) for x in val_samples]\n        val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n        val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n        val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n\n        # Model & optimizer\n        model = TransformerXLModel(\n            vocab_size, embed_dim, num_heads, mem_size, random_retain\n        ).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        criterion = nn.CrossEntropyLoss()\n\n        # Storage init\n        experiment_data[ablation][dataset_key] = {\n            \"metrics\": {\n                \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n                \"Entropy-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # Train & validate\n        for epoch in range(num_epochs):\n            model.train()\n            train_loss, train_ratios, train_eme = 0.0, [], []\n            for batch in train_loader:\n                inp, tgt = [t.to(device) for t in batch]\n                mem_x = mem_ent = None\n                optimizer.zero_grad()\n                acc_loss = 0.0\n                for i in range(0, inp.size(1), chunk_size):\n                    ic = inp[:, i : i + chunk_size]\n                    tc = tgt[:, i : i + chunk_size]\n                    logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                    loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                    acc_loss += loss\n                    train_ratios.append(ratio)\n                    train_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                acc_loss.backward()\n                optimizer.step()\n                train_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n            avg_tr_loss = train_loss / len(train_loader)\n            avg_tr_ratio = sum(train_ratios) / len(train_ratios)\n            avg_tr_eme = sum(train_eme) / len(train_eme)\n            ed = experiment_data[ablation][dataset_key]\n            ed[\"losses\"][\"train\"].append(avg_tr_loss)\n            ed[\"metrics\"][\"Memory Retention Ratio\"][\"train\"].append(avg_tr_ratio)\n            ed[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"].append(\n                avg_tr_eme\n            )\n\n            model.eval()\n            val_loss, val_ratios, val_eme = 0.0, [], []\n            with torch.no_grad():\n                for batch in val_loader:\n                    inp, tgt = [t.to(device) for t in batch]\n                    mem_x = mem_ent = None\n                    acc_loss = 0.0\n                    preds = []\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                        acc_loss += criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        val_ratios.append(ratio)\n                        val_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                        p = logits.argmax(dim=-1)[0].cpu()\n                        preds.append(p)\n                    val_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n                    # record preds & ground truth\n                    batch_pred = torch.cat(preds, dim=0).numpy()\n                    batch_tgt = tgt[0].cpu().numpy()\n                    ed[\"predictions\"].append(batch_pred)\n                    ed[\"ground_truth\"].append(batch_tgt)\n            avg_val_loss = val_loss / len(val_loader)\n            avg_val_ratio = sum(val_ratios) / len(val_ratios)\n            avg_val_eme = sum(val_eme) / len(val_eme)\n            ed[\"losses\"][\"val\"].append(avg_val_loss)\n            ed[\"metrics\"][\"Memory Retention Ratio\"][\"val\"].append(avg_val_ratio)\n            ed[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"].append(\n                avg_val_eme\n            )\n            print(f\"Epoch {epoch}: validation_loss = {avg_val_loss:.4f}\")\n\n# Save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport itertools\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\n\n# set up working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# hyperparams\nvocab_size = 256\nmax_len = 128\nembed_dim = 32\nnum_heads = 2\nmem_size = 50\nchunk_size = 32\nnum_epochs = 2\nlr = 1e-3\n\n\n# baseline memory layer\nclass ImprovedMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], 1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)\n        ent_tok = ent_h[0].max(dim=0)[0]\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], 0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], 0)\n        if mem_x_new.size(0) > self.mem_size:\n            total_ent = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\n# ablated layer: no feedforward\nclass AblatedMemoryTransformerLayer(ImprovedMemoryTransformerLayer):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__(embed_dim, num_heads, mem_size)\n        self.ff = nn.Identity()\n\n\n# main model class\nclass ImprovedTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size, layer_cls):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = layer_cls(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x, mem_ent, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x, mem_ent, ratio\n\n\n# dataset configs\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n\n# encoding fn\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * (max_len + 1 - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# define ablations\nablations = {\n    \"baseline\": ImprovedMemoryTransformerLayer,\n    \"ffn_removed\": AblatedMemoryTransformerLayer,\n}\n\n# container for all results\nexperiment_data = {abt: {} for abt in ablations}\n\n# criterion\ncriterion = nn.CrossEntropyLoss()\n\nfor abt_name, layer_cls in ablations.items():\n    print(f\"\\n=== Ablation: {abt_name} ===\")\n    for ds_name, cfg in configs:\n        key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n        print(f\"\\nDataset: {key}\")\n        # init storage\n        experiment_data[abt_name][key] = {\n            \"metrics\": {\n                \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n                \"Entropy-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        data_dict = experiment_data[abt_name][key]\n        # load train\n        train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n        train_samples = list(itertools.islice(train_stream, 200))\n        train_enc = [encode_fn(x) for x in train_samples]\n        train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n        train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n        train_loader = DataLoader(\n            TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n        )\n        # load val\n        val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n        val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n        val_samples = list(itertools.islice(val_stream, 100))\n        val_enc = [encode_fn(x) for x in val_samples]\n        val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n        val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n        val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n        # init model & optimizer\n        model = ImprovedTransformerXLModel(\n            vocab_size, embed_dim, num_heads, mem_size, layer_cls\n        ).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        # train & val loops\n        for epoch in range(num_epochs):\n            # train\n            model.train()\n            train_loss, tr_ratios, tr_eme = 0.0, [], []\n            for inp, tgt in train_loader:\n                inp, tgt = inp.to(device), tgt.to(device)\n                mem_x = mem_ent = None\n                optimizer.zero_grad()\n                acc_loss = 0.0\n                for i in range(0, inp.size(1), chunk_size):\n                    ic = inp[:, i : i + chunk_size]\n                    tc = tgt[:, i : i + chunk_size]\n                    logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                    loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                    acc_loss += loss\n                    tr_ratios.append(ratio)\n                    eme = mem_ent.sum().item() / mem_ent.numel()\n                    tr_eme.append(eme)\n                acc_loss.backward()\n                optimizer.step()\n                train_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n            avg_tr_loss = train_loss / len(train_loader)\n            avg_tr_ratio = sum(tr_ratios) / len(tr_ratios)\n            avg_tr_eme = sum(tr_eme) / len(tr_eme)\n            data_dict[\"losses\"][\"train\"].append(avg_tr_loss)\n            data_dict[\"metrics\"][\"Memory Retention Ratio\"][\"train\"].append(avg_tr_ratio)\n            data_dict[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"].append(\n                avg_tr_eme\n            )\n            # val\n            model.eval()\n            val_loss, vl_ratios, vl_eme = 0.0, [], []\n            with torch.no_grad():\n                for inp, tgt in val_loader:\n                    inp, tgt = inp.to(device), tgt.to(device)\n                    mem_x = mem_ent = None\n                    acc_loss = 0.0\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                        acc_loss += criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        vl_ratios.append(ratio)\n                        eme = mem_ent.sum().item() / mem_ent.numel()\n                        vl_eme.append(eme)\n                        preds = logits.argmax(-1).cpu().numpy().flatten().tolist()\n                        gts = tc.cpu().numpy().flatten().tolist()\n                        data_dict[\"predictions\"].extend(preds)\n                        data_dict[\"ground_truth\"].extend(gts)\n                    val_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n            avg_val_loss = val_loss / len(val_loader)\n            avg_va_ratio = sum(vl_ratios) / len(vl_ratios)\n            avg_va_eme = sum(vl_eme) / len(vl_eme)\n            data_dict[\"losses\"][\"val\"].append(avg_val_loss)\n            data_dict[\"metrics\"][\"Memory Retention Ratio\"][\"val\"].append(avg_va_ratio)\n            data_dict[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"].append(\n                avg_va_eme\n            )\n            print(\n                f\"Ablation={abt_name} Dataset={key} Epoch={epoch} ValLoss={avg_val_loss:.4f}\"\n            )\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\nimport itertools\nimport numpy as np\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device and hyperparameters\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvocab_size = 256\nmax_len = 128\nembed_dim = 32\nnum_heads = 2\nmem_size = 50\nchunk_size = 32\nnum_epochs = 2\nlr = 1e-3\n\n# container for results\nexperiment_data = {\"no_layernorm\": {}}\n\n\n# ablated memory layer without any LayerNorm\nclass MemoryTransformerLayerNoLN(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        # skip all LayerNorms\n        x2 = x + attn_out\n        out = x2 + self.ff(x2)\n        # entropy\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)  # B, heads, T\n        ent_tok = ent_h[0].max(dim=0)[0]  # T\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n        if mem_x_new.size(0) > self.mem_size:\n            total_ent = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\n# ablated Transformer-XL style model\nclass AblatedTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = MemoryTransformerLayerNoLN(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# simple char\u2192id encoder\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * (max_len + 1 - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# datasets to run\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\nfor ds_name, cfg in configs:\n    key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n    print(f\"=== Ablation no_layernorm on {key} ===\")\n    experiment_data[\"no_layernorm\"][key] = {\n        \"metrics\": {\n            \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n            \"Entropy-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n        },\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # prepare data\n    train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n    train_samples = list(itertools.islice(train_stream, 200))\n    train_enc = [encode_fn(x) for x in train_samples]\n    tr_in = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n    tr_tg = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n    train_loader = DataLoader(TensorDataset(tr_in, tr_tg), batch_size=1, shuffle=True)\n\n    val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n    val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n    val_samples = list(itertools.islice(val_stream, 100))\n    val_enc = [encode_fn(x) for x in val_samples]\n    vl_in = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n    vl_tg = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n    val_loader = DataLoader(TensorDataset(vl_in, vl_tg), batch_size=1)\n\n    # model, optimizer, loss\n    model = AblatedTransformerXLModel(vocab_size, embed_dim, num_heads, mem_size).to(\n        device\n    )\n    opt = optim.Adam(model.parameters(), lr=lr)\n    crit = nn.CrossEntropyLoss()\n\n    # for final-epoch preds\n    val_preds, val_gts = [], []\n\n    # training & validation\n    for epoch in range(num_epochs):\n        # train\n        model.train()\n        train_loss, train_ratios, train_eme = 0.0, [], []\n        for inp, tgt in train_loader:\n            inp, tgt = inp.to(device), tgt.to(device)\n            mem_x = mem_ent = None\n            opt.zero_grad()\n            acc_loss = 0.0\n            for i in range(0, inp.size(1), chunk_size):\n                ic = inp[:, i : i + chunk_size]\n                tc = tgt[:, i : i + chunk_size]\n                logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                loss = crit(logits.view(-1, vocab_size), tc.view(-1))\n                acc_loss += loss\n                train_ratios.append(ratio)\n                eme = mem_ent.sum().item() / mem_ent.numel()\n                train_eme.append(eme)\n            acc_loss.backward()\n            opt.step()\n            train_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n        avg_tr_loss = train_loss / len(train_loader)\n        avg_tr_ratio = sum(train_ratios) / len(train_ratios)\n        avg_tr_eme = sum(train_eme) / len(train_eme)\n        md = experiment_data[\"no_layernorm\"][key]\n        md[\"losses\"][\"train\"].append(avg_tr_loss)\n        md[\"metrics\"][\"Memory Retention Ratio\"][\"train\"].append(avg_tr_ratio)\n        md[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"].append(avg_tr_eme)\n\n        # val\n        model.eval()\n        val_loss, val_ratios, val_eme = 0.0, [], []\n        record = epoch == num_epochs - 1\n        with torch.no_grad():\n            for inp, tgt in val_loader:\n                inp, tgt = inp.to(device), tgt.to(device)\n                mem_x = mem_ent = None\n                acc_loss = 0.0\n                if record:\n                    pred_tokens, true_tokens = [], []\n                for i in range(0, inp.size(1), chunk_size):\n                    ic = inp[:, i : i + chunk_size]\n                    tc = tgt[:, i : i + chunk_size]\n                    logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                    acc_loss += crit(logits.view(-1, vocab_size), tc.view(-1))\n                    val_ratios.append(ratio)\n                    eme = mem_ent.sum().item() / mem_ent.numel()\n                    val_eme.append(eme)\n                    if record:\n                        pred_tokens.extend(logits.argmax(-1)[0].tolist())\n                        true_tokens.extend(tc[0].tolist())\n                val_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n                if record:\n                    val_preds.append(pred_tokens)\n                    val_gts.append(true_tokens)\n        avg_val_loss = val_loss / len(val_loader)\n        avg_val_ratio = sum(val_ratios) / len(val_ratios)\n        avg_val_eme = sum(val_eme) / len(val_eme)\n        md[\"losses\"][\"val\"].append(avg_val_loss)\n        md[\"metrics\"][\"Memory Retention Ratio\"][\"val\"].append(avg_val_ratio)\n        md[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"].append(avg_val_eme)\n        print(f\"{key} Epoch {epoch}: val_loss={avg_val_loss:.4f}\")\n    # store final predictions & ground truth\n    experiment_data[\"no_layernorm\"][key][\"predictions\"] = val_preds\n    experiment_data[\"no_layernorm\"][key][\"ground_truth\"] = val_gts\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\nimport itertools\nimport numpy as np\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Hyperparameters\nvocab_size = 256\nmax_len = 128\nembed_dim = 32\nnum_heads = 2\nmem_size = 50\nchunk_size = 32\nnum_epochs = 2\nlr = 1e-3\n\n\n# Define improved memory layer and model\nclass ImprovedMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)  # B, heads, T\n        ent_tok = ent_h[0].max(dim=0)[0]  # take max across heads, shape T\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n        if mem_x_new.size(0) > self.mem_size:\n            total_ent = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass ImprovedTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = ImprovedMemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# Data\u2192token encoding\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * (max_len + 1 - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# Datasets to use\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n# Prepare experiment_data container\nablation_types = [(\"original\", False), (\"no_memory_attention\", True)]\nexperiment_data = {name: {} for name, _ in ablation_types}\n\nfor ablation_name, disable_memory in ablation_types:\n    for ds_name, cfg in configs:\n        key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n        print(f\"\\n=== Ablation: {ablation_name}, Dataset: {key} ===\")\n        # init storage\n        experiment_data[ablation_name][key] = {\n            \"metrics\": {\n                \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n                \"Entropy-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        # load train\n        train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n        train_samples = list(itertools.islice(train_stream, 200))\n        train_enc = [encode_fn(x) for x in train_samples]\n        train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n        train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n        train_loader = DataLoader(\n            TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n        )\n        # load val\n        val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n        val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n        val_samples = list(itertools.islice(val_stream, 100))\n        val_enc = [encode_fn(x) for x in val_samples]\n        val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n        val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n        val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n        # init model & optimizer\n        model = ImprovedTransformerXLModel(\n            vocab_size, embed_dim, num_heads, mem_size\n        ).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        criterion = nn.CrossEntropyLoss()\n        # training & validation\n        for epoch in range(num_epochs):\n            # TRAIN\n            model.train()\n            train_loss, train_ratios, train_eme = 0.0, [], []\n            for batch in train_loader:\n                inp, tgt = batch[0].to(device), batch[1].to(device)\n                optimizer.zero_grad()\n                acc_loss = 0.0\n                mem_x = None\n                mem_ent = None\n                for i in range(0, inp.size(1), chunk_size):\n                    ic = inp[:, i : i + chunk_size]\n                    tc = tgt[:, i : i + chunk_size]\n                    if disable_memory:\n                        cur_mem_x, cur_mem_ent = None, None\n                    else:\n                        cur_mem_x, cur_mem_ent = mem_x, mem_ent\n                    logits, new_mem_x, new_mem_ent, ratio = model(\n                        ic, cur_mem_x, cur_mem_ent\n                    )\n                    loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                    acc_loss += loss\n                    train_ratios.append(ratio)\n                    eme = new_mem_ent.sum().item() / new_mem_ent.numel()\n                    train_eme.append(eme)\n                    if not disable_memory:\n                        mem_x, mem_ent = new_mem_x, new_mem_ent\n                acc_loss.backward()\n                optimizer.step()\n                train_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n            # record train metrics\n            avg_tr_loss = train_loss / len(train_loader)\n            avg_tr_ratio = sum(train_ratios) / len(train_ratios)\n            avg_tr_eme = sum(train_eme) / len(train_eme)\n            experiment_data[ablation_name][key][\"losses\"][\"train\"].append(avg_tr_loss)\n            experiment_data[ablation_name][key][\"metrics\"][\"Memory Retention Ratio\"][\n                \"train\"\n            ].append(avg_tr_ratio)\n            experiment_data[ablation_name][key][\"metrics\"][\n                \"Entropy-Weighted Memory Efficiency\"\n            ][\"train\"].append(avg_tr_eme)\n            # VAL\n            model.eval()\n            val_loss, val_ratios, val_eme = 0.0, [], []\n            record_preds = epoch == num_epochs - 1\n            with torch.no_grad():\n                for batch in val_loader:\n                    inp, tgt = batch[0].to(device), batch[1].to(device)\n                    acc_loss = 0.0\n                    mem_x = None\n                    mem_ent = None\n                    if record_preds:\n                        pred_seq, gt_seq = [], []\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        if disable_memory:\n                            cur_mem_x, cur_mem_ent = None, None\n                        else:\n                            cur_mem_x, cur_mem_ent = mem_x, mem_ent\n                        logits, new_mem_x, new_mem_ent, ratio = model(\n                            ic, cur_mem_x, cur_mem_ent\n                        )\n                        loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        acc_loss += loss\n                        val_ratios.append(ratio)\n                        eme = new_mem_ent.sum().item() / new_mem_ent.numel()\n                        val_eme.append(eme)\n                        if record_preds:\n                            preds = logits.argmax(dim=-1).squeeze(0).tolist()\n                            gts = tc.squeeze(0).tolist()\n                            pred_seq.extend(preds)\n                            gt_seq.extend(gts)\n                        if not disable_memory:\n                            mem_x, mem_ent = new_mem_x, new_mem_ent\n                    val_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n                    if record_preds:\n                        experiment_data[ablation_name][key][\"predictions\"].append(\n                            np.array(pred_seq)\n                        )\n                        experiment_data[ablation_name][key][\"ground_truth\"].append(\n                            np.array(gt_seq)\n                        )\n            avg_val_loss = val_loss / len(val_loader)\n            avg_val_ratio = sum(val_ratios) / len(val_ratios)\n            avg_val_eme = sum(val_eme) / len(val_eme)\n            experiment_data[ablation_name][key][\"losses\"][\"val\"].append(avg_val_loss)\n            experiment_data[ablation_name][key][\"metrics\"][\"Memory Retention Ratio\"][\n                \"val\"\n            ].append(avg_val_ratio)\n            experiment_data[ablation_name][key][\"metrics\"][\n                \"Entropy-Weighted Memory Efficiency\"\n            ][\"val\"].append(avg_val_eme)\n            print(\n                f\"{ablation_name} | {key} | Epoch {epoch}: val_loss={avg_val_loss:.4f}\"\n            )\n\n# save all metrics and predictions\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, itertools, numpy as np, torch\nimport torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# hyperparameters\nvocab_size, max_len, embed_dim = 256, 128, 32\nnum_heads, mem_size, chunk_size = 2, 50, 32\nnum_epochs, lr = 2, 1e-3\n\n\n# memory layer with switchable retention\nclass ImprovedMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size, retention_mode):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n        self.mode = retention_mode\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], 1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(-1)  # B,heads,T\n        ent_tok = ent_h[0].max(0)[0]  # T\n        x_det = x.detach()[0]  # T,E\n\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], 0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], 0)\n\n        if mem_x_new.size(0) > self.mem_size:\n            if self.mode == \"entropy\":\n                scores = mem_ent_new\n            else:  # norm\n                scores = mem_x_new.norm(p=2, dim=1)\n            total = scores.sum().item() + eps\n            _, idx = torch.topk(scores, self.mem_size)\n            kept = scores[idx].sum().item()\n            ratio = kept / total\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\n# model wrapper\nclass ImprovedTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size, mode):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = ImprovedMemoryTransformerLayer(\n            embed_dim, num_heads, mem_size, mode\n        )\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mx, me, r = self.mem_layer(emb, mem_x, mem_ent)\n        return self.out(out), mx, me, r\n\n\n# datasets\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n\ndef encode_fn(ex):\n    txt = ex.get(\"text\") or ex.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * ((max_len + 1) - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# initialize experiment_data\nablation_types = [\"entropy\", \"norm\"]\nexperiment_data = {a: {} for a in ablation_types}\n\n# run experiments\nfor mode in ablation_types:\n    for ds_name, cfg in configs:\n        key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n        # load & encode\n        train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n        train_samples = list(itertools.islice(train_stream, 200))\n        train_enc = [encode_fn(x) for x in train_samples]\n        ti = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n        to = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n        train_loader = DataLoader(TensorDataset(ti, to), batch_size=1, shuffle=True)\n        val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n        val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n        val_samples = list(itertools.islice(val_stream, 100))\n        val_enc = [encode_fn(x) for x in val_samples]\n        vi = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n        vo = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n        val_loader = DataLoader(TensorDataset(vi, vo), batch_size=1)\n\n        # storage\n        experiment_data[mode][key] = {\n            \"metrics\": {\n                \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n                \"Entropy-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # model & opt\n        model = ImprovedTransformerXLModel(\n            vocab_size, embed_dim, num_heads, mem_size, mode\n        ).to(device)\n        opt = optim.Adam(model.parameters(), lr=lr)\n        crit = nn.CrossEntropyLoss()\n\n        # train & validate\n        for ep in range(num_epochs):\n            # train\n            model.train()\n            tr_loss, tr_ratios, tr_eme = 0, [], []\n            for inp, tgt in train_loader:\n                inp, tgt = inp.to(device), tgt.to(device)\n                mem_x = mem_ent = None\n                opt.zero_grad()\n                acc = 0\n                for i in range(0, inp.size(1), chunk_size):\n                    ic, tc = inp[:, i : i + chunk_size], tgt[:, i : i + chunk_size]\n                    logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                    loss = crit(logits.view(-1, vocab_size), tc.view(-1))\n                    acc += loss\n                    tr_ratios.append(ratio)\n                    tr_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                acc.backward()\n                opt.step()\n                tr_loss += acc.item() / (inp.size(1) / chunk_size)\n            ave_tr_loss = tr_loss / len(train_loader)\n            experiment_data[mode][key][\"losses\"][\"train\"].append(ave_tr_loss)\n            experiment_data[mode][key][\"metrics\"][\"Memory Retention Ratio\"][\n                \"train\"\n            ].append(sum(tr_ratios) / len(tr_ratios))\n            experiment_data[mode][key][\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\n                \"train\"\n            ].append(sum(tr_eme) / len(tr_eme))\n\n            # val\n            model.eval()\n            val_loss, val_ratios, val_eme = 0, [], []\n            with torch.no_grad():\n                for inp, tgt in val_loader:\n                    inp, tgt = inp.to(device), tgt.to(device)\n                    mem_x = mem_ent = None\n                    acc = 0\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic, tc = inp[:, i : i + chunk_size], tgt[:, i : i + chunk_size]\n                        logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                        acc += crit(logits.view(-1, vocab_size), tc.view(-1))\n                        val_ratios.append(ratio)\n                        val_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                        # record preds & gt\n                        preds = logits.argmax(-1).cpu().flatten().tolist()\n                        gts = tc.cpu().flatten().tolist()\n                        experiment_data[mode][key][\"predictions\"].extend(preds)\n                        experiment_data[mode][key][\"ground_truth\"].extend(gts)\n                    val_loss += acc.item() / (inp.size(1) / chunk_size)\n            ave_val_loss = val_loss / len(val_loader)\n            experiment_data[mode][key][\"losses\"][\"val\"].append(ave_val_loss)\n            experiment_data[mode][key][\"metrics\"][\"Memory Retention Ratio\"][\n                \"val\"\n            ].append(sum(val_ratios) / len(val_ratios))\n            experiment_data[mode][key][\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\n                \"val\"\n            ].append(sum(val_eme) / len(val_eme))\n            print(f\"[{mode}][{key}] Epoch {ep} val_loss={ave_val_loss:.4f}\")\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport itertools\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\n\n# working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# hyperparams\nvocab_size = 256\nmax_len = 128\nembed_dim = 32\nnum_heads = 2\nmem_size = 50\nchunk_size = 32\nnum_epochs = 2\nlr = 1e-3\n\n\n# memory\u2010aware transformer layer\nclass ImprovedMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size, detach_mem=True):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n        self.detach_mem = detach_mem\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1).to(device)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)\n        ent_tok = ent_h[0].max(dim=0)[0]\n\n        # update memory using input embeddings (can switch to out.detach() if desired)\n        mem_update = x.detach()[0] if self.detach_mem else x[0]\n        if mem_x is None:\n            mem_x_new, mem_ent_new = mem_update, ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, mem_update], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n\n        # prune to mem_size\n        if mem_x_new.size(0) > self.mem_size:\n            total_ent = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n\n        return out, mem_x_new, mem_ent_new, ratio\n\n\n# full model\nclass ImprovedTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size, detach_mem=True):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = ImprovedMemoryTransformerLayer(\n            embed_dim, num_heads, mem_size, detach_mem\n        )\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# simple encoder\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * (max_len + 1 - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# datasets and experiment storage\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\nexperiment_data = {}\nablation_types = [\"baseline\", \"memory_grad_flow\"]\n\nfor ablation in ablation_types:\n    detach_flag = ablation == \"baseline\"\n    experiment_data[ablation] = {}\n    for ds_name, cfg in configs:\n        key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n        print(f\"\\n=== Ablation: {ablation} | Dataset: {key} ===\")\n\n        # load train subset\n        train_split = \"train[:200]\"\n        train_ds = load_dataset(ds_name, cfg, split=train_split)\n        train_enc = [encode_fn(x) for x in train_ds]\n        train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n        train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n        train_loader = DataLoader(\n            TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n        )\n\n        # load val subset\n        val_split_name = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n        val_split = f\"{val_split_name}[:100]\"\n        val_ds = load_dataset(ds_name, cfg, split=val_split)\n        val_enc = [encode_fn(x) for x in val_ds]\n        val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n        val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n        val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n\n        # model, optimizer, criterion\n        model = ImprovedTransformerXLModel(\n            vocab_size, embed_dim, num_heads, mem_size, detach_flag\n        ).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        criterion = nn.CrossEntropyLoss()\n\n        # storage for this dataset\n        dataset_data = {\n            \"metrics\": {\n                \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n                \"Entropy-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # train & evaluate\n        for epoch in range(num_epochs):\n            # training\n            model.train()\n            tr_ratios, tr_eme = [], []\n            total_tr_loss = 0.0\n            for inputs, targets in train_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                mem_x = mem_ent = None\n                optimizer.zero_grad()\n                acc_loss = 0.0\n                for i in range(0, inputs.size(1), chunk_size):\n                    ic = inputs[:, i : i + chunk_size]\n                    tc = targets[:, i : i + chunk_size]\n                    logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                    loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                    acc_loss += loss\n                    tr_ratios.append(ratio)\n                    tr_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                acc_loss.backward()\n                optimizer.step()\n                total_tr_loss += acc_loss.item() / (inputs.size(1) / chunk_size)\n            avg_tr_loss = total_tr_loss / len(train_loader)\n            dataset_data[\"losses\"][\"train\"].append(avg_tr_loss)\n            dataset_data[\"metrics\"][\"Memory Retention Ratio\"][\"train\"].append(\n                sum(tr_ratios) / len(tr_ratios)\n            )\n            dataset_data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\n                \"train\"\n            ].append(sum(tr_eme) / len(tr_eme))\n\n            # validation\n            model.eval()\n            vl_ratios, vl_eme = [], []\n            total_val_loss = 0.0\n            preds_all, gts_all = [], []\n            with torch.no_grad():\n                for inputs, targets in val_loader:\n                    inputs, targets = inputs.to(device), targets.to(device)\n                    mem_x = mem_ent = None\n                    acc_loss = 0.0\n                    gt_seq = targets.squeeze(0).tolist()\n                    pred_seq = []\n                    for i in range(0, inputs.size(1), chunk_size):\n                        ic = inputs[:, i : i + chunk_size]\n                        tc = targets[:, i : i + chunk_size]\n                        logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                        acc_loss += criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        vl_ratios.append(ratio)\n                        vl_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                        pred_seq.extend(logits.argmax(-1).squeeze(0).tolist())\n                    total_val_loss += acc_loss.item() / (inputs.size(1) / chunk_size)\n                    preds_all.append(pred_seq)\n                    gts_all.append(gt_seq)\n            avg_val_loss = total_val_loss / len(val_loader)\n            dataset_data[\"losses\"][\"val\"].append(avg_val_loss)\n            dataset_data[\"metrics\"][\"Memory Retention Ratio\"][\"val\"].append(\n                sum(vl_ratios) / len(vl_ratios)\n            )\n            dataset_data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"].append(\n                sum(vl_eme) / len(vl_eme)\n            )\n            print(\n                f\"{ablation}-{key} Epoch {epoch}: validation_loss = {avg_val_loss:.4f}\"\n            )\n\n            if epoch == num_epochs - 1:\n                dataset_data[\"predictions\"] = preds_all\n                dataset_data[\"ground_truth\"] = gts_all\n\n        experiment_data[ablation][key] = dataset_data\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport itertools\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\n\n# prepare working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# hyperparameters\nvocab_size = 256\nmax_len = 128\nembed_dim = 32\nnum_heads = 2\nmem_size = 50\nchunk_size = 32\nnum_epochs = 2\nlr = 1e-3\n\n\n# memory-aware transformer layer\nclass ImprovedMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size, detach_mem=True):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n        self.detach_mem = detach_mem\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1).to(device)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n\n        # entropy per token (max over heads)\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)  # (B, heads, T)\n        ent_tok = ent_h[0].max(dim=0)[0]  # (T,)\n\n        # memory update\n        mem_update = x.detach()[0] if self.detach_mem else x[0]\n        if mem_x is None:\n            mem_x_new, mem_ent_new = mem_update, ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, mem_update], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n\n        # prune if over budget\n        if mem_x_new.size(0) > self.mem_size:\n            total_ent = mem_ent_new.sum().item() + eps\n            kept_ent = mem_ent_new.topk(self.mem_size)[0].sum().item()\n            idx = torch.topk(mem_ent_new, self.mem_size)[1]\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n            ratio = kept_ent / total_ent\n        else:\n            ratio = 1.0\n\n        return out, mem_x_new, mem_ent_new, ratio\n\n\n# full model\nclass ImprovedTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size, detach_mem=True):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = ImprovedMemoryTransformerLayer(\n            embed_dim, num_heads, mem_size, detach_mem\n        )\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x, mem_ent, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x, mem_ent, ratio\n\n\n# simple encoder for text fields\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * (max_len + 1 - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# datasets to run\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n# preload all splits once with streaming to avoid full downloads\npreloaded = {}\nfor ds_name, cfg in configs:\n    key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n    # training split\n    train_split = f\"train[:200]\"\n    raw_train = load_dataset(ds_name, cfg, split=train_split, streaming=True)\n    train_enc = [encode_fn(x) for x in raw_train]\n    train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n    train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n    train_loader = DataLoader(\n        TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n    )\n    # validation split\n    val_base = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n    val_split = f\"{val_base}[:100]\"\n    raw_val = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n    val_enc = [encode_fn(x) for x in raw_val]\n    val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n    val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n    val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n    preloaded[key] = {\"train\": train_loader, \"val\": val_loader}\n\n# main experiment loop\nablation_types = [\"baseline\", \"memory_grad_flow\"]\nexperiment_data = {}\n\nfor ablation in ablation_types:\n    detach_flag = ablation == \"baseline\"\n    experiment_data[ablation] = {}\n    for ds_name, cfg in configs:\n        key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n        print(f\"\\n=== Ablation: {ablation} | Dataset: {key} ===\")\n\n        # fetch preloaded loaders\n        train_loader = preloaded[key][\"train\"]\n        val_loader = preloaded[key][\"val\"]\n\n        # init model and optimizer\n        model = ImprovedTransformerXLModel(\n            vocab_size, embed_dim, num_heads, mem_size, detach_flag\n        ).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        criterion = nn.CrossEntropyLoss()\n\n        # storage\n        dataset_data = {\n            \"metrics\": {\n                \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n                \"Entropy-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # train & eval\n        for epoch in range(num_epochs):\n            # training\n            model.train()\n            tr_ratios, tr_eme, total_tr_loss = [], [], 0.0\n            for inputs, targets in train_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                mem_x = mem_ent = None\n                optimizer.zero_grad()\n                acc_loss = 0.0\n                for i in range(0, inputs.size(1), chunk_size):\n                    ic = inputs[:, i : i + chunk_size]\n                    tc = targets[:, i : i + chunk_size]\n                    logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                    loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                    acc_loss += loss\n                    tr_ratios.append(ratio)\n                    tr_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                acc_loss.backward()\n                optimizer.step()\n                total_tr_loss += acc_loss.item() / (inputs.size(1) / chunk_size)\n            avg_tr_loss = total_tr_loss / len(train_loader)\n            dataset_data[\"losses\"][\"train\"].append(avg_tr_loss)\n            dataset_data[\"metrics\"][\"Memory Retention Ratio\"][\"train\"].append(\n                sum(tr_ratios) / len(tr_ratios)\n            )\n            dataset_data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\n                \"train\"\n            ].append(sum(tr_eme) / len(tr_eme))\n\n            # validation\n            model.eval()\n            vl_ratios, vl_eme, total_val_loss = [], [], 0.0\n            preds_all, gts_all = [], []\n            with torch.no_grad():\n                for inputs, targets in val_loader:\n                    inputs, targets = inputs.to(device), targets.to(device)\n                    mem_x = mem_ent = None\n                    acc_loss = 0.0\n                    gt_seq = targets.squeeze(0).tolist()\n                    pred_seq = []\n                    for i in range(0, inputs.size(1), chunk_size):\n                        ic = inputs[:, i : i + chunk_size]\n                        tc = targets[:, i : i + chunk_size]\n                        logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                        acc_loss += criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        vl_ratios.append(ratio)\n                        vl_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                        pred_seq.extend(logits.argmax(-1).squeeze(0).tolist())\n                    total_val_loss += acc_loss.item() / (inputs.size(1) / chunk_size)\n                    preds_all.append(pred_seq)\n                    gts_all.append(gt_seq)\n            avg_val_loss = total_val_loss / len(val_loader)\n            dataset_data[\"losses\"][\"val\"].append(avg_val_loss)\n            dataset_data[\"metrics\"][\"Memory Retention Ratio\"][\"val\"].append(\n                sum(vl_ratios) / len(vl_ratios)\n            )\n            dataset_data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"].append(\n                sum(vl_eme) / len(vl_eme)\n            )\n            print(\n                f\"{ablation}-{key} Epoch {epoch}: validation_loss = {avg_val_loss:.4f}\"\n            )\n\n            if epoch == num_epochs - 1:\n                dataset_data[\"predictions\"] = preds_all\n                dataset_data[\"ground_truth\"] = gts_all\n\n        experiment_data[ablation][key] = dataset_data\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport itertools\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# hyperparams\nvocab_size, max_len, embed_dim = 256, 128, 32\nnum_heads, mem_size, chunk_size = 2, 50, 32\nnum_epochs, lr = 2, 1e-3\n\n\nclass UsageMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_usage):\n        B, T, E = x.size()\n        # build key/value\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        # attend\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        x_det = x.detach()[0]  # (T,E)\n\n        # update memory and usage\n        if mem_x is None:\n            # no prior mem \u2192 new mem slots have zero usage\n            mem_x_new = x_det\n            mem_usage_new = torch.zeros(T, device=x.device)\n            ratio = 1.0\n        else:\n            prev_slots = mem_x.size(0)\n            w = attn_w[0]  # (heads, T, prev_slots+T)\n            w_mem = w[:, :, :prev_slots]  # focus on mem slots\n            usage_delta = w_mem.sum(dim=(0, 1))  # (prev_slots,)\n            new_usage_existing = mem_usage + usage_delta\n            # concat new slots with zero initial usage\n            mem_x_concat = torch.cat([mem_x, x_det], dim=0)\n            usage_concat = torch.cat(\n                [new_usage_existing, torch.zeros(T, device=x.device)], dim=0\n            )\n            # prune if over cap\n            if mem_x_concat.size(0) > self.mem_size:\n                total = usage_concat.sum().item() + 1e-10\n                _, idx = torch.topk(usage_concat, self.mem_size)\n                kept = usage_concat[idx]\n                mem_x_new = mem_x_concat[idx]\n                mem_usage_new = kept\n                ratio = kept.sum().item() / total\n            else:\n                mem_x_new = mem_x_concat\n                mem_usage_new = usage_concat\n                ratio = 1.0\n        return out, mem_x_new, mem_usage_new, ratio\n\n\nclass UsageTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = UsageMemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_usage):\n        emb = self.embed(x)  # (B,T,E)\n        out, mem_x_new, mem_usage_new, ratio = self.mem_layer(emb, mem_x, mem_usage)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_usage_new, ratio\n\n\n# container\nexperiment_data = {\"usage_based\": {}}\n\n# datasets\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * ((max_len + 1) - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\nfor ds_name, cfg in configs:\n    key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n    print(f\"\\n=== Dataset: {key} ===\")\n    # load\n    train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n    train_samples = list(itertools.islice(train_stream, 200))\n    train_enc = [encode_fn(x) for x in train_samples]\n    train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n    train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n    train_loader = DataLoader(\n        TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n    )\n\n    val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n    val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n    val_samples = list(itertools.islice(val_stream, 100))\n    val_enc = [encode_fn(x) for x in val_samples]\n    val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n    val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n    val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n\n    # init storage\n    experiment_data[\"usage_based\"][key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    # model & optimizer\n    model = UsageTransformerXLModel(vocab_size, embed_dim, num_heads, mem_size).to(\n        device\n    )\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    # train/val\n    for epoch in range(num_epochs):\n        # train\n        model.train()\n        train_loss, train_ratios = 0.0, []\n        for inp, tgt in train_loader:\n            inp, tgt = inp.to(device), tgt.to(device)\n            mem_x = mem_usage = None\n            acc_loss = 0.0\n            chunks = inp.size(1) // chunk_size\n            for i in range(0, inp.size(1), chunk_size):\n                ic = inp[:, i : i + chunk_size]\n                tc = tgt[:, i : i + chunk_size]\n                logits, mem_x, mem_usage, ratio = model(ic, mem_x, mem_usage)\n                loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                acc_loss += loss\n                train_ratios.append(ratio)\n            optimizer.zero_grad()\n            acc_loss.backward()\n            optimizer.step()\n            train_loss += acc_loss.item() / chunks\n        avg_tr_loss = train_loss / len(train_loader)\n        avg_tr_ratio = sum(train_ratios) / len(train_ratios)\n        experiment_data[\"usage_based\"][key][\"losses\"][\"train\"].append(avg_tr_loss)\n        experiment_data[\"usage_based\"][key][\"metrics\"][\"train\"].append(avg_tr_ratio)\n\n        # val\n        model.eval()\n        val_loss, val_ratios = 0.0, []\n        # reset preds/gt on final epoch\n        if epoch == num_epochs - 1:\n            experiment_data[\"usage_based\"][key][\"predictions\"] = []\n            experiment_data[\"usage_based\"][key][\"ground_truth\"] = []\n        with torch.no_grad():\n            for inp, tgt in val_loader:\n                inp, tgt = inp.to(device), tgt.to(device)\n                mem_x = mem_usage = None\n                acc_loss = 0.0\n                chunks = inp.size(1) // chunk_size\n                for i in range(0, inp.size(1), chunk_size):\n                    ic = inp[:, i : i + chunk_size]\n                    tc = tgt[:, i : i + chunk_size]\n                    logits, mem_x, mem_usage, ratio = model(ic, mem_x, mem_usage)\n                    loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                    acc_loss += loss\n                    val_ratios.append(ratio)\n                    if epoch == num_epochs - 1:\n                        preds = logits.argmax(-1)[0].cpu().tolist()\n                        gts = tc[0].cpu().tolist()\n                        experiment_data[\"usage_based\"][key][\"predictions\"].extend(preds)\n                        experiment_data[\"usage_based\"][key][\"ground_truth\"].extend(gts)\n                val_loss += acc_loss.item() / chunks\n        avg_val_loss = val_loss / len(val_loader)\n        avg_val_ratio = sum(val_ratios) / len(val_ratios)\n        experiment_data[\"usage_based\"][key][\"losses\"][\"val\"].append(avg_val_loss)\n        experiment_data[\"usage_based\"][key][\"metrics\"][\"val\"].append(avg_val_ratio)\n        print(\n            f\"{key} Epoch {epoch} \u2192 val_loss {avg_val_loss:.4f}, retention {avg_val_ratio:.4f}\"\n        )\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport itertools\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# hyperparams\nvocab_size = 256\nmax_len = 128\nembed_dim = 32\nnum_heads = 2\nmem_size = 50\nchunk_size = 32\nnum_epochs = 2\nlr = 1e-3\n\n\n# Transformer with optional ReLU\nclass ImprovedMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size, use_activation=True):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        layers = [nn.Linear(embed_dim, 4 * embed_dim)]\n        layers.append(nn.ReLU() if use_activation else nn.Identity())\n        layers.append(nn.Linear(4 * embed_dim, embed_dim))\n        self.ff = nn.Sequential(*layers)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)\n        ent_tok = ent_h[0].max(dim=0)[0]\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n        if mem_x_new.size(0) > self.mem_size:\n            total_ent = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass ImprovedTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size, use_activation=True):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = ImprovedMemoryTransformerLayer(\n            embed_dim, num_heads, mem_size, use_activation\n        )\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# encoding fn\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * (max_len + 1 - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# datasets\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n# ablation setups\nablations = [(\"baseline\", True), (\"feedforward_identity\", False)]\nexperiment_data = {name: {} for name, _ in ablations}\n\nfor ablation_name, use_act in ablations:\n    for ds_name, cfg in configs:\n        ds_key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n        print(f\"\\n=== Ablation {ablation_name}, Dataset {ds_key} ===\")\n        # load train\n        train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n        train_samples = list(itertools.islice(train_stream, 200))\n        train_enc = [encode_fn(x) for x in train_samples]\n        train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n        train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n        train_loader = DataLoader(\n            TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n        )\n        # load val\n        val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n        val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n        val_samples = list(itertools.islice(val_stream, 100))\n        val_enc = [encode_fn(x) for x in val_samples]\n        val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n        val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n        val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n\n        # storage init\n        experiment_data[ablation_name][ds_key] = {\n            \"metrics\": {\n                \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n                \"Entropy-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # model and optimizer\n        model = ImprovedTransformerXLModel(\n            vocab_size, embed_dim, num_heads, mem_size, use_act\n        ).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        criterion = nn.CrossEntropyLoss()\n\n        # training loop\n        for epoch in range(num_epochs):\n            # train\n            model.train()\n            train_loss, tr_ratios, tr_eme = 0.0, [], []\n            for inp, tgt in train_loader:\n                inp, tgt = inp.to(device), tgt.to(device)\n                mem_x = mem_ent = None\n                optimizer.zero_grad()\n                acc_loss = 0.0\n                for i in range(0, inp.size(1), chunk_size):\n                    ic = inp[:, i : i + chunk_size]\n                    tc = tgt[:, i : i + chunk_size]\n                    logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                    loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                    acc_loss += loss\n                    tr_ratios.append(ratio)\n                    tr_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                acc_loss.backward()\n                optimizer.step()\n                train_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n            avg_tr_loss = train_loss / len(train_loader)\n            avg_tr_ratio = sum(tr_ratios) / len(tr_ratios)\n            avg_tr_eme = sum(tr_eme) / len(tr_eme)\n            ed = experiment_data[ablation_name][ds_key]\n            ed[\"losses\"][\"train\"].append(avg_tr_loss)\n            ed[\"metrics\"][\"Memory Retention Ratio\"][\"train\"].append(avg_tr_ratio)\n            ed[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"].append(\n                avg_tr_eme\n            )\n\n            # val\n            model.eval()\n            val_loss, va_ratios, va_eme = 0.0, [], []\n            record_pred = epoch == num_epochs - 1\n            if record_pred:\n                pred_list, gt_list = [], []\n            with torch.no_grad():\n                for inp, tgt in val_loader:\n                    inp, tgt = inp.to(device), tgt.to(device)\n                    mem_x = mem_ent = None\n                    acc_loss = 0.0\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                        acc_loss += criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        va_ratios.append(ratio)\n                        va_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                        if record_pred:\n                            preds = logits.argmax(dim=-1)[0].cpu().tolist()\n                            gts = tc[0].cpu().tolist()\n                            pred_list.extend(preds)\n                            gt_list.extend(gts)\n                    val_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n            avg_val_loss = val_loss / len(val_loader)\n            avg_val_ratio = sum(va_ratios) / len(va_ratios)\n            avg_val_eme = sum(va_eme) / len(va_eme)\n            ed[\"losses\"][\"val\"].append(avg_val_loss)\n            ed[\"metrics\"][\"Memory Retention Ratio\"][\"val\"].append(avg_val_ratio)\n            ed[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"].append(\n                avg_val_eme\n            )\n            if record_pred:\n                ed[\"predictions\"] = pred_list\n                ed[\"ground_truth\"] = gt_list\n            print(\n                f\"{ablation_name} {ds_key} Epoch {epoch}: val_loss={avg_val_loss:.4f}\"\n            )\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport itertools\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\n\n# working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# hyperparams\nvocab_size, max_len = 256, 128\nembed_dim, num_heads, mem_size = 32, 2, 50\nchunk_size, num_epochs, lr = 32, 2, 1e-3\n\n\n# memory layer & model\nclass ImprovedMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], 1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(-1)  # B,heads,T\n        ent_tok = ent_h[0].max(0)[0]  # T\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new, mem_ent_new = x_det, ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], 0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], 0)\n        if mem_x_new.size(0) > self.mem_size:\n            total_ent = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass ImprovedTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = ImprovedMemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# data configs & encoder\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * ((max_len + 1) - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# experiment container\nexperiment_data = {}\n\nfor ablation in [\"baseline\", \"continuous_memory\"]:\n    experiment_data[ablation] = {}\n    for ds_name, cfg in configs:\n        key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n        print(f\"\\n--- Ablation: {ablation}, Dataset: {key} ---\")\n        # load train\n        train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n        train_samples = list(itertools.islice(train_stream, 200))\n        train_enc = [encode_fn(x) for x in train_samples]\n        train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n        train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n        train_loader = DataLoader(\n            TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n        )\n        # load val\n        vsplit = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n        val_stream = load_dataset(ds_name, cfg, split=vsplit, streaming=True)\n        val_samples = list(itertools.islice(val_stream, 100))\n        val_enc = [encode_fn(x) for x in val_samples]\n        val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n        val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n        val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n\n        # model, optimizer, loss\n        model = ImprovedTransformerXLModel(\n            vocab_size, embed_dim, num_heads, mem_size\n        ).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        criterion = nn.CrossEntropyLoss()\n\n        # init storage\n        experiment_data[ablation][key] = {\n            \"metrics\": {\n                \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n                \"Entropy-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # epochs\n        for epoch in range(num_epochs):\n            print(f\"Abl {ablation}, {key} \u2014 Epoch {epoch}\")\n            # TRAIN\n            model.train()\n            train_loss, train_ratios, train_eme = 0.0, [], []\n            if ablation == \"continuous_memory\":\n                mem_x_train, mem_ent_train = None, None\n            for batch in train_loader:\n                inp, tgt = batch[0].to(device), batch[1].to(device)\n                if ablation == \"baseline\":\n                    mem_x, mem_ent = None, None\n                else:\n                    mem_x, mem_ent = mem_x_train, mem_ent_train\n                optimizer.zero_grad()\n                acc_loss = 0.0\n                for i in range(0, inp.size(1), chunk_size):\n                    ic = inp[:, i : i + chunk_size]\n                    tc = tgt[:, i : i + chunk_size]\n                    logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                    loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                    acc_loss += loss\n                    train_ratios.append(ratio)\n                    eme = mem_ent.sum().item() / mem_ent.numel()\n                    train_eme.append(eme)\n                acc_loss.backward()\n                optimizer.step()\n                train_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n                if ablation == \"continuous_memory\":\n                    mem_x_train, mem_ent_train = mem_x, mem_ent\n            avg_tr_loss = train_loss / len(train_loader)\n            avg_tr_ratio = sum(train_ratios) / len(train_ratios)\n            avg_tr_eme = sum(train_eme) / len(train_eme)\n            experiment_data[ablation][key][\"losses\"][\"train\"].append(avg_tr_loss)\n            experiment_data[ablation][key][\"metrics\"][\"Memory Retention Ratio\"][\n                \"train\"\n            ].append(avg_tr_ratio)\n            experiment_data[ablation][key][\"metrics\"][\n                \"Entropy-Weighted Memory Efficiency\"\n            ][\"train\"].append(avg_tr_eme)\n\n            # VAL\n            model.eval()\n            val_loss, val_ratios, val_eme = 0.0, [], []\n            record_preds = epoch == num_epochs - 1\n            if record_preds:\n                val_preds, val_gts = [], []\n            if ablation == \"continuous_memory\":\n                mem_x_val, mem_ent_val = None, None\n            with torch.no_grad():\n                for batch in val_loader:\n                    inp, tgt = batch[0].to(device), batch[1].to(device)\n                    if ablation == \"baseline\":\n                        mem_x, mem_ent = None, None\n                    else:\n                        mem_x, mem_ent = mem_x_val, mem_ent_val\n                    acc_loss = 0.0\n                    seq_p, seq_g = [], []\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                        loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        acc_loss += loss\n                        val_ratios.append(ratio)\n                        eme = mem_ent.sum().item() / mem_ent.numel()\n                        val_eme.append(eme)\n                        if record_preds:\n                            seq_p.extend(logits.argmax(-1).squeeze(0).cpu().tolist())\n                            seq_g.extend(tc.squeeze(0).cpu().tolist())\n                    val_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n                    if record_preds:\n                        val_preds.append(seq_p)\n                        val_gts.append(seq_g)\n                    if ablation == \"continuous_memory\":\n                        mem_x_val, mem_ent_val = mem_x, mem_ent\n            avg_val_loss = val_loss / len(val_loader)\n            avg_val_ratio = sum(val_ratios) / len(val_ratios)\n            avg_val_eme = sum(val_eme) / len(val_eme)\n            experiment_data[ablation][key][\"losses\"][\"val\"].append(avg_val_loss)\n            experiment_data[ablation][key][\"metrics\"][\"Memory Retention Ratio\"][\n                \"val\"\n            ].append(avg_val_ratio)\n            experiment_data[ablation][key][\"metrics\"][\n                \"Entropy-Weighted Memory Efficiency\"\n            ][\"val\"].append(avg_val_eme)\n            print(f\"  val_loss={avg_val_loss:.4f}\")\n\n        # save final preds & gts\n        experiment_data[ablation][key][\"predictions\"] = np.array(val_preds)\n        experiment_data[ablation][key][\"ground_truth\"] = np.array(val_gts)\n\n# save all\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport itertools\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\n\n# Prepare working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Hyperparameters\nvocab_size, max_len, embed_dim = 256, 128, 32\nnum_heads, mem_size, chunk_size = 2, 50, 32\nnum_epochs, lr = 2, 1e-3\n\n\nclass UsageMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_usage):\n        B, T, E = x.size()\n        if mem_x is None:\n            # No prior memory\n            k = v = x\n            prev_slots = 0\n        else:\n            prev_slots = mem_x.size(0)\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        x_det = x.detach()[0]  # (T, E)\n\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_usage_new = torch.zeros(T, device=x.device)\n            ratio = 1.0\n            mem_concat_size = T\n        else:\n            # accumulate usage from attention on old slots\n            w = attn_w[0]  # (heads, T, prev_slots+T)\n            w_mem = w[:, :, :prev_slots]\n            usage_delta = w_mem.sum(dim=(0, 1))  # (prev_slots,)\n            new_usage_existing = mem_usage + usage_delta\n\n            # concatenate old + new slots\n            mem_x_concat = torch.cat([mem_x, x_det], dim=0)\n            usage_concat = torch.cat(\n                [new_usage_existing, torch.zeros(T, device=x.device)], dim=0\n            )\n            mem_concat_size = mem_x_concat.size(0)\n\n            # prune if over capacity\n            if mem_concat_size > self.mem_size:\n                _, idx = torch.topk(usage_concat, self.mem_size)\n                mem_x_new = mem_x_concat[idx]\n                mem_usage_new = usage_concat[idx]\n            else:\n                mem_x_new = mem_x_concat\n                mem_usage_new = usage_concat\n\n            # compute slot\u2010retention ratio\n            ratio = float(mem_x_new.size(0)) / float(mem_concat_size)\n\n        return out, mem_x_new, mem_usage_new, ratio\n\n\nclass UsageTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = UsageMemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_usage):\n        emb = self.embed(x).to(device)\n        out, mem_x_new, mem_usage_new, ratio = self.mem_layer(emb, mem_x, mem_usage)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_usage_new, ratio\n\n\n# storage for metrics, losses, preds, gts\nexperiment_data = {\"usage_based\": {}}\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * ((max_len + 1) - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\nfor ds_name, cfg in configs:\n    key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n    print(f\"\\n=== Dataset: {key} ===\")\n\n    # prepare training data\n    train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n    train_samples = list(itertools.islice(train_stream, 200))\n    train_enc = [encode_fn(x) for x in train_samples]\n    train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n    train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n    train_loader = DataLoader(\n        TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n    )\n\n    # prepare validation data\n    val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n    val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n    val_samples = list(itertools.islice(val_stream, 100))\n    val_enc = [encode_fn(x) for x in val_samples]\n    val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n    val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n    val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n\n    # init storage\n    experiment_data[\"usage_based\"][key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    # model, optimizer, loss\n    model = UsageTransformerXLModel(vocab_size, embed_dim, num_heads, mem_size).to(\n        device\n    )\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    # training & validation loops\n    for epoch in range(num_epochs):\n        # training\n        model.train()\n        train_loss, train_ratios = 0.0, []\n        for inp, tgt in train_loader:\n            inp, tgt = inp.to(device), tgt.to(device)\n            mem_x = mem_usage = None\n            acc_loss = 0.0\n            chunks = inp.size(1) // chunk_size\n            for i in range(0, inp.size(1), chunk_size):\n                ic = inp[:, i : i + chunk_size]\n                tc = tgt[:, i : i + chunk_size]\n                logits, mem_x, mem_usage, ratio = model(ic, mem_x, mem_usage)\n                loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                acc_loss += loss\n                train_ratios.append(ratio)\n            optimizer.zero_grad()\n            acc_loss.backward()\n            optimizer.step()\n            train_loss += acc_loss.item() / chunks\n        avg_tr_loss = train_loss / len(train_loader)\n        avg_tr_ratio = sum(train_ratios) / len(train_ratios)\n        experiment_data[\"usage_based\"][key][\"losses\"][\"train\"].append(avg_tr_loss)\n        experiment_data[\"usage_based\"][key][\"metrics\"][\"train\"].append(avg_tr_ratio)\n\n        # validation\n        model.eval()\n        val_loss, val_ratios = 0.0, []\n        if epoch == num_epochs - 1:\n            experiment_data[\"usage_based\"][key][\"predictions\"] = []\n            experiment_data[\"usage_based\"][key][\"ground_truth\"] = []\n        with torch.no_grad():\n            for inp, tgt in val_loader:\n                inp, tgt = inp.to(device), tgt.to(device)\n                mem_x = mem_usage = None\n                acc_loss = 0.0\n                chunks = inp.size(1) // chunk_size\n                for i in range(0, inp.size(1), chunk_size):\n                    ic = inp[:, i : i + chunk_size]\n                    tc = tgt[:, i : i + chunk_size]\n                    logits, mem_x, mem_usage, ratio = model(ic, mem_x, mem_usage)\n                    loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                    acc_loss += loss\n                    val_ratios.append(ratio)\n                    if epoch == num_epochs - 1:\n                        preds = logits.argmax(-1)[0].cpu().tolist()\n                        gts = tc[0].cpu().tolist()\n                        experiment_data[\"usage_based\"][key][\"predictions\"].extend(preds)\n                        experiment_data[\"usage_based\"][key][\"ground_truth\"].extend(gts)\n                val_loss += acc_loss.item() / chunks\n        avg_val_loss = val_loss / len(val_loader)\n        avg_val_ratio = sum(val_ratios) / len(val_ratios)\n        experiment_data[\"usage_based\"][key][\"losses\"][\"val\"].append(avg_val_loss)\n        experiment_data[\"usage_based\"][key][\"metrics\"][\"val\"].append(avg_val_ratio)\n        print(\n            f\"{key} Epoch {epoch} \u2192 validation_loss = {avg_val_loss:.4f}, retention = {avg_val_ratio:.4f}\"\n        )\n\n# save all metrics and results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os, itertools\nimport torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\nimport numpy as np\n\n# prepare working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# hyperparameters\nvocab_size = 256\nmax_len = 128\nembed_dim = 32\nnum_heads = 2\nmem_size = 50\nchunk_size = 32\nnum_epochs = 2\nlr = 1e-3\n\n\n# Memory layer with head aggregation choice\nclass ImprovedMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size, head_agg=\"max\"):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n        self.head_agg = head_agg\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        # key/value concatenation\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        # attention\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        # per-head entropy\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)  # B, heads, T\n        if self.head_agg == \"max\":\n            ent_tok = ent_h[0].max(dim=0)[0]\n        elif self.head_agg == \"mean\":\n            ent_tok = ent_h[0].mean(dim=0)\n        else:\n            raise ValueError(\"Unknown head_agg\")\n        # detach for memory\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n        # trim memory\n        if mem_x_new.size(0) > self.mem_size:\n            total_ent = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\n# full model\nclass ImprovedTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size, head_agg):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = ImprovedMemoryTransformerLayer(\n            embed_dim, num_heads, mem_size, head_agg\n        )\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, m_x, m_e, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, m_x, m_e, ratio\n\n\n# tokenizer\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * (max_len + 1 - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# dataset configs\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n# orchestrate experiments\nexperiment_data = {}\n\nfor head_agg in [\"max\", \"mean\"]:\n    experiment_data[head_agg] = {}\n    for ds_name, cfg in configs:\n        key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n        print(f\"\\n=== Ablation: {head_agg}, Dataset: {key} ===\")\n        # load train\n        train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n        train_samples = list(itertools.islice(train_stream, 200))\n        train_enc = [encode_fn(x) for x in train_samples]\n        train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n        train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n        train_loader = DataLoader(\n            TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n        )\n        # load val\n        val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n        val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n        val_samples = list(itertools.islice(val_stream, 100))\n        val_enc = [encode_fn(x) for x in val_samples]\n        val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n        val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n        val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n\n        # init storage\n        ds_data = {\n            \"metrics\": {\n                \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n                \"Entropy-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # model, optimizer\n        model = ImprovedTransformerXLModel(\n            vocab_size, embed_dim, num_heads, mem_size, head_agg\n        ).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        criterion = nn.CrossEntropyLoss()\n\n        # train/val\n        for epoch in range(num_epochs):\n            # train\n            model.train()\n            tr_loss, tr_ratios, tr_eme = 0.0, [], []\n            for inp, tgt in train_loader:\n                inp, tgt = inp.to(device), tgt.to(device)\n                mem_x = mem_ent = None\n                optimizer.zero_grad()\n                tot_loss = 0.0\n                for i in range(0, inp.size(1), chunk_size):\n                    ic = inp[:, i : i + chunk_size]\n                    tc = tgt[:, i : i + chunk_size]\n                    logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                    loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                    tot_loss += loss\n                    tr_ratios.append(ratio)\n                    tr_eme.append(mem_ent.sum().item() / (mem_ent.numel() + 1e-10))\n                tot_loss.backward()\n                optimizer.step()\n                tr_loss += tot_loss.item() / (inp.size(1) / chunk_size)\n            avg_tr_loss = tr_loss / len(train_loader)\n            avg_tr_ratio = sum(tr_ratios) / len(tr_ratios)\n            avg_tr_eme = sum(tr_eme) / len(tr_eme)\n            ds_data[\"losses\"][\"train\"].append(avg_tr_loss)\n            ds_data[\"metrics\"][\"Memory Retention Ratio\"][\"train\"].append(avg_tr_ratio)\n            ds_data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"].append(\n                avg_tr_eme\n            )\n\n            # val\n            model.eval()\n            val_loss, val_ratios, val_eme = 0.0, [], []\n            val_preds_epoch, val_gts_epoch = [], []\n            with torch.no_grad():\n                for inp, tgt in val_loader:\n                    inp, tgt = inp.to(device), tgt.to(device)\n                    mem_x = mem_ent = None\n                    seq_preds, seq_gts = [], []\n                    tot_loss = 0.0\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                        tot_loss += criterion(\n                            logits.view(-1, vocab_size), tc.view(-1)\n                        ).item()\n                        val_ratios.append(ratio)\n                        val_eme.append(mem_ent.sum().item() / (mem_ent.numel() + 1e-10))\n                        preds = logits.argmax(dim=-1).squeeze(0).tolist()\n                        gts = tc.squeeze(0).tolist()\n                        seq_preds.extend(preds)\n                        seq_gts.extend(gts)\n                    val_loss += tot_loss / (inp.size(1) / chunk_size)\n                    val_preds_epoch.append(seq_preds)\n                    val_gts_epoch.append(seq_gts)\n            avg_val_loss = val_loss / len(val_loader)\n            avg_val_ratio = sum(val_ratios) / len(val_ratios)\n            avg_val_eme = sum(val_eme) / len(val_eme)\n            ds_data[\"losses\"][\"val\"].append(avg_val_loss)\n            ds_data[\"metrics\"][\"Memory Retention Ratio\"][\"val\"].append(avg_val_ratio)\n            ds_data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"].append(\n                avg_val_eme\n            )\n            # record final predictions\n            if epoch == num_epochs - 1:\n                ds_data[\"predictions\"] = val_preds_epoch\n                ds_data[\"ground_truth\"] = val_gts_epoch\n            print(f\"[{head_agg}][{key}] Epoch {epoch} val_loss={avg_val_loss:.4f}\")\n\n        experiment_data[head_agg][key] = ds_data\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\nimport itertools\nimport numpy as np\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# hyperparameters\nvocab_size = 256\nmax_len = 128\nembed_dim = 32\nnum_heads = 2\nmem_size = 50\nchunk_size = 32\nnum_epochs = 2\nlr = 1e-3\n\n\n# full-memory layer\nclass ImprovedMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)  # B, heads, T\n        ent_tok = ent_h[0].max(dim=0)[0]\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n        if mem_x_new.size(0) > self.mem_size:\n            total_ent = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass ImprovedTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = ImprovedMemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# key-only ablation layer\nclass KeyOnlyMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = x  # only current input as values\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)\n        ent_tok = ent_h[0].max(dim=0)[0]\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n        if mem_x_new.size(0) > self.mem_size:\n            total_ent = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass KeyOnlyTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = KeyOnlyMemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# datasets to run on\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n\n# encode function\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * (max_len + 1 - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# experiment data container\nexperiment_data = {}\n\n# define ablation variants\nablations = {\n    \"baseline\": ImprovedTransformerXLModel,\n    \"memory_key_only\": KeyOnlyTransformerXLModel,\n}\n\nmetric_names = [\"Memory Retention Ratio\", \"Entropy-Weighted Memory Efficiency\"]\n\nfor ablation_name, ModelClass in ablations.items():\n    experiment_data[ablation_name] = {}\n    for ds_name, cfg in configs:\n        ds_key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n        print(f\"\\n=== Ablation: {ablation_name}, Dataset: {ds_key} ===\")\n        # load & encode train\n        train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n        train_samples = list(itertools.islice(train_stream, 200))\n        train_enc = [encode_fn(x) for x in train_samples]\n        train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n        train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n        train_loader = DataLoader(\n            TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n        )\n        # load & encode val\n        val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n        val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n        val_samples = list(itertools.islice(val_stream, 100))\n        val_enc = [encode_fn(x) for x in val_samples]\n        val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n        val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n        val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n\n        # init model, optimizer, loss\n        model = ModelClass(vocab_size, embed_dim, num_heads, mem_size).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        criterion = nn.CrossEntropyLoss()\n\n        # storage for this dataset/ablation\n        data_dict = {\n            \"metrics\": {m: {\"train\": [], \"val\": []} for m in metric_names},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # train & val\n        for epoch in range(num_epochs):\n            # training\n            model.train()\n            train_loss = 0.0\n            train_ratios, train_eme = [], []\n            for inp, tgt in train_loader:\n                inp, tgt = inp.to(device), tgt.to(device)\n                mem_x = mem_ent = None\n                optimizer.zero_grad()\n                acc_loss = 0.0\n                for i in range(0, inp.size(1), chunk_size):\n                    ic = inp[:, i : i + chunk_size]\n                    tc = tgt[:, i : i + chunk_size]\n                    logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                    loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                    acc_loss += loss\n                    train_ratios.append(ratio)\n                    train_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                acc_loss.backward()\n                optimizer.step()\n                train_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n            avg_tr_loss = train_loss / len(train_loader)\n            avg_tr_ratio = sum(train_ratios) / len(train_ratios)\n            avg_tr_eme = sum(train_eme) / len(train_eme)\n            data_dict[\"losses\"][\"train\"].append(avg_tr_loss)\n            data_dict[\"metrics\"][\"Memory Retention Ratio\"][\"train\"].append(avg_tr_ratio)\n            data_dict[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"].append(\n                avg_tr_eme\n            )\n\n            # validation\n            model.eval()\n            val_loss = 0.0\n            val_ratios, val_eme = [], []\n            preds_epoch = []\n            gts_epoch = []\n            with torch.no_grad():\n                for inp, tgt in val_loader:\n                    inp, tgt = inp.to(device), tgt.to(device)\n                    mem_x = mem_ent = None\n                    acc_loss = 0.0\n                    sample_preds = []\n                    sample_gts = []\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                        acc_loss += criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        val_ratios.append(ratio)\n                        val_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                        pred_chunk = logits.argmax(-1).view(-1).cpu().tolist()\n                        sample_preds.extend(pred_chunk)\n                        sample_gts.extend(tc.view(-1).cpu().tolist())\n                    val_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n                    preds_epoch.append(sample_preds)\n                    gts_epoch.append(sample_gts)\n            avg_val_loss = val_loss / len(val_loader)\n            avg_val_ratio = sum(val_ratios) / len(val_ratios)\n            avg_val_eme = sum(val_eme) / len(val_eme)\n            data_dict[\"losses\"][\"val\"].append(avg_val_loss)\n            data_dict[\"metrics\"][\"Memory Retention Ratio\"][\"val\"].append(avg_val_ratio)\n            data_dict[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"].append(\n                avg_val_eme\n            )\n            data_dict[\"predictions\"].append(np.array(preds_epoch))\n            data_dict[\"ground_truth\"].append(np.array(gts_epoch))\n\n            print(f\"Epoch {epoch}: val_loss={avg_val_loss:.4f}\")\n\n        experiment_data[ablation_name][ds_key] = data_dict\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\nimport itertools\nimport numpy as np\n\n# prepare working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# hyperparameters\nvocab_size = 256\nmax_len = 128\nembed_dim = 32\nnum_heads = 2\nmem_size = 50\nchunk_size = 32\nnum_epochs = 2\nlr = 1e-3\n\n\n# no\u2010residual memory transformer layer\nclass ImprovedMemoryTransformerLayerNoRes(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        # remove residuals\n        x2 = self.norm1(attn_out)\n        ff_out = self.ff(x2)\n        out = self.norm2(ff_out)\n\n        # compute entropies\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)\n        ent_tok = ent_h[0].max(dim=0)[0]\n        x_det = x.detach()[0]\n\n        # update memory\n        if mem_x is None:\n            mem_x_new, mem_ent_new = x_det, ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n        if mem_x_new.size(0) > self.mem_size:\n            total_ent = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n\n        return out, mem_x_new, mem_ent_new, ratio\n\n\n# model with no\u2010residual memory layer\nclass ImprovedTransformerXLModelNoRes(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = ImprovedMemoryTransformerLayerNoRes(\n            embed_dim, num_heads, mem_size\n        )\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# simple encoder: char\u2192id\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * (max_len + 1 - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# datasets\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n# initialize storage\nexperiment_data = {\"residual_connection_ablation\": {}}\ncriterion = nn.CrossEntropyLoss()\n\nfor ds_name, cfg in configs:\n    key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n    experiment_data[\"residual_connection_ablation\"][key] = {\n        \"metrics\": {\n            \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n            \"Entropy-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n        },\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    # prepare train\n    train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n    train_samples = list(itertools.islice(train_stream, 200))\n    train_enc = [encode_fn(x) for x in train_samples]\n    train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n    train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n    train_loader = DataLoader(\n        TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n    )\n\n    # prepare val\n    val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n    val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n    val_samples = list(itertools.islice(val_stream, 100))\n    val_enc = [encode_fn(x) for x in val_samples]\n    val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n    val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n    val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n\n    model = ImprovedTransformerXLModelNoRes(\n        vocab_size, embed_dim, num_heads, mem_size\n    ).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    for epoch in range(num_epochs):\n        # training\n        model.train()\n        tr_loss, tr_ratios, tr_eme = 0.0, [], []\n        for inp, tgt in train_loader:\n            inp, tgt = inp.to(device), tgt.to(device)\n            mem_x = mem_ent = None\n            optimizer.zero_grad()\n            acc_loss = 0.0\n            for i in range(0, inp.size(1), chunk_size):\n                ic, tc = inp[:, i : i + chunk_size], tgt[:, i : i + chunk_size]\n                logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                acc_loss += loss\n                tr_ratios.append(ratio)\n                tr_eme.append(mem_ent.sum().item() / mem_ent.numel())\n            acc_loss.backward()\n            optimizer.step()\n            tr_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n        avg_tr_loss = tr_loss / len(train_loader)\n        avg_tr_ratio = sum(tr_ratios) / len(tr_ratios)\n        avg_tr_eme = sum(tr_eme) / len(tr_eme)\n        ed = experiment_data[\"residual_connection_ablation\"][key]\n        ed[\"losses\"][\"train\"].append(avg_tr_loss)\n        ed[\"metrics\"][\"Memory Retention Ratio\"][\"train\"].append(avg_tr_ratio)\n        ed[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"].append(avg_tr_eme)\n\n        # validation + predictions\n        model.eval()\n        val_loss, val_ratios, val_eme = 0.0, [], []\n        preds, gts = [], []\n        with torch.no_grad():\n            for inp, tgt in val_loader:\n                inp, tgt = inp.to(device), tgt.to(device)\n                mem_x = mem_ent = None\n                acc_loss = 0.0\n                for i in range(0, inp.size(1), chunk_size):\n                    ic, tc = inp[:, i : i + chunk_size], tgt[:, i : i + chunk_size]\n                    logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                    acc_loss += criterion(logits.view(-1, vocab_size), tc.view(-1))\n                    val_ratios.append(ratio)\n                    val_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                    preds.append(logits.argmax(dim=-1).cpu().numpy().flatten())\n                    gts.append(tc.cpu().numpy().flatten())\n                val_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n        avg_val_loss = val_loss / len(val_loader)\n        avg_val_ratio = sum(val_ratios) / len(val_ratios)\n        avg_val_eme = sum(val_eme) / len(val_eme)\n        ed[\"losses\"][\"val\"].append(avg_val_loss)\n        ed[\"metrics\"][\"Memory Retention Ratio\"][\"val\"].append(avg_val_ratio)\n        ed[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"].append(avg_val_eme)\n        ed[\"predictions\"] = np.concatenate(preds)\n        ed[\"ground_truth\"] = np.concatenate(gts)\n        print(f\"{key} epoch {epoch}: val_loss={avg_val_loss:.4f}\")\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\nimport itertools\nimport numpy as np\n\n# setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# hyperparams\nvocab_size = 256\nmax_len = 128\nembed_dim = 32\nnum_heads = 2\nmem_size = 50\nchunk_size = 32\nnum_epochs = 2\nlr = 1e-3\n\n\n# entropy\u2010based memory layer\nclass EntropyMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)  # B, heads, T\n        ent_tok = ent_h[0].max(dim=0)[0]  # shape T\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n        if mem_x_new.size(0) > self.mem_size:\n            total = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept = mem_ent_new[idx].sum().item()\n            ratio = kept / total\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass EntropyTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = EntropyMemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# gradient\u2010based memory layer (no pruning inside)\nclass GradMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n\n    def forward(self, x, mem_x):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, _ = self.attn(x, k, v)\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        return out\n\n\nclass GradTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = GradMemoryTransformerLayer(embed_dim, num_heads)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x):\n        emb = self.embed(x)\n        out = self.mem_layer(emb, mem_x)\n        logits = self.out(out)\n        return logits, out\n\n\n# datasets\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * (max_len + 1 - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# prepare experiment_data\nexperiment_data = {\"entropy\": {}, \"grad\": {}}\n\nfor ablation in [\"entropy\", \"grad\"]:\n    for ds_name, cfg in configs:\n        key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n        print(f\"\\n=== Ablation: {ablation} | Dataset: {key} ===\")\n        # setup stats container\n        experiment_data[ablation][key] = {\n            \"metrics\": {\n                \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n                \"Score-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        # load data\n        train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n        train_samples = list(itertools.islice(train_stream, 200))\n        train_enc = [encode_fn(x) for x in train_samples]\n        train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n        train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n        train_loader = DataLoader(\n            TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n        )\n        val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n        val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n        val_samples = list(itertools.islice(val_stream, 100))\n        val_enc = [encode_fn(x) for x in val_samples]\n        val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n        val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n        val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n\n        # init model\n        if ablation == \"entropy\":\n            model = EntropyTransformerXLModel(\n                vocab_size, embed_dim, num_heads, mem_size\n            ).to(device)\n        else:\n            model = GradTransformerXLModel(vocab_size, embed_dim, num_heads).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        criterion = nn.CrossEntropyLoss()\n\n        # training & validation\n        for epoch in range(num_epochs):\n            # train\n            model.train()\n            total_loss = 0.0\n            train_ratios, train_eff = [], []\n            for batch in train_loader:\n                inp, tgt = [b.to(device) for b in batch]\n                if ablation == \"entropy\":\n                    # baseline block\u2010bptt\n                    mem_x = mem_ent = None\n                    optimizer.zero_grad()\n                    acc_loss = 0.0\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                        loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        acc_loss += loss\n                        train_ratios.append(ratio)\n                        eme = mem_ent.sum().item() / mem_ent.numel()\n                        train_eff.append(eme)\n                    acc_loss.backward()\n                    optimizer.step()\n                    total_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n                else:\n                    # gradient\u2010based per\u2010chunk\n                    losses_sample = 0.0\n                    num_chunks = 0\n                    mem_x_all = None\n                    mem_scores_all = None\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        optimizer.zero_grad()\n                        logits, out = model(ic, mem_x_all)\n                        out.retain_grad()\n                        loss_chunk = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        loss_chunk.backward()\n                        # score & prune\n                        grad_norms = torch.norm(out.grad[0], dim=1).detach()\n                        x_det = out.detach()[0]\n                        if mem_x_all is None:\n                            mem_x_all = x_det\n                            mem_scores_all = grad_norms\n                        else:\n                            mem_x_all = torch.cat([mem_x_all, x_det], dim=0)\n                            mem_scores_all = torch.cat(\n                                [mem_scores_all, grad_norms], dim=0\n                            )\n                        if mem_x_all.size(0) > mem_size:\n                            prev = mem_scores_all.sum().item() + 1e-10\n                            _, idx = torch.topk(mem_scores_all, mem_size)\n                            mem_x_all = mem_x_all[idx]\n                            mem_scores_all = mem_scores_all[idx]\n                            ratio = mem_scores_all.sum().item() / prev\n                        else:\n                            ratio = 1.0\n                        train_ratios.append(ratio)\n                        train_eff.append(\n                            mem_scores_all.sum().item() / mem_scores_all.numel()\n                        )\n                        optimizer.step()\n                        losses_sample += loss_chunk.item()\n                        num_chunks += 1\n                    total_loss += losses_sample / max(1, num_chunks)\n            avg_tr_loss = total_loss / len(train_loader)\n            avg_tr_ratio = sum(train_ratios) / len(train_ratios)\n            avg_tr_eff = sum(train_eff) / len(train_eff)\n            ed = experiment_data[ablation][key]\n            ed[\"losses\"][\"train\"].append(avg_tr_loss)\n            ed[\"metrics\"][\"Memory Retention Ratio\"][\"train\"].append(avg_tr_ratio)\n            ed[\"metrics\"][\"Score-Weighted Memory Efficiency\"][\"train\"].append(\n                avg_tr_eff\n            )\n\n            # val\n            model.eval()\n            val_loss_sum = 0.0\n            val_ratios, val_eff = [], []\n            with torch.set_grad_enabled(ablation == \"grad\"):\n                for batch in val_loader:\n                    inp, tgt = [b.to(device) for b in batch]\n                    losses_sample = 0.0\n                    num_chunks = 0\n                    mem_x_all = None\n                    mem_scores_all = None\n                    sample_preds, sample_gts = [], []\n                    # choose whether we allow grad\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        if ablation == \"entropy\":\n                            with torch.no_grad():\n                                logits, mem_x_all, mem_scores_all, ratio = model(\n                                    ic, mem_x_all, mem_scores_all\n                                )\n                        else:\n                            optimizer.zero_grad()\n                            logits, out = model(ic, mem_x_all)\n                            out.retain_grad()\n                            loss_chunk = criterion(\n                                logits.view(-1, vocab_size), tc.view(-1)\n                            )\n                            loss_chunk.backward()\n                            grad_norms = torch.norm(out.grad[0], dim=1).detach()\n                            x_det = out.detach()[0]\n                            if mem_x_all is None:\n                                mem_x_all = x_det\n                                mem_scores_all = grad_norms\n                            else:\n                                mem_x_all = torch.cat([mem_x_all, x_det], dim=0)\n                                mem_scores_all = torch.cat(\n                                    [mem_scores_all, grad_norms], dim=0\n                                )\n                            if mem_x_all.size(0) > mem_size:\n                                prev = mem_scores_all.sum().item() + 1e-10\n                                _, idx = torch.topk(mem_scores_all, mem_size)\n                                mem_x_all = mem_x_all[idx]\n                                mem_scores_all = mem_scores_all[idx]\n                                ratio = mem_scores_all.sum().item() / prev\n                            else:\n                                ratio = 1.0\n                        # collect preds/gts\n                        pred_toks = logits.argmax(dim=-1)[0].cpu().tolist()\n                        sample_preds.extend(pred_toks)\n                        sample_gts.extend(tc[0].cpu().tolist())\n                        # loss accounting\n                        if ablation == \"entropy\":\n                            loss_chunk = criterion(\n                                logits.view(-1, vocab_size), tc.view(-1)\n                            )\n                        losses_sample += loss_chunk.item()\n                        num_chunks += 1\n                        val_ratios.append(ratio)\n                        if mem_scores_all is not None:\n                            val_eff.append(\n                                mem_scores_all.sum().item() / mem_scores_all.numel()\n                            )\n                    val_loss_sum += losses_sample / max(1, num_chunks)\n                    experiment_data[ablation][key][\"predictions\"].append(sample_preds)\n                    experiment_data[ablation][key][\"ground_truth\"].append(sample_gts)\n            avg_val_loss = val_loss_sum / len(val_loader)\n            avg_val_ratio = sum(val_ratios) / len(val_ratios)\n            avg_val_eff = sum(val_eff) / len(val_eff) if val_eff else 0.0\n            ed[\"losses\"][\"val\"].append(avg_val_loss)\n            ed[\"metrics\"][\"Memory Retention Ratio\"][\"val\"].append(avg_val_ratio)\n            ed[\"metrics\"][\"Score-Weighted Memory Efficiency\"][\"val\"].append(avg_val_eff)\n            print(\n                f\"Ablation {ablation} | {key} | Epoch {epoch}: val_loss={avg_val_loss:.4f}\"\n            )\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\nimport itertools\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# hyperparameters\nvocab_size = 256\nmax_len = 128\nembed_dim = 32\nnum_heads = 2\nmem_size = 50\nchunk_size = 32\nnum_epochs = 2\nlr = 1e-3\n\n\nclass ImprovedMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = torch.cat([mem, x], dim=1)\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)  # B, heads, T\n        ent_tok = ent_h[0].max(dim=0)[0]\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n        if mem_x_new.size(0) > self.mem_size:\n            total_ent = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass KeyOnlyMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            pad = torch.zeros_like(mem)\n            v = torch.cat([pad, x], dim=1)\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)\n        ent_tok = ent_h[0].max(dim=0)[0]\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n        if mem_x_new.size(0) > self.mem_size:\n            total_ent = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass ImprovedTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = ImprovedMemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\nclass KeyOnlyTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = KeyOnlyMemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * (max_len + 1 - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\nexperiment_data = {}\nablations = {\n    \"baseline\": ImprovedTransformerXLModel,\n    \"memory_key_only\": KeyOnlyTransformerXLModel,\n}\nmetric_names = [\"Entropy Retention Ratio\", \"Entropy-Weighted Memory Efficiency\"]\n\nfor ablation_name, ModelClass in ablations.items():\n    experiment_data[ablation_name] = {}\n    for ds_name, cfg in configs:\n        ds_key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n        print(f\"\\n=== Ablation: {ablation_name}, Dataset: {ds_key} ===\")\n        train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n        train_samples = list(itertools.islice(train_stream, 200))\n        train_enc = [encode_fn(x) for x in train_samples]\n        train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n        train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n        train_loader = DataLoader(\n            TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n        )\n\n        val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n        val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n        val_samples = list(itertools.islice(val_stream, 100))\n        val_enc = [encode_fn(x) for x in val_samples]\n        val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n        val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n        val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n\n        model = ModelClass(vocab_size, embed_dim, num_heads, mem_size).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        criterion = nn.CrossEntropyLoss()\n\n        data_dict = {\n            \"metrics\": {m: {\"train\": [], \"val\": []} for m in metric_names},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        for epoch in range(num_epochs):\n            model.train()\n            train_loss = 0.0\n            train_ratios, train_eme = [], []\n            for inp, tgt in train_loader:\n                inp, tgt = inp.to(device), tgt.to(device)\n                mem_x = mem_ent = None\n                optimizer.zero_grad()\n                acc_loss = 0.0\n                for i in range(0, inp.size(1), chunk_size):\n                    ic = inp[:, i : i + chunk_size]\n                    tc = tgt[:, i : i + chunk_size]\n                    logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                    loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                    acc_loss += loss\n                    train_ratios.append(ratio)\n                    train_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                acc_loss.backward()\n                optimizer.step()\n                train_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n            avg_tr_loss = train_loss / len(train_loader)\n            avg_tr_ratio = sum(train_ratios) / len(train_ratios)\n            avg_tr_eme = sum(train_eme) / len(train_eme)\n            data_dict[\"losses\"][\"train\"].append(avg_tr_loss)\n            data_dict[\"metrics\"][\"Entropy Retention Ratio\"][\"train\"].append(\n                avg_tr_ratio\n            )\n            data_dict[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"].append(\n                avg_tr_eme\n            )\n\n            model.eval()\n            val_loss = 0.0\n            val_ratios, val_eme = [], []\n            preds_epoch, gts_epoch = [], []\n            with torch.no_grad():\n                for inp, tgt in val_loader:\n                    inp, tgt = inp.to(device), tgt.to(device)\n                    mem_x = mem_ent = None\n                    acc_loss = 0.0\n                    sample_preds, sample_gts = [], []\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                        acc_loss += criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        val_ratios.append(ratio)\n                        val_eme.append(mem_ent.sum().item() / mem_ent.numel())\n                        sample_preds.extend(logits.argmax(-1).view(-1).cpu().tolist())\n                        sample_gts.extend(tc.view(-1).cpu().tolist())\n                    val_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n                    preds_epoch.append(sample_preds)\n                    gts_epoch.append(sample_gts)\n            avg_val_loss = val_loss / len(val_loader)\n            avg_val_ratio = sum(val_ratios) / len(val_ratios)\n            avg_val_eme = sum(val_eme) / len(val_eme)\n            data_dict[\"losses\"][\"val\"].append(avg_val_loss)\n            data_dict[\"metrics\"][\"Entropy Retention Ratio\"][\"val\"].append(avg_val_ratio)\n            data_dict[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"].append(\n                avg_val_eme\n            )\n            data_dict[\"predictions\"].append(np.array(preds_epoch))\n            data_dict[\"ground_truth\"].append(np.array(gts_epoch))\n\n            print(f\"Epoch {epoch}: validation_loss = {avg_val_loss:.4f}\")\n\n        experiment_data[ablation_name][ds_key] = data_dict\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\nimport itertools\nimport numpy as np\n\n# setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# hyperparams\nvocab_size = 256\nmax_len = 128\nembed_dim = 32\nnum_heads = 2\nmem_size = 50\nchunk_size = 32\nnum_epochs = 2\nlr = 1e-3\n\n\n# entropy\u2010based memory layer\nclass EntropyMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)  # B, heads, T\n        ent_tok = ent_h[0].max(dim=0)[0]  # shape T\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n        if mem_x_new.size(0) > self.mem_size:\n            total = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept = mem_ent_new[idx].sum().item()\n            ratio = kept / total\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass EntropyTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = EntropyMemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# gradient\u2010based memory layer (no pruning inside)\nclass GradMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n\n    def forward(self, x, mem_x):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, _ = self.attn(x, k, v)\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        return out\n\n\nclass GradTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = GradMemoryTransformerLayer(embed_dim, num_heads)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x):\n        emb = self.embed(x)\n        out = self.mem_layer(emb, mem_x)\n        logits = self.out(out)\n        return logits, out\n\n\n# datasets\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * (max_len + 1 - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# prepare experiment_data\nexperiment_data = {\"entropy\": {}, \"grad\": {}}\n\nfor ablation in [\"entropy\", \"grad\"]:\n    for ds_name, cfg in configs:\n        key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n        print(f\"\\n=== Ablation: {ablation} | Dataset: {key} ===\")\n        # setup stats container\n        experiment_data[ablation][key] = {\n            \"metrics\": {\n                \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n                \"Score-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        # load data\n        train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n        train_samples = list(itertools.islice(train_stream, 200))\n        train_enc = [encode_fn(x) for x in train_samples]\n        train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n        train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n        train_loader = DataLoader(\n            TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n        )\n        val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n        val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n        val_samples = list(itertools.islice(val_stream, 100))\n        val_enc = [encode_fn(x) for x in val_samples]\n        val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n        val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n        val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n\n        # init model\n        if ablation == \"entropy\":\n            model = EntropyTransformerXLModel(\n                vocab_size, embed_dim, num_heads, mem_size\n            ).to(device)\n        else:\n            model = GradTransformerXLModel(vocab_size, embed_dim, num_heads).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        criterion = nn.CrossEntropyLoss()\n\n        # training & validation\n        for epoch in range(num_epochs):\n            # train\n            model.train()\n            total_loss = 0.0\n            train_ratios, train_eff = [], []\n            for batch in train_loader:\n                inp, tgt = [b.to(device) for b in batch]\n                if ablation == \"entropy\":\n                    # baseline block\u2010bptt\n                    mem_x = mem_ent = None\n                    optimizer.zero_grad()\n                    acc_loss = 0.0\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                        loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        acc_loss += loss\n                        train_ratios.append(ratio)\n                        eme = mem_ent.sum().item() / mem_ent.numel()\n                        train_eff.append(eme)\n                    acc_loss.backward()\n                    optimizer.step()\n                    total_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n                else:\n                    # gradient\u2010based per\u2010chunk\n                    losses_sample = 0.0\n                    num_chunks = 0\n                    mem_x_all = None\n                    mem_scores_all = None\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        optimizer.zero_grad()\n                        logits, out = model(ic, mem_x_all)\n                        out.retain_grad()\n                        loss_chunk = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        loss_chunk.backward()\n                        # score & prune\n                        grad_norms = torch.norm(out.grad[0], dim=1).detach()\n                        x_det = out.detach()[0]\n                        if mem_x_all is None:\n                            mem_x_all = x_det\n                            mem_scores_all = grad_norms\n                        else:\n                            mem_x_all = torch.cat([mem_x_all, x_det], dim=0)\n                            mem_scores_all = torch.cat(\n                                [mem_scores_all, grad_norms], dim=0\n                            )\n                        if mem_x_all.size(0) > mem_size:\n                            prev = mem_scores_all.sum().item() + 1e-10\n                            _, idx = torch.topk(mem_scores_all, mem_size)\n                            mem_x_all = mem_x_all[idx]\n                            mem_scores_all = mem_scores_all[idx]\n                            ratio = mem_scores_all.sum().item() / prev\n                        else:\n                            ratio = 1.0\n                        train_ratios.append(ratio)\n                        train_eff.append(\n                            mem_scores_all.sum().item() / mem_scores_all.numel()\n                        )\n                        optimizer.step()\n                        losses_sample += loss_chunk.item()\n                        num_chunks += 1\n                    total_loss += losses_sample / max(1, num_chunks)\n            avg_tr_loss = total_loss / len(train_loader)\n            avg_tr_ratio = sum(train_ratios) / len(train_ratios)\n            avg_tr_eff = sum(train_eff) / len(train_eff)\n            ed = experiment_data[ablation][key]\n            ed[\"losses\"][\"train\"].append(avg_tr_loss)\n            ed[\"metrics\"][\"Memory Retention Ratio\"][\"train\"].append(avg_tr_ratio)\n            ed[\"metrics\"][\"Score-Weighted Memory Efficiency\"][\"train\"].append(\n                avg_tr_eff\n            )\n\n            # val\n            model.eval()\n            val_loss_sum = 0.0\n            val_ratios, val_eff = [], []\n            with torch.set_grad_enabled(ablation == \"grad\"):\n                for batch in val_loader:\n                    inp, tgt = [b.to(device) for b in batch]\n                    losses_sample = 0.0\n                    num_chunks = 0\n                    mem_x_all = None\n                    mem_scores_all = None\n                    sample_preds, sample_gts = [], []\n                    # choose whether we allow grad\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        if ablation == \"entropy\":\n                            with torch.no_grad():\n                                logits, mem_x_all, mem_scores_all, ratio = model(\n                                    ic, mem_x_all, mem_scores_all\n                                )\n                        else:\n                            optimizer.zero_grad()\n                            logits, out = model(ic, mem_x_all)\n                            out.retain_grad()\n                            loss_chunk = criterion(\n                                logits.view(-1, vocab_size), tc.view(-1)\n                            )\n                            loss_chunk.backward()\n                            grad_norms = torch.norm(out.grad[0], dim=1).detach()\n                            x_det = out.detach()[0]\n                            if mem_x_all is None:\n                                mem_x_all = x_det\n                                mem_scores_all = grad_norms\n                            else:\n                                mem_x_all = torch.cat([mem_x_all, x_det], dim=0)\n                                mem_scores_all = torch.cat(\n                                    [mem_scores_all, grad_norms], dim=0\n                                )\n                            if mem_x_all.size(0) > mem_size:\n                                prev = mem_scores_all.sum().item() + 1e-10\n                                _, idx = torch.topk(mem_scores_all, mem_size)\n                                mem_x_all = mem_x_all[idx]\n                                mem_scores_all = mem_scores_all[idx]\n                                ratio = mem_scores_all.sum().item() / prev\n                            else:\n                                ratio = 1.0\n                        # collect preds/gts\n                        pred_toks = logits.argmax(dim=-1)[0].cpu().tolist()\n                        sample_preds.extend(pred_toks)\n                        sample_gts.extend(tc[0].cpu().tolist())\n                        # loss accounting\n                        if ablation == \"entropy\":\n                            loss_chunk = criterion(\n                                logits.view(-1, vocab_size), tc.view(-1)\n                            )\n                        losses_sample += loss_chunk.item()\n                        num_chunks += 1\n                        val_ratios.append(ratio)\n                        if mem_scores_all is not None:\n                            val_eff.append(\n                                mem_scores_all.sum().item() / mem_scores_all.numel()\n                            )\n                    val_loss_sum += losses_sample / max(1, num_chunks)\n                    experiment_data[ablation][key][\"predictions\"].append(sample_preds)\n                    experiment_data[ablation][key][\"ground_truth\"].append(sample_gts)\n            avg_val_loss = val_loss_sum / len(val_loader)\n            avg_val_ratio = sum(val_ratios) / len(val_ratios)\n            avg_val_eff = sum(val_eff) / len(val_eff) if val_eff else 0.0\n            ed[\"losses\"][\"val\"].append(avg_val_loss)\n            ed[\"metrics\"][\"Memory Retention Ratio\"][\"val\"].append(avg_val_ratio)\n            ed[\"metrics\"][\"Score-Weighted Memory Efficiency\"][\"val\"].append(avg_val_eff)\n            print(\n                f\"Ablation {ablation} | {key} | Epoch {epoch}: val_loss={avg_val_loss:.4f}\"\n            )\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\nimport itertools\nimport numpy as np\n\n# setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# hyperparams\nvocab_size = 256\nmax_len = 128\nembed_dim = 32\nnum_heads = 2\nmem_size = 50\nchunk_size = 32\nnum_epochs = 2\nlr = 1e-3\n\n\n# entropy\u2010based memory layer\nclass EntropyMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)  # B, heads, T\n        ent_tok = ent_h[0].max(dim=0)[0]  # shape T\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n        if mem_x_new.size(0) > self.mem_size:\n            total = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept = mem_ent_new[idx].sum().item()\n            ratio = kept / total\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass EntropyTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = EntropyMemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# gradient\u2010based memory layer (no pruning inside)\nclass GradMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n\n    def forward(self, x, mem_x):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, _ = self.attn(x, k, v)\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        return out\n\n\nclass GradTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = GradMemoryTransformerLayer(embed_dim, num_heads)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x):\n        emb = self.embed(x)\n        out = self.mem_layer(emb, mem_x)\n        logits = self.out(out)\n        return logits, out\n\n\n# datasets\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * (max_len + 1 - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# prepare experiment_data\nexperiment_data = {\"entropy\": {}, \"grad\": {}}\n\nfor ablation in [\"entropy\", \"grad\"]:\n    for ds_name, cfg in configs:\n        key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n        print(f\"\\n=== Ablation: {ablation} | Dataset: {key} ===\")\n        # setup stats container\n        experiment_data[ablation][key] = {\n            \"metrics\": {\n                \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n                \"Score-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        # load data\n        train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n        train_samples = list(itertools.islice(train_stream, 200))\n        train_enc = [encode_fn(x) for x in train_samples]\n        train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n        train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n        train_loader = DataLoader(\n            TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n        )\n        val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n        val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n        val_samples = list(itertools.islice(val_stream, 100))\n        val_enc = [encode_fn(x) for x in val_samples]\n        val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n        val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n        val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n\n        # init model\n        if ablation == \"entropy\":\n            model = EntropyTransformerXLModel(\n                vocab_size, embed_dim, num_heads, mem_size\n            ).to(device)\n        else:\n            model = GradTransformerXLModel(vocab_size, embed_dim, num_heads).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        criterion = nn.CrossEntropyLoss()\n\n        # training & validation\n        for epoch in range(num_epochs):\n            # train\n            model.train()\n            total_loss = 0.0\n            train_ratios, train_eff = [], []\n            for batch in train_loader:\n                inp, tgt = [b.to(device) for b in batch]\n                if ablation == \"entropy\":\n                    # baseline block\u2010bptt\n                    mem_x = mem_ent = None\n                    optimizer.zero_grad()\n                    acc_loss = 0.0\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                        loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        acc_loss += loss\n                        train_ratios.append(ratio)\n                        eme = mem_ent.sum().item() / mem_ent.numel()\n                        train_eff.append(eme)\n                    acc_loss.backward()\n                    optimizer.step()\n                    total_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n                else:\n                    # gradient\u2010based per\u2010chunk\n                    losses_sample = 0.0\n                    num_chunks = 0\n                    mem_x_all = None\n                    mem_scores_all = None\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        optimizer.zero_grad()\n                        logits, out = model(ic, mem_x_all)\n                        out.retain_grad()\n                        loss_chunk = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        loss_chunk.backward()\n                        # score & prune\n                        grad_norms = torch.norm(out.grad[0], dim=1).detach()\n                        x_det = out.detach()[0]\n                        if mem_x_all is None:\n                            mem_x_all = x_det\n                            mem_scores_all = grad_norms\n                        else:\n                            mem_x_all = torch.cat([mem_x_all, x_det], dim=0)\n                            mem_scores_all = torch.cat(\n                                [mem_scores_all, grad_norms], dim=0\n                            )\n                        if mem_x_all.size(0) > mem_size:\n                            prev = mem_scores_all.sum().item() + 1e-10\n                            _, idx = torch.topk(mem_scores_all, mem_size)\n                            mem_x_all = mem_x_all[idx]\n                            mem_scores_all = mem_scores_all[idx]\n                            ratio = mem_scores_all.sum().item() / prev\n                        else:\n                            ratio = 1.0\n                        train_ratios.append(ratio)\n                        train_eff.append(\n                            mem_scores_all.sum().item() / mem_scores_all.numel()\n                        )\n                        optimizer.step()\n                        losses_sample += loss_chunk.item()\n                        num_chunks += 1\n                    total_loss += losses_sample / max(1, num_chunks)\n            avg_tr_loss = total_loss / len(train_loader)\n            avg_tr_ratio = sum(train_ratios) / len(train_ratios)\n            avg_tr_eff = sum(train_eff) / len(train_eff)\n            ed = experiment_data[ablation][key]\n            ed[\"losses\"][\"train\"].append(avg_tr_loss)\n            ed[\"metrics\"][\"Memory Retention Ratio\"][\"train\"].append(avg_tr_ratio)\n            ed[\"metrics\"][\"Score-Weighted Memory Efficiency\"][\"train\"].append(\n                avg_tr_eff\n            )\n\n            # val\n            model.eval()\n            val_loss_sum = 0.0\n            val_ratios, val_eff = [], []\n            with torch.set_grad_enabled(ablation == \"grad\"):\n                for batch in val_loader:\n                    inp, tgt = [b.to(device) for b in batch]\n                    losses_sample = 0.0\n                    num_chunks = 0\n                    mem_x_all = None\n                    mem_scores_all = None\n                    sample_preds, sample_gts = [], []\n                    # choose whether we allow grad\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        if ablation == \"entropy\":\n                            with torch.no_grad():\n                                logits, mem_x_all, mem_scores_all, ratio = model(\n                                    ic, mem_x_all, mem_scores_all\n                                )\n                        else:\n                            optimizer.zero_grad()\n                            logits, out = model(ic, mem_x_all)\n                            out.retain_grad()\n                            loss_chunk = criterion(\n                                logits.view(-1, vocab_size), tc.view(-1)\n                            )\n                            loss_chunk.backward()\n                            grad_norms = torch.norm(out.grad[0], dim=1).detach()\n                            x_det = out.detach()[0]\n                            if mem_x_all is None:\n                                mem_x_all = x_det\n                                mem_scores_all = grad_norms\n                            else:\n                                mem_x_all = torch.cat([mem_x_all, x_det], dim=0)\n                                mem_scores_all = torch.cat(\n                                    [mem_scores_all, grad_norms], dim=0\n                                )\n                            if mem_x_all.size(0) > mem_size:\n                                prev = mem_scores_all.sum().item() + 1e-10\n                                _, idx = torch.topk(mem_scores_all, mem_size)\n                                mem_x_all = mem_x_all[idx]\n                                mem_scores_all = mem_scores_all[idx]\n                                ratio = mem_scores_all.sum().item() / prev\n                            else:\n                                ratio = 1.0\n                        # collect preds/gts\n                        pred_toks = logits.argmax(dim=-1)[0].cpu().tolist()\n                        sample_preds.extend(pred_toks)\n                        sample_gts.extend(tc[0].cpu().tolist())\n                        # loss accounting\n                        if ablation == \"entropy\":\n                            loss_chunk = criterion(\n                                logits.view(-1, vocab_size), tc.view(-1)\n                            )\n                        losses_sample += loss_chunk.item()\n                        num_chunks += 1\n                        val_ratios.append(ratio)\n                        if mem_scores_all is not None:\n                            val_eff.append(\n                                mem_scores_all.sum().item() / mem_scores_all.numel()\n                            )\n                    val_loss_sum += losses_sample / max(1, num_chunks)\n                    experiment_data[ablation][key][\"predictions\"].append(sample_preds)\n                    experiment_data[ablation][key][\"ground_truth\"].append(sample_gts)\n            avg_val_loss = val_loss_sum / len(val_loader)\n            avg_val_ratio = sum(val_ratios) / len(val_ratios)\n            avg_val_eff = sum(val_eff) / len(val_eff) if val_eff else 0.0\n            ed[\"losses\"][\"val\"].append(avg_val_loss)\n            ed[\"metrics\"][\"Memory Retention Ratio\"][\"val\"].append(avg_val_ratio)\n            ed[\"metrics\"][\"Score-Weighted Memory Efficiency\"][\"val\"].append(avg_val_eff)\n            print(\n                f\"Ablation {ablation} | {key} | Epoch {epoch}: val_loss={avg_val_loss:.4f}\"\n            )\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\nimport itertools\nimport numpy as np\n\n# setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# hyperparams\nvocab_size = 256\nmax_len = 128\nembed_dim = 32\nnum_heads = 2\nmem_size = 50\nchunk_size = 32\nnum_epochs = 2\nlr = 1e-3\n\n\n# entropy\u2010based memory layer\nclass EntropyMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        eps = 1e-10\n        ent_h = -(attn_w * (attn_w + eps).log()).sum(dim=-1)  # B, heads, T\n        ent_tok = ent_h[0].max(dim=0)[0]  # shape T\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent_tok\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent_tok], dim=0)\n        if mem_x_new.size(0) > self.mem_size:\n            total = mem_ent_new.sum().item() + eps\n            _, idx = torch.topk(mem_ent_new, self.mem_size)\n            kept = mem_ent_new[idx].sum().item()\n            ratio = kept / total\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = 1.0\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass EntropyTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = EntropyMemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# gradient\u2010based memory layer (no pruning inside)\nclass GradMemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n\n    def forward(self, x, mem_x):\n        B, T, E = x.size()\n        if mem_x is None:\n            k = v = x\n        else:\n            mem = mem_x.unsqueeze(0).expand(B, -1, -1)\n            k = torch.cat([mem, x], dim=1)\n            v = k\n        attn_out, _ = self.attn(x, k, v)\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        return out\n\n\nclass GradTransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = GradMemoryTransformerLayer(embed_dim, num_heads)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x):\n        emb = self.embed(x)\n        out = self.mem_layer(emb, mem_x)\n        logits = self.out(out)\n        return logits, out\n\n\n# datasets\nconfigs = [\n    (\"pg19\", None),\n    (\"scientific_papers\", \"arxiv\"),\n    (\"wikitext\", \"wikitext-2-raw-v1\"),\n]\n\n\ndef encode_fn(example):\n    txt = example.get(\"text\") or example.get(\"abstract\", \"\")\n    txt = txt[: max_len + 1]\n    ids = [ord(c) % vocab_size for c in txt]\n    if len(ids) < max_len + 1:\n        ids += [0] * (max_len + 1 - len(ids))\n    return {\"input\": ids[:-1], \"target\": ids[1:]}\n\n\n# prepare experiment_data\nexperiment_data = {\"entropy\": {}, \"grad\": {}}\n\nfor ablation in [\"entropy\", \"grad\"]:\n    for ds_name, cfg in configs:\n        key = ds_name if cfg is None else f\"{ds_name}_{cfg}\"\n        print(f\"\\n=== Ablation: {ablation} | Dataset: {key} ===\")\n        # setup stats container\n        experiment_data[ablation][key] = {\n            \"metrics\": {\n                \"Memory Retention Ratio\": {\"train\": [], \"val\": []},\n                \"Score-Weighted Memory Efficiency\": {\"train\": [], \"val\": []},\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        # load data\n        train_stream = load_dataset(ds_name, cfg, split=\"train\", streaming=True)\n        train_samples = list(itertools.islice(train_stream, 200))\n        train_enc = [encode_fn(x) for x in train_samples]\n        train_inputs = torch.tensor([d[\"input\"] for d in train_enc], dtype=torch.long)\n        train_targets = torch.tensor([d[\"target\"] for d in train_enc], dtype=torch.long)\n        train_loader = DataLoader(\n            TensorDataset(train_inputs, train_targets), batch_size=1, shuffle=True\n        )\n        val_split = \"validation\" if ds_name != \"scientific_papers\" else \"test\"\n        val_stream = load_dataset(ds_name, cfg, split=val_split, streaming=True)\n        val_samples = list(itertools.islice(val_stream, 100))\n        val_enc = [encode_fn(x) for x in val_samples]\n        val_inputs = torch.tensor([d[\"input\"] for d in val_enc], dtype=torch.long)\n        val_targets = torch.tensor([d[\"target\"] for d in val_enc], dtype=torch.long)\n        val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=1)\n\n        # init model\n        if ablation == \"entropy\":\n            model = EntropyTransformerXLModel(\n                vocab_size, embed_dim, num_heads, mem_size\n            ).to(device)\n        else:\n            model = GradTransformerXLModel(vocab_size, embed_dim, num_heads).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        criterion = nn.CrossEntropyLoss()\n\n        # training & validation\n        for epoch in range(num_epochs):\n            # train\n            model.train()\n            total_loss = 0.0\n            train_ratios, train_eff = [], []\n            for batch in train_loader:\n                inp, tgt = [b.to(device) for b in batch]\n                if ablation == \"entropy\":\n                    # baseline block\u2010bptt\n                    mem_x = mem_ent = None\n                    optimizer.zero_grad()\n                    acc_loss = 0.0\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                        loss = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        acc_loss += loss\n                        train_ratios.append(ratio)\n                        eme = mem_ent.sum().item() / mem_ent.numel()\n                        train_eff.append(eme)\n                    acc_loss.backward()\n                    optimizer.step()\n                    total_loss += acc_loss.item() / (inp.size(1) / chunk_size)\n                else:\n                    # gradient\u2010based per\u2010chunk\n                    losses_sample = 0.0\n                    num_chunks = 0\n                    mem_x_all = None\n                    mem_scores_all = None\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        optimizer.zero_grad()\n                        logits, out = model(ic, mem_x_all)\n                        out.retain_grad()\n                        loss_chunk = criterion(logits.view(-1, vocab_size), tc.view(-1))\n                        loss_chunk.backward()\n                        # score & prune\n                        grad_norms = torch.norm(out.grad[0], dim=1).detach()\n                        x_det = out.detach()[0]\n                        if mem_x_all is None:\n                            mem_x_all = x_det\n                            mem_scores_all = grad_norms\n                        else:\n                            mem_x_all = torch.cat([mem_x_all, x_det], dim=0)\n                            mem_scores_all = torch.cat(\n                                [mem_scores_all, grad_norms], dim=0\n                            )\n                        if mem_x_all.size(0) > mem_size:\n                            prev = mem_scores_all.sum().item() + 1e-10\n                            _, idx = torch.topk(mem_scores_all, mem_size)\n                            mem_x_all = mem_x_all[idx]\n                            mem_scores_all = mem_scores_all[idx]\n                            ratio = mem_scores_all.sum().item() / prev\n                        else:\n                            ratio = 1.0\n                        train_ratios.append(ratio)\n                        train_eff.append(\n                            mem_scores_all.sum().item() / mem_scores_all.numel()\n                        )\n                        optimizer.step()\n                        losses_sample += loss_chunk.item()\n                        num_chunks += 1\n                    total_loss += losses_sample / max(1, num_chunks)\n            avg_tr_loss = total_loss / len(train_loader)\n            avg_tr_ratio = sum(train_ratios) / len(train_ratios)\n            avg_tr_eff = sum(train_eff) / len(train_eff)\n            ed = experiment_data[ablation][key]\n            ed[\"losses\"][\"train\"].append(avg_tr_loss)\n            ed[\"metrics\"][\"Memory Retention Ratio\"][\"train\"].append(avg_tr_ratio)\n            ed[\"metrics\"][\"Score-Weighted Memory Efficiency\"][\"train\"].append(\n                avg_tr_eff\n            )\n\n            # val\n            model.eval()\n            val_loss_sum = 0.0\n            val_ratios, val_eff = [], []\n            with torch.set_grad_enabled(ablation == \"grad\"):\n                for batch in val_loader:\n                    inp, tgt = [b.to(device) for b in batch]\n                    losses_sample = 0.0\n                    num_chunks = 0\n                    mem_x_all = None\n                    mem_scores_all = None\n                    sample_preds, sample_gts = [], []\n                    # choose whether we allow grad\n                    for i in range(0, inp.size(1), chunk_size):\n                        ic = inp[:, i : i + chunk_size]\n                        tc = tgt[:, i : i + chunk_size]\n                        if ablation == \"entropy\":\n                            with torch.no_grad():\n                                logits, mem_x_all, mem_scores_all, ratio = model(\n                                    ic, mem_x_all, mem_scores_all\n                                )\n                        else:\n                            optimizer.zero_grad()\n                            logits, out = model(ic, mem_x_all)\n                            out.retain_grad()\n                            loss_chunk = criterion(\n                                logits.view(-1, vocab_size), tc.view(-1)\n                            )\n                            loss_chunk.backward()\n                            grad_norms = torch.norm(out.grad[0], dim=1).detach()\n                            x_det = out.detach()[0]\n                            if mem_x_all is None:\n                                mem_x_all = x_det\n                                mem_scores_all = grad_norms\n                            else:\n                                mem_x_all = torch.cat([mem_x_all, x_det], dim=0)\n                                mem_scores_all = torch.cat(\n                                    [mem_scores_all, grad_norms], dim=0\n                                )\n                            if mem_x_all.size(0) > mem_size:\n                                prev = mem_scores_all.sum().item() + 1e-10\n                                _, idx = torch.topk(mem_scores_all, mem_size)\n                                mem_x_all = mem_x_all[idx]\n                                mem_scores_all = mem_scores_all[idx]\n                                ratio = mem_scores_all.sum().item() / prev\n                            else:\n                                ratio = 1.0\n                        # collect preds/gts\n                        pred_toks = logits.argmax(dim=-1)[0].cpu().tolist()\n                        sample_preds.extend(pred_toks)\n                        sample_gts.extend(tc[0].cpu().tolist())\n                        # loss accounting\n                        if ablation == \"entropy\":\n                            loss_chunk = criterion(\n                                logits.view(-1, vocab_size), tc.view(-1)\n                            )\n                        losses_sample += loss_chunk.item()\n                        num_chunks += 1\n                        val_ratios.append(ratio)\n                        if mem_scores_all is not None:\n                            val_eff.append(\n                                mem_scores_all.sum().item() / mem_scores_all.numel()\n                            )\n                    val_loss_sum += losses_sample / max(1, num_chunks)\n                    experiment_data[ablation][key][\"predictions\"].append(sample_preds)\n                    experiment_data[ablation][key][\"ground_truth\"].append(sample_gts)\n            avg_val_loss = val_loss_sum / len(val_loader)\n            avg_val_ratio = sum(val_ratios) / len(val_ratios)\n            avg_val_eff = sum(val_eff) / len(val_eff) if val_eff else 0.0\n            ed[\"losses\"][\"val\"].append(avg_val_loss)\n            ed[\"metrics\"][\"Memory Retention Ratio\"][\"val\"].append(avg_val_ratio)\n            ed[\"metrics\"][\"Score-Weighted Memory Efficiency\"][\"val\"].append(avg_val_eff)\n            print(\n                f\"Ablation {ablation} | {key} | Epoch {epoch}: val_loss={avg_val_loss:.4f}\"\n            )\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '\\n=== Dataset: pg19 ===', '\\n', 'Epoch 0:\nvalidation_loss = 2.9374', '\\n', 'Epoch 1: validation_loss = 2.7136', '\\n',\n'\\n=== Dataset: scientific_papers_arxiv ===', '\\n', 'Epoch 0: validation_loss =\n2.6360', '\\n', 'Epoch 1: validation_loss = 2.5154', '\\n', '\\n=== Dataset:\nwikitext_wikitext-2-raw-v1 ===', '\\n', 'Epoch 0: validation_loss = 1.5025',\n'\\n', 'Epoch 1: validation_loss = 1.3526', '\\n', 'Execution time: 14 minutes\nseconds (time limit is an hour).']", "['Running original on pg19', '\\n', 'original pg19 Epoch 0 val_loss=2.9374',\n'\\n', 'original pg19 Epoch 1 val_loss=2.7136', '\\n', 'Running original on\nscientific_papers_arxiv', '\\n', 'original scientific_papers_arxiv Epoch 0\nval_loss=2.6360', '\\n', 'original scientific_papers_arxiv Epoch 1\nval_loss=2.5154', '\\n', 'Running original on wikitext_wikitext-2-raw-v1', '\\n',\n'original wikitext_wikitext-2-raw-v1 Epoch 0 val_loss=1.5025', '\\n', 'original\nwikitext_wikitext-2-raw-v1 Epoch 1 val_loss=1.3526', '\\n', 'Running recency on\npg19', '\\n', 'recency pg19 Epoch 0 val_loss=2.9201', '\\n', 'recency pg19 Epoch 1\nval_loss=2.7338', '\\n', 'Running recency on scientific_papers_arxiv', '\\n',\n'recency scientific_papers_arxiv Epoch 0 val_loss=2.6225', '\\n', 'recency\nscientific_papers_arxiv Epoch 1 val_loss=2.4929', '\\n', 'Running recency on\nwikitext_wikitext-2-raw-v1', '\\n', 'recency wikitext_wikitext-2-raw-v1 Epoch 0\nval_loss=1.5069', '\\n', 'recency wikitext_wikitext-2-raw-v1 Epoch 1\nval_loss=1.3529', '\\n', 'Execution time: 29 minutes seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', '\\n=== Ablation: baseline | Dataset: pg19 ===',\n'\\n', 'Traceback (most recent call last):\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/aiohttp/client_proto.py\", line 93, in connection_lost\\n    uncompleted\n= self._parser.feed_eof()\\n                  ^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"aiohttp/_http_parser.pyx\", line 511, in aiohttp._http_parser.HttpParser.feed_eo\nf\\naiohttp.http_exceptions.ContentLengthError: 400, message:\\n  Not enough data\nto satisfy content length header.\\n\\nThe above exception was the direct cause of\nthe following exception:\\n\\nTraceback (most recent call last):\\n  File\n\"runfile.py\", line 136, in <module>\\n    train_samples =\nlist(itertools.islice(train_stream, 200))\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/iterable_dataset.py\", line 2270, in __iter__\\n    for key,\nexample in ex_iterable:\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/iterable_dataset.py\", line 1856, in __iter__\\n    for key,\npa_table in self._iter_arrow():\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/iterable_dataset.py\", line 1879, in _iter_arrow\\n    for key,\npa_table in self.ex_iterable._iter_arrow():\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/iterable_dataset.py\", line 476, in _iter_arrow\\n    for key,\npa_table in iterator:\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/iterable_dataset.py\", line 128, in _convert_to_arrow\\n    for\nkey, example in iterator:\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/iterable_dataset.py\", line 222, in __iter__\\n    for\nkey_example in islice(self.generate_examples_fn(**gen_kwags),\nshard_example_idx_start, None):\\n  File \"/home/chenhui/.cache/huggingface/module\ns/datasets_modules/datasets/pg19/fb74320038a3c19e3cc87375222fc75ed3c8dc5a739b3e8\ndc835736388a7a882/pg19.py\", line 127, in _generate_examples\\n    id2metadata =\n{row[\"_id\"]: row for row in reader}\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/chenhui/.cache/huggingface/mo\ndules/datasets_modules/datasets/pg19/fb74320038a3c19e3cc87375222fc75ed3c8dc5a739\nb3e8dc835736388a7a882/pg19.py\", line 127, in <dictcomp>\\n    id2metadata =\n{row[\"_id\"]: row for row in reader}\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/csv.py\", line 111, in\n__next__\\n    row = next(self.reader)\\n          ^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/fsspec/implementations/http.py\", line 627, in read\\n    return\nsuper().read(length)\\n           ^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/fsspec/spec.py\", line 2083, in read\\n    out =\nself.cache._fetch(self.loc, self.loc + length)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/fsspec/caching.py\", line 496, in _fetch\\n    self.cache =\nself.fetcher(start, bend)\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/fsspec/asyn.py\", line 118, in wrapper\\n    return sync(self.loop, func,\n*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/fsspec/asyn.py\", line 103, in sync\\n    raise return_result\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/fsspec/asyn.py\", line 56, in _runner\\n    result[0] = await coro\\n\n^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/fsspec/implementations/http.py\", line 696, in async_fetch_range\\n\nout = await r.read()\\n          ^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/aiohttp/client_reqrep.py\", line 671, in read\\n    self._body = await\nself.content.read()\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/aiohttp/streams.py\", line 418, in read\\n    block = await\nself.readany()\\n            ^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/aiohttp/streams.py\", line 440, in readany\\n    await\nself._wait(\"readany\")\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/aiohttp/streams.py\", line 347, in _wait\\n    await\nwaiter\\naiohttp.client_exceptions.ClientPayloadError: Response payload is not\ncompleted: <ContentLengthError: 400, message=\\'Not enough data to satisfy\ncontent length header.\\'>. ConnectionResetError(104, \\'Connection reset by\npeer\\')\\n', 'Execution time: 2 minutes seconds (time limit is an hour).']", "['\\n=== Ablation: entropy_based, Dataset: pg19 ===', '\\n', 'Epoch 0:\nvalidation_loss = 2.8985', '\\n', 'Epoch 1: validation_loss = 2.6569', '\\n',\n'\\n=== Ablation: entropy_based, Dataset: scientific_papers_arxiv ===', '\\n',\n'Epoch 0: validation_loss = 2.6262', '\\n', 'Epoch 1: validation_loss = 2.4968',\n'\\n', '\\n=== Ablation: entropy_based, Dataset: wikitext_wikitext-2-raw-v1 ===',\n'\\n', 'Epoch 0: validation_loss = 1.4869', '\\n', 'Epoch 1: validation_loss =\n1.3463', '\\n', '\\n=== Ablation: random_retention, Dataset: pg19 ===', '\\n',\n'Epoch 0: validation_loss = 2.8864', '\\n', 'Epoch 1: validation_loss = 2.6999',\n'\\n', '\\n=== Ablation: random_retention, Dataset: scientific_papers_arxiv ===',\n'\\n', 'Epoch 0: validation_loss = 2.6347', '\\n', 'Epoch 1: validation_loss =\n2.5067', '\\n', '\\n=== Ablation: random_retention, Dataset:\nwikitext_wikitext-2-raw-v1 ===', '\\n', 'Epoch 0: validation_loss = 1.5329',\n'\\n', 'Epoch 1: validation_loss = 1.3645', '\\n', 'Execution time: 29 minutes\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Ablation: baseline ===', '\\n', '\\nDataset:\npg19', '\\n', 'Ablation=baseline Dataset=pg19 Epoch=0 ValLoss=2.9374', '\\n',\n'Ablation=baseline Dataset=pg19 Epoch=1 ValLoss=2.7136', '\\n', '\\nDataset:\nscientific_papers_arxiv', '\\n', 'Ablation=baseline\nDataset=scientific_papers_arxiv Epoch=0 ValLoss=2.6360', '\\n',\n'Ablation=baseline Dataset=scientific_papers_arxiv Epoch=1 ValLoss=2.5154',\n'\\n', '\\nDataset: wikitext_wikitext-2-raw-v1', '\\n', 'Ablation=baseline\nDataset=wikitext_wikitext-2-raw-v1 Epoch=0 ValLoss=1.5025', '\\n',\n'Ablation=baseline Dataset=wikitext_wikitext-2-raw-v1 Epoch=1 ValLoss=1.3526',\n'\\n', '\\n=== Ablation: ffn_removed ===', '\\n', '\\nDataset: pg19', '\\n',\n'Ablation=ffn_removed Dataset=pg19 Epoch=0 ValLoss=3.0778', '\\n',\n'Ablation=ffn_removed Dataset=pg19 Epoch=1 ValLoss=2.8088', '\\n', '\\nDataset:\nscientific_papers_arxiv', '\\n', 'Ablation=ffn_removed\nDataset=scientific_papers_arxiv Epoch=0 ValLoss=2.7428', '\\n',\n'Ablation=ffn_removed Dataset=scientific_papers_arxiv Epoch=1 ValLoss=2.5527',\n'\\n', '\\nDataset: wikitext_wikitext-2-raw-v1', '\\n', 'Ablation=ffn_removed\nDataset=wikitext_wikitext-2-raw-v1 Epoch=0 ValLoss=1.5718', '\\n',\n'Ablation=ffn_removed Dataset=wikitext_wikitext-2-raw-v1 Epoch=1\nValLoss=1.4069', '\\n', 'Execution time: 29 minutes seconds (time limit is an\nhour).']", "['=== Ablation no_layernorm on pg19 ===', '\\n', 'pg19 Epoch 0: val_loss=2.9066',\n'\\n', 'pg19 Epoch 1: val_loss=2.7576', '\\n', '=== Ablation no_layernorm on\nscientific_papers_arxiv ===', '\\n', 'scientific_papers_arxiv Epoch 0:\nval_loss=2.5505', '\\n', 'scientific_papers_arxiv Epoch 1: val_loss=2.4884',\n'\\n', '=== Ablation no_layernorm on wikitext_wikitext-2-raw-v1 ===', '\\n',\n'wikitext_wikitext-2-raw-v1 Epoch 0: val_loss=1.3916', '\\n',\n'wikitext_wikitext-2-raw-v1 Epoch 1: val_loss=1.3257', '\\n', 'Execution time: 14\nminutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Ablation: original, Dataset: pg19 ===',\n'\\n', 'original | pg19 | Epoch 0: val_loss=2.9374', '\\n', 'original | pg19 |\nEpoch 1: val_loss=2.7136', '\\n', '\\n=== Ablation: original, Dataset:\nscientific_papers_arxiv ===', '\\n', 'original | scientific_papers_arxiv | Epoch\n0: val_loss=2.6360', '\\n', 'original | scientific_papers_arxiv | Epoch 1:\nval_loss=2.5154', '\\n', '\\n=== Ablation: original, Dataset:\nwikitext_wikitext-2-raw-v1 ===', '\\n', 'original | wikitext_wikitext-2-raw-v1 |\nEpoch 0: val_loss=1.5025', '\\n', 'original | wikitext_wikitext-2-raw-v1 | Epoch\n1: val_loss=1.3526', '\\n', '\\n=== Ablation: no_memory_attention, Dataset: pg19\n===', '\\n', 'no_memory_attention | pg19 | Epoch 0: val_loss=2.8569', '\\n',\n'no_memory_attention | pg19 | Epoch 1: val_loss=2.6604', '\\n', '\\n=== Ablation:\nno_memory_attention, Dataset: scientific_papers_arxiv ===', '\\n',\n'no_memory_attention | scientific_papers_arxiv | Epoch 0: val_loss=2.6133',\n'\\n', 'no_memory_attention | scientific_papers_arxiv | Epoch 1:\nval_loss=2.4547', '\\n', '\\n=== Ablation: no_memory_attention, Dataset:\nwikitext_wikitext-2-raw-v1 ===', '\\n', 'no_memory_attention |\nwikitext_wikitext-2-raw-v1 | Epoch 0: val_loss=1.5062', '\\n',\n'no_memory_attention | wikitext_wikitext-2-raw-v1 | Epoch 1: val_loss=1.3520',\n'\\n', 'Execution time: 29 minutes seconds (time limit is an hour).']", "['[entropy][pg19] Epoch 0 val_loss=2.9374', '\\n', '[entropy][pg19] Epoch 1\nval_loss=2.7136', '\\n', '[entropy][scientific_papers_arxiv] Epoch 0\nval_loss=2.6360', '\\n', '[entropy][scientific_papers_arxiv] Epoch 1\nval_loss=2.5154', '\\n', '[entropy][wikitext_wikitext-2-raw-v1] Epoch 0\nval_loss=1.5025', '\\n', '[entropy][wikitext_wikitext-2-raw-v1] Epoch 1\nval_loss=1.3526', '\\n', '[norm][pg19] Epoch 0 val_loss=2.9114', '\\n',\n'[norm][pg19] Epoch 1 val_loss=2.7067', '\\n', '[norm][scientific_papers_arxiv]\nEpoch 0 val_loss=2.6259', '\\n', '[norm][scientific_papers_arxiv] Epoch 1\nval_loss=2.4918', '\\n', '[norm][wikitext_wikitext-2-raw-v1] Epoch 0\nval_loss=1.5104', '\\n', '[norm][wikitext_wikitext-2-raw-v1] Epoch 1\nval_loss=1.3542', '\\n', 'Execution time: 29 minutes seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', '\\n=== Ablation: baseline | Dataset: pg19 ===',\n'\\n', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading data:   0%|          | 0.00/178k\n[00:00<?, ?B/s]', '\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A',\n'\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading data:  28%|##8       | 50.7k/178k [00:00<00:00,\n138kB/s]', '\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:  46%|####6     | 82.0k/178k [00:00<00:00, 116kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  74%|#######4  | 133k/178k [00:00<00:00, 184kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  93%|#########2| 165k/178k [00:00<00:00, 202kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 178k/178k [00:00<00:00, 189kB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/184k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  28%|##8       | 52.0k/184k [00:00<00:00, 141kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  45%|####5     | 83.4k/184k [00:00<00:00, 118kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  72%|#######2  | 133k/184k [00:00<00:00, 183kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  90%|######### | 165k/184k [00:00<00:00, 197kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 184k/184k [00:00<00:00, 193kB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/331k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  15%|#4        | 49.3k/331k [00:00<00:02, 134kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  25%|##5       | 83.4k/331k [00:00<00:02, 119kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  40%|####      | 134k/331k [00:00<00:01, 187kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  50%|####9     | 165k/331k [00:00<00:00, 197kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  65%|######5   | 216k/331k [00:01<00:00, 251kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  80%|#######9  | 264k/331k [00:01<00:00, 301kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 331k/331k [00:01<00:00, 265kB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/532k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  10%|9         | 52.0k/532k [00:00<00:03, 141kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  15%|#5        | 82.0k/532k [00:00<00:03, 115kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  25%|##5       | 134k/532k [00:00<00:02, 186kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  31%|###1      | 165k/532k [00:00<00:01, 196kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  40%|####      | 214k/532k [00:01<00:01, 247kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  50%|####9     | 264k/532k [00:01<00:00, 300kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  65%|######4   | 344k/532k [00:01<00:00, 413kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  86%|########5 | 457k/532k [00:01<00:00, 574kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 532k/532k [00:01<00:00, 357kB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/269k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  24%|##3       | 64.3k/269k [00:00<00:01, 176kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  36%|###6      | 97.0k/269k [00:00<00:01, 136kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  54%|#####4    | 146k/269k [00:00<00:00, 198kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  67%|######6   | 179k/269k [00:00<00:00, 208kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  85%|########4 | 228k/269k [00:01<00:00, 257kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 269k/269k [00:01<00:00, 233kB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/505k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  13%|#2        | 64.3k/505k [00:00<00:02, 175kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  19%|#9        | 97.0k/505k [00:00<00:03, 136kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  29%|##8       | 146k/505k [00:00<00:01, 197kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  35%|###5      | 179k/505k [00:00<00:01, 207kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  45%|####5     | 228k/505k [00:01<00:01, 256kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  55%|#####4    | 277k/505k [00:01<00:00, 308kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  71%|#######1  | 359k/505k [00:01<00:00, 423kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  91%|######### | 457k/505k [00:01<00:00, 546kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 505k/505k [00:01<00:00, 348kB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/232k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  28%|##7       | 64.3k/232k [00:00<00:00, 173kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  42%|####1     | 97.0k/232k [00:00<00:01, 134kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  63%|######3   | 146k/232k [00:00<00:00, 195kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  77%|#######7  | 179k/232k [00:00<00:00, 206kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata: 100%|##########| 232k/232k [00:01<00:00, 262kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 232k/232k [00:01<00:00, 215kB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/138k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  47%|####6     | 64.3k/138k [00:00<00:00, 176kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  70%|#######   | 97.0k/138k [00:00<00:00, 136kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata: 100%|##########| 138k/138k [00:00<00:00, 189kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 138k/138k [00:00<00:00, 176kB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/25.1k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 25.1k/25.1k [00:00<00:00, 1.20MB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/329k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  15%|#5        | 50.7k/329k [00:00<00:02, 138kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  25%|##5       | 83.4k/329k [00:00<00:02, 119kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  40%|####      | 133k/329k [00:00<00:01, 184kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  50%|#####     | 165k/329k [00:00<00:00, 198kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  66%|######5   | 216k/329k [00:01<00:00, 252kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  80%|#######9  | 262k/329k [00:01<00:00, 298kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 329k/329k [00:01<00:00, 263kB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/408k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  12%|#2        | 50.7k/408k [00:00<00:02, 137kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  20%|##        | 82.0k/408k [00:00<00:02, 116kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  32%|###2      | 133k/408k [00:00<00:01, 184kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  41%|####      | 165k/408k [00:00<00:01, 198kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  52%|#####2    | 213k/408k [00:01<00:00, 246kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  65%|######5   | 266k/408k [00:01<00:00, 309kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  85%|########4 | 346k/408k [00:01<00:00, 417kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 408k/408k [00:01<00:00, 303kB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/289k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  18%|#7        | 50.7k/289k [00:00<00:01, 138kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  29%|##9       | 84.8k/289k [00:00<00:01, 122kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  46%|####5     | 133k/289k [00:00<00:00, 184kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  57%|#####6    | 164k/289k [00:00<00:00, 196kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  74%|#######4  | 214k/289k [00:01<00:00, 251kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  92%|#########1| 265k/289k [00:01<00:00, 307kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 289k/289k [00:01<00:00, 244kB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/692k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:   9%|9         | 64.3k/692k [00:00<00:03, 175kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  14%|#4        | 97.0k/692k [00:00<00:04, 136kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  21%|##1       | 146k/692k [00:00<00:02, 197kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  26%|##5       | 179k/692k [00:00<00:02, 208kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  33%|###2      | 228k/692k [00:01<00:01, 256kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  40%|####      | 277k/692k [00:01<00:01, 308kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  52%|#####1    | 359k/692k [00:01<00:00, 418kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  66%|######6   | 457k/692k [00:01<00:00, 546kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  80%|########  | 556k/692k [00:01<00:00, 646kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 692k/692k [00:01<00:00, 431kB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/1.68M [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:   4%|3         | 64.3k/1.68M [00:00<00:09, 176kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:   6%|5         | 97.0k/1.68M [00:00<00:11, 136kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:   9%|8         | 146k/1.68M [00:00<00:07, 198kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  11%|#         | 179k/1.68M [00:00<00:07, 208kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  14%|#3        | 228k/1.68M [00:01<00:05, 258kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  17%|#6        | 277k/1.68M [00:01<00:04, 309kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  21%|##1       | 359k/1.68M [00:01<00:03, 421kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  27%|##7       | 457k/1.68M [00:01<00:02, 548kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  33%|###3      | 556k/1.68M [00:01<00:01, 650kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  42%|####1     | 703k/1.68M [00:01<00:01, 852kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  54%|#####3    | 900k/1.68M [00:01<00:00, 1.14MB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  66%|######6   | 1.11M/1.68M [00:01<00:00, 1.41MB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  83%|########2 | 1.39M/1.68M [00:01<00:00, 1.79MB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 1.68M/1.68M [00:02<00:00, 839kB/s] ', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/4.10M [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:   2%|1         | 64.3k/4.10M [00:00<00:22, 176kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:   2%|2         | 97.0k/4.10M [00:00<00:29, 137kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:   4%|3         | 146k/4.10M [00:00<00:19, 199kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:   4%|4         | 179k/4.10M [00:00<00:18, 209kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:   6%|5         | 228k/4.10M [00:01<00:14, 258kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:   7%|6         | 277k/4.10M [00:01<00:12, 311kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:   9%|8         | 359k/4.10M [00:01<00:08, 421kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  11%|#1        | 457k/4.10M [00:01<00:06, 550kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  14%|#3        | 556k/4.10M [00:01<00:05, 651kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  17%|#7        | 703k/4.10M [00:01<00:03, 860kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  22%|##1       | 883k/4.10M [00:01<00:02, 1.12MB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  27%|##7       | 1.11M/4.10M [00:01<00:02, 1.43MB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  34%|###4      | 1.41M/4.10M [00:01<00:01, 1.84MB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  43%|####3     | 1.77M/4.10M [00:02<00:00, 2.33MB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  55%|#####4    | 2.24M/4.10M [00:02<00:00, 3.04MB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  70%|######9   | 2.85M/4.10M [00:02<00:00, 3.92MB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  88%|########7 | 3.60M/4.10M [00:02<00:00, 4.98MB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 4.10M/4.10M [00:02<00:00, 1.73MB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/182k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  35%|###5      | 64.3k/182k [00:00<00:00, 174kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  53%|#####3    | 97.0k/182k [00:00<00:00, 135kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  80%|########  | 146k/182k [00:00<00:00, 196kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata: 100%|##########| 182k/182k [00:00<00:00, 213kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 182k/182k [00:00<00:00, 191kB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/1.68M [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:   4%|3         | 64.3k/1.68M [00:00<00:09, 175kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:   6%|5         | 97.0k/1.68M [00:00<00:11, 136kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:   9%|8         | 146k/1.68M [00:00<00:07, 197kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  11%|#         | 179k/1.68M [00:00<00:07, 208kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  14%|#3        | 228k/1.68M [00:01<00:05, 257kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  16%|#6        | 277k/1.68M [00:01<00:04, 309kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  21%|##1       | 359k/1.68M [00:01<00:03, 419kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  27%|##7       | 457k/1.68M [00:01<00:02, 547kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  33%|###3      | 556k/1.68M [00:01<00:01, 649kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  42%|####1     | 703k/1.68M [00:01<00:01, 849kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  53%|#####3    | 900k/1.68M [00:01<00:00, 1.13MB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  66%|######6   | 1.11M/1.68M [00:01<00:00, 1.41MB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  83%|########2 | 1.39M/1.68M [00:01<00:00, 1.80MB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 1.68M/1.68M [00:02<00:00, 840kB/s] ', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/109k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  59%|#####8    | 64.3k/109k [00:00<00:00, 174kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  89%|########9 | 97.0k/109k [00:00<00:00, 135kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 109k/109k [00:00<00:00, 154kB/s] ', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/445k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  14%|#4        | 64.3k/445k [00:00<00:02, 173kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  22%|##1       | 97.0k/445k [00:00<00:02, 134kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  33%|###2      | 146k/445k [00:00<00:01, 195kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  40%|####      | 179k/445k [00:00<00:01, 205kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  51%|#####1    | 228k/445k [00:01<00:00, 254kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  62%|######2   | 277k/445k [00:01<00:00, 306kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  81%|########  | 359k/445k [00:01<00:00, 420kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 445k/445k [00:01<00:00, 320kB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/1.14M [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:   6%|5         | 64.3k/1.14M [00:00<00:06, 174kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:   9%|8         | 97.0k/1.14M [00:00<00:07, 135kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  13%|#2        | 146k/1.14M [00:00<00:05, 196kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  16%|#5        | 179k/1.14M [00:00<00:04, 207kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  20%|##        | 228k/1.14M [00:01<00:03, 256kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  24%|##4       | 277k/1.14M [00:01<00:02, 308kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  32%|###1      | 359k/1.14M [00:01<00:01, 423kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  40%|####      | 457k/1.14M [00:01<00:01, 546kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  49%|####8     | 556k/1.14M [00:01<00:00, 638kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  62%|######1   | 703k/1.14M [00:01<00:00, 848kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  79%|#######9  | 900k/1.14M [00:01<00:00, 1.14MB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  98%|#########7| 1.11M/1.14M [00:01<00:00, 1.39MB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 1.14M/1.14M [00:01<00:00, 618kB/s] ', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/662k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  10%|9         | 64.3k/662k [00:00<00:03, 174kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  15%|#4        | 97.0k/662k [00:00<00:04, 135kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  22%|##2       | 146k/662k [00:00<00:02, 196kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  27%|##7       | 179k/662k [00:00<00:02, 207kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  34%|###4      | 228k/662k [00:01<00:01, 255kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  42%|####1     | 277k/662k [00:01<00:01, 307kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  54%|#####4    | 359k/662k [00:01<00:00, 422kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  69%|######9   | 457k/662k [00:01<00:00, 544kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  84%|########3 | 556k/662k [00:01<00:00, 637kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 662k/662k [00:01<00:00, 416kB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/267k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  24%|##4       | 64.3k/267k [00:00<00:01, 175kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  36%|###6      | 97.0k/267k [00:00<00:01, 136kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  55%|#####4    | 146k/267k [00:00<00:00, 198kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  67%|######6   | 179k/267k [00:00<00:00, 208kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  85%|########5 | 228k/267k [00:01<00:00, 257kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 267k/267k [00:01<00:00, 234kB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/184k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  35%|###4      | 64.3k/184k [00:00<00:00, 173kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  53%|#####2    | 97.0k/184k [00:00<00:00, 134kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  79%|#######9  | 146k/184k [00:00<00:00, 194kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata: 100%|##########| 184k/184k [00:00<00:00, 216kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '', '\\rDownloading data:\n100%|##########| 184k/184k [00:00<00:00, 192kB/s]', '\\n', '\\n\\n\\n\\n\\n\\n\\n',\n'\\rDownloading data:   0%|          | 0.00/283k [00:00<?, ?B/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  23%|##2       | 64.3k/283k [00:00<00:01, 174kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  34%|###4      | 97.0k/283k [00:00<00:01, 135kB/s]',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', '\\n\\n\\n\\n\\n\\n\\n', '\\rDownloading\ndata:  52%|#####1    | 146k/283k [00:00<00:00, 196kB/s] ',\n'\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A\\x1b[A', 'Traceback (most recent call\nlast):\\n  File \"runfile.py\", line 129, in <module>\\n    train_ds =\nload_dataset(ds_name, cfg, split=train_split)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2084, in load_dataset\\n\nbuilder_instance.download_and_prepare(\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/builder.py\", line 925, in download_and_prepare\\n\nself._download_and_prepare(\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/builder.py\", line 1649, in _download_and_prepare\\n\nsuper()._download_and_prepare(\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/builder.py\", line 979, in _download_and_prepare\\n\nsplit_generators = self._split_generators(dl_manager,\n**split_generators_kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/che\nnhui/.cache/huggingface/modules/datasets_modules/datasets/pg19/fb74320038a3c19e3\ncc87375222fc75ed3c8dc5a739b3e8dc835736388a7a882/pg19.py\", line 93, in\n_split_generators\\n    downloaded_files = dl_manager.download(files)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/download/download_manager.py\", line 159, in download\\n\ndownloaded_path_or_paths = map_nested(\\n\n^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/utils/py_utils.py\", line 521, in map_nested\\n    mapped = [\\n\n^\\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/utils/py_utils.py\", line 522, in <listcomp>\\n\n_single_map_nested((function, obj, batched, batch_size, types, None, True,\nNone))\\n  File \"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/utils/py_utils.py\", line 409, in _single_map_nested\\n\nmapped = [_single_map_nested((function, v, batched, batch_size, types, None,\nTrue, None)) for v in pbar]\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/utils/py_utils.py\", line 409, in <listcomp>\\n    mapped =\n[_single_map_nested((function, v, batched, batch_size, types, None, True, None))\nfor v in pbar]\\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/utils/py_utils.py\", line 405, in _single_map_nested\\n\nreturn {\\n           ^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/utils/py_utils.py\", line 406, in <dictcomp>\\n    k:\n_single_map_nested((function, v, batched, batch_size, types, None, True, None))\nfor k, v in pbar\\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/utils/py_utils.py\", line 381, in _single_map_nested\\n\nreturn function([data_struct])[0]\\n           ^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/download/download_manager.py\", line 219, in\n_download_batched\\n    return [\\n           ^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/download/download_manager.py\", line 220, in <listcomp>\\n\nself._download_single(url_or_filename, download_config=download_config)\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/download/download_manager.py\", line 229, in _download_single\\n\nout = cached_path(url_or_filename, download_config=download_config)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/utils/file_utils.py\", line 214, in cached_path\\n\noutput_path = get_from_cache(\\n                  ^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/utils/file_utils.py\", line 415, in get_from_cache\\n\nfsspec_get(url, temp_file, storage_options=storage_options, desc=download_desc,\ndisable_tqdm=disable_tqdm)\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/utils/file_utils.py\", line 339, in fsspec_get\\n\nfs.get_file(path, temp_file.name, callback=callback)\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/fsspec/asyn.py\", line 118, in wrapper\\n    return sync(self.loop, func,\n*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/fsspec/asyn.py\", line 91, in sync\\n    if event.wait(1):\\n\n^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/threading.py\", line\n629, in wait\\n    signaled = self._cond.wait(timeout)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/threading.py\", line\n331, in wait\\n    gotit = waiter.acquire(True, timeout)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nKeyboardInterrupt\\n', 'TimeoutError: Execution\nexceeded the time limit of an hour']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 125, in <module>\\n    raw_train = load_dataset(ds_name, cfg,\nsplit=train_split, streaming=True)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2081, in load_dataset\\n    return\nbuilder_instance.as_streaming_dataset(split=split)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/builder.py\", line 1272, in as_streaming_dataset\\n    raise\nValueError(f\"Bad split: {split}. Available splits:\n{list(splits_generators)}\")\\nValueError: Bad split: train[:200]. Available\nsplits: [\\'train\\', \\'validation\\', \\'test\\']\\n', 'Execution time: 7 seconds\nseconds (time limit is an hour).']", "['\\n=== Dataset: pg19 ===', '\\n', 'pg19 Epoch 0 \u2192 val_loss 2.9441, retention\n1.0000', '\\n', 'pg19 Epoch 1 \u2192 val_loss 2.7340, retention 1.0000', '\\n', '\\n===\nDataset: scientific_papers_arxiv ===', '\\n', 'scientific_papers_arxiv Epoch 0 \u2192\nval_loss 2.6335, retention 1.0000', '\\n', 'scientific_papers_arxiv Epoch 1 \u2192\nval_loss 2.5144, retention 1.0000', '\\n', '\\n=== Dataset:\nwikitext_wikitext-2-raw-v1 ===', '\\n', 'wikitext_wikitext-2-raw-v1 Epoch 0 \u2192\nval_loss 1.5023, retention 1.0000', '\\n', 'wikitext_wikitext-2-raw-v1 Epoch 1 \u2192\nval_loss 1.3523, retention 1.0000', '\\n', 'Execution time: 15 minutes seconds\n(time limit is an hour).']", "['\\n=== Ablation baseline, Dataset pg19 ===', '\\n', 'baseline pg19 Epoch 0:\nval_loss=2.9374', '\\n', 'baseline pg19 Epoch 1: val_loss=2.7136', '\\n', '\\n===\nAblation baseline, Dataset scientific_papers_arxiv ===', '\\n', 'baseline\nscientific_papers_arxiv Epoch 0: val_loss=2.6360', '\\n', 'baseline\nscientific_papers_arxiv Epoch 1: val_loss=2.5154', '\\n', '\\n=== Ablation\nbaseline, Dataset wikitext_wikitext-2-raw-v1 ===', '\\n', 'baseline\nwikitext_wikitext-2-raw-v1 Epoch 0: val_loss=1.5025', '\\n', 'baseline\nwikitext_wikitext-2-raw-v1 Epoch 1: val_loss=1.3526', '\\n', '\\n=== Ablation\nfeedforward_identity, Dataset pg19 ===', '\\n', 'feedforward_identity pg19 Epoch\n0: val_loss=2.9050', '\\n', 'feedforward_identity pg19 Epoch 1: val_loss=2.7590',\n'\\n', '\\n=== Ablation feedforward_identity, Dataset scientific_papers_arxiv\n===', '\\n', 'feedforward_identity scientific_papers_arxiv Epoch 0:\nval_loss=2.6213', '\\n', 'feedforward_identity scientific_papers_arxiv Epoch 1:\nval_loss=2.4980', '\\n', '\\n=== Ablation feedforward_identity, Dataset\nwikitext_wikitext-2-raw-v1 ===', '\\n', 'feedforward_identity\nwikitext_wikitext-2-raw-v1 Epoch 0: val_loss=1.5188', '\\n',\n'feedforward_identity wikitext_wikitext-2-raw-v1 Epoch 1: val_loss=1.3704',\n'\\n', 'Execution time: 29 minutes seconds (time limit is an hour).']", "['Using device:', ' ', 'cuda', '\\n', '\\n--- Ablation: baseline, Dataset: pg19\n---', '\\n', 'Abl baseline, pg19 \u2014 Epoch 0', '\\n', '  val_loss=2.9374', '\\n',\n'Abl baseline, pg19 \u2014 Epoch 1', '\\n', '  val_loss=2.7136', '\\n', '\\n---\nAblation: baseline, Dataset: scientific_papers_arxiv ---', '\\n', 'Abl baseline,\nscientific_papers_arxiv \u2014 Epoch 0', '\\n', '  val_loss=2.6360', '\\n', 'Abl\nbaseline, scientific_papers_arxiv \u2014 Epoch 1', '\\n', '  val_loss=2.5154', '\\n',\n'\\n--- Ablation: baseline, Dataset: wikitext_wikitext-2-raw-v1 ---', '\\n', 'Abl\nbaseline, wikitext_wikitext-2-raw-v1 \u2014 Epoch 0', '\\n', '  val_loss=1.5025',\n'\\n', 'Abl baseline, wikitext_wikitext-2-raw-v1 \u2014 Epoch 1', '\\n', '\nval_loss=1.3526', '\\n', '\\n--- Ablation: continuous_memory, Dataset: pg19 ---',\n'\\n', 'Abl continuous_memory, pg19 \u2014 Epoch 0', '\\n', '  val_loss=2.9870', '\\n',\n'Abl continuous_memory, pg19 \u2014 Epoch 1', '\\n', '  val_loss=2.7548', '\\n', '\\n---\nAblation: continuous_memory, Dataset: scientific_papers_arxiv ---', '\\n', 'Abl\ncontinuous_memory, scientific_papers_arxiv \u2014 Epoch 0', '\\n', '\nval_loss=2.6413', '\\n', 'Abl continuous_memory, scientific_papers_arxiv \u2014 Epoch\n1', '\\n', '  val_loss=2.5277', '\\n', '\\n--- Ablation: continuous_memory,\nDataset: wikitext_wikitext-2-raw-v1 ---', '\\n', 'Abl continuous_memory,\nwikitext_wikitext-2-raw-v1 \u2014 Epoch 0', '\\n', '  val_loss=1.5183', '\\n', 'Abl\ncontinuous_memory, wikitext_wikitext-2-raw-v1 \u2014 Epoch 1', '\\n', '\nval_loss=1.3524', '\\n', 'Execution time: 28 minutes seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', '\\n=== Dataset: pg19 ===', '\\n', 'pg19 Epoch 0 \u2192\nvalidation_loss = 2.9441, retention = 0.7502', '\\n', 'pg19 Epoch 1 \u2192\nvalidation_loss = 2.7340, retention = 0.7502', '\\n', '\\n=== Dataset:\nscientific_papers_arxiv ===', '\\n', 'scientific_papers_arxiv Epoch 0 \u2192\nvalidation_loss = 2.6335, retention = 0.7502', '\\n', 'scientific_papers_arxiv\nEpoch 1 \u2192 validation_loss = 2.5144, retention = 0.7502', '\\n', '\\n=== Dataset:\nwikitext_wikitext-2-raw-v1 ===', '\\n', 'wikitext_wikitext-2-raw-v1 Epoch 0 \u2192\nvalidation_loss = 1.5023, retention = 0.7502', '\\n', 'wikitext_wikitext-2-raw-v1\nEpoch 1 \u2192 validation_loss = 1.3523, retention = 0.7502', '\\n', 'Execution time:\n15 minutes seconds (time limit is an hour).']", "['Using device:', ' ', 'cuda', '\\n', '\\n=== Ablation: max, Dataset: pg19 ===',\n'\\n', '[max][pg19] Epoch 0 val_loss=2.9374', '\\n', '[max][pg19] Epoch 1\nval_loss=2.7136', '\\n', '\\n=== Ablation: max, Dataset: scientific_papers_arxiv\n===', '\\n', '[max][scientific_papers_arxiv] Epoch 0 val_loss=2.6360', '\\n',\n'[max][scientific_papers_arxiv] Epoch 1 val_loss=2.5154', '\\n', '\\n=== Ablation:\nmax, Dataset: wikitext_wikitext-2-raw-v1 ===', '\\n',\n'[max][wikitext_wikitext-2-raw-v1] Epoch 0 val_loss=1.5025', '\\n',\n'[max][wikitext_wikitext-2-raw-v1] Epoch 1 val_loss=1.3526', '\\n', '\\n===\nAblation: mean, Dataset: pg19 ===', '\\n', '[mean][pg19] Epoch 0\nval_loss=2.9266', '\\n', '[mean][pg19] Epoch 1 val_loss=2.7212', '\\n', '\\n===\nAblation: mean, Dataset: scientific_papers_arxiv ===', '\\n',\n'[mean][scientific_papers_arxiv] Epoch 0 val_loss=2.6214', '\\n',\n'[mean][scientific_papers_arxiv] Epoch 1 val_loss=2.4925', '\\n', '\\n===\nAblation: mean, Dataset: wikitext_wikitext-2-raw-v1 ===', '\\n',\n'[mean][wikitext_wikitext-2-raw-v1] Epoch 0 val_loss=1.5081', '\\n',\n'[mean][wikitext_wikitext-2-raw-v1] Epoch 1 val_loss=1.3537', '\\n', 'Execution\ntime: 30 minutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Ablation: baseline, Dataset: pg19 ===',\n'\\n', 'Epoch 0: val_loss=2.9374', '\\n', 'Epoch 1: val_loss=2.7136', '\\n', '\\n===\nAblation: baseline, Dataset: scientific_papers_arxiv ===', '\\n', 'Epoch 0:\nval_loss=2.6360', '\\n', 'Epoch 1: val_loss=2.5154', '\\n', '\\n=== Ablation:\nbaseline, Dataset: wikitext_wikitext-2-raw-v1 ===', '\\n', 'Epoch 0:\nval_loss=1.5025', '\\n', 'Epoch 1: val_loss=1.3526', '\\n', '\\n=== Ablation:\nmemory_key_only, Dataset: pg19 ===', '\\n', 'Traceback (most recent call last):\\n\nFile \"runfile.py\", line 234, in <module>\\n    logits, mem_x, mem_ent, ratio =\nmodel(ic, mem_x, mem_ent)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"runfile.py\", line 150, in forward\\n    out, mem_x_new, mem_ent_new, ratio =\nself.mem_layer(emb, mem_x, mem_ent)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"runfile.py\", line 114, in forward\\n    attn_out, attn_w = self.attn(\\n\n^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/activation.py\", line 1368, in forward\\n\nattn_output, attn_output_weights = F.multi_head_attention_forward(\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/functional.py\", line 6086, in multi_head_attention_forward\\n\nassert (\\nAssertionError: key shape torch.Size([64, 1, 32]) does not match value\nshape torch.Size([32, 1, 32])\\n', 'Execution time: 29 minutes seconds (time\nlimit is an hour).']", "['pg19 epoch 0: val_loss=2.8950', '\\n', 'pg19 epoch 1: val_loss=2.7047', '\\n',\n'scientific_papers_arxiv epoch 0: val_loss=2.6254', '\\n',\n'scientific_papers_arxiv epoch 1: val_loss=2.5150', '\\n',\n'wikitext_wikitext-2-raw-v1 epoch 0: val_loss=1.5106', '\\n',\n'wikitext_wikitext-2-raw-v1 epoch 1: val_loss=1.3697', '\\n', 'Execution time: 15\nminutes seconds (time limit is an hour).']", "['\\n=== Ablation: entropy | Dataset: pg19 ===', '\\n', 'Ablation entropy | pg19 |\nEpoch 0: val_loss=2.9374', '\\n', 'Ablation entropy | pg19 | Epoch 1:\nval_loss=2.7136', '\\n', '\\n=== Ablation: entropy | Dataset:\nscientific_papers_arxiv ===', '\\n', 'Ablation entropy | scientific_papers_arxiv\n| Epoch 0: val_loss=2.6360', '\\n', 'Ablation entropy | scientific_papers_arxiv |\nEpoch 1: val_loss=2.5154', '\\n', '\\n=== Ablation: entropy | Dataset:\nwikitext_wikitext-2-raw-v1 ===', '\\n', 'Ablation entropy |\nwikitext_wikitext-2-raw-v1 | Epoch 0: val_loss=1.5025', '\\n', 'Ablation entropy\n| wikitext_wikitext-2-raw-v1 | Epoch 1: val_loss=1.3526', '\\n', '\\n=== Ablation:\ngrad | Dataset: pg19 ===', '\\n', 'Ablation grad | pg19 | Epoch 0:\nval_loss=2.6412', '\\n', 'Ablation grad | pg19 | Epoch 1: val_loss=2.6102', '\\n',\n'\\n=== Ablation: grad | Dataset: scientific_papers_arxiv ===', '\\n', 'Ablation\ngrad | scientific_papers_arxiv | Epoch 0: val_loss=2.4729', '\\n', 'Ablation grad\n| scientific_papers_arxiv | Epoch 1: val_loss=2.3840', '\\n', '\\n=== Ablation:\ngrad | Dataset: wikitext_wikitext-2-raw-v1 ===', '\\n', 'Ablation grad |\nwikitext_wikitext-2-raw-v1 | Epoch 0: val_loss=1.3263', '\\n', 'Ablation grad |\nwikitext_wikitext-2-raw-v1 | Epoch 1: val_loss=1.2601', '\\n', 'Execution time:\n29 minutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Ablation: baseline, Dataset: pg19 ===',\n'\\n', 'Epoch 0: validation_loss = 2.9374', '\\n', 'Epoch 1: validation_loss =\n2.7136', '\\n', '\\n=== Ablation: baseline, Dataset: scientific_papers_arxiv ===',\n'\\n', 'Epoch 0: validation_loss = 2.6360', '\\n', 'Epoch 1: validation_loss =\n2.5154', '\\n', '\\n=== Ablation: baseline, Dataset: wikitext_wikitext-2-raw-v1\n===', '\\n', 'Epoch 0: validation_loss = 1.5025', '\\n', 'Epoch 1: validation_loss\n= 1.3526', '\\n', '\\n=== Ablation: memory_key_only, Dataset: pg19 ===', '\\n',\n'Epoch 0: validation_loss = 2.9015', '\\n', 'Epoch 1: validation_loss = 2.6812',\n'\\n', '\\n=== Ablation: memory_key_only, Dataset: scientific_papers_arxiv ===',\n'\\n', 'Epoch 0: validation_loss = 2.6193', '\\n', 'Epoch 1: validation_loss =\n2.4786', '\\n', '\\n=== Ablation: memory_key_only, Dataset:\nwikitext_wikitext-2-raw-v1 ===', '\\n', 'Epoch 0: validation_loss = 1.5091',\n'\\n', 'Epoch 1: validation_loss = 1.3499', '\\n', 'Execution time: 28 minutes\nseconds (time limit is an hour).']", "['\\n=== Ablation: entropy | Dataset: pg19 ===', '\\n', 'Ablation entropy | pg19 |\nEpoch 0: val_loss=2.8985', '\\n', 'Ablation entropy | pg19 | Epoch 1:\nval_loss=2.6569', '\\n', '\\n=== Ablation: entropy | Dataset:\nscientific_papers_arxiv ===', '\\n', 'Ablation entropy | scientific_papers_arxiv\n| Epoch 0: val_loss=2.6262', '\\n', 'Ablation entropy | scientific_papers_arxiv |\nEpoch 1: val_loss=2.4968', '\\n', '\\n=== Ablation: entropy | Dataset:\nwikitext_wikitext-2-raw-v1 ===', '\\n', 'Ablation entropy |\nwikitext_wikitext-2-raw-v1 | Epoch 0: val_loss=1.4869', '\\n', 'Ablation entropy\n| wikitext_wikitext-2-raw-v1 | Epoch 1: val_loss=1.3463', '\\n', '\\n=== Ablation:\ngrad | Dataset: pg19 ===', '\\n', 'Ablation grad | pg19 | Epoch 0:\nval_loss=2.6820', '\\n', 'Ablation grad | pg19 | Epoch 1: val_loss=2.5988', '\\n',\n'\\n=== Ablation: grad | Dataset: scientific_papers_arxiv ===', '\\n', 'Ablation\ngrad | scientific_papers_arxiv | Epoch 0: val_loss=2.4838', '\\n', 'Ablation grad\n| scientific_papers_arxiv | Epoch 1: val_loss=2.3885', '\\n', '\\n=== Ablation:\ngrad | Dataset: wikitext_wikitext-2-raw-v1 ===', '\\n', 'Ablation grad |\nwikitext_wikitext-2-raw-v1 | Epoch 0: val_loss=1.3256', '\\n', 'Ablation grad |\nwikitext_wikitext-2-raw-v1 | Epoch 1: val_loss=1.2523', '\\n', 'Execution time:\n30 minutes seconds (time limit is an hour).']", "['\\n=== Ablation: entropy | Dataset: pg19 ===', '\\n', 'Ablation entropy | pg19 |\nEpoch 0: val_loss=2.8793', '\\n', 'Ablation entropy | pg19 | Epoch 1:\nval_loss=2.6916', '\\n', '\\n=== Ablation: entropy | Dataset:\nscientific_papers_arxiv ===', '\\n', 'Ablation entropy | scientific_papers_arxiv\n| Epoch 0: val_loss=2.6195', '\\n', 'Ablation entropy | scientific_papers_arxiv |\nEpoch 1: val_loss=2.5014', '\\n', '\\n=== Ablation: entropy | Dataset:\nwikitext_wikitext-2-raw-v1 ===', '\\n', 'Ablation entropy |\nwikitext_wikitext-2-raw-v1 | Epoch 0: val_loss=1.5204', '\\n', 'Ablation entropy\n| wikitext_wikitext-2-raw-v1 | Epoch 1: val_loss=1.3494', '\\n', '\\n=== Ablation:\ngrad | Dataset: pg19 ===', '\\n', 'Ablation grad | pg19 | Epoch 0:\nval_loss=2.6489', '\\n', 'Ablation grad | pg19 | Epoch 1: val_loss=2.5571', '\\n',\n'\\n=== Ablation: grad | Dataset: scientific_papers_arxiv ===', '\\n', 'Ablation\ngrad | scientific_papers_arxiv | Epoch 0: val_loss=2.4946', '\\n', 'Ablation grad\n| scientific_papers_arxiv | Epoch 1: val_loss=2.4173', '\\n', '\\n=== Ablation:\ngrad | Dataset: wikitext_wikitext-2-raw-v1 ===', '\\n', 'Ablation grad |\nwikitext_wikitext-2-raw-v1 | Epoch 0: val_loss=1.3291', '\\n', 'Ablation grad |\nwikitext_wikitext-2-raw-v1 | Epoch 1: val_loss=1.2586', '\\n', 'Execution time:\n29 minutes seconds (time limit is an hour).']", "['\\n=== Ablation: entropy | Dataset: pg19 ===', '\\n', 'Ablation entropy | pg19 |\nEpoch 0: val_loss=2.8355', '\\n', 'Ablation entropy | pg19 | Epoch 1:\nval_loss=2.7013', '\\n', '\\n=== Ablation: entropy | Dataset:\nscientific_papers_arxiv ===', '\\n', 'Ablation entropy | scientific_papers_arxiv\n| Epoch 0: val_loss=2.6199', '\\n', 'Ablation entropy | scientific_papers_arxiv |\nEpoch 1: val_loss=2.4968', '\\n', '\\n=== Ablation: entropy | Dataset:\nwikitext_wikitext-2-raw-v1 ===', '\\n', 'Ablation entropy |\nwikitext_wikitext-2-raw-v1 | Epoch 0: val_loss=1.4987', '\\n', 'Ablation entropy\n| wikitext_wikitext-2-raw-v1 | Epoch 1: val_loss=1.3448', '\\n', '\\n=== Ablation:\ngrad | Dataset: pg19 ===', '\\n', 'Ablation grad | pg19 | Epoch 0:\nval_loss=2.5923', '\\n', 'Ablation grad | pg19 | Epoch 1: val_loss=2.5031', '\\n',\n'\\n=== Ablation: grad | Dataset: scientific_papers_arxiv ===', '\\n', 'Ablation\ngrad | scientific_papers_arxiv | Epoch 0: val_loss=2.4722', '\\n', 'Ablation grad\n| scientific_papers_arxiv | Epoch 1: val_loss=2.3988', '\\n', '\\n=== Ablation:\ngrad | Dataset: wikitext_wikitext-2-raw-v1 ===', '\\n', 'Ablation grad |\nwikitext_wikitext-2-raw-v1 | Epoch 0: val_loss=1.3320', '\\n', 'Ablation grad |\nwikitext_wikitext-2-raw-v1 | Epoch 1: val_loss=1.2632', '\\n', 'Execution time:\n27 minutes seconds (time limit is an hour).']", ""], "analysis": ["", "", "The pipeline crashes immediately when loading the PG19 dataset in streaming\nmode, raising an aiohttp ContentLengthError (HTTP payload incomplete). This\nprevents any training or evaluation from occurring. Proposed fixes: 1. Disable\nstreaming (use load_dataset(..., streaming=False)) so the full dataset is\ndownloaded and cached locally before iteration. 2. Alternatively, add retry\nlogic or exception handling around the streaming loader to catch\nClientPayloadError/ContentLengthError and retry the request. 3. Ensure your\nfsspec/aiohttp and datasets libraries are up to date, as some older versions had\nstreaming bugs.", "", "", "", "The experiment ran successfully with no errors. All ablation variants (original\nvs. no_memory_attention) completed two epochs on three datasets within the hour.\nValidation losses steadily decreased for both configurations and, somewhat\nsurprisingly, disabling memory attention (\u201cno_memory_attention\u201d) led to\nmarginally better validation loss across all datasets. This suggests that the\ncurrent entropy\u2010aware memory layer may not yet be providing a benefit in this\nsetup. Next steps: inspect the recorded Memory Retention Ratio and\nEntropy\u2010Weighted Memory Efficiency metrics to diagnose the memory behavior,\nadjust hyperparameters or thresholding policies, and potentially refine the\nentropy\u2010based selection mechanism.", "", "The experiment script never proceeded to training because downloading large HF\ndatasets (pg19, scientific_papers) took too long, leading to a TimeoutError. The\nload_dataset calls without streaming download full dataset archives before\nsplitting, causing excessive download time. Fix: enable streaming mode in\nload_dataset (e.g., load_dataset(..., streaming=True)) or pre-cache / use\nsmaller local subsets so only the required split portions are downloaded,\nensuring the pipeline completes within time limits.", "The code crashes when calling load_dataset with streaming=True and\nsplit=\"train[:200]\". Streaming mode does not support slicing notation in the\nsplit argument; it only accepts plain split names. Proposed fix: either remove\nthe slicing from the split parameter (e.g., use split=\"train\") and then manually\nlimit to the first N samples with itertools.islice or Dataset.take, or disable\nstreaming (streaming=False) if you want to use slicing directly in the split\nargument.", "The retention metric `ratio` is always 1.0 because it\u2019s computed as the sum of\nkept slot usages divided by the total usage. Dropped slots have zero usage, so\nkept.sum() == total, yielding a ratio of 1.0 even when pruning occurs. This\nmasks whether any slots were actually removed. To fix this, compute retention\nbased on slot counts (e.g., mem_x_new.size(0)/mem_x_concat.size(0) or\nmem_x_new.size(0)/self.mem_size) or assign new slots a small non-zero initial\nusage so that their removal affects the ratio.", "", "", "", "", "The key-only ablation implementation in KeyOnlyMemoryTransformerLayer is causing\na shape mismatch in MultiheadAttention: k has shape [B, mem_len+T, E] while v is\nset to only the current input x ([B, T, E]), triggering an assertion error. To\nfix this, ensure that the value tensor has the same sequence length as the key\ntensor. For example, change the line `v = x` to `v = torch.cat([mem, x],\ndim=1)`, or pad the memory portion of v with zeros, so that v and k have\nidentical lengths before calling MultiheadAttention.", "", "", "", "", "", "The script executed successfully through all training and validation loops for\nboth ablations (entropy and gradient-based) across all three datasets and two\nepochs each. There were no crashes or exceptions, and the val_loss metrics were\nprinted as expected. However, the entropy-based compressive memory ablation\nconsistently underperformed the gradient-based baseline: the entropy model\u2019s\nvalidation loss remained higher across PG19 (2.7013 vs. 2.5031), arXiv (2.4968\nvs. 2.3988), and Wikitext (1.3448 vs. 1.2632). This suggests that while the\npipeline is correct, the proposed entropy-guided memory compression did not\nyield improvements in this initial run. Further investigation is needed to\nverify the entropy calculation, memory pruning logic, and hyperparameter\nsettings.", ""], "exc_type": [null, null, "ClientPayloadError", null, null, null, null, null, "TimeoutError", "ValueError", null, null, null, null, null, "AssertionError", null, null, null, null, null, null, null], "exc_info": [null, null, {"args": ["Response payload is not completed: <ContentLengthError: 400, message='Not enough data to satisfy content length header.'>. ConnectionResetError(104, 'Connection reset by peer')"]}, null, null, null, null, null, {"args": []}, {"args": ["Bad split: train[:200]. Available splits: ['train', 'validation', 'test']"]}, null, null, null, null, null, {"args": ["key shape torch.Size([64, 1, 32]) does not match value shape torch.Size([32, 1, 32])"]}, null, null, null, null, null, null, null], "exc_stack": [null, null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 136, "<module>", "train_samples = list(itertools.islice(train_stream, 200))"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/iterable_dataset.py", 2270, "__iter__", "for key, example in ex_iterable:"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/iterable_dataset.py", 1856, "__iter__", "for key, pa_table in self._iter_arrow():"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/iterable_dataset.py", 1879, "_iter_arrow", "for key, pa_table in self.ex_iterable._iter_arrow():"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/iterable_dataset.py", 476, "_iter_arrow", "for key, pa_table in iterator:"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/iterable_dataset.py", 128, "_convert_to_arrow", "for key, example in iterator:"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/iterable_dataset.py", 222, "__iter__", "for key_example in islice(self.generate_examples_fn(**gen_kwags), shard_example_idx_start, None):"], ["/home/chenhui/.cache/huggingface/modules/datasets_modules/datasets/pg19/fb74320038a3c19e3cc87375222fc75ed3c8dc5a739b3e8dc835736388a7a882/pg19.py", 127, "_generate_examples", "id2metadata = {row[\"_id\"]: row for row in reader}"], ["/home/chenhui/.cache/huggingface/modules/datasets_modules/datasets/pg19/fb74320038a3c19e3cc87375222fc75ed3c8dc5a739b3e8dc835736388a7a882/pg19.py", 127, "<dictcomp>", "id2metadata = {row[\"_id\"]: row for row in reader}"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/csv.py", 111, "__next__", "row = next(self.reader)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/fsspec/implementations/http.py", 627, "read", "return super().read(length)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/fsspec/spec.py", 2083, "read", "out = self.cache._fetch(self.loc, self.loc + length)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/fsspec/caching.py", 496, "_fetch", "self.cache = self.fetcher(start, bend)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/fsspec/asyn.py", 118, "wrapper", "return sync(self.loop, func, *args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/fsspec/asyn.py", 103, "sync", "raise return_result"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/fsspec/asyn.py", 56, "_runner", "result[0] = await coro"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/fsspec/implementations/http.py", 696, "async_fetch_range", "out = await r.read()"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/aiohttp/client_reqrep.py", 671, "read", "self._body = await self.content.read()"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/aiohttp/streams.py", 418, "read", "block = await self.readany()"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/aiohttp/streams.py", 440, "readany", "await self._wait(\"readany\")"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/aiohttp/streams.py", 347, "_wait", "await waiter"]], null, null, null, null, null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 129, "<module>", "train_ds = load_dataset(ds_name, cfg, split=train_split)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2084, "load_dataset", "builder_instance.download_and_prepare("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/builder.py", 925, "download_and_prepare", "self._download_and_prepare("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/builder.py", 1649, "_download_and_prepare", "super()._download_and_prepare("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/builder.py", 979, "_download_and_prepare", "split_generators = self._split_generators(dl_manager, **split_generators_kwargs)"], ["/home/chenhui/.cache/huggingface/modules/datasets_modules/datasets/pg19/fb74320038a3c19e3cc87375222fc75ed3c8dc5a739b3e8dc835736388a7a882/pg19.py", 93, "_split_generators", "downloaded_files = dl_manager.download(files)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/download/download_manager.py", 159, "download", "downloaded_path_or_paths = map_nested("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/utils/py_utils.py", 521, "map_nested", "mapped = ["], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/utils/py_utils.py", 522, "<listcomp>", "_single_map_nested((function, obj, batched, batch_size, types, None, True, None))"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/utils/py_utils.py", 409, "_single_map_nested", "mapped = [_single_map_nested((function, v, batched, batch_size, types, None, True, None)) for v in pbar]"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/utils/py_utils.py", 409, "<listcomp>", "mapped = [_single_map_nested((function, v, batched, batch_size, types, None, True, None)) for v in pbar]"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/utils/py_utils.py", 405, "_single_map_nested", "return {"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/utils/py_utils.py", 406, "<dictcomp>", "k: _single_map_nested((function, v, batched, batch_size, types, None, True, None)) for k, v in pbar"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/utils/py_utils.py", 381, "_single_map_nested", "return function([data_struct])[0]"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/download/download_manager.py", 219, "_download_batched", "return ["], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/download/download_manager.py", 220, "<listcomp>", "self._download_single(url_or_filename, download_config=download_config)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/download/download_manager.py", 229, "_download_single", "out = cached_path(url_or_filename, download_config=download_config)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/utils/file_utils.py", 214, "cached_path", "output_path = get_from_cache("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/utils/file_utils.py", 415, "get_from_cache", "fsspec_get(url, temp_file, storage_options=storage_options, desc=download_desc, disable_tqdm=disable_tqdm)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/utils/file_utils.py", 339, "fsspec_get", "fs.get_file(path, temp_file.name, callback=callback)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/fsspec/asyn.py", 118, "wrapper", "return sync(self.loop, func, *args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/fsspec/asyn.py", 91, "sync", "if event.wait(1):"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/threading.py", 629, "wait", "signaled = self._cond.wait(timeout)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/threading.py", 331, "wait", "gotit = waiter.acquire(True, timeout)"]], [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 125, "<module>", "raw_train = load_dataset(ds_name, cfg, split=train_split, streaming=True)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2081, "load_dataset", "return builder_instance.as_streaming_dataset(split=split)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/builder.py", 1272, "as_streaming_dataset", "raise ValueError(f\"Bad split: {split}. Available splits: {list(splits_generators)}\")"]], null, null, null, null, null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 234, "<module>", "logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["runfile.py", 150, "forward", "out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["runfile.py", 114, "forward", "attn_out, attn_w = self.attn("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/activation.py", 1368, "forward", "attn_output, attn_output_weights = F.multi_head_attention_forward("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/functional.py", 6086, "multi_head_attention_forward", "assert ("]], null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "pg19", "final_value": 2.4175, "best_value": 2.4175}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.5532, "best_value": 2.5532}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.3883, "best_value": 1.3883}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "pg19", "final_value": 2.7136, "best_value": 2.7136}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.5154, "best_value": 2.5154}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.3526, "best_value": 1.3526}]}, {"metric_name": "training memory retention ratio", "lower_is_better": false, "description": "Training Memory Retention Ratio", "data": [{"dataset_name": "pg19", "final_value": 0.7724, "best_value": 0.7724}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7811, "best_value": 0.7811}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.768, "best_value": 0.768}]}, {"metric_name": "validation memory retention ratio", "lower_is_better": false, "description": "Validation Memory Retention Ratio", "data": [{"dataset_name": "pg19", "final_value": 0.7729, "best_value": 0.7729}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7805, "best_value": 0.7805}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.768, "best_value": 0.768}]}, {"metric_name": "training entropy-weighted memory efficiency", "lower_is_better": false, "description": "Training Entropy-Weighted Memory Efficiency", "data": [{"dataset_name": "pg19", "final_value": 3.7555, "best_value": 3.7555}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.6297, "best_value": 3.6297}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.8879, "best_value": 3.8879}]}, {"metric_name": "validation entropy-weighted memory efficiency", "lower_is_better": false, "description": "Validation Entropy-Weighted Memory Efficiency", "data": [{"dataset_name": "pg19", "final_value": 3.7426, "best_value": 3.7426}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.6209, "best_value": 3.6209}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.8817, "best_value": 3.8817}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "pg19 (original model)", "final_value": 2.4175, "best_value": 2.4175}, {"dataset_name": "scientific_papers_arxiv (original model)", "final_value": 2.5532, "best_value": 2.5532}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (original model)", "final_value": 1.3883, "best_value": 1.3883}, {"dataset_name": "pg19 (recency model)", "final_value": 2.4257, "best_value": 2.4257}, {"dataset_name": "scientific_papers_arxiv (recency model)", "final_value": 2.528, "best_value": 2.528}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (recency model)", "final_value": 1.3901, "best_value": 1.3901}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "pg19 (original model)", "final_value": 2.7136, "best_value": 2.7136}, {"dataset_name": "scientific_papers_arxiv (original model)", "final_value": 2.5154, "best_value": 2.5154}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (original model)", "final_value": 1.3526, "best_value": 1.3526}, {"dataset_name": "pg19 (recency model)", "final_value": 2.7338, "best_value": 2.7338}, {"dataset_name": "scientific_papers_arxiv (recency model)", "final_value": 2.4929, "best_value": 2.4929}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (recency model)", "final_value": 1.3529, "best_value": 1.3529}]}, {"metric_name": "training Memory Retention Ratio", "lower_is_better": false, "description": "Final training Memory Retention Ratio", "data": [{"dataset_name": "pg19 (original model)", "final_value": 0.7724, "best_value": 0.7724}, {"dataset_name": "scientific_papers_arxiv (original model)", "final_value": 0.7811, "best_value": 0.7811}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (original model)", "final_value": 0.768, "best_value": 0.768}, {"dataset_name": "pg19 (recency model)", "final_value": 0.7502, "best_value": 0.7502}, {"dataset_name": "scientific_papers_arxiv (recency model)", "final_value": 0.7502, "best_value": 0.7502}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (recency model)", "final_value": 0.7502, "best_value": 0.7502}]}, {"metric_name": "validation Memory Retention Ratio", "lower_is_better": false, "description": "Final validation Memory Retention Ratio", "data": [{"dataset_name": "pg19 (original model)", "final_value": 0.7729, "best_value": 0.7729}, {"dataset_name": "scientific_papers_arxiv (original model)", "final_value": 0.7805, "best_value": 0.7805}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (original model)", "final_value": 0.768, "best_value": 0.768}, {"dataset_name": "pg19 (recency model)", "final_value": 0.7502, "best_value": 0.7502}, {"dataset_name": "scientific_papers_arxiv (recency model)", "final_value": 0.7502, "best_value": 0.7502}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (recency model)", "final_value": 0.7502, "best_value": 0.7502}]}, {"metric_name": "training Entropy-Weighted Memory Efficiency", "lower_is_better": false, "description": "Final training Entropy-Weighted Memory Efficiency", "data": [{"dataset_name": "pg19 (original model)", "final_value": 3.7555, "best_value": 3.7555}, {"dataset_name": "scientific_papers_arxiv (original model)", "final_value": 3.6297, "best_value": 3.6297}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (original model)", "final_value": 3.8879, "best_value": 3.8879}, {"dataset_name": "pg19 (recency model)", "final_value": 3.7758, "best_value": 3.7758}, {"dataset_name": "scientific_papers_arxiv (recency model)", "final_value": 3.6059, "best_value": 3.6059}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (recency model)", "final_value": 3.8718, "best_value": 3.8718}]}, {"metric_name": "validation Entropy-Weighted Memory Efficiency", "lower_is_better": false, "description": "Final validation Entropy-Weighted Memory Efficiency", "data": [{"dataset_name": "pg19 (original model)", "final_value": 3.7426, "best_value": 3.7426}, {"dataset_name": "scientific_papers_arxiv (original model)", "final_value": 3.6209, "best_value": 3.6209}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (original model)", "final_value": 3.8817, "best_value": 3.8817}, {"dataset_name": "pg19 (recency model)", "final_value": 3.746, "best_value": 3.746}, {"dataset_name": "scientific_papers_arxiv (recency model)", "final_value": 3.6063, "best_value": 3.6063}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (recency model)", "final_value": 3.8546, "best_value": 3.8546}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "pg19 (entropy_based)", "final_value": 2.4087, "best_value": 2.4087}, {"dataset_name": "scientific_papers_arxiv (entropy_based)", "final_value": 2.5389, "best_value": 2.5389}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (entropy_based)", "final_value": 1.3843, "best_value": 1.3843}, {"dataset_name": "pg19 (random_retention)", "final_value": 2.4129, "best_value": 2.4129}, {"dataset_name": "scientific_papers_arxiv (random_retention)", "final_value": 2.5428, "best_value": 2.5428}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (random_retention)", "final_value": 1.4146, "best_value": 1.4146}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "pg19 (entropy_based)", "final_value": 2.6569, "best_value": 2.6569}, {"dataset_name": "scientific_papers_arxiv (entropy_based)", "final_value": 2.4968, "best_value": 2.4968}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (entropy_based)", "final_value": 1.3463, "best_value": 1.3463}, {"dataset_name": "pg19 (random_retention)", "final_value": 2.6999, "best_value": 2.6999}, {"dataset_name": "scientific_papers_arxiv (random_retention)", "final_value": 2.5067, "best_value": 2.5067}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (random_retention)", "final_value": 1.3645, "best_value": 1.3645}]}, {"metric_name": "train memory retention ratio", "lower_is_better": false, "description": "Training Memory Retention Ratio", "data": [{"dataset_name": "pg19 (entropy_based)", "final_value": 0.7708, "best_value": 0.7708}, {"dataset_name": "scientific_papers_arxiv (entropy_based)", "final_value": 0.7736, "best_value": 0.7736}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (entropy_based)", "final_value": 0.7711, "best_value": 0.7711}, {"dataset_name": "pg19 (random_retention)", "final_value": 0.7501, "best_value": 0.7501}, {"dataset_name": "scientific_papers_arxiv (random_retention)", "final_value": 0.75, "best_value": 0.75}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (random_retention)", "final_value": 0.7503, "best_value": 0.7503}]}, {"metric_name": "validation memory retention ratio", "lower_is_better": false, "description": "Validation Memory Retention Ratio", "data": [{"dataset_name": "pg19 (entropy_based)", "final_value": 0.7702, "best_value": 0.7702}, {"dataset_name": "scientific_papers_arxiv (entropy_based)", "final_value": 0.7772, "best_value": 0.7772}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (entropy_based)", "final_value": 0.7724, "best_value": 0.7724}, {"dataset_name": "pg19 (random_retention)", "final_value": 0.75, "best_value": 0.75}, {"dataset_name": "scientific_papers_arxiv (random_retention)", "final_value": 0.7502, "best_value": 0.7502}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (random_retention)", "final_value": 0.75, "best_value": 0.75}]}, {"metric_name": "train entropy-weighted memory efficiency", "lower_is_better": false, "description": "Training Entropy-Weighted Memory Efficiency", "data": [{"dataset_name": "pg19 (entropy_based)", "final_value": 3.8007, "best_value": 3.8007}, {"dataset_name": "scientific_papers_arxiv (entropy_based)", "final_value": 3.7238, "best_value": 3.7238}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (entropy_based)", "final_value": 3.8791, "best_value": 3.8791}, {"dataset_name": "pg19 (random_retention)", "final_value": 3.6122, "best_value": 3.6122}, {"dataset_name": "scientific_papers_arxiv (random_retention)", "final_value": 3.5321, "best_value": 3.5321}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (random_retention)", "final_value": 3.7214, "best_value": 3.7214}]}, {"metric_name": "validation entropy-weighted memory efficiency", "lower_is_better": false, "description": "Validation Entropy-Weighted Memory Efficiency", "data": [{"dataset_name": "pg19 (entropy_based)", "final_value": 3.7955, "best_value": 3.7955}, {"dataset_name": "scientific_papers_arxiv (entropy_based)", "final_value": 3.7111, "best_value": 3.7111}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (entropy_based)", "final_value": 3.8611, "best_value": 3.8611}, {"dataset_name": "pg19 (random_retention)", "final_value": 3.6116, "best_value": 3.6116}, {"dataset_name": "scientific_papers_arxiv (random_retention)", "final_value": 3.4629, "best_value": 3.4629}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (random_retention)", "final_value": 3.7117, "best_value": 3.7117}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 2.4175, "best_value": 2.4175}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 2.5532, "best_value": 2.5532}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 1.3883, "best_value": 1.3883}, {"dataset_name": "pg19 (ffn_removed)", "final_value": 2.617, "best_value": 2.617}, {"dataset_name": "scientific_papers_arxiv (ffn_removed)", "final_value": 2.6072, "best_value": 2.6072}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (ffn_removed)", "final_value": 1.4621, "best_value": 1.4621}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 2.7136, "best_value": 2.7136}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 2.5154, "best_value": 2.5154}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 1.3526, "best_value": 1.3526}, {"dataset_name": "pg19 (ffn_removed)", "final_value": 2.8088, "best_value": 2.8088}, {"dataset_name": "scientific_papers_arxiv (ffn_removed)", "final_value": 2.5527, "best_value": 2.5527}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (ffn_removed)", "final_value": 1.4069, "best_value": 1.4069}]}, {"metric_name": "training Memory Retention Ratio", "lower_is_better": false, "description": "Final training Memory Retention Ratio", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 0.7724, "best_value": 0.7724}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 0.7811, "best_value": 0.7811}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 0.768, "best_value": 0.768}, {"dataset_name": "pg19 (ffn_removed)", "final_value": 0.7818, "best_value": 0.7818}, {"dataset_name": "scientific_papers_arxiv (ffn_removed)", "final_value": 0.7928, "best_value": 0.7928}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (ffn_removed)", "final_value": 0.773, "best_value": 0.773}]}, {"metric_name": "validation Memory Retention Ratio", "lower_is_better": false, "description": "Final validation Memory Retention Ratio", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 0.7729, "best_value": 0.7729}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 0.7805, "best_value": 0.7805}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 0.768, "best_value": 0.768}, {"dataset_name": "pg19 (ffn_removed)", "final_value": 0.7798, "best_value": 0.7798}, {"dataset_name": "scientific_papers_arxiv (ffn_removed)", "final_value": 0.7945, "best_value": 0.7945}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (ffn_removed)", "final_value": 0.7708, "best_value": 0.7708}]}, {"metric_name": "training Entropy-Weighted Memory Efficiency", "lower_is_better": false, "description": "Final training Entropy-Weighted Memory Efficiency", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 3.7555, "best_value": 3.7555}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 3.6297, "best_value": 3.6297}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 3.8879, "best_value": 3.8879}, {"dataset_name": "pg19 (ffn_removed)", "final_value": 3.562, "best_value": 3.562}, {"dataset_name": "scientific_papers_arxiv (ffn_removed)", "final_value": 3.2237, "best_value": 3.2237}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (ffn_removed)", "final_value": 3.8292, "best_value": 3.8292}]}, {"metric_name": "validation Entropy-Weighted Memory Efficiency", "lower_is_better": false, "description": "Final validation Entropy-Weighted Memory Efficiency", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 3.7426, "best_value": 3.7426}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 3.6209, "best_value": 3.6209}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 3.8817, "best_value": 3.8817}, {"dataset_name": "pg19 (ffn_removed)", "final_value": 3.5978, "best_value": 3.5978}, {"dataset_name": "scientific_papers_arxiv (ffn_removed)", "final_value": 3.1855, "best_value": 3.1855}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (ffn_removed)", "final_value": 3.8655, "best_value": 3.8655}]}]}, {"metric_names": [{"metric_name": "training Memory Retention Ratio", "lower_is_better": false, "description": "Memory Retention Ratio on the training set", "data": [{"dataset_name": "pg19", "final_value": 0.77, "best_value": 0.77}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.774, "best_value": 0.774}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.7688, "best_value": 0.7688}]}, {"metric_name": "validation Memory Retention Ratio", "lower_is_better": false, "description": "Memory Retention Ratio on the validation set", "data": [{"dataset_name": "pg19", "final_value": 0.771, "best_value": 0.771}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7713, "best_value": 0.7713}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.7686, "best_value": 0.7686}]}, {"metric_name": "training Entropy-Weighted Memory Efficiency", "lower_is_better": false, "description": "Entropy-Weighted Memory Efficiency on the training set", "data": [{"dataset_name": "pg19", "final_value": 3.7777, "best_value": 3.7777}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.7573, "best_value": 3.7573}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.8677, "best_value": 3.8677}]}, {"metric_name": "validation Entropy-Weighted Memory Efficiency", "lower_is_better": false, "description": "Entropy-Weighted Memory Efficiency on the validation set", "data": [{"dataset_name": "pg19", "final_value": 3.7525, "best_value": 3.7525}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.7693, "best_value": 3.7693}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.8738, "best_value": 3.8738}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss.", "data": [{"dataset_name": "pg19 (original)", "final_value": 2.4175, "best_value": 2.4175}, {"dataset_name": "scientific_papers_arxiv (original)", "final_value": 2.5532, "best_value": 2.5532}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (original)", "final_value": 1.3883, "best_value": 1.3883}, {"dataset_name": "pg19 (no_memory_attention)", "final_value": 2.354, "best_value": 2.354}, {"dataset_name": "scientific_papers_arxiv (no_memory_attention)", "final_value": 2.5024, "best_value": 2.5024}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (no_memory_attention)", "final_value": 1.3884, "best_value": 1.3884}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss.", "data": [{"dataset_name": "pg19 (original)", "final_value": 2.7136, "best_value": 2.7136}, {"dataset_name": "scientific_papers_arxiv (original)", "final_value": 2.5154, "best_value": 2.5154}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (original)", "final_value": 1.3526, "best_value": 1.3526}, {"dataset_name": "pg19 (no_memory_attention)", "final_value": 2.6604, "best_value": 2.6604}, {"dataset_name": "scientific_papers_arxiv (no_memory_attention)", "final_value": 2.4547, "best_value": 2.4547}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (no_memory_attention)", "final_value": 1.352, "best_value": 1.352}]}, {"metric_name": "training memory retention ratio", "lower_is_better": false, "description": "Final training memory retention ratio.", "data": [{"dataset_name": "pg19 (original)", "final_value": 0.7724, "best_value": 0.7724}, {"dataset_name": "scientific_papers_arxiv (original)", "final_value": 0.7811, "best_value": 0.7811}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (original)", "final_value": 0.768, "best_value": 0.768}, {"dataset_name": "pg19 (no_memory_attention)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "scientific_papers_arxiv (no_memory_attention)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (no_memory_attention)", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation memory retention ratio", "lower_is_better": false, "description": "Final validation memory retention ratio.", "data": [{"dataset_name": "pg19 (original)", "final_value": 0.7729, "best_value": 0.7729}, {"dataset_name": "scientific_papers_arxiv (original)", "final_value": 0.7805, "best_value": 0.7805}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (original)", "final_value": 0.768, "best_value": 0.768}, {"dataset_name": "pg19 (no_memory_attention)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "scientific_papers_arxiv (no_memory_attention)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (no_memory_attention)", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training entropy-weighted memory efficiency", "lower_is_better": false, "description": "Final training entropy-weighted memory efficiency.", "data": [{"dataset_name": "pg19 (original)", "final_value": 3.7555, "best_value": 3.7555}, {"dataset_name": "scientific_papers_arxiv (original)", "final_value": 3.6297, "best_value": 3.6297}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (original)", "final_value": 3.8879, "best_value": 3.8879}, {"dataset_name": "pg19 (no_memory_attention)", "final_value": 3.2057, "best_value": 3.2057}, {"dataset_name": "scientific_papers_arxiv (no_memory_attention)", "final_value": 3.1361, "best_value": 3.1361}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (no_memory_attention)", "final_value": 3.298, "best_value": 3.298}]}, {"metric_name": "validation entropy-weighted memory efficiency", "lower_is_better": false, "description": "Final validation entropy-weighted memory efficiency.", "data": [{"dataset_name": "pg19 (original)", "final_value": 3.7426, "best_value": 3.7426}, {"dataset_name": "scientific_papers_arxiv (original)", "final_value": 3.6209, "best_value": 3.6209}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (original)", "final_value": 3.8817, "best_value": 3.8817}, {"dataset_name": "pg19 (no_memory_attention)", "final_value": 3.1635, "best_value": 3.1635}, {"dataset_name": "scientific_papers_arxiv (no_memory_attention)", "final_value": 3.1455, "best_value": 3.1455}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (no_memory_attention)", "final_value": 3.27, "best_value": 3.27}]}]}, {"metric_names": [{"metric_name": "final train loss (entropy)", "lower_is_better": true, "description": "Cross-entropy loss on the training set using entropy mode.", "data": [{"dataset_name": "pg19", "final_value": 2.4175, "best_value": 2.4175}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.5532, "best_value": 2.5532}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.3883, "best_value": 1.3883}]}, {"metric_name": "final validation loss (entropy)", "lower_is_better": true, "description": "Cross-entropy loss on the validation set using entropy mode.", "data": [{"dataset_name": "pg19", "final_value": 2.7136, "best_value": 2.7136}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.5154, "best_value": 2.5154}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.3526, "best_value": 1.3526}]}, {"metric_name": "final train Memory Retention Ratio (entropy)", "lower_is_better": false, "description": "Memory Retention Ratio on the training set using entropy mode.", "data": [{"dataset_name": "pg19", "final_value": 0.7724, "best_value": 0.7724}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7811, "best_value": 0.7811}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.768, "best_value": 0.768}]}, {"metric_name": "final validation Memory Retention Ratio (entropy)", "lower_is_better": false, "description": "Memory Retention Ratio on the validation set using entropy mode.", "data": [{"dataset_name": "pg19", "final_value": 0.7729, "best_value": 0.7729}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7805, "best_value": 0.7805}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.768, "best_value": 0.768}]}, {"metric_name": "final train Entropy-Weighted Memory Efficiency (entropy)", "lower_is_better": false, "description": "Entropy-Weighted Memory Efficiency on the training set using entropy mode.", "data": [{"dataset_name": "pg19", "final_value": 3.7555, "best_value": 3.7555}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.6297, "best_value": 3.6297}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.8879, "best_value": 3.8879}]}, {"metric_name": "final validation Entropy-Weighted Memory Efficiency (entropy)", "lower_is_better": false, "description": "Entropy-Weighted Memory Efficiency on the validation set using entropy mode.", "data": [{"dataset_name": "pg19", "final_value": 3.7426, "best_value": 3.7426}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.6209, "best_value": 3.6209}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.8817, "best_value": 3.8817}]}, {"metric_name": "final train loss (norm)", "lower_is_better": true, "description": "Cross-entropy loss on the training set using norm mode.", "data": [{"dataset_name": "pg19", "final_value": 2.4182, "best_value": 2.4182}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.5299, "best_value": 2.5299}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.3924, "best_value": 1.3924}]}, {"metric_name": "final validation loss (norm)", "lower_is_better": true, "description": "Cross-entropy loss on the validation set using norm mode.", "data": [{"dataset_name": "pg19", "final_value": 2.7067, "best_value": 2.7067}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.4918, "best_value": 2.4918}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.3542, "best_value": 1.3542}]}, {"metric_name": "final train Memory Retention Ratio (norm)", "lower_is_better": false, "description": "Memory Retention Ratio on the training set using norm mode.", "data": [{"dataset_name": "pg19", "final_value": 0.7755, "best_value": 0.7755}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7763, "best_value": 0.7763}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.7636, "best_value": 0.7636}]}, {"metric_name": "final validation Memory Retention Ratio (norm)", "lower_is_better": false, "description": "Memory Retention Ratio on the validation set using norm mode.", "data": [{"dataset_name": "pg19", "final_value": 0.7741, "best_value": 0.7741}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.776, "best_value": 0.776}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.7629, "best_value": 0.7629}]}, {"metric_name": "final train Entropy-Weighted Memory Efficiency (norm)", "lower_is_better": false, "description": "Entropy-Weighted Memory Efficiency on the training set using norm mode.", "data": [{"dataset_name": "pg19", "final_value": 3.5444, "best_value": 3.5444}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.3076, "best_value": 3.3076}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.567, "best_value": 3.567}]}, {"metric_name": "final validation Entropy-Weighted Memory Efficiency (norm)", "lower_is_better": false, "description": "Entropy-Weighted Memory Efficiency on the validation set using norm mode.", "data": [{"dataset_name": "pg19", "final_value": 3.5347, "best_value": 3.5347}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.2953, "best_value": 3.2953}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.5619, "best_value": 3.5619}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training retention", "lower_is_better": false, "description": "Fraction of tokens retained in the training set", "data": [{"dataset_name": "pg19", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "scientific_papers_arxiv", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation retention", "lower_is_better": false, "description": "Fraction of tokens retained in the validation set", "data": [{"dataset_name": "pg19", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "scientific_papers_arxiv", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Cross-entropy loss on the training set", "data": [{"dataset_name": "pg19", "final_value": 2.431, "best_value": 2.431}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.5507, "best_value": 2.5507}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.3891, "best_value": 1.3891}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Cross-entropy loss on the validation set", "data": [{"dataset_name": "pg19", "final_value": 2.734, "best_value": 2.734}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.5144, "best_value": 2.5144}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.3523, "best_value": 1.3523}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "training loss", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 2.4175, "best_value": 2.4175}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 2.5532, "best_value": 2.5532}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 1.3883, "best_value": 1.3883}, {"dataset_name": "pg19 (feedforward_identity)", "final_value": 2.435, "best_value": 2.435}, {"dataset_name": "scientific_papers_arxiv (feedforward_identity)", "final_value": 2.5287, "best_value": 2.5287}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (feedforward_identity)", "final_value": 1.4038, "best_value": 1.4038}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "validation loss", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 2.7136, "best_value": 2.7136}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 2.5154, "best_value": 2.5154}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 1.3526, "best_value": 1.3526}, {"dataset_name": "pg19 (feedforward_identity)", "final_value": 2.759, "best_value": 2.759}, {"dataset_name": "scientific_papers_arxiv (feedforward_identity)", "final_value": 2.498, "best_value": 2.498}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (feedforward_identity)", "final_value": 1.3704, "best_value": 1.3704}]}, {"metric_name": "train memory retention ratio", "lower_is_better": false, "description": "training memory retention ratio", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 0.7724, "best_value": 0.7724}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 0.7811, "best_value": 0.7811}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 0.768, "best_value": 0.768}, {"dataset_name": "pg19 (feedforward_identity)", "final_value": 0.7701, "best_value": 0.7701}, {"dataset_name": "scientific_papers_arxiv (feedforward_identity)", "final_value": 0.7733, "best_value": 0.7733}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (feedforward_identity)", "final_value": 0.7696, "best_value": 0.7696}]}, {"metric_name": "validation memory retention ratio", "lower_is_better": false, "description": "validation memory retention ratio", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 0.7729, "best_value": 0.7729}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 0.7805, "best_value": 0.7805}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 0.768, "best_value": 0.768}, {"dataset_name": "pg19 (feedforward_identity)", "final_value": 0.7706, "best_value": 0.7706}, {"dataset_name": "scientific_papers_arxiv (feedforward_identity)", "final_value": 0.7734, "best_value": 0.7734}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (feedforward_identity)", "final_value": 0.7695, "best_value": 0.7695}]}, {"metric_name": "train entropy-weighted memory efficiency", "lower_is_better": false, "description": "training entropy-weighted memory efficiency", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 3.7555, "best_value": 3.7555}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 3.6297, "best_value": 3.6297}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 3.8879, "best_value": 3.8879}, {"dataset_name": "pg19 (feedforward_identity)", "final_value": 3.7913, "best_value": 3.7913}, {"dataset_name": "scientific_papers_arxiv (feedforward_identity)", "final_value": 3.7431, "best_value": 3.7431}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (feedforward_identity)", "final_value": 3.8564, "best_value": 3.8564}]}, {"metric_name": "validation entropy-weighted memory efficiency", "lower_is_better": false, "description": "validation entropy-weighted memory efficiency", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 3.7426, "best_value": 3.7426}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 3.6209, "best_value": 3.6209}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 3.8817, "best_value": 3.8817}, {"dataset_name": "pg19 (feedforward_identity)", "final_value": 3.7424, "best_value": 3.7424}, {"dataset_name": "scientific_papers_arxiv (feedforward_identity)", "final_value": 3.7238, "best_value": 3.7238}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (feedforward_identity)", "final_value": 3.8592, "best_value": 3.8592}]}]}, {"metric_names": [{"metric_name": "Baseline Training Memory Retention Ratio", "lower_is_better": false, "description": "Ratio of memory retained during training for the baseline ablation", "data": [{"dataset_name": "pg19", "final_value": 0.7724, "best_value": 0.7724}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7811, "best_value": 0.7811}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.768, "best_value": 0.768}]}, {"metric_name": "Baseline Validation Memory Retention Ratio", "lower_is_better": false, "description": "Ratio of memory retained during validation for the baseline ablation", "data": [{"dataset_name": "pg19", "final_value": 0.7729, "best_value": 0.7729}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7805, "best_value": 0.7805}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.768, "best_value": 0.768}]}, {"metric_name": "Baseline Training Entropy-Weighted Memory Efficiency", "lower_is_better": false, "description": "Entropy-weighted memory efficiency during training for the baseline ablation", "data": [{"dataset_name": "pg19", "final_value": 3.7555, "best_value": 3.7555}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.6297, "best_value": 3.6297}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.8879, "best_value": 3.8879}]}, {"metric_name": "Baseline Validation Entropy-Weighted Memory Efficiency", "lower_is_better": false, "description": "Entropy-weighted memory efficiency during validation for the baseline ablation", "data": [{"dataset_name": "pg19", "final_value": 3.7426, "best_value": 3.7426}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.6209, "best_value": 3.6209}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.8817, "best_value": 3.8817}]}, {"metric_name": "Continuous Memory Training Memory Retention Ratio", "lower_is_better": false, "description": "Ratio of memory retained during training for the continuous_memory ablation", "data": [{"dataset_name": "pg19", "final_value": 0.622, "best_value": 0.622}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.6375, "best_value": 0.6375}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.6212, "best_value": 0.6212}]}, {"metric_name": "Continuous Memory Validation Memory Retention Ratio", "lower_is_better": false, "description": "Ratio of memory retained during validation for the continuous_memory ablation", "data": [{"dataset_name": "pg19", "final_value": 0.6314, "best_value": 0.6314}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.6406, "best_value": 0.6406}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.6248, "best_value": 0.6248}]}, {"metric_name": "Continuous Memory Training Entropy-Weighted Memory Efficiency", "lower_is_better": false, "description": "Entropy-weighted memory efficiency during training for the continuous_memory ablation", "data": [{"dataset_name": "pg19", "final_value": 4.3833, "best_value": 4.3833}, {"dataset_name": "scientific_papers_arxiv", "final_value": 4.3594, "best_value": 4.3594}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 4.4048, "best_value": 4.4048}]}, {"metric_name": "Continuous Memory Validation Entropy-Weighted Memory Efficiency", "lower_is_better": false, "description": "Entropy-weighted memory efficiency during validation for the continuous_memory ablation", "data": [{"dataset_name": "pg19", "final_value": 4.3415, "best_value": 4.3415}, {"dataset_name": "scientific_papers_arxiv", "final_value": 4.358, "best_value": 4.358}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 4.4029, "best_value": 4.4029}]}]}, {"metric_names": [{"metric_name": "train retention ratio", "lower_is_better": false, "description": "Final retention ratio on the training set", "data": [{"dataset_name": "pg19", "final_value": 0.7502, "best_value": 0.7502}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7502, "best_value": 0.7502}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.7502, "best_value": 0.7502}]}, {"metric_name": "validation retention ratio", "lower_is_better": false, "description": "Final retention ratio on the validation set", "data": [{"dataset_name": "pg19", "final_value": 0.7502, "best_value": 0.7502}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7502, "best_value": 0.7502}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.7502, "best_value": 0.7502}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Final loss on the training set", "data": [{"dataset_name": "pg19", "final_value": 2.431, "best_value": 2.431}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.5507, "best_value": 2.5507}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.3891, "best_value": 1.3891}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final loss on the validation set", "data": [{"dataset_name": "pg19", "final_value": 2.734, "best_value": 2.734}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.5144, "best_value": 2.5144}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.3523, "best_value": 1.3523}]}]}, {"metric_names": [{"metric_name": "training memory retention ratio", "lower_is_better": false, "description": "Ratio of memory retained during training.", "data": [{"dataset_name": "pg19", "final_value": 0.7733, "best_value": 0.7733}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7812, "best_value": 0.7812}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.7722, "best_value": 0.7722}]}, {"metric_name": "validation memory retention ratio", "lower_is_better": false, "description": "Ratio of memory retained during validation.", "data": [{"dataset_name": "pg19", "final_value": 0.7728, "best_value": 0.7729}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.782, "best_value": 0.782}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.7726, "best_value": 0.7726}]}, {"metric_name": "training entropy-weighted memory efficiency", "lower_is_better": false, "description": "Efficiency of memory usage weighted by entropy during training.", "data": [{"dataset_name": "pg19", "final_value": 3.6593, "best_value": 3.7555}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.5224, "best_value": 3.6297}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.7783, "best_value": 3.8879}]}, {"metric_name": "validation entropy-weighted memory efficiency", "lower_is_better": false, "description": "Efficiency of memory usage weighted by entropy during validation.", "data": [{"dataset_name": "pg19", "final_value": 3.6469, "best_value": 3.7426}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.5136, "best_value": 3.6209}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.7528, "best_value": 3.8817}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "pg19", "final_value": 2.3893, "best_value": 2.3893}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.5535, "best_value": 2.5535}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.4152, "best_value": 1.4152}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "pg19", "final_value": 2.7047, "best_value": 2.7047}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.515, "best_value": 2.515}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.3697, "best_value": 1.3697}]}, {"metric_name": "train Memory Retention Ratio", "lower_is_better": false, "description": "Memory Retention Ratio on the training set", "data": [{"dataset_name": "pg19", "final_value": 0.7685, "best_value": 0.7685}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7698, "best_value": 0.7698}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.7676, "best_value": 0.7676}]}, {"metric_name": "validation Memory Retention Ratio", "lower_is_better": false, "description": "Memory Retention Ratio on the validation set", "data": [{"dataset_name": "pg19", "final_value": 0.769, "best_value": 0.769}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.77, "best_value": 0.77}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.7678, "best_value": 0.7678}]}, {"metric_name": "train Entropy-Weighted Memory Efficiency", "lower_is_better": false, "description": "Entropy-Weighted Memory Efficiency on the training set", "data": [{"dataset_name": "pg19", "final_value": 3.859, "best_value": 3.859}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.8249, "best_value": 3.8249}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.8928, "best_value": 3.8928}]}, {"metric_name": "validation Entropy-Weighted Memory Efficiency", "lower_is_better": false, "description": "Entropy-Weighted Memory Efficiency on the validation set", "data": [{"dataset_name": "pg19", "final_value": 3.8377, "best_value": 3.8377}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.8085, "best_value": 3.8085}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.8952, "best_value": 3.8952}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Cross-entropy training loss", "data": [{"dataset_name": "pg19 (Entropy-based memory ablation)", "final_value": 2.4175, "best_value": 2.4175}, {"dataset_name": "scientific_papers_arxiv (Entropy-based memory ablation)", "final_value": 2.5532, "best_value": 2.5532}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (Entropy-based memory ablation)", "final_value": 1.3883, "best_value": 1.3883}, {"dataset_name": "pg19 (Gradient-based memory ablation)", "final_value": 2.0926, "best_value": 2.0926}, {"dataset_name": "scientific_papers_arxiv (Gradient-based memory ablation)", "final_value": 2.4001, "best_value": 2.4001}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (Gradient-based memory ablation)", "final_value": 1.2399, "best_value": 1.2399}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Cross-entropy validation loss", "data": [{"dataset_name": "pg19 (Entropy-based memory ablation)", "final_value": 2.7136, "best_value": 2.7136}, {"dataset_name": "scientific_papers_arxiv (Entropy-based memory ablation)", "final_value": 2.5154, "best_value": 2.5154}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (Entropy-based memory ablation)", "final_value": 1.3526, "best_value": 1.3526}, {"dataset_name": "pg19 (Gradient-based memory ablation)", "final_value": 2.6102, "best_value": 2.6102}, {"dataset_name": "scientific_papers_arxiv (Gradient-based memory ablation)", "final_value": 2.384, "best_value": 2.384}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (Gradient-based memory ablation)", "final_value": 1.2601, "best_value": 1.2601}]}, {"metric_name": "training memory retention ratio", "lower_is_better": false, "description": "Training memory retention ratio", "data": [{"dataset_name": "pg19 (Entropy-based memory ablation)", "final_value": 0.7724, "best_value": 0.7724}, {"dataset_name": "scientific_papers_arxiv (Entropy-based memory ablation)", "final_value": 0.7811, "best_value": 0.7811}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (Entropy-based memory ablation)", "final_value": 0.768, "best_value": 0.768}, {"dataset_name": "pg19 (Gradient-based memory ablation)", "final_value": 0.8401, "best_value": 0.8401}, {"dataset_name": "scientific_papers_arxiv (Gradient-based memory ablation)", "final_value": 0.7968, "best_value": 0.7968}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (Gradient-based memory ablation)", "final_value": 0.8437, "best_value": 0.8437}]}, {"metric_name": "validation memory retention ratio", "lower_is_better": false, "description": "Validation memory retention ratio", "data": [{"dataset_name": "pg19 (Entropy-based memory ablation)", "final_value": 0.7729, "best_value": 0.7729}, {"dataset_name": "scientific_papers_arxiv (Entropy-based memory ablation)", "final_value": 0.7805, "best_value": 0.7805}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (Entropy-based memory ablation)", "final_value": 0.768, "best_value": 0.768}, {"dataset_name": "pg19 (Gradient-based memory ablation)", "final_value": 0.841, "best_value": 0.841}, {"dataset_name": "scientific_papers_arxiv (Gradient-based memory ablation)", "final_value": 0.7948, "best_value": 0.7948}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (Gradient-based memory ablation)", "final_value": 0.8198, "best_value": 0.8198}]}, {"metric_name": "training score-weighted memory efficiency", "lower_is_better": false, "description": "Training score-weighted memory efficiency", "data": [{"dataset_name": "pg19 (Entropy-based memory ablation)", "final_value": 3.7555, "best_value": 3.7555}, {"dataset_name": "scientific_papers_arxiv (Entropy-based memory ablation)", "final_value": 3.6297, "best_value": 3.6297}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (Entropy-based memory ablation)", "final_value": 3.8879, "best_value": 3.8879}, {"dataset_name": "pg19 (Gradient-based memory ablation)", "final_value": 0.0243, "best_value": 0.0243}, {"dataset_name": "scientific_papers_arxiv (Gradient-based memory ablation)", "final_value": 0.023, "best_value": 0.023}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (Gradient-based memory ablation)", "final_value": 0.0133, "best_value": 0.0133}]}, {"metric_name": "validation score-weighted memory efficiency", "lower_is_better": false, "description": "Validation score-weighted memory efficiency", "data": [{"dataset_name": "pg19 (Entropy-based memory ablation)", "final_value": 3.7426, "best_value": 3.7426}, {"dataset_name": "scientific_papers_arxiv (Entropy-based memory ablation)", "final_value": 3.6209, "best_value": 3.6209}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (Entropy-based memory ablation)", "final_value": 3.8817, "best_value": 3.8817}, {"dataset_name": "pg19 (Gradient-based memory ablation)", "final_value": 0.0271, "best_value": 0.0271}, {"dataset_name": "scientific_papers_arxiv (Gradient-based memory ablation)", "final_value": 0.0236, "best_value": 0.0236}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (Gradient-based memory ablation)", "final_value": 0.0125, "best_value": 0.0125}]}]}, {"metric_names": [{"metric_name": "Training Loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 2.4175, "best_value": 2.4175}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 2.5532, "best_value": 2.5532}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 1.3883, "best_value": 1.3883}, {"dataset_name": "pg19 (memory_key_only)", "final_value": 2.3746, "best_value": 2.3746}, {"dataset_name": "scientific_papers_arxiv (memory_key_only)", "final_value": 2.5192, "best_value": 2.5192}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (memory_key_only)", "final_value": 1.3897, "best_value": 1.3897}]}, {"metric_name": "Validation Loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 2.7136, "best_value": 2.7136}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 2.5154, "best_value": 2.5154}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 1.3526, "best_value": 1.3526}, {"dataset_name": "pg19 (memory_key_only)", "final_value": 2.6812, "best_value": 2.6812}, {"dataset_name": "scientific_papers_arxiv (memory_key_only)", "final_value": 2.4786, "best_value": 2.4786}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (memory_key_only)", "final_value": 1.3499, "best_value": 1.3499}]}, {"metric_name": "Training Entropy Retention Ratio", "lower_is_better": false, "description": "Final training entropy retention ratio", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 0.7724, "best_value": 0.7724}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 0.7811, "best_value": 0.7811}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 0.768, "best_value": 0.768}, {"dataset_name": "pg19 (memory_key_only)", "final_value": 0.7716, "best_value": 0.7716}, {"dataset_name": "scientific_papers_arxiv (memory_key_only)", "final_value": 0.7774, "best_value": 0.7774}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (memory_key_only)", "final_value": 0.769, "best_value": 0.769}]}, {"metric_name": "Validation Entropy Retention Ratio", "lower_is_better": false, "description": "Final validation entropy retention ratio", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 0.7729, "best_value": 0.7729}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 0.7805, "best_value": 0.7805}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 0.768, "best_value": 0.768}, {"dataset_name": "pg19 (memory_key_only)", "final_value": 0.7713, "best_value": 0.7713}, {"dataset_name": "scientific_papers_arxiv (memory_key_only)", "final_value": 0.7743, "best_value": 0.7743}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (memory_key_only)", "final_value": 0.7688, "best_value": 0.7688}]}, {"metric_name": "Training Entropy-Weighted Memory Efficiency", "lower_is_better": false, "description": "Final training entropy-weighted memory efficiency", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 3.7555, "best_value": 3.7555}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 3.6297, "best_value": 3.6297}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 3.8879, "best_value": 3.8879}, {"dataset_name": "pg19 (memory_key_only)", "final_value": 3.7321, "best_value": 3.7321}, {"dataset_name": "scientific_papers_arxiv (memory_key_only)", "final_value": 3.7048, "best_value": 3.7048}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (memory_key_only)", "final_value": 3.9001, "best_value": 3.9001}]}, {"metric_name": "Validation Entropy-Weighted Memory Efficiency", "lower_is_better": false, "description": "Final validation entropy-weighted memory efficiency", "data": [{"dataset_name": "pg19 (baseline)", "final_value": 3.7426, "best_value": 3.7426}, {"dataset_name": "scientific_papers_arxiv (baseline)", "final_value": 3.6209, "best_value": 3.6209}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (baseline)", "final_value": 3.8817, "best_value": 3.8817}, {"dataset_name": "pg19 (memory_key_only)", "final_value": 3.6999, "best_value": 3.6999}, {"dataset_name": "scientific_papers_arxiv (memory_key_only)", "final_value": 3.7322, "best_value": 3.7322}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (memory_key_only)", "final_value": 3.8933, "best_value": 3.8933}]}]}, {"metric_names": [{"metric_name": "Training Loss (Entropy-based memory ablation)", "lower_is_better": true, "description": "Cross-entropy loss on the training set under entropy-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 2.4087, "best_value": 2.4087}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.5389, "best_value": 2.5389}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.3843, "best_value": 1.3843}]}, {"metric_name": "Validation Loss (Entropy-based memory ablation)", "lower_is_better": true, "description": "Cross-entropy loss on the validation set under entropy-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 2.6569, "best_value": 2.6569}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.4968, "best_value": 2.4968}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.3463, "best_value": 1.3463}]}, {"metric_name": "Training Memory Retention Ratio (Entropy-based memory ablation)", "lower_is_better": false, "description": "Memory retention ratio on the training set under entropy-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 0.7708, "best_value": 0.7708}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7736, "best_value": 0.7736}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.7711, "best_value": 0.7711}]}, {"metric_name": "Validation Memory Retention Ratio (Entropy-based memory ablation)", "lower_is_better": false, "description": "Memory retention ratio on the validation set under entropy-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 0.7702, "best_value": 0.7702}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7772, "best_value": 0.7772}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.7724, "best_value": 0.7724}]}, {"metric_name": "Training Score-Weighted Memory Efficiency (Entropy-based memory ablation)", "lower_is_better": false, "description": "Score-weighted memory efficiency on the training set under entropy-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 3.8007, "best_value": 3.8007}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.7238, "best_value": 3.7238}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.8791, "best_value": 3.8791}]}, {"metric_name": "Validation Score-Weighted Memory Efficiency (Entropy-based memory ablation)", "lower_is_better": false, "description": "Score-weighted memory efficiency on the validation set under entropy-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 3.7955, "best_value": 3.7955}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.7111, "best_value": 3.7111}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.8611, "best_value": 3.8611}]}, {"metric_name": "Training Loss (Gradient-based memory ablation)", "lower_is_better": true, "description": "Cross-entropy loss on the training set under gradient-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 2.0673, "best_value": 2.0673}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.4086, "best_value": 2.4086}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.2461, "best_value": 1.2461}]}, {"metric_name": "Validation Loss (Gradient-based memory ablation)", "lower_is_better": true, "description": "Cross-entropy loss on the validation set under gradient-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 2.5988, "best_value": 2.5988}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.3885, "best_value": 2.3885}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.2523, "best_value": 1.2523}]}, {"metric_name": "Training Memory Retention Ratio (Gradient-based memory ablation)", "lower_is_better": false, "description": "Memory retention ratio on the training set under gradient-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 0.8399, "best_value": 0.8399}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7971, "best_value": 0.7971}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.8441, "best_value": 0.8441}]}, {"metric_name": "Validation Memory Retention Ratio (Gradient-based memory ablation)", "lower_is_better": false, "description": "Memory retention ratio on the validation set under gradient-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 0.8395, "best_value": 0.8395}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.8021, "best_value": 0.8021}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.8194, "best_value": 0.8194}]}, {"metric_name": "Training Score-Weighted Memory Efficiency (Gradient-based memory ablation)", "lower_is_better": false, "description": "Score-weighted memory efficiency on the training set under gradient-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 0.0245, "best_value": 0.0245}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.0225, "best_value": 0.0225}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.0126, "best_value": 0.0126}]}, {"metric_name": "Validation Score-Weighted Memory Efficiency (Gradient-based memory ablation)", "lower_is_better": false, "description": "Score-weighted memory efficiency on the validation set under gradient-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 0.0269, "best_value": 0.0269}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.0233, "best_value": 0.0233}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.0118, "best_value": 0.0118}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Cross-entropy loss on the training set", "data": [{"dataset_name": "pg19 (entropy-based memory ablation)", "final_value": 2.3873, "best_value": 2.3873}, {"dataset_name": "scientific_papers_arxiv (entropy-based memory ablation)", "final_value": 2.5328, "best_value": 2.5328}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (entropy-based memory ablation)", "final_value": 1.404, "best_value": 1.404}, {"dataset_name": "pg19 (gradient-based memory ablation)", "final_value": 2.0913, "best_value": 2.0913}, {"dataset_name": "scientific_papers_arxiv (gradient-based memory ablation)", "final_value": 2.4323, "best_value": 2.4323}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (gradient-based memory ablation)", "final_value": 1.2384, "best_value": 1.2384}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Cross-entropy loss on the validation set", "data": [{"dataset_name": "pg19 (entropy-based memory ablation)", "final_value": 2.6916, "best_value": 2.6916}, {"dataset_name": "scientific_papers_arxiv (entropy-based memory ablation)", "final_value": 2.5014, "best_value": 2.5014}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (entropy-based memory ablation)", "final_value": 1.3494, "best_value": 1.3494}, {"dataset_name": "pg19 (gradient-based memory ablation)", "final_value": 2.5571, "best_value": 2.5571}, {"dataset_name": "scientific_papers_arxiv (gradient-based memory ablation)", "final_value": 2.4173, "best_value": 2.4173}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (gradient-based memory ablation)", "final_value": 1.2586, "best_value": 1.2586}]}, {"metric_name": "training memory retention ratio", "lower_is_better": false, "description": "Memory retention ratio on the training set", "data": [{"dataset_name": "pg19 (entropy-based memory ablation)", "final_value": 0.7701, "best_value": 0.7701}, {"dataset_name": "scientific_papers_arxiv (entropy-based memory ablation)", "final_value": 0.7795, "best_value": 0.7795}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (entropy-based memory ablation)", "final_value": 0.7691, "best_value": 0.7691}, {"dataset_name": "pg19 (gradient-based memory ablation)", "final_value": 0.8392, "best_value": 0.8392}, {"dataset_name": "scientific_papers_arxiv (gradient-based memory ablation)", "final_value": 0.7988, "best_value": 0.7988}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (gradient-based memory ablation)", "final_value": 0.8435, "best_value": 0.8435}]}, {"metric_name": "validation memory retention ratio", "lower_is_better": false, "description": "Memory retention ratio on the validation set", "data": [{"dataset_name": "pg19 (entropy-based memory ablation)", "final_value": 0.7701, "best_value": 0.7701}, {"dataset_name": "scientific_papers_arxiv (entropy-based memory ablation)", "final_value": 0.7808, "best_value": 0.7808}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (entropy-based memory ablation)", "final_value": 0.7692, "best_value": 0.7692}, {"dataset_name": "pg19 (gradient-based memory ablation)", "final_value": 0.8411, "best_value": 0.8411}, {"dataset_name": "scientific_papers_arxiv (gradient-based memory ablation)", "final_value": 0.799, "best_value": 0.799}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (gradient-based memory ablation)", "final_value": 0.8194, "best_value": 0.8194}]}, {"metric_name": "training score-weighted memory efficiency", "lower_is_better": false, "description": "Score-weighted memory efficiency on the training set", "data": [{"dataset_name": "pg19 (entropy-based memory ablation)", "final_value": 3.8157, "best_value": 3.8157}, {"dataset_name": "scientific_papers_arxiv (entropy-based memory ablation)", "final_value": 3.5983, "best_value": 3.5983}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (entropy-based memory ablation)", "final_value": 3.881, "best_value": 3.881}, {"dataset_name": "pg19 (gradient-based memory ablation)", "final_value": 0.0243, "best_value": 0.0243}, {"dataset_name": "scientific_papers_arxiv (gradient-based memory ablation)", "final_value": 0.0228, "best_value": 0.0228}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (gradient-based memory ablation)", "final_value": 0.0132, "best_value": 0.0132}]}, {"metric_name": "validation score-weighted memory efficiency", "lower_is_better": false, "description": "Score-weighted memory efficiency on the validation set", "data": [{"dataset_name": "pg19 (entropy-based memory ablation)", "final_value": 3.8287, "best_value": 3.8287}, {"dataset_name": "scientific_papers_arxiv (entropy-based memory ablation)", "final_value": 3.6257, "best_value": 3.6257}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (entropy-based memory ablation)", "final_value": 3.8922, "best_value": 3.8922}, {"dataset_name": "pg19 (gradient-based memory ablation)", "final_value": 0.0266, "best_value": 0.0266}, {"dataset_name": "scientific_papers_arxiv (gradient-based memory ablation)", "final_value": 0.0233, "best_value": 0.0233}, {"dataset_name": "wikitext_wikitext-2-raw-v1 (gradient-based memory ablation)", "final_value": 0.0124, "best_value": 0.0124}]}]}, {"metric_names": [{"metric_name": "entropy-based training loss", "lower_is_better": true, "description": "Training loss under entropy-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 2.369, "best_value": 2.369}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.5333, "best_value": 2.5333}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.3942, "best_value": 1.3942}]}, {"metric_name": "entropy-based validation loss", "lower_is_better": true, "description": "Validation loss under entropy-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 2.7013, "best_value": 2.7013}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.4968, "best_value": 2.4968}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.3448, "best_value": 1.3448}]}, {"metric_name": "entropy-based training memory retention ratio", "lower_is_better": false, "description": "Training memory retention ratio under entropy-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 0.7732, "best_value": 0.7732}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7748, "best_value": 0.7748}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.7724, "best_value": 0.7724}]}, {"metric_name": "entropy-based validation memory retention ratio", "lower_is_better": false, "description": "Validation memory retention ratio under entropy-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 0.7744, "best_value": 0.7744}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7733, "best_value": 0.7733}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.7713, "best_value": 0.7713}]}, {"metric_name": "entropy-based training score-weighted memory efficiency", "lower_is_better": false, "description": "Training score-weighted memory efficiency under entropy-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 3.8329, "best_value": 3.8329}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.6851, "best_value": 3.6851}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.8336, "best_value": 3.8336}]}, {"metric_name": "entropy-based validation score-weighted memory efficiency", "lower_is_better": false, "description": "Validation score-weighted memory efficiency under entropy-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 3.8001, "best_value": 3.8001}, {"dataset_name": "scientific_papers_arxiv", "final_value": 3.6585, "best_value": 3.6585}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 3.8423, "best_value": 3.8423}]}, {"metric_name": "gradient-based training loss", "lower_is_better": true, "description": "Training loss under gradient-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 2.0646, "best_value": 2.0646}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.412, "best_value": 2.412}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.2504, "best_value": 1.2504}]}, {"metric_name": "gradient-based validation loss", "lower_is_better": true, "description": "Validation loss under gradient-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 2.5031, "best_value": 2.5031}, {"dataset_name": "scientific_papers_arxiv", "final_value": 2.3988, "best_value": 2.3988}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 1.2632, "best_value": 1.2632}]}, {"metric_name": "gradient-based training memory retention ratio", "lower_is_better": false, "description": "Training memory retention ratio under gradient-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 0.8371, "best_value": 0.8371}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.7967, "best_value": 0.7967}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.8486, "best_value": 0.8486}]}, {"metric_name": "gradient-based validation memory retention ratio", "lower_is_better": false, "description": "Validation memory retention ratio under gradient-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 0.8397, "best_value": 0.8397}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.8, "best_value": 0.8}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.8328, "best_value": 0.8328}]}, {"metric_name": "gradient-based training score-weighted memory efficiency", "lower_is_better": false, "description": "Training score-weighted memory efficiency under gradient-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 0.0241, "best_value": 0.0241}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.0225, "best_value": 0.0225}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.013, "best_value": 0.013}]}, {"metric_name": "gradient-based validation score-weighted memory efficiency", "lower_is_better": false, "description": "Validation score-weighted memory efficiency under gradient-based memory ablation", "data": [{"dataset_name": "pg19", "final_value": 0.0268, "best_value": 0.0268}, {"dataset_name": "scientific_papers_arxiv", "final_value": 0.0233, "best_value": 0.0233}, {"dataset_name": "wikitext_wikitext-2-raw-v1", "final_value": 0.0122, "best_value": 0.0122}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_91fed57ba54e4309b07946531098d06b_proc_3959670/all_datasets_loss_curves.png", "../../logs/0-run/experiment_results/experiment_91fed57ba54e4309b07946531098d06b_proc_3959670/all_datasets_entropy_weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_91fed57ba54e4309b07946531098d06b_proc_3959670/all_datasets_retention_ratios.png"], ["../../logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/pg19_ewme.png", "../../logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/wikitext_wikitext-2-raw-v1_mrr.png", "../../logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/wikitext_wikitext-2-raw-v1_loss.png", "../../logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/scientific_papers_arxiv_loss.png", "../../logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/scientific_papers_arxiv_mrr.png", "../../logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/wikitext_wikitext-2-raw-v1_ewme.png", "../../logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/pg19_mrr.png", "../../logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/scientific_papers_arxiv_ewme.png", "../../logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/pg19_loss.png"], [], ["../../logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/scientific_papers_arxiv_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/pg19_loss_curve.png", "../../logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/wikitext_wikitext-2-raw-v1_loss_curve.png", "../../logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/scientific_papers_arxiv_entropy_weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/wikitext_wikitext-2-raw-v1_entropy_weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/pg19_entropy_weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/scientific_papers_arxiv_loss_curve.png", "../../logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/pg19_memory_retention_ratio.png"], ["../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/pg19_ffn_removed_eme.png", "../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/wikitext_wikitext-2-raw-v1_baseline_loss_curve.png", "../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/wikitext_wikitext-2-raw-v1_baseline_eme.png", "../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/scientific_papers_arxiv_ffn_removed_eme.png", "../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/scientific_papers_arxiv_baseline_loss_curve.png", "../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/scientific_papers_arxiv_ffn_removed_loss_curve.png", "../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/pg19_baseline_loss_curve.png", "../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/pg19_baseline_mem_retention.png", "../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/wikitext_wikitext-2-raw-v1_ffn_removed_eme.png", "../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/scientific_papers_arxiv_baseline_mem_retention.png", "../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/pg19_ffn_removed_mem_retention.png", "../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/scientific_papers_arxiv_baseline_eme.png", "../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/wikitext_wikitext-2-raw-v1_baseline_mem_retention.png", "../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/pg19_baseline_eme.png", "../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/pg19_ffn_removed_loss_curve.png", "../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/scientific_papers_arxiv_ffn_removed_mem_retention.png", "../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/wikitext_wikitext-2-raw-v1_ffn_removed_loss_curve.png", "../../logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/wikitext_wikitext-2-raw-v1_ffn_removed_mem_retention.png"], ["../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_pred_vs_gt_3.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_pred_vs_gt_0.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_entropy_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_pred_vs_gt_1.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_entropy_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_pred_vs_gt_2.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/pg19_loss_curve.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_pred_vs_gt_1.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_pred_vs_gt_0.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_pred_vs_gt_2.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/pg19_entropy_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_loss_curve.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_pred_vs_gt_4.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_pred_vs_gt_3.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/pg19_pred_vs_gt_3.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/pg19_pred_vs_gt_2.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/pg19_pred_vs_gt_0.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/pg19_pred_vs_gt_4.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_loss_curve.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/pg19_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_pred_vs_gt_4.png", "../../logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/pg19_pred_vs_gt_1.png"], ["../../logs/0-run/experiment_results/experiment_cd314d23272245a589bd4b5274df66ef_proc_3980098/combined_entropy_weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_cd314d23272245a589bd4b5274df66ef_proc_3980098/combined_loss_curve.png", "../../logs/0-run/experiment_results/experiment_cd314d23272245a589bd4b5274df66ef_proc_3980098/combined_memory_retention_ratio.png"], ["../../logs/0-run/experiment_results/experiment_d6eeb2dda23240f7a2a526aa755194cd_proc_3980097/loss_curves.png", "../../logs/0-run/experiment_results/experiment_d6eeb2dda23240f7a2a526aa755194cd_proc_3980097/entropy_weighted_efficiency.png", "../../logs/0-run/experiment_results/experiment_d6eeb2dda23240f7a2a526aa755194cd_proc_3980097/memory_retention_ratio.png"], [], [], [], ["../../logs/0-run/experiment_results/experiment_ab42ae294a5641b1bd6e0007baa60979_proc_3980097/scientific_papers_arxiv_metrics_curves.png", "../../logs/0-run/experiment_results/experiment_ab42ae294a5641b1bd6e0007baa60979_proc_3980097/wikitext_wikitext-2-raw-v1_metrics_curves.png", "../../logs/0-run/experiment_results/experiment_ab42ae294a5641b1bd6e0007baa60979_proc_3980097/pg19_metrics_curves.png"], ["../../logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/memory_retention_ratio_scientific_papers_arxiv.png", "../../logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/memory_retention_ratio_pg19.png", "../../logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/entropy-weighted_memory_efficiency_pg19.png", "../../logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/losses_wikitext_wikitext-2-raw-v1.png", "../../logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/entropy-weighted_memory_efficiency_scientific_papers_arxiv.png", "../../logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/losses_scientific_papers_arxiv.png", "../../logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/entropy-weighted_memory_efficiency_wikitext_wikitext-2-raw-v1.png", "../../logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/losses_pg19.png", "../../logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/memory_retention_ratio_wikitext_wikitext-2-raw-v1.png"], ["../../logs/0-run/experiment_results/experiment_77c82f20c9534df8a0398a76e3f5d002_proc_3980097/usage_loss_curves.png", "../../logs/0-run/experiment_results/experiment_77c82f20c9534df8a0398a76e3f5d002_proc_3980097/usage_retention_ratios.png", "../../logs/0-run/experiment_results/experiment_77c82f20c9534df8a0398a76e3f5d002_proc_3980097/usage_predictions_vs_ground_truth.png"], ["../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_max_sample_comp.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_max_eme.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_mean_sample_comp.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_mean_mrr.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_max_loss.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_max_mrr.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_mean_eme.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_max_loss.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_max_sample_comp.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_max_sample_comp.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_mean_eme.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_mean_loss.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_max_loss.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_mean_sample_comp.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_mean_sample_comp.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_mean_loss.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_mean_mrr.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_max_mrr.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_mean_eme.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_mean_mrr.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_max_mrr.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_max_eme.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_max_eme.png", "../../logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_mean_loss.png"], [], ["../../logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/scientific_papers_arxiv_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/scientific_papers_arxiv_tokens_comparison.png", "../../logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/wikitext_wikitext-2-raw-v1_tokens_comparison.png", "../../logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/pg19_loss_curve.png", "../../logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/wikitext_wikitext-2-raw-v1_loss_curve.png", "../../logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/scientific_papers_arxiv_entropy_weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/wikitext_wikitext-2-raw-v1_entropy_weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/pg19_entropy_weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/scientific_papers_arxiv_loss_curve.png", "../../logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/pg19_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/pg19_tokens_comparison.png"], ["../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_pg19_loss.png", "../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_loss.png", "../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_scientific_papers_arxiv_loss.png", "../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_scientific_papers_arxiv_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_scientific_papers_arxiv_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_pg19_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_pg19_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_scientific_papers_arxiv_loss.png", "../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_scientific_papers_arxiv_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_pg19_loss.png", "../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_pg19_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_wikitext_wikitext-2-raw-v1_loss.png", "../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_pg19_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_scientific_papers_arxiv_memory_retention_ratio.png"], ["../../logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/memory_key_only_entropy_retention_ratio_curves.png", "../../logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/baseline_loss_curves.png", "../../logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/memory_key_only_entropy_weighted_memory_efficiency_curves.png", "../../logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/memory_key_only_loss_curves.png", "../../logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/baseline_entropy_retention_ratio_curves.png", "../../logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/baseline_entropy_weighted_memory_efficiency_curves.png"], ["../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_pg19_loss.png", "../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_loss.png", "../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_scientific_papers_arxiv_loss.png", "../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_scientific_papers_arxiv_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_scientific_papers_arxiv_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_pg19_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_pg19_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_scientific_papers_arxiv_loss.png", "../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_scientific_papers_arxiv_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_pg19_loss.png", "../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_pg19_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_wikitext_wikitext-2-raw-v1_loss.png", "../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_pg19_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_scientific_papers_arxiv_memory_retention_ratio.png"], ["../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_pg19_loss.png", "../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_loss.png", "../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_scientific_papers_arxiv_loss.png", "../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_scientific_papers_arxiv_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_scientific_papers_arxiv_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_pg19_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_pg19_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_scientific_papers_arxiv_loss.png", "../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_scientific_papers_arxiv_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_pg19_loss.png", "../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_pg19_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_wikitext_wikitext-2-raw-v1_loss.png", "../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_pg19_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_scientific_papers_arxiv_memory_retention_ratio.png"], ["../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_pg19_loss.png", "../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_loss.png", "../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_scientific_papers_arxiv_loss.png", "../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_scientific_papers_arxiv_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_scientific_papers_arxiv_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_pg19_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_pg19_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_scientific_papers_arxiv_loss.png", "../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_scientific_papers_arxiv_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_pg19_loss.png", "../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_pg19_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png", "../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_wikitext_wikitext-2-raw-v1_loss.png", "../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_pg19_memory_retention_ratio.png", "../../logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_scientific_papers_arxiv_memory_retention_ratio.png"], []], "plot_paths": [["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_91fed57ba54e4309b07946531098d06b_proc_3959670/all_datasets_loss_curves.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_91fed57ba54e4309b07946531098d06b_proc_3959670/all_datasets_entropy_weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_91fed57ba54e4309b07946531098d06b_proc_3959670/all_datasets_retention_ratios.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/pg19_ewme.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/wikitext_wikitext-2-raw-v1_mrr.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/wikitext_wikitext-2-raw-v1_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/scientific_papers_arxiv_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/scientific_papers_arxiv_mrr.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/wikitext_wikitext-2-raw-v1_ewme.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/pg19_mrr.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/scientific_papers_arxiv_ewme.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/pg19_loss.png"], [], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/scientific_papers_arxiv_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/pg19_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/wikitext_wikitext-2-raw-v1_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/scientific_papers_arxiv_entropy_weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/wikitext_wikitext-2-raw-v1_entropy_weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/pg19_entropy_weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/scientific_papers_arxiv_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/pg19_memory_retention_ratio.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/pg19_ffn_removed_eme.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/wikitext_wikitext-2-raw-v1_baseline_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/wikitext_wikitext-2-raw-v1_baseline_eme.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/scientific_papers_arxiv_ffn_removed_eme.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/scientific_papers_arxiv_baseline_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/scientific_papers_arxiv_ffn_removed_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/pg19_baseline_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/pg19_baseline_mem_retention.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/wikitext_wikitext-2-raw-v1_ffn_removed_eme.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/scientific_papers_arxiv_baseline_mem_retention.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/pg19_ffn_removed_mem_retention.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/scientific_papers_arxiv_baseline_eme.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/wikitext_wikitext-2-raw-v1_baseline_mem_retention.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/pg19_baseline_eme.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/pg19_ffn_removed_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/scientific_papers_arxiv_ffn_removed_mem_retention.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/wikitext_wikitext-2-raw-v1_ffn_removed_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/wikitext_wikitext-2-raw-v1_ffn_removed_mem_retention.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_pred_vs_gt_3.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_pred_vs_gt_0.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_entropy_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_pred_vs_gt_1.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_entropy_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_pred_vs_gt_2.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/pg19_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_pred_vs_gt_1.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_pred_vs_gt_0.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_pred_vs_gt_2.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/pg19_entropy_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_pred_vs_gt_4.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_pred_vs_gt_3.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/pg19_pred_vs_gt_3.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/pg19_pred_vs_gt_2.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/pg19_pred_vs_gt_0.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/pg19_pred_vs_gt_4.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/pg19_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_pred_vs_gt_4.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/pg19_pred_vs_gt_1.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_cd314d23272245a589bd4b5274df66ef_proc_3980098/combined_entropy_weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_cd314d23272245a589bd4b5274df66ef_proc_3980098/combined_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_cd314d23272245a589bd4b5274df66ef_proc_3980098/combined_memory_retention_ratio.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_d6eeb2dda23240f7a2a526aa755194cd_proc_3980097/loss_curves.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_d6eeb2dda23240f7a2a526aa755194cd_proc_3980097/entropy_weighted_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_d6eeb2dda23240f7a2a526aa755194cd_proc_3980097/memory_retention_ratio.png"], [], [], [], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ab42ae294a5641b1bd6e0007baa60979_proc_3980097/scientific_papers_arxiv_metrics_curves.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ab42ae294a5641b1bd6e0007baa60979_proc_3980097/wikitext_wikitext-2-raw-v1_metrics_curves.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ab42ae294a5641b1bd6e0007baa60979_proc_3980097/pg19_metrics_curves.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/memory_retention_ratio_scientific_papers_arxiv.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/memory_retention_ratio_pg19.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/entropy-weighted_memory_efficiency_pg19.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/losses_wikitext_wikitext-2-raw-v1.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/entropy-weighted_memory_efficiency_scientific_papers_arxiv.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/losses_scientific_papers_arxiv.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/entropy-weighted_memory_efficiency_wikitext_wikitext-2-raw-v1.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/losses_pg19.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/memory_retention_ratio_wikitext_wikitext-2-raw-v1.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_77c82f20c9534df8a0398a76e3f5d002_proc_3980097/usage_loss_curves.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_77c82f20c9534df8a0398a76e3f5d002_proc_3980097/usage_retention_ratios.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_77c82f20c9534df8a0398a76e3f5d002_proc_3980097/usage_predictions_vs_ground_truth.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_max_sample_comp.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_max_eme.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_mean_sample_comp.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_mean_mrr.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_max_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_max_mrr.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_mean_eme.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_max_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_max_sample_comp.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_max_sample_comp.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_mean_eme.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_mean_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_max_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_mean_sample_comp.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_mean_sample_comp.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_mean_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_mean_mrr.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_max_mrr.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_mean_eme.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_mean_mrr.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_max_mrr.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_max_eme.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_max_eme.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_mean_loss.png"], [], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/scientific_papers_arxiv_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/scientific_papers_arxiv_tokens_comparison.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/wikitext_wikitext-2-raw-v1_tokens_comparison.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/pg19_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/wikitext_wikitext-2-raw-v1_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/scientific_papers_arxiv_entropy_weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/wikitext_wikitext-2-raw-v1_entropy_weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/pg19_entropy_weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/scientific_papers_arxiv_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/pg19_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/pg19_tokens_comparison.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_pg19_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_scientific_papers_arxiv_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_scientific_papers_arxiv_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_scientific_papers_arxiv_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_pg19_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_pg19_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_scientific_papers_arxiv_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_scientific_papers_arxiv_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_pg19_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_pg19_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_wikitext_wikitext-2-raw-v1_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_pg19_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_scientific_papers_arxiv_memory_retention_ratio.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/memory_key_only_entropy_retention_ratio_curves.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/baseline_loss_curves.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/memory_key_only_entropy_weighted_memory_efficiency_curves.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/memory_key_only_loss_curves.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/baseline_entropy_retention_ratio_curves.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/baseline_entropy_weighted_memory_efficiency_curves.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_pg19_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_scientific_papers_arxiv_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_scientific_papers_arxiv_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_scientific_papers_arxiv_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_pg19_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_pg19_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_scientific_papers_arxiv_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_scientific_papers_arxiv_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_pg19_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_pg19_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_wikitext_wikitext-2-raw-v1_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_pg19_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_scientific_papers_arxiv_memory_retention_ratio.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_pg19_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_scientific_papers_arxiv_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_scientific_papers_arxiv_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_scientific_papers_arxiv_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_pg19_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_pg19_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_scientific_papers_arxiv_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_scientific_papers_arxiv_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_pg19_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_pg19_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_wikitext_wikitext-2-raw-v1_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_pg19_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_scientific_papers_arxiv_memory_retention_ratio.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_pg19_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_scientific_papers_arxiv_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_scientific_papers_arxiv_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_scientific_papers_arxiv_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_pg19_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_pg19_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_scientific_papers_arxiv_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_scientific_papers_arxiv_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_pg19_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_pg19_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_wikitext_wikitext-2-raw-v1_loss.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_pg19_memory_retention_ratio.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_scientific_papers_arxiv_memory_retention_ratio.png"], []], "plot_analyses": [[{"analysis": "Loss curves on all three datasets show a clear decrease in both training and validation losses from epoch 0 to epoch 1. Training loss drops most steeply: pg19 from ~3.45 to ~2.42, scientific_papers_arxiv from ~3.28 to ~2.77, and wikitext_wikitext-2-raw-v1 from ~2.47 to ~1.40. Validation loss also decreases but more modestly: pg19 from ~2.95 to ~2.70, scientific_papers_arxiv from ~2.63 to ~2.54, and wikitext from ~1.50 to ~1.33. This indicates that the model is learning effectively across all domains, with no immediate signs of overfitting in the first two epochs.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_91fed57ba54e4309b07946531098d06b_proc_3959670/all_datasets_loss_curves.png"}, {"analysis": "Entropy-weighted memory efficiency declines slightly across epochs for both train and validation splits. On pg19, efficiency goes from ~3.83 to ~3.76 (train) and ~3.76 to ~3.75 (val). On scientific_papers_arxiv, it moves from ~3.75 to ~3.63 (train) and ~3.65 to ~3.63 (val). On wikitext, it decreases from ~3.94 to ~3.89 (train) and ~3.92 to ~3.88 (val). The consistent but small drop suggests that as the model compresses more tokens, per-slot informational value is marginally reduced, yet remains high, with wikitext retaining the best efficiency.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_91fed57ba54e4309b07946531098d06b_proc_3959670/all_datasets_entropy_weighted_memory_efficiency.png"}, {"analysis": "Memory retention ratios are stable and slightly increasing over epochs, hovering around 0.77\u20130.78. pg19 moves from ~0.7715 to ~0.7725 (train) and ~0.773 to ~0.773 (val). scientific_papers_arxiv increases from ~0.777 to ~0.781 (train) and ~0.779 to ~0.785 (val). wikitext_wikitext-2-raw-v1 rises from ~0.7665 to ~0.7679 (train) and ~0.7672 to ~0.7680 (val). Higher retention on scientific_papers_arxiv indicates that more tokens deemed novel (high entropy) are preserved, whereas wikitext sees the lowest retention but remains consistent.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_91fed57ba54e4309b07946531098d06b_proc_3959670/all_datasets_retention_ratios.png"}], [{"analysis": "pg19 Entropy-Weighted Memory Efficiency shows a steady decline in both train and val curves from epoch 1 to epoch 2 across original and recency variants. The recency-based method consistently lies about 0.02\u20130.03 points above the original on both splits, indicating a small but persistent efficiency advantage. Both techniques improve (efficiency drops) by roughly 0.08 on train and 0.06 on validation, and the gap between them remains stable across epochs.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/pg19_ewme.png"}, {"analysis": "wikitext_wikitext-2-raw-v1 Memory Retention Ratio reveals that the dynamic (original) compression maintains about 0.767\u20130.768 retention on train and 0.7674\u20130.7678 on val, while the recency baseline holds fixed at 0.75. This shows that entropy-aware selection preserves roughly 1.7\u20131.8 percentage points more tokens than time-based eviction, with only a slight upward drift over epochs for the dynamic method.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/wikitext_wikitext-2-raw-v1_mrr.png"}, {"analysis": "wikitext_wikitext-2-raw-v1 Loss Curves indicate both variants reduce loss sharply from epoch 1 to epoch 2. Recency-based compression attains slightly lower training loss (2.40\u21922.36) than the original (2.45\u21922.38). On validation, the recency curve drops from ~1.50 to 1.35 versus original\u2019s 1.52 to 1.45, a ~0.10 difference at epoch 2, suggesting better generalization under time-based memory over this short run.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/wikitext_wikitext-2-raw-v1_loss.png"}, {"analysis": "scientific_papers_arxiv Loss Curves for train and val both decline linearly. Recency and original closely overlap on training (\u22483.28\u21922.53) and show a marginal advantage for recency on validation (2.63\u21922.50) versus original (2.63\u21922.54). The validation gap peaks at ~0.04 by epoch 2, again hinting that simpler recency compression can sometimes match or slightly outperform the adaptive scheme in few-shot settings.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/scientific_papers_arxiv_loss.png"}, {"analysis": "scientific_papers_arxiv Memory Retention Ratio mirrors the WikiText pattern: original retains ~0.777\u21920.781 on train and ~0.795\u21920.797 on val, whereas recency stays fixed at 0.75. Adaptive compression thus captures an additional ~2.7\u20133.0 percent of tokens deemed novel by entropy, with a small positive trend across epochs for the dynamic selector.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/scientific_papers_arxiv_mrr.png"}, {"analysis": "wikitext_wikitext-2-raw-v1 Entropy-Weighted Memory Efficiency again declines between epochs for all curves. Original train moves from 3.95 to 3.89 and recency train from 3.93 to 3.87; original val 3.93\u21923.88 against recency val 3.91\u21923.86. The original holds a minor 0.02\u20130.03 lead in efficiency, but the recency gap is narrow and constant.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/wikitext_wikitext-2-raw-v1_ewme.png"}, {"analysis": "pg19 Memory Retention Ratio confirms that original retains ~0.771\u21920.773 on train and ~0.7725\u21920.773 on val, versus recency\u2019s flat 0.75. This replicates the 2.2\u20132.3 point retention gain for the entropy-aware policy over fixed-time eviction across both training and validation splits.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/pg19_mrr.png"}, {"analysis": "scientific_papers_arxiv Entropy-Weighted Memory Efficiency shows the original train falling from 3.75 to 3.63 and recency train from 3.71 to 3.61; on validation the original dips slightly (3.65\u21923.62) while recency edges up (3.60\u21923.61). The adaptive approach begins ~0.05 ahead but the margin shrinks to ~0.01 by epoch 2 as recency catches up.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/scientific_papers_arxiv_ewme.png"}, {"analysis": "pg19 Loss Curves depict original training loss decreasing from 3.45 to 2.44, and recency from 3.53 to 2.43; recency is initially worse but slightly outperforms by epoch 2. On validation, recency starts at ~2.93 versus 2.95 for original, but crosses over by the second epoch (2.73 vs 2.71), indicating a steeper improvement trajectory for the fixed-time baseline in this short run.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_720fca0b634841ab94bf787e39369cd0_proc_3980096/pg19_loss.png"}], [], [{"analysis": "scientific_papers_arxiv Memory Retention Ratio shows that entropy-based retention steadily increases from about 0.769 to 0.773 on train and from 0.771 to 0.778 on validation across epochs, whereas random retention remains nearly constant at 0.750. This indicates that the entropy signal effectively preserves more salient tokens over time, improving overall memory utilization compared to a fixed random policy.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/scientific_papers_arxiv_memory_retention_ratio.png"}, {"analysis": "pg19 Training and Validation Loss illustrates that entropy-based compression yields slightly lower loss than random retention. Training loss for entropy-based drops from ~3.45 to ~2.40 versus ~3.48 to ~2.42 for random. Validation loss decreases from ~2.90 to ~2.65 against ~2.87 to ~2.70 for random. These gains, though modest, confirm better learning and generalization when salient tokens are adaptively retained.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/pg19_loss_curve.png"}, {"analysis": "wikitext_wikitext-2-raw-v1 Memory Retention Ratio reveals the same pattern: entropy-based retention climbs from ~0.768 to 0.771 on train and from ~0.769 to 0.772 on validation, while random stays flat at ~0.750. Consistent improvements across both textual corpora underscore the robustness of the entropy-guided selection mechanism.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/wikitext_wikitext-2-raw-v1_memory_retention_ratio.png"}, {"analysis": "wikitext_wikitext-2-raw-v1 Training and Validation Loss shows a clear advantage for entropy-based compression. Training loss is lower by ~0.10 at epoch 0 (2.19 vs. 2.29) and by ~0.005 at epoch 1 (1.40 vs. 1.405). Validation loss improves from ~1.48\u21921.35 compared to ~1.53\u21921.38. These persistent improvements indicate that preserving high-novelty tokens enhances model fitting and reduces overfitting.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/wikitext_wikitext-2-raw-v1_loss_curve.png"}, {"analysis": "scientific_papers_arxiv Entropy-Weighted Memory Efficiency declines slightly over epochs for both methods but remains markedly higher for entropy-based retention (3.83\u21923.77 train, 3.75\u21923.71 val) versus random (3.54\u21923.53 train, 3.58\u21923.47 val). This metric shows that per-unit memory content carries more information under the adaptive scheme, validating the core hypothesis.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/scientific_papers_arxiv_entropy_weighted_memory_efficiency.png"}, {"analysis": "wikitext_wikitext-2-raw-v1 Entropy-Weighted Memory Efficiency mirrors the trend: entropy-based method descends from ~3.92 to 3.88 on train and ~3.90 to 3.86 on validation, outperforming random retention (~3.77\u21923.72 train, ~3.75\u21923.70 val). The consistent gap (~0.15\u20130.20) signifies better memory utilization in natural language contexts.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/wikitext_wikitext-2-raw-v1_entropy_weighted_memory_efficiency.png"}, {"analysis": "pg19 Entropy-Weighted Memory Efficiency shows that entropy-based retention starts at ~3.85 and falls to ~3.80 for train, and from ~3.82 to ~3.80 for validation; random retention drops more sharply from ~3.70 to ~3.61 train and ~3.64 to ~3.61 val. This again confirms that entropy-guided memory packs more predictive power per retained unit.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/pg19_entropy_weighted_memory_efficiency.png"}, {"analysis": "scientific_papers_arxiv Training and Validation Loss indicates almost identical convergence for both methods, with entropy-based loss going from ~3.27\u21922.55 (train) and ~2.63\u21922.495 (val) versus random ~3.31\u21922.54 (train) and ~2.62\u21922.505 (val). Differences here are minimal, suggesting that on this task, the primary benefits of entropy-guided retention manifest more in memory metrics than in bulk perplexity.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/scientific_papers_arxiv_loss_curve.png"}, {"analysis": "pg19 Memory Retention Ratio remains high and stable for entropy-based retention (~0.772\u21920.771 train, ~0.772\u21920.770 val) while random retains at ~0.750. The slight plateau in entropy-based retention suggests diminishing marginal gains after initial training, inviting exploration of dynamic budget schedules.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_ef66f212832a419888bbc2485d69f689_proc_3980098/pg19_memory_retention_ratio.png"}], [{"analysis": "wikitext_wikitext-2-raw-v1 (baseline) shows a steep decrease in training loss from ~2.45 to ~1.40 and a moderate drop in validation loss from ~1.50 to ~1.35 over one epoch. The training curve remains consistently above validation, indicating the model is learning rapidly without significant overfitting in this single-epoch run.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/pg19_ffn_removed_eme.png"}, {"analysis": "wikitext_wikitext-2-raw-v1 (ffn_removed) exhibits higher initial losses (train ~2.62, val ~1.57) and converges to higher final values (train ~1.46, val ~1.40) compared to the baseline. The removal of the FFN slows convergence and degrades both training and validation performance on Wikitext-2.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/wikitext_wikitext-2-raw-v1_baseline_loss_curve.png"}, {"analysis": "pg19 (baseline) displays a drop in training loss from ~3.45 to ~2.42 and in validation loss from ~2.93 to ~2.71. The model learns effectively on PG19, with validation lagging behind training but maintaining a healthy gap without overfitting in this short run.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/wikitext_wikitext-2-raw-v1_baseline_eme.png"}, {"analysis": "pg19 (ffn_removed) starts at a higher training loss (~3.82 vs ~3.45) and ends at ~2.62, while validation falls from ~3.05 to ~2.81. Compared to the baseline, both curves are shifted upward, confirming that FFN layers are important for learning expressive representations over long-range text.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/scientific_papers_arxiv_ffn_removed_eme.png"}, {"analysis": "scientific_papers_arxiv (baseline) sees training loss reduce from ~3.28 to ~2.55 and validation from ~2.63 to ~2.52. The baseline again demonstrates solid learning behavior, with consistent gains and no signs of divergence.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/scientific_papers_arxiv_baseline_loss_curve.png"}, {"analysis": "scientific_papers_arxiv (ffn_removed) has higher starting losses (~3.56 train, ~2.75 val) and finishes at (~2.60 train, ~2.55 val). The pattern mirrors other datasets: removing FFN impairs convergence speed and final performance.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/scientific_papers_arxiv_ffn_removed_loss_curve.png"}, {"analysis": "pg19 (baseline) memory retention ratio rises slightly on both splits (train ~0.7716\u21920.7722, val ~0.7726\u21920.7729). Validation consistently retains a marginally higher ratio, reflecting stable compressive behavior without drastic fluctuations.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/pg19_baseline_loss_curve.png"}, {"analysis": "pg19 (ffn_removed) shows higher overall retention (train ~0.7750\u21920.7818, val ~0.7818\u21920.7798). Train retention climbs notably, while validation dips slightly. The FFN removal leads to more tokens being kept, possibly as a fallback for reduced representational capacity.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/pg19_baseline_mem_retention.png"}, {"analysis": "pg19 (baseline) entropy-weighted memory efficiency decreases modestly (train ~3.837\u21923.755, val ~3.758\u21923.742). The model becomes slightly less efficient at retaining informative tokens over epochs, but the drop is small.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/wikitext_wikitext-2-raw-v1_ffn_removed_eme.png"}, {"analysis": "pg19 (ffn_removed) entropy-weighted efficiency declines sharply in training (~3.751\u21923.560) and increases marginally on validation (~3.549\u21923.598). Without FFN, the model retains more tokens but uses memory less efficiently, especially during training.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_dd09bf09d1b443d7ae39497e2b1aef9f_proc_3980097/scientific_papers_arxiv_baseline_mem_retention.png"}], [{"analysis": "Loss on scientific_papers_arxiv drops steadily from 3.05 to 2.50 on training and from 2.55 to 2.49 on validation between epochs 1 and 2, indicating consistent improvement without a large train\u2013val gap.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_memory_retention_ratio.png"}, {"analysis": "Memory retention ratio (MRR) for scientific_papers_arxiv is nearly flat on training (0.7739\u21920.7740) but declines on validation (0.7754\u21920.7713), suggesting that adaptive compression may be discarding slightly more context in held\u2010out data under the current threshold.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_pred_vs_gt_3.png"}, {"analysis": "Entropy-weighted memory efficiency (EME) on scientific_papers_arxiv falls on training (3.800\u21923.757) while it rises on validation (3.758\u21923.769), indicating that the module is using less memory per unit of novelty during training but may be over-retaining less informative tokens in validation.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_pred_vs_gt_0.png"}, {"analysis": "On wikitext-2, training loss decreases from 2.06 to 1.32 and validation loss from 1.395 to 1.325, showing good training dynamics and generalization under EA-ACM.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_entropy_memory_efficiency.png"}, {"analysis": "Wikitext-2 MRR increases on both splits: training from 0.7697 to 0.7689 (slight dip) then overall improvement in validation from 0.7677 to 0.7686, reflecting better memory for relevant tokens on held\u2010out data.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_pred_vs_gt_1.png"}, {"analysis": "EME for wikitext-2 declines on both training (3.942\u21923.867) and validation (3.922\u21923.874), which suggests the entropy-guided compressor is achieving higher efficiency (less memory usage for the same novelty) as epochs progress.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_entropy_memory_efficiency.png"}, {"analysis": "For PG19, training loss falls sharply from 3.21 to 2.39 and validation loss modestly from 2.90 to 2.75, demonstrating strong learning but a smaller generalization gain, potentially due to PG19\u2019s diverse long contexts.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_pred_vs_gt_2.png"}, {"analysis": "PG19 MRR climbs slightly on training (0.7693\u21920.7699) and on validation (0.7707\u21920.7710), implying the memory module successfully retains slightly more useful information across epochs.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/pg19_loss_curve.png"}, {"analysis": "PG19 EME decreases on both splits (training 3.861\u21923.778, validation 3.784\u21923.753), showing that the entropy-aware compressor is becoming more efficient, using less total memory to capture novelty.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/wikitext_wikitext-2-raw-v1_memory_retention_ratio.png"}, {"analysis": "Generated sample for scientific_papers_arxiv deviate markedly from ground truth: the model produces garbled tokens (\u2018t eta hen t netneh tn\u2026\u2019), indicating that aggressive compression may have removed critical context for coherent generative predictions.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_896fcdf7b9f94c0f89c3d16f1d3c3a2f_proc_3980096/scientific_papers_arxiv_pred_vs_gt_1.png"}], [{"analysis": "Entropy-weighted memory efficiency is consistently higher for the EA-ACM\u2013equipped models (original_*) than for the no-memory_attention baselines across all three datasets. At epoch 1, original models start in the 3.8\u20133.95 range, whereas baselines lie around 3.15\u20133.55. Both groups show a mild downward trend over two epochs (\u22480.05\u20130.1 drop), indicating that as training proceeds, the entropy signal slightly diminishes but remains robust. The gap between train and validation curves is small, suggesting stable generalization of the compression strategy.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_cd314d23272245a589bd4b5274df66ef_proc_3980098/combined_entropy_weighted_memory_efficiency.png"}, {"analysis": "All models exhibit steep loss reductions between epochs 1 and 2, but the differences between EA-ACM and no-memory baselines are marginal. On PG19, no-memory baselines achieve slightly lower validation loss (~1.31 vs. 1.34) while EA-ACM holds a small edge on Wikitext2. On the scientific-papers_arxiv split, both configurations converge near 2.3\u20132.5, with no-memory again a hair lower. This pattern hints at a modest trade-off: entropy-aware compression may introduce a slight regularization effect that benefits some datasets and lags on others.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_cd314d23272245a589bd4b5274df66ef_proc_3980098/combined_loss_curve.png"}, {"analysis": "The memory retention ratio for EA-ACM models remains at 100% across epochs, confirming that the entropy threshold or budget policy allowed full preservation of salient tokens. In contrast, fixed-budget no-memory baselines consistently retain ~77\u201379% of their past key/value pairs. The constancy of both curves suggests that the dynamic allocation policy is stable over training, but may not yet exploit more aggressive compression for non-salient content.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_cd314d23272245a589bd4b5274df66ef_proc_3980098/combined_memory_retention_ratio.png"}], [{"analysis": "Loss curves across epochs for both training and validation reveal that all models steadily reduce loss, but the norm-based baseline slightly outperforms the entropy-aware variant. On each dataset (PG19, ArXiv, WikiText-2) the entropy-based model shows marginally higher training and validation loss (\u22480.05\u20130.1 points) after one epoch, indicating that entropy-guided compression incurs a small performance hit in terms of perplexity at this stage.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_d6eeb2dda23240f7a2a526aa755194cd_proc_3980097/loss_curves.png"}, {"analysis": "Entropy-weighted memory efficiency (EME) is consistently higher for the entropy-aware module across datasets and splits. The entropy-aware curves sit \u22480.2\u20130.3 points above the norm baseline in both train and validation, demonstrating that measuring token novelty via attention entropy yields more efficient use of the memory budget by prioritizing informative tokens.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_d6eeb2dda23240f7a2a526aa755194cd_proc_3980097/entropy_weighted_efficiency.png"}, {"analysis": "Memory retention ratio trends show that the entropy-aware approach retains a slightly larger fraction of past tokens (in the 76\u201378% range) compared to the norm baseline (\u224876\u201377%). This higher retention aligns with improved EME but also indicates a less aggressive compression schedule, highlighting a trade-off between information retention and overall compression ratio.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_d6eeb2dda23240f7a2a526aa755194cd_proc_3980097/memory_retention_ratio.png"}], [], [], [], [], [{"analysis": "Memory Retention Ratio for scientific_papers_arxiv shows that the baseline model maintains a high, stable retention ratio (~0.775\u21920.780 on train, ~0.778\u21920.782 on validation) across epochs. The entropy-aware adaptive compressive memory (ContMem) operates at a lower retention ratio (~0.625\u21920.637 on train, ~0.633\u21920.640 on validation), reflecting its tighter memory budget. ContMem\u2019s retention ratio increases modestly over epochs, suggesting that the adaptive threshold learns to preserve slightly more content as training progresses.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/memory_retention_ratio_scientific_papers_arxiv.png"}, {"analysis": "Memory Retention Ratio for pg19 indicates a similar pattern: the baseline retention ratio stays near ~0.772\u20130.773 for both train and validation. ContMem begins at a lower retention (~0.615\u21920.622 on train, ~0.618\u21920.631 on validation) but also shows a gradual increase, hinting that the novelty-driven selection is adapting to hold more tokens over time.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/memory_retention_ratio_pg19.png"}, {"analysis": "Entropy-Weighted Memory Efficiency for pg19 reveals that ContMem achieves substantially higher efficiency (~4.402\u21924.383 on train, ~4.372\u21924.343 on validation) compared to the baseline (~3.845\u21923.754 on train, ~3.755\u21923.745 on validation). Both methods see a slight efficiency decline over epochs, but the gap (~0.55) remains large, confirming that entropy-guided compression delivers more information per memory unit.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/entropy-weighted_memory_efficiency_pg19.png"}, {"analysis": "Losses for wikitext_wikitext-2-raw-v1 demonstrate nearly identical learning curves between baseline and ContMem. Both achieve steep loss reductions from epoch 1 to 2 (train: ~2.45\u21921.40 baseline vs. ~2.42\u21921.39 ContMem; validation: ~1.50\u21921.38 baseline vs. ~1.52\u21921.37 ContMem). ContMem matches or slightly improves on baseline performance, indicating no accuracy sacrifice despite lower retention.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/losses_wikitext_wikitext-2-raw-v1.png"}, {"analysis": "Entropy-Weighted Memory Efficiency for scientific_papers_arxiv shows that ContMem outperforms the baseline by ~0.63 in efficiency (train: ~4.393\u21924.373 vs. baseline ~3.745\u21923.633; validation: ~4.366\u21924.360 vs. baseline ~3.655\u21923.620). Slight downward trends occur in both methods, but ContMem maintains a clear advantage.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/entropy-weighted_memory_efficiency_scientific_papers_arxiv.png"}, {"analysis": "Losses for scientific_papers_arxiv are almost the same for both methods. Train loss drops from ~3.28\u21922.55 baseline and ~3.27\u21922.53 ContMem; validation goes from ~2.64\u21922.50 baseline and ~2.64\u21922.51 ContMem. Differences are negligible (<0.02), confirming that adaptive compression does not harm learning on long scientific texts.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/losses_scientific_papers_arxiv.png"}, {"analysis": "Entropy-Weighted Memory Efficiency for wikitext_wikitext-2-raw-v1 again highlights ContMem\u2019s superior efficiency (~4.402\u21924.398) versus baseline (~3.942\u21923.886 on train, similar gap on validation). ContMem curves are nearly flat across epochs, showing stable high efficiency.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/entropy-weighted_memory_efficiency_wikitext_wikitext-2-raw-v1.png"}, {"analysis": "Losses for pg19 reveal a slight edge for the baseline: train loss moves from ~3.45\u21922.42 baseline versus ~3.50\u21922.43 ContMem; validation from ~2.95\u21922.71 baseline versus ~3.00\u21922.75 ContMem. Differences shrink by epoch 2 (<0.04), suggesting that ContMem\u2019s accuracy approaches baseline given more fine-tuning.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/losses_pg19.png"}, {"analysis": "Memory Retention Ratio for wikitext_wikitext-2-raw-v1 mirrors the other datasets: baseline holds ~0.768\u21920.769; ContMem uses a lower retention (~0.615\u21920.620 on train, ~0.620\u21920.625 on validation) that increases slightly over epochs. This confirms consistent adaptive behavior across domains.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b1501c40fee5425298135be497e87b95_proc_3980098/memory_retention_ratio_wikitext_wikitext-2-raw-v1.png"}], [{"analysis": "Training and validation loss decreased across epochs and datasets; the model shows more rapid loss reduction on WikiText (from ~2.45 to ~1.40 on train) than on PG19 and ArXiv, indicating dataset-dependent convergence speeds and potential complexity differences. Validation losses also decline (e.g., PG19 val from ~2.95 to ~2.75), suggesting improved generalization without major overfitting after two epochs.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_77c82f20c9534df8a0398a76e3f5d002_proc_3980097/usage_loss_curves.png"}, {"analysis": "Retention ratios remain flat at approximately 80.5% for all datasets and both epochs, implying that the adaptive compression mechanism under the current hyperparameters does not adjust the memory buffer size or retention proportion during training iterations.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_77c82f20c9534df8a0398a76e3f5d002_proc_3980097/usage_retention_ratios.png"}, {"analysis": "Predicted usage versus ground truth for PG19 and ArXiv follow similar temporal patterns but exhibit minor amplitude shifts and smoothing effects, indicating good overall trend capture with slight prediction lag or smoothing bias. For WikiText, both ground truth and predictions are zero across the sample, reflecting either a trivial constant usage scenario or an edge case in the dataset where no memory usage events occur.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_77c82f20c9534df8a0398a76e3f5d002_proc_3980097/usage_predictions_vs_ground_truth.png"}], [{"analysis": "Loss curves on the scientific_papers_arxiv split show a steep decrease in training loss from about 3.28 to 2.53 between epochs 1 and 2, while validation loss falls from roughly 2.62 to 2.49. The larger gap and sharper decline in training loss compared to validation suggests the model is learning signal rapidly but with some overfitting potential, as the validation improvement is more modest.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_max_sample_comp.png"}, {"analysis": "Scatter of ground-truth versus generated token IDs on scientific_papers_arxiv reveals that generated samples capture the broad distribution of high-frequency tokens (IDs around 100\u2013120) and low-frequency drops (IDs around 30\u201340). However, there is noticeable dispersion in peak locations and some missing or extra spikes in the generated trace, indicating that while the model preserves coarse patterns, it sometimes misaligns on precise token-level peaks.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_max_eme.png"}, {"analysis": "Memory Retention Ratio (MRR) on scientific_papers_arxiv rises from about 0.777 to 0.781 on the training set and from 0.7812 to 0.7819 on validation. The consistent upward trend confirms that the entropy-aware adaptive compression is increasingly effective at retaining informative key/value pairs across epochs, boosting recall under a fixed budget.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_mean_sample_comp.png"}, {"analysis": "Entropy-Weighted Memory Efficiency (EME) on scientific_papers_arxiv falls from around 3.65 to 3.52 (train) and from 3.53 to 3.51 (val). The downward trend reflects that the module is compressing more aggressively based on entropy signals; slightly steeper decline on training data suggests the model is learning to discard less novel tokens faster than in validation.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_mean_mrr.png"}, {"analysis": "Loss curves on wikitext_wikitext-2-raw-v1 show training loss decreasing sharply from about 2.40 to 1.40 and validation loss from roughly 1.50 to 1.35 across two epochs. The narrower gap and parallel slopes imply better generalization on this dataset compared to scientific_papers_arxiv, with entropy-guided compression having minimal adverse effects on validation performance.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_max_loss.png"}, {"analysis": "Scatter of ground-truth versus generated token IDs on wikitext_wikitext-2-raw-v1 is trivial (all points lie at token ID 0). Generated samples exactly overlay ground truth, indicating that for this slice the model reproduces deterministic or repeated tokens perfectly, likely reflecting a simple or padding-heavy input scenario.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_max_mrr.png"}, {"analysis": "Entropy-Weighted Memory Efficiency (EME) on wikitext_wikitext-2-raw-v1 declines from about 3.90 to 3.78 (train) and from 3.84 to 3.75 (val). As with scientific_papers_arxiv, this decreasing trend indicates more aggressive entropy-based pruning, while the modest spread between train and val suggests robust behavior on both splits.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_mean_eme.png"}, {"analysis": "Loss curves on pg19 decrease from around 3.55 to 2.42 (train) and from 2.92 to 2.72 (val). The crossing of train below validation at epoch 2 hints at potential overfitting, though both curves improve, demonstrating that EA-ACM drives better perplexity even on long-range book data.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/pg19_max_loss.png"}, {"analysis": "Scatter of ground-truth versus generated token IDs on pg19 shows that generated samples mirror the dual-cluster pattern (low IDs \u223c10\u201320 then high IDs \u223c60\u2013125) seen in ground truth. Slight shifts in peak positions and a few outlier points suggest accurate large-scale structure retention but imperfect token-level ordering.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/scientific_papers_arxiv_max_sample_comp.png"}, {"analysis": "Memory Retention Ratio (MRR) on pg19 edges up from about 0.77313 to 0.77332 on training data, while slipping slightly from roughly 0.77288 to 0.77279 on validation. This split behavior suggests the adaptive mechanism boosts recall in contexts seen during training, though validation retention remains flat-to-declining, pointing to room for calibration of entropy thresholds on unseen book text.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_8a1c7956311845a1a2c09ffb83922436_proc_3980098/wikitext_wikitext-2-raw-v1_max_sample_comp.png"}], [], [{"analysis": "scientific_papers_arxiv - Memory Retention Ratio Curve shows a consistent upward trend from epoch 1 to epoch 2. Training MRR rises from approximately 0.7677 to 0.7698 and validation MRR from about 0.7690 to 0.7705. The gain in training retention (\u2248+0.0021) slightly outpaces validation gain (\u2248+0.0015), indicating that EA-ACM is progressively better at selecting and preserving tokens with high informational content. The modest but clear improvement suggests that even within two epochs, the entropy-guided strategy yields a measurable boost over a fixed-budget baseline, though potential overfitting should be monitored if the gap continues to widen.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/scientific_papers_arxiv_memory_retention_ratio.png"}, {"analysis": "wikitext_wikitext-2-raw-v1 - Ground Truth vs. Predicted Tokens visualization unexpectedly appears empty, with zero counts on both sides. This likely indicates a plotting or data-extraction issue rather than a genuine content match between ground truth and predictions. It will be important to verify that sample sequences are correctly logged and token indices are passed to the plotting routine. Without valid data points, no conclusion can be drawn about the model\u2019s qualitative prediction behavior on wikimedia content.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/scientific_papers_arxiv_tokens_comparison.png"}, {"analysis": "pg19 - Loss Curve demonstrates strong convergence over two epochs. Training loss drops sharply from roughly 3.40 to 2.42 (\u0394\u2248\u20130.98), while validation loss declines more moderately from about 2.89 to 2.71 (\u0394\u2248\u20130.18). The wider reduction in training loss relative to validation suggests the model is fitting the training distribution rapidly, and a slight performance gap may begin to emerge. Continued monitoring across more epochs (and potentially adding regularization) will clarify if EA-ACM\u2019s adaptive memory induces overfitting or maintains generalization.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/wikitext_wikitext-2-raw-v1_tokens_comparison.png"}, {"analysis": "wikitext_wikitext-2-raw-v1 - Memory Retention Ratio Curve increases steadily from epoch 1 to epoch 2. Training MRR moves from approximately 0.7664 to 0.7676 (+0.0012) while validation follows a similar trajectory from around 0.7690 to 0.7698 (+0.0008). These gains are consistent with those seen on scientific_papers_arxiv, indicating that the entropy threshold effectively prioritizes salient tokens across different domains. The magnitude of improvement is modest, underscoring the need for longer training or finer threshold tuning to unlock larger retention benefits.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/pg19_loss_curve.png"}, {"analysis": "wikitext_wikitext-2-raw-v1 - Loss Curve shows training loss decreasing from about 2.22 to 1.42 (\u0394\u2248\u20130.80) and validation loss from around 1.50 to 1.38 (\u0394\u2248\u20130.12). The disparity between steep training improvement and more gradual validation gains mirrors the pattern observed on PG19. This suggests that while EA-ACM helps the model learn rapidly, the adaptation of memory may require more epochs or stronger regularization to translate into comparable validation performance.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/wikitext_wikitext-2-raw-v1_memory_retention_ratio.png"}, {"analysis": "scientific_papers_arxiv - Entropy-Weighted Memory Efficiency Curve (EME) declines across epochs: training EME falls from roughly 3.91 to 3.83 (\u0394\u2248\u20130.08) and validation EME from about 3.85 to 3.81 (\u0394\u2248\u20130.04). If lower EME represents a better trade-off of retained information per memory unit, the downward trend confirms that EA-ACM is improving its entropy-guided allocation. The slightly larger drop on training data suggests the model optimizes memory usage faster on familiar patterns, with validation efficiency catching up more slowly.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/wikitext_wikitext-2-raw-v1_loss_curve.png"}, {"analysis": "wikitext_wikitext-2-raw-v1 - Entropy-Weighted Memory Efficiency Curve also trends downward from epoch 1 to 2. Training EME decreases from ~3.95 to 3.90 (\u0394\u2248\u20130.05) and validation from ~3.93 to 3.90 (\u0394\u2248\u20130.03). The parallel behavior across two different language data sets underscores the generality of entropy scoring: EA-ACM consistently reduces entropy-weighted overhead, though small absolute changes suggest that more training or adaptive budget schedules might further amplify gains.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/scientific_papers_arxiv_entropy_weighted_memory_efficiency.png"}, {"analysis": "pg19 - Entropy-Weighted Memory Efficiency Curve shows a reduction in training EME from about 3.92 to 3.86 (\u0394\u2248\u20130.06) and in validation EME from roughly 3.88 to 3.84 (\u0394\u2248\u20130.04). The pattern aligns with the other benchmarks, again indicating that entropy-aware compression is edging memory efficiency in the right direction. PG19\u2019s slightly larger EME swing hints at dataset-specific behavior, possibly due to its distinct document structure and token distributions.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/wikitext_wikitext-2-raw-v1_entropy_weighted_memory_efficiency.png"}, {"analysis": "scientific_papers_arxiv - Loss Curve exhibits steady learning: train loss drops from ~3.17 to 2.55 (\u0394\u2248\u20130.62) while validation loss goes from ~2.62 to 2.51 (\u0394\u2248\u20130.11). The fact that both losses decrease confirms that the EA-ACM module can be integrated without destabilizing training dynamics. The gap between training and validation shrinkage remains modest, suggesting controlled generalization alongside memory adaptation.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/pg19_entropy_weighted_memory_efficiency.png"}, {"analysis": "pg19 - Memory Retention Ratio Curve rises moderately: training MRR climbs from approximately 0.7676 to 0.7685 (+0.0009) and validation MRR from about 0.7680 to 0.7690 (+0.0010). Though the absolute MRR gains are smaller than on scientific_papers_arxiv, they stay positive and consistent across datasets. This general trend supports the central hypothesis that entropy-guided compression improves long-context retention relative to fixed schemes, even early in training.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_fad417ea3e344d4fa67cf27c991737b7_proc_3980098/scientific_papers_arxiv_loss_curve.png"}], [{"analysis": "Dataset pg19 under gradient-ablation shows a steep drop in training loss from about 2.79 to 2.10 over two epochs, while validation loss decreases only slightly from roughly 2.64 to 2.61. This suggests strong fitting capability but a widening train/val gap and limited generalization benefit in early epochs.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_pg19_loss.png"}, {"analysis": "On pg19 with entropy-ablation, training loss starts higher (\u22483.45) and falls to around 2.42, validation moves from about 2.93 to 2.71. Although both losses improve, they remain notably higher than under gradient-ablation, indicating slower convergence and a larger generalization gap when entropy-based memory selection is removed.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png"}, {"analysis": "Score-weighted memory efficiency for pg19 under gradient-ablation rises steadily on both splits: training efficiency climbs from about 0.0210 to 0.0243 and validation from 0.0244 to 0.0271. This upward trend implies that the gradient-driven component contributes positively to retaining high-impact tokens in memory.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_loss.png"}, {"analysis": "Under entropy-ablation on pg19, score-weighted memory efficiency actually decreases: training from roughly 3.837 to 3.755 and validation from 3.757 to 3.742. The drop suggests that without the entropy-guidance mechanism, the model\u2019s compression becomes less effective at prioritizing novel information.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_scientific_papers_arxiv_loss.png"}, {"analysis": "Wikitext-2 (raw) with gradient-ablation achieves a drop in training loss from about 1.67 to 1.24 and validation from 1.32 to 1.26 over two epochs. The modest validation improvement alongside a larger training reduction again points to some overfitting but overall solid learning progress.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png"}, {"analysis": "On wikitext-2 under entropy-ablation, training loss falls from approximately 2.45 to 1.39, validation from 1.50 to 1.35. Losses stay higher than in the gradient-ablation setting, indicating that discarding the entropy module leads to slower adaptation and higher perplexity across both splits.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_scientific_papers_arxiv_memory_retention_ratio.png"}, {"analysis": "Score-weighted memory efficiency on wikitext-2 with gradient-ablation shows a slight decrease in training (0.01375\u21920.01330) but an increase in validation (0.01180\u21920.01250). This mixed behavior may reflect an aggressive memory selection that improves generalization memory use at the cost of some training buffer richness.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_scientific_papers_arxiv_score-weighted_memory_efficiency.png"}, {"analysis": "Entropy-ablation on wikitext-2 yields a consistent decline in efficiency for both splits (training from \u22483.949 to 3.887; validation from 3.928 to 3.882), mirroring the pg19 trend and reinforcing the necessity of entropy-based novelty scoring for maintaining an informative memory buffer.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_pg19_memory_retention_ratio.png"}, {"analysis": "Scientific Papers (ArXiv) under gradient-ablation records a decrease in training loss from about 2.78 to 2.40 and validation from 2.47 to 2.38. The steady improvements illustrate that gradient-driven selection continues to benefit long-range modeling on highly technical text.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/grad_pg19_score-weighted_memory_efficiency.png"}, {"analysis": "Entropy-ablation on the ArXiv dataset shows training loss dropping from roughly 3.28 to 2.55 and validation from 2.63 to 2.52. Although the pattern of improvement holds, absolute losses remain higher than in the gradient-ablation setting, again highlighting the poorer fitting and generalization when entropy signals are omitted.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_453594005ee5410fb07369f44f04b3ae_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png"}], [{"analysis": "Entropy retention trends under the memory_key_only ablation show that on pg19 both train and validation retention ratios climb modestly over epochs, suggesting that the key-only memory buffer is gradually preserving slightly more novel tokens from that corpus. scientific_papers_arxiv train also rises, but its validation set drops by about 0.005, indicating that aggressive key-only compression may discard some patterns important for held-out generalization. wikitext-2 train and validation remain nearly flat with only +0.001\u20130.0005 changes, implying a minimal effect of this component on that dataset\u2019s novelty scheme.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/memory_key_only_entropy_retention_ratio_curves.png"}, {"analysis": "Baseline loss curves demonstrate healthy training dynamics across all corpora. pg19 and scientific_papers_arxiv show roughly \u22121.0 to \u22121.1 drops in training loss and \u22120.2 to \u22120.3 drops in validation over two epochs. wikitext-2 exhibits an even steeper training improvement (~\u22121.06) and a modest validation gain (~\u22120.17). These results confirm that the model is converging rapidly under the baseline memory mechanism.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/baseline_loss_curves.png"}, {"analysis": "Entropy-weighted memory efficiency for memory_key_only steadily improves (i.e., metric decreases) on pg19 train (\u22120.14) and validation (\u22120.08), as well as on wikitext-2 train (\u22120.04) and val (\u22120.03), showing that per-bit novelty is being stored more effectively. scientific_papers_arxiv train also dips moderately (\u22120.07), but its validation side ticks upward by ~0.04, again flagging a mismatch between training compression patterns and validation-time information needs.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/memory_key_only_entropy_weighted_memory_efficiency_curves.png"}, {"analysis": "The memory_key_only loss curves mirror or slightly outperform the baseline on certain splits. pg19 train drops by \u22121.14 vs. baseline \u22121.05, scientific_papers_arxiv train by \u22120.77 vs. \u22120.73, while validation gaps remain on par or slightly smaller. wikitext-2 gains are identical to baseline. Overall, key-only memory does not degrade perplexity and even yields minor improvements for some datasets.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/memory_key_only_loss_curves.png"}, {"analysis": "Under the baseline scheme, entropy retention ratios edge up only marginally: pg19 train +0.001, val +0.0006; scientific_papers_arxiv train +0.004, val +0.001; wikitext-2 train +0.0015, val +0.0006. These gains are smaller than memory_key_only on pg19 but more consistent on scientific_papers_arxiv validation.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/baseline_entropy_retention_ratio_curves.png"}, {"analysis": "Baseline entropy-weighted memory efficiency uniformly improves by \u22120.08 to \u22120.06 across all splits (pg19, scientific_papers_arxiv, wikitext-2), showing modest but stable gains in novelty stored per memory unit.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a25c5fb050c642c882d9831af9f350c4_proc_3980098/baseline_entropy_weighted_memory_efficiency_curves.png"}], [{"analysis": "PG19 with gradient ablation shows a sharp drop in training loss from ~2.77 to ~2.07 between epochs 1 and 2, while validation loss decreases modestly from ~2.68 to ~2.60. This indicates strong learning on the training side and some generalization gain, though a small train\u2013validation gap remains.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_pg19_loss.png"}, {"analysis": "PG19 with entropy ablation starts at a higher training loss (~3.47) and drops to ~2.41 by epoch 2; validation loss falls from ~2.90 to ~2.65. Final losses are higher than with gradient ablation, suggesting that removing entropy-guided compression hurts both convergence speed and eventual performance.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png"}, {"analysis": "Wikitext-2 (raw) under gradient ablation sees training loss fall from ~1.67 to ~1.25, and validation loss from ~1.32 to ~1.25. The train\u2013validation gap nearly closes by epoch 2, pointing to stable generalization when the gradient-based component is retained.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_loss.png"}, {"analysis": "Wikitext-2 (raw) under entropy ablation shows training loss decreasing from ~2.19 to ~1.38 and validation loss from ~1.48 to ~1.35. Although both curves improve, final losses remain higher than the gradient-ablation counterpart, again highlighting the benefit of the entropy signal for faster convergence and lower perplexity.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_scientific_papers_arxiv_loss.png"}, {"analysis": "Scientific Papers (ArXiv) with gradient ablation exhibits training loss reducing from ~2.79 to ~2.41, and validation loss from ~2.48 to ~2.39. The pattern is consistent with other datasets: steady learning on train and modest but meaningful gains on validation.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png"}, {"analysis": "Scientific Papers (ArXiv) with entropy ablation starts with training loss ~3.26 and drops to ~2.54, while validation goes from ~2.62 to ~2.50. Both are worse than gradient ablation throughout, confirming that ablating the entropy component impairs fit and generalization.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_scientific_papers_arxiv_memory_retention_ratio.png"}, {"analysis": "PG19 memory efficiency under gradient ablation increases from ~0.0214 to ~0.0245 on train and from ~0.0248 to ~0.0269 on validation across epochs. This upward trend suggests that the remaining components become more efficient at storing salient information over time when gradient guidance is preserved.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_scientific_papers_arxiv_score-weighted_memory_efficiency.png"}, {"analysis": "PG19 memory efficiency under entropy ablation decreases slightly from ~3.85 to ~3.80 on train and from ~3.815 to ~3.795 on validation. The small downward drift indicates that without the entropy-based novelty signal, memory utilization becomes marginally less effective as training proceeds.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_pg19_memory_retention_ratio.png"}, {"analysis": "Scientific Papers (ArXiv) memory efficiency with gradient ablation climbs from ~0.0202 to ~0.0225 on train and ~0.0218 to ~0.0233 on validation. The consistent increase mirrors the PG19 trend and underscores that gradient-guided updates alone can progressively boost storage efficiency.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/grad_pg19_score-weighted_memory_efficiency.png"}, {"analysis": "Scientific Papers (ArXiv) memory efficiency under entropy ablation drops from ~3.83 to ~3.72 on train and from ~3.758 to ~3.710 on validation. This decline further reinforces that removing entropy-driven selection leads to gradually less efficient memory usage.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png"}], [{"analysis": "On wikitext_wikitext-2-raw-v1 under Grad Ablation, training loss drops from about 1.67 to 1.24 over two epochs, while validation loss falls from 1.33 to 1.26. This indicates that with gradient-based gating removed, the model still converges smoothly and maintains a modest generalization gap.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_pg19_loss.png"}, {"analysis": "On wikitext_wikitext-2-raw-v1 under Entropy Ablation, training loss starts much higher (around 2.36) and only falls to 1.40 by epoch two; validation loss declines from 1.52 to 1.35. The substantially higher losses and slower convergence highlight that removing entropy-based novelty signals significantly impairs learning efficiency.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png"}, {"analysis": "Score-Weighted Memory Efficiency on wikitext under Grad Ablation shows a slight drop in training efficiency (from ~0.01350 to ~0.01315) but an increase on validation (from ~0.01168 to ~0.01240). This suggests that without gradient-based score weighting, memory retention on the training side is marginally less efficient while validation memory usage can improve slightly.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_loss.png"}, {"analysis": "Under Entropy Ablation on wikitext, memory efficiency declines for both training (from ~3.935 to ~3.880) and validation (from ~3.910 to ~3.893). The consistent drop underscores the importance of entropy-guided selection for preserving informative tokens in memory.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_scientific_papers_arxiv_loss.png"}, {"analysis": "On pg19 under Grad Ablation, training loss decreases sharply from about 2.75 to 2.09, and validation loss from 2.65 to 2.56. This indicates effective convergence even when gradient-based gating is removed.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png"}, {"analysis": "On pg19 under Entropy Ablation, training loss falls from around 3.42 to 2.39, and validation loss from 2.88 to 2.69. The higher starting point and slower decrease relative to gradient ablation confirm that entropy signals are critical for efficient learning on this long-context dataset.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_scientific_papers_arxiv_memory_retention_ratio.png"}, {"analysis": "Score-Weighted Memory Efficiency on pg19 under Grad Ablation increases steadily on both train (0.0213 \u2192 0.0243) and validation (0.0244 \u2192 0.0267), indicating that gradient-based scoring contributes substantially to more efficient memory compression.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_scientific_papers_arxiv_score-weighted_memory_efficiency.png"}, {"analysis": "With Entropy Ablation on pg19, memory efficiency drops on training (3.872 \u2192 3.815) while validation sees a slight rise (3.817 \u2192 3.838). The mixed pattern but overall weaker efficiency underlines that without entropy-driven novelty metrics, memory becomes less informative.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_pg19_memory_retention_ratio.png"}, {"analysis": "On scientific_papers_arxiv under Grad Ablation, training loss reduces from approximately 2.795 to 2.43 and validation from 2.495 to 2.42. The model retains good convergence when gradient-based gating is absent.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/grad_pg19_score-weighted_memory_efficiency.png"}, {"analysis": "On scientific_papers_arxiv under Entropy Ablation, training loss starts higher (around 3.22) and drops to 2.53, and validation loss goes from 2.62 to 2.50. The elevated losses across both splits reaffirm that entropy-based novelty gating is vital across domains.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png"}], [{"analysis": "Loss decreases substantially on PG19 when using entropy-guided memory: training loss drops from ~3.41 to ~2.37 and validation from ~2.83 to ~2.70 between epochs 1 and 2, indicating strong learning and generalization improvements.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_pg19_loss.png"}, {"analysis": "Memory retention ratio on PG19 climbs consistently across epochs for both training (from ~0.7705 to ~0.7732) and validation (from ~0.7723 to ~0.7745), showing the entropy signal is guiding the model to preserve a slightly larger portion of tokens over time.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_wikitext_wikitext-2-raw-v1_score-weighted_memory_efficiency.png"}, {"analysis": "Score-weighted memory efficiency on PG19 declines modestly with training (from ~3.88 to ~3.83) and validation (from ~3.85 to ~3.80), suggesting that allocating budget by entropy trades off a bit of per-token scoring efficiency in order to retain more information overall.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_loss.png"}, {"analysis": "WikiText-2 loss under entropy ablation falls sharply for training (from ~2.40 to ~1.39) and validation (from ~1.50 to ~1.35), mirroring the PG19 trend and confirming that entropy-based retention consistently lowers perplexity on a different language modeling benchmark.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_scientific_papers_arxiv_loss.png"}, {"analysis": "On WikiText-2 the memory retention ratio increases for training (from ~0.768 to ~0.7725) and validation (from ~0.7705 to ~0.7713), reinforcing that the adaptive memory buffer retains more context as the model converges.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png"}, {"analysis": "Score-weighted memory efficiency on WikiText-2 decreases from ~3.93 to ~3.83 for training and from ~3.87 to ~3.84 for validation, again reflecting a small efficiency cost in exchange for richer memory content.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_scientific_papers_arxiv_memory_retention_ratio.png"}, {"analysis": "On ArXiv the entropy-guided model achieves lower loss across epochs (training: ~3.26\u21922.53, validation: ~2.62\u21922.50), demonstrating the method\u2019s robustness on a domain-specific, long-range dataset.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_scientific_papers_arxiv_score-weighted_memory_efficiency.png"}, {"analysis": "Memory retention ratio on ArXiv grows for training (from ~0.7708 to ~0.7748) while validation retention nudges down slightly (from ~0.7746 to ~0.7733), hinting at dataset-specific tuning needs even as the overall trend favors more retention.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_pg19_memory_retention_ratio.png"}, {"analysis": "ArXiv score-weighted memory efficiency decreases from ~3.83 to ~3.68 in training and from ~3.72 to ~3.66 in validation, consistent with other datasets: entropy adaptation increases total retained information at the expense of per-unit efficiency.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/grad_pg19_score-weighted_memory_efficiency.png"}, {"analysis": "Using gradient-magnitude ablation on PG19 still yields training loss improvements (from ~2.77 to ~2.06) and validation gains (from ~2.59 to ~2.50), but the smaller reduction in validation loss compared to the entropy-based variant suggests entropy is a more effective salience signal.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/entropy_wikitext_wikitext-2-raw-v1_memory_retention_ratio.png"}], []], "vlm_feedback_summary": ["Training shows strong initial learning with diminishing but stable validation\nimprovements. Entropy-aware compression maintains high memory efficiency with\nonly small drops, and retention ratios confirm the model adaptively keeps high-\nentropy tokens. Overall, EA-ACM effectively balances compression and information\npreservation across diverse long-context datasets.", "Across the three datasets, entropy-aware adaptive compression consistently\nretains ~2\u20133 points more tokens than the recency baseline, confirming its\neffectiveness at preserving salient information. However, memory efficiency and\ndownstream loss trends vary: recency often matches or slightly surpasses the\nadaptive scheme in efficiency and generalization on WikiText-2 and arXiv, and\nshows a mixed picture on PG19. These results suggest that while per-token\nentropy signals substantially boost retention ratio, further tuning of the\nthresholding or budget allocation is needed to translate that advantage into\nconsistent perplexity or efficiency gains across diverse corpora.", "[]", "Entropy-guided adaptive compression consistently outperforms random retention\nacross memory retention, entropy-weighted efficiency, and yields modest\nreductions in perplexity on multiple long-range datasets. Effects are most\npronounced in memory metrics, with marginal loss improvements. Key next steps:\ntune entropy thresholding and compression budgets, measure runtime overhead,\nevaluate on repetitive or noisy contexts, and consider fallback strategies when\ntoken novelty is misestimated.", "Removing the feed-forward network consistently worsens language modeling metrics\nacross all three datasets: higher training and validation losses and slower\nconvergence. Although the FFN-removed ablation tends to retain more tokens\n(increased retention ratio), its entropy-weighted memory efficiency drops\nsignificantly, indicating that extra retained tokens are less informative. These\nresults confirm the FFN\u2019s key role in compressive memory, balancing memory\nfootprint with salience-based selection.", "Training and validation losses consistently decrease across datasets, confirming\nEA-ACM\u2019s effectiveness in reducing perplexity. Memory retention ratios remain\nstable or improve slightly on some splits, though validation retention drops on\narXiv, signaling threshold tuning is needed. EME trends show growing efficiency\noverall, but sample generation on arXiv is incoherent, suggesting over-\ncompression of key semantic tokens. Further dataset-specific tuning of entropy\nthresholds and integration strategies is recommended to balance compression and\ngeneration quality.", "EA-ACM delivers significant gains in entropy-weighted memory efficiency versus\nfixed budgets and maintains competitive perplexity. The dynamic retention policy\nensures full preservation of informative context, although downstream loss\nimprovements over baselines are dataset-dependent and require further tuning.", "In this ablation the entropy-aware compression module improves memory efficiency\nbut at a modest cost in perplexity and results in higher retention ratios. Next\nsteps should focus on tuning the entropy threshold or budget policy to achieve a\nbetter balance between compression aggressiveness and model performance.", "[]", "[]", "[]", "[]", "Across three datasets, entropy-aware adaptive compression yields 15\u201320% higher\nentropy-weighted memory efficiency with only minimal or no loss in modeling\nperformance. Retention ratios for ContMem start lower but rise over epochs,\nshowing adaptive selection. Perplexity/loss remains on par with or within a few\nhundredths of baseline, confirming that ContMem meets long-context retention\ngoals without sacrificing accuracy.", "Loss curves confirm effective training improvements; flat retention ratios\nsuggest static memory allocation under current settings; prediction alignment is\nstrong on long-context corpora with minor discrepancies, and WikiText usage\nappears trivial.", "Across scientific_papers_arxiv, wikitext-2, and pg19, loss curves consistently\ndrop on both training and validation, with more pronounced training gains.\nGenerated-sample analysis shows that entropy-aware compression preserves the\noverall token distribution and major structural patterns but occasionally\nmisaligns precise peaks or local ordering. MRR trends upward in most training\nsplits, confirming improved memory recall under budget, though validation\nretention on pg19 dips slightly. EME steadily decreases, reflecting successful\nentropy-guided pruning that enhances compression efficiency. These findings\nsupport the hypothesis that token-level entropy signals in EA-ACM boost long-\ncontext retention and generation quality beyond fixed-rate baselines, while\nhighlighting opportunities to fine-tune thresholds for better generalization.", "[]", "All quantitative metrics\u2014including loss, memory retention ratio, and entropy-\nweighted efficiency\u2014show consistent, modest improvements from epoch 1 to 2\nacross three corpora (PG19, wikitext-2, arXiv). The blank token visualization\nfor wikitext suggests a plotting error to address. Loss curves demonstrate rapid\ntrain convergence with smaller validation gains, indicating potential\noverfitting risk if left unchecked. Memory metrics validate EA-ACM\u2019s ability to\npreserve high-information tokens more effectively than fixed schemes, though the\neffect size remains small at this early ablation stage.", "Across three datasets and both metrics, removing the entropy-guided component\nconsistently yields higher losses and falling memory efficiency, whereas the\ngradient-driven ablation maintains or improves efficiency and achieves lower\nlosses. These results validate the contribution of entropy-based novelty scoring\nto both effective compression and generalization.", "The memory_key_only ablation yields comparable or slightly better\ntraining/validation loss trajectories and stronger retention/efficiency gains on\npg19 and wikitext-2, but underperforms on scientific_papers_arxiv validation. In\ncontrast, the baseline is more consistent on that split. This suggests that key-\nonly compression can boost memory efficiency for some corpora but may need\nadaptive thresholds or fallback strategies to handle domains with noisier or\nmore heterogeneous content.", "Across all three datasets, retaining the entropy-guided component (i.e.,\nablating the gradient-only variant) yields lower final losses and faster\nconvergence, while gradient-guided-only models degrade memory efficiency over\ntime. Conversely, models that rely solely on gradient signals show steadily\nimproving memory-efficiency metrics but suffer higher losses overall. These\nresults underscore the complementary role of entropy-based novelty scoring for\nboth perplexity reduction and effective memory utilization in long-context\nlanguage tasks.", "Across wikitext, PG19, and ArXiv, removal of entropy-based novelty gating\nconsistently causes higher initial losses, slower convergence, and degraded\nmemory efficiency. In contrast, ablation of gradient-based score weighting has\nsmaller impacts on loss reduction but does affect memory efficiency dynamics.\nThese results show that the entropy signal is the primary driver of performance\ngains, while gradient-based scoring fine-tunes memory utilization.", "Entropy-guided adaptive compressive memory consistently lowers loss across all\ndatasets, increases memory retention ratios modestly, and incurs a slight drop\nin score-weighted efficiency. A gradient-based ablation yields some benefits but\nshows weaker validation improvements, underlining the advantage of token-level\nentropy for adaptive memory management.", "[]"], "exec_time": [848.1619365215302, 1783.293329000473, 169.0738480091095, 1769.2184290885925, 1774.319619178772, 854.4597778320312, 1787.7056155204773, 1777.6502439975739, 3600.9407000541687, 7.460437297821045, 958.4426999092102, 1769.095566034317, 1735.8989088535309, 909.7719595432281, 1803.710025548935, 1753.6198751926422, 915.5474035739899, 1743.7673890590668, 1724.5593008995056, 1802.3488631248474, 1748.2656540870667, 1649.942632675171, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"pg19\"", "\"scientific_papers_arxiv\"", "\"wikitext_wikitext-2-raw-v1\"]"], ["['pg19'", "'wikitext_wikitext-2-raw-v1'", "'scientific_papers_arxiv']"], [], ["['scientific_papers_arxiv'", "'pg19'", "'wikitext_wikitext-2-raw-v1']"], ["[wikitext_wikitext-2-raw-v1", "pg19", "scientific_papers_arxiv]"], ["[\"wikitext-2\"", "\"PG19\"]"], ["[Wikitext2]"], ["PG19", "ArXiv", "WikiText-2"], [], [], [], [], ["[\"scientific_papers_arxiv\"", "\"pg19\"", "\"wikitext_wikitext-2-raw-v1\"]"], ["['PG19'", "'ArXiv']"], ["['scientific_papers_arxiv'", "'wikitext_wikitext-2-raw-v1'", "'pg19']"], [], ["scientific_papers_arxiv", "pg19"], ["['pg19'", "'wikitext-2'", "'arxiv']"], ["pg19", "wikitext-2"], ["['PG19'", "'Wikitext-2'", "'Scientific Papers (ArXiv)']"], ["[\"wikitext_wikitext-2-raw-v1\"", "\"pg19\"", "\"scientific_papers_arxiv\"]"], ["[PG19", "WikiText-2", "ArXiv]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# Load data\ntry:\n    experiment_data = np.load(data_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Print final metrics\nfor ds, ds_data in experiment_data.items():\n    try:\n        final_val_loss = ds_data[\"losses\"][\"val\"][-1]\n        final_val_ratio = ds_data[\"metrics\"][\"Memory Retention Ratio\"][\"val\"][-1]\n        final_val_eme = ds_data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"][\n            -1\n        ]\n        print(\n            f\"{ds} Final Val Loss: {final_val_loss:.4f}, \"\n            f\"Retention: {final_val_ratio:.4f}, EME: {final_val_eme:.4f}\"\n        )\n    except Exception as e:\n        print(f\"Error printing final metrics for {ds}: {e}\")\n\n# Loss curves comparison\ntry:\n    plt.figure()\n    for ds, ds_data in experiment_data.items():\n        epochs = range(len(ds_data[\"losses\"][\"train\"]))\n        plt.plot(epochs, ds_data[\"losses\"][\"train\"], label=f\"{ds} train\")\n        plt.plot(epochs, ds_data[\"losses\"][\"val\"], linestyle=\"--\", label=f\"{ds} val\")\n    plt.title(\n        \"Loss Curves Across Datasets\\nDatasets: \" + \", \".join(experiment_data.keys())\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"all_datasets_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Memory retention ratio comparison\ntry:\n    plt.figure()\n    for ds, ds_data in experiment_data.items():\n        m = ds_data[\"metrics\"][\"Memory Retention Ratio\"]\n        epochs = range(len(m[\"train\"]))\n        plt.plot(epochs, m[\"train\"], label=f\"{ds} train\")\n        plt.plot(epochs, m[\"val\"], linestyle=\"--\", label=f\"{ds} val\")\n    plt.title(\n        \"Memory Retention Ratios Across Datasets\\nDatasets: \"\n        + \", \".join(experiment_data.keys())\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Retention Ratio\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"all_datasets_retention_ratios.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating retention ratios plot: {e}\")\n    plt.close()\n\n# Entropy-weighted memory efficiency comparison\ntry:\n    plt.figure()\n    for ds, ds_data in experiment_data.items():\n        m = ds_data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"]\n        epochs = range(len(m[\"train\"]))\n        plt.plot(epochs, m[\"train\"], label=f\"{ds} train\")\n        plt.plot(epochs, m[\"val\"], linestyle=\"--\", label=f\"{ds} val\")\n    plt.title(\n        \"Entropy-Weighted Memory Efficiency Across Datasets\\nDatasets: \"\n        + \", \".join(experiment_data.keys())\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Entropy-Weighted Memory Efficiency\")\n    plt.legend()\n    plt.savefig(\n        os.path.join(working_dir, \"all_datasets_entropy_weighted_memory_efficiency.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating efficiency plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\ndatasets = list(experiment_data.get(\"original\", {}).keys())\nablations = [\"original\", \"recency\"]\nfor ds in datasets:\n    # prepare curves\n    curves = {}\n    for ab in ablations:\n        ed = experiment_data[ab][ds]\n        curves[ab] = {\n            \"loss_train\": ed[\"losses\"][\"train\"],\n            \"loss_val\": ed[\"losses\"][\"val\"],\n            \"mrr_train\": ed[\"metrics\"][\"Memory Retention Ratio\"][\"train\"],\n            \"mrr_val\": ed[\"metrics\"][\"Memory Retention Ratio\"][\"val\"],\n            \"ewme_train\": ed[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"],\n            \"ewme_val\": ed[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"],\n        }\n    epochs = range(1, len(curves[\"original\"][\"loss_train\"]) + 1)\n    # Plot Loss\n    try:\n        plt.figure()\n        for ab in ablations:\n            plt.plot(epochs, curves[ab][\"loss_train\"], label=f\"{ab} train\")\n            plt.plot(epochs, curves[ab][\"loss_val\"], \"--\", label=f\"{ab} val\")\n        plt.title(f\"{ds} Loss Curves\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ds}_loss.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating {ds} loss plot: {e}\")\n        plt.close()\n    # Plot Memory Retention Ratio\n    try:\n        plt.figure()\n        for ab in ablations:\n            plt.plot(epochs, curves[ab][\"mrr_train\"], label=f\"{ab} train\")\n            plt.plot(epochs, curves[ab][\"mrr_val\"], \"--\", label=f\"{ab} val\")\n        plt.title(f\"{ds} Memory Retention Ratio\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Retention Ratio\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ds}_mrr.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating {ds} MRR plot: {e}\")\n        plt.close()\n    # Plot Entropy-Weighted Memory Efficiency\n    try:\n        plt.figure()\n        for ab in ablations:\n            plt.plot(epochs, curves[ab][\"ewme_train\"], label=f\"{ab} train\")\n            plt.plot(epochs, curves[ab][\"ewme_val\"], \"--\", label=f\"{ab} val\")\n        plt.title(f\"{ds} Entropy-Weighted Memory Efficiency\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Efficiency\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ds}_ewme.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating {ds} EWME plot: {e}\")\n        plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Print final validation metrics\nfor ablation, ds_dict in experiment_data.items():\n    for dataset_key, ed in ds_dict.items():\n        val_loss = ed[\"losses\"][\"val\"][-1]\n        val_ratio = ed[\"metrics\"][\"Memory Retention Ratio\"][\"val\"][-1]\n        val_eme = ed[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"][-1]\n        print(\n            f\"{ablation} | {dataset_key} - Final Val Loss: {val_loss:.4f}, \"\n            f\"Retention Ratio: {val_ratio:.4f}, EME: {val_eme:.4f}\"\n        )\n\n# Plotting curves for each dataset\ndataset_keys = (\n    list(experiment_data[next(iter(experiment_data))].keys()) if experiment_data else []\n)\nfor dataset_key in dataset_keys:\n    # Loss curve\n    try:\n        plt.figure()\n        for ablation, ds_dict in experiment_data.items():\n            ed = ds_dict[dataset_key]\n            epochs = range(len(ed[\"losses\"][\"train\"]))\n            plt.plot(epochs, ed[\"losses\"][\"train\"], label=f\"{ablation} train\")\n            plt.plot(epochs, ed[\"losses\"][\"val\"], \"--\", label=f\"{ablation} val\")\n        plt.title(\n            f\"{dataset_key}: Training and Validation Loss\\nSolid: Train, Dashed: Validation\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{dataset_key}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {dataset_key}: {e}\")\n        plt.close()\n\n    # Memory Retention Ratio\n    try:\n        plt.figure()\n        for ablation, ds_dict in experiment_data.items():\n            ed = ds_dict[dataset_key]\n            epochs = range(len(ed[\"metrics\"][\"Memory Retention Ratio\"][\"train\"]))\n            plt.plot(\n                epochs,\n                ed[\"metrics\"][\"Memory Retention Ratio\"][\"train\"],\n                label=f\"{ablation} train\",\n            )\n            plt.plot(\n                epochs,\n                ed[\"metrics\"][\"Memory Retention Ratio\"][\"val\"],\n                \"--\",\n                label=f\"{ablation} val\",\n            )\n        plt.title(\n            f\"{dataset_key}: Memory Retention Ratio\\nSolid: Train, Dashed: Validation\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Memory Retention Ratio\")\n        plt.legend()\n        plt.savefig(\n            os.path.join(working_dir, f\"{dataset_key}_memory_retention_ratio.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating retention ratio plot for {dataset_key}: {e}\")\n        plt.close()\n\n    # Entropy-Weighted Memory Efficiency\n    try:\n        plt.figure()\n        for ablation, ds_dict in experiment_data.items():\n            ed = ds_dict[dataset_key]\n            epochs = range(\n                len(ed[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"])\n            )\n            plt.plot(\n                epochs,\n                ed[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"],\n                label=f\"{ablation} train\",\n            )\n            plt.plot(\n                epochs,\n                ed[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"],\n                \"--\",\n                label=f\"{ablation} val\",\n            )\n        plt.title(\n            f\"{dataset_key}: Entropy-Weighted Memory Efficiency\\nSolid: Train, Dashed: Validation\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Entropy-Weighted Memory Efficiency\")\n        plt.legend()\n        plt.savefig(\n            os.path.join(\n                working_dir, f\"{dataset_key}_entropy_weighted_memory_efficiency.png\"\n            )\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating EME plot for {dataset_key}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nelse:\n    for ablation, ds_dict in experiment_data.items():\n        for ds_name, data in ds_dict.items():\n            # Loss curves\n            try:\n                plt.figure()\n                plt.plot(data[\"losses\"][\"train\"], marker=\"o\", label=\"train\")\n                plt.plot(data[\"losses\"][\"val\"], marker=\"x\", label=\"val\")\n                plt.title(f\"{ds_name} ({ablation}) - Training and Validation Loss\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Loss\")\n                plt.legend()\n                plt.savefig(\n                    os.path.join(working_dir, f\"{ds_name}_{ablation}_loss_curve.png\")\n                )\n            except Exception as e:\n                print(f\"Error creating loss plot for {ds_name} {ablation}: {e}\")\n            finally:\n                plt.close()\n\n            # Memory Retention Ratio\n            try:\n                plt.figure()\n                tr = data[\"metrics\"][\"Memory Retention Ratio\"][\"train\"]\n                va = data[\"metrics\"][\"Memory Retention Ratio\"][\"val\"]\n                plt.plot(tr, marker=\"o\", label=\"train\")\n                plt.plot(va, marker=\"x\", label=\"val\")\n                plt.title(\n                    f\"{ds_name} ({ablation}) - Memory Retention Ratio\\nTrain vs Validation\"\n                )\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Retention Ratio\")\n                plt.legend()\n                plt.savefig(\n                    os.path.join(working_dir, f\"{ds_name}_{ablation}_mem_retention.png\")\n                )\n            except Exception as e:\n                print(\n                    f\"Error creating memory retention plot for {ds_name} {ablation}: {e}\"\n                )\n            finally:\n                plt.close()\n\n            # Entropy-Weighted Memory Efficiency\n            try:\n                plt.figure()\n                tr_e = data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"]\n                va_e = data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"]\n                plt.plot(tr_e, marker=\"o\", label=\"train\")\n                plt.plot(va_e, marker=\"x\", label=\"val\")\n                plt.title(\n                    f\"{ds_name} ({ablation}) - Entropy-Weighted Memory Efficiency\\nTrain vs Validation\"\n                )\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Efficiency\")\n                plt.legend()\n                plt.savefig(os.path.join(working_dir, f\"{ds_name}_{ablation}_eme.png\"))\n            except Exception as e:\n                print(f\"Error creating EME plot for {ds_name} {ablation}: {e}\")\n            finally:\n                plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor ds_name, ds_data in experiment_data.get(\"no_layernorm\", {}).items():\n    losses = ds_data.get(\"losses\", {})\n    metrics = ds_data.get(\"metrics\", {})\n\n    # Loss curve\n    try:\n        plt.figure()\n        tr, va = losses.get(\"train\"), losses.get(\"val\")\n        if tr and va:\n            epochs = range(1, len(tr) + 1)\n            plt.plot(epochs, tr, marker=\"o\", label=\"Train Loss\")\n            plt.plot(epochs, va, marker=\"o\", label=\"Val Loss\")\n            plt.title(f\"{ds_name} Loss Curve\\nTraining vs Validation Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds_name}: {e}\")\n        plt.close()\n\n    # Memory Retention Ratio\n    try:\n        plt.figure()\n        mr = metrics.get(\"Memory Retention Ratio\", {})\n        tr_mr, va_mr = mr.get(\"train\"), mr.get(\"val\")\n        if tr_mr and va_mr:\n            epochs = range(1, len(tr_mr) + 1)\n            plt.plot(epochs, tr_mr, marker=\"o\", label=\"Train MRR\")\n            plt.plot(epochs, va_mr, marker=\"o\", label=\"Val MRR\")\n            plt.title(f\"{ds_name} Memory Retention Ratio\\nTraining vs Validation\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Ratio\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{ds_name}_memory_retention_ratio.png\")\n            )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating MRR plot for {ds_name}: {e}\")\n        plt.close()\n\n    # Entropy-Weighted Memory Efficiency\n    try:\n        plt.figure()\n        eme = metrics.get(\"Entropy-Weighted Memory Efficiency\", {})\n        tr_eme, va_eme = eme.get(\"train\"), eme.get(\"val\")\n        if tr_eme and va_eme:\n            epochs = range(1, len(tr_eme) + 1)\n            plt.plot(epochs, tr_eme, marker=\"o\", label=\"Train EME\")\n            plt.plot(epochs, va_eme, marker=\"o\", label=\"Val EME\")\n            plt.title(\n                f\"{ds_name} Entropy-Weighted Memory Efficiency\\nTraining vs Validation\"\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Efficiency\")\n            plt.legend()\n            plt.savefig(\n                os.path.join(working_dir, f\"{ds_name}_entropy_memory_efficiency.png\")\n            )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating EME plot for {ds_name}: {e}\")\n        plt.close()\n\n    # Predictions vs Ground Truth\n    preds = ds_data.get(\"predictions\", [])\n    gts = ds_data.get(\"ground_truth\", [])\n    for i in range(min(5, len(preds))):\n        try:\n            gt_str = \"\".join(chr(c) for c in gts[i])\n            pred_str = \"\".join(chr(c) for c in preds[i])\n            plt.figure(figsize=(8, 4))\n            plt.axis(\"off\")\n            plt.text(0.05, 0.6, gt_str, wrap=True)\n            plt.text(0.05, 0.4, pred_str, wrap=True)\n            plt.title(\n                f\"{ds_name} Predictions vs Ground Truth (Sample {i})\\nLeft: Ground Truth, Right: Generated Samples\"\n            )\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_pred_vs_gt_{i}.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating prediction plot for {ds_name} sample {i}: {e}\")\n            plt.close()\n\n    # Print final metrics\n    final_loss = losses.get(\"val\")[-1] if losses.get(\"val\") else None\n    final_mrr = metrics.get(\"Memory Retention Ratio\", {}).get(\"val\", [None])[-1]\n    final_eme = metrics.get(\"Entropy-Weighted Memory Efficiency\", {}).get(\n        \"val\", [None]\n    )[-1]\n    print(\n        f\"{ds_name}: Final Val Loss = {final_loss}, MRR = {final_mrr}, EME = {final_eme}\"\n    )\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_data = {}\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nfor key_type, metric_name, subtitle, filename in [\n    (\"losses\", \"Loss\", \"Loss over Epochs\", \"combined_loss_curve.png\"),\n    (\n        \"metrics\",\n        \"Memory Retention Ratio\",\n        \"Memory Retention Ratio over Epochs\",\n        \"combined_memory_retention_ratio.png\",\n    ),\n    (\n        \"metrics\",\n        \"Entropy-Weighted Memory Efficiency\",\n        \"Entropy-Weighted Memory Efficiency over Epochs\",\n        \"combined_entropy_weighted_memory_efficiency.png\",\n    ),\n]:\n    try:\n        plt.figure()\n        for ablation, ds_dict in experiment_data.items():\n            for dataset, data in ds_dict.items():\n                if key_type == \"losses\":\n                    train_vals = data[\"losses\"][\"train\"]\n                    val_vals = data[\"losses\"][\"val\"]\n                else:\n                    train_vals = data[\"metrics\"][metric_name][\"train\"]\n                    val_vals = data[\"metrics\"][metric_name][\"val\"]\n                epochs = np.arange(1, len(train_vals) + 1)\n                plt.plot(\n                    epochs,\n                    train_vals,\n                    label=f\"{ablation}_{dataset}_train\",\n                    linestyle=\"-\",\n                )\n                plt.plot(\n                    epochs, val_vals, label=f\"{ablation}_{dataset}_val\", linestyle=\"--\"\n                )\n        plt.title(metric_name)\n        plt.suptitle(subtitle)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(metric_name)\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, filename))\n    except Exception as e:\n        print(f\"Error creating plot for {metric_name}: {e}\")\n    finally:\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load data and print final validation metrics\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    for mode, ds_dict in experiment_data.items():\n        for ds_name, data in ds_dict.items():\n            val_losses = data[\"losses\"][\"val\"]\n            val_ret = data[\"metrics\"][\"Memory Retention Ratio\"][\"val\"]\n            val_eme = data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"]\n            if val_losses:\n                print(\n                    f\"[{mode}][{ds_name}] Final Val Loss={val_losses[-1]:.4f}, \"\n                    f\"Retention={val_ret[-1]:.4f}, Efficiency={val_eme[-1]:.4f}\"\n                )\nexcept Exception as e:\n    print(f\"Error loading or printing metrics: {e}\")\n\n# Plot loss curves\ntry:\n    plt.figure()\n    plt.subplot(1, 2, 1)\n    for mode, ds_dict in experiment_data.items():\n        for ds_name, data in ds_dict.items():\n            tr = data[\"losses\"][\"train\"]\n            plt.plot(tr, label=f\"{ds_name}-{mode}\")\n    plt.title(\"Training Loss\")\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    for mode, ds_dict in experiment_data.items():\n        for ds_name, data in ds_dict.items():\n            vl = data[\"losses\"][\"val\"]\n            plt.plot(vl, label=f\"{ds_name}-{mode}\")\n    plt.title(\"Validation Loss\")\n    plt.suptitle(\"Loss Curves Across Epochs\")\n    plt.savefig(os.path.join(working_dir, \"loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# Plot memory retention ratio\ntry:\n    plt.figure()\n    plt.subplot(1, 2, 1)\n    for mode, ds_dict in experiment_data.items():\n        for ds_name, data in ds_dict.items():\n            tr = data[\"metrics\"][\"Memory Retention Ratio\"][\"train\"]\n            plt.plot(tr, label=f\"{ds_name}-{mode}\")\n    plt.title(\"Train Retention Ratio\")\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    for mode, ds_dict in experiment_data.items():\n        for ds_name, data in ds_dict.items():\n            vl = data[\"metrics\"][\"Memory Retention Ratio\"][\"val\"]\n            plt.plot(vl, label=f\"{ds_name}-{mode}\")\n    plt.title(\"Val Retention Ratio\")\n    plt.suptitle(\"Memory Retention Ratio Across Epochs\")\n    plt.savefig(os.path.join(working_dir, \"memory_retention_ratio.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating retention plot: {e}\")\n    plt.close()\n\n# Plot entropy-weighted memory efficiency\ntry:\n    plt.figure()\n    plt.subplot(1, 2, 1)\n    for mode, ds_dict in experiment_data.items():\n        for ds_name, data in ds_dict.items():\n            tr = data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"]\n            plt.plot(tr, label=f\"{ds_name}-{mode}\")\n    plt.title(\"Train EME\")\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    for mode, ds_dict in experiment_data.items():\n        for ds_name, data in ds_dict.items():\n            vl = data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"]\n            plt.plot(vl, label=f\"{ds_name}-{mode}\")\n    plt.title(\"Val EME\")\n    plt.suptitle(\"Entropy-Weighted Memory Efficiency\")\n    plt.savefig(os.path.join(working_dir, \"entropy_weighted_efficiency.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating EME plot: {e}\")\n    plt.close()\n", null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nfor ds_key in experiment_data.get(\"baseline\", {}):\n    try:\n        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n        fig.suptitle(\n            f\"Metrics Curves for {ds_key}\\n\"\n            \"Left: Loss; Middle: Memory Retention Ratio; Right: Entropy-Weighted Memory Efficiency\"\n        )\n        for ablation in [\"baseline\", \"feedforward_identity\"]:\n            ed = experiment_data[ablation][ds_key]\n            epochs = np.arange(1, len(ed[\"losses\"][\"train\"]) + 1)\n            # Loss curves\n            axes[0].plot(epochs, ed[\"losses\"][\"train\"], label=f\"{ablation} Train\")\n            axes[0].plot(\n                epochs, ed[\"losses\"][\"val\"], linestyle=\"--\", label=f\"{ablation} Val\"\n            )\n            axes[0].set_xlabel(\"Epoch\")\n            axes[0].set_ylabel(\"Loss\")\n            # Memory Retention Ratio curves\n            mrr_tr = ed[\"metrics\"][\"Memory Retention Ratio\"][\"train\"]\n            mrr_val = ed[\"metrics\"][\"Memory Retention Ratio\"][\"val\"]\n            axes[1].plot(epochs, mrr_tr, label=f\"{ablation} Train\")\n            axes[1].plot(epochs, mrr_val, linestyle=\"--\", label=f\"{ablation} Val\")\n            axes[1].set_xlabel(\"Epoch\")\n            axes[1].set_ylabel(\"Memory Retention Ratio\")\n            # Entropy-Weighted Memory Efficiency curves\n            eme_tr = ed[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"]\n            eme_val = ed[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"]\n            axes[2].plot(epochs, eme_tr, label=f\"{ablation} Train\")\n            axes[2].plot(epochs, eme_val, linestyle=\"--\", label=f\"{ablation} Val\")\n            axes[2].set_xlabel(\"Epoch\")\n            axes[2].set_ylabel(\"Entropy-Weighted Memory Efficiency\")\n        for ax in axes:\n            ax.legend()\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        plt.savefig(os.path.join(working_dir, f\"{ds_key}_metrics_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for {ds_key}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Print final validation losses\nfor ablation in [\"baseline\", \"continuous_memory\"]:\n    for ds_key, ds_data in experiment_data.get(ablation, {}).items():\n        try:\n            final_loss = ds_data[\"losses\"][\"val\"][-1]\n            print(f\"{ablation} {ds_key} final val loss: {final_loss:.4f}\")\n        except Exception:\n            pass\n\n# Plotting metrics\nfor metric in [\n    \"losses\",\n    \"Memory Retention Ratio\",\n    \"Entropy-Weighted Memory Efficiency\",\n]:\n    metric_fname = metric.lower().replace(\" \", \"_\")\n    for ds_key in experiment_data.get(\"baseline\", {}):\n        try:\n            plt.figure()\n            # Determine data paths\n            if metric == \"losses\":\n                b_train = experiment_data[\"baseline\"][ds_key][\"losses\"][\"train\"]\n                b_val = experiment_data[\"baseline\"][ds_key][\"losses\"][\"val\"]\n                c_train = experiment_data[\"continuous_memory\"][ds_key][\"losses\"][\n                    \"train\"\n                ]\n                c_val = experiment_data[\"continuous_memory\"][ds_key][\"losses\"][\"val\"]\n            else:\n                b_train = experiment_data[\"baseline\"][ds_key][\"metrics\"][metric][\n                    \"train\"\n                ]\n                b_val = experiment_data[\"baseline\"][ds_key][\"metrics\"][metric][\"val\"]\n                c_train = experiment_data[\"continuous_memory\"][ds_key][\"metrics\"][\n                    metric\n                ][\"train\"]\n                c_val = experiment_data[\"continuous_memory\"][ds_key][\"metrics\"][metric][\n                    \"val\"\n                ]\n            epochs = range(1, len(b_train) + 1)\n            # Plot lines\n            plt.plot(epochs, b_train, label=\"Baseline Train\")\n            plt.plot(epochs, b_val, label=\"Baseline Val\")\n            plt.plot(epochs, c_train, label=\"ContMem Train\")\n            plt.plot(epochs, c_val, label=\"ContMem Val\")\n            # Labels and title\n            plt.title(f\"{metric} for {ds_key}\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(metric)\n            plt.legend()\n            # Save figure\n            fname = f\"{metric_fname}_{ds_key}.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating {metric} plot for {ds_key}: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot 1: Loss curves\ntry:\n    plt.figure()\n    for key, d in experiment_data[\"usage_based\"].items():\n        epochs = np.arange(1, len(d[\"losses\"][\"train\"]) + 1)\n        plt.plot(epochs, d[\"losses\"][\"train\"], marker=\"o\", label=f\"{key} train\")\n        plt.plot(epochs, d[\"losses\"][\"val\"], marker=\"x\", label=f\"{key} val\")\n    plt.title(\"Usage-based Model Loss Curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"usage_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Plot 2: Retention ratio curves\ntry:\n    plt.figure()\n    for key, d in experiment_data[\"usage_based\"].items():\n        epochs = np.arange(1, len(d[\"metrics\"][\"train\"]) + 1)\n        plt.plot(\n            epochs, d[\"metrics\"][\"train\"], marker=\"o\", label=f\"{key} train retention\"\n        )\n        plt.plot(epochs, d[\"metrics\"][\"val\"], marker=\"x\", label=f\"{key} val retention\")\n    plt.title(\"Usage-based Model Retention Ratios\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Retention Ratio\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"usage_retention_ratios.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating retention ratios plot: {e}\")\n    plt.close()\n\n# Plot 3: Predictions vs Ground Truth\ntry:\n    keys = list(experiment_data[\"usage_based\"].keys())\n    n = len(keys)\n    fig, axes = plt.subplots(n, 2, figsize=(10, 4 * n))\n    for i, key in enumerate(keys):\n        d = experiment_data[\"usage_based\"][key]\n        gt = d[\"ground_truth\"][:100]\n        pred = d[\"predictions\"][:100]\n        axes[i, 0].plot(gt, color=\"blue\")\n        axes[i, 0].set_title(f\"{key} Ground Truth\")\n        axes[i, 1].plot(pred, color=\"orange\")\n        axes[i, 1].set_title(f\"{key} Predictions\")\n    fig.suptitle(\n        \"Usage-based Predictions vs Ground Truth (Left: Ground Truth, Right: Predictions)\"\n    )\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(os.path.join(working_dir, \"usage_predictions_vs_ground_truth.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating predictions vs ground truth plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor head_agg, ds_dict in experiment_data.items():\n    for ds_name, ds_data in ds_dict.items():\n        epochs = range(1, len(ds_data[\"losses\"][\"train\"]) + 1)\n        # Loss curves\n        try:\n            plt.figure()\n            plt.plot(epochs, ds_data[\"losses\"][\"train\"], label=\"Train Loss\")\n            plt.plot(epochs, ds_data[\"losses\"][\"val\"], label=\"Val Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.title(f\"{ds_name} ({head_agg}) Loss Curves\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_{head_agg}_loss.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {head_agg}-{ds_name}: {e}\")\n            plt.close()\n        # Memory Retention Ratio\n        try:\n            plt.figure()\n            plt.plot(\n                epochs,\n                ds_data[\"metrics\"][\"Memory Retention Ratio\"][\"train\"],\n                label=\"Train MRR\",\n            )\n            plt.plot(\n                epochs,\n                ds_data[\"metrics\"][\"Memory Retention Ratio\"][\"val\"],\n                label=\"Val MRR\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Memory Retention Ratio\")\n            plt.title(f\"{ds_name} ({head_agg}) Memory Retention Ratio\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_{head_agg}_mrr.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating MRR plot for {head_agg}-{ds_name}: {e}\")\n            plt.close()\n        # Entropy-Weighted Memory Efficiency\n        try:\n            plt.figure()\n            plt.plot(\n                epochs,\n                ds_data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"],\n                label=\"Train EME\",\n            )\n            plt.plot(\n                epochs,\n                ds_data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"],\n                label=\"Val EME\",\n            )\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Entropy-Weighted Memory Efficiency\")\n            plt.title(f\"{ds_name} ({head_agg}) EME\")\n            plt.legend()\n            plt.savefig(os.path.join(working_dir, f\"{ds_name}_{head_agg}_eme.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating EME plot for {head_agg}-{ds_name}: {e}\")\n            plt.close()\n        # Sample comparison\n        try:\n            gt_seq = ds_data.get(\"ground_truth\", [[]])[0][:50]\n            pred_seq = ds_data.get(\"predictions\", [[]])[0][:50]\n            plt.figure(figsize=(8, 3))\n            plt.subplot(1, 2, 1)\n            plt.plot(gt_seq, marker=\"o\")\n            plt.title(\"Ground Truth\")\n            plt.xlabel(\"Token Index\")\n            plt.ylabel(\"Token ID\")\n            plt.subplot(1, 2, 2)\n            plt.plot(pred_seq, marker=\"x\")\n            plt.title(\"Generated Samples\")\n            plt.suptitle(\n                f\"{ds_name} ({head_agg}) - Left: Ground Truth, Right: Generated Samples\"\n            )\n            plt.tight_layout(rect=[0, 0, 1, 0.95])\n            plt.savefig(\n                os.path.join(working_dir, f\"{ds_name}_{head_agg}_sample_comp.png\")\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating sample comparison for {head_agg}-{ds_name}: {e}\")\n            plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    exp_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    datasets = exp_data.get(\"residual_connection_ablation\", {})\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    datasets = {}\n\n# Plot loss curves\nfor ds, data in datasets.items():\n    try:\n        plt.figure()\n        epochs = list(range(1, len(data[\"losses\"][\"train\"]) + 1))\n        plt.plot(epochs, data[\"losses\"][\"train\"], label=\"Train Loss\")\n        plt.plot(epochs, data[\"losses\"][\"val\"], label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ds} - Loss Curve\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ds}_loss_curve.png\"))\n    except Exception as e:\n        print(f\"Error creating loss curve for {ds}: {e}\")\n    finally:\n        plt.close()\n\n# Plot Memory Retention Ratio\nfor ds, data in datasets.items():\n    try:\n        plt.figure()\n        epochs = list(\n            range(1, len(data[\"metrics\"][\"Memory Retention Ratio\"][\"train\"]) + 1)\n        )\n        plt.plot(\n            epochs,\n            data[\"metrics\"][\"Memory Retention Ratio\"][\"train\"],\n            label=\"Train MRR\",\n        )\n        plt.plot(\n            epochs, data[\"metrics\"][\"Memory Retention Ratio\"][\"val\"], label=\"Val MRR\"\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Memory Retention Ratio\")\n        plt.title(f\"{ds} - Memory Retention Ratio Curve\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ds}_memory_retention_ratio.png\"))\n    except Exception as e:\n        print(f\"Error creating Memory Retention Ratio for {ds}: {e}\")\n    finally:\n        plt.close()\n\n# Plot Entropy-Weighted Memory Efficiency\nfor ds, data in datasets.items():\n    try:\n        plt.figure()\n        epochs = list(\n            range(\n                1,\n                len(data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"]) + 1,\n            )\n        )\n        plt.plot(\n            epochs,\n            data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"],\n            label=\"Train EME\",\n        )\n        plt.plot(\n            epochs,\n            data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"],\n            label=\"Val EME\",\n        )\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Entropy-Weighted Memory Efficiency\")\n        plt.title(f\"{ds} - Entropy-Weighted Memory Efficiency Curve\")\n        plt.legend()\n        plt.savefig(\n            os.path.join(working_dir, f\"{ds}_entropy_weighted_memory_efficiency.png\")\n        )\n    except Exception as e:\n        print(f\"Error creating EME curve for {ds}: {e}\")\n    finally:\n        plt.close()\n\n# Plot predictions vs ground truth tokens\nfor ds, data in datasets.items():\n    try:\n        gt = data.get(\"ground_truth\")\n        preds = data.get(\"predictions\")\n        if gt is None or preds is None or len(gt) == 0:\n            continue\n        n = min(100, len(gt))\n        fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n        axes[0].plot(gt[:n], color=\"blue\")\n        axes[0].set_title(\"Ground Truth Tokens\")\n        axes[1].plot(preds[:n], color=\"orange\")\n        axes[1].set_title(\"Predicted Tokens\")\n        fig.suptitle(f\"{ds} - Left: Ground Truth, Right: Predicted Tokens\")\n        fig.savefig(os.path.join(working_dir, f\"{ds}_tokens_comparison.png\"))\n    except Exception as e:\n        print(f\"Error creating token comparison for {ds}: {e}\")\n    finally:\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n# Plotting loops\nfor ablation, ab_data in exp.items():\n    for ds, ds_data in ab_data.items():\n        # Loss curves\n        try:\n            tr = ds_data[\"losses\"][\"train\"]\n            va = ds_data[\"losses\"][\"val\"]\n            epochs = np.arange(1, len(tr) + 1)\n            plt.figure()\n            plt.suptitle(f\"{ds} - {ablation.title()} Ablation\")\n            plt.title(\"Loss over Epochs\")\n            plt.plot(epochs, tr, marker=\"o\", label=\"Train\")\n            plt.plot(epochs, va, marker=\"o\", linestyle=\"--\", label=\"Val\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            fname = f\"{ablation}_{ds}_loss.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {ablation}, {ds}: {e}\")\n            plt.close()\n\n        # Metric curves\n        for metric, splits in ds_data.get(\"metrics\", {}).items():\n            try:\n                trm = splits[\"train\"]\n                vam = splits[\"val\"]\n                epochs = np.arange(1, len(trm) + 1)\n                plt.figure()\n                plt.suptitle(f\"{ds} - {ablation.title()} Ablation\")\n                plt.title(metric)\n                plt.plot(epochs, trm, marker=\"o\", label=\"Train\")\n                plt.plot(epochs, vam, marker=\"o\", linestyle=\"--\", label=\"Val\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(metric)\n                plt.legend()\n                metric_clean = metric.lower().replace(\" \", \"_\")\n                fname = f\"{ablation}_{ds}_{metric_clean}.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating {metric} plot for {ablation}, {ds}: {e}\")\n                plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nfor ablation, ab_data in experiment_data.items():\n    try:\n        plt.figure()\n        for ds_key, ds_data in ab_data.items():\n            epochs = range(1, len(ds_data[\"losses\"][\"train\"]) + 1)\n            plt.plot(epochs, ds_data[\"losses\"][\"train\"], label=f\"{ds_key} train\")\n            plt.plot(epochs, ds_data[\"losses\"][\"val\"], label=f\"{ds_key} val\")\n        plt.title(f\"{ablation} Loss Curves\\nDatasets comparison\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ablation}_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ablation}: {e}\")\n        plt.close()\n\n    try:\n        plt.figure()\n        for ds_key, ds_data in ab_data.items():\n            epochs = range(\n                1, len(ds_data[\"metrics\"][\"Entropy Retention Ratio\"][\"train\"]) + 1\n            )\n            plt.plot(\n                epochs,\n                ds_data[\"metrics\"][\"Entropy Retention Ratio\"][\"train\"],\n                label=f\"{ds_key} train\",\n            )\n            plt.plot(\n                epochs,\n                ds_data[\"metrics\"][\"Entropy Retention Ratio\"][\"val\"],\n                label=f\"{ds_key} val\",\n            )\n        plt.title(f\"{ablation} Entropy Retention Ratio\\nDatasets comparison\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Entropy Retention Ratio\")\n        plt.legend()\n        plt.savefig(\n            os.path.join(working_dir, f\"{ablation}_entropy_retention_ratio_curves.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating entropy retention ratio plot for {ablation}: {e}\")\n        plt.close()\n\n    try:\n        plt.figure()\n        for ds_key, ds_data in ab_data.items():\n            epochs = range(\n                1,\n                len(ds_data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"])\n                + 1,\n            )\n            plt.plot(\n                epochs,\n                ds_data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"],\n                label=f\"{ds_key} train\",\n            )\n            plt.plot(\n                epochs,\n                ds_data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"],\n                label=f\"{ds_key} val\",\n            )\n        plt.title(f\"{ablation} Entropy-Weighted Memory Efficiency\\nDatasets comparison\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Entropy-Weighted Memory Efficiency\")\n        plt.legend()\n        plt.savefig(\n            os.path.join(\n                working_dir, f\"{ablation}_entropy_weighted_memory_efficiency_curves.png\"\n            )\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating memory efficiency plot for {ablation}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n# Plotting loops\nfor ablation, ab_data in exp.items():\n    for ds, ds_data in ab_data.items():\n        # Loss curves\n        try:\n            tr = ds_data[\"losses\"][\"train\"]\n            va = ds_data[\"losses\"][\"val\"]\n            epochs = np.arange(1, len(tr) + 1)\n            plt.figure()\n            plt.suptitle(f\"{ds} - {ablation.title()} Ablation\")\n            plt.title(\"Loss over Epochs\")\n            plt.plot(epochs, tr, marker=\"o\", label=\"Train\")\n            plt.plot(epochs, va, marker=\"o\", linestyle=\"--\", label=\"Val\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            fname = f\"{ablation}_{ds}_loss.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {ablation}, {ds}: {e}\")\n            plt.close()\n\n        # Metric curves\n        for metric, splits in ds_data.get(\"metrics\", {}).items():\n            try:\n                trm = splits[\"train\"]\n                vam = splits[\"val\"]\n                epochs = np.arange(1, len(trm) + 1)\n                plt.figure()\n                plt.suptitle(f\"{ds} - {ablation.title()} Ablation\")\n                plt.title(metric)\n                plt.plot(epochs, trm, marker=\"o\", label=\"Train\")\n                plt.plot(epochs, vam, marker=\"o\", linestyle=\"--\", label=\"Val\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(metric)\n                plt.legend()\n                metric_clean = metric.lower().replace(\" \", \"_\")\n                fname = f\"{ablation}_{ds}_{metric_clean}.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating {metric} plot for {ablation}, {ds}: {e}\")\n                plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n# Plotting loops\nfor ablation, ab_data in exp.items():\n    for ds, ds_data in ab_data.items():\n        # Loss curves\n        try:\n            tr = ds_data[\"losses\"][\"train\"]\n            va = ds_data[\"losses\"][\"val\"]\n            epochs = np.arange(1, len(tr) + 1)\n            plt.figure()\n            plt.suptitle(f\"{ds} - {ablation.title()} Ablation\")\n            plt.title(\"Loss over Epochs\")\n            plt.plot(epochs, tr, marker=\"o\", label=\"Train\")\n            plt.plot(epochs, va, marker=\"o\", linestyle=\"--\", label=\"Val\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            fname = f\"{ablation}_{ds}_loss.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {ablation}, {ds}: {e}\")\n            plt.close()\n\n        # Metric curves\n        for metric, splits in ds_data.get(\"metrics\", {}).items():\n            try:\n                trm = splits[\"train\"]\n                vam = splits[\"val\"]\n                epochs = np.arange(1, len(trm) + 1)\n                plt.figure()\n                plt.suptitle(f\"{ds} - {ablation.title()} Ablation\")\n                plt.title(metric)\n                plt.plot(epochs, trm, marker=\"o\", label=\"Train\")\n                plt.plot(epochs, vam, marker=\"o\", linestyle=\"--\", label=\"Val\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(metric)\n                plt.legend()\n                metric_clean = metric.lower().replace(\" \", \"_\")\n                fname = f\"{ablation}_{ds}_{metric_clean}.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating {metric} plot for {ablation}, {ds}: {e}\")\n                plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n# Plotting loops\nfor ablation, ab_data in exp.items():\n    for ds, ds_data in ab_data.items():\n        # Loss curves\n        try:\n            tr = ds_data[\"losses\"][\"train\"]\n            va = ds_data[\"losses\"][\"val\"]\n            epochs = np.arange(1, len(tr) + 1)\n            plt.figure()\n            plt.suptitle(f\"{ds} - {ablation.title()} Ablation\")\n            plt.title(\"Loss over Epochs\")\n            plt.plot(epochs, tr, marker=\"o\", label=\"Train\")\n            plt.plot(epochs, va, marker=\"o\", linestyle=\"--\", label=\"Val\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            fname = f\"{ablation}_{ds}_loss.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating loss plot for {ablation}, {ds}: {e}\")\n            plt.close()\n\n        # Metric curves\n        for metric, splits in ds_data.get(\"metrics\", {}).items():\n            try:\n                trm = splits[\"train\"]\n                vam = splits[\"val\"]\n                epochs = np.arange(1, len(trm) + 1)\n                plt.figure()\n                plt.suptitle(f\"{ds} - {ablation.title()} Ablation\")\n                plt.title(metric)\n                plt.plot(epochs, trm, marker=\"o\", label=\"Train\")\n                plt.plot(epochs, vam, marker=\"o\", linestyle=\"--\", label=\"Val\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(metric)\n                plt.legend()\n                metric_clean = metric.lower().replace(\" \", \"_\")\n                fname = f\"{ablation}_{ds}_{metric_clean}.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating {metric} plot for {ablation}, {ds}: {e}\")\n                plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Paths to the three replicates\nexperiment_data_path_list = [\n    \"experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_07421c1465bc4311a832a0670685bf7c_proc_3980098/experiment_data.npy\",\n    \"experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_5fc1910ac4d34fbdad984d95ba748f5d_proc_3980098/experiment_data.npy\",\n    \"experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_f22cf116ae2045f98cb77a0b0fb55bf1_proc_3980098/experiment_data.npy\",\n]\n\n# Load experiment data from all replicates\nall_experiments = []\ntry:\n    for rel_path in experiment_data_path_list:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), rel_path)\n        data = np.load(full_path, allow_pickle=True).item()\n        all_experiments.append(data)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nif not all_experiments:\n    print(\"No data loaded, exiting.\")\nelse:\n    # Use the first replicate to get dataset and metric keys\n    for ds in all_experiments[0].keys():\n        # Aggregate losses\n        try:\n            train_losses = []\n            val_losses = []\n            for exp in all_experiments:\n                ds_data = exp.get(ds, {})\n                train_losses.append(ds_data[\"losses\"][\"train\"])\n                val_losses.append(ds_data[\"losses\"][\"val\"])\n            train_losses = np.array(train_losses)\n            val_losses = np.array(val_losses)\n            epochs = np.arange(1, train_losses.shape[1] + 1)\n            mean_tr = train_losses.mean(axis=0)\n            sem_tr = train_losses.std(axis=0) / np.sqrt(train_losses.shape[0])\n            mean_va = val_losses.mean(axis=0)\n            sem_va = val_losses.std(axis=0) / np.sqrt(val_losses.shape[0])\n\n            plt.figure()\n            plt.suptitle(f\"{ds} - Aggregated over {len(all_experiments)} runs\")\n            plt.title(\"Loss over Epochs with SEM\")\n            plt.plot(epochs, mean_tr, marker=\"o\", label=\"Train Mean\")\n            plt.fill_between(epochs, mean_tr - sem_tr, mean_tr + sem_tr, alpha=0.3)\n            plt.plot(epochs, mean_va, marker=\"o\", linestyle=\"--\", label=\"Val Mean\")\n            plt.fill_between(epochs, mean_va - sem_va, mean_va + sem_va, alpha=0.3)\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            fname = f\"{ds}_aggregate_loss_sem.png\"\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating aggregated loss plot for {ds}: {e}\")\n            plt.close()\n\n        # Aggregate each metric\n        for metric, splits in all_experiments[0][ds].get(\"metrics\", {}).items():\n            try:\n                train_vals = []\n                val_vals = []\n                for exp in all_experiments:\n                    m = exp[ds][\"metrics\"][metric]\n                    train_vals.append(m[\"train\"])\n                    val_vals.append(m[\"val\"])\n                train_vals = np.array(train_vals)\n                val_vals = np.array(val_vals)\n                epochs = np.arange(1, train_vals.shape[1] + 1)\n                mean_trm = train_vals.mean(axis=0)\n                sem_trm = train_vals.std(axis=0) / np.sqrt(train_vals.shape[0])\n                mean_vam = val_vals.mean(axis=0)\n                sem_vam = val_vals.std(axis=0) / np.sqrt(val_vals.shape[0])\n\n                plt.figure()\n                plt.suptitle(f\"{ds} - {metric} Aggregated\")\n                plt.title(f\"{metric} over Epochs with SEM\")\n                plt.plot(epochs, mean_trm, marker=\"o\", label=\"Train Mean\")\n                plt.fill_between(\n                    epochs, mean_trm - sem_trm, mean_trm + sem_trm, alpha=0.3\n                )\n                plt.plot(epochs, mean_vam, marker=\"o\", linestyle=\"--\", label=\"Val Mean\")\n                plt.fill_between(\n                    epochs, mean_vam - sem_vam, mean_vam + sem_vam, alpha=0.3\n                )\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(metric)\n                plt.legend()\n                metric_clean = metric.lower().replace(\" \", \"_\")\n                fname = f\"{ds}_aggregate_{metric_clean}_sem.png\"\n                plt.savefig(os.path.join(working_dir, fname))\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating aggregated {metric} plot for {ds}: {e}\")\n                plt.close()\n"], "plot_plan": ["We will load the saved experiment_data.npy and then print out the final validation loss, memory retention ratio, and entropy\u2010weighted memory efficiency for each dataset. Next, we prepare three comparison plots across all datasets: loss curves, memory retention ratios, and entropy\u2010weighted memory efficiency. For each plot we iterate over the datasets, plotting both training and validation series with distinct linestyles and comprehensive legends. Titles will include the metric name plus a subtitle listing the datasets being compared. All plotting is wrapped in separate try\u2010except blocks so that any error is caught without stopping subsequent figures, and we always close the figure after saving. Filenames are descriptive and include \u201call_datasets\u201d plus the specific metric type. The code uses only matplotlib, numpy, and os, and strictly saves plots to the working directory. We ensure concise, clear visuals and error handling per the guidelines.", "We will load the saved experiment data and iterate over each dataset key to extract the train and validation curves for loss, memory retention ratio (MRR), and entropy\u2010weighted memory efficiency (EWME) for both the original and recency ablation.  For each metric we will create a separate figure: plotting both ablations\u2019 train and validation curves, labeling axes and adding a legend.  Each figure will be wrapped in its own try\u2010except to catch any errors, printing an error message if plotting fails and always closing the figure.  Figures will be saved under working_dir with filenames including the dataset name and metric type.  This ensures at most three plots per dataset (three metrics) and stays within the 5\u2010figure guideline.", null, "I will load the saved experiment_data from the working directory and print a summary of the final validation loss, memory retention ratio, and entropy\u2010weighted memory efficiency for each ablation and dataset. Then I\u2019ll iterate over each dataset key and create three separate plots (loss curve, memory retention ratio, and entropy\u2010weighted memory efficiency), each wrapped in its own try\u2010except block to handle errors gracefully. For each plot, I\u2019ll draw solid lines for training and dashed lines for validation for both ablation types. The figures will include titles specifying the dataset and metric, with a subtitle clarifying line styles, then saved to the working directory with descriptive filenames. I will close each figure after saving or upon catching an exception. All code uses basic matplotlib without custom styles and only visualizes data loaded from experiment_data.npy.", "We first load the experiment data from the saved numpy file in the working directory. Then we iterate through each ablation variant and each dataset, extracting training and validation curves for losses, memory retention ratio, and entropy-weighted memory efficiency. For each metric we create a separate plot in its own try\u2010except block, adding titles that include both the dataset and ablation names, axis labels, and a legend distinguishing train vs val. Figures are saved into the working directory with descriptive filenames containing dataset, ablation, and metric type. We ensure each figure is closed in both success and error cases to free up resources. Any errors during plotting are caught and printed without stopping the script.", "First, we load the saved experiment_data.npy from the working directory. We then iterate over each dataset variant under the \"no_layernorm\" key and extract the stored training and validation losses, as well as the two memory-based metrics per epoch. For each dataset, we create and save three line plots: one for loss, one for Memory Retention Ratio, and one for Entropy-Weighted Memory Efficiency, each comparing training vs validation. Each plot is wrapped in its own try-except block to handle errors gracefully and always closes the figure after saving. Next, for each dataset we render up to five sample figures showing the ground truth sequence vs the model-generated sequence side by side, with a clear subtitle indicating \"Left: Ground Truth, Right: Generated Samples.\" These prediction plots are also saved separately with descriptive names and protected by try-except to ensure the figure is closed on error. Finally, we print out the final validation loss, Memory Retention Ratio, and Entropy-Weighted Memory Efficiency for each dataset to summarize performance. All plots are saved into \"working\" under files named with the dataset and plot type.", "We begin by loading the saved experiment_data.npy from the working directory into a Python dictionary. We then list our three plotting targets: loss, memory retention ratio, and entropy-weighted memory efficiency. For each target metric, we wrap the plotting logic in a try-except block to handle any errors gracefully. Inside each block we generate a matplotlib figure and iterate over all ablation settings and datasets, plotting both training (solid) and validation (dashed) curves for each. Each curve is annotated with its ablation type and dataset, and we set clear titles and subtitles that specify the metric and data context. Axes are labeled, a legend is added, and the figure is saved to working_dir with a descriptive file name. We always call plt.close() to release resources after saving. Any exceptions during plotting are caught and printed to avoid silent failures.", "We load the saved experiment data and print out the final validation loss, memory retention ratio, and entropy-weighted memory efficiency for each mode and dataset. Then we generate three figures\u2014one each for loss curves, memory retention ratio, and entropy-weighted memory efficiency\u2014using side-by-side subplots for training vs. validation. In each plot we iterate over both ablation modes (\u201centropy\u201d and \u201cnorm\u201d) and all datasets, plotting one line per mode-dataset combination with appropriate labels. Each figure is wrapped in its own try-except block, and we save them under descriptive names in the working directory. We also ensure that each figure is closed after saving to free resources.", null, null, null, "We will load the saved `experiment_data.npy` and iterate over each dataset key. For each dataset, we create a single figure with three horizontal subplots showing (left to right) the loss curves, memory retention ratio curves, and entropy\u2010weighted memory efficiency curves. Each subplot plots both training (solid lines) and validation (dashed lines) for both \"baseline\" and \"feedforward_identity\" ablations. We add a combined suptitle specifying the dataset and a subtitle \u201cLeft: Loss; Middle: Memory Retention Ratio; Right: Entropy-Weighted Memory Efficiency.\u201d Axes are labeled, legends are added, and figures are saved into `working_dir` using descriptive filenames. Each figure creation and saving is wrapped in a try-except block, and figures are properly closed after saving.", "First, we load the saved experiment data from the working directory and catch any loading errors. We then summarize the final validation loss for both ablations and all datasets. Next, we iterate through the three metrics\u2014loss, Memory Retention Ratio, and Entropy\u2010Weighted Memory Efficiency\u2014and for each dataset we plot both train and validation curves for baseline versus continuous_memory. Each figure is created in a try\u2010except block, given a clear title with the metric and dataset name, labeled lines, and saved to the working directory with a descriptive filename. Finally, each plot is closed immediately after saving to free resources.", "We load the saved experiment data and then generate three sets of plots in separate try\u2010except blocks. First, we plot the training and validation loss curves across epochs for each dataset on a single figure and save it. Next, we plot the retention\u2010ratio metrics (train vs. validation) for each dataset in another figure. Finally, we produce a multi\u2010panel figure comparing the first 100 ground\u2010truth tokens to the model\u2019s predictions, with side\u2010by\u2010side subplots for each dataset and a clear suptitle noting \u201cLeft: Ground Truth, Right: Predictions.\u201d All figures are titled, saved to the working directory with descriptive filenames, and figures are closed after saving even on error.", "I will load the saved experiment data and iterate over each head aggregation method and dataset. For each, I will plot the training and validation losses against epochs, followed by memory retention ratio and entropy-weighted memory efficiency curves. Each plot will be wrapped in its own try-except block, ensuring any errors are caught and figures are always closed. Additionally, I'll create a sample comparison plot showing the first validation sequence as ground truth on the left and generated/predicted tokens on the right, with clear subtitles and titles including the dataset name and aggregation method. All figures will be saved to the working directory with descriptive filenames and closed properly.", null, "I load the stored experiment_data from the working directory and extract the 'residual_connection_ablation' results. For each dataset, I generate and save the train/validation loss curve across epochs. Then I plot the Memory Retention Ratio (train vs val) for each dataset. Next, I plot the Entropy-Weighted Memory Efficiency over epochs. To compare model outputs qualitatively, I plot the first 100 predicted token IDs against the ground truth token IDs side by side as two subplots. Each plotting block is wrapped in its own try-except to catch errors without stopping the pipeline. All figures are titled with dataset names and descriptive subtitles, and saved with clear filenames under working_dir. Figures are closed after saving to free resources.", "The code will set up the working directory and load the saved `experiment_data.npy`, then iterate through each ablation setting and dataset. For each case it will plot training vs validation curves for losses, memory retention ratio, and score\u2010weighted memory efficiency, making sure not to fabricate any data. Each plot is wrapped in its own try\u2010except block to catch errors and always close the figure. The x\u2010axis denotes epochs, and the y\u2010axis shows the corresponding metric. Titles clearly state the ablation type and dataset, while legends distinguish training and validation. Filenames include ablation, dataset, and metric names for clarity, and all plots are saved to `working_dir`. Only existing data from the experiment file are used, and basic matplotlib calls are employed without custom styling.", "First, load the experiment_data dictionary from the numpy file.  \nIterate over each ablation configuration (e.g., baseline and memory_key_only).  \nFor each ablation, create a single loss curve comparing training and validation loss across all datasets.  \nThen plot two additional figures per ablation: one for Entropy Retention Ratio and another for Entropy-Weighted Memory Efficiency, again comparing datasets.  \nEach plot is wrapped in its own try-except block, printing any errors and always closing the figure.  \nTitles include the ablation name with a clear subtitle \u201cDatasets comparison.\u201d  \nAll plots are saved to the working directory with descriptive filenames that include the ablation and metric names.  \nThis ensures we only visualize the actual data and adhere to matplotlib and file-naming guidelines.", null, null, null, null], "ablation_name": [null, "Recency-Based Memory Retention Ablation", "Memory Gradient Flow Ablation", "Random Memory Retention Ablation", "Feedforward Sub-layer Removal Ablation", "Layer Normalization Ablation", "No Memory Attention Ablation", "Norm-Based Memory Retention Ablation", null, null, "Attention-Usage\u2013Based Memory Retention Ablation", "Feedforward Activation Ablation", "Memory Reset Policy Ablation", null, "Head Aggregation Function Ablation", "Memory Key-Only Ablation", "Residual Connection Ablation", "Gradient-Based Memory Retention Ablation", null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script sets the working directory and loads the `experiment_data.npy` file\nas a pickled dictionary. It then iterates over each dataset entry, extracts the\nfinal epoch values for training and validation loss as well as the two metrics\n(\"Memory Retention Ratio\" and \"Entropy-Weighted Memory Efficiency\"), and prints\nthem with fully specified labels.", "The script below loads the `experiment_data.npy` file from the working\ndirectory, iterates over each model variant and dataset, and prints the final\nepoch values for training and validation losses as well as the memory retention\nratio and entropy\u2010weighted memory efficiency. Each metric is labeled with a\nclear, descriptive name (e.g., \u201cFinal training loss,\u201d \u201cFinal validation memory\nretention ratio\u201d) and formatted to four decimal places. No plotting or\nentry\u2010point guards are used, and the code executes immediately at the global\nscope.", "", "I will load the saved NumPy file, iterate over each ablation and dataset, and\nthen print only the final recorded values for training and validation losses as\nwell as for \u201cMemory Retention Ratio\u201d and \u201cEntropy-Weighted Memory Efficiency,\u201d\nprefixing each line with clear labels.", "The script below loads the saved `experiment_data.npy` from the working\ndirectory, iterates over each ablation variant and dataset, then extracts the\nfinal (last\u2010epoch) values for training and validation loss, Memory Retention\nRatio, and Entropy\u2010Weighted Memory Efficiency. Each dataset and ablation\ncombination is printed first, followed by clearly labeled metric values. No\nplots are generated; all operations occur at global scope and execute\nimmediately.", "The following script loads the saved NumPy file, extracts the \u201cno_layernorm\u201d\nexperiments, and iterates through each dataset. For each metric (Memory\nRetention Ratio and Entropy-Weighted Memory Efficiency), it retrieves the values\nrecorded over epochs and prints out the final training and validation values\nwith clear labels. All code runs at the global scope without any `__main__`\nguard.", "The following script loads the saved experiment data from the `working` folder,\niterates through each ablation setting and dataset, and prints the final epoch\nvalues for training loss, validation loss, training/validation Memory Retention\nRatio, and training/validation Entropy-Weighted Memory Efficiency. Each dataset\nname (including the ablation variant) is printed before its associated metrics,\nand each metric is clearly labeled with a full descriptive name. The script runs\nimmediately when executed without any additional entry-point guard or plotting.", "The script imports os and numpy, loads the experiment_data.npy file from the\nworking directory, and then iterates over each ablation mode and dataset entry.\nFor each dataset, it extracts the lists of train and validation losses and\nmetrics, and selects the final value of each list. It prints the dataset name\n(including mode) and then each metric with a precise label (for example, 'final\ntrain loss' or 'final validation Memory Retention Ratio'). This code runs\nimmediately at the global scope without any main guard.", "", "", "I will load the saved experiment data, loop through each dataset under the\n\"usage_based\" key, and extract the final values of retention ratios and losses.\nFor each dataset, I print its name followed by clearly labeled metrics: final\ntraining retention, final validation retention, final training loss, and final\nvalidation loss. The script will execute immediately without any `if __name__ ==\n\"__main__\":` guard.", "I will load the saved experiment data from the \u201cworking\u201d directory, then iterate\nover each ablation and dataset combination to extract the final epoch\u2019s training\nand validation loss, memory retention ratio, and entropy-weighted memory\nefficiency. For each dataset, I print its name (including the ablation) followed\nby clearly labeled metrics. The script runs immediately at import time without\nany special entry point.", "The following script loads the saved experiment data and iterates over each\nablation and dataset, extracting the final epoch values of Memory Retention\nRatio and Entropy-Weighted Memory Efficiency for both the training and\nvalidation splits. It prints the ablation condition and dataset name first, then\nreports each metric with clear labels like \u201cTraining Memory Retention Ratio\u201d and\n\u201cValidation Entropy-Weighted Memory Efficiency.\u201d The script runs immediately\nwhen executed and does not require any special entry point or plotting\nlibraries.", "I will load the NumPy file from the `working` directory, access the\n`usage_based` section of the experiment data, and then iterate over each dataset\nkey to retrieve the final epoch values for train/validation retention ratios and\nlosses. For each dataset I will print its name followed by clearly labeled\nmetrics: final train retention ratio, final validation retention ratio, final\ntrain loss, and final validation loss. The script runs immediately upon\nexecution and does not require an entry-point guard.", "The following script loads the saved experiment data, iterates over each\nablation setting and dataset, and retrieves the final training and validation\nvalues for both Memory Retention Ratio and Entropy-Weighted Memory Efficiency.\nIt prints each dataset name followed by clearly labeled metrics. This runs\nimmediately upon execution and requires no special entry point.", "", "I will load the saved NumPy file from the working directory, access the\n`residual_connection_ablation` section, and iterate over each dataset key. For\neach dataset, I will extract the last recorded values of train and validation\nloss, Memory Retention Ratio, and Entropy-Weighted Memory Efficiency, formatting\neach metric label clearly. The script runs immediately without an entry-point\nguard and prints each dataset name followed by its final metrics.", "I\u2019ll load the saved `experiment_data.npy` from the `working` directory with\n`allow_pickle=True`, extract the nested dictionary, and iterate over both the\nentropy\u2010based and gradient\u2010based experiments. For each dataset, I\u2019ll print its\nname (including which memory ablation was used) followed by the final epoch\u2019s\ntraining and validation losses, memory retention ratios, and score\u2010weighted\nmemory efficiencies, each with a clear, self\u2010contained label. All code is at the\nglobal scope so it runs immediately upon execution.", "The following script loads the saved experiment data from the working directory\nand iterates over each ablation and dataset combination. For each, it prints the\ndataset name and ablation, then extracts and prints the final epoch\u2019s training\nand validation loss, entropy retention ratio, and entropy\u2010weighted memory\nefficiency with clear, descriptive labels. The code runs immediately upon\nexecution without any special entry point.", "I\u2019ll load the saved `experiment_data.npy` from the `working` directory with\n`allow_pickle=True`, extract the nested dictionary, and iterate over both the\nentropy\u2010based and gradient\u2010based experiments. For each dataset, I\u2019ll print its\nname (including which memory ablation was used) followed by the final epoch\u2019s\ntraining and validation losses, memory retention ratios, and score\u2010weighted\nmemory efficiencies, each with a clear, self\u2010contained label. All code is at the\nglobal scope so it runs immediately upon execution.", "I\u2019ll load the saved `experiment_data.npy` from the `working` directory with\n`allow_pickle=True`, extract the nested dictionary, and iterate over both the\nentropy\u2010based and gradient\u2010based experiments. For each dataset, I\u2019ll print its\nname (including which memory ablation was used) followed by the final epoch\u2019s\ntraining and validation losses, memory retention ratios, and score\u2010weighted\nmemory efficiencies, each with a clear, self\u2010contained label. All code is at the\nglobal scope so it runs immediately upon execution.", "I\u2019ll load the saved `experiment_data.npy` from the `working` directory with\n`allow_pickle=True`, extract the nested dictionary, and iterate over both the\nentropy\u2010based and gradient\u2010based experiments. For each dataset, I\u2019ll print its\nname (including which memory ablation was used) followed by the final epoch\u2019s\ntraining and validation losses, memory retention ratios, and score\u2010weighted\nmemory efficiencies, each with a clear, self\u2010contained label. All code is at the\nglobal scope so it runs immediately upon execution.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Define working directory and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, stats in experiment_data.items():\n    print(f\"\\nDataset: {dataset_name}\")\n    # Extract final training and validation loss\n    final_train_loss = stats[\"losses\"][\"train\"][-1]\n    final_val_loss = stats[\"losses\"][\"val\"][-1]\n    print(f\"Training loss: {final_train_loss:.4f}\")\n    print(f\"Validation loss: {final_val_loss:.4f}\")\n    # Extract final Memory Retention Ratio\n    train_mrr = stats[\"metrics\"][\"Memory Retention Ratio\"][\"train\"][-1]\n    val_mrr = stats[\"metrics\"][\"Memory Retention Ratio\"][\"val\"][-1]\n    print(f\"Training Memory Retention Ratio: {train_mrr:.4f}\")\n    print(f\"Validation Memory Retention Ratio: {val_mrr:.4f}\")\n    # Extract final Entropy-Weighted Memory Efficiency\n    train_eme = stats[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"][-1]\n    val_eme = stats[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"][-1]\n    print(f\"Training Entropy-Weighted Memory Efficiency: {train_eme:.4f}\")\n    print(f\"Validation Entropy-Weighted Memory Efficiency: {val_eme:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each model variant and dataset, printing final epoch metrics\nfor model_variant, datasets in experiment_data.items():\n    for dataset_name, stats in datasets.items():\n        print(f\"Dataset: {dataset_name} ({model_variant} model)\")\n        # Extract final epoch values\n        final_train_loss = stats[\"losses\"][\"train\"][-1]\n        final_val_loss = stats[\"losses\"][\"val\"][-1]\n        final_train_mrr = stats[\"metrics\"][\"Memory Retention Ratio\"][\"train\"][-1]\n        final_val_mrr = stats[\"metrics\"][\"Memory Retention Ratio\"][\"val\"][-1]\n        final_train_eme = stats[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\n            \"train\"\n        ][-1]\n        final_val_eme = stats[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"][\n            -1\n        ]\n\n        # Print metrics with clear labels\n        print(f\"Final training loss: {final_train_loss:.4f}\")\n        print(f\"Final validation loss: {final_val_loss:.4f}\")\n        print(f\"Final training Memory Retention Ratio: {final_train_mrr:.4f}\")\n        print(f\"Final validation Memory Retention Ratio: {final_val_mrr:.4f}\")\n        print(\n            f\"Final training Entropy-Weighted Memory Efficiency: {final_train_eme:.4f}\"\n        )\n        print(\n            f\"Final validation Entropy-Weighted Memory Efficiency: {final_val_eme:.4f}\"\n        )\n        print()\n", "", "import os\nimport numpy as np\n\n# 0. Get working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# 1. Load the experiment_data.npy file\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# 2. Iterate through ablations and datasets, extracting final metrics\nfor ablation, datasets in experiment_data.items():\n    for dataset_name, results in datasets.items():\n        # 3. Print dataset and ablation name\n        print(f\"Ablation: {ablation}, Dataset: {dataset_name}\")\n        # 5. Print final loss values\n        final_train_loss = results[\"losses\"][\"train\"][-1]\n        final_val_loss = results[\"losses\"][\"val\"][-1]\n        print(f\"Train loss: {final_train_loss:.4f}\")\n        print(f\"Validation loss: {final_val_loss:.4f}\")\n        # 4. Print final metric values\n        for metric, splits in results[\"metrics\"].items():\n            train_val = splits[\"train\"][-1]\n            val_val = splits[\"val\"][-1]\n            print(f\"Train {metric}: {train_val:.4f}\")\n            print(f\"Validation {metric}: {val_val:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Locate and load the experiment data file\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each ablation and dataset, extracting and printing final metrics\nfor ablation_name, datasets in experiment_data.items():\n    for dataset_name, results in datasets.items():\n        print(f\"Dataset: {dataset_name} (Ablation: {ablation_name})\")\n        # Extract final training and validation losses\n        final_train_loss = results[\"losses\"][\"train\"][-1]\n        final_val_loss = results[\"losses\"][\"val\"][-1]\n        # Extract final training and validation Memory Retention Ratio\n        final_train_mrr = results[\"metrics\"][\"Memory Retention Ratio\"][\"train\"][-1]\n        final_val_mrr = results[\"metrics\"][\"Memory Retention Ratio\"][\"val\"][-1]\n        # Extract final training and validation Entropy-Weighted Memory Efficiency\n        final_train_eme = results[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\n            \"train\"\n        ][-1]\n        final_val_eme = results[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"][\n            -1\n        ]\n\n        # Print all metrics with precise labels\n        print(f\"Final training loss: {final_train_loss:.4f}\")\n        print(f\"Final validation loss: {final_val_loss:.4f}\")\n        print(f\"Final training Memory Retention Ratio: {final_train_mrr:.4f}\")\n        print(f\"Final validation Memory Retention Ratio: {final_val_mrr:.4f}\")\n        print(\n            f\"Final training Entropy-Weighted Memory Efficiency: {final_train_eme:.4f}\"\n        )\n        print(\n            f\"Final validation Entropy-Weighted Memory Efficiency: {final_val_eme:.4f}\"\n        )\n        print()  # Blank line for readability\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Extract the no_layernorm experiments\nno_ln_data = experiment_data.get(\"no_layernorm\", {})\n\n# Iterate over each dataset and print final metric values\nfor dataset_name, data in no_ln_data.items():\n    print(f\"Dataset: {dataset_name}\")\n    metrics = data.get(\"metrics\", {})\n    for metric_name, split_vals in metrics.items():\n        train_vals = split_vals.get(\"train\", [])\n        val_vals = split_vals.get(\"val\", [])\n        if train_vals:\n            print(f\"Final training {metric_name}: {train_vals[-1]:.4f}\")\n        if val_vals:\n            print(f\"Final validation {metric_name}: {val_vals[-1]:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each ablation setting and dataset\nfor ablation_name, datasets in experiment_data.items():\n    for dataset_name, results in datasets.items():\n        # Print dataset and ablation context\n        print(f\"Dataset: {dataset_name} (Ablation: {ablation_name})\")\n        # Extract final epoch values for losses\n        train_loss = results[\"losses\"][\"train\"][-1]\n        val_loss = results[\"losses\"][\"val\"][-1]\n        print(f\"Training loss: {train_loss:.4f}\")\n        print(f\"Validation loss: {val_loss:.4f}\")\n        # Extract final epoch values for Memory Retention Ratio\n        mrr_train = results[\"metrics\"][\"Memory Retention Ratio\"][\"train\"][-1]\n        mrr_val = results[\"metrics\"][\"Memory Retention Ratio\"][\"val\"][-1]\n        print(f\"Training Memory Retention Ratio: {mrr_train:.4f}\")\n        print(f\"Validation Memory Retention Ratio: {mrr_val:.4f}\")\n        # Extract final epoch values for Entropy-Weighted Memory Efficiency\n        ewe_train = results[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"][\n            -1\n        ]\n        ewe_val = results[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"][-1]\n        print(f\"Training Entropy-Weighted Memory Efficiency: {ewe_train:.4f}\")\n        print(f\"Validation Entropy-Weighted Memory Efficiency: {ewe_val:.4f}\")\n        print()  # Blank line for readability\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\ndata_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each ablation mode and dataset\nfor mode, datasets in experiment_data.items():\n    for dataset_name, results in datasets.items():\n        print(f\"Mode: {mode}, Dataset: {dataset_name}\")\n        # Extract final epoch values\n        final_train_loss = results[\"losses\"][\"train\"][-1]\n        final_validation_loss = results[\"losses\"][\"val\"][-1]\n        final_train_retention = results[\"metrics\"][\"Memory Retention Ratio\"][\"train\"][\n            -1\n        ]\n        final_validation_retention = results[\"metrics\"][\"Memory Retention Ratio\"][\n            \"val\"\n        ][-1]\n        final_train_eme = results[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\n            \"train\"\n        ][-1]\n        final_validation_eme = results[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\n            \"val\"\n        ][-1]\n\n        # Print metrics with precise labels\n        print(f\"  final train loss: {final_train_loss:.4f}\")\n        print(f\"  final validation loss: {final_validation_loss:.4f}\")\n        print(f\"  final train Memory Retention Ratio: {final_train_retention:.4f}\")\n        print(\n            f\"  final validation Memory Retention Ratio: {final_validation_retention:.4f}\"\n        )\n        print(\n            f\"  final train Entropy-Weighted Memory Efficiency: {final_train_eme:.4f}\"\n        )\n        print(\n            f\"  final validation Entropy-Weighted Memory Efficiency: {final_validation_eme:.4f}\"\n        )\n        print()\n", "", "", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over each dataset and print final metrics\nfor dataset_name, data in experiment_data[\"usage_based\"].items():\n    # Extract final metrics\n    final_train_retention = data[\"metrics\"][\"train\"][-1]\n    final_val_retention = data[\"metrics\"][\"val\"][-1]\n    final_train_loss = data[\"losses\"][\"train\"][-1]\n    final_val_loss = data[\"losses\"][\"val\"][-1]\n\n    # Print dataset name and metrics with clear labels\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"Final training retention: {final_train_retention:.4f}\")\n    print(f\"Final validation retention: {final_val_retention:.4f}\")\n    print(f\"Final training loss: {final_train_loss:.4f}\")\n    print(f\"Final validation loss: {final_val_loss:.4f}\")\n    print()  # Blank line for readability\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through ablation settings and datasets\nfor ablation_name, ds_results in experiment_data.items():\n    for dataset_name, result in ds_results.items():\n        # Print dataset and ablation\n        print(f\"Dataset: {dataset_name} (Ablation: {ablation_name})\")\n\n        # Extract final training and validation losses\n        train_loss = result[\"losses\"][\"train\"][-1]\n        val_loss = result[\"losses\"][\"val\"][-1]\n        print(f\"train loss: {train_loss:.4f}\")\n        print(f\"validation loss: {val_loss:.4f}\")\n\n        # Extract final memory retention ratios\n        train_mrr = result[\"metrics\"][\"Memory Retention Ratio\"][\"train\"][-1]\n        val_mrr = result[\"metrics\"][\"Memory Retention Ratio\"][\"val\"][-1]\n        print(f\"train memory retention ratio: {train_mrr:.4f}\")\n        print(f\"validation memory retention ratio: {val_mrr:.4f}\")\n\n        # Extract final entropy-weighted memory efficiencies\n        train_eme = result[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"][-1]\n        val_eme = result[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"][-1]\n        print(f\"train entropy-weighted memory efficiency: {train_eme:.4f}\")\n        print(f\"validation entropy-weighted memory efficiency: {val_eme:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over ablations and datasets, printing final metrics\nfor ablation, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"Ablation: {ablation} | Dataset: {dataset_name}\")\n        metrics = data.get(\"metrics\", {})\n        for metric_name, splits in metrics.items():\n            train_vals = splits.get(\"train\", [])\n            val_vals = splits.get(\"val\", [])\n            if train_vals:\n                final_train = train_vals[-1]\n                print(f\"  Training {metric_name}: {final_train:.4f}\")\n            if val_vals:\n                final_val = val_vals[-1]\n                print(f\"  Validation {metric_name}: {final_val:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over each dataset and print final metrics\nfor dataset_name, stats in experiment_data[\"usage_based\"].items():\n    final_train_retention = stats[\"metrics\"][\"train\"][-1]\n    final_validation_retention = stats[\"metrics\"][\"val\"][-1]\n    final_train_loss = stats[\"losses\"][\"train\"][-1]\n    final_validation_loss = stats[\"losses\"][\"val\"][-1]\n\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"Final train retention ratio: {final_train_retention:.4f}\")\n    print(f\"Final validation retention ratio: {final_validation_retention:.4f}\")\n    print(f\"Final train loss: {final_train_loss:.4f}\")\n    print(f\"Final validation loss: {final_validation_loss:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Locate and load the saved experimental results\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each ablation (head aggregation) and dataset\nfor head_agg, datasets in experiment_data.items():\n    for ds_name, ds_data in datasets.items():\n        # Extract the final epoch values for each metric\n        mrr_train = ds_data[\"metrics\"][\"Memory Retention Ratio\"][\"train\"][-1]\n        mrr_val = ds_data[\"metrics\"][\"Memory Retention Ratio\"][\"val\"][-1]\n        eme_train = ds_data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"train\"][\n            -1\n        ]\n        eme_val = ds_data[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"][\"val\"][-1]\n\n        # Print results with clear labels\n        print(f\"Dataset: {ds_name}\")\n        print(f\"Training Memory Retention Ratio: {mrr_train:.4f}\")\n        print(f\"Validation Memory Retention Ratio: {mrr_val:.4f}\")\n        print(f\"Training Entropy-Weighted Memory Efficiency: {eme_train:.4f}\")\n        print(f\"Validation Entropy-Weighted Memory Efficiency: {eme_val:.4f}\\n\")\n", "", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Extract residual connection ablation results\nablation_data = experiment_data.get(\"residual_connection_ablation\", {})\n\n# Iterate over each dataset and print final metrics\nfor dataset_name, ed in ablation_data.items():\n    print(dataset_name)\n\n    # Final train and validation losses\n    train_losses = ed[\"losses\"][\"train\"]\n    val_losses = ed[\"losses\"][\"val\"]\n    if train_losses:\n        print(f\"train loss: {train_losses[-1]:.4f}\")\n    if val_losses:\n        print(f\"validation loss: {val_losses[-1]:.4f}\")\n\n    # Final Memory Retention Ratio\n    mrr = ed[\"metrics\"][\"Memory Retention Ratio\"]\n    tr_mrr = mrr[\"train\"]\n    val_mrr = mrr[\"val\"]\n    if tr_mrr:\n        print(f\"train Memory Retention Ratio: {tr_mrr[-1]:.4f}\")\n    if val_mrr:\n        print(f\"validation Memory Retention Ratio: {val_mrr[-1]:.4f}\")\n\n    # Final Entropy-Weighted Memory Efficiency\n    eme = ed[\"metrics\"][\"Entropy-Weighted Memory Efficiency\"]\n    tr_eme = eme[\"train\"]\n    val_eme = eme[\"val\"]\n    if tr_eme:\n        print(f\"train Entropy-Weighted Memory Efficiency: {tr_eme[-1]:.4f}\")\n    if val_eme:\n        print(f\"validation Entropy-Weighted Memory Efficiency: {val_eme[-1]:.4f}\")\n\n    print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each ablation and dataset, printing final metrics\nfor ablation, ab_data in experiment_data.items():\n    ab_label = (\n        \"Entropy-based memory ablation\"\n        if ablation == \"entropy\"\n        else \"Gradient-based memory ablation\"\n    )\n    for dataset_name, data in ab_data.items():\n        print(f\"Dataset: {dataset_name} ({ab_label})\")\n        # Final training and validation losses\n        if data[\"losses\"][\"train\"]:\n            tr_loss = data[\"losses\"][\"train\"][-1]\n            print(f\"Training Loss: {tr_loss:.4f}\")\n        if data[\"losses\"][\"val\"]:\n            val_loss = data[\"losses\"][\"val\"][-1]\n            print(f\"Validation Loss: {val_loss:.4f}\")\n        # Final memory retention ratios\n        mrr_train = data[\"metrics\"][\"Memory Retention Ratio\"][\"train\"]\n        mrr_val = data[\"metrics\"][\"Memory Retention Ratio\"][\"val\"]\n        if mrr_train:\n            print(f\"Training Memory Retention Ratio: {mrr_train[-1]:.4f}\")\n        if mrr_val:\n            print(f\"Validation Memory Retention Ratio: {mrr_val[-1]:.4f}\")\n        # Final score\u2010weighted memory efficiencies\n        swe_train = data[\"metrics\"][\"Score-Weighted Memory Efficiency\"][\"train\"]\n        swe_val = data[\"metrics\"][\"Score-Weighted Memory Efficiency\"][\"val\"]\n        if swe_train:\n            print(f\"Training Score-Weighted Memory Efficiency: {swe_train[-1]:.4f}\")\n        if swe_val:\n            print(f\"Validation Score-Weighted Memory Efficiency: {swe_val[-1]:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# 0. Define the working directory and load the experiment_data.npy file\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# 1. Iterate through each ablation and dataset to extract and print final metrics\nfor ablation_name, datasets in experiment_data.items():\n    for dataset_name, results in datasets.items():\n        print(f\"Dataset: {dataset_name} (Ablation: {ablation_name})\")\n\n        # Extract final training and validation losses\n        final_train_loss = results[\"losses\"][\"train\"][-1]\n        final_val_loss = results[\"losses\"][\"val\"][-1]\n        print(f\"Training Loss: {final_train_loss:.4f}\")\n        print(f\"Validation Loss: {final_val_loss:.4f}\")\n\n        # Extract final training and validation metrics\n        for metric, splits in results[\"metrics\"].items():\n            final_train_metric = splits[\"train\"][-1]\n            final_val_metric = splits[\"val\"][-1]\n            print(f\"Training {metric}: {final_train_metric:.4f}\")\n            print(f\"Validation {metric}: {final_val_metric:.4f}\")\n\n        print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each ablation and dataset, printing final metrics\nfor ablation, ab_data in experiment_data.items():\n    ab_label = (\n        \"Entropy-based memory ablation\"\n        if ablation == \"entropy\"\n        else \"Gradient-based memory ablation\"\n    )\n    for dataset_name, data in ab_data.items():\n        print(f\"Dataset: {dataset_name} ({ab_label})\")\n        # Final training and validation losses\n        if data[\"losses\"][\"train\"]:\n            tr_loss = data[\"losses\"][\"train\"][-1]\n            print(f\"Training Loss: {tr_loss:.4f}\")\n        if data[\"losses\"][\"val\"]:\n            val_loss = data[\"losses\"][\"val\"][-1]\n            print(f\"Validation Loss: {val_loss:.4f}\")\n        # Final memory retention ratios\n        mrr_train = data[\"metrics\"][\"Memory Retention Ratio\"][\"train\"]\n        mrr_val = data[\"metrics\"][\"Memory Retention Ratio\"][\"val\"]\n        if mrr_train:\n            print(f\"Training Memory Retention Ratio: {mrr_train[-1]:.4f}\")\n        if mrr_val:\n            print(f\"Validation Memory Retention Ratio: {mrr_val[-1]:.4f}\")\n        # Final score\u2010weighted memory efficiencies\n        swe_train = data[\"metrics\"][\"Score-Weighted Memory Efficiency\"][\"train\"]\n        swe_val = data[\"metrics\"][\"Score-Weighted Memory Efficiency\"][\"val\"]\n        if swe_train:\n            print(f\"Training Score-Weighted Memory Efficiency: {swe_train[-1]:.4f}\")\n        if swe_val:\n            print(f\"Validation Score-Weighted Memory Efficiency: {swe_val[-1]:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each ablation and dataset, printing final metrics\nfor ablation, ab_data in experiment_data.items():\n    ab_label = (\n        \"Entropy-based memory ablation\"\n        if ablation == \"entropy\"\n        else \"Gradient-based memory ablation\"\n    )\n    for dataset_name, data in ab_data.items():\n        print(f\"Dataset: {dataset_name} ({ab_label})\")\n        # Final training and validation losses\n        if data[\"losses\"][\"train\"]:\n            tr_loss = data[\"losses\"][\"train\"][-1]\n            print(f\"Training Loss: {tr_loss:.4f}\")\n        if data[\"losses\"][\"val\"]:\n            val_loss = data[\"losses\"][\"val\"][-1]\n            print(f\"Validation Loss: {val_loss:.4f}\")\n        # Final memory retention ratios\n        mrr_train = data[\"metrics\"][\"Memory Retention Ratio\"][\"train\"]\n        mrr_val = data[\"metrics\"][\"Memory Retention Ratio\"][\"val\"]\n        if mrr_train:\n            print(f\"Training Memory Retention Ratio: {mrr_train[-1]:.4f}\")\n        if mrr_val:\n            print(f\"Validation Memory Retention Ratio: {mrr_val[-1]:.4f}\")\n        # Final score\u2010weighted memory efficiencies\n        swe_train = data[\"metrics\"][\"Score-Weighted Memory Efficiency\"][\"train\"]\n        swe_val = data[\"metrics\"][\"Score-Weighted Memory Efficiency\"][\"val\"]\n        if swe_train:\n            print(f\"Training Score-Weighted Memory Efficiency: {swe_train[-1]:.4f}\")\n        if swe_val:\n            print(f\"Validation Score-Weighted Memory Efficiency: {swe_val[-1]:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each ablation and dataset, printing final metrics\nfor ablation, ab_data in experiment_data.items():\n    ab_label = (\n        \"Entropy-based memory ablation\"\n        if ablation == \"entropy\"\n        else \"Gradient-based memory ablation\"\n    )\n    for dataset_name, data in ab_data.items():\n        print(f\"Dataset: {dataset_name} ({ab_label})\")\n        # Final training and validation losses\n        if data[\"losses\"][\"train\"]:\n            tr_loss = data[\"losses\"][\"train\"][-1]\n            print(f\"Training Loss: {tr_loss:.4f}\")\n        if data[\"losses\"][\"val\"]:\n            val_loss = data[\"losses\"][\"val\"][-1]\n            print(f\"Validation Loss: {val_loss:.4f}\")\n        # Final memory retention ratios\n        mrr_train = data[\"metrics\"][\"Memory Retention Ratio\"][\"train\"]\n        mrr_val = data[\"metrics\"][\"Memory Retention Ratio\"][\"val\"]\n        if mrr_train:\n            print(f\"Training Memory Retention Ratio: {mrr_train[-1]:.4f}\")\n        if mrr_val:\n            print(f\"Validation Memory Retention Ratio: {mrr_val[-1]:.4f}\")\n        # Final score\u2010weighted memory efficiencies\n        swe_train = data[\"metrics\"][\"Score-Weighted Memory Efficiency\"][\"train\"]\n        swe_val = data[\"metrics\"][\"Score-Weighted Memory Efficiency\"][\"val\"]\n        if swe_train:\n            print(f\"Training Score-Weighted Memory Efficiency: {swe_train[-1]:.4f}\")\n        if swe_val:\n            print(f\"Validation Score-Weighted Memory Efficiency: {swe_val[-1]:.4f}\")\n        print()\n", ""], "parse_term_out": ["['\\nDataset: pg19', '\\n', 'Training loss: 2.4175', '\\n', 'Validation loss:\n2.7136', '\\n', 'Training Memory Retention Ratio: 0.7724', '\\n', 'Validation\nMemory Retention Ratio: 0.7729', '\\n', 'Training Entropy-Weighted Memory\nEfficiency: 3.7555', '\\n', 'Validation Entropy-Weighted Memory Efficiency:\n3.7426', '\\n', '\\nDataset: scientific_papers_arxiv', '\\n', 'Training loss:\n2.5532', '\\n', 'Validation loss: 2.5154', '\\n', 'Training Memory Retention\nRatio: 0.7811', '\\n', 'Validation Memory Retention Ratio: 0.7805', '\\n',\n'Training Entropy-Weighted Memory Efficiency: 3.6297', '\\n', 'Validation\nEntropy-Weighted Memory Efficiency: 3.6209', '\\n', '\\nDataset:\nwikitext_wikitext-2-raw-v1', '\\n', 'Training loss: 1.3883', '\\n', 'Validation\nloss: 1.3526', '\\n', 'Training Memory Retention Ratio: 0.7680', '\\n',\n'Validation Memory Retention Ratio: 0.7680', '\\n', 'Training Entropy-Weighted\nMemory Efficiency: 3.8879', '\\n', 'Validation Entropy-Weighted Memory\nEfficiency: 3.8817', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Dataset: pg19 (original model)', '\\n', 'Final training loss: 2.4175', '\\n',\n'Final validation loss: 2.7136', '\\n', 'Final training Memory Retention Ratio:\n0.7724', '\\n', 'Final validation Memory Retention Ratio: 0.7729', '\\n', 'Final\ntraining Entropy-Weighted Memory Efficiency: 3.7555', '\\n', 'Final validation\nEntropy-Weighted Memory Efficiency: 3.7426', '\\n', '\\n', 'Dataset:\nscientific_papers_arxiv (original model)', '\\n', 'Final training loss: 2.5532',\n'\\n', 'Final validation loss: 2.5154', '\\n', 'Final training Memory Retention\nRatio: 0.7811', '\\n', 'Final validation Memory Retention Ratio: 0.7805', '\\n',\n'Final training Entropy-Weighted Memory Efficiency: 3.6297', '\\n', 'Final\nvalidation Entropy-Weighted Memory Efficiency: 3.6209', '\\n', '\\n', 'Dataset:\nwikitext_wikitext-2-raw-v1 (original model)', '\\n', 'Final training loss:\n1.3883', '\\n', 'Final validation loss: 1.3526', '\\n', 'Final training Memory\nRetention Ratio: 0.7680', '\\n', 'Final validation Memory Retention Ratio:\n0.7680', '\\n', 'Final training Entropy-Weighted Memory Efficiency: 3.8879',\n'\\n', 'Final validation Entropy-Weighted Memory Efficiency: 3.8817', '\\n', '\\n',\n'Dataset: pg19 (recency model)', '\\n', 'Final training loss: 2.4257', '\\n',\n'Final validation loss: 2.7338', '\\n', 'Final training Memory Retention Ratio:\n0.7502', '\\n', 'Final validation Memory Retention Ratio: 0.7502', '\\n', 'Final\ntraining Entropy-Weighted Memory Efficiency: 3.7758', '\\n', 'Final validation\nEntropy-Weighted Memory Efficiency: 3.7460', '\\n', '\\n', 'Dataset:\nscientific_papers_arxiv (recency model)', '\\n', 'Final training loss: 2.5280',\n'\\n', 'Final validation loss: 2.4929', '\\n', 'Final training Memory Retention\nRatio: 0.7502', '\\n', 'Final validation Memory Retention Ratio: 0.7502', '\\n',\n'Final training Entropy-Weighted Memory Efficiency: 3.6059', '\\n', 'Final\nvalidation Entropy-Weighted Memory Efficiency: 3.6063', '\\n', '\\n', 'Dataset:\nwikitext_wikitext-2-raw-v1 (recency model)', '\\n', 'Final training loss:\n1.3901', '\\n', 'Final validation loss: 1.3529', '\\n', 'Final training Memory\nRetention Ratio: 0.7502', '\\n', 'Final validation Memory Retention Ratio:\n0.7502', '\\n', 'Final training Entropy-Weighted Memory Efficiency: 3.8718',\n'\\n', 'Final validation Entropy-Weighted Memory Efficiency: 3.8546', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "", "['Ablation: entropy_based, Dataset: pg19', '\\n', 'Train loss: 2.4087', '\\n',\n'Validation loss: 2.6569', '\\n', 'Train Memory Retention Ratio: 0.7708', '\\n',\n'Validation Memory Retention Ratio: 0.7702', '\\n', 'Train Entropy-Weighted\nMemory Efficiency: 3.8007', '\\n', 'Validation Entropy-Weighted Memory\nEfficiency: 3.7955', '\\n', '\\n', 'Ablation: entropy_based, Dataset:\nscientific_papers_arxiv', '\\n', 'Train loss: 2.5389', '\\n', 'Validation loss:\n2.4968', '\\n', 'Train Memory Retention Ratio: 0.7736', '\\n', 'Validation Memory\nRetention Ratio: 0.7772', '\\n', 'Train Entropy-Weighted Memory Efficiency:\n3.7238', '\\n', 'Validation Entropy-Weighted Memory Efficiency: 3.7111', '\\n',\n'\\n', 'Ablation: entropy_based, Dataset: wikitext_wikitext-2-raw-v1', '\\n',\n'Train loss: 1.3843', '\\n', 'Validation loss: 1.3463', '\\n', 'Train Memory\nRetention Ratio: 0.7711', '\\n', 'Validation Memory Retention Ratio: 0.7724',\n'\\n', 'Train Entropy-Weighted Memory Efficiency: 3.8791', '\\n', 'Validation\nEntropy-Weighted Memory Efficiency: 3.8611', '\\n', '\\n', 'Ablation:\nrandom_retention, Dataset: pg19', '\\n', 'Train loss: 2.4129', '\\n', 'Validation\nloss: 2.6999', '\\n', 'Train Memory Retention Ratio: 0.7501', '\\n', 'Validation\nMemory Retention Ratio: 0.7500', '\\n', 'Train Entropy-Weighted Memory\nEfficiency: 3.6122', '\\n', 'Validation Entropy-Weighted Memory Efficiency:\n3.6116', '\\n', '\\n', 'Ablation: random_retention, Dataset:\nscientific_papers_arxiv', '\\n', 'Train loss: 2.5428', '\\n', 'Validation loss:\n2.5067', '\\n', 'Train Memory Retention Ratio: 0.7500', '\\n', 'Validation Memory\nRetention Ratio: 0.7502', '\\n', 'Train Entropy-Weighted Memory Efficiency:\n3.5321', '\\n', 'Validation Entropy-Weighted Memory Efficiency: 3.4629', '\\n',\n'\\n', 'Ablation: random_retention, Dataset: wikitext_wikitext-2-raw-v1', '\\n',\n'Train loss: 1.4146', '\\n', 'Validation loss: 1.3645', '\\n', 'Train Memory\nRetention Ratio: 0.7503', '\\n', 'Validation Memory Retention Ratio: 0.7500',\n'\\n', 'Train Entropy-Weighted Memory Efficiency: 3.7214', '\\n', 'Validation\nEntropy-Weighted Memory Efficiency: 3.7117', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['Dataset: pg19 (Ablation: baseline)', '\\n', 'Final training loss: 2.4175',\n'\\n', 'Final validation loss: 2.7136', '\\n', 'Final training Memory Retention\nRatio: 0.7724', '\\n', 'Final validation Memory Retention Ratio: 0.7729', '\\n',\n'Final training Entropy-Weighted Memory Efficiency: 3.7555', '\\n', 'Final\nvalidation Entropy-Weighted Memory Efficiency: 3.7426', '\\n', '\\n', 'Dataset:\nscientific_papers_arxiv (Ablation: baseline)', '\\n', 'Final training loss:\n2.5532', '\\n', 'Final validation loss: 2.5154', '\\n', 'Final training Memory\nRetention Ratio: 0.7811', '\\n', 'Final validation Memory Retention Ratio:\n0.7805', '\\n', 'Final training Entropy-Weighted Memory Efficiency: 3.6297',\n'\\n', 'Final validation Entropy-Weighted Memory Efficiency: 3.6209', '\\n', '\\n',\n'Dataset: wikitext_wikitext-2-raw-v1 (Ablation: baseline)', '\\n', 'Final\ntraining loss: 1.3883', '\\n', 'Final validation loss: 1.3526', '\\n', 'Final\ntraining Memory Retention Ratio: 0.7680', '\\n', 'Final validation Memory\nRetention Ratio: 0.7680', '\\n', 'Final training Entropy-Weighted Memory\nEfficiency: 3.8879', '\\n', 'Final validation Entropy-Weighted Memory Efficiency:\n3.8817', '\\n', '\\n', 'Dataset: pg19 (Ablation: ffn_removed)', '\\n', 'Final\ntraining loss: 2.6170', '\\n', 'Final validation loss: 2.8088', '\\n', 'Final\ntraining Memory Retention Ratio: 0.7818', '\\n', 'Final validation Memory\nRetention Ratio: 0.7798', '\\n', 'Final training Entropy-Weighted Memory\nEfficiency: 3.5620', '\\n', 'Final validation Entropy-Weighted Memory Efficiency:\n3.5978', '\\n', '\\n', 'Dataset: scientific_papers_arxiv (Ablation: ffn_removed)',\n'\\n', 'Final training loss: 2.6072', '\\n', 'Final validation loss: 2.5527',\n'\\n', 'Final training Memory Retention Ratio: 0.7928', '\\n', 'Final validation\nMemory Retention Ratio: 0.7945', '\\n', 'Final training Entropy-Weighted Memory\nEfficiency: 3.2237', '\\n', 'Final validation Entropy-Weighted Memory Efficiency:\n3.1855', '\\n', '\\n', 'Dataset: wikitext_wikitext-2-raw-v1 (Ablation:\nffn_removed)', '\\n', 'Final training loss: 1.4621', '\\n', 'Final validation\nloss: 1.4069', '\\n', 'Final training Memory Retention Ratio: 0.7730', '\\n',\n'Final validation Memory Retention Ratio: 0.7708', '\\n', 'Final training\nEntropy-Weighted Memory Efficiency: 3.8292', '\\n', 'Final validation Entropy-\nWeighted Memory Efficiency: 3.8655', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: pg19', '\\n', 'Final training Memory Retention Ratio: 0.7700', '\\n',\n'Final validation Memory Retention Ratio: 0.7710', '\\n', 'Final training\nEntropy-Weighted Memory Efficiency: 3.7777', '\\n', 'Final validation Entropy-\nWeighted Memory Efficiency: 3.7525', '\\n', '\\n', 'Dataset:\nscientific_papers_arxiv', '\\n', 'Final training Memory Retention Ratio: 0.7740',\n'\\n', 'Final validation Memory Retention Ratio: 0.7713', '\\n', 'Final training\nEntropy-Weighted Memory Efficiency: 3.7573', '\\n', 'Final validation Entropy-\nWeighted Memory Efficiency: 3.7693', '\\n', '\\n', 'Dataset:\nwikitext_wikitext-2-raw-v1', '\\n', 'Final training Memory Retention Ratio:\n0.7688', '\\n', 'Final validation Memory Retention Ratio: 0.7686', '\\n', 'Final\ntraining Entropy-Weighted Memory Efficiency: 3.8677', '\\n', 'Final validation\nEntropy-Weighted Memory Efficiency: 3.8738', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['Dataset: pg19 (Ablation: original)', '\\n', 'Training loss: 2.4175', '\\n',\n'Validation loss: 2.7136', '\\n', 'Training Memory Retention Ratio: 0.7724',\n'\\n', 'Validation Memory Retention Ratio: 0.7729', '\\n', 'Training Entropy-\nWeighted Memory Efficiency: 3.7555', '\\n', 'Validation Entropy-Weighted Memory\nEfficiency: 3.7426', '\\n', '\\n', 'Dataset: scientific_papers_arxiv (Ablation:\noriginal)', '\\n', 'Training loss: 2.5532', '\\n', 'Validation loss: 2.5154',\n'\\n', 'Training Memory Retention Ratio: 0.7811', '\\n', 'Validation Memory\nRetention Ratio: 0.7805', '\\n', 'Training Entropy-Weighted Memory Efficiency:\n3.6297', '\\n', 'Validation Entropy-Weighted Memory Efficiency: 3.6209', '\\n',\n'\\n', 'Dataset: wikitext_wikitext-2-raw-v1 (Ablation: original)', '\\n',\n'Training loss: 1.3883', '\\n', 'Validation loss: 1.3526', '\\n', 'Training Memory\nRetention Ratio: 0.7680', '\\n', 'Validation Memory Retention Ratio: 0.7680',\n'\\n', 'Training Entropy-Weighted Memory Efficiency: 3.8879', '\\n', 'Validation\nEntropy-Weighted Memory Efficiency: 3.8817', '\\n', '\\n', 'Dataset: pg19\n(Ablation: no_memory_attention)', '\\n', 'Training loss: 2.3540', '\\n',\n'Validation loss: 2.6604', '\\n', 'Training Memory Retention Ratio: 1.0000',\n'\\n', 'Validation Memory Retention Ratio: 1.0000', '\\n', 'Training Entropy-\nWeighted Memory Efficiency: 3.2057', '\\n', 'Validation Entropy-Weighted Memory\nEfficiency: 3.1635', '\\n', '\\n', 'Dataset: scientific_papers_arxiv (Ablation:\nno_memory_attention)', '\\n', 'Training loss: 2.5024', '\\n', 'Validation loss:\n2.4547', '\\n', 'Training Memory Retention Ratio: 1.0000', '\\n', 'Validation\nMemory Retention Ratio: 1.0000', '\\n', 'Training Entropy-Weighted Memory\nEfficiency: 3.1361', '\\n', 'Validation Entropy-Weighted Memory Efficiency:\n3.1455', '\\n', '\\n', 'Dataset: wikitext_wikitext-2-raw-v1 (Ablation:\nno_memory_attention)', '\\n', 'Training loss: 1.3884', '\\n', 'Validation loss:\n1.3520', '\\n', 'Training Memory Retention Ratio: 1.0000', '\\n', 'Validation\nMemory Retention Ratio: 1.0000', '\\n', 'Training Entropy-Weighted Memory\nEfficiency: 3.2980', '\\n', 'Validation Entropy-Weighted Memory Efficiency:\n3.2700', '\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Mode: entropy, Dataset: pg19', '\\n', '  final train loss: 2.4175', '\\n', '\nfinal validation loss: 2.7136', '\\n', '  final train Memory Retention Ratio:\n0.7724', '\\n', '  final validation Memory Retention Ratio: 0.7729', '\\n', '\nfinal train Entropy-Weighted Memory Efficiency: 3.7555', '\\n', '  final\nvalidation Entropy-Weighted Memory Efficiency: 3.7426', '\\n', '\\n', 'Mode:\nentropy, Dataset: scientific_papers_arxiv', '\\n', '  final train loss: 2.5532',\n'\\n', '  final validation loss: 2.5154', '\\n', '  final train Memory Retention\nRatio: 0.7811', '\\n', '  final validation Memory Retention Ratio: 0.7805', '\\n',\n'  final train Entropy-Weighted Memory Efficiency: 3.6297', '\\n', '  final\nvalidation Entropy-Weighted Memory Efficiency: 3.6209', '\\n', '\\n', 'Mode:\nentropy, Dataset: wikitext_wikitext-2-raw-v1', '\\n', '  final train loss:\n1.3883', '\\n', '  final validation loss: 1.3526', '\\n', '  final train Memory\nRetention Ratio: 0.7680', '\\n', '  final validation Memory Retention Ratio:\n0.7680', '\\n', '  final train Entropy-Weighted Memory Efficiency: 3.8879', '\\n',\n'  final validation Entropy-Weighted Memory Efficiency: 3.8817', '\\n', '\\n',\n'Mode: norm, Dataset: pg19', '\\n', '  final train loss: 2.4182', '\\n', '  final\nvalidation loss: 2.7067', '\\n', '  final train Memory Retention Ratio: 0.7755',\n'\\n', '  final validation Memory Retention Ratio: 0.7741', '\\n', '  final train\nEntropy-Weighted Memory Efficiency: 3.5444', '\\n', '  final validation Entropy-\nWeighted Memory Efficiency: 3.5347', '\\n', '\\n', 'Mode: norm, Dataset:\nscientific_papers_arxiv', '\\n', '  final train loss: 2.5299', '\\n', '  final\nvalidation loss: 2.4918', '\\n', '  final train Memory Retention Ratio: 0.7763',\n'\\n', '  final validation Memory Retention Ratio: 0.7760', '\\n', '  final train\nEntropy-Weighted Memory Efficiency: 3.3076', '\\n', '  final validation Entropy-\nWeighted Memory Efficiency: 3.2953', '\\n', '\\n', 'Mode: norm, Dataset:\nwikitext_wikitext-2-raw-v1', '\\n', '  final train loss: 1.3924', '\\n', '  final\nvalidation loss: 1.3542', '\\n', '  final train Memory Retention Ratio: 0.7636',\n'\\n', '  final validation Memory Retention Ratio: 0.7629', '\\n', '  final train\nEntropy-Weighted Memory Efficiency: 3.5670', '\\n', '  final validation Entropy-\nWeighted Memory Efficiency: 3.5619', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "", "", "['Dataset: pg19', '\\n', 'Final training retention: 1.0000', '\\n', 'Final\nvalidation retention: 1.0000', '\\n', 'Final training loss: 2.4310', '\\n', 'Final\nvalidation loss: 2.7340', '\\n', '\\n', 'Dataset: scientific_papers_arxiv', '\\n',\n'Final training retention: 1.0000', '\\n', 'Final validation retention: 1.0000',\n'\\n', 'Final training loss: 2.5507', '\\n', 'Final validation loss: 2.5144',\n'\\n', '\\n', 'Dataset: wikitext_wikitext-2-raw-v1', '\\n', 'Final training\nretention: 1.0000', '\\n', 'Final validation retention: 1.0000', '\\n', 'Final\ntraining loss: 1.3891', '\\n', 'Final validation loss: 1.3523', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: pg19 (Ablation: baseline)', '\\n', 'train loss: 2.4175', '\\n',\n'validation loss: 2.7136', '\\n', 'train memory retention ratio: 0.7724', '\\n',\n'validation memory retention ratio: 0.7729', '\\n', 'train entropy-weighted\nmemory efficiency: 3.7555', '\\n', 'validation entropy-weighted memory\nefficiency: 3.7426\\n', '\\n', 'Dataset: scientific_papers_arxiv (Ablation:\nbaseline)', '\\n', 'train loss: 2.5532', '\\n', 'validation loss: 2.5154', '\\n',\n'train memory retention ratio: 0.7811', '\\n', 'validation memory retention\nratio: 0.7805', '\\n', 'train entropy-weighted memory efficiency: 3.6297', '\\n',\n'validation entropy-weighted memory efficiency: 3.6209\\n', '\\n', 'Dataset:\nwikitext_wikitext-2-raw-v1 (Ablation: baseline)', '\\n', 'train loss: 1.3883',\n'\\n', 'validation loss: 1.3526', '\\n', 'train memory retention ratio: 0.7680',\n'\\n', 'validation memory retention ratio: 0.7680', '\\n', 'train entropy-weighted\nmemory efficiency: 3.8879', '\\n', 'validation entropy-weighted memory\nefficiency: 3.8817\\n', '\\n', 'Dataset: pg19 (Ablation: feedforward_identity)',\n'\\n', 'train loss: 2.4350', '\\n', 'validation loss: 2.7590', '\\n', 'train memory\nretention ratio: 0.7701', '\\n', 'validation memory retention ratio: 0.7706',\n'\\n', 'train entropy-weighted memory efficiency: 3.7913', '\\n', 'validation\nentropy-weighted memory efficiency: 3.7424\\n', '\\n', 'Dataset:\nscientific_papers_arxiv (Ablation: feedforward_identity)', '\\n', 'train loss:\n2.5287', '\\n', 'validation loss: 2.4980', '\\n', 'train memory retention ratio:\n0.7733', '\\n', 'validation memory retention ratio: 0.7734', '\\n', 'train\nentropy-weighted memory efficiency: 3.7431', '\\n', 'validation entropy-weighted\nmemory efficiency: 3.7238\\n', '\\n', 'Dataset: wikitext_wikitext-2-raw-v1\n(Ablation: feedforward_identity)', '\\n', 'train loss: 1.4038', '\\n', 'validation\nloss: 1.3704', '\\n', 'train memory retention ratio: 0.7696', '\\n', 'validation\nmemory retention ratio: 0.7695', '\\n', 'train entropy-weighted memory\nefficiency: 3.8564', '\\n', 'validation entropy-weighted memory efficiency:\n3.8592\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Ablation: baseline | Dataset: pg19', '\\n', '  Training Memory Retention Ratio:\n0.7724', '\\n', '  Validation Memory Retention Ratio: 0.7729', '\\n', '  Training\nEntropy-Weighted Memory Efficiency: 3.7555', '\\n', '  Validation Entropy-\nWeighted Memory Efficiency: 3.7426', '\\n', '\\n', 'Ablation: baseline | Dataset:\nscientific_papers_arxiv', '\\n', '  Training Memory Retention Ratio: 0.7811',\n'\\n', '  Validation Memory Retention Ratio: 0.7805', '\\n', '  Training Entropy-\nWeighted Memory Efficiency: 3.6297', '\\n', '  Validation Entropy-Weighted Memory\nEfficiency: 3.6209', '\\n', '\\n', 'Ablation: baseline | Dataset:\nwikitext_wikitext-2-raw-v1', '\\n', '  Training Memory Retention Ratio: 0.7680',\n'\\n', '  Validation Memory Retention Ratio: 0.7680', '\\n', '  Training Entropy-\nWeighted Memory Efficiency: 3.8879', '\\n', '  Validation Entropy-Weighted Memory\nEfficiency: 3.8817', '\\n', '\\n', 'Ablation: continuous_memory | Dataset: pg19',\n'\\n', '  Training Memory Retention Ratio: 0.6220', '\\n', '  Validation Memory\nRetention Ratio: 0.6314', '\\n', '  Training Entropy-Weighted Memory Efficiency:\n4.3833', '\\n', '  Validation Entropy-Weighted Memory Efficiency: 4.3415', '\\n',\n'\\n', 'Ablation: continuous_memory | Dataset: scientific_papers_arxiv', '\\n', '\nTraining Memory Retention Ratio: 0.6375', '\\n', '  Validation Memory Retention\nRatio: 0.6406', '\\n', '  Training Entropy-Weighted Memory Efficiency: 4.3594',\n'\\n', '  Validation Entropy-Weighted Memory Efficiency: 4.3580', '\\n', '\\n',\n'Ablation: continuous_memory | Dataset: wikitext_wikitext-2-raw-v1', '\\n', '\nTraining Memory Retention Ratio: 0.6212', '\\n', '  Validation Memory Retention\nRatio: 0.6248', '\\n', '  Training Entropy-Weighted Memory Efficiency: 4.4048',\n'\\n', '  Validation Entropy-Weighted Memory Efficiency: 4.4029', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: pg19', '\\n', 'Final train retention ratio: 0.7502', '\\n', 'Final\nvalidation retention ratio: 0.7502', '\\n', 'Final train loss: 2.4310', '\\n',\n'Final validation loss: 2.7340', '\\n', '\\n', 'Dataset: scientific_papers_arxiv',\n'\\n', 'Final train retention ratio: 0.7502', '\\n', 'Final validation retention\nratio: 0.7502', '\\n', 'Final train loss: 2.5507', '\\n', 'Final validation loss:\n2.5144', '\\n', '\\n', 'Dataset: wikitext_wikitext-2-raw-v1', '\\n', 'Final train\nretention ratio: 0.7502', '\\n', 'Final validation retention ratio: 0.7502',\n'\\n', 'Final train loss: 1.3891', '\\n', 'Final validation loss: 1.3523', '\\n',\n'\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: pg19', '\\n', 'Training Memory Retention Ratio: 0.7724', '\\n',\n'Validation Memory Retention Ratio: 0.7729', '\\n', 'Training Entropy-Weighted\nMemory Efficiency: 3.7555', '\\n', 'Validation Entropy-Weighted Memory\nEfficiency: 3.7426\\n', '\\n', 'Dataset: scientific_papers_arxiv', '\\n', 'Training\nMemory Retention Ratio: 0.7811', '\\n', 'Validation Memory Retention Ratio:\n0.7805', '\\n', 'Training Entropy-Weighted Memory Efficiency: 3.6297', '\\n',\n'Validation Entropy-Weighted Memory Efficiency: 3.6209\\n', '\\n', 'Dataset:\nwikitext_wikitext-2-raw-v1', '\\n', 'Training Memory Retention Ratio: 0.7680',\n'\\n', 'Validation Memory Retention Ratio: 0.7680', '\\n', 'Training Entropy-\nWeighted Memory Efficiency: 3.8879', '\\n', 'Validation Entropy-Weighted Memory\nEfficiency: 3.8817\\n', '\\n', 'Dataset: pg19', '\\n', 'Training Memory Retention\nRatio: 0.7733', '\\n', 'Validation Memory Retention Ratio: 0.7728', '\\n',\n'Training Entropy-Weighted Memory Efficiency: 3.6593', '\\n', 'Validation\nEntropy-Weighted Memory Efficiency: 3.6469\\n', '\\n', 'Dataset:\nscientific_papers_arxiv', '\\n', 'Training Memory Retention Ratio: 0.7812', '\\n',\n'Validation Memory Retention Ratio: 0.7820', '\\n', 'Training Entropy-Weighted\nMemory Efficiency: 3.5224', '\\n', 'Validation Entropy-Weighted Memory\nEfficiency: 3.5136\\n', '\\n', 'Dataset: wikitext_wikitext-2-raw-v1', '\\n',\n'Training Memory Retention Ratio: 0.7722', '\\n', 'Validation Memory Retention\nRatio: 0.7726', '\\n', 'Training Entropy-Weighted Memory Efficiency: 3.7783',\n'\\n', 'Validation Entropy-Weighted Memory Efficiency: 3.7528\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "", "['pg19', '\\n', 'train loss: 2.3893', '\\n', 'validation loss: 2.7047', '\\n',\n'train Memory Retention Ratio: 0.7685', '\\n', 'validation Memory Retention\nRatio: 0.7690', '\\n', 'train Entropy-Weighted Memory Efficiency: 3.8590', '\\n',\n'validation Entropy-Weighted Memory Efficiency: 3.8377', '\\n', '\\n',\n'scientific_papers_arxiv', '\\n', 'train loss: 2.5535', '\\n', 'validation loss:\n2.5150', '\\n', 'train Memory Retention Ratio: 0.7698', '\\n', 'validation Memory\nRetention Ratio: 0.7700', '\\n', 'train Entropy-Weighted Memory Efficiency:\n3.8249', '\\n', 'validation Entropy-Weighted Memory Efficiency: 3.8085', '\\n',\n'\\n', 'wikitext_wikitext-2-raw-v1', '\\n', 'train loss: 1.4152', '\\n',\n'validation loss: 1.3697', '\\n', 'train Memory Retention Ratio: 0.7676', '\\n',\n'validation Memory Retention Ratio: 0.7678', '\\n', 'train Entropy-Weighted\nMemory Efficiency: 3.8928', '\\n', 'validation Entropy-Weighted Memory\nEfficiency: 3.8952', '\\n', '\\n', 'Execution time: a moment seconds (time limit\nis an hour).']", "['Dataset: pg19 (Entropy-based memory ablation)', '\\n', 'Training Loss: 2.4175',\n'\\n', 'Validation Loss: 2.7136', '\\n', 'Training Memory Retention Ratio:\n0.7724', '\\n', 'Validation Memory Retention Ratio: 0.7729', '\\n', 'Training\nScore-Weighted Memory Efficiency: 3.7555', '\\n', 'Validation Score-Weighted\nMemory Efficiency: 3.7426', '\\n', '\\n', 'Dataset: scientific_papers_arxiv\n(Entropy-based memory ablation)', '\\n', 'Training Loss: 2.5532', '\\n',\n'Validation Loss: 2.5154', '\\n', 'Training Memory Retention Ratio: 0.7811',\n'\\n', 'Validation Memory Retention Ratio: 0.7805', '\\n', 'Training Score-\nWeighted Memory Efficiency: 3.6297', '\\n', 'Validation Score-Weighted Memory\nEfficiency: 3.6209', '\\n', '\\n', 'Dataset: wikitext_wikitext-2-raw-v1 (Entropy-\nbased memory ablation)', '\\n', 'Training Loss: 1.3883', '\\n', 'Validation Loss:\n1.3526', '\\n', 'Training Memory Retention Ratio: 0.7680', '\\n', 'Validation\nMemory Retention Ratio: 0.7680', '\\n', 'Training Score-Weighted Memory\nEfficiency: 3.8879', '\\n', 'Validation Score-Weighted Memory Efficiency:\n3.8817', '\\n', '\\n', 'Dataset: pg19 (Gradient-based memory ablation)', '\\n',\n'Training Loss: 2.0926', '\\n', 'Validation Loss: 2.6102', '\\n', 'Training Memory\nRetention Ratio: 0.8401', '\\n', 'Validation Memory Retention Ratio: 0.8410',\n'\\n', 'Training Score-Weighted Memory Efficiency: 0.0243', '\\n', 'Validation\nScore-Weighted Memory Efficiency: 0.0271', '\\n', '\\n', 'Dataset:\nscientific_papers_arxiv (Gradient-based memory ablation)', '\\n', 'Training Loss:\n2.4001', '\\n', 'Validation Loss: 2.3840', '\\n', 'Training Memory Retention\nRatio: 0.7968', '\\n', 'Validation Memory Retention Ratio: 0.7948', '\\n',\n'Training Score-Weighted Memory Efficiency: 0.0230', '\\n', 'Validation Score-\nWeighted Memory Efficiency: 0.0236', '\\n', '\\n', 'Dataset:\nwikitext_wikitext-2-raw-v1 (Gradient-based memory ablation)', '\\n', 'Training\nLoss: 1.2399', '\\n', 'Validation Loss: 1.2601', '\\n', 'Training Memory Retention\nRatio: 0.8437', '\\n', 'Validation Memory Retention Ratio: 0.8198', '\\n',\n'Training Score-Weighted Memory Efficiency: 0.0133', '\\n', 'Validation Score-\nWeighted Memory Efficiency: 0.0125', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: pg19 (Ablation: baseline)', '\\n', 'Training Loss: 2.4175', '\\n',\n'Validation Loss: 2.7136', '\\n', 'Training Entropy Retention Ratio: 0.7724',\n'\\n', 'Validation Entropy Retention Ratio: 0.7729', '\\n', 'Training Entropy-\nWeighted Memory Efficiency: 3.7555', '\\n', 'Validation Entropy-Weighted Memory\nEfficiency: 3.7426', '\\n', '\\n', 'Dataset: scientific_papers_arxiv (Ablation:\nbaseline)', '\\n', 'Training Loss: 2.5532', '\\n', 'Validation Loss: 2.5154',\n'\\n', 'Training Entropy Retention Ratio: 0.7811', '\\n', 'Validation Entropy\nRetention Ratio: 0.7805', '\\n', 'Training Entropy-Weighted Memory Efficiency:\n3.6297', '\\n', 'Validation Entropy-Weighted Memory Efficiency: 3.6209', '\\n',\n'\\n', 'Dataset: wikitext_wikitext-2-raw-v1 (Ablation: baseline)', '\\n',\n'Training Loss: 1.3883', '\\n', 'Validation Loss: 1.3526', '\\n', 'Training\nEntropy Retention Ratio: 0.7680', '\\n', 'Validation Entropy Retention Ratio:\n0.7680', '\\n', 'Training Entropy-Weighted Memory Efficiency: 3.8879', '\\n',\n'Validation Entropy-Weighted Memory Efficiency: 3.8817', '\\n', '\\n', 'Dataset:\npg19 (Ablation: memory_key_only)', '\\n', 'Training Loss: 2.3746', '\\n',\n'Validation Loss: 2.6812', '\\n', 'Training Entropy Retention Ratio: 0.7716',\n'\\n', 'Validation Entropy Retention Ratio: 0.7713', '\\n', 'Training Entropy-\nWeighted Memory Efficiency: 3.7321', '\\n', 'Validation Entropy-Weighted Memory\nEfficiency: 3.6999', '\\n', '\\n', 'Dataset: scientific_papers_arxiv (Ablation:\nmemory_key_only)', '\\n', 'Training Loss: 2.5192', '\\n', 'Validation Loss:\n2.4786', '\\n', 'Training Entropy Retention Ratio: 0.7774', '\\n', 'Validation\nEntropy Retention Ratio: 0.7743', '\\n', 'Training Entropy-Weighted Memory\nEfficiency: 3.7048', '\\n', 'Validation Entropy-Weighted Memory Efficiency:\n3.7322', '\\n', '\\n', 'Dataset: wikitext_wikitext-2-raw-v1 (Ablation:\nmemory_key_only)', '\\n', 'Training Loss: 1.3897', '\\n', 'Validation Loss:\n1.3499', '\\n', 'Training Entropy Retention Ratio: 0.7690', '\\n', 'Validation\nEntropy Retention Ratio: 0.7688', '\\n', 'Training Entropy-Weighted Memory\nEfficiency: 3.9001', '\\n', 'Validation Entropy-Weighted Memory Efficiency:\n3.8933', '\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Dataset: pg19 (Entropy-based memory ablation)', '\\n', 'Training Loss: 2.4087',\n'\\n', 'Validation Loss: 2.6569', '\\n', 'Training Memory Retention Ratio:\n0.7708', '\\n', 'Validation Memory Retention Ratio: 0.7702', '\\n', 'Training\nScore-Weighted Memory Efficiency: 3.8007', '\\n', 'Validation Score-Weighted\nMemory Efficiency: 3.7955', '\\n', '\\n', 'Dataset: scientific_papers_arxiv\n(Entropy-based memory ablation)', '\\n', 'Training Loss: 2.5389', '\\n',\n'Validation Loss: 2.4968', '\\n', 'Training Memory Retention Ratio: 0.7736',\n'\\n', 'Validation Memory Retention Ratio: 0.7772', '\\n', 'Training Score-\nWeighted Memory Efficiency: 3.7238', '\\n', 'Validation Score-Weighted Memory\nEfficiency: 3.7111', '\\n', '\\n', 'Dataset: wikitext_wikitext-2-raw-v1 (Entropy-\nbased memory ablation)', '\\n', 'Training Loss: 1.3843', '\\n', 'Validation Loss:\n1.3463', '\\n', 'Training Memory Retention Ratio: 0.7711', '\\n', 'Validation\nMemory Retention Ratio: 0.7724', '\\n', 'Training Score-Weighted Memory\nEfficiency: 3.8791', '\\n', 'Validation Score-Weighted Memory Efficiency:\n3.8611', '\\n', '\\n', 'Dataset: pg19 (Gradient-based memory ablation)', '\\n',\n'Training Loss: 2.0673', '\\n', 'Validation Loss: 2.5988', '\\n', 'Training Memory\nRetention Ratio: 0.8399', '\\n', 'Validation Memory Retention Ratio: 0.8395',\n'\\n', 'Training Score-Weighted Memory Efficiency: 0.0245', '\\n', 'Validation\nScore-Weighted Memory Efficiency: 0.0269', '\\n', '\\n', 'Dataset:\nscientific_papers_arxiv (Gradient-based memory ablation)', '\\n', 'Training Loss:\n2.4086', '\\n', 'Validation Loss: 2.3885', '\\n', 'Training Memory Retention\nRatio: 0.7971', '\\n', 'Validation Memory Retention Ratio: 0.8021', '\\n',\n'Training Score-Weighted Memory Efficiency: 0.0225', '\\n', 'Validation Score-\nWeighted Memory Efficiency: 0.0233', '\\n', '\\n', 'Dataset:\nwikitext_wikitext-2-raw-v1 (Gradient-based memory ablation)', '\\n', 'Training\nLoss: 1.2461', '\\n', 'Validation Loss: 1.2523', '\\n', 'Training Memory Retention\nRatio: 0.8441', '\\n', 'Validation Memory Retention Ratio: 0.8194', '\\n',\n'Training Score-Weighted Memory Efficiency: 0.0126', '\\n', 'Validation Score-\nWeighted Memory Efficiency: 0.0118', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: pg19 (Entropy-based memory ablation)', '\\n', 'Training Loss: 2.3873',\n'\\n', 'Validation Loss: 2.6916', '\\n', 'Training Memory Retention Ratio:\n0.7701', '\\n', 'Validation Memory Retention Ratio: 0.7701', '\\n', 'Training\nScore-Weighted Memory Efficiency: 3.8157', '\\n', 'Validation Score-Weighted\nMemory Efficiency: 3.8287', '\\n', '\\n', 'Dataset: scientific_papers_arxiv\n(Entropy-based memory ablation)', '\\n', 'Training Loss: 2.5328', '\\n',\n'Validation Loss: 2.5014', '\\n', 'Training Memory Retention Ratio: 0.7795',\n'\\n', 'Validation Memory Retention Ratio: 0.7808', '\\n', 'Training Score-\nWeighted Memory Efficiency: 3.5983', '\\n', 'Validation Score-Weighted Memory\nEfficiency: 3.6257', '\\n', '\\n', 'Dataset: wikitext_wikitext-2-raw-v1 (Entropy-\nbased memory ablation)', '\\n', 'Training Loss: 1.4040', '\\n', 'Validation Loss:\n1.3494', '\\n', 'Training Memory Retention Ratio: 0.7691', '\\n', 'Validation\nMemory Retention Ratio: 0.7692', '\\n', 'Training Score-Weighted Memory\nEfficiency: 3.8810', '\\n', 'Validation Score-Weighted Memory Efficiency:\n3.8922', '\\n', '\\n', 'Dataset: pg19 (Gradient-based memory ablation)', '\\n',\n'Training Loss: 2.0913', '\\n', 'Validation Loss: 2.5571', '\\n', 'Training Memory\nRetention Ratio: 0.8392', '\\n', 'Validation Memory Retention Ratio: 0.8411',\n'\\n', 'Training Score-Weighted Memory Efficiency: 0.0243', '\\n', 'Validation\nScore-Weighted Memory Efficiency: 0.0266', '\\n', '\\n', 'Dataset:\nscientific_papers_arxiv (Gradient-based memory ablation)', '\\n', 'Training Loss:\n2.4323', '\\n', 'Validation Loss: 2.4173', '\\n', 'Training Memory Retention\nRatio: 0.7988', '\\n', 'Validation Memory Retention Ratio: 0.7990', '\\n',\n'Training Score-Weighted Memory Efficiency: 0.0228', '\\n', 'Validation Score-\nWeighted Memory Efficiency: 0.0233', '\\n', '\\n', 'Dataset:\nwikitext_wikitext-2-raw-v1 (Gradient-based memory ablation)', '\\n', 'Training\nLoss: 1.2384', '\\n', 'Validation Loss: 1.2586', '\\n', 'Training Memory Retention\nRatio: 0.8435', '\\n', 'Validation Memory Retention Ratio: 0.8194', '\\n',\n'Training Score-Weighted Memory Efficiency: 0.0132', '\\n', 'Validation Score-\nWeighted Memory Efficiency: 0.0124', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: pg19 (Entropy-based memory ablation)', '\\n', 'Training Loss: 2.3690',\n'\\n', 'Validation Loss: 2.7013', '\\n', 'Training Memory Retention Ratio:\n0.7732', '\\n', 'Validation Memory Retention Ratio: 0.7744', '\\n', 'Training\nScore-Weighted Memory Efficiency: 3.8329', '\\n', 'Validation Score-Weighted\nMemory Efficiency: 3.8001', '\\n', '\\n', 'Dataset: scientific_papers_arxiv\n(Entropy-based memory ablation)', '\\n', 'Training Loss: 2.5333', '\\n',\n'Validation Loss: 2.4968', '\\n', 'Training Memory Retention Ratio: 0.7748',\n'\\n', 'Validation Memory Retention Ratio: 0.7733', '\\n', 'Training Score-\nWeighted Memory Efficiency: 3.6851', '\\n', 'Validation Score-Weighted Memory\nEfficiency: 3.6585', '\\n', '\\n', 'Dataset: wikitext_wikitext-2-raw-v1 (Entropy-\nbased memory ablation)', '\\n', 'Training Loss: 1.3942', '\\n', 'Validation Loss:\n1.3448', '\\n', 'Training Memory Retention Ratio: 0.7724', '\\n', 'Validation\nMemory Retention Ratio: 0.7713', '\\n', 'Training Score-Weighted Memory\nEfficiency: 3.8336', '\\n', 'Validation Score-Weighted Memory Efficiency:\n3.8423', '\\n', '\\n', 'Dataset: pg19 (Gradient-based memory ablation)', '\\n',\n'Training Loss: 2.0646', '\\n', 'Validation Loss: 2.5031', '\\n', 'Training Memory\nRetention Ratio: 0.8371', '\\n', 'Validation Memory Retention Ratio: 0.8397',\n'\\n', 'Training Score-Weighted Memory Efficiency: 0.0241', '\\n', 'Validation\nScore-Weighted Memory Efficiency: 0.0268', '\\n', '\\n', 'Dataset:\nscientific_papers_arxiv (Gradient-based memory ablation)', '\\n', 'Training Loss:\n2.4120', '\\n', 'Validation Loss: 2.3988', '\\n', 'Training Memory Retention\nRatio: 0.7967', '\\n', 'Validation Memory Retention Ratio: 0.8000', '\\n',\n'Training Score-Weighted Memory Efficiency: 0.0225', '\\n', 'Validation Score-\nWeighted Memory Efficiency: 0.0233', '\\n', '\\n', 'Dataset:\nwikitext_wikitext-2-raw-v1 (Gradient-based memory ablation)', '\\n', 'Training\nLoss: 1.2504', '\\n', 'Validation Loss: 1.2632', '\\n', 'Training Memory Retention\nRatio: 0.8486', '\\n', 'Validation Memory Retention Ratio: 0.8328', '\\n',\n'Training Score-Weighted Memory Efficiency: 0.0130', '\\n', 'Validation Score-\nWeighted Memory Efficiency: 0.0122', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"]}
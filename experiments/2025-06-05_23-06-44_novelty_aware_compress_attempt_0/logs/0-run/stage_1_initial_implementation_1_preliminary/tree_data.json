{"edges": [[1, 4], [1, 3], [1, 5], [1, 6]], "layout": [[0.16666666666666666, 0.0], [0.5, 0.0], [0.8333333333333334, 0.0], [0.0, 1.0], [0.3333333333333333, 1.0], [0.6666666666666666, 1.0], [1.0, 1.0]], "plan": ["We will generate a synthetic next-token prediction dataset by sampling random\ninteger sequences for train and validation. A single-layer Transformer with a\nrecurrent memory buffer will be implemented, where past hidden states are\nconcatenated with current embeddings before feeding into the encoder. The EA-ACM\nmodule projects memory states to keys and current outputs to queries to compute\naverage attention weights per token and uses this as an importance score.\nWhenever the buffer exceeds the prescribed maximum length, we adaptively\ncompress it by keeping the top-K tokens with highest attention-based importance\nscores, and compute the Entropy Preservation Ratio (EPR) as the fraction of\ntotal attention weight retained. We train the network with cross-entropy loss,\nupdating weights with Adam, and log both training/validation loss and EPR at\neach epoch. Predictions and ground-truth sequences for the validation set are\nrecorded for qualitative inspection, and all losses, metrics, predictions, and\ntargets are stored in an experiment_data dictionary. At the end of training, we\nsave this dictionary to disk as a numpy file in the ./working directory. This\nbaseline confirms that our entropy-aware compression is functional and yields\ninterpretable retention metrics before scaling to real benchmarks.", "We propose a simple Transformer-XL style baseline with a memory compression\nmechanism guided by token\u2010level self\u2010attention entropy. We generate a synthetic\ndataset of random integer sequences for next\u2010token prediction and split it into\ntraining and validation sets. Our model uses a single memory\u2010aware Transformer\nlayer that concatenates past token embeddings to current inputs, computes\nattention distributions, and derives entropy scores per token. After each chunk\nof tokens, we update memory by preserving the top\u2010entropy tokens up to a fixed\ncapacity, and we track the Entropy Preservation Ratio. Training optimizes\ncross\u2010entropy loss over chunks of sequence, and we evaluate loss and average\nentropy preservation on both splits each epoch. We store per\u2010epoch train/val\nlosses and preservation ratios, then save them in numpy format. Finally, we\nautoregressively generate one validation sequence, compare predicted vs ground\ntruth token IDs, and plot the results for visualization in the working\ndirectory. All stages of data preparation, model training, evaluation, and\nvisualization are implemented in a single self\u2010contained script that writes\noutputs to ./working.", "We generate synthetic sequences of tokens and split into train/validation sets.\nA small TransformerLM model is defined with an embedding layer, positional\nembeddings, a single MultiHeadAttention block (returning per-head weights) and a\nfeed-forward head. During each forward pass we concatenate an adaptive memory\nbuffer (capped at a maximum length) with the current token segment, compute\nnext-token logits and extract attention weights. We then compute per-token\nnovelty as the entropy of its attention distribution across keys, compress\nmemory by selecting the highest-entropy tokens up to the budget, and optimize\ncross-entropy loss on the new segment. We detach memory between segments to\navoid backprop through long histories. Validation follows the same process\nwithout gradient updates, and we track both validation loss and an Entropy\nPreservation Ratio (sum of entropies of retained tokens over sum of all\nentropies). All losses and metrics are logged per epoch, stored in\n`experiment_data` and saved as a numpy file at the end.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, size, seq_len, vocab_size):\n        self.size = size\n        self.seq_len = seq_len\n        self.vocab_size = vocab_size\n\n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        seq = torch.randint(0, self.vocab_size, (self.seq_len + 1,))\n        return {\"input\": seq[:-1], \"target\": seq[1:]}\n\n\ntrain_set = SyntheticDataset(1000, 20, 100)\nval_set = SyntheticDataset(200, 20, 100)\ntrain_loader = DataLoader(train_set, batch_size=16, shuffle=False)\nval_loader = DataLoader(val_set, batch_size=16, shuffle=False)\n\n\nclass EAACMTransformer(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size, compress_size):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.max_mem = mem_size\n        self.compress_size = compress_size\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.pos_embed = nn.Embedding(500, embed_dim)\n        layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n        self.transformer = nn.TransformerEncoder(layer, num_layers=1)\n        self.decoder = nn.Linear(embed_dim, vocab_size)\n        self.memory = None\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n\n    def init_memory(self, batch_size):\n        self.memory = torch.zeros(batch_size, 0, self.embed_dim, device=device)\n\n    def forward(self, x):\n        b, L = x.shape\n        positions = torch.arange(L, device=x.device).unsqueeze(0).expand(b, L)\n        x_embed = self.embed(x) + self.pos_embed(positions)\n        if self.memory is None:\n            self.init_memory(b)\n        mem_old = self.memory\n        old_m = mem_old.size(1)\n        combo = torch.cat([mem_old, x_embed], dim=1)\n        seq = combo.transpose(0, 1)\n        out = self.transformer(seq).transpose(0, 1)\n        out_cur = out[:, old_m:, :]\n        logits = self.decoder(out_cur)\n        new_mem = torch.cat([mem_old, out_cur.detach()], dim=1)\n        if new_mem.size(1) > self.max_mem:\n            new_mem, epr = self.compress_memory(new_mem, old_m)\n        else:\n            epr = 1.0\n        self.memory = new_mem[:, -self.max_mem :, :].detach()\n        self.latest_epr = epr\n        return logits\n\n    def compress_memory(self, mem, old_m):\n        b, m_new, d = mem.size()\n        L = m_new - old_m\n        keys = self.k_proj(mem.reshape(-1, d)).reshape(b, m_new, d)\n        queries = self.q_proj(mem[:, old_m:, :].reshape(-1, d)).reshape(b, L, d)\n        scores = torch.matmul(queries, keys.transpose(-2, -1)) / (d**0.5)\n        attn = torch.softmax(scores, dim=-1)\n        w = attn.mean(dim=(0, 1))\n        k_sel = torch.topk(w, self.compress_size)[1]\n        k_sorted, _ = torch.sort(k_sel)\n        new_mem = mem[:, k_sorted, :]\n        epr = w[k_sorted].sum() / w.sum()\n        return new_mem, epr.item()\n\n\n# Hyperparameters\nmodel = EAACMTransformer(\n    vocab_size=100, embed_dim=64, num_heads=4, mem_size=50, compress_size=25\n).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    train_loss, train_epr = 0.0, 0.0\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        logits = model(batch[\"input\"])\n        loss = criterion(logits.view(-1, logits.size(-1)), batch[\"target\"].view(-1))\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        train_epr += model.latest_epr\n    train_loss /= len(train_loader)\n    train_epr /= len(train_loader)\n\n    model.eval()\n    val_loss, val_epr = 0.0, 0.0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input\"])\n            loss = criterion(logits.view(-1, logits.size(-1)), batch[\"target\"].view(-1))\n            val_loss += loss.item()\n            val_epr += model.latest_epr\n            preds = logits.argmax(dim=-1).cpu().numpy()\n            gts = batch[\"target\"].cpu().numpy()\n        val_loss /= len(val_loader)\n        val_epr /= len(val_loader)\n\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(train_epr)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(val_epr)\n    experiment_data[\"synthetic\"][\"predictions\"].append(preds)\n    experiment_data[\"synthetic\"][\"ground_truth\"].append(gts)\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic dataset\nclass RandomSeqDataset(Dataset):\n    def __init__(self, num_seqs, total_len, vocab_size):\n        self.data = torch.randint(\n            1, vocab_size, (num_seqs, total_len), dtype=torch.long\n        )\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        seq = self.data[idx]\n        return {\"input\": seq[:-1], \"target\": seq[1:]}\n\n\nvocab_size = 50\nseq_total = 51  # input 50, target 50\ntrain_ds = RandomSeqDataset(200, seq_total, vocab_size)\nval_ds = RandomSeqDataset(50, seq_total, vocab_size)\ntrain_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=1)\n\n\n# Model with entropy\u2010aware memory\nclass MemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        if mem_x is None:\n            k = v = x\n        else:\n            k = torch.cat([mem_x.unsqueeze(0), x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        # compute entropy per query token\n        aw = attn_w.mean(dim=1)[0]  # (tgt_len, src_len)\n        eps = 1e-10\n        ent = -(aw * (aw + eps).log()).sum(dim=-1)  # (tgt_len,)\n        ent = ent.detach()\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent], dim=0)\n        total_ent = mem_ent_new.sum().item() + eps\n        if mem_x_new.size(0) > self.mem_size:\n            idx = torch.topk(mem_ent_new, self.mem_size).indices\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = total_ent / total_ent\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass TransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = MemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# Setup\nmodel = TransformerXLModel(vocab_size, embed_dim=64, num_heads=2, mem_size=20).to(\n    device\n)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# Training and evaluation\nnum_epochs = 3\nchunk_size = 10\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss, train_ratios = 0.0, []\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        inp, tgt = batch[\"input\"], batch[\"target\"]\n        mem_x = mem_ent = None\n        optimizer.zero_grad()\n        loss_accum = 0.0\n        for i in range(0, inp.size(1), chunk_size):\n            ic = inp[:, i : i + chunk_size]\n            tc = tgt[:, i : i + chunk_size]\n            logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n            loss = criterion(logits.reshape(-1, vocab_size), tc.reshape(-1))\n            loss_accum += loss\n            train_ratios.append(ratio)\n        loss_accum.backward()\n        optimizer.step()\n        train_loss += loss_accum.item() / (inp.size(1) / chunk_size)\n    avg_train_loss = train_loss / len(train_loader)\n    avg_train_ratio = sum(train_ratios) / len(train_ratios)\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(avg_train_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(avg_train_ratio)\n\n    model.eval()\n    val_loss, val_ratios = 0.0, []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            inp, tgt = batch[\"input\"], batch[\"target\"]\n            mem_x = mem_ent = None\n            loss_acc = 0.0\n            for i in range(0, inp.size(1), chunk_size):\n                ic = inp[:, i : i + chunk_size]\n                tc = tgt[:, i : i + chunk_size]\n                logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                loss_acc += criterion(logits.reshape(-1, vocab_size), tc.reshape(-1))\n                val_ratios.append(ratio)\n            val_loss += loss_acc.item() / (inp.size(1) / chunk_size)\n    avg_val_loss = val_loss / len(val_loader)\n    avg_val_ratio = sum(val_ratios) / len(val_ratios)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(avg_val_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(avg_val_ratio)\n    print(f\"Epoch {epoch}: validation_loss = {avg_val_loss:.4f}\")\n\n# Generate and visualize\nmodel.eval()\nwith torch.no_grad():\n    sample = val_ds[0][\"input\"].unsqueeze(0).to(device)\n    target = val_ds[0][\"target\"].tolist()\n    mem_x = mem_ent = None\n    preds = []\n    for t in range(sample.size(1)):\n        inp = sample[:, t].unsqueeze(1)\n        logits, mem_x, mem_ent, _ = model(inp, mem_x, mem_ent)\n        p = torch.softmax(logits.squeeze(0).squeeze(0), dim=-1)\n        preds.append(int(p.argmax().item()))\n    experiment_data[\"synthetic\"][\"predictions\"] = preds\n    experiment_data[\"synthetic\"][\"ground_truth\"] = target\n\nplt.figure()\nplt.plot(range(len(preds)), preds, label=\"predicted\")\nplt.plot(range(len(target)), target, label=\"ground_truth\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"synthetic_generation.png\"))\nplt.close()\n\n# Save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nimport math\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import Dataset\n\n# device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# synthetic dataset\nclass SyntheticLM(Dataset):\n    def __init__(self, num_seqs, seq_len, vocab_size):\n        self.data = torch.randint(0, vocab_size, (num_seqs, seq_len), dtype=torch.long)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n\n# transformer block with attention weight output\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=512):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=False)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.ff = nn.Sequential(\n            nn.Linear(d_model, dim_feedforward),\n            nn.ReLU(),\n            nn.Linear(dim_feedforward, d_model),\n        )\n        self.norm2 = nn.LayerNorm(d_model)\n        self.nhead = nhead\n\n    def forward(self, x):\n        attn_out, attn_w = self.attn(\n            x, x, x, need_weights=True, average_attn_weights=False\n        )\n        x = self.norm1(x + attn_out)\n        x2 = self.ff(x)\n        x = self.norm2(x + x2)\n        return x, attn_w\n\n\n# full model\nclass TransformerLM(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, max_seq_len, dim_feedforward):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n        self.block = TransformerBlock(d_model, nhead, dim_feedforward)\n        self.out = nn.Linear(d_model, vocab_size)\n        self.d_model = d_model\n        self.nhead = nhead\n\n    def forward(self, input_ids):\n        # input_ids: [batch, seq_total]\n        b, seq_total = input_ids.shape\n        x = self.embed(input_ids) * math.sqrt(self.d_model)\n        pos = (\n            torch.arange(seq_total, device=input_ids.device)\n            .unsqueeze(0)\n            .expand(b, seq_total)\n        )\n        x = x + self.pos_embed(pos)\n        x = x.permute(1, 0, 2)  # [seq_total, batch, d_model]\n        x, attn_w = self.block(x)\n        logits = self.out(x).permute(1, 0, 2)  # [batch, seq_total, vocab]\n        return logits, attn_w\n\n\n# hyperparameters\nvocab_size = 1000\nd_model = 128\nnhead = 8\nff_dim = 512\nseq_len = 200\nseg_len = 50\nmem_max_len = 100\nmax_seq_len = seg_len + mem_max_len\nlr = 1e-3\nepochs = 3\n\n# data\ntrain_ds = SyntheticLM(800, seq_len, vocab_size)\nval_ds = SyntheticLM(200, seq_len, vocab_size)\n\n# model, loss, optimizer\nmodel = TransformerLM(vocab_size, d_model, nhead, max_seq_len, ff_dim).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n# experiment data store\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# training/validation loops\nfor epoch in range(1, epochs + 1):\n    # training\n    model.train()\n    train_loss_sum = 0.0\n    train_ratio_sum = 0.0\n    train_count = 0\n    for seq in train_ds:\n        seq = seq.to(device).unsqueeze(0)\n        memory_ids = torch.tensor([], dtype=torch.long, device=device).unsqueeze(0)\n        num_segs = seq_len // seg_len\n        for i in range(num_segs):\n            # segment and concat memory\n            seg = seq[:, i * seg_len : (i + 1) * seg_len]\n            inp = torch.cat([memory_ids, seg], dim=1)\n            logits, attn_w = model(inp)\n            # LM loss on new segment\n            if seg_len > 1:\n                lm_logits = logits[:, memory_ids.size(1) : -1, :].reshape(\n                    -1, vocab_size\n                )\n                targets = seg[:, 1:].reshape(-1)\n                loss = criterion(lm_logits, targets)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                train_loss_sum += loss.item()\n            # compute entropy novelty\n            B = attn_w.shape[0] // nhead\n            T = inp.size(1)\n            attn = attn_w.view(B, nhead, T, T)[0]  # [heads, T, T]\n            entropy = -(attn * (attn + 1e-8).log()).sum(dim=2)  # [heads, T]\n            novelty = entropy.mean(dim=0)  # [T]\n            total_entropy = novelty.sum().item()\n            # compress memory\n            if T > mem_max_len:\n                vals, idxs = novelty.topk(mem_max_len)\n                keep = idxs.sort().values\n            else:\n                keep = torch.arange(T, device=device)\n            kept = novelty[keep]\n            ratio = kept.sum().item() / (total_entropy + 1e-8)\n            train_ratio_sum += ratio\n            train_count += 1\n            memory_ids = inp[:, keep]\n    train_loss = train_loss_sum / max(train_count, 1)\n    train_ratio = train_ratio_sum / max(train_count, 1)\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(train_ratio)\n\n    # validation\n    model.eval()\n    val_loss_sum = 0.0\n    val_ratio_sum = 0.0\n    val_count = 0\n    with torch.no_grad():\n        for seq in val_ds:\n            seq = seq.to(device).unsqueeze(0)\n            memory_ids = torch.tensor([], dtype=torch.long, device=device).unsqueeze(0)\n            for i in range(seq_len // seg_len):\n                seg = seq[:, i * seg_len : (i + 1) * seg_len]\n                inp = torch.cat([memory_ids, seg], dim=1)\n                logits, attn_w = model(inp)\n                if seg_len > 1:\n                    lm_logits = logits[:, memory_ids.size(1) : -1, :].reshape(\n                        -1, vocab_size\n                    )\n                    targets = seg[:, 1:].reshape(-1)\n                    loss = criterion(lm_logits, targets)\n                    val_loss_sum += loss.item()\n                B = attn_w.shape[0] // nhead\n                T = inp.size(1)\n                attn = attn_w.view(B, nhead, T, T)[0]\n                entropy = -(attn * (attn + 1e-8).log()).sum(dim=2)\n                novelty = entropy.mean(dim=0)\n                total_entropy = novelty.sum().item()\n                if T > mem_max_len:\n                    vals, idxs = novelty.topk(mem_max_len)\n                    keep = idxs.sort().values\n                else:\n                    keep = torch.arange(T, device=device)\n                kept = novelty[keep]\n                ratio = kept.sum().item() / (total_entropy + 1e-8)\n                val_ratio_sum += ratio\n                val_count += 1\n                memory_ids = inp[:, keep]\n    val_loss = val_loss_sum / max(val_count, 1)\n    val_ratio = val_ratio_sum / max(val_count, 1)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(val_ratio)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic dataset\nclass RandomSeqDataset(Dataset):\n    def __init__(self, num_seqs, total_len, vocab_size):\n        self.data = torch.randint(\n            1, vocab_size, (num_seqs, total_len), dtype=torch.long\n        )\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        seq = self.data[idx]\n        return {\"input\": seq[:-1], \"target\": seq[1:]}\n\n\nvocab_size = 50\nseq_total = 51  # input 50, target 50\ntrain_ds = RandomSeqDataset(200, seq_total, vocab_size)\nval_ds = RandomSeqDataset(50, seq_total, vocab_size)\ntrain_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=1)\n\n\n# Model with entropy\u2010aware memory\nclass MemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        if mem_x is None:\n            k = v = x\n        else:\n            k = torch.cat([mem_x.unsqueeze(0), x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        # compute entropy per query token\n        aw = attn_w.mean(dim=1)[0]  # (tgt_len, src_len)\n        eps = 1e-10\n        ent = -(aw * (aw + eps).log()).sum(dim=-1)  # (tgt_len,)\n        ent = ent.detach()\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent], dim=0)\n        total_ent = mem_ent_new.sum().item() + eps\n        if mem_x_new.size(0) > self.mem_size:\n            idx = torch.topk(mem_ent_new, self.mem_size).indices\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = total_ent / total_ent\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass TransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = MemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# Setup\nmodel = TransformerXLModel(vocab_size, embed_dim=64, num_heads=2, mem_size=20).to(\n    device\n)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# Training and evaluation\nnum_epochs = 3\nchunk_size = 10\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss, train_ratios = 0.0, []\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        inp, tgt = batch[\"input\"], batch[\"target\"]\n        mem_x = mem_ent = None\n        optimizer.zero_grad()\n        loss_accum = 0.0\n        for i in range(0, inp.size(1), chunk_size):\n            ic = inp[:, i : i + chunk_size]\n            tc = tgt[:, i : i + chunk_size]\n            logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n            loss = criterion(logits.reshape(-1, vocab_size), tc.reshape(-1))\n            loss_accum += loss\n            train_ratios.append(ratio)\n        loss_accum.backward()\n        optimizer.step()\n        train_loss += loss_accum.item() / (inp.size(1) / chunk_size)\n    avg_train_loss = train_loss / len(train_loader)\n    avg_train_ratio = sum(train_ratios) / len(train_ratios)\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(avg_train_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(avg_train_ratio)\n\n    model.eval()\n    val_loss, val_ratios = 0.0, []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            inp, tgt = batch[\"input\"], batch[\"target\"]\n            mem_x = mem_ent = None\n            loss_acc = 0.0\n            for i in range(0, inp.size(1), chunk_size):\n                ic = inp[:, i : i + chunk_size]\n                tc = tgt[:, i : i + chunk_size]\n                logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                loss_acc += criterion(logits.reshape(-1, vocab_size), tc.reshape(-1))\n                val_ratios.append(ratio)\n            val_loss += loss_acc.item() / (inp.size(1) / chunk_size)\n    avg_val_loss = val_loss / len(val_loader)\n    avg_val_ratio = sum(val_ratios) / len(val_ratios)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(avg_val_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(avg_val_ratio)\n    print(f\"Epoch {epoch}: validation_loss = {avg_val_loss:.4f}\")\n\n# Generate and visualize\nmodel.eval()\nwith torch.no_grad():\n    sample = val_ds[0][\"input\"].unsqueeze(0).to(device)\n    target = val_ds[0][\"target\"].tolist()\n    mem_x = mem_ent = None\n    preds = []\n    for t in range(sample.size(1)):\n        inp = sample[:, t].unsqueeze(1)\n        logits, mem_x, mem_ent, _ = model(inp, mem_x, mem_ent)\n        p = torch.softmax(logits.squeeze(0).squeeze(0), dim=-1)\n        preds.append(int(p.argmax().item()))\n    experiment_data[\"synthetic\"][\"predictions\"] = preds\n    experiment_data[\"synthetic\"][\"ground_truth\"] = target\n\nplt.figure()\nplt.plot(range(len(preds)), preds, label=\"predicted\")\nplt.plot(range(len(target)), target, label=\"ground_truth\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"synthetic_generation.png\"))\nplt.close()\n\n# Save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic dataset\nclass RandomSeqDataset(Dataset):\n    def __init__(self, num_seqs, total_len, vocab_size):\n        self.data = torch.randint(\n            1, vocab_size, (num_seqs, total_len), dtype=torch.long\n        )\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        seq = self.data[idx]\n        return {\"input\": seq[:-1], \"target\": seq[1:]}\n\n\nvocab_size = 50\nseq_total = 51  # input 50, target 50\ntrain_ds = RandomSeqDataset(200, seq_total, vocab_size)\nval_ds = RandomSeqDataset(50, seq_total, vocab_size)\ntrain_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=1)\n\n\n# Model with entropy\u2010aware memory\nclass MemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        if mem_x is None:\n            k = v = x\n        else:\n            k = torch.cat([mem_x.unsqueeze(0), x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        # compute entropy per query token\n        aw = attn_w.mean(dim=1)[0]  # (tgt_len, src_len)\n        eps = 1e-10\n        ent = -(aw * (aw + eps).log()).sum(dim=-1)  # (tgt_len,)\n        ent = ent.detach()\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent], dim=0)\n        total_ent = mem_ent_new.sum().item() + eps\n        if mem_x_new.size(0) > self.mem_size:\n            idx = torch.topk(mem_ent_new, self.mem_size).indices\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = total_ent / total_ent\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass TransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = MemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# Setup\nmodel = TransformerXLModel(vocab_size, embed_dim=64, num_heads=2, mem_size=20).to(\n    device\n)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# Training and evaluation\nnum_epochs = 3\nchunk_size = 10\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss, train_ratios = 0.0, []\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        inp, tgt = batch[\"input\"], batch[\"target\"]\n        mem_x = mem_ent = None\n        optimizer.zero_grad()\n        loss_accum = 0.0\n        for i in range(0, inp.size(1), chunk_size):\n            ic = inp[:, i : i + chunk_size]\n            tc = tgt[:, i : i + chunk_size]\n            logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n            loss = criterion(logits.reshape(-1, vocab_size), tc.reshape(-1))\n            loss_accum += loss\n            train_ratios.append(ratio)\n        loss_accum.backward()\n        optimizer.step()\n        train_loss += loss_accum.item() / (inp.size(1) / chunk_size)\n    avg_train_loss = train_loss / len(train_loader)\n    avg_train_ratio = sum(train_ratios) / len(train_ratios)\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(avg_train_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(avg_train_ratio)\n\n    model.eval()\n    val_loss, val_ratios = 0.0, []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            inp, tgt = batch[\"input\"], batch[\"target\"]\n            mem_x = mem_ent = None\n            loss_acc = 0.0\n            for i in range(0, inp.size(1), chunk_size):\n                ic = inp[:, i : i + chunk_size]\n                tc = tgt[:, i : i + chunk_size]\n                logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                loss_acc += criterion(logits.reshape(-1, vocab_size), tc.reshape(-1))\n                val_ratios.append(ratio)\n            val_loss += loss_acc.item() / (inp.size(1) / chunk_size)\n    avg_val_loss = val_loss / len(val_loader)\n    avg_val_ratio = sum(val_ratios) / len(val_ratios)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(avg_val_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(avg_val_ratio)\n    print(f\"Epoch {epoch}: validation_loss = {avg_val_loss:.4f}\")\n\n# Generate and visualize\nmodel.eval()\nwith torch.no_grad():\n    sample = val_ds[0][\"input\"].unsqueeze(0).to(device)\n    target = val_ds[0][\"target\"].tolist()\n    mem_x = mem_ent = None\n    preds = []\n    for t in range(sample.size(1)):\n        inp = sample[:, t].unsqueeze(1)\n        logits, mem_x, mem_ent, _ = model(inp, mem_x, mem_ent)\n        p = torch.softmax(logits.squeeze(0).squeeze(0), dim=-1)\n        preds.append(int(p.argmax().item()))\n    experiment_data[\"synthetic\"][\"predictions\"] = preds\n    experiment_data[\"synthetic\"][\"ground_truth\"] = target\n\nplt.figure()\nplt.plot(range(len(preds)), preds, label=\"predicted\")\nplt.plot(range(len(target)), target, label=\"ground_truth\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"synthetic_generation.png\"))\nplt.close()\n\n# Save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic dataset\nclass RandomSeqDataset(Dataset):\n    def __init__(self, num_seqs, total_len, vocab_size):\n        self.data = torch.randint(\n            1, vocab_size, (num_seqs, total_len), dtype=torch.long\n        )\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        seq = self.data[idx]\n        return {\"input\": seq[:-1], \"target\": seq[1:]}\n\n\nvocab_size = 50\nseq_total = 51  # input 50, target 50\ntrain_ds = RandomSeqDataset(200, seq_total, vocab_size)\nval_ds = RandomSeqDataset(50, seq_total, vocab_size)\ntrain_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=1)\n\n\n# Model with entropy\u2010aware memory\nclass MemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        if mem_x is None:\n            k = v = x\n        else:\n            k = torch.cat([mem_x.unsqueeze(0), x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        # compute entropy per query token\n        aw = attn_w.mean(dim=1)[0]  # (tgt_len, src_len)\n        eps = 1e-10\n        ent = -(aw * (aw + eps).log()).sum(dim=-1)  # (tgt_len,)\n        ent = ent.detach()\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent], dim=0)\n        total_ent = mem_ent_new.sum().item() + eps\n        if mem_x_new.size(0) > self.mem_size:\n            idx = torch.topk(mem_ent_new, self.mem_size).indices\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = total_ent / total_ent\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass TransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = MemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# Setup\nmodel = TransformerXLModel(vocab_size, embed_dim=64, num_heads=2, mem_size=20).to(\n    device\n)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# Training and evaluation\nnum_epochs = 3\nchunk_size = 10\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss, train_ratios = 0.0, []\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        inp, tgt = batch[\"input\"], batch[\"target\"]\n        mem_x = mem_ent = None\n        optimizer.zero_grad()\n        loss_accum = 0.0\n        for i in range(0, inp.size(1), chunk_size):\n            ic = inp[:, i : i + chunk_size]\n            tc = tgt[:, i : i + chunk_size]\n            logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n            loss = criterion(logits.reshape(-1, vocab_size), tc.reshape(-1))\n            loss_accum += loss\n            train_ratios.append(ratio)\n        loss_accum.backward()\n        optimizer.step()\n        train_loss += loss_accum.item() / (inp.size(1) / chunk_size)\n    avg_train_loss = train_loss / len(train_loader)\n    avg_train_ratio = sum(train_ratios) / len(train_ratios)\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(avg_train_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(avg_train_ratio)\n\n    model.eval()\n    val_loss, val_ratios = 0.0, []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            inp, tgt = batch[\"input\"], batch[\"target\"]\n            mem_x = mem_ent = None\n            loss_acc = 0.0\n            for i in range(0, inp.size(1), chunk_size):\n                ic = inp[:, i : i + chunk_size]\n                tc = tgt[:, i : i + chunk_size]\n                logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                loss_acc += criterion(logits.reshape(-1, vocab_size), tc.reshape(-1))\n                val_ratios.append(ratio)\n            val_loss += loss_acc.item() / (inp.size(1) / chunk_size)\n    avg_val_loss = val_loss / len(val_loader)\n    avg_val_ratio = sum(val_ratios) / len(val_ratios)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(avg_val_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(avg_val_ratio)\n    print(f\"Epoch {epoch}: validation_loss = {avg_val_loss:.4f}\")\n\n# Generate and visualize\nmodel.eval()\nwith torch.no_grad():\n    sample = val_ds[0][\"input\"].unsqueeze(0).to(device)\n    target = val_ds[0][\"target\"].tolist()\n    mem_x = mem_ent = None\n    preds = []\n    for t in range(sample.size(1)):\n        inp = sample[:, t].unsqueeze(1)\n        logits, mem_x, mem_ent, _ = model(inp, mem_x, mem_ent)\n        p = torch.softmax(logits.squeeze(0).squeeze(0), dim=-1)\n        preds.append(int(p.argmax().item()))\n    experiment_data[\"synthetic\"][\"predictions\"] = preds\n    experiment_data[\"synthetic\"][\"ground_truth\"] = target\n\nplt.figure()\nplt.plot(range(len(preds)), preds, label=\"predicted\")\nplt.plot(range(len(target)), target, label=\"ground_truth\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"synthetic_generation.png\"))\nplt.close()\n\n# Save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 113, in <module>\\n    logits = model(batch[\"input\"])\\n\n^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"runfile.py\", line 61, in forward\\n    combo = torch.cat([mem_old, x_embed],\ndim=1)\\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: Sizes of\ntensors must match except in dimension 1. Expected size 16 but got size 8 for\ntensor number 1 in the list.\\n', 'Execution time: 2 seconds seconds (time limit\nis an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 3.8777', '\\n', 'Epoch\n1: validation_loss = 3.8316', '\\n', 'Epoch 2: validation_loss = 3.7955', '\\n',\n'Execution time: 18 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 140, in <module>\\n    attn = attn_w.view(B, nhead, T, T)[0]\n# [heads, T, T]\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: shape\n\\'[0, 8, 50, 50]\\' is invalid for input of size 20000\\n', 'Execution time: a\nsecond seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 3.8989', '\\n', 'Epoch\n1: validation_loss = 3.8453', '\\n', 'Epoch 2: validation_loss = 3.7947', '\\n',\n'Execution time: 18 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 3.8975', '\\n', 'Epoch\n1: validation_loss = 3.8438', '\\n', 'Epoch 2: validation_loss = 3.8039', '\\n',\n'Execution time: 18 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 3.8975', '\\n', 'Epoch\n1: validation_loss = 3.8438', '\\n', 'Epoch 2: validation_loss = 3.8039', '\\n',\n'Execution time: 18 seconds seconds (time limit is an hour).']", ""], "analysis": ["The model keeps a persistent memory tensor across batches but does not handle\nvarying batch sizes. During validation, the final batch has size 8 (instead of\n16), so when forwarding this smaller batch the code attempts to concatenate a\nmemory of shape (16, \u2026) with inputs of shape (8, \u2026), causing a dimension\nmismatch and runtime error. To fix this, reset or reinitialize the memory at the\nstart of each batch (e.g., call init_memory(batch_size) before processing) or\nadd logic in forward() to reinitialize memory whenever the input batch size\nchanges. Alternatively, force uniform batch sizes (e.g., use drop_last=True) to\navoid mismatches.", "", "The code incorrectly assumes that attn_w returned by nn.MultiheadAttention has\nshape [batch * nhead, T, T], and thus does attn_w.view(B, nhead, T, T). However,\nin the current PyTorch version, with average_attn_weights=False, attn_w has\nshape [batch, nhead, T, T], so attn_w.shape[0]//nhead becomes 0 and the view\ncall fails with an invalid reshape ([0, 8, 50, 50]). To fix this, the code\nshould inspect attn_w.dim(): if it's 4D ([batch, nhead, T, T]), simply index\nattn = attn_w[0]; if it's 3D ([batch * nhead, T, T]), then reshape with view(B,\nnhead, T, T) as intended.", "", "", "", ""], "exc_type": ["RuntimeError", null, "RuntimeError", null, null, null, null], "exc_info": [{"args": ["Sizes of tensors must match except in dimension 1. Expected size 16 but got size 8 for tensor number 1 in the list."]}, null, {"args": ["shape '[0, 8, 50, 50]' is invalid for input of size 20000"]}, null, null, null, null], "exc_stack": [[["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 113, "<module>", "logits = model(batch[\"input\"])"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["runfile.py", 61, "forward", "combo = torch.cat([mem_old, x_embed], dim=1)"]], null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 140, "<module>", "attn = attn_w.view(B, nhead, T, T)[0]  # [heads, T, T]"]], null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training memory retention ratio", "lower_is_better": false, "description": "Final memory retention ratio on the training dataset.", "data": [{"dataset_name": "synthetic", "final_value": 0.8214380449300438, "best_value": 0.8214380449300438}]}, {"metric_name": "validation memory retention ratio", "lower_is_better": false, "description": "Final memory retention ratio on the validation dataset.", "data": [{"dataset_name": "synthetic", "final_value": 0.8210265552539907, "best_value": 0.8210265552539907}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Final loss on the training dataset.", "data": [{"dataset_name": "synthetic", "final_value": 3.6700600471496587, "best_value": 3.6700600471496587}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final loss on the validation dataset.", "data": [{"dataset_name": "synthetic", "final_value": 3.795520591735841, "best_value": 3.795520591735841}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training memory retention ratio", "lower_is_better": false, "description": "Final memory retention ratio on the training split of the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.8215399167850012, "best_value": 0.8215399167850012}]}, {"metric_name": "validation memory retention ratio", "lower_is_better": false, "description": "Final memory retention ratio on the validation split of the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.8213713723810073, "best_value": 0.8213713723810073}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Final loss on the training split of the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 3.6716410655975364, "best_value": 3.6716410655975364}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final loss on the validation split of the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 3.794670776367188, "best_value": 3.794670776367188}]}]}, {"metric_names": [{"metric_name": "training memory retention ratio", "lower_is_better": false, "description": "Ratio of memory retained by the training process", "data": [{"dataset_name": "synthetic", "final_value": 0.8215293000352482, "best_value": 0.8215293000352482}]}, {"metric_name": "validation memory retention ratio", "lower_is_better": false, "description": "Ratio of memory retained on validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.8209828211691476, "best_value": 0.8209828211691476}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss value on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 3.680085485458373, "best_value": 3.680085485458373}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 3.8039309997558592, "best_value": 3.8039309997558592}]}]}, {"metric_names": [{"metric_name": "train memory retention ratio", "lower_is_better": false, "description": "Memory retention ratio on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.8215293000352482, "best_value": 0.8215293000352482}]}, {"metric_name": "validation memory retention ratio", "lower_is_better": false, "description": "Memory retention ratio on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.8209828211691476, "best_value": 0.8209828211691476}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 3.680085485458373, "best_value": 3.680085485458373}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 3.8039309997558592, "best_value": 3.8039309997558592}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false], "plots": [[], ["../../logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_loss_curve.png", "../../logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_metric_curve.png", "../../logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_generation_comparison.png", "../../logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_generation.png"], [], ["../../logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_loss_curve.png", "../../logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_metric_curve.png", "../../logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_generation_comparison.png", "../../logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_generation.png"], ["../../logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_loss_curve.png", "../../logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_metric_curve.png", "../../logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_generation_comparison.png", "../../logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_generation.png"], ["../../logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_loss_curve.png", "../../logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_metric_curve.png", "../../logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_generation_comparison.png", "../../logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_generation.png"], ["../../logs/0-run/experiment_results/seed_aggregation_2f0dcd7ded134795b6d7a2bed0cd728f/synthetic_loss_mean_se.png", "../../logs/0-run/experiment_results/seed_aggregation_2f0dcd7ded134795b6d7a2bed0cd728f/synthetic_generation_mean_se.png", "../../logs/0-run/experiment_results/seed_aggregation_2f0dcd7ded134795b6d7a2bed0cd728f/synthetic_metric_mean_se.png"]], "plot_paths": [[], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_metric_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_generation_comparison.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_generation.png"], [], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_metric_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_generation_comparison.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_generation.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_metric_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_generation_comparison.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_generation.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_metric_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_generation_comparison.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_generation.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/seed_aggregation_2f0dcd7ded134795b6d7a2bed0cd728f/synthetic_loss_mean_se.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/seed_aggregation_2f0dcd7ded134795b6d7a2bed0cd728f/synthetic_generation_mean_se.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/seed_aggregation_2f0dcd7ded134795b6d7a2bed0cd728f/synthetic_metric_mean_se.png"]], "plot_analyses": [[], [{"analysis": "Training loss decreases steadily from about 3.95 at epoch 1 to 3.67 at epoch 3, while validation loss also diminishes from around 3.88 to 3.80 over the same span. The gap between train and val loss narrows slightly, suggesting no overfitting at this preliminary stage and that the model is learning meaningful structure beyond a random baseline (which would be ~ln(50)\u22483.91).", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_loss_curve.png"}, {"analysis": "Memory retention ratios remain quite stable across epochs. The training retention ratio rises marginally from ~0.8207 to a peak of ~0.8223 at epoch 2, then lightly recedes to ~0.8214. Validation retention falls gently from ~0.8217 to ~0.8201. These small fluctuations imply that the current adaptive compression mechanism is preserving roughly the same fraction of tokens, with only minor gains mid-training.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_metric_curve.png"}, {"analysis": "Generated sequences display a token distribution roughly similar to ground truth: both cover the full 0\u201349 range and share similar volatility patterns. However, specific time-step alignments are inconsistent\u2014peaks and troughs in the generated plot often do not line up with those in the ground-truth sequence. This suggests that while the model captures the overall distribution, it has not yet learned to predict exact token values reliably.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_generation_comparison.png"}, {"analysis": "Overlaying predicted vs. ground-truth tokens confirms misalignments: there are many time steps where the prediction deviates markedly from the true token. Although both curves occupy similar ranges, the lack of tight correspondence indicates the model\u2019s generative performance remains weak at this stage. It is effectively sampling from a learned distribution rather than performing accurate next-token prediction.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_generation.png"}], [], [{"analysis": "Synthetic Dataset Loss Curves depict smooth, monotonic declines in both training and validation loss across three epochs. Training loss falls more steeply (from ~3.95 to ~3.67) compared to validation (from ~3.90 to ~3.80), indicating steady learning with a modest but widening generalization gap. No signs of overfitting yet; the parallel downward trend suggests the model is improving and still under capacity relative to the synthetic task.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_loss_curve.png"}, {"analysis": "Memory Retention over Epochs remains highly stable, with training ratio rising slightly from ~0.8208 to ~0.8219 at epoch 2 before a minor drop to ~0.8215 at epoch 3. Validation retention follows a gentle upward drift from ~0.8182 to ~0.8186 at epoch 2 then dips marginally by epoch 3. Fluctuations are within 0.001, reflecting that the adaptive compressive memory mechanism is maintaining a consistent information retention level without large swings.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_metric_curve.png"}, {"analysis": "Ground truth sequence shows tokens drawn uniformly across the full integer range (5\u201350) with no clear periodic structure. Generated samples capture the same value range and approximate variability but differ in the ordering and timing of peaks and troughs. This indicates the model\u2019s output distribution aligns with the data distribution, but precise sequence-level fidelity is not yet achieved.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_generation_comparison.png"}, {"analysis": "Predicted versus ground truth overlay reveals partial alignment: high-value spikes occasionally coincide, and low-value dips are sometimes matched, yet many time steps exhibit offsets in magnitude or phase. The model captures global distributional characteristics but lacks precise temporal localization. Further tuning of the entropy-aware compression or longer training may improve sequence accuracy.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_generation.png"}], [{"analysis": "Training loss steadily decreases from about 3.96 to 3.68 over three epochs, while validation loss decreases more modestly from roughly 3.90 to 3.80. Both curves trend downward, indicating successful learning and no dramatic overfitting within the small epoch window. The small increase in train\u2013validation gap from epoch 1 (\u22480.06) to epoch 3 (\u22480.08) suggests only a mild divergence, but further monitoring of this gap across additional epochs would be prudent to ensure generalization remains robust.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_loss_curve.png"}, {"analysis": "Memory retention ratio on the training set climbs from \u22480.8205 at epoch 1 to \u22480.8219 at epoch 2, then slightly dips to \u22480.8215 at epoch 3. Validation retention follows a similar pattern, rising to \u22480.8216 at epoch 2 before falling back to \u22480.8210. This transient peak at epoch 2 implies that the entropy-aware compression is effectively preserving novel tokens early on, but may require threshold adjustments or a longer warm-up to stabilize and sustain the retention benefit.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_metric_curve.png"}, {"analysis": "Ground truth sequence exhibits high variability across 50 time steps, with tokens spanning nearly the full 0\u201350 range. Generated samples capture the global range but misalign in timing and magnitude, often holding values too long or failing to reproduce sharp transitions. Although the model reproduces some high and low regimes, it struggles with the exact temporal pattern, suggesting limited sequence fidelity under the current memory scheme.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_generation_comparison.png"}, {"analysis": "Overlaying predicted versus ground truth sequences reinforces these discrepancies: early steps show high predictions against low ground-truth tokens and vice versa, mid-sequence predictions flatten around 30\u201340 while the true signal remains erratic, and later steps exhibit both abrupt deviations and occasional alignment. The model learns the general distribution but not the precise ordering, indicating that further tuning of the compressive memory (e.g., entropy thresholds, memory budget) or extended training is needed to better retain fine-grained contextual information.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_generation.png"}], [{"analysis": "Training loss drops from ~3.96 to ~3.68 over three epochs while validation loss decreases more modestly from ~3.90 to ~3.80, indicating stable learning without severe overfitting yet. The training curve falls faster than validation, causing a small widening of the generalization gap by epoch three. Continued monitoring beyond three epochs will clarify if this trend persists.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_loss_curve.png"}, {"analysis": "Memory retention ratio climbs slightly from ~0.8205 to ~0.8219 on training data and from ~0.8209 to ~0.8216 on validation at epoch two, then lightly declines by epoch three. The persistent ~0.001 gap between train and validation suggests the entropy-aware mechanism preserves slightly more tokens during mid-training but yields only marginal gains in this preliminary stage.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_metric_curve.png"}, {"analysis": "Generated samples capture rough token-value ranges seen in the ground truth but exhibit temporal misalignments and missing valleys. Peaks in the synthetic sequence sometimes align, yet there is a noticeable shift in time steps and underrepresentation of lower token values. This points to an initial ability to mimic global distribution but limited local coherence.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_generation_comparison.png"}, {"analysis": "Direct comparison of predicted versus ground-truth sequences reveals intermittent correlation: certain mid-sequence peaks are tracked, but large segments show flat or plateaued predictions that fail to follow the rapid fluctuations of the ground truth. The model reproduces coarse patterns but struggles with token-level variability, indicating room for improving attention entropy thresholds or training dynamics.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_generation.png"}], []], "vlm_feedback_summary": ["[]", "Loss curves show consistent learning beyond the random baseline, but memory\nretention exhibits only marginal improvements. Generated samples match the\nmarginal token distribution but fail to track the true sequence precisely, as\nconfirmed by the overlay. Overall, the basic implementation works: loss declines\nand memory compression remains stable, yet sequence generation accuracy is low.\nFurther work should focus on strengthening token-level novelty signals and\nensuring they guide retention to improve exact next-token predictions.", "[]", "Training shows solid loss reduction and stable memory retention. Generated\noutputs match overall token distribution but require improved temporal fidelity\nrelative to ground truth. Adaptive memory appears stable but may need stronger\nsignals or longer training to sharpen sequence-level predictions.", "Loss curves confirm basic convergence with a small and stable train\u2013validation\ngap. Memory retention peaks at epoch 2, demonstrating the entropy-aware\nmechanism\u2019s initial effectiveness but also its slight instability afterward.\nGenerative outputs reveal that while the model captures coarse token\ndistributions, it fails to reproduce the exact temporal dynamics. Suggested next\nsteps include extending training, refining the entropy-based compression\nthreshold, and potentially increasing memory budget or warm-up duration to\nreinforce precise long-range sequence retention.", "Training and validation losses decrease smoothly with no immediate overfitting;\nmemory retention sees modest mid-training gain; generation quality shows global\npattern matching but poor local alignment; next steps include longer training,\nhyperparameter tuning, and refinement of the entropy-based compression criteria.", "[]"], "exec_time": [2.4083096981048584, 18.679852724075317, 1.7737486362457275, 18.812530040740967, 18.808860301971436, 18.805821657180786, null], "exec_time_feedback": ["", "", "", "", "", "", ""], "datasets_successfully_tested": [[], ["[synthetic]"], [], ["[synthetic]"], ["[]"], ["['synthetic']"], []], "plot_code": [null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nlosses = experiment_data.get(\"synthetic\", {}).get(\"losses\", {})\nmetrics = experiment_data.get(\"synthetic\", {}).get(\"metrics\", {})\npreds = experiment_data.get(\"synthetic\", {}).get(\"predictions\", [])\ngt = experiment_data.get(\"synthetic\", {}).get(\"ground_truth\", [])\n\nprint(\"Final Train Loss:\", losses.get(\"train\", []))\nprint(\"Final Val Loss:\", losses.get(\"val\", []))\nprint(\"Final Train Metric:\", metrics.get(\"train\", []))\nprint(\"Final Val Metric:\", metrics.get(\"val\", []))\n\ntry:\n    plt.figure()\n    tl = losses.get(\"train\", [])\n    vl = losses.get(\"val\", [])\n    epochs = range(1, len(tl) + 1)\n    plt.plot(epochs, tl, label=\"Train Loss\")\n    plt.plot(epochs, vl, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Synthetic Dataset Loss Curves\\nTraining vs Validation Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    tm = metrics.get(\"train\", [])\n    vm = metrics.get(\"val\", [])\n    epochs = range(1, len(tm) + 1)\n    plt.plot(epochs, tm, label=\"Train Ratio\")\n    plt.plot(epochs, vm, label=\"Val Ratio\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Memory Retention Ratio\")\n    plt.title(\"Synthetic Dataset Metric Curves\\nMemory Retention over Epochs\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_metric_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating metric curve: {e}\")\n    plt.close()\n\ntry:\n    if preds and gt:\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        axs[0].plot(range(len(gt)), gt)\n        axs[0].set_title(\"Ground Truth Sequence\")\n        axs[0].set_xlabel(\"Time Step\")\n        axs[0].set_ylabel(\"Token\")\n        axs[1].plot(range(len(preds)), preds, color=\"orange\")\n        axs[1].set_title(\"Generated Samples\")\n        axs[1].set_xlabel(\"Time Step\")\n        plt.suptitle(\n            \"Synthetic Dataset Generation\\nLeft: Ground Truth, Right: Generated Samples\"\n        )\n        plt.savefig(os.path.join(working_dir, \"synthetic_generation_comparison.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating generation comparison: {e}\")\n    plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nlosses = experiment_data.get(\"synthetic\", {}).get(\"losses\", {})\nmetrics = experiment_data.get(\"synthetic\", {}).get(\"metrics\", {})\npreds = experiment_data.get(\"synthetic\", {}).get(\"predictions\", [])\ngt = experiment_data.get(\"synthetic\", {}).get(\"ground_truth\", [])\n\nprint(\"Final Train Loss:\", losses.get(\"train\", []))\nprint(\"Final Val Loss:\", losses.get(\"val\", []))\nprint(\"Final Train Metric:\", metrics.get(\"train\", []))\nprint(\"Final Val Metric:\", metrics.get(\"val\", []))\n\ntry:\n    plt.figure()\n    tl = losses.get(\"train\", [])\n    vl = losses.get(\"val\", [])\n    epochs = range(1, len(tl) + 1)\n    plt.plot(epochs, tl, label=\"Train Loss\")\n    plt.plot(epochs, vl, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Synthetic Dataset Loss Curves\\nTraining vs Validation Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    tm = metrics.get(\"train\", [])\n    vm = metrics.get(\"val\", [])\n    epochs = range(1, len(tm) + 1)\n    plt.plot(epochs, tm, label=\"Train Ratio\")\n    plt.plot(epochs, vm, label=\"Val Ratio\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Memory Retention Ratio\")\n    plt.title(\"Synthetic Dataset Metric Curves\\nMemory Retention over Epochs\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_metric_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating metric curve: {e}\")\n    plt.close()\n\ntry:\n    if preds and gt:\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        axs[0].plot(range(len(gt)), gt)\n        axs[0].set_title(\"Ground Truth Sequence\")\n        axs[0].set_xlabel(\"Time Step\")\n        axs[0].set_ylabel(\"Token\")\n        axs[1].plot(range(len(preds)), preds, color=\"orange\")\n        axs[1].set_title(\"Generated Samples\")\n        axs[1].set_xlabel(\"Time Step\")\n        plt.suptitle(\n            \"Synthetic Dataset Generation\\nLeft: Ground Truth, Right: Generated Samples\"\n        )\n        plt.savefig(os.path.join(working_dir, \"synthetic_generation_comparison.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating generation comparison: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nlosses = experiment_data.get(\"synthetic\", {}).get(\"losses\", {})\nmetrics = experiment_data.get(\"synthetic\", {}).get(\"metrics\", {})\npreds = experiment_data.get(\"synthetic\", {}).get(\"predictions\", [])\ngt = experiment_data.get(\"synthetic\", {}).get(\"ground_truth\", [])\n\nprint(\"Final Train Loss:\", losses.get(\"train\", []))\nprint(\"Final Val Loss:\", losses.get(\"val\", []))\nprint(\"Final Train Metric:\", metrics.get(\"train\", []))\nprint(\"Final Val Metric:\", metrics.get(\"val\", []))\n\ntry:\n    plt.figure()\n    tl = losses.get(\"train\", [])\n    vl = losses.get(\"val\", [])\n    epochs = range(1, len(tl) + 1)\n    plt.plot(epochs, tl, label=\"Train Loss\")\n    plt.plot(epochs, vl, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Synthetic Dataset Loss Curves\\nTraining vs Validation Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    tm = metrics.get(\"train\", [])\n    vm = metrics.get(\"val\", [])\n    epochs = range(1, len(tm) + 1)\n    plt.plot(epochs, tm, label=\"Train Ratio\")\n    plt.plot(epochs, vm, label=\"Val Ratio\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Memory Retention Ratio\")\n    plt.title(\"Synthetic Dataset Metric Curves\\nMemory Retention over Epochs\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_metric_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating metric curve: {e}\")\n    plt.close()\n\ntry:\n    if preds and gt:\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        axs[0].plot(range(len(gt)), gt)\n        axs[0].set_title(\"Ground Truth Sequence\")\n        axs[0].set_xlabel(\"Time Step\")\n        axs[0].set_ylabel(\"Token\")\n        axs[1].plot(range(len(preds)), preds, color=\"orange\")\n        axs[1].set_title(\"Generated Samples\")\n        axs[1].set_xlabel(\"Time Step\")\n        plt.suptitle(\n            \"Synthetic Dataset Generation\\nLeft: Ground Truth, Right: Generated Samples\"\n        )\n        plt.savefig(os.path.join(working_dir, \"synthetic_generation_comparison.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating generation comparison: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nlosses = experiment_data.get(\"synthetic\", {}).get(\"losses\", {})\nmetrics = experiment_data.get(\"synthetic\", {}).get(\"metrics\", {})\npreds = experiment_data.get(\"synthetic\", {}).get(\"predictions\", [])\ngt = experiment_data.get(\"synthetic\", {}).get(\"ground_truth\", [])\n\nprint(\"Final Train Loss:\", losses.get(\"train\", []))\nprint(\"Final Val Loss:\", losses.get(\"val\", []))\nprint(\"Final Train Metric:\", metrics.get(\"train\", []))\nprint(\"Final Val Metric:\", metrics.get(\"val\", []))\n\ntry:\n    plt.figure()\n    tl = losses.get(\"train\", [])\n    vl = losses.get(\"val\", [])\n    epochs = range(1, len(tl) + 1)\n    plt.plot(epochs, tl, label=\"Train Loss\")\n    plt.plot(epochs, vl, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Synthetic Dataset Loss Curves\\nTraining vs Validation Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    tm = metrics.get(\"train\", [])\n    vm = metrics.get(\"val\", [])\n    epochs = range(1, len(tm) + 1)\n    plt.plot(epochs, tm, label=\"Train Ratio\")\n    plt.plot(epochs, vm, label=\"Val Ratio\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Memory Retention Ratio\")\n    plt.title(\"Synthetic Dataset Metric Curves\\nMemory Retention over Epochs\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_metric_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating metric curve: {e}\")\n    plt.close()\n\ntry:\n    if preds and gt:\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        axs[0].plot(range(len(gt)), gt)\n        axs[0].set_title(\"Ground Truth Sequence\")\n        axs[0].set_xlabel(\"Time Step\")\n        axs[0].set_ylabel(\"Token\")\n        axs[1].plot(range(len(preds)), preds, color=\"orange\")\n        axs[1].set_title(\"Generated Samples\")\n        axs[1].set_xlabel(\"Time Step\")\n        plt.suptitle(\n            \"Synthetic Dataset Generation\\nLeft: Ground Truth, Right: Generated Samples\"\n        )\n        plt.savefig(os.path.join(working_dir, \"synthetic_generation_comparison.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating generation comparison: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load all experiment data\ntry:\n    experiment_data_path_list = [\n        \"experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/experiment_data.npy\",\n        \"experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/experiment_data.npy\",\n        \"experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/experiment_data.npy\",\n    ]\n    all_experiment_data = []\n    for p in experiment_data_path_list:\n        exp = np.load(\n            os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\"), p), allow_pickle=True\n        ).item()\n        all_experiment_data.append(exp.get(\"synthetic\", {}))\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_experiment_data = []\n\n# Aggregate losses and metrics\nloss_trains = [e.get(\"losses\", {}).get(\"train\", []) for e in all_experiment_data]\nloss_vals = [e.get(\"losses\", {}).get(\"val\", []) for e in all_experiment_data]\nmet_trains = [e.get(\"metrics\", {}).get(\"train\", []) for e in all_experiment_data]\nmet_vals = [e.get(\"metrics\", {}).get(\"val\", []) for e in all_experiment_data]\n\n\ndef compute_stats(list_of_lists):\n    valid = [l for l in list_of_lists if len(l) > 0]\n    if not valid:\n        return None, None\n    min_len = min(len(l) for l in valid)\n    arr = np.array([l[:min_len] for l in valid])\n    mean = arr.mean(axis=0)\n    se = arr.std(axis=0, ddof=0) / np.sqrt(arr.shape[0])\n    return mean, se\n\n\nlt_mean, lt_se = compute_stats(loss_trains)\nlv_mean, lv_se = compute_stats(loss_vals)\nmt_mean, mt_se = compute_stats(met_trains)\nmv_mean, mv_se = compute_stats(met_vals)\n\n# Print final means and SE\nif lt_mean is not None:\n    print(f\"Final Train Loss Mean: {lt_mean[-1]:.4f} \u00b1 {lt_se[-1]:.4f}\")\nif lv_mean is not None:\n    print(f\"Final Val   Loss Mean: {lv_mean[-1]:.4f} \u00b1 {lv_se[-1]:.4f}\")\nif mt_mean is not None:\n    print(f\"Final Train Metric Mean: {mt_mean[-1]:.4f} \u00b1 {mt_se[-1]:.4f}\")\nif mv_mean is not None:\n    print(f\"Final Val   Metric Mean: {mv_mean[-1]:.4f} \u00b1 {mv_se[-1]:.4f}\")\n\n# Plot mean \u00b1 SE loss\ntry:\n    if lt_mean is not None and lv_mean is not None:\n        plt.figure()\n        epochs = np.arange(1, len(lt_mean) + 1)\n        plt.errorbar(epochs, lt_mean, yerr=lt_se, label=\"Train Loss Mean \u00b1 SE\")\n        plt.errorbar(epochs, lv_mean, yerr=lv_se, label=\"Val Loss Mean \u00b1 SE\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\n            \"Synthetic Dataset Loss Curves with Mean and SE\\nTraining vs Validation Loss\"\n        )\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_loss_mean_se.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss mean/se plot: {e}\")\n    plt.close()\n\n# Plot mean \u00b1 SE metrics\ntry:\n    if mt_mean is not None and mv_mean is not None:\n        plt.figure()\n        epochs = np.arange(1, len(mt_mean) + 1)\n        plt.errorbar(epochs, mt_mean, yerr=mt_se, label=\"Train Metric Mean \u00b1 SE\")\n        plt.errorbar(epochs, mv_mean, yerr=mv_se, label=\"Val Metric Mean \u00b1 SE\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Memory Retention Ratio\")\n        plt.title(\n            \"Synthetic Dataset Metric Curves with Mean and SE\\nMemory Retention over Epochs\"\n        )\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_metric_mean_se.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating metric mean/se plot: {e}\")\n    plt.close()\n\n# Aggregate and plot generation sequences with mean \u00b1 SE\ntry:\n    preds_list = [e.get(\"predictions\", []) for e in all_experiment_data]\n    gt_list = [e.get(\"ground_truth\", []) for e in all_experiment_data]\n    gt_mean, gt_se = compute_stats(gt_list)\n    pd_mean, pd_se = compute_stats(preds_list)\n    if gt_mean is not None and pd_mean is not None:\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        x_gt = np.arange(1, len(gt_mean) + 1)\n        axs[0].errorbar(x_gt, gt_mean, yerr=gt_se, label=\"Mean \u00b1 SE\")\n        axs[0].set_title(\"Ground Truth Sequence (Mean \u00b1 SE)\")\n        axs[0].set_xlabel(\"Time Step\")\n        axs[0].set_ylabel(\"Token\")\n        axs[0].legend()\n        x_pd = np.arange(1, len(pd_mean) + 1)\n        axs[1].errorbar(x_pd, pd_mean, yerr=pd_se, color=\"orange\", label=\"Mean \u00b1 SE\")\n        axs[1].set_title(\"Generated Samples (Mean \u00b1 SE)\")\n        axs[1].set_xlabel(\"Time Step\")\n        axs[1].legend()\n        plt.suptitle(\n            \"Synthetic Dataset Generation Comparison\\nLeft: Ground Truth, Right: Generated Samples\"\n        )\n        plt.savefig(os.path.join(working_dir, \"synthetic_generation_mean_se.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating generation mean/se plot: {e}\")\n    plt.close()\n"], "plot_plan": [null, "The script will load the saved experiment_data.npy from the working directory and extract training/validation losses, metrics, predictions, and ground truth for the synthetic dataset. It prints out the final recorded loss and ratio values for quick review in the console. Using basic matplotlib routines and wrapped in individual try-except blocks, it then produces three distinct plots: the loss curves, the memory retention metric curves, and a side\u2010by\u2010side generation comparison. Each figure is clearly titled with the dataset name and plot type, with subtitles indicating left vs right when relevant. The plots are saved to the working directory with descriptive filenames and closed immediately to free resources. Any loading or plotting errors are caught and printed without stopping the script. This approach ensures reproducibility and easy inspection of both curve trajectories and sample generation.", null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null], "is_seed_node": [false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, true], "parse_metrics_plan": ["", "The script builds the working directory path, loads the saved NumPy file with\n`allow_pickle`, and retrieves the `experiment_data` dictionary. It then loops\nover each dataset in the dictionary, extracts the final values from the\n`metrics` (memory retention ratios) and `losses` (training and validation loss)\nlists, and prints them with clear, descriptive labels. No entry\u2010point guard is\nused and the code runs immediately when executed.", "", "The script builds the working directory path, loads the saved NumPy file with\n`allow_pickle`, and retrieves the `experiment_data` dictionary. It then loops\nover each dataset in the dictionary, extracts the final values from the\n`metrics` (memory retention ratios) and `losses` (training and validation loss)\nlists, and prints them with clear, descriptive labels. No entry\u2010point guard is\nused and the code runs immediately when executed.", "The script builds the working directory path, loads the saved NumPy file with\n`allow_pickle`, and retrieves the `experiment_data` dictionary. It then loops\nover each dataset in the dictionary, extracts the final values from the\n`metrics` (memory retention ratios) and `losses` (training and validation loss)\nlists, and prints them with clear, descriptive labels. No entry\u2010point guard is\nused and the code runs immediately when executed.", "The script builds the working directory path, loads the saved NumPy file with\n`allow_pickle`, and retrieves the `experiment_data` dictionary. It then loops\nover each dataset in the dictionary, extracts the final values from the\n`metrics` (memory retention ratios) and `losses` (training and validation loss)\nlists, and prints them with clear, descriptive labels. No entry\u2010point guard is\nused and the code runs immediately when executed.", ""], "parse_metrics_code": ["", "import os\nimport numpy as np\n\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, dataset in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract and print final memory retention ratios\n    train_ratios = dataset.get(\"metrics\", {}).get(\"train\", [])\n    val_ratios = dataset.get(\"metrics\", {}).get(\"val\", [])\n    if train_ratios:\n        print(f\"Final training memory retention ratio: {train_ratios[-1]}\")\n    if val_ratios:\n        print(f\"Final validation memory retention ratio: {val_ratios[-1]}\")\n\n    # Extract and print final losses\n    train_losses = dataset.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dataset.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"Final training loss: {train_losses[-1]}\")\n    if val_losses:\n        print(f\"Final validation loss: {val_losses[-1]}\")\n\n    print()\n", "", "import os\nimport numpy as np\n\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, dataset in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract and print final memory retention ratios\n    train_ratios = dataset.get(\"metrics\", {}).get(\"train\", [])\n    val_ratios = dataset.get(\"metrics\", {}).get(\"val\", [])\n    if train_ratios:\n        print(f\"Final training memory retention ratio: {train_ratios[-1]}\")\n    if val_ratios:\n        print(f\"Final validation memory retention ratio: {val_ratios[-1]}\")\n\n    # Extract and print final losses\n    train_losses = dataset.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dataset.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"Final training loss: {train_losses[-1]}\")\n    if val_losses:\n        print(f\"Final validation loss: {val_losses[-1]}\")\n\n    print()\n", "import os\nimport numpy as np\n\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, dataset in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract and print final memory retention ratios\n    train_ratios = dataset.get(\"metrics\", {}).get(\"train\", [])\n    val_ratios = dataset.get(\"metrics\", {}).get(\"val\", [])\n    if train_ratios:\n        print(f\"Final training memory retention ratio: {train_ratios[-1]}\")\n    if val_ratios:\n        print(f\"Final validation memory retention ratio: {val_ratios[-1]}\")\n\n    # Extract and print final losses\n    train_losses = dataset.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dataset.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"Final training loss: {train_losses[-1]}\")\n    if val_losses:\n        print(f\"Final validation loss: {val_losses[-1]}\")\n\n    print()\n", "import os\nimport numpy as np\n\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, dataset in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract and print final memory retention ratios\n    train_ratios = dataset.get(\"metrics\", {}).get(\"train\", [])\n    val_ratios = dataset.get(\"metrics\", {}).get(\"val\", [])\n    if train_ratios:\n        print(f\"Final training memory retention ratio: {train_ratios[-1]}\")\n    if val_ratios:\n        print(f\"Final validation memory retention ratio: {val_ratios[-1]}\")\n\n    # Extract and print final losses\n    train_losses = dataset.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dataset.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"Final training loss: {train_losses[-1]}\")\n    if val_losses:\n        print(f\"Final validation loss: {val_losses[-1]}\")\n\n    print()\n", ""], "parse_term_out": ["", "['Dataset: synthetic', '\\n', 'Final training memory retention ratio:\n0.8214380449300438', '\\n', 'Final validation memory retention ratio:\n0.8210265552539907', '\\n', 'Final training loss: 3.6700600471496587', '\\n',\n'Final validation loss: 3.795520591735841', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "", "['Dataset: synthetic', '\\n', 'Final training memory retention ratio:\n0.8215399167850012', '\\n', 'Final validation memory retention ratio:\n0.8213713723810073', '\\n', 'Final training loss: 3.6716410655975364', '\\n',\n'Final validation loss: 3.794670776367188', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Final training memory retention ratio:\n0.8215293000352482', '\\n', 'Final validation memory retention ratio:\n0.8209828211691476', '\\n', 'Final training loss: 3.680085485458373', '\\n',\n'Final validation loss: 3.8039309997558592', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Final training memory retention ratio:\n0.8215293000352482', '\\n', 'Final validation memory retention ratio:\n0.8209828211691476', '\\n', 'Final training loss: 3.680085485458373', '\\n',\n'Final validation loss: 3.8039309997558592', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null], "completed_stages": ["Stage_1"]}
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[1, 4], [1, 3], [1, 5], [1, 6]], "layout": [[0.16666666666666666, 0.0], [0.5, 0.0], [0.8333333333333334, 0.0], [0.0, 1.0], [0.3333333333333333, 1.0], [0.6666666666666666, 1.0], [1.0, 1.0]], "plan": ["We will generate a synthetic next-token prediction dataset by sampling random\ninteger sequences for train and validation. A single-layer Transformer with a\nrecurrent memory buffer will be implemented, where past hidden states are\nconcatenated with current embeddings before feeding into the encoder. The EA-ACM\nmodule projects memory states to keys and current outputs to queries to compute\naverage attention weights per token and uses this as an importance score.\nWhenever the buffer exceeds the prescribed maximum length, we adaptively\ncompress it by keeping the top-K tokens with highest attention-based importance\nscores, and compute the Entropy Preservation Ratio (EPR) as the fraction of\ntotal attention weight retained. We train the network with cross-entropy loss,\nupdating weights with Adam, and log both training/validation loss and EPR at\neach epoch. Predictions and ground-truth sequences for the validation set are\nrecorded for qualitative inspection, and all losses, metrics, predictions, and\ntargets are stored in an experiment_data dictionary. At the end of training, we\nsave this dictionary to disk as a numpy file in the ./working directory. This\nbaseline confirms that our entropy-aware compression is functional and yields\ninterpretable retention metrics before scaling to real benchmarks.", "We propose a simple Transformer-XL style baseline with a memory compression\nmechanism guided by token\u2010level self\u2010attention entropy. We generate a synthetic\ndataset of random integer sequences for next\u2010token prediction and split it into\ntraining and validation sets. Our model uses a single memory\u2010aware Transformer\nlayer that concatenates past token embeddings to current inputs, computes\nattention distributions, and derives entropy scores per token. After each chunk\nof tokens, we update memory by preserving the top\u2010entropy tokens up to a fixed\ncapacity, and we track the Entropy Preservation Ratio. Training optimizes\ncross\u2010entropy loss over chunks of sequence, and we evaluate loss and average\nentropy preservation on both splits each epoch. We store per\u2010epoch train/val\nlosses and preservation ratios, then save them in numpy format. Finally, we\nautoregressively generate one validation sequence, compare predicted vs ground\ntruth token IDs, and plot the results for visualization in the working\ndirectory. All stages of data preparation, model training, evaluation, and\nvisualization are implemented in a single self\u2010contained script that writes\noutputs to ./working.", "We generate synthetic sequences of tokens and split into train/validation sets.\nA small TransformerLM model is defined with an embedding layer, positional\nembeddings, a single MultiHeadAttention block (returning per-head weights) and a\nfeed-forward head. During each forward pass we concatenate an adaptive memory\nbuffer (capped at a maximum length) with the current token segment, compute\nnext-token logits and extract attention weights. We then compute per-token\nnovelty as the entropy of its attention distribution across keys, compress\nmemory by selecting the highest-entropy tokens up to the budget, and optimize\ncross-entropy loss on the new segment. We detach memory between segments to\navoid backprop through long histories. Validation follows the same process\nwithout gradient updates, and we track both validation loss and an Entropy\nPreservation Ratio (sum of entropies of retained tokens over sum of all\nentropies). All losses and metrics are logged per epoch, stored in\n`experiment_data` and saved as a numpy file at the end.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, size, seq_len, vocab_size):\n        self.size = size\n        self.seq_len = seq_len\n        self.vocab_size = vocab_size\n\n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        seq = torch.randint(0, self.vocab_size, (self.seq_len + 1,))\n        return {\"input\": seq[:-1], \"target\": seq[1:]}\n\n\ntrain_set = SyntheticDataset(1000, 20, 100)\nval_set = SyntheticDataset(200, 20, 100)\ntrain_loader = DataLoader(train_set, batch_size=16, shuffle=False)\nval_loader = DataLoader(val_set, batch_size=16, shuffle=False)\n\n\nclass EAACMTransformer(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size, compress_size):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.max_mem = mem_size\n        self.compress_size = compress_size\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.pos_embed = nn.Embedding(500, embed_dim)\n        layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n        self.transformer = nn.TransformerEncoder(layer, num_layers=1)\n        self.decoder = nn.Linear(embed_dim, vocab_size)\n        self.memory = None\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n\n    def init_memory(self, batch_size):\n        self.memory = torch.zeros(batch_size, 0, self.embed_dim, device=device)\n\n    def forward(self, x):\n        b, L = x.shape\n        positions = torch.arange(L, device=x.device).unsqueeze(0).expand(b, L)\n        x_embed = self.embed(x) + self.pos_embed(positions)\n        if self.memory is None:\n            self.init_memory(b)\n        mem_old = self.memory\n        old_m = mem_old.size(1)\n        combo = torch.cat([mem_old, x_embed], dim=1)\n        seq = combo.transpose(0, 1)\n        out = self.transformer(seq).transpose(0, 1)\n        out_cur = out[:, old_m:, :]\n        logits = self.decoder(out_cur)\n        new_mem = torch.cat([mem_old, out_cur.detach()], dim=1)\n        if new_mem.size(1) > self.max_mem:\n            new_mem, epr = self.compress_memory(new_mem, old_m)\n        else:\n            epr = 1.0\n        self.memory = new_mem[:, -self.max_mem :, :].detach()\n        self.latest_epr = epr\n        return logits\n\n    def compress_memory(self, mem, old_m):\n        b, m_new, d = mem.size()\n        L = m_new - old_m\n        keys = self.k_proj(mem.reshape(-1, d)).reshape(b, m_new, d)\n        queries = self.q_proj(mem[:, old_m:, :].reshape(-1, d)).reshape(b, L, d)\n        scores = torch.matmul(queries, keys.transpose(-2, -1)) / (d**0.5)\n        attn = torch.softmax(scores, dim=-1)\n        w = attn.mean(dim=(0, 1))\n        k_sel = torch.topk(w, self.compress_size)[1]\n        k_sorted, _ = torch.sort(k_sel)\n        new_mem = mem[:, k_sorted, :]\n        epr = w[k_sorted].sum() / w.sum()\n        return new_mem, epr.item()\n\n\n# Hyperparameters\nmodel = EAACMTransformer(\n    vocab_size=100, embed_dim=64, num_heads=4, mem_size=50, compress_size=25\n).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    train_loss, train_epr = 0.0, 0.0\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        logits = model(batch[\"input\"])\n        loss = criterion(logits.view(-1, logits.size(-1)), batch[\"target\"].view(-1))\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        train_epr += model.latest_epr\n    train_loss /= len(train_loader)\n    train_epr /= len(train_loader)\n\n    model.eval()\n    val_loss, val_epr = 0.0, 0.0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch[\"input\"])\n            loss = criterion(logits.view(-1, logits.size(-1)), batch[\"target\"].view(-1))\n            val_loss += loss.item()\n            val_epr += model.latest_epr\n            preds = logits.argmax(dim=-1).cpu().numpy()\n            gts = batch[\"target\"].cpu().numpy()\n        val_loss /= len(val_loader)\n        val_epr /= len(val_loader)\n\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(train_epr)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(val_epr)\n    experiment_data[\"synthetic\"][\"predictions\"].append(preds)\n    experiment_data[\"synthetic\"][\"ground_truth\"].append(gts)\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic dataset\nclass RandomSeqDataset(Dataset):\n    def __init__(self, num_seqs, total_len, vocab_size):\n        self.data = torch.randint(\n            1, vocab_size, (num_seqs, total_len), dtype=torch.long\n        )\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        seq = self.data[idx]\n        return {\"input\": seq[:-1], \"target\": seq[1:]}\n\n\nvocab_size = 50\nseq_total = 51  # input 50, target 50\ntrain_ds = RandomSeqDataset(200, seq_total, vocab_size)\nval_ds = RandomSeqDataset(50, seq_total, vocab_size)\ntrain_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=1)\n\n\n# Model with entropy\u2010aware memory\nclass MemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        if mem_x is None:\n            k = v = x\n        else:\n            k = torch.cat([mem_x.unsqueeze(0), x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        # compute entropy per query token\n        aw = attn_w.mean(dim=1)[0]  # (tgt_len, src_len)\n        eps = 1e-10\n        ent = -(aw * (aw + eps).log()).sum(dim=-1)  # (tgt_len,)\n        ent = ent.detach()\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent], dim=0)\n        total_ent = mem_ent_new.sum().item() + eps\n        if mem_x_new.size(0) > self.mem_size:\n            idx = torch.topk(mem_ent_new, self.mem_size).indices\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = total_ent / total_ent\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass TransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = MemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# Setup\nmodel = TransformerXLModel(vocab_size, embed_dim=64, num_heads=2, mem_size=20).to(\n    device\n)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# Training and evaluation\nnum_epochs = 3\nchunk_size = 10\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss, train_ratios = 0.0, []\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        inp, tgt = batch[\"input\"], batch[\"target\"]\n        mem_x = mem_ent = None\n        optimizer.zero_grad()\n        loss_accum = 0.0\n        for i in range(0, inp.size(1), chunk_size):\n            ic = inp[:, i : i + chunk_size]\n            tc = tgt[:, i : i + chunk_size]\n            logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n            loss = criterion(logits.reshape(-1, vocab_size), tc.reshape(-1))\n            loss_accum += loss\n            train_ratios.append(ratio)\n        loss_accum.backward()\n        optimizer.step()\n        train_loss += loss_accum.item() / (inp.size(1) / chunk_size)\n    avg_train_loss = train_loss / len(train_loader)\n    avg_train_ratio = sum(train_ratios) / len(train_ratios)\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(avg_train_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(avg_train_ratio)\n\n    model.eval()\n    val_loss, val_ratios = 0.0, []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            inp, tgt = batch[\"input\"], batch[\"target\"]\n            mem_x = mem_ent = None\n            loss_acc = 0.0\n            for i in range(0, inp.size(1), chunk_size):\n                ic = inp[:, i : i + chunk_size]\n                tc = tgt[:, i : i + chunk_size]\n                logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                loss_acc += criterion(logits.reshape(-1, vocab_size), tc.reshape(-1))\n                val_ratios.append(ratio)\n            val_loss += loss_acc.item() / (inp.size(1) / chunk_size)\n    avg_val_loss = val_loss / len(val_loader)\n    avg_val_ratio = sum(val_ratios) / len(val_ratios)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(avg_val_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(avg_val_ratio)\n    print(f\"Epoch {epoch}: validation_loss = {avg_val_loss:.4f}\")\n\n# Generate and visualize\nmodel.eval()\nwith torch.no_grad():\n    sample = val_ds[0][\"input\"].unsqueeze(0).to(device)\n    target = val_ds[0][\"target\"].tolist()\n    mem_x = mem_ent = None\n    preds = []\n    for t in range(sample.size(1)):\n        inp = sample[:, t].unsqueeze(1)\n        logits, mem_x, mem_ent, _ = model(inp, mem_x, mem_ent)\n        p = torch.softmax(logits.squeeze(0).squeeze(0), dim=-1)\n        preds.append(int(p.argmax().item()))\n    experiment_data[\"synthetic\"][\"predictions\"] = preds\n    experiment_data[\"synthetic\"][\"ground_truth\"] = target\n\nplt.figure()\nplt.plot(range(len(preds)), preds, label=\"predicted\")\nplt.plot(range(len(target)), target, label=\"ground_truth\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"synthetic_generation.png\"))\nplt.close()\n\n# Save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nimport math\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import Dataset\n\n# device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# synthetic dataset\nclass SyntheticLM(Dataset):\n    def __init__(self, num_seqs, seq_len, vocab_size):\n        self.data = torch.randint(0, vocab_size, (num_seqs, seq_len), dtype=torch.long)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n\n# transformer block with attention weight output\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=512):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=False)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.ff = nn.Sequential(\n            nn.Linear(d_model, dim_feedforward),\n            nn.ReLU(),\n            nn.Linear(dim_feedforward, d_model),\n        )\n        self.norm2 = nn.LayerNorm(d_model)\n        self.nhead = nhead\n\n    def forward(self, x):\n        attn_out, attn_w = self.attn(\n            x, x, x, need_weights=True, average_attn_weights=False\n        )\n        x = self.norm1(x + attn_out)\n        x2 = self.ff(x)\n        x = self.norm2(x + x2)\n        return x, attn_w\n\n\n# full model\nclass TransformerLM(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, max_seq_len, dim_feedforward):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n        self.block = TransformerBlock(d_model, nhead, dim_feedforward)\n        self.out = nn.Linear(d_model, vocab_size)\n        self.d_model = d_model\n        self.nhead = nhead\n\n    def forward(self, input_ids):\n        # input_ids: [batch, seq_total]\n        b, seq_total = input_ids.shape\n        x = self.embed(input_ids) * math.sqrt(self.d_model)\n        pos = (\n            torch.arange(seq_total, device=input_ids.device)\n            .unsqueeze(0)\n            .expand(b, seq_total)\n        )\n        x = x + self.pos_embed(pos)\n        x = x.permute(1, 0, 2)  # [seq_total, batch, d_model]\n        x, attn_w = self.block(x)\n        logits = self.out(x).permute(1, 0, 2)  # [batch, seq_total, vocab]\n        return logits, attn_w\n\n\n# hyperparameters\nvocab_size = 1000\nd_model = 128\nnhead = 8\nff_dim = 512\nseq_len = 200\nseg_len = 50\nmem_max_len = 100\nmax_seq_len = seg_len + mem_max_len\nlr = 1e-3\nepochs = 3\n\n# data\ntrain_ds = SyntheticLM(800, seq_len, vocab_size)\nval_ds = SyntheticLM(200, seq_len, vocab_size)\n\n# model, loss, optimizer\nmodel = TransformerLM(vocab_size, d_model, nhead, max_seq_len, ff_dim).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n# experiment data store\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# training/validation loops\nfor epoch in range(1, epochs + 1):\n    # training\n    model.train()\n    train_loss_sum = 0.0\n    train_ratio_sum = 0.0\n    train_count = 0\n    for seq in train_ds:\n        seq = seq.to(device).unsqueeze(0)\n        memory_ids = torch.tensor([], dtype=torch.long, device=device).unsqueeze(0)\n        num_segs = seq_len // seg_len\n        for i in range(num_segs):\n            # segment and concat memory\n            seg = seq[:, i * seg_len : (i + 1) * seg_len]\n            inp = torch.cat([memory_ids, seg], dim=1)\n            logits, attn_w = model(inp)\n            # LM loss on new segment\n            if seg_len > 1:\n                lm_logits = logits[:, memory_ids.size(1) : -1, :].reshape(\n                    -1, vocab_size\n                )\n                targets = seg[:, 1:].reshape(-1)\n                loss = criterion(lm_logits, targets)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                train_loss_sum += loss.item()\n            # compute entropy novelty\n            B = attn_w.shape[0] // nhead\n            T = inp.size(1)\n            attn = attn_w.view(B, nhead, T, T)[0]  # [heads, T, T]\n            entropy = -(attn * (attn + 1e-8).log()).sum(dim=2)  # [heads, T]\n            novelty = entropy.mean(dim=0)  # [T]\n            total_entropy = novelty.sum().item()\n            # compress memory\n            if T > mem_max_len:\n                vals, idxs = novelty.topk(mem_max_len)\n                keep = idxs.sort().values\n            else:\n                keep = torch.arange(T, device=device)\n            kept = novelty[keep]\n            ratio = kept.sum().item() / (total_entropy + 1e-8)\n            train_ratio_sum += ratio\n            train_count += 1\n            memory_ids = inp[:, keep]\n    train_loss = train_loss_sum / max(train_count, 1)\n    train_ratio = train_ratio_sum / max(train_count, 1)\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(train_ratio)\n\n    # validation\n    model.eval()\n    val_loss_sum = 0.0\n    val_ratio_sum = 0.0\n    val_count = 0\n    with torch.no_grad():\n        for seq in val_ds:\n            seq = seq.to(device).unsqueeze(0)\n            memory_ids = torch.tensor([], dtype=torch.long, device=device).unsqueeze(0)\n            for i in range(seq_len // seg_len):\n                seg = seq[:, i * seg_len : (i + 1) * seg_len]\n                inp = torch.cat([memory_ids, seg], dim=1)\n                logits, attn_w = model(inp)\n                if seg_len > 1:\n                    lm_logits = logits[:, memory_ids.size(1) : -1, :].reshape(\n                        -1, vocab_size\n                    )\n                    targets = seg[:, 1:].reshape(-1)\n                    loss = criterion(lm_logits, targets)\n                    val_loss_sum += loss.item()\n                B = attn_w.shape[0] // nhead\n                T = inp.size(1)\n                attn = attn_w.view(B, nhead, T, T)[0]\n                entropy = -(attn * (attn + 1e-8).log()).sum(dim=2)\n                novelty = entropy.mean(dim=0)\n                total_entropy = novelty.sum().item()\n                if T > mem_max_len:\n                    vals, idxs = novelty.topk(mem_max_len)\n                    keep = idxs.sort().values\n                else:\n                    keep = torch.arange(T, device=device)\n                kept = novelty[keep]\n                ratio = kept.sum().item() / (total_entropy + 1e-8)\n                val_ratio_sum += ratio\n                val_count += 1\n                memory_ids = inp[:, keep]\n    val_loss = val_loss_sum / max(val_count, 1)\n    val_ratio = val_ratio_sum / max(val_count, 1)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(val_ratio)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic dataset\nclass RandomSeqDataset(Dataset):\n    def __init__(self, num_seqs, total_len, vocab_size):\n        self.data = torch.randint(\n            1, vocab_size, (num_seqs, total_len), dtype=torch.long\n        )\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        seq = self.data[idx]\n        return {\"input\": seq[:-1], \"target\": seq[1:]}\n\n\nvocab_size = 50\nseq_total = 51  # input 50, target 50\ntrain_ds = RandomSeqDataset(200, seq_total, vocab_size)\nval_ds = RandomSeqDataset(50, seq_total, vocab_size)\ntrain_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=1)\n\n\n# Model with entropy\u2010aware memory\nclass MemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        if mem_x is None:\n            k = v = x\n        else:\n            k = torch.cat([mem_x.unsqueeze(0), x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        # compute entropy per query token\n        aw = attn_w.mean(dim=1)[0]  # (tgt_len, src_len)\n        eps = 1e-10\n        ent = -(aw * (aw + eps).log()).sum(dim=-1)  # (tgt_len,)\n        ent = ent.detach()\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent], dim=0)\n        total_ent = mem_ent_new.sum().item() + eps\n        if mem_x_new.size(0) > self.mem_size:\n            idx = torch.topk(mem_ent_new, self.mem_size).indices\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = total_ent / total_ent\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass TransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = MemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# Setup\nmodel = TransformerXLModel(vocab_size, embed_dim=64, num_heads=2, mem_size=20).to(\n    device\n)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# Training and evaluation\nnum_epochs = 3\nchunk_size = 10\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss, train_ratios = 0.0, []\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        inp, tgt = batch[\"input\"], batch[\"target\"]\n        mem_x = mem_ent = None\n        optimizer.zero_grad()\n        loss_accum = 0.0\n        for i in range(0, inp.size(1), chunk_size):\n            ic = inp[:, i : i + chunk_size]\n            tc = tgt[:, i : i + chunk_size]\n            logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n            loss = criterion(logits.reshape(-1, vocab_size), tc.reshape(-1))\n            loss_accum += loss\n            train_ratios.append(ratio)\n        loss_accum.backward()\n        optimizer.step()\n        train_loss += loss_accum.item() / (inp.size(1) / chunk_size)\n    avg_train_loss = train_loss / len(train_loader)\n    avg_train_ratio = sum(train_ratios) / len(train_ratios)\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(avg_train_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(avg_train_ratio)\n\n    model.eval()\n    val_loss, val_ratios = 0.0, []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            inp, tgt = batch[\"input\"], batch[\"target\"]\n            mem_x = mem_ent = None\n            loss_acc = 0.0\n            for i in range(0, inp.size(1), chunk_size):\n                ic = inp[:, i : i + chunk_size]\n                tc = tgt[:, i : i + chunk_size]\n                logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                loss_acc += criterion(logits.reshape(-1, vocab_size), tc.reshape(-1))\n                val_ratios.append(ratio)\n            val_loss += loss_acc.item() / (inp.size(1) / chunk_size)\n    avg_val_loss = val_loss / len(val_loader)\n    avg_val_ratio = sum(val_ratios) / len(val_ratios)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(avg_val_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(avg_val_ratio)\n    print(f\"Epoch {epoch}: validation_loss = {avg_val_loss:.4f}\")\n\n# Generate and visualize\nmodel.eval()\nwith torch.no_grad():\n    sample = val_ds[0][\"input\"].unsqueeze(0).to(device)\n    target = val_ds[0][\"target\"].tolist()\n    mem_x = mem_ent = None\n    preds = []\n    for t in range(sample.size(1)):\n        inp = sample[:, t].unsqueeze(1)\n        logits, mem_x, mem_ent, _ = model(inp, mem_x, mem_ent)\n        p = torch.softmax(logits.squeeze(0).squeeze(0), dim=-1)\n        preds.append(int(p.argmax().item()))\n    experiment_data[\"synthetic\"][\"predictions\"] = preds\n    experiment_data[\"synthetic\"][\"ground_truth\"] = target\n\nplt.figure()\nplt.plot(range(len(preds)), preds, label=\"predicted\")\nplt.plot(range(len(target)), target, label=\"ground_truth\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"synthetic_generation.png\"))\nplt.close()\n\n# Save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic dataset\nclass RandomSeqDataset(Dataset):\n    def __init__(self, num_seqs, total_len, vocab_size):\n        self.data = torch.randint(\n            1, vocab_size, (num_seqs, total_len), dtype=torch.long\n        )\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        seq = self.data[idx]\n        return {\"input\": seq[:-1], \"target\": seq[1:]}\n\n\nvocab_size = 50\nseq_total = 51  # input 50, target 50\ntrain_ds = RandomSeqDataset(200, seq_total, vocab_size)\nval_ds = RandomSeqDataset(50, seq_total, vocab_size)\ntrain_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=1)\n\n\n# Model with entropy\u2010aware memory\nclass MemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        if mem_x is None:\n            k = v = x\n        else:\n            k = torch.cat([mem_x.unsqueeze(0), x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        # compute entropy per query token\n        aw = attn_w.mean(dim=1)[0]  # (tgt_len, src_len)\n        eps = 1e-10\n        ent = -(aw * (aw + eps).log()).sum(dim=-1)  # (tgt_len,)\n        ent = ent.detach()\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent], dim=0)\n        total_ent = mem_ent_new.sum().item() + eps\n        if mem_x_new.size(0) > self.mem_size:\n            idx = torch.topk(mem_ent_new, self.mem_size).indices\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = total_ent / total_ent\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass TransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = MemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# Setup\nmodel = TransformerXLModel(vocab_size, embed_dim=64, num_heads=2, mem_size=20).to(\n    device\n)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# Training and evaluation\nnum_epochs = 3\nchunk_size = 10\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss, train_ratios = 0.0, []\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        inp, tgt = batch[\"input\"], batch[\"target\"]\n        mem_x = mem_ent = None\n        optimizer.zero_grad()\n        loss_accum = 0.0\n        for i in range(0, inp.size(1), chunk_size):\n            ic = inp[:, i : i + chunk_size]\n            tc = tgt[:, i : i + chunk_size]\n            logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n            loss = criterion(logits.reshape(-1, vocab_size), tc.reshape(-1))\n            loss_accum += loss\n            train_ratios.append(ratio)\n        loss_accum.backward()\n        optimizer.step()\n        train_loss += loss_accum.item() / (inp.size(1) / chunk_size)\n    avg_train_loss = train_loss / len(train_loader)\n    avg_train_ratio = sum(train_ratios) / len(train_ratios)\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(avg_train_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(avg_train_ratio)\n\n    model.eval()\n    val_loss, val_ratios = 0.0, []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            inp, tgt = batch[\"input\"], batch[\"target\"]\n            mem_x = mem_ent = None\n            loss_acc = 0.0\n            for i in range(0, inp.size(1), chunk_size):\n                ic = inp[:, i : i + chunk_size]\n                tc = tgt[:, i : i + chunk_size]\n                logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                loss_acc += criterion(logits.reshape(-1, vocab_size), tc.reshape(-1))\n                val_ratios.append(ratio)\n            val_loss += loss_acc.item() / (inp.size(1) / chunk_size)\n    avg_val_loss = val_loss / len(val_loader)\n    avg_val_ratio = sum(val_ratios) / len(val_ratios)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(avg_val_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(avg_val_ratio)\n    print(f\"Epoch {epoch}: validation_loss = {avg_val_loss:.4f}\")\n\n# Generate and visualize\nmodel.eval()\nwith torch.no_grad():\n    sample = val_ds[0][\"input\"].unsqueeze(0).to(device)\n    target = val_ds[0][\"target\"].tolist()\n    mem_x = mem_ent = None\n    preds = []\n    for t in range(sample.size(1)):\n        inp = sample[:, t].unsqueeze(1)\n        logits, mem_x, mem_ent, _ = model(inp, mem_x, mem_ent)\n        p = torch.softmax(logits.squeeze(0).squeeze(0), dim=-1)\n        preds.append(int(p.argmax().item()))\n    experiment_data[\"synthetic\"][\"predictions\"] = preds\n    experiment_data[\"synthetic\"][\"ground_truth\"] = target\n\nplt.figure()\nplt.plot(range(len(preds)), preds, label=\"predicted\")\nplt.plot(range(len(target)), target, label=\"ground_truth\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"synthetic_generation.png\"))\nplt.close()\n\n# Save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic dataset\nclass RandomSeqDataset(Dataset):\n    def __init__(self, num_seqs, total_len, vocab_size):\n        self.data = torch.randint(\n            1, vocab_size, (num_seqs, total_len), dtype=torch.long\n        )\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        seq = self.data[idx]\n        return {\"input\": seq[:-1], \"target\": seq[1:]}\n\n\nvocab_size = 50\nseq_total = 51  # input 50, target 50\ntrain_ds = RandomSeqDataset(200, seq_total, vocab_size)\nval_ds = RandomSeqDataset(50, seq_total, vocab_size)\ntrain_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=1)\n\n\n# Model with entropy\u2010aware memory\nclass MemoryTransformerLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mem_size = mem_size\n\n    def forward(self, x, mem_x, mem_ent):\n        if mem_x is None:\n            k = v = x\n        else:\n            k = torch.cat([mem_x.unsqueeze(0), x], dim=1)\n            v = k\n        attn_out, attn_w = self.attn(\n            x, k, v, need_weights=True, average_attn_weights=False\n        )\n        x2 = self.norm1(x + attn_out)\n        out = self.norm2(x2 + self.ff(x2))\n        # compute entropy per query token\n        aw = attn_w.mean(dim=1)[0]  # (tgt_len, src_len)\n        eps = 1e-10\n        ent = -(aw * (aw + eps).log()).sum(dim=-1)  # (tgt_len,)\n        ent = ent.detach()\n        x_det = x.detach()[0]\n        if mem_x is None:\n            mem_x_new = x_det\n            mem_ent_new = ent\n        else:\n            mem_x_new = torch.cat([mem_x, x_det], dim=0)\n            mem_ent_new = torch.cat([mem_ent, ent], dim=0)\n        total_ent = mem_ent_new.sum().item() + eps\n        if mem_x_new.size(0) > self.mem_size:\n            idx = torch.topk(mem_ent_new, self.mem_size).indices\n            kept_ent = mem_ent_new[idx].sum().item()\n            ratio = kept_ent / total_ent\n            mem_x_new = mem_x_new[idx]\n            mem_ent_new = mem_ent_new[idx]\n        else:\n            ratio = total_ent / total_ent\n        return out, mem_x_new, mem_ent_new, ratio\n\n\nclass TransformerXLModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, mem_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.mem_layer = MemoryTransformerLayer(embed_dim, num_heads, mem_size)\n        self.out = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x, mem_x, mem_ent):\n        emb = self.embed(x)\n        out, mem_x_new, mem_ent_new, ratio = self.mem_layer(emb, mem_x, mem_ent)\n        logits = self.out(out)\n        return logits, mem_x_new, mem_ent_new, ratio\n\n\n# Setup\nmodel = TransformerXLModel(vocab_size, embed_dim=64, num_heads=2, mem_size=20).to(\n    device\n)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# Training and evaluation\nnum_epochs = 3\nchunk_size = 10\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss, train_ratios = 0.0, []\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        inp, tgt = batch[\"input\"], batch[\"target\"]\n        mem_x = mem_ent = None\n        optimizer.zero_grad()\n        loss_accum = 0.0\n        for i in range(0, inp.size(1), chunk_size):\n            ic = inp[:, i : i + chunk_size]\n            tc = tgt[:, i : i + chunk_size]\n            logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n            loss = criterion(logits.reshape(-1, vocab_size), tc.reshape(-1))\n            loss_accum += loss\n            train_ratios.append(ratio)\n        loss_accum.backward()\n        optimizer.step()\n        train_loss += loss_accum.item() / (inp.size(1) / chunk_size)\n    avg_train_loss = train_loss / len(train_loader)\n    avg_train_ratio = sum(train_ratios) / len(train_ratios)\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(avg_train_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(avg_train_ratio)\n\n    model.eval()\n    val_loss, val_ratios = 0.0, []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            inp, tgt = batch[\"input\"], batch[\"target\"]\n            mem_x = mem_ent = None\n            loss_acc = 0.0\n            for i in range(0, inp.size(1), chunk_size):\n                ic = inp[:, i : i + chunk_size]\n                tc = tgt[:, i : i + chunk_size]\n                logits, mem_x, mem_ent, ratio = model(ic, mem_x, mem_ent)\n                loss_acc += criterion(logits.reshape(-1, vocab_size), tc.reshape(-1))\n                val_ratios.append(ratio)\n            val_loss += loss_acc.item() / (inp.size(1) / chunk_size)\n    avg_val_loss = val_loss / len(val_loader)\n    avg_val_ratio = sum(val_ratios) / len(val_ratios)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(avg_val_loss)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(avg_val_ratio)\n    print(f\"Epoch {epoch}: validation_loss = {avg_val_loss:.4f}\")\n\n# Generate and visualize\nmodel.eval()\nwith torch.no_grad():\n    sample = val_ds[0][\"input\"].unsqueeze(0).to(device)\n    target = val_ds[0][\"target\"].tolist()\n    mem_x = mem_ent = None\n    preds = []\n    for t in range(sample.size(1)):\n        inp = sample[:, t].unsqueeze(1)\n        logits, mem_x, mem_ent, _ = model(inp, mem_x, mem_ent)\n        p = torch.softmax(logits.squeeze(0).squeeze(0), dim=-1)\n        preds.append(int(p.argmax().item()))\n    experiment_data[\"synthetic\"][\"predictions\"] = preds\n    experiment_data[\"synthetic\"][\"ground_truth\"] = target\n\nplt.figure()\nplt.plot(range(len(preds)), preds, label=\"predicted\")\nplt.plot(range(len(target)), target, label=\"ground_truth\")\nplt.legend()\nplt.savefig(os.path.join(working_dir, \"synthetic_generation.png\"))\nplt.close()\n\n# Save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 113, in <module>\\n    logits = model(batch[\"input\"])\\n\n^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"runfile.py\", line 61, in forward\\n    combo = torch.cat([mem_old, x_embed],\ndim=1)\\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: Sizes of\ntensors must match except in dimension 1. Expected size 16 but got size 8 for\ntensor number 1 in the list.\\n', 'Execution time: 2 seconds seconds (time limit\nis an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 3.8777', '\\n', 'Epoch\n1: validation_loss = 3.8316', '\\n', 'Epoch 2: validation_loss = 3.7955', '\\n',\n'Execution time: 18 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 140, in <module>\\n    attn = attn_w.view(B, nhead, T, T)[0]\n# [heads, T, T]\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: shape\n\\'[0, 8, 50, 50]\\' is invalid for input of size 20000\\n', 'Execution time: a\nsecond seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 3.8989', '\\n', 'Epoch\n1: validation_loss = 3.8453', '\\n', 'Epoch 2: validation_loss = 3.7947', '\\n',\n'Execution time: 18 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 3.8975', '\\n', 'Epoch\n1: validation_loss = 3.8438', '\\n', 'Epoch 2: validation_loss = 3.8039', '\\n',\n'Execution time: 18 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 3.8975', '\\n', 'Epoch\n1: validation_loss = 3.8438', '\\n', 'Epoch 2: validation_loss = 3.8039', '\\n',\n'Execution time: 18 seconds seconds (time limit is an hour).']", ""], "analysis": ["The model keeps a persistent memory tensor across batches but does not handle\nvarying batch sizes. During validation, the final batch has size 8 (instead of\n16), so when forwarding this smaller batch the code attempts to concatenate a\nmemory of shape (16, \u2026) with inputs of shape (8, \u2026), causing a dimension\nmismatch and runtime error. To fix this, reset or reinitialize the memory at the\nstart of each batch (e.g., call init_memory(batch_size) before processing) or\nadd logic in forward() to reinitialize memory whenever the input batch size\nchanges. Alternatively, force uniform batch sizes (e.g., use drop_last=True) to\navoid mismatches.", "", "The code incorrectly assumes that attn_w returned by nn.MultiheadAttention has\nshape [batch * nhead, T, T], and thus does attn_w.view(B, nhead, T, T). However,\nin the current PyTorch version, with average_attn_weights=False, attn_w has\nshape [batch, nhead, T, T], so attn_w.shape[0]//nhead becomes 0 and the view\ncall fails with an invalid reshape ([0, 8, 50, 50]). To fix this, the code\nshould inspect attn_w.dim(): if it's 4D ([batch, nhead, T, T]), simply index\nattn = attn_w[0]; if it's 3D ([batch * nhead, T, T]), then reshape with view(B,\nnhead, T, T) as intended.", "", "", "", ""], "exc_type": ["RuntimeError", null, "RuntimeError", null, null, null, null], "exc_info": [{"args": ["Sizes of tensors must match except in dimension 1. Expected size 16 but got size 8 for tensor number 1 in the list."]}, null, {"args": ["shape '[0, 8, 50, 50]' is invalid for input of size 20000"]}, null, null, null, null], "exc_stack": [[["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 113, "<module>", "logits = model(batch[\"input\"])"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["runfile.py", 61, "forward", "combo = torch.cat([mem_old, x_embed], dim=1)"]], null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 140, "<module>", "attn = attn_w.view(B, nhead, T, T)[0]  # [heads, T, T]"]], null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training memory retention ratio", "lower_is_better": false, "description": "Final memory retention ratio on the training dataset.", "data": [{"dataset_name": "synthetic", "final_value": 0.8214380449300438, "best_value": 0.8214380449300438}]}, {"metric_name": "validation memory retention ratio", "lower_is_better": false, "description": "Final memory retention ratio on the validation dataset.", "data": [{"dataset_name": "synthetic", "final_value": 0.8210265552539907, "best_value": 0.8210265552539907}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Final loss on the training dataset.", "data": [{"dataset_name": "synthetic", "final_value": 3.6700600471496587, "best_value": 3.6700600471496587}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final loss on the validation dataset.", "data": [{"dataset_name": "synthetic", "final_value": 3.795520591735841, "best_value": 3.795520591735841}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training memory retention ratio", "lower_is_better": false, "description": "Final memory retention ratio on the training split of the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.8215399167850012, "best_value": 0.8215399167850012}]}, {"metric_name": "validation memory retention ratio", "lower_is_better": false, "description": "Final memory retention ratio on the validation split of the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.8213713723810073, "best_value": 0.8213713723810073}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Final loss on the training split of the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 3.6716410655975364, "best_value": 3.6716410655975364}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final loss on the validation split of the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 3.794670776367188, "best_value": 3.794670776367188}]}]}, {"metric_names": [{"metric_name": "training memory retention ratio", "lower_is_better": false, "description": "Ratio of memory retained by the training process", "data": [{"dataset_name": "synthetic", "final_value": 0.8215293000352482, "best_value": 0.8215293000352482}]}, {"metric_name": "validation memory retention ratio", "lower_is_better": false, "description": "Ratio of memory retained on validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.8209828211691476, "best_value": 0.8209828211691476}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss value on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 3.680085485458373, "best_value": 3.680085485458373}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss value on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 3.8039309997558592, "best_value": 3.8039309997558592}]}]}, {"metric_names": [{"metric_name": "train memory retention ratio", "lower_is_better": false, "description": "Memory retention ratio on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.8215293000352482, "best_value": 0.8215293000352482}]}, {"metric_name": "validation memory retention ratio", "lower_is_better": false, "description": "Memory retention ratio on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.8209828211691476, "best_value": 0.8209828211691476}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 3.680085485458373, "best_value": 3.680085485458373}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 3.8039309997558592, "best_value": 3.8039309997558592}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false], "plots": [[], ["../../logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_loss_curve.png", "../../logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_metric_curve.png", "../../logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_generation_comparison.png", "../../logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_generation.png"], [], ["../../logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_loss_curve.png", "../../logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_metric_curve.png", "../../logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_generation_comparison.png", "../../logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_generation.png"], ["../../logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_loss_curve.png", "../../logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_metric_curve.png", "../../logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_generation_comparison.png", "../../logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_generation.png"], ["../../logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_loss_curve.png", "../../logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_metric_curve.png", "../../logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_generation_comparison.png", "../../logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_generation.png"], ["../../logs/0-run/experiment_results/seed_aggregation_2f0dcd7ded134795b6d7a2bed0cd728f/synthetic_loss_mean_se.png", "../../logs/0-run/experiment_results/seed_aggregation_2f0dcd7ded134795b6d7a2bed0cd728f/synthetic_generation_mean_se.png", "../../logs/0-run/experiment_results/seed_aggregation_2f0dcd7ded134795b6d7a2bed0cd728f/synthetic_metric_mean_se.png"]], "plot_paths": [[], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_metric_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_generation_comparison.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_generation.png"], [], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_metric_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_generation_comparison.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_generation.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_metric_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_generation_comparison.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_generation.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_loss_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_metric_curve.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_generation_comparison.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_generation.png"], ["experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/seed_aggregation_2f0dcd7ded134795b6d7a2bed0cd728f/synthetic_loss_mean_se.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/seed_aggregation_2f0dcd7ded134795b6d7a2bed0cd728f/synthetic_generation_mean_se.png", "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/seed_aggregation_2f0dcd7ded134795b6d7a2bed0cd728f/synthetic_metric_mean_se.png"]], "plot_analyses": [[], [{"analysis": "Training loss decreases steadily from about 3.95 at epoch 1 to 3.67 at epoch 3, while validation loss also diminishes from around 3.88 to 3.80 over the same span. The gap between train and val loss narrows slightly, suggesting no overfitting at this preliminary stage and that the model is learning meaningful structure beyond a random baseline (which would be ~ln(50)\u22483.91).", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_loss_curve.png"}, {"analysis": "Memory retention ratios remain quite stable across epochs. The training retention ratio rises marginally from ~0.8207 to a peak of ~0.8223 at epoch 2, then lightly recedes to ~0.8214. Validation retention falls gently from ~0.8217 to ~0.8201. These small fluctuations imply that the current adaptive compression mechanism is preserving roughly the same fraction of tokens, with only minor gains mid-training.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_metric_curve.png"}, {"analysis": "Generated sequences display a token distribution roughly similar to ground truth: both cover the full 0\u201349 range and share similar volatility patterns. However, specific time-step alignments are inconsistent\u2014peaks and troughs in the generated plot often do not line up with those in the ground-truth sequence. This suggests that while the model captures the overall distribution, it has not yet learned to predict exact token values reliably.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_generation_comparison.png"}, {"analysis": "Overlaying predicted vs. ground-truth tokens confirms misalignments: there are many time steps where the prediction deviates markedly from the true token. Although both curves occupy similar ranges, the lack of tight correspondence indicates the model\u2019s generative performance remains weak at this stage. It is effectively sampling from a learned distribution rather than performing accurate next-token prediction.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_09cbad3e88e94bc6b23a4de2543fcc20_proc_3946600/synthetic_generation.png"}], [], [{"analysis": "Synthetic Dataset Loss Curves depict smooth, monotonic declines in both training and validation loss across three epochs. Training loss falls more steeply (from ~3.95 to ~3.67) compared to validation (from ~3.90 to ~3.80), indicating steady learning with a modest but widening generalization gap. No signs of overfitting yet; the parallel downward trend suggests the model is improving and still under capacity relative to the synthetic task.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_loss_curve.png"}, {"analysis": "Memory Retention over Epochs remains highly stable, with training ratio rising slightly from ~0.8208 to ~0.8219 at epoch 2 before a minor drop to ~0.8215 at epoch 3. Validation retention follows a gentle upward drift from ~0.8182 to ~0.8186 at epoch 2 then dips marginally by epoch 3. Fluctuations are within 0.001, reflecting that the adaptive compressive memory mechanism is maintaining a consistent information retention level without large swings.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_metric_curve.png"}, {"analysis": "Ground truth sequence shows tokens drawn uniformly across the full integer range (5\u201350) with no clear periodic structure. Generated samples capture the same value range and approximate variability but differ in the ordering and timing of peaks and troughs. This indicates the model\u2019s output distribution aligns with the data distribution, but precise sequence-level fidelity is not yet achieved.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_generation_comparison.png"}, {"analysis": "Predicted versus ground truth overlay reveals partial alignment: high-value spikes occasionally coincide, and low-value dips are sometimes matched, yet many time steps exhibit offsets in magnitude or phase. The model captures global distributional characteristics but lacks precise temporal localization. Further tuning of the entropy-aware compression or longer training may improve sequence accuracy.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/synthetic_generation.png"}], [{"analysis": "Training loss steadily decreases from about 3.96 to 3.68 over three epochs, while validation loss decreases more modestly from roughly 3.90 to 3.80. Both curves trend downward, indicating successful learning and no dramatic overfitting within the small epoch window. The small increase in train\u2013validation gap from epoch 1 (\u22480.06) to epoch 3 (\u22480.08) suggests only a mild divergence, but further monitoring of this gap across additional epochs would be prudent to ensure generalization remains robust.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_loss_curve.png"}, {"analysis": "Memory retention ratio on the training set climbs from \u22480.8205 at epoch 1 to \u22480.8219 at epoch 2, then slightly dips to \u22480.8215 at epoch 3. Validation retention follows a similar pattern, rising to \u22480.8216 at epoch 2 before falling back to \u22480.8210. This transient peak at epoch 2 implies that the entropy-aware compression is effectively preserving novel tokens early on, but may require threshold adjustments or a longer warm-up to stabilize and sustain the retention benefit.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_metric_curve.png"}, {"analysis": "Ground truth sequence exhibits high variability across 50 time steps, with tokens spanning nearly the full 0\u201350 range. Generated samples capture the global range but misalign in timing and magnitude, often holding values too long or failing to reproduce sharp transitions. Although the model reproduces some high and low regimes, it struggles with the exact temporal pattern, suggesting limited sequence fidelity under the current memory scheme.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_generation_comparison.png"}, {"analysis": "Overlaying predicted versus ground truth sequences reinforces these discrepancies: early steps show high predictions against low ground-truth tokens and vice versa, mid-sequence predictions flatten around 30\u201340 while the true signal remains erratic, and later steps exhibit both abrupt deviations and occasional alignment. The model learns the general distribution but not the precise ordering, indicating that further tuning of the compressive memory (e.g., entropy thresholds, memory budget) or extended training is needed to better retain fine-grained contextual information.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/synthetic_generation.png"}], [{"analysis": "Training loss drops from ~3.96 to ~3.68 over three epochs while validation loss decreases more modestly from ~3.90 to ~3.80, indicating stable learning without severe overfitting yet. The training curve falls faster than validation, causing a small widening of the generalization gap by epoch three. Continued monitoring beyond three epochs will clarify if this trend persists.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_loss_curve.png"}, {"analysis": "Memory retention ratio climbs slightly from ~0.8205 to ~0.8219 on training data and from ~0.8209 to ~0.8216 on validation at epoch two, then lightly declines by epoch three. The persistent ~0.001 gap between train and validation suggests the entropy-aware mechanism preserves slightly more tokens during mid-training but yields only marginal gains in this preliminary stage.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_metric_curve.png"}, {"analysis": "Generated samples capture rough token-value ranges seen in the ground truth but exhibit temporal misalignments and missing valleys. Peaks in the synthetic sequence sometimes align, yet there is a noticeable shift in time steps and underrepresentation of lower token values. This points to an initial ability to mimic global distribution but limited local coherence.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_generation_comparison.png"}, {"analysis": "Direct comparison of predicted versus ground-truth sequences reveals intermittent correlation: certain mid-sequence peaks are tracked, but large segments show flat or plateaued predictions that fail to follow the rapid fluctuations of the ground truth. The model reproduces coarse patterns but struggles with token-level variability, indicating room for improving attention entropy thresholds or training dynamics.", "plot_path": "experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/synthetic_generation.png"}], []], "vlm_feedback_summary": ["[]", "Loss curves show consistent learning beyond the random baseline, but memory\nretention exhibits only marginal improvements. Generated samples match the\nmarginal token distribution but fail to track the true sequence precisely, as\nconfirmed by the overlay. Overall, the basic implementation works: loss declines\nand memory compression remains stable, yet sequence generation accuracy is low.\nFurther work should focus on strengthening token-level novelty signals and\nensuring they guide retention to improve exact next-token predictions.", "[]", "Training shows solid loss reduction and stable memory retention. Generated\noutputs match overall token distribution but require improved temporal fidelity\nrelative to ground truth. Adaptive memory appears stable but may need stronger\nsignals or longer training to sharpen sequence-level predictions.", "Loss curves confirm basic convergence with a small and stable train\u2013validation\ngap. Memory retention peaks at epoch 2, demonstrating the entropy-aware\nmechanism\u2019s initial effectiveness but also its slight instability afterward.\nGenerative outputs reveal that while the model captures coarse token\ndistributions, it fails to reproduce the exact temporal dynamics. Suggested next\nsteps include extending training, refining the entropy-based compression\nthreshold, and potentially increasing memory budget or warm-up duration to\nreinforce precise long-range sequence retention.", "Training and validation losses decrease smoothly with no immediate overfitting;\nmemory retention sees modest mid-training gain; generation quality shows global\npattern matching but poor local alignment; next steps include longer training,\nhyperparameter tuning, and refinement of the entropy-based compression criteria.", "[]"], "exec_time": [2.4083096981048584, 18.679852724075317, 1.7737486362457275, 18.812530040740967, 18.808860301971436, 18.805821657180786, null], "exec_time_feedback": ["", "", "", "", "", "", ""], "datasets_successfully_tested": [[], ["[synthetic]"], [], ["[synthetic]"], ["[]"], ["['synthetic']"], []], "plot_code": [null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nlosses = experiment_data.get(\"synthetic\", {}).get(\"losses\", {})\nmetrics = experiment_data.get(\"synthetic\", {}).get(\"metrics\", {})\npreds = experiment_data.get(\"synthetic\", {}).get(\"predictions\", [])\ngt = experiment_data.get(\"synthetic\", {}).get(\"ground_truth\", [])\n\nprint(\"Final Train Loss:\", losses.get(\"train\", []))\nprint(\"Final Val Loss:\", losses.get(\"val\", []))\nprint(\"Final Train Metric:\", metrics.get(\"train\", []))\nprint(\"Final Val Metric:\", metrics.get(\"val\", []))\n\ntry:\n    plt.figure()\n    tl = losses.get(\"train\", [])\n    vl = losses.get(\"val\", [])\n    epochs = range(1, len(tl) + 1)\n    plt.plot(epochs, tl, label=\"Train Loss\")\n    plt.plot(epochs, vl, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Synthetic Dataset Loss Curves\\nTraining vs Validation Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    tm = metrics.get(\"train\", [])\n    vm = metrics.get(\"val\", [])\n    epochs = range(1, len(tm) + 1)\n    plt.plot(epochs, tm, label=\"Train Ratio\")\n    plt.plot(epochs, vm, label=\"Val Ratio\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Memory Retention Ratio\")\n    plt.title(\"Synthetic Dataset Metric Curves\\nMemory Retention over Epochs\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_metric_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating metric curve: {e}\")\n    plt.close()\n\ntry:\n    if preds and gt:\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        axs[0].plot(range(len(gt)), gt)\n        axs[0].set_title(\"Ground Truth Sequence\")\n        axs[0].set_xlabel(\"Time Step\")\n        axs[0].set_ylabel(\"Token\")\n        axs[1].plot(range(len(preds)), preds, color=\"orange\")\n        axs[1].set_title(\"Generated Samples\")\n        axs[1].set_xlabel(\"Time Step\")\n        plt.suptitle(\n            \"Synthetic Dataset Generation\\nLeft: Ground Truth, Right: Generated Samples\"\n        )\n        plt.savefig(os.path.join(working_dir, \"synthetic_generation_comparison.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating generation comparison: {e}\")\n    plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nlosses = experiment_data.get(\"synthetic\", {}).get(\"losses\", {})\nmetrics = experiment_data.get(\"synthetic\", {}).get(\"metrics\", {})\npreds = experiment_data.get(\"synthetic\", {}).get(\"predictions\", [])\ngt = experiment_data.get(\"synthetic\", {}).get(\"ground_truth\", [])\n\nprint(\"Final Train Loss:\", losses.get(\"train\", []))\nprint(\"Final Val Loss:\", losses.get(\"val\", []))\nprint(\"Final Train Metric:\", metrics.get(\"train\", []))\nprint(\"Final Val Metric:\", metrics.get(\"val\", []))\n\ntry:\n    plt.figure()\n    tl = losses.get(\"train\", [])\n    vl = losses.get(\"val\", [])\n    epochs = range(1, len(tl) + 1)\n    plt.plot(epochs, tl, label=\"Train Loss\")\n    plt.plot(epochs, vl, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Synthetic Dataset Loss Curves\\nTraining vs Validation Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    tm = metrics.get(\"train\", [])\n    vm = metrics.get(\"val\", [])\n    epochs = range(1, len(tm) + 1)\n    plt.plot(epochs, tm, label=\"Train Ratio\")\n    plt.plot(epochs, vm, label=\"Val Ratio\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Memory Retention Ratio\")\n    plt.title(\"Synthetic Dataset Metric Curves\\nMemory Retention over Epochs\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_metric_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating metric curve: {e}\")\n    plt.close()\n\ntry:\n    if preds and gt:\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        axs[0].plot(range(len(gt)), gt)\n        axs[0].set_title(\"Ground Truth Sequence\")\n        axs[0].set_xlabel(\"Time Step\")\n        axs[0].set_ylabel(\"Token\")\n        axs[1].plot(range(len(preds)), preds, color=\"orange\")\n        axs[1].set_title(\"Generated Samples\")\n        axs[1].set_xlabel(\"Time Step\")\n        plt.suptitle(\n            \"Synthetic Dataset Generation\\nLeft: Ground Truth, Right: Generated Samples\"\n        )\n        plt.savefig(os.path.join(working_dir, \"synthetic_generation_comparison.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating generation comparison: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nlosses = experiment_data.get(\"synthetic\", {}).get(\"losses\", {})\nmetrics = experiment_data.get(\"synthetic\", {}).get(\"metrics\", {})\npreds = experiment_data.get(\"synthetic\", {}).get(\"predictions\", [])\ngt = experiment_data.get(\"synthetic\", {}).get(\"ground_truth\", [])\n\nprint(\"Final Train Loss:\", losses.get(\"train\", []))\nprint(\"Final Val Loss:\", losses.get(\"val\", []))\nprint(\"Final Train Metric:\", metrics.get(\"train\", []))\nprint(\"Final Val Metric:\", metrics.get(\"val\", []))\n\ntry:\n    plt.figure()\n    tl = losses.get(\"train\", [])\n    vl = losses.get(\"val\", [])\n    epochs = range(1, len(tl) + 1)\n    plt.plot(epochs, tl, label=\"Train Loss\")\n    plt.plot(epochs, vl, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Synthetic Dataset Loss Curves\\nTraining vs Validation Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    tm = metrics.get(\"train\", [])\n    vm = metrics.get(\"val\", [])\n    epochs = range(1, len(tm) + 1)\n    plt.plot(epochs, tm, label=\"Train Ratio\")\n    plt.plot(epochs, vm, label=\"Val Ratio\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Memory Retention Ratio\")\n    plt.title(\"Synthetic Dataset Metric Curves\\nMemory Retention over Epochs\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_metric_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating metric curve: {e}\")\n    plt.close()\n\ntry:\n    if preds and gt:\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        axs[0].plot(range(len(gt)), gt)\n        axs[0].set_title(\"Ground Truth Sequence\")\n        axs[0].set_xlabel(\"Time Step\")\n        axs[0].set_ylabel(\"Token\")\n        axs[1].plot(range(len(preds)), preds, color=\"orange\")\n        axs[1].set_title(\"Generated Samples\")\n        axs[1].set_xlabel(\"Time Step\")\n        plt.suptitle(\n            \"Synthetic Dataset Generation\\nLeft: Ground Truth, Right: Generated Samples\"\n        )\n        plt.savefig(os.path.join(working_dir, \"synthetic_generation_comparison.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating generation comparison: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nlosses = experiment_data.get(\"synthetic\", {}).get(\"losses\", {})\nmetrics = experiment_data.get(\"synthetic\", {}).get(\"metrics\", {})\npreds = experiment_data.get(\"synthetic\", {}).get(\"predictions\", [])\ngt = experiment_data.get(\"synthetic\", {}).get(\"ground_truth\", [])\n\nprint(\"Final Train Loss:\", losses.get(\"train\", []))\nprint(\"Final Val Loss:\", losses.get(\"val\", []))\nprint(\"Final Train Metric:\", metrics.get(\"train\", []))\nprint(\"Final Val Metric:\", metrics.get(\"val\", []))\n\ntry:\n    plt.figure()\n    tl = losses.get(\"train\", [])\n    vl = losses.get(\"val\", [])\n    epochs = range(1, len(tl) + 1)\n    plt.plot(epochs, tl, label=\"Train Loss\")\n    plt.plot(epochs, vl, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Synthetic Dataset Loss Curves\\nTraining vs Validation Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    tm = metrics.get(\"train\", [])\n    vm = metrics.get(\"val\", [])\n    epochs = range(1, len(tm) + 1)\n    plt.plot(epochs, tm, label=\"Train Ratio\")\n    plt.plot(epochs, vm, label=\"Val Ratio\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Memory Retention Ratio\")\n    plt.title(\"Synthetic Dataset Metric Curves\\nMemory Retention over Epochs\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_metric_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating metric curve: {e}\")\n    plt.close()\n\ntry:\n    if preds and gt:\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        axs[0].plot(range(len(gt)), gt)\n        axs[0].set_title(\"Ground Truth Sequence\")\n        axs[0].set_xlabel(\"Time Step\")\n        axs[0].set_ylabel(\"Token\")\n        axs[1].plot(range(len(preds)), preds, color=\"orange\")\n        axs[1].set_title(\"Generated Samples\")\n        axs[1].set_xlabel(\"Time Step\")\n        plt.suptitle(\n            \"Synthetic Dataset Generation\\nLeft: Ground Truth, Right: Generated Samples\"\n        )\n        plt.savefig(os.path.join(working_dir, \"synthetic_generation_comparison.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating generation comparison: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load all experiment data\ntry:\n    experiment_data_path_list = [\n        \"experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_a5277dcb69a643d281445a6a6bcc1ba4_proc_3946599/experiment_data.npy\",\n        \"experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_96860331a4694cbd962fd21ef89ceee5_proc_3946601/experiment_data.npy\",\n        \"experiments/2025-06-05_23-06-44_novelty_aware_compress_attempt_0/logs/0-run/experiment_results/experiment_b684f4a36dd74c988a170c3aab6505de_proc_3946600/experiment_data.npy\",\n    ]\n    all_experiment_data = []\n    for p in experiment_data_path_list:\n        exp = np.load(\n            os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\"), p), allow_pickle=True\n        ).item()\n        all_experiment_data.append(exp.get(\"synthetic\", {}))\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_experiment_data = []\n\n# Aggregate losses and metrics\nloss_trains = [e.get(\"losses\", {}).get(\"train\", []) for e in all_experiment_data]\nloss_vals = [e.get(\"losses\", {}).get(\"val\", []) for e in all_experiment_data]\nmet_trains = [e.get(\"metrics\", {}).get(\"train\", []) for e in all_experiment_data]\nmet_vals = [e.get(\"metrics\", {}).get(\"val\", []) for e in all_experiment_data]\n\n\ndef compute_stats(list_of_lists):\n    valid = [l for l in list_of_lists if len(l) > 0]\n    if not valid:\n        return None, None\n    min_len = min(len(l) for l in valid)\n    arr = np.array([l[:min_len] for l in valid])\n    mean = arr.mean(axis=0)\n    se = arr.std(axis=0, ddof=0) / np.sqrt(arr.shape[0])\n    return mean, se\n\n\nlt_mean, lt_se = compute_stats(loss_trains)\nlv_mean, lv_se = compute_stats(loss_vals)\nmt_mean, mt_se = compute_stats(met_trains)\nmv_mean, mv_se = compute_stats(met_vals)\n\n# Print final means and SE\nif lt_mean is not None:\n    print(f\"Final Train Loss Mean: {lt_mean[-1]:.4f} \u00b1 {lt_se[-1]:.4f}\")\nif lv_mean is not None:\n    print(f\"Final Val   Loss Mean: {lv_mean[-1]:.4f} \u00b1 {lv_se[-1]:.4f}\")\nif mt_mean is not None:\n    print(f\"Final Train Metric Mean: {mt_mean[-1]:.4f} \u00b1 {mt_se[-1]:.4f}\")\nif mv_mean is not None:\n    print(f\"Final Val   Metric Mean: {mv_mean[-1]:.4f} \u00b1 {mv_se[-1]:.4f}\")\n\n# Plot mean \u00b1 SE loss\ntry:\n    if lt_mean is not None and lv_mean is not None:\n        plt.figure()\n        epochs = np.arange(1, len(lt_mean) + 1)\n        plt.errorbar(epochs, lt_mean, yerr=lt_se, label=\"Train Loss Mean \u00b1 SE\")\n        plt.errorbar(epochs, lv_mean, yerr=lv_se, label=\"Val Loss Mean \u00b1 SE\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\n            \"Synthetic Dataset Loss Curves with Mean and SE\\nTraining vs Validation Loss\"\n        )\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_loss_mean_se.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss mean/se plot: {e}\")\n    plt.close()\n\n# Plot mean \u00b1 SE metrics\ntry:\n    if mt_mean is not None and mv_mean is not None:\n        plt.figure()\n        epochs = np.arange(1, len(mt_mean) + 1)\n        plt.errorbar(epochs, mt_mean, yerr=mt_se, label=\"Train Metric Mean \u00b1 SE\")\n        plt.errorbar(epochs, mv_mean, yerr=mv_se, label=\"Val Metric Mean \u00b1 SE\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Memory Retention Ratio\")\n        plt.title(\n            \"Synthetic Dataset Metric Curves with Mean and SE\\nMemory Retention over Epochs\"\n        )\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_metric_mean_se.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating metric mean/se plot: {e}\")\n    plt.close()\n\n# Aggregate and plot generation sequences with mean \u00b1 SE\ntry:\n    preds_list = [e.get(\"predictions\", []) for e in all_experiment_data]\n    gt_list = [e.get(\"ground_truth\", []) for e in all_experiment_data]\n    gt_mean, gt_se = compute_stats(gt_list)\n    pd_mean, pd_se = compute_stats(preds_list)\n    if gt_mean is not None and pd_mean is not None:\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        x_gt = np.arange(1, len(gt_mean) + 1)\n        axs[0].errorbar(x_gt, gt_mean, yerr=gt_se, label=\"Mean \u00b1 SE\")\n        axs[0].set_title(\"Ground Truth Sequence (Mean \u00b1 SE)\")\n        axs[0].set_xlabel(\"Time Step\")\n        axs[0].set_ylabel(\"Token\")\n        axs[0].legend()\n        x_pd = np.arange(1, len(pd_mean) + 1)\n        axs[1].errorbar(x_pd, pd_mean, yerr=pd_se, color=\"orange\", label=\"Mean \u00b1 SE\")\n        axs[1].set_title(\"Generated Samples (Mean \u00b1 SE)\")\n        axs[1].set_xlabel(\"Time Step\")\n        axs[1].legend()\n        plt.suptitle(\n            \"Synthetic Dataset Generation Comparison\\nLeft: Ground Truth, Right: Generated Samples\"\n        )\n        plt.savefig(os.path.join(working_dir, \"synthetic_generation_mean_se.png\"))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating generation mean/se plot: {e}\")\n    plt.close()\n"], "plot_plan": [null, "The script will load the saved experiment_data.npy from the working directory and extract training/validation losses, metrics, predictions, and ground truth for the synthetic dataset. It prints out the final recorded loss and ratio values for quick review in the console. Using basic matplotlib routines and wrapped in individual try-except blocks, it then produces three distinct plots: the loss curves, the memory retention metric curves, and a side\u2010by\u2010side generation comparison. Each figure is clearly titled with the dataset name and plot type, with subtitles indicating left vs right when relevant. The plots are saved to the working directory with descriptive filenames and closed immediately to free resources. Any loading or plotting errors are caught and printed without stopping the script. This approach ensures reproducibility and easy inspection of both curve trajectories and sample generation.", null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null], "is_seed_node": [false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, true], "parse_metrics_plan": ["", "The script builds the working directory path, loads the saved NumPy file with\n`allow_pickle`, and retrieves the `experiment_data` dictionary. It then loops\nover each dataset in the dictionary, extracts the final values from the\n`metrics` (memory retention ratios) and `losses` (training and validation loss)\nlists, and prints them with clear, descriptive labels. No entry\u2010point guard is\nused and the code runs immediately when executed.", "", "The script builds the working directory path, loads the saved NumPy file with\n`allow_pickle`, and retrieves the `experiment_data` dictionary. It then loops\nover each dataset in the dictionary, extracts the final values from the\n`metrics` (memory retention ratios) and `losses` (training and validation loss)\nlists, and prints them with clear, descriptive labels. No entry\u2010point guard is\nused and the code runs immediately when executed.", "The script builds the working directory path, loads the saved NumPy file with\n`allow_pickle`, and retrieves the `experiment_data` dictionary. It then loops\nover each dataset in the dictionary, extracts the final values from the\n`metrics` (memory retention ratios) and `losses` (training and validation loss)\nlists, and prints them with clear, descriptive labels. No entry\u2010point guard is\nused and the code runs immediately when executed.", "The script builds the working directory path, loads the saved NumPy file with\n`allow_pickle`, and retrieves the `experiment_data` dictionary. It then loops\nover each dataset in the dictionary, extracts the final values from the\n`metrics` (memory retention ratios) and `losses` (training and validation loss)\nlists, and prints them with clear, descriptive labels. No entry\u2010point guard is\nused and the code runs immediately when executed.", ""], "parse_metrics_code": ["", "import os\nimport numpy as np\n\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, dataset in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract and print final memory retention ratios\n    train_ratios = dataset.get(\"metrics\", {}).get(\"train\", [])\n    val_ratios = dataset.get(\"metrics\", {}).get(\"val\", [])\n    if train_ratios:\n        print(f\"Final training memory retention ratio: {train_ratios[-1]}\")\n    if val_ratios:\n        print(f\"Final validation memory retention ratio: {val_ratios[-1]}\")\n\n    # Extract and print final losses\n    train_losses = dataset.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dataset.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"Final training loss: {train_losses[-1]}\")\n    if val_losses:\n        print(f\"Final validation loss: {val_losses[-1]}\")\n\n    print()\n", "", "import os\nimport numpy as np\n\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, dataset in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract and print final memory retention ratios\n    train_ratios = dataset.get(\"metrics\", {}).get(\"train\", [])\n    val_ratios = dataset.get(\"metrics\", {}).get(\"val\", [])\n    if train_ratios:\n        print(f\"Final training memory retention ratio: {train_ratios[-1]}\")\n    if val_ratios:\n        print(f\"Final validation memory retention ratio: {val_ratios[-1]}\")\n\n    # Extract and print final losses\n    train_losses = dataset.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dataset.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"Final training loss: {train_losses[-1]}\")\n    if val_losses:\n        print(f\"Final validation loss: {val_losses[-1]}\")\n\n    print()\n", "import os\nimport numpy as np\n\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, dataset in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract and print final memory retention ratios\n    train_ratios = dataset.get(\"metrics\", {}).get(\"train\", [])\n    val_ratios = dataset.get(\"metrics\", {}).get(\"val\", [])\n    if train_ratios:\n        print(f\"Final training memory retention ratio: {train_ratios[-1]}\")\n    if val_ratios:\n        print(f\"Final validation memory retention ratio: {val_ratios[-1]}\")\n\n    # Extract and print final losses\n    train_losses = dataset.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dataset.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"Final training loss: {train_losses[-1]}\")\n    if val_losses:\n        print(f\"Final validation loss: {val_losses[-1]}\")\n\n    print()\n", "import os\nimport numpy as np\n\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, dataset in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract and print final memory retention ratios\n    train_ratios = dataset.get(\"metrics\", {}).get(\"train\", [])\n    val_ratios = dataset.get(\"metrics\", {}).get(\"val\", [])\n    if train_ratios:\n        print(f\"Final training memory retention ratio: {train_ratios[-1]}\")\n    if val_ratios:\n        print(f\"Final validation memory retention ratio: {val_ratios[-1]}\")\n\n    # Extract and print final losses\n    train_losses = dataset.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dataset.get(\"losses\", {}).get(\"val\", [])\n    if train_losses:\n        print(f\"Final training loss: {train_losses[-1]}\")\n    if val_losses:\n        print(f\"Final validation loss: {val_losses[-1]}\")\n\n    print()\n", ""], "parse_term_out": ["", "['Dataset: synthetic', '\\n', 'Final training memory retention ratio:\n0.8214380449300438', '\\n', 'Final validation memory retention ratio:\n0.8210265552539907', '\\n', 'Final training loss: 3.6700600471496587', '\\n',\n'Final validation loss: 3.795520591735841', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "", "['Dataset: synthetic', '\\n', 'Final training memory retention ratio:\n0.8215399167850012', '\\n', 'Final validation memory retention ratio:\n0.8213713723810073', '\\n', 'Final training loss: 3.6716410655975364', '\\n',\n'Final validation loss: 3.794670776367188', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Final training memory retention ratio:\n0.8215293000352482', '\\n', 'Final validation memory retention ratio:\n0.8209828211691476', '\\n', 'Final training loss: 3.680085485458373', '\\n',\n'Final validation loss: 3.8039309997558592', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Final training memory retention ratio:\n0.8215293000352482', '\\n', 'Final validation memory retention ratio:\n0.8209828211691476', '\\n', 'Final training loss: 3.680085485458373', '\\n',\n'Final validation loss: 3.8039309997558592', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null], "completed_stages": ["Stage_1"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>

\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{koh2017understandingbp}
\citation{ghorbani2019datase}
\citation{killamsetty2020glistergb}
\citation{ren2018learningtr}
\citation{koh2017understandingbp}
\citation{ghorbani2019datase}
\citation{killamsetty2020glistergb}
\citation{ren2018learningtr}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{1}{section.3}\protected@file@percent }
\bibdata{references.bib}
\bibcite{ghorbani2019datase}{{1}{2019}{{Ghorbani \& Zou}}{{Ghorbani and Zou}}}
\bibcite{killamsetty2020glistergb}{{2}{2020}{{Killamsetty et~al.}}{{Killamsetty, Sivasubramanian, Ramakrishnan, and Iyer}}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Setup}{2}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Experiments}{2}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Synthetic diagnostics.}{2}{section*.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Synthetic regression: (\textit  {left}) training (solid) and validation (dashed) loss under DVN sampling vs.\ baselines. (\textit  {right}) Spearman ρ between true and predicted contributions over training.}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:synthetic}{{1}{2}{Synthetic regression: (\textit {left}) training (solid) and validation (dashed) loss under DVN sampling vs.\ baselines. (\textit {right}) Spearman ρ between true and predicted contributions over training}{figure.1}{}}
\newlabel{fig:synthetic@cref}{{[figure][1][]1}{[1][2][]2}}
\@writefile{toc}{\contentsline {paragraph}{Classification performance.}{2}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ablation: softmax normalization.}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{2}{section.7}\protected@file@percent }
\bibcite{koh2017understandingbp}{{3}{2017}{{Koh \& Liang}}{{Koh and Liang}}}
\bibcite{ren2018learningtr}{{4}{2018}{{Ren et~al.}}{{Ren, Zeng, Yang, and Urtasun}}}
\bibstyle{iclr2025}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Text classification: validation loss (top) and accuracy (bottom) over epochs for AG News, Yelp, and DBpedia. DVN sampling improves or matches baselines.}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:classif}{{2}{3}{Text classification: validation loss (top) and accuracy (bottom) over epochs for AG News, Yelp, and DBpedia. DVN sampling improves or matches baselines}{figure.2}{}}
\newlabel{fig:classif@cref}{{[figure][2][]2}{[1][2][]3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Softmax-ablation: (\textit  {a}) Spearman ρ drops when normalization is removed. (\textit  {b}) Meta-batch size spikes erratically without softmax.}}{3}{figure.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{3}{figure.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{3}{figure.3}\protected@file@percent }
\newlabel{fig:weight_ablate}{{3}{3}{Softmax-ablation: (\textit {a}) Spearman ρ drops when normalization is removed. (\textit {b}) Meta-batch size spikes erratically without softmax}{figure.3}{}}
\newlabel{fig:weight_ablate@cref}{{[figure][3][]3}{[1][2][]3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Label-noise ablation (AG News): DVN Spearman ρ and validation accuracy under 0\%, 10\%, 20\%, 50\% label-flip noise. Higher noise degrades correlation and accuracy, and destabilizes sampling.}}{4}{figure.4}\protected@file@percent }
\newlabel{fig:ln_ablate}{{4}{4}{Label-noise ablation (AG News): DVN Spearman ρ and validation accuracy under 0\%, 10\%, 20\%, 50\% label-flip noise. Higher noise degrades correlation and accuracy, and destabilizes sampling}{figure.4}{}}
\newlabel{fig:ln_ablate@cref}{{[figure][4][2147483647]4}{[1][3][]4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Omitting the embedding-norm feature: (\textit  {a}) validation accuracy drops by 2–4\%; (\textit  {b}) validation loss increases correspondingly.}}{4}{figure.5}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{4}{figure.5}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{4}{figure.5}\protected@file@percent }
\newlabel{fig:rn_ablate}{{5}{4}{Omitting the embedding-norm feature: (\textit {a}) validation accuracy drops by 2–4\%; (\textit {b}) validation loss increases correspondingly}{figure.5}{}}
\newlabel{fig:rn_ablate@cref}{{[figure][5][2147483647]5}{[1][3][]4}}
\gdef \@abspage@last{4}

{"edges": [[0, 9], [0, 10], [0, 2], [0, 5], [0, 1], [0, 4], [0, 3], [0, 11], [0, 12], [0, 8], [0, 7], [0, 6], [1, 13], [1, 16], [1, 15], [1, 14]], "layout": [[0.56, 0.0], [0.12, 0.5], [0.2, 0.5], [0.28, 0.5], [0.36, 0.5], [0.44, 0.5], [0.52, 0.5], [0.6, 0.5], [0.68, 0.5], [0.76, 0.5], [0.84, 0.5], [0.92, 0.5], [1.0, 0.5], [0.0, 1.0], [0.08, 1.0], [0.16, 1.0], [0.24, 1.0]], "plan": ["We generate a simple synthetic regression dataset (sinusoidal plus noise) and\nsplit into training and held-out sets. We use a small MLP as the foundation\nmodel and another small MLP as the DVN. During each epoch, the foundation model\nis trained on random minibatches, where per-sample losses are reweighted by\nsoftmax-normalized DVN predictions. After each epoch, we measure true per-sample\ncontributions on a random subset of training samples by evaluating the held-out\nloss before and after a one-step update on each sample. We then train the DVN on\nthese (loss, contribution) pairs to better predict sample utility. We compute\nthe Spearman rank correlation between DVN-predicted scores and the actual\nmeasured contributions after each meta-update. We track and print the training\nloss, held-out loss, and contribution prediction correlation at each epoch. All\nmetrics, losses, predictions, and ground-truth contributions are stored in\nexperiment_data and saved to disk at the end. This provides a minimal prototype\nof the meta-learned sampler that can be extended in future work.", "Hyperparam tuning name: EPOCHS. We wrap the training and DVN meta-update in a\nloop over different EPOCHS settings (e.g. [5, 20, 50]), reinitializing models\nand optimizers for each.  For each run, we record per-epoch train/val losses,\nSpearman correlations, DVN predictions and true contributions, and store them\nunder `hyperparam_tuning_type_1 -> synthetic`, along with the parameter name and\nvalues.  At the end, we save everything into `experiment_data.npy`.", "Hyperparam tuning name: MAIN_MODEL_LR. I propose adding an outer loop over a\npredefined grid of learning rates, reinitializing the main and DVN models for\neach rate, and collecting per\u2010epoch metrics exactly as before. Each run\u2019s\ntraining/validation losses, Spearman correlations, predictions, and ground\ntruths are stored under a distinct key in a nested `experiment_data` dictionary.\nAfter tuning over all candidates, the structure is saved in a single\n`experiment_data.npy` file. This keeps the script self\u2010contained and directly\ncomparable across different initial learning rates.", "Hyperparam tuning name: BATCH_SIZE. We loop over batch sizes [16,32,64,128], and\nfor each we reinitialize our foundation and DVN models, data loaders, and\noptimizers. We train for a fixed number of epochs, computing per-epoch train and\nvalidation losses and performing the DVN meta\u2010updates as before; we collect\nmetrics, losses, DVN predictions and ground truths per epoch. After each\nbatch\u2010size trial we append the results into our nested `experiment_data` dict\nunder the \u201cbatch_size\u201d tuning type. Finally, we save the complete\n`experiment_data` object as `experiment_data.npy` in the working directory.", "Hyperparam tuning name: softmax_temperature. Here's the plan: we wrap the\noriginal training loop in an outer loop over a list of temperatures T,\nre\u2010initializing the main model and DVN for each T, and replace the softmax\nweights computation with `weights = softmax(scores / T)`.  We collect per\u2010epoch\ntrain/val metrics, losses, DVN prediction vs. ground\u2010truth arrays (and optional\ncorrelation) into a nested `experiment_data` dict keyed under\n`\"softmax_temperature\" -> \"synthetic\"`, then `np.save` it at the end.", "Hyperparam tuning name: DVN_LR. We loop over a set of DVN learning\u2010rates, and\nfor each we reinitialize the main and DVN models and optimizers accordingly.\nDuring each run we train the main model with DVN\u2010computed weights, meta\u2010update\nthe DVN on a small validation sample, and record train/val losses plus\ncontribution predictions and ground\u2010truths per epoch. All results are stored in\n`experiment_data['DVN_LR']['synthetic'][<lr>]` and finally saved to\n`experiment_data.npy`.", "Hyperparam tuning name: weight_decay. I will loop over the specified weight\ndecay values for both the main model and DVN optimizers, re\u2010initializing models\nand optimizers for each combination and running the exact training + meta\u2010update\nroutine as in the baseline. For each run, I record per\u2010epoch train/val losses,\npredictions, ground truth contributions, and the weight\u2010decay hyperparameters\ninto a unified `experiment_data` dict under the key `\"weight_decay\"`. After\nfinishing the full grid, I save all collected data with `np.save(...)` as\n`experiment_data.npy`. The code is self\u2010contained and executable as\u2010is.", "Hyperparam tuning name: META_SAMPLE. I propose to sweep META_SAMPLE over a\npredefined list (e.g. [5, 10, 20, 50, 100]), and for each value fully\nreinitialize the main model and DVN, then run the existing training+meta-update\nloop for a fixed number of epochs. We record per-epoch train/validation losses,\npredicted vs. true contributions, and their Spearman correlation, storing these\nnested under the hyperparameter key in experiment_data. Finally, we save the\ncomplete experiment_data dictionary via np.save(). Below is the complete self-\ncontained script.", "Hyperparam tuning name: DVN_STEPS_PER_EPOCH. We wrap the existing training and\nmeta\u2010update loops in an outer sweep over DVN_STEPS_PER_EPOCH values,\nre\u2010initializing the main and DVN models for each setting.  For each run we log\nper\u2010epoch train/val losses and contribution\u2010prediction correlations, storing\ntrain/val metrics, losses, DVN predictions, and ground truth contributions under\na dictionary keyed by the current hyperparameter.  Finally, we save all\ncollected results as experiment_data.npy.", "Hyperparam tuning name: ADAM_BETA1. We will sweep \u03b2\u2081 over a list of candidate\nvalues, re\u2010initializing both the main and DVN models and their Adam optimizers\n(setting betas=(\u03b2\u2081,0.999)) for each run. For each \u03b2\u2081 we record per\u2010epoch\ntraining/validation metrics and losses, as well as DVN prediction and\nground\u2010truth arrays, storing them in a nested `experiment_data` structure under\n`'ADAM_BETA1'`. At the end we save the entire dictionary with `np.save(...,\nexperiment_data)` as `experiment_data.npy`.", "Hyperparam tuning name: META_UPDATE_LR. We loop over a predefined grid of\nMETA_UPDATE_LR values, re\u2010initializing and training both the foundation and DVN\nmodels for each candidate. During each training run, we use the current meta-\nupdate LR for the one-step inner update when estimating per-sample\ncontributions, and we record train/val losses and contribution predictions at\nevery epoch. All collected metrics, losses, predictions, and ground-truths are\nstored under `experiment_data['meta_update_lr_tuning']['synthetic']` along with\nthe list of tried meta_lrs, and finally saved with `np.save`.", "Hyperparam tuning name: DVN_HIDDEN_SIZE. We\u2019ll wrap the training loop in an\nouter sweep over DVN hidden sizes [8,16,32,64], re\u2010initializing both the\nfoundation and DVN models for each value, and storing their per\u2010epoch train/val\nmetrics, losses, predictions and ground truth in a nested dictionary keyed by\nhidden size. After all sweeps complete, we dump the entire `experiment_data`\ndict as \u201cexperiment_data.npy\u201d so that each hidden\u2010size run\u2019s results can be\nplotted or analyzed downstream. All code remains in one self\u2010contained file with\nthe same synthetic dataset and meta\u2010learning structure as before.", "Hyperparam tuning name: ADAM_BETA2. I will loop over a set of beta2 values, and\nfor each value reinitialize the main and DVN models with Adam(tuned_betas)\noptimizers.  For each hyperparameter setting, I\u2019ll run the full bilevel training\nprocedure for a fixed number of epochs, record per-epoch train/val losses, and\nSpearman correlations of DVN predictions vs. true contributions.  All results\nwill be stored in a nested `experiment_data` dict under `ADAM_BETA2 ->\nbeta2_<value> -> synthetic`, then saved via `np.save(...,\n\"experiment_data.npy\")`.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Synthetic dataset\nN = 1000\nx = torch.rand(N, 1) * 6 - 3\ny = torch.sin(x) + 0.1 * torch.randn_like(x)\nx_train, y_train = x[:800], y[:800]\nx_val, y_val = x[800:], y[800:]\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\nx_val_tensor, y_val_tensor = x_val.to(device), y_val.to(device)\n\n\n# Models\nclass PretrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nmain_model = PretrainModel().to(device)\ndvn_model = DVN().to(device)\n\noptimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-2)\noptimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-2)\ncriterion_main = nn.MSELoss(reduction=\"none\").to(device)\ncriterion_dvn = nn.MSELoss(reduction=\"mean\").to(device)\n\n\n# Utility: spearman correlation via numpy\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\n# Experiment data storage\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\nEPOCHS = 5\nMETA_SAMPLE = 20\n\nfor epoch in range(EPOCHS):\n    # Train foundation model with DVN weights\n    main_model.train()\n    running_train_loss = 0.0\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        preds = main_model(xb)\n        loss_i = criterion_main(preds, yb)  # per-sample\n        feats = loss_i.detach().unsqueeze(1)\n        scores = dvn_model(feats).squeeze(1)\n        weights = torch.softmax(scores, dim=0)\n        loss = (weights * loss_i).sum()\n        optimizer_main.zero_grad()\n        loss.backward()\n        optimizer_main.step()\n        running_train_loss += loss.item()\n    train_loss = running_train_loss / len(train_loader)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(train_loss)\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(train_loss)\n\n    # Validation loss\n    main_model.eval()\n    with torch.no_grad():\n        val_preds = main_model(x_val_tensor)\n        val_loss = criterion_main(val_preds, y_val_tensor).mean().item()\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(val_loss)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(val_loss)\n\n    # Meta-update DVN: measure actual contributions\n    features_list, contr_list = [], []\n    base_state = main_model.state_dict()\n    for idx in np.random.choice(len(x_train), META_SAMPLE, replace=False):\n        xi = x_train[idx].unsqueeze(0).to(device)\n        yi = y_train[idx].unsqueeze(0).to(device)\n        # feature = current loss\n        with torch.no_grad():\n            feature_val = criterion_main(main_model(xi), yi).item()\n        # clone model for one-step update\n        clone = PretrainModel().to(device)\n        clone.load_state_dict(base_state)\n        opt_clone = torch.optim.Adam(clone.parameters(), lr=1e-2)\n        clone.eval()\n        with torch.no_grad():\n            L0 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n        clone.train()\n        loss_ci = criterion_main(clone(xi), yi).mean()\n        opt_clone.zero_grad()\n        loss_ci.backward()\n        opt_clone.step()\n        clone.eval()\n        with torch.no_grad():\n            L1 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n        contr = L0 - L1\n        features_list.append([feature_val])\n        contr_list.append([contr])\n\n    feats = torch.tensor(features_list, dtype=torch.float32).to(device)\n    contrs = torch.tensor(contr_list, dtype=torch.float32).to(device)\n    # Train DVN\n    for _ in range(5):\n        dvn_model.train()\n        pred_c = dvn_model(feats)\n        dvn_loss = criterion_dvn(pred_c, contrs)\n        optimizer_dvn.zero_grad()\n        dvn_loss.backward()\n        optimizer_dvn.step()\n    # Evaluate correlation\n    dvn_model.eval()\n    with torch.no_grad():\n        preds_np = dvn_model(feats).cpu().numpy().flatten()\n    true_np = contrs.cpu().numpy().flatten()\n    corr = spearman_corr(preds_np, true_np)\n    print(f\"Epoch {epoch}: Contribution Prediction Correlation = {corr:.4f}\")\n    experiment_data[\"synthetic\"][\"predictions\"].append(preds_np)\n    experiment_data[\"synthetic\"][\"ground_truth\"].append(true_np)\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data\nN = 1000\nx = torch.rand(N, 1) * 6 - 3\ny = torch.sin(x) + 0.1 * torch.randn_like(x)\nx_train, y_train = x[:800], y[:800]\nx_val, y_val = x[800:], y[800:]\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\nx_val_tensor, y_val_tensor = x_val.to(device), y_val.to(device)\n\n\n# models\nclass PretrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# spearman correlation\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\n# hyperparameter sweep over EPOCHS\nepoch_values = [5, 20, 50]\nexperiment_data = {\n    \"hyperparam_tuning_type_1\": {\n        \"synthetic\": {\n            \"param_name\": \"EPOCHS\",\n            \"param_values\": epoch_values,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"correlations\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor EPOCHS in epoch_values:\n    # storage for this run\n    run_train_losses, run_val_losses = [], []\n    run_corrs, run_preds, run_truth = [], [], []\n    # initialize models & optimizers\n    main_model = PretrainModel().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-2)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-2)\n    criterion_main = nn.MSELoss(reduction=\"none\").to(device)\n    criterion_dvn = nn.MSELoss(reduction=\"mean\").to(device)\n\n    for epoch in range(EPOCHS):\n        # train main model\n        main_model.train()\n        running_loss = 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = main_model(xb)\n            loss_i = criterion_main(preds, yb)  # per-sample\n            feats = loss_i.detach().unsqueeze(1)\n            scores = dvn_model(feats).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n            running_loss += loss.item()\n        train_loss = running_loss / len(train_loader)\n        run_train_losses.append(train_loss)\n\n        # validation\n        main_model.eval()\n        with torch.no_grad():\n            val_preds = main_model(x_val_tensor)\n            val_loss = criterion_main(val_preds, y_val_tensor).mean().item()\n        run_val_losses.append(val_loss)\n        print(\n            f\"[E={EPOCHS}] Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\"\n        )\n\n        # meta-update DVN\n        features_list, contr_list = [], []\n        base_state = main_model.state_dict()\n        # sample\n        for idx in np.random.choice(len(x_train), 20, replace=False):\n            xi = x_train[idx].unsqueeze(0).to(device)\n            yi = y_train[idx].unsqueeze(0).to(device)\n            # feature\n            with torch.no_grad():\n                feat_val = criterion_main(main_model(xi), yi).item()\n            # clone & step\n            clone = PretrainModel().to(device)\n            clone.load_state_dict(base_state)\n            opt_c = torch.optim.Adam(clone.parameters(), lr=1e-2)\n            clone.eval()\n            with torch.no_grad():\n                L0 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            clone.train()\n            loss_ci = criterion_main(clone(xi), yi).mean()\n            opt_c.zero_grad()\n            loss_ci.backward()\n            opt_c.step()\n            clone.eval()\n            with torch.no_grad():\n                L1 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            contr = L0 - L1\n            features_list.append([feat_val])\n            contr_list.append([contr])\n        feats = torch.tensor(features_list, dtype=torch.float32).to(device)\n        contrs = torch.tensor(contr_list, dtype=torch.float32).to(device)\n        # train DVN\n        for _ in range(5):\n            dvn_model.train()\n            pred_c = dvn_model(feats)\n            dvn_loss = criterion_dvn(pred_c, contrs)\n            optimizer_dvn.zero_grad()\n            dvn_loss.backward()\n            optimizer_dvn.step()\n        # eval DVN\n        dvn_model.eval()\n        with torch.no_grad():\n            preds_np = dvn_model(feats).cpu().numpy().flatten()\n        true_np = contrs.cpu().numpy().flatten()\n        corr = spearman_corr(preds_np, true_np)\n        run_corrs.append(corr)\n        run_preds.append(preds_np)\n        run_truth.append(true_np)\n        print(f\"[E={EPOCHS}] Epoch {epoch}: Spearman Corr={corr:.4f}\")\n\n    # record this hyperparam run\n    sd = experiment_data[\"hyperparam_tuning_type_1\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(run_train_losses)\n    sd[\"metrics\"][\"val\"].append(run_val_losses)\n    sd[\"losses\"][\"train\"].append(run_train_losses)\n    sd[\"losses\"][\"val\"].append(run_val_losses)\n    sd[\"correlations\"].append(run_corrs)\n    sd[\"predictions\"].append(run_preds)\n    sd[\"ground_truth\"].append(run_truth)\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Synthetic dataset\nN = 1000\nx = torch.rand(N, 1) * 6 - 3\ny = torch.sin(x) + 0.1 * torch.randn_like(x)\nx_train, y_train = x[:800], y[:800]\nx_val, y_val = x[800:], y[800:]\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\nx_val_tensor, y_val_tensor = x_val.to(device), y_val.to(device)\n\n# Hyperparameters\nEPOCHS = 5\nMETA_SAMPLE = 20\nlr_grid = [1e-3, 5e-3, 1e-2, 5e-2, 1e-1]\n\n\n# Models\nclass PretrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Utility: spearman correlation via numpy\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\n# Container for all experiments\nexperiment_data = {\"hyperparam_tuning_main_model_lr\": {\"synthetic\": {}}}\n\n# Loop over learning rates\nfor lr in lr_grid:\n    print(f\"\\n=== Tuning main_model_lr = {lr} ===\")\n    # Initialize models & optimizers\n    main_model = PretrainModel().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=lr)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-2)\n    criterion_main = nn.MSELoss(reduction=\"none\").to(device)\n    criterion_dvn = nn.MSELoss(reduction=\"mean\").to(device)\n\n    # Local storage\n    local = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    # Training loop\n    for epoch in range(EPOCHS):\n        main_model.train()\n        running_train_loss = 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = main_model(xb)\n            loss_i = criterion_main(preds, yb)\n            feats = loss_i.detach().unsqueeze(1)\n            scores = dvn_model(feats).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n            running_train_loss += loss.item()\n        train_loss = running_train_loss / len(train_loader)\n        local[\"metrics\"][\"train\"].append(train_loss)\n        local[\"losses\"][\"train\"].append(train_loss)\n\n        # Validation\n        main_model.eval()\n        with torch.no_grad():\n            val_preds = main_model(x_val_tensor)\n            val_loss = criterion_main(val_preds, y_val_tensor).mean().item()\n        print(f\"lr={lr} Epoch {epoch}: val_loss={val_loss:.4f}\")\n        local[\"metrics\"][\"val\"].append(val_loss)\n        local[\"losses\"][\"val\"].append(val_loss)\n\n        # Meta-update DVN\n        features_list, contr_list = [], []\n        base_state = main_model.state_dict()\n        for idx in np.random.choice(len(x_train), META_SAMPLE, replace=False):\n            xi = x_train[idx].unsqueeze(0).to(device)\n            yi = y_train[idx].unsqueeze(0).to(device)\n            with torch.no_grad():\n                f0 = criterion_main(main_model(xi), yi).item()\n            # Clone and one-step update\n            clone = PretrainModel().to(device)\n            clone.load_state_dict(base_state)\n            opt_clone = torch.optim.Adam(clone.parameters(), lr=lr)\n            clone.eval()\n            with torch.no_grad():\n                L0 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            clone.train()\n            loss_ci = criterion_main(clone(xi), yi).mean()\n            opt_clone.zero_grad()\n            loss_ci.backward()\n            opt_clone.step()\n            clone.eval()\n            with torch.no_grad():\n                L1 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            features_list.append([f0])\n            contr_list.append([L0 - L1])\n        feats = torch.tensor(features_list, dtype=torch.float32).to(device)\n        contrs = torch.tensor(contr_list, dtype=torch.float32).to(device)\n\n        # Train DVN\n        for _ in range(5):\n            dvn_model.train()\n            pred_c = dvn_model(feats)\n            dvn_loss = criterion_dvn(pred_c, contrs)\n            optimizer_dvn.zero_grad()\n            dvn_loss.backward()\n            optimizer_dvn.step()\n\n        # Evaluate correlation\n        dvn_model.eval()\n        with torch.no_grad():\n            preds_np = dvn_model(feats).cpu().numpy().flatten()\n        true_np = contrs.cpu().numpy().flatten()\n        corr = spearman_corr(preds_np, true_np)\n        print(f\"lr={lr} Epoch {epoch}: Spearman Corr={corr:.4f}\")\n        local[\"predictions\"].append(preds_np)\n        local[\"ground_truth\"].append(true_np)\n\n    # Save this run\n    experiment_data[\"hyperparam_tuning_main_model_lr\"][\"synthetic\"][f\"lr_{lr}\"] = local\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Synthetic dataset\nN = 1000\nx = torch.rand(N, 1) * 6 - 3\ny = torch.sin(x) + 0.1 * torch.randn_like(x)\nx_train, y_train = x[:800], y[:800]\nx_val, y_val = x[800:], y[800:]\nx_val_tensor, y_val_tensor = x_val.to(device), y_val.to(device)\n\n\n# Model definitions\nclass PretrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Spearman correlation util\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\n# Experiment data container\nbatch_sizes = [16, 32, 64, 128]\nexperiment_data = {\n    \"batch_size\": {\n        \"synthetic\": {\n            \"batch_sizes\": batch_sizes,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],  # shape: [#batch_sizes][#epochs][META_SAMPLE]\n            \"ground_truth\": [],  # same shape\n        }\n    }\n}\n\nEPOCHS = 5\nMETA_SAMPLE = 20\n\nfor bsize in batch_sizes:\n    print(f\"\\n=== Tuning batch size = {bsize} ===\")\n    # reinit models, optimizers, criterion, loader\n    main_model = PretrainModel().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-2)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-2)\n    criterion_main = nn.MSELoss(reduction=\"none\").to(device)\n    criterion_dvn = nn.MSELoss(reduction=\"mean\").to(device)\n    train_loader = DataLoader(\n        TensorDataset(x_train, y_train), batch_size=bsize, shuffle=True\n    )\n    # per-bsize storage\n    train_metrics, val_metrics = [], []\n    train_losses, val_losses = [], []\n    all_preds, all_truths = [], []\n\n    for epoch in range(EPOCHS):\n        # Train foundation model\n        main_model.train()\n        running_train_loss = 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = main_model(xb)\n            loss_i = criterion_main(preds, yb)  # per-sample losses\n            feats = loss_i.detach().unsqueeze(1)\n            scores = dvn_model(feats).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n            running_train_loss += loss.item()\n        train_loss = running_train_loss / len(train_loader)\n        train_metrics.append(train_loss)\n        train_losses.append(train_loss)\n\n        # Validation\n        main_model.eval()\n        with torch.no_grad():\n            vpred = main_model(x_val_tensor)\n            vloss = criterion_main(vpred, y_val_tensor).mean().item()\n        val_metrics.append(vloss)\n        val_losses.append(vloss)\n        print(f\"Batch {bsize} Epoch {epoch}: train={train_loss:.4f}, val={vloss:.4f}\")\n\n        # Meta-update DVN\n        features_list, contr_list = [], []\n        base_state = main_model.state_dict()\n        for idx in np.random.choice(len(x_train), META_SAMPLE, replace=False):\n            xi = x_train[idx].unsqueeze(0).to(device)\n            yi = y_train[idx].unsqueeze(0).to(device)\n            # feature = current loss\n            with torch.no_grad():\n                fval = criterion_main(main_model(xi), yi).item()\n            # clone\n            clone = PretrainModel().to(device)\n            clone.load_state_dict(base_state)\n            opt_clone = torch.optim.Adam(clone.parameters(), lr=1e-2)\n            # L0\n            clone.eval()\n            with torch.no_grad():\n                L0 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            # single-sample update\n            clone.train()\n            loss_ci = criterion_main(clone(xi), yi).mean()\n            opt_clone.zero_grad()\n            loss_ci.backward()\n            opt_clone.step()\n            # L1\n            clone.eval()\n            with torch.no_grad():\n                L1 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            contr = L0 - L1\n            features_list.append([fval])\n            contr_list.append([contr])\n\n        feats = torch.tensor(features_list, dtype=torch.float32).to(device)\n        contrs = torch.tensor(contr_list, dtype=torch.float32).to(device)\n        # train DVN\n        for _ in range(5):\n            dvn_model.train()\n            pred_c = dvn_model(feats)\n            dvn_loss = criterion_dvn(pred_c, contrs)\n            optimizer_dvn.zero_grad()\n            dvn_loss.backward()\n            optimizer_dvn.step()\n        # eval DVN correlation\n        dvn_model.eval()\n        with torch.no_grad():\n            preds_np = dvn_model(feats).cpu().numpy().flatten()\n        true_np = contrs.cpu().numpy().flatten()\n        corr = spearman_corr(preds_np, true_np)\n        print(f\"Batch {bsize} Epoch {epoch}: Spearman corr = {corr:.4f}\")\n        all_preds.append(preds_np)\n        all_truths.append(true_np)\n\n    # append per-bsize results\n    sd = experiment_data[\"batch_size\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(train_metrics)\n    sd[\"metrics\"][\"val\"].append(val_metrics)\n    sd[\"losses\"][\"train\"].append(train_losses)\n    sd[\"losses\"][\"val\"].append(val_losses)\n    sd[\"predictions\"].append(all_preds)\n    sd[\"ground_truth\"].append(all_truths)\n\n# Save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"\\nSaved experiment_data to {working_dir}/experiment_data.npy\")\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Set up working directory and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Synthetic data\nN = 1000\nx = torch.rand(N, 1) * 6 - 3\ny = torch.sin(x) + 0.1 * torch.randn_like(x)\nx_train, y_train = x[:800], y[:800]\nx_val, y_val = x[800:], y[800:]\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\nx_val_tensor, y_val_tensor = x_val.to(device), y_val.to(device)\n\n\n# Model definitions\nclass PretrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Losses and experiment data scaffold\ncriterion_main = nn.MSELoss(reduction=\"none\").to(device)\ncriterion_dvn = nn.MSELoss(reduction=\"mean\").to(device)\nT_list = [0.1, 0.5, 1.0, 2.0]\nexperiment_data = {\n    \"softmax_temperature\": {\n        \"synthetic\": {\n            \"T_values\": T_list,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"correlation\": [],\n        }\n    }\n}\n\n\n# Utility for Spearman correlation\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\nEPOCHS = 5\nMETA_SAMPLE = 20\n\nfor T in T_list:\n    print(f\"\\n=== Running with temperature T = {T} ===\")\n    # Re-init models & optimizers per T\n    main_model = PretrainModel().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-2)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-2)\n\n    # Containers for this T\n    train_metrics, val_metrics = [], []\n    train_losses, val_losses = [], []\n    preds_list, gt_list, corr_list = [], [], []\n\n    for epoch in range(EPOCHS):\n        # 1) Train main model with weighted loss\n        main_model.train()\n        running_loss = 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = main_model(xb)\n            loss_i = criterion_main(preds, yb)  # per-sample losses\n            feats = loss_i.detach().unsqueeze(1)\n            scores = dvn_model(feats).squeeze(1)\n            weights = torch.softmax(scores / T, dim=0)  # apply temperature\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n            running_loss += loss.item()\n        avg_train = running_loss / len(train_loader)\n        train_metrics.append(avg_train)\n        train_losses.append(avg_train)\n\n        # 2) Compute val loss\n        main_model.eval()\n        with torch.no_grad():\n            val_pred = main_model(x_val_tensor)\n            avg_val = criterion_main(val_pred, y_val_tensor).mean().item()\n        val_metrics.append(avg_val)\n        val_losses.append(avg_val)\n        print(f\" T={T}, Epoch {epoch} | Train {avg_train:.4f} | Val {avg_val:.4f}\")\n\n        # 3) Meta-update DVN\n        features_list, contr_list = [], []\n        base_state = main_model.state_dict()\n        for idx in np.random.choice(len(x_train), META_SAMPLE, replace=False):\n            xi = x_train[idx].unsqueeze(0).to(device)\n            yi = y_train[idx].unsqueeze(0).to(device)\n            # feature = current sample loss\n            with torch.no_grad():\n                fval = criterion_main(main_model(xi), yi).item()\n            # one-step clone update to measure contribution\n            clone = PretrainModel().to(device)\n            clone.load_state_dict(base_state)\n            opt_clone = torch.optim.Adam(clone.parameters(), lr=1e-2)\n            with torch.no_grad():\n                L0 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            clone.train()\n            lc = criterion_main(clone(xi), yi).mean()\n            opt_clone.zero_grad()\n            lc.backward()\n            opt_clone.step()\n            clone.eval()\n            with torch.no_grad():\n                L1 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            contr = L0 - L1\n            features_list.append([fval])\n            contr_list.append([contr])\n\n        feats = torch.tensor(features_list, dtype=torch.float32).to(device)\n        contrs = torch.tensor(contr_list, dtype=torch.float32).to(device)\n        # Train DVN\n        for _ in range(5):\n            dvn_model.train()\n            pred_c = dvn_model(feats)\n            loss_d = criterion_dvn(pred_c, contrs)\n            optimizer_dvn.zero_grad()\n            loss_d.backward()\n            optimizer_dvn.step()\n        # Eval DVN correlation\n        dvn_model.eval()\n        with torch.no_grad():\n            p_np = dvn_model(feats).cpu().numpy().flatten()\n        t_np = contrs.cpu().numpy().flatten()\n        corr = spearman_corr(p_np, t_np)\n        preds_list.append(p_np)\n        gt_list.append(t_np)\n        corr_list.append(corr)\n        print(f\" T={T}, Epoch {epoch} | DVN Spearman = {corr:.4f}\")\n\n    # Save results for this T\n    sd = experiment_data[\"softmax_temperature\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(train_metrics)\n    sd[\"metrics\"][\"val\"].append(val_metrics)\n    sd[\"losses\"][\"train\"].append(train_losses)\n    sd[\"losses\"][\"val\"].append(val_losses)\n    sd[\"predictions\"].append(preds_list)\n    sd[\"ground_truth\"].append(gt_list)\n    sd[\"correlation\"].append(corr_list)\n\n# Save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Synthetic dataset\nN = 1000\nx = torch.rand(N, 1) * 6 - 3\ny = torch.sin(x) + 0.1 * torch.randn_like(x)\nx_train, y_train = x[:800], y[:800]\nx_val, y_val = x[800:], y[800:]\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\nx_val_tensor, y_val_tensor = x_val.to(device), y_val.to(device)\n\n\n# Models\nclass PretrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Spearman correlation\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\n# Hyperparameters\nEPOCHS = 5\nMETA_SAMPLE = 20\ndvn_lrs = [1e-3, 5e-4, 1e-4]\n\n# Prepare experiment data structure\nexperiment_data = {\"DVN_LR\": {\"synthetic\": {}}}\n\nfor dvn_lr in dvn_lrs:\n    key = str(dvn_lr)\n    exp = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    experiment_data[\"DVN_LR\"][\"synthetic\"][key] = exp\n\n    # Initialize models and optimizers\n    main_model = PretrainModel().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-2)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=dvn_lr)\n    criterion_main = nn.MSELoss(reduction=\"none\").to(device)\n    criterion_dvn = nn.MSELoss(reduction=\"mean\").to(device)\n\n    # Training loop\n    for epoch in range(EPOCHS):\n        # 1) Train main model with DVN weights\n        main_model.train()\n        running_train_loss = 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = main_model(xb)\n            loss_i = criterion_main(preds, yb)  # per-sample losses\n            feats = loss_i.detach().unsqueeze(1)\n            scores = dvn_model(feats).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n            running_train_loss += loss.item()\n        train_loss = running_train_loss / len(train_loader)\n        exp[\"metrics\"][\"train\"].append(train_loss)\n        exp[\"losses\"][\"train\"].append(train_loss)\n\n        # 2) Validation\n        main_model.eval()\n        with torch.no_grad():\n            val_preds = main_model(x_val_tensor)\n            val_loss = criterion_main(val_preds, y_val_tensor).mean().item()\n        print(f\"LR {dvn_lr} Epoch {epoch}: val_loss = {val_loss:.4f}\")\n        exp[\"metrics\"][\"val\"].append(val_loss)\n        exp[\"losses\"][\"val\"].append(val_loss)\n\n        # 3) Meta-update DVN\n        features_list, contr_list = [], []\n        base_state = main_model.state_dict()\n        for idx in np.random.choice(len(x_train), META_SAMPLE, replace=False):\n            xi = x_train[idx].unsqueeze(0).to(device)\n            yi = y_train[idx].unsqueeze(0).to(device)\n            with torch.no_grad():\n                feature_val = criterion_main(main_model(xi), yi).item()\n            # one-step clone update\n            clone = PretrainModel().to(device)\n            clone.load_state_dict(base_state)\n            opt_clone = torch.optim.Adam(clone.parameters(), lr=1e-2)\n            with torch.no_grad():\n                L0 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            clone.train()\n            loss_ci = criterion_main(clone(xi), yi).mean()\n            opt_clone.zero_grad()\n            loss_ci.backward()\n            opt_clone.step()\n            clone.eval()\n            with torch.no_grad():\n                L1 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            contr = L0 - L1\n            features_list.append([feature_val])\n            contr_list.append([contr])\n\n        feats = torch.tensor(features_list, dtype=torch.float32).to(device)\n        contrs = torch.tensor(contr_list, dtype=torch.float32).to(device)\n\n        # Train DVN\n        for _ in range(5):\n            dvn_model.train()\n            pred_c = dvn_model(feats)\n            dvn_loss = criterion_dvn(pred_c, contrs)\n            optimizer_dvn.zero_grad()\n            dvn_loss.backward()\n            optimizer_dvn.step()\n\n        # Evaluate DVN\n        dvn_model.eval()\n        with torch.no_grad():\n            preds_np = dvn_model(feats).cpu().numpy().flatten()\n        true_np = contrs.cpu().numpy().flatten()\n        corr = spearman_corr(preds_np, true_np)\n        print(f\"LR {dvn_lr} Epoch {epoch}: Spearman corr = {corr:.4f}\")\n\n        exp[\"predictions\"].append(preds_np)\n        exp[\"ground_truth\"].append(true_np)\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Synthetic data\nN = 1000\nx = torch.rand(N, 1) * 6 - 3\ny = torch.sin(x) + 0.1 * torch.randn_like(x)\nx_train, y_train = x[:800], y[:800]\nx_val, y_val = x[800:], y[800:]\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\nx_val_tensor, y_val_tensor = x_val.to(device), y_val.to(device)\n\n\n# Models\nclass PretrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Losses and utility\ncriterion_main = nn.MSELoss(reduction=\"none\").to(device)\ncriterion_dvn = nn.MSELoss(reduction=\"mean\").to(device)\n\n\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\n# Hyperparameter grid\nwd_list = [0.0, 1e-5, 1e-4, 1e-3]\nEPOCHS = 5\nMETA_SAMPLE = 20\n\n# Experiment storage\nexperiment_data = {\n    \"weight_decay\": {\n        \"synthetic\": {\n            \"weight_decay_main\": [],\n            \"weight_decay_dvn\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Grid search\nfor wd_main in wd_list:\n    for wd_dvn in wd_list:\n        print(f\"=== wd_main={wd_main}, wd_dvn={wd_dvn} ===\")\n        main_model = PretrainModel().to(device)\n        dvn_model = DVN().to(device)\n        optimizer_main = torch.optim.Adam(\n            main_model.parameters(), lr=1e-2, weight_decay=wd_main\n        )\n        optimizer_dvn = torch.optim.Adam(\n            dvn_model.parameters(), lr=1e-2, weight_decay=wd_dvn\n        )\n\n        run_train_metrics, run_val_metrics = [], []\n        run_train_losses, run_val_losses = [], []\n        run_preds, run_gts = [], []\n\n        for epoch in range(EPOCHS):\n            # --- Train main model ---\n            main_model.train()\n            running_loss = 0.0\n            for xb, yb in train_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                pred = main_model(xb)\n                loss_i = criterion_main(pred, yb)\n                feats = loss_i.detach().unsqueeze(1)\n                scores = dvn_model(feats).squeeze(1)\n                w = torch.softmax(scores, dim=0)\n                loss = (w * loss_i).sum()\n                optimizer_main.zero_grad()\n                loss.backward()\n                optimizer_main.step()\n                running_loss += loss.item()\n            avg_train = running_loss / len(train_loader)\n            run_train_metrics.append(avg_train)\n            run_train_losses.append(avg_train)\n\n            # --- Validation ---\n            main_model.eval()\n            with torch.no_grad():\n                vp = main_model(x_val_tensor)\n                avg_val = criterion_main(vp, y_val_tensor).mean().item()\n            print(\n                f\"Run(wd_m={wd_main},wd_d={wd_dvn}) Epoch {epoch}: val_loss={avg_val:.4f}\"\n            )\n            run_val_metrics.append(avg_val)\n            run_val_losses.append(avg_val)\n\n            # --- Meta-update DVN ---\n            feats_list, contr_list = [], []\n            base_state = main_model.state_dict()\n            for idx in np.random.choice(len(x_train), META_SAMPLE, replace=False):\n                xi = x_train[idx].unsqueeze(0).to(device)\n                yi = y_train[idx].unsqueeze(0).to(device)\n                with torch.no_grad():\n                    fval = criterion_main(main_model(xi), yi).item()\n                # one-step clone\n                clone = PretrainModel().to(device)\n                clone.load_state_dict(base_state)\n                opt_clone = torch.optim.Adam(clone.parameters(), lr=1e-2)\n                with torch.no_grad():\n                    L0 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n                clone.train()\n                loss_ci = criterion_main(clone(xi), yi).mean()\n                opt_clone.zero_grad()\n                loss_ci.backward()\n                opt_clone.step()\n                with torch.no_grad():\n                    L1 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n                feats_list.append([fval])\n                contr_list.append([L0 - L1])\n\n            feats = torch.tensor(feats_list, dtype=torch.float32).to(device)\n            contrs = torch.tensor(contr_list, dtype=torch.float32).to(device)\n            for _ in range(5):\n                dvn_model.train()\n                pc = dvn_model(feats)\n                dvn_loss = criterion_dvn(pc, contrs)\n                optimizer_dvn.zero_grad()\n                dvn_loss.backward()\n                optimizer_dvn.step()\n\n            dvn_model.eval()\n            with torch.no_grad():\n                pred_np = dvn_model(feats).cpu().numpy().flatten()\n            true_np = contrs.cpu().numpy().flatten()\n            corr = spearman_corr(pred_np, true_np)\n            print(f\"Epoch {epoch}: Spearman corr = {corr:.4f}\")\n            run_preds.append(pred_np)\n            run_gts.append(true_np)\n\n        # Save run results\n        sd = experiment_data[\"weight_decay\"][\"synthetic\"]\n        sd[\"weight_decay_main\"].append(wd_main)\n        sd[\"weight_decay_dvn\"].append(wd_dvn)\n        sd[\"metrics\"][\"train\"].append(run_train_metrics)\n        sd[\"metrics\"][\"val\"].append(run_val_metrics)\n        sd[\"losses\"][\"train\"].append(run_train_losses)\n        sd[\"losses\"][\"val\"].append(run_val_losses)\n        sd[\"predictions\"].append(run_preds)\n        sd[\"ground_truth\"].append(run_gts)\n\n# Final save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Prepare working dir and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Synthetic dataset\nN = 1000\nx = torch.rand(N, 1) * 6 - 3\ny = torch.sin(x) + 0.1 * torch.randn_like(x)\nx_train, y_train = x[:800], y[:800]\nx_val, y_val = x[800:], y[800:]\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\nx_val_tensor, y_val_tensor = x_val.to(device), y_val.to(device)\n\n\n# Model definitions\nclass PretrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Losses\ncriterion_main = nn.MSELoss(reduction=\"none\").to(device)\ncriterion_dvn = nn.MSELoss(reduction=\"mean\").to(device)\n\n\n# Spearman correlation\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\n# Hyperparameter sweep\nEPOCHS = 5\nMETA_SAMPLES = [5, 10, 20, 50, 100]\n\n# Data structure for results\nexperiment_data = {\n    \"meta_sample_tuning\": {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"corr\": [],\n            \"meta_sample_values\": [],\n        }\n    }\n}\n\nfor meta_sample in META_SAMPLES:\n    print(f\"=== Running META_SAMPLE = {meta_sample} ===\")\n    # Initialize models & optimizers\n    main_model = PretrainModel().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-2)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-2)\n    # Lists to collect run-level data\n    tr_metrics, val_metrics = [], []\n    tr_losses, val_losses = [], []\n    all_preds, all_truths, all_corr = [], [], []\n\n    # Training loop\n    for epoch in range(EPOCHS):\n        main_model.train()\n        running_loss = 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = main_model(xb)\n            loss_i = criterion_main(preds, yb)  # per-sample\n            feats = loss_i.detach().unsqueeze(1)\n            scores = dvn_model(feats).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n            running_loss += loss.item()\n        train_loss = running_loss / len(train_loader)\n        tr_metrics.append(train_loss)\n        tr_losses.append(train_loss)\n\n        main_model.eval()\n        with torch.no_grad():\n            val_pred = main_model(x_val_tensor)\n            val_loss = criterion_main(val_pred, y_val_tensor).mean().item()\n        val_metrics.append(val_loss)\n        val_losses.append(val_loss)\n        print(f\"META_SAMPLE={meta_sample} Epoch={epoch} Val Loss={val_loss:.4f}\")\n\n        # Meta-update for DVN\n        features_list, contr_list = [], []\n        base_state = main_model.state_dict()\n        for idx in np.random.choice(len(x_train), meta_sample, replace=False):\n            xi = x_train[idx].unsqueeze(0).to(device)\n            yi = y_train[idx].unsqueeze(0).to(device)\n            with torch.no_grad():\n                fval = criterion_main(main_model(xi), yi).item()\n            clone = PretrainModel().to(device)\n            clone.load_state_dict(base_state)\n            opt_clone = torch.optim.Adam(clone.parameters(), lr=1e-2)\n            with torch.no_grad():\n                L0 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            clone.train()\n            loss_ci = criterion_main(clone(xi), yi).mean()\n            opt_clone.zero_grad()\n            loss_ci.backward()\n            opt_clone.step()\n            with torch.no_grad():\n                L1 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            features_list.append([fval])\n            contr_list.append([L0 - L1])\n\n        feats = torch.tensor(features_list, dtype=torch.float32).to(device)\n        contrs = torch.tensor(contr_list, dtype=torch.float32).to(device)\n        for _ in range(5):\n            dvn_model.train()\n            pc = dvn_model(feats)\n            dvn_loss = criterion_dvn(pc, contrs)\n            optimizer_dvn.zero_grad()\n            dvn_loss.backward()\n            optimizer_dvn.step()\n\n        dvn_model.eval()\n        with torch.no_grad():\n            preds_np = dvn_model(feats).cpu().numpy().flatten()\n        truth_np = contrs.cpu().numpy().flatten()\n        corr = spearman_corr(preds_np, truth_np)\n        all_preds.append(preds_np)\n        all_truths.append(truth_np)\n        all_corr.append(corr)\n        print(f\"Epoch={epoch} Spearman Corr={corr:.4f}\")\n\n    # Store this run's data\n    ds = experiment_data[\"meta_sample_tuning\"][\"synthetic\"]\n    ds[\"metrics\"][\"train\"].append(tr_metrics)\n    ds[\"metrics\"][\"val\"].append(val_metrics)\n    ds[\"losses\"][\"train\"].append(tr_losses)\n    ds[\"losses\"][\"val\"].append(val_losses)\n    ds[\"predictions\"].append(all_preds)\n    ds[\"ground_truth\"].append(all_truths)\n    ds[\"corr\"].append(all_corr)\n    ds[\"meta_sample_values\"].append(meta_sample)\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Synthetic data\nN = 1000\nx = torch.rand(N, 1) * 6 - 3\ny = torch.sin(x) + 0.1 * torch.randn_like(x)\nx_train, y_train = x[:800], y[:800]\nx_val, y_val = x[800:], y[800:]\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\nx_val_tensor, y_val_tensor = x_val.to(device), y_val.to(device)\n\n\n# Models\nclass PretrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\n# Hyperparameter sweep\nDVN_STEPS_LIST = [1, 5, 10, 20]\nEPOCHS = 5\nMETA_SAMPLE = 20\n\nexperiment_data = {\"hyperparam_tuning_type_1\": {}}\n\nfor steps in DVN_STEPS_LIST:\n    print(f\"\\n=== Running DVN_STEPS_PER_EPOCH = {steps} ===\")\n    key = str(steps)\n    experiment_data[\"hyperparam_tuning_type_1\"][key] = {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n\n    # (Re)initialize models and optimizers\n    main_model = PretrainModel().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-2)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-2)\n    criterion_main = nn.MSELoss(reduction=\"none\").to(device)\n    criterion_dvn = nn.MSELoss(reduction=\"mean\").to(device)\n\n    for epoch in range(EPOCHS):\n        # 1) Train main model with DVN\u2010weighted losses\n        main_model.train()\n        running_loss = 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = main_model(xb)\n            losses = criterion_main(preds, yb)  # per-sample\n            with torch.no_grad():\n                scores = dvn_model(losses.detach().unsqueeze(1)).squeeze(1)\n                weights = torch.softmax(scores, dim=0)\n            loss = (weights * losses).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n            running_loss += loss.item()\n        train_loss = running_loss / len(train_loader)\n        experiment_data[\"hyperparam_tuning_type_1\"][key][\"synthetic\"][\"metrics\"][\n            \"train\"\n        ].append(train_loss)\n        experiment_data[\"hyperparam_tuning_type_1\"][key][\"synthetic\"][\"losses\"][\n            \"train\"\n        ].append(train_loss)\n\n        # 2) Validation\n        main_model.eval()\n        with torch.no_grad():\n            val_preds = main_model(x_val_tensor)\n            val_loss = criterion_main(val_preds, y_val_tensor).mean().item()\n        print(f\"Hyper {steps} Epoch {epoch} | val_loss={val_loss:.4f}\")\n        experiment_data[\"hyperparam_tuning_type_1\"][key][\"synthetic\"][\"metrics\"][\n            \"val\"\n        ].append(val_loss)\n        experiment_data[\"hyperparam_tuning_type_1\"][key][\"synthetic\"][\"losses\"][\n            \"val\"\n        ].append(val_loss)\n\n        # 3) Meta\u2010update DVN\n        features_list, contr_list = [], []\n        base_state = main_model.state_dict()\n        for idx in np.random.choice(len(x_train), META_SAMPLE, replace=False):\n            xi = x_train[idx].unsqueeze(0).to(device)\n            yi = y_train[idx].unsqueeze(0).to(device)\n            # feature = current loss on xi\n            feature_val = criterion_main(main_model(xi), yi).item()\n            # clone and one-step update\n            clone = PretrainModel().to(device)\n            clone.load_state_dict(base_state)\n            opt_clone = torch.optim.Adam(clone.parameters(), lr=1e-2)\n            clone.eval()\n            with torch.no_grad():\n                L0 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            clone.train()\n            loss_ci = criterion_main(clone(xi), yi).mean()\n            opt_clone.zero_grad()\n            loss_ci.backward()\n            opt_clone.step()\n            clone.eval()\n            with torch.no_grad():\n                L1 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            contr = L0 - L1\n            features_list.append([feature_val])\n            contr_list.append([contr])\n\n        feats = torch.tensor(features_list, dtype=torch.float32).to(device)\n        contrs = torch.tensor(contr_list, dtype=torch.float32).to(device)\n        # DVN training\n        for _ in range(steps):\n            dvn_model.train()\n            pred_c = dvn_model(feats)\n            dvn_loss = criterion_dvn(pred_c, contrs)\n            optimizer_dvn.zero_grad()\n            dvn_loss.backward()\n            optimizer_dvn.step()\n        # correlation\n        dvn_model.eval()\n        with torch.no_grad():\n            preds_np = dvn_model(feats).cpu().numpy().flatten()\n        true_np = contrs.cpu().numpy().flatten()\n        corr = spearman_corr(preds_np, true_np)\n        print(f\"Hyper {steps} Epoch {epoch} | Spearman={corr:.4f}\")\n        experiment_data[\"hyperparam_tuning_type_1\"][key][\"synthetic\"][\n            \"predictions\"\n        ].append(preds_np)\n        experiment_data[\"hyperparam_tuning_type_1\"][key][\"synthetic\"][\n            \"ground_truth\"\n        ].append(true_np)\n\n# Save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Hyperparameter sweep values for beta1\nbeta1_list = [0.8, 0.9, 0.99]\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Synthetic dataset\nN = 1000\nx = torch.rand(N, 1) * 6 - 3\ny = torch.sin(x) + 0.1 * torch.randn_like(x)\nx_train, y_train = x[:800], y[:800]\nx_val, y_val = x[800:], y[800:]\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\nx_val_tensor, y_val_tensor = x_val.to(device), y_val.to(device)\n\n\n# Models\nclass PretrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Losses & utility\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\nEPOCHS = 5\nMETA_SAMPLE = 20\n\n# Experiment data container\nexperiment_data = {\"ADAM_BETA1\": {}}\n\nfor beta1 in beta1_list:\n    key = str(beta1)\n    experiment_data[\"ADAM_BETA1\"][key] = {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n    # Initialize models & optimizers with this beta1\n    main_model = PretrainModel().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(\n        main_model.parameters(), lr=1e-2, betas=(beta1, 0.999)\n    )\n    optimizer_dvn = torch.optim.Adam(\n        dvn_model.parameters(), lr=1e-2, betas=(beta1, 0.999)\n    )\n    criterion_main = nn.MSELoss(reduction=\"none\").to(device)\n    criterion_dvn = nn.MSELoss(reduction=\"mean\").to(device)\n\n    for epoch in range(EPOCHS):\n        # Train main model with DVN weights\n        main_model.train()\n        running_train_loss = 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = main_model(xb)\n            loss_i = criterion_main(preds, yb)  # per-sample\n            feats = loss_i.detach().unsqueeze(1)\n            scores = dvn_model(feats).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n            running_train_loss += loss.item()\n        train_loss = running_train_loss / len(train_loader)\n        experiment_data[\"ADAM_BETA1\"][key][\"synthetic\"][\"metrics\"][\"train\"].append(\n            train_loss\n        )\n        experiment_data[\"ADAM_BETA1\"][key][\"synthetic\"][\"losses\"][\"train\"].append(\n            train_loss\n        )\n\n        # Validation\n        main_model.eval()\n        with torch.no_grad():\n            val_preds = main_model(x_val_tensor)\n            val_loss = criterion_main(val_preds, y_val_tensor).mean().item()\n        experiment_data[\"ADAM_BETA1\"][key][\"synthetic\"][\"metrics\"][\"val\"].append(\n            val_loss\n        )\n        experiment_data[\"ADAM_BETA1\"][key][\"synthetic\"][\"losses\"][\"val\"].append(\n            val_loss\n        )\n        print(f\"\u03b21={beta1} Epoch {epoch}: val_loss={val_loss:.4f}\")\n\n        # Meta-update DVN\n        features_list, contr_list = [], []\n        base_state = main_model.state_dict()\n        for idx in np.random.choice(len(x_train), META_SAMPLE, replace=False):\n            xi = x_train[idx].unsqueeze(0).to(device)\n            yi = y_train[idx].unsqueeze(0).to(device)\n            with torch.no_grad():\n                feature_val = criterion_main(main_model(xi), yi).item()\n            clone = PretrainModel().to(device)\n            clone.load_state_dict(base_state)\n            opt_clone = torch.optim.Adam(clone.parameters(), lr=1e-2)\n            clone.eval()\n            with torch.no_grad():\n                L0 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            clone.train()\n            loss_ci = criterion_main(clone(xi), yi).mean()\n            opt_clone.zero_grad()\n            loss_ci.backward()\n            opt_clone.step()\n            clone.eval()\n            with torch.no_grad():\n                L1 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            contr = L0 - L1\n            features_list.append([feature_val])\n            contr_list.append([contr])\n\n        feats = torch.tensor(features_list, dtype=torch.float32).to(device)\n        contrs = torch.tensor(contr_list, dtype=torch.float32).to(device)\n        for _ in range(5):\n            dvn_model.train()\n            pred_c = dvn_model(feats)\n            dvn_loss = criterion_dvn(pred_c, contrs)\n            optimizer_dvn.zero_grad()\n            dvn_loss.backward()\n            optimizer_dvn.step()\n\n        # Evaluate correlation\n        dvn_model.eval()\n        with torch.no_grad():\n            preds_np = dvn_model(feats).cpu().numpy().flatten()\n        true_np = contrs.cpu().numpy().flatten()\n        corr = spearman_corr(preds_np, true_np)\n        print(f\"\u03b21={beta1} Epoch {epoch}: Spearman Corr={corr:.4f}\")\n        experiment_data[\"ADAM_BETA1\"][key][\"synthetic\"][\"predictions\"].append(preds_np)\n        experiment_data[\"ADAM_BETA1\"][key][\"synthetic\"][\"ground_truth\"].append(true_np)\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Setup working directory and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Synthetic dataset\nN = 1000\nx = torch.rand(N, 1) * 6 - 3\ny = torch.sin(x) + 0.1 * torch.randn_like(x)\nx_train, y_train = x[:800], y[:800]\nx_val, y_val = x[800:], y[800:]\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\nx_val_tensor, y_val_tensor = x_val.to(device), y_val.to(device)\n\n\n# Model definitions\nclass PretrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Spearman correlation utility\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\n# Experiment data structure\nexperiment_data = {\n    \"meta_update_lr_tuning\": {\n        \"synthetic\": {\n            \"meta_lrs\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Hyperparameter grid\nmeta_lrs = np.logspace(-4, -1, num=6)\nEPOCHS = 5\nMETA_SAMPLE = 20\n\n# Main tuning loop\nfor meta_lr in meta_lrs:\n    print(f\"\\n=== Tuning META_UPDATE_LR = {meta_lr:.1e} ===\")\n    # Initialize models and optimizers\n    main_model = PretrainModel().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-2)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-2)\n    criterion_main = nn.MSELoss(reduction=\"none\").to(device)\n    criterion_dvn = nn.MSELoss(reduction=\"mean\").to(device)\n\n    # Containers for this meta_lr\n    train_metrics, val_metrics = [], []\n    train_losses, val_losses = [], []\n    all_preds, all_truths = [], []\n\n    for epoch in range(EPOCHS):\n        # Train foundation model\n        main_model.train()\n        running_train = 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = main_model(xb)\n            loss_i = criterion_main(preds, yb)\n            feats = loss_i.detach().unsqueeze(1)\n            scores = dvn_model(feats).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n            running_train += loss.item()\n        tr_loss = running_train / len(train_loader)\n        train_metrics.append(tr_loss)\n        train_losses.append(tr_loss)\n\n        # Validation\n        main_model.eval()\n        with torch.no_grad():\n            val_preds = main_model(x_val_tensor)\n            v_loss = criterion_main(val_preds, y_val_tensor).mean().item()\n        print(f\"MetaLR {meta_lr:.1e}, Epoch {epoch}: Val Loss = {v_loss:.4f}\")\n        val_metrics.append(v_loss)\n        val_losses.append(v_loss)\n\n        # Meta-update DVN\n        features_list, contr_list = [], []\n        base_state = main_model.state_dict()\n        for idx in np.random.choice(len(x_train), META_SAMPLE, replace=False):\n            xi = x_train[idx].unsqueeze(0).to(device)\n            yi = y_train[idx].unsqueeze(0).to(device)\n            with torch.no_grad():\n                f0 = criterion_main(main_model(xi), yi).item()\n            clone = PretrainModel().to(device)\n            clone.load_state_dict(base_state)\n            opt_clone = torch.optim.Adam(clone.parameters(), lr=float(meta_lr))\n            clone.eval()\n            with torch.no_grad():\n                L0 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            clone.train()\n            loss_ci = criterion_main(clone(xi), yi).mean()\n            opt_clone.zero_grad()\n            loss_ci.backward()\n            opt_clone.step()\n            clone.eval()\n            with torch.no_grad():\n                L1 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            features_list.append([f0])\n            contr_list.append([L0 - L1])\n        feats = torch.tensor(features_list, dtype=torch.float32).to(device)\n        contrs = torch.tensor(contr_list, dtype=torch.float32).to(device)\n\n        # Train DVN\n        for _ in range(5):\n            dvn_model.train()\n            pred_c = dvn_model(feats)\n            dvn_loss = criterion_dvn(pred_c, contrs)\n            optimizer_dvn.zero_grad()\n            dvn_loss.backward()\n            optimizer_dvn.step()\n\n        # Evaluate contribution correlation\n        dvn_model.eval()\n        with torch.no_grad():\n            preds_np = dvn_model(feats).cpu().numpy().flatten()\n        true_np = contrs.cpu().numpy().flatten()\n        corr = spearman_corr(preds_np, true_np)\n        print(f\"MetaLR {meta_lr:.1e}, Epoch {epoch}: Spearman = {corr:.4f}\")\n        all_preds.append(preds_np)\n        all_truths.append(true_np)\n\n    # Store results\n    d = experiment_data[\"meta_update_lr_tuning\"][\"synthetic\"]\n    d[\"meta_lrs\"].append(float(meta_lr))\n    d[\"metrics\"][\"train\"].append(train_metrics)\n    d[\"metrics\"][\"val\"].append(val_metrics)\n    d[\"losses\"][\"train\"].append(train_losses)\n    d[\"losses\"][\"val\"].append(val_losses)\n    d[\"predictions\"].append(all_preds)\n    d[\"ground_truth\"].append(all_truths)\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Synthetic dataset\nN = 1000\nx = torch.rand(N, 1) * 6 - 3\ny = torch.sin(x) + 0.1 * torch.randn_like(x)\nx_train, y_train = x[:800], y[:800]\nx_val, y_val = x[800:], y[800:]\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\nx_val_tensor, y_val_tensor = x_val.to(device), y_val.to(device)\n\n\n# Models\nclass PretrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(1, hidden_size), nn.ReLU(), nn.Linear(hidden_size, 1)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# Utility: spearman correlation via numpy\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\n# Hyperparameter sweep set up\nhidden_sizes = [8, 16, 32, 64]\nexperiment_data = {\"DVN_hidden_size_tuning\": {\"synthetic\": {}}}\nEPOCHS = 5\nMETA_SAMPLE = 20\n\nfor size in hidden_sizes:\n    key = str(size)\n    experiment_data[\"DVN_hidden_size_tuning\"][\"synthetic\"][key] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    container = experiment_data[\"DVN_hidden_size_tuning\"][\"synthetic\"][key]\n\n    # instantiate fresh models and optimizers for each hidden size\n    main_model = PretrainModel().to(device)\n    dvn_model = DVN(size).to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-2)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-2)\n    criterion_main = nn.MSELoss(reduction=\"none\").to(device)\n    criterion_dvn = nn.MSELoss(reduction=\"mean\").to(device)\n\n    print(f\"Starting sweep for DVN hidden size = {size}\")\n    for epoch in range(EPOCHS):\n        # -- Train main model weighted by DVN\n        main_model.train()\n        running_train_loss = 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = main_model(xb)\n            loss_i = criterion_main(preds, yb)  # per-sample losses\n            feats = loss_i.detach().unsqueeze(1)\n            scores = dvn_model(feats).squeeze(1)  # contribution scores\n            weights = torch.softmax(scores, dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n            running_train_loss += loss.item()\n        train_loss = running_train_loss / len(train_loader)\n        container[\"metrics\"][\"train\"].append(train_loss)\n        container[\"losses\"][\"train\"].append(train_loss)\n\n        # -- Validation loss\n        main_model.eval()\n        with torch.no_grad():\n            val_preds = main_model(x_val_tensor)\n            val_loss = criterion_main(val_preds, y_val_tensor).mean().item()\n        print(f\"Size {size} Epoch {epoch}: val_loss = {val_loss:.4f}\")\n        container[\"metrics\"][\"val\"].append(val_loss)\n        container[\"losses\"][\"val\"].append(val_loss)\n\n        # -- Meta-update DVN via one-step contributions\n        features_list, contr_list = [], []\n        base_state = main_model.state_dict()\n        for idx in np.random.choice(len(x_train), META_SAMPLE, replace=False):\n            xi = x_train[idx].unsqueeze(0).to(device)\n            yi = y_train[idx].unsqueeze(0).to(device)\n            with torch.no_grad():\n                feature_val = criterion_main(main_model(xi), yi).item()\n            clone = PretrainModel().to(device)\n            clone.load_state_dict(base_state)\n            opt_clone = torch.optim.Adam(clone.parameters(), lr=1e-2)\n            clone.eval()\n            with torch.no_grad():\n                L0 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            clone.train()\n            loss_ci = criterion_main(clone(xi), yi).mean()\n            opt_clone.zero_grad()\n            loss_ci.backward()\n            opt_clone.step()\n            clone.eval()\n            with torch.no_grad():\n                L1 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            contr = L0 - L1\n            features_list.append([feature_val])\n            contr_list.append([contr])\n\n        feats = torch.tensor(features_list, dtype=torch.float32).to(device)\n        contrs = torch.tensor(contr_list, dtype=torch.float32).to(device)\n        for _ in range(5):\n            dvn_model.train()\n            pred_c = dvn_model(feats)\n            dvn_loss = criterion_dvn(pred_c, contrs)\n            optimizer_dvn.zero_grad()\n            dvn_loss.backward()\n            optimizer_dvn.step()\n\n        # -- Evaluate DVN correlation\n        dvn_model.eval()\n        with torch.no_grad():\n            preds_np = dvn_model(feats).cpu().numpy().flatten()\n        true_np = contrs.cpu().numpy().flatten()\n        corr = spearman_corr(preds_np, true_np)\n        print(f\"Size {size} Epoch {epoch}: Spearman corr = {corr:.4f}\")\n        container[\"predictions\"].append(preds_np)\n        container[\"ground_truth\"].append(true_np)\n\n# Save experiment_data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# prepare working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# reproducibility\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data\nN = 1000\nx = torch.rand(N, 1) * 6 - 3\ny = torch.sin(x) + 0.1 * torch.randn_like(x)\nx_train, y_train = x[:800], y[:800]\nx_val, y_val = x[800:], y[800:]\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\nx_val_tensor, y_val_tensor = x_val.to(device), y_val.to(device)\n\n\n# models\nclass PretrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# losses\ncriterion_main = nn.MSELoss(reduction=\"none\").to(device)\ncriterion_dvn = nn.MSELoss(reduction=\"mean\").to(device)\n\n\n# spearman\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\n# hyperparameter tuning structure\nexperiment_data = {\"ADAM_BETA2\": {}}\nbeta2_list = [0.9, 0.98, 0.999]\nEPOCHS = 5\nMETA_SAMPLE = 20\n\nfor b2 in beta2_list:\n    key = f\"beta2_{b2}\"\n    # init storage\n    data = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # init models + optimizers\n    main_model = PretrainModel().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-2, betas=(0.9, b2))\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-2, betas=(0.9, b2))\n\n    for epoch in range(EPOCHS):\n        # train main\n        main_model.train()\n        running_train = 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = main_model(xb)\n            loss_i = criterion_main(preds, yb)\n            feats = loss_i.detach().unsqueeze(1)\n            scores = dvn_model(feats).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n            running_train += loss.item()\n        tr_loss = running_train / len(train_loader)\n        data[\"metrics\"][\"train\"].append(tr_loss)\n        data[\"losses\"][\"train\"].append(tr_loss)\n        # val\n        main_model.eval()\n        with torch.no_grad():\n            val_preds = main_model(x_val_tensor)\n            vl = criterion_main(val_preds, y_val_tensor).mean().item()\n        data[\"metrics\"][\"val\"].append(vl)\n        data[\"losses\"][\"val\"].append(vl)\n        print(f\"[{key}] Epoch {epoch} Train {tr_loss:.4f} Val {vl:.4f}\")\n\n        # meta-update DVN\n        features_list, contr_list = [], []\n        base_state = main_model.state_dict()\n        for idx in np.random.choice(len(x_train), META_SAMPLE, replace=False):\n            xi = x_train[idx].unsqueeze(0).to(device)\n            yi = y_train[idx].unsqueeze(0).to(device)\n            with torch.no_grad():\n                f0 = criterion_main(main_model(xi), yi).item()\n            clone = PretrainModel().to(device)\n            clone.load_state_dict(base_state)\n            opt_clone = torch.optim.Adam(clone.parameters(), lr=1e-2, betas=(0.9, b2))\n            clone.eval()\n            with torch.no_grad():\n                L0 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            clone.train()\n            loss_ci = criterion_main(clone(xi), yi).mean()\n            opt_clone.zero_grad()\n            loss_ci.backward()\n            opt_clone.step()\n            clone.eval()\n            with torch.no_grad():\n                L1 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            features_list.append([f0])\n            contr_list.append([L0 - L1])\n\n        feats = torch.tensor(features_list, dtype=torch.float32).to(device)\n        contrs = torch.tensor(contr_list, dtype=torch.float32).to(device)\n        for _ in range(5):\n            dvn_model.train()\n            pred_c = dvn_model(feats)\n            l_dvn = criterion_dvn(pred_c, contrs)\n            optimizer_dvn.zero_grad()\n            l_dvn.backward()\n            optimizer_dvn.step()\n\n        dvn_model.eval()\n        with torch.no_grad():\n            p_np = dvn_model(feats).cpu().numpy().flatten()\n        t_np = contrs.cpu().numpy().flatten()\n        corr = spearman_corr(p_np, t_np)\n        print(f\"[{key}] Epoch {epoch} Spearman Corr: {corr:.4f}\")\n        data[\"predictions\"].append(p_np)\n        data[\"ground_truth\"].append(t_np)\n\n    # store\n    experiment_data[\"ADAM_BETA2\"][key] = {\"synthetic\": data}\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data\nN = 1000\nx = torch.rand(N, 1) * 6 - 3\ny = torch.sin(x) + 0.1 * torch.randn_like(x)\nx_train, y_train = x[:800], y[:800]\nx_val, y_val = x[800:], y[800:]\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\nx_val_tensor, y_val_tensor = x_val.to(device), y_val.to(device)\n\n\n# models\nclass PretrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# spearman correlation\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\n# hyperparameter sweep over EPOCHS\nepoch_values = [5, 20, 50]\nexperiment_data = {\n    \"hyperparam_tuning_type_1\": {\n        \"synthetic\": {\n            \"param_name\": \"EPOCHS\",\n            \"param_values\": epoch_values,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"correlations\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor EPOCHS in epoch_values:\n    # storage for this run\n    run_train_losses, run_val_losses = [], []\n    run_corrs, run_preds, run_truth = [], [], []\n    # initialize models & optimizers\n    main_model = PretrainModel().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-2)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-2)\n    criterion_main = nn.MSELoss(reduction=\"none\").to(device)\n    criterion_dvn = nn.MSELoss(reduction=\"mean\").to(device)\n\n    for epoch in range(EPOCHS):\n        # train main model\n        main_model.train()\n        running_loss = 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = main_model(xb)\n            loss_i = criterion_main(preds, yb)  # per-sample\n            feats = loss_i.detach().unsqueeze(1)\n            scores = dvn_model(feats).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n            running_loss += loss.item()\n        train_loss = running_loss / len(train_loader)\n        run_train_losses.append(train_loss)\n\n        # validation\n        main_model.eval()\n        with torch.no_grad():\n            val_preds = main_model(x_val_tensor)\n            val_loss = criterion_main(val_preds, y_val_tensor).mean().item()\n        run_val_losses.append(val_loss)\n        print(\n            f\"[E={EPOCHS}] Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\"\n        )\n\n        # meta-update DVN\n        features_list, contr_list = [], []\n        base_state = main_model.state_dict()\n        # sample\n        for idx in np.random.choice(len(x_train), 20, replace=False):\n            xi = x_train[idx].unsqueeze(0).to(device)\n            yi = y_train[idx].unsqueeze(0).to(device)\n            # feature\n            with torch.no_grad():\n                feat_val = criterion_main(main_model(xi), yi).item()\n            # clone & step\n            clone = PretrainModel().to(device)\n            clone.load_state_dict(base_state)\n            opt_c = torch.optim.Adam(clone.parameters(), lr=1e-2)\n            clone.eval()\n            with torch.no_grad():\n                L0 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            clone.train()\n            loss_ci = criterion_main(clone(xi), yi).mean()\n            opt_c.zero_grad()\n            loss_ci.backward()\n            opt_c.step()\n            clone.eval()\n            with torch.no_grad():\n                L1 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            contr = L0 - L1\n            features_list.append([feat_val])\n            contr_list.append([contr])\n        feats = torch.tensor(features_list, dtype=torch.float32).to(device)\n        contrs = torch.tensor(contr_list, dtype=torch.float32).to(device)\n        # train DVN\n        for _ in range(5):\n            dvn_model.train()\n            pred_c = dvn_model(feats)\n            dvn_loss = criterion_dvn(pred_c, contrs)\n            optimizer_dvn.zero_grad()\n            dvn_loss.backward()\n            optimizer_dvn.step()\n        # eval DVN\n        dvn_model.eval()\n        with torch.no_grad():\n            preds_np = dvn_model(feats).cpu().numpy().flatten()\n        true_np = contrs.cpu().numpy().flatten()\n        corr = spearman_corr(preds_np, true_np)\n        run_corrs.append(corr)\n        run_preds.append(preds_np)\n        run_truth.append(true_np)\n        print(f\"[E={EPOCHS}] Epoch {epoch}: Spearman Corr={corr:.4f}\")\n\n    # record this hyperparam run\n    sd = experiment_data[\"hyperparam_tuning_type_1\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(run_train_losses)\n    sd[\"metrics\"][\"val\"].append(run_val_losses)\n    sd[\"losses\"][\"train\"].append(run_train_losses)\n    sd[\"losses\"][\"val\"].append(run_val_losses)\n    sd[\"correlations\"].append(run_corrs)\n    sd[\"predictions\"].append(run_preds)\n    sd[\"ground_truth\"].append(run_truth)\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data\nN = 1000\nx = torch.rand(N, 1) * 6 - 3\ny = torch.sin(x) + 0.1 * torch.randn_like(x)\nx_train, y_train = x[:800], y[:800]\nx_val, y_val = x[800:], y[800:]\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\nx_val_tensor, y_val_tensor = x_val.to(device), y_val.to(device)\n\n\n# models\nclass PretrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# spearman correlation\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\n# hyperparameter sweep over EPOCHS\nepoch_values = [5, 20, 50]\nexperiment_data = {\n    \"hyperparam_tuning_type_1\": {\n        \"synthetic\": {\n            \"param_name\": \"EPOCHS\",\n            \"param_values\": epoch_values,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"correlations\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor EPOCHS in epoch_values:\n    # storage for this run\n    run_train_losses, run_val_losses = [], []\n    run_corrs, run_preds, run_truth = [], [], []\n    # initialize models & optimizers\n    main_model = PretrainModel().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-2)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-2)\n    criterion_main = nn.MSELoss(reduction=\"none\").to(device)\n    criterion_dvn = nn.MSELoss(reduction=\"mean\").to(device)\n\n    for epoch in range(EPOCHS):\n        # train main model\n        main_model.train()\n        running_loss = 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = main_model(xb)\n            loss_i = criterion_main(preds, yb)  # per-sample\n            feats = loss_i.detach().unsqueeze(1)\n            scores = dvn_model(feats).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n            running_loss += loss.item()\n        train_loss = running_loss / len(train_loader)\n        run_train_losses.append(train_loss)\n\n        # validation\n        main_model.eval()\n        with torch.no_grad():\n            val_preds = main_model(x_val_tensor)\n            val_loss = criterion_main(val_preds, y_val_tensor).mean().item()\n        run_val_losses.append(val_loss)\n        print(\n            f\"[E={EPOCHS}] Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\"\n        )\n\n        # meta-update DVN\n        features_list, contr_list = [], []\n        base_state = main_model.state_dict()\n        # sample\n        for idx in np.random.choice(len(x_train), 20, replace=False):\n            xi = x_train[idx].unsqueeze(0).to(device)\n            yi = y_train[idx].unsqueeze(0).to(device)\n            # feature\n            with torch.no_grad():\n                feat_val = criterion_main(main_model(xi), yi).item()\n            # clone & step\n            clone = PretrainModel().to(device)\n            clone.load_state_dict(base_state)\n            opt_c = torch.optim.Adam(clone.parameters(), lr=1e-2)\n            clone.eval()\n            with torch.no_grad():\n                L0 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            clone.train()\n            loss_ci = criterion_main(clone(xi), yi).mean()\n            opt_c.zero_grad()\n            loss_ci.backward()\n            opt_c.step()\n            clone.eval()\n            with torch.no_grad():\n                L1 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            contr = L0 - L1\n            features_list.append([feat_val])\n            contr_list.append([contr])\n        feats = torch.tensor(features_list, dtype=torch.float32).to(device)\n        contrs = torch.tensor(contr_list, dtype=torch.float32).to(device)\n        # train DVN\n        for _ in range(5):\n            dvn_model.train()\n            pred_c = dvn_model(feats)\n            dvn_loss = criterion_dvn(pred_c, contrs)\n            optimizer_dvn.zero_grad()\n            dvn_loss.backward()\n            optimizer_dvn.step()\n        # eval DVN\n        dvn_model.eval()\n        with torch.no_grad():\n            preds_np = dvn_model(feats).cpu().numpy().flatten()\n        true_np = contrs.cpu().numpy().flatten()\n        corr = spearman_corr(preds_np, true_np)\n        run_corrs.append(corr)\n        run_preds.append(preds_np)\n        run_truth.append(true_np)\n        print(f\"[E={EPOCHS}] Epoch {epoch}: Spearman Corr={corr:.4f}\")\n\n    # record this hyperparam run\n    sd = experiment_data[\"hyperparam_tuning_type_1\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(run_train_losses)\n    sd[\"metrics\"][\"val\"].append(run_val_losses)\n    sd[\"losses\"][\"train\"].append(run_train_losses)\n    sd[\"losses\"][\"val\"].append(run_val_losses)\n    sd[\"correlations\"].append(run_corrs)\n    sd[\"predictions\"].append(run_preds)\n    sd[\"ground_truth\"].append(run_truth)\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data\nN = 1000\nx = torch.rand(N, 1) * 6 - 3\ny = torch.sin(x) + 0.1 * torch.randn_like(x)\nx_train, y_train = x[:800], y[:800]\nx_val, y_val = x[800:], y[800:]\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\nx_val_tensor, y_val_tensor = x_val.to(device), y_val.to(device)\n\n\n# models\nclass PretrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# spearman correlation\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\n# hyperparameter sweep over EPOCHS\nepoch_values = [5, 20, 50]\nexperiment_data = {\n    \"hyperparam_tuning_type_1\": {\n        \"synthetic\": {\n            \"param_name\": \"EPOCHS\",\n            \"param_values\": epoch_values,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"correlations\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor EPOCHS in epoch_values:\n    # storage for this run\n    run_train_losses, run_val_losses = [], []\n    run_corrs, run_preds, run_truth = [], [], []\n    # initialize models & optimizers\n    main_model = PretrainModel().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-2)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-2)\n    criterion_main = nn.MSELoss(reduction=\"none\").to(device)\n    criterion_dvn = nn.MSELoss(reduction=\"mean\").to(device)\n\n    for epoch in range(EPOCHS):\n        # train main model\n        main_model.train()\n        running_loss = 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = main_model(xb)\n            loss_i = criterion_main(preds, yb)  # per-sample\n            feats = loss_i.detach().unsqueeze(1)\n            scores = dvn_model(feats).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n            running_loss += loss.item()\n        train_loss = running_loss / len(train_loader)\n        run_train_losses.append(train_loss)\n\n        # validation\n        main_model.eval()\n        with torch.no_grad():\n            val_preds = main_model(x_val_tensor)\n            val_loss = criterion_main(val_preds, y_val_tensor).mean().item()\n        run_val_losses.append(val_loss)\n        print(\n            f\"[E={EPOCHS}] Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\"\n        )\n\n        # meta-update DVN\n        features_list, contr_list = [], []\n        base_state = main_model.state_dict()\n        # sample\n        for idx in np.random.choice(len(x_train), 20, replace=False):\n            xi = x_train[idx].unsqueeze(0).to(device)\n            yi = y_train[idx].unsqueeze(0).to(device)\n            # feature\n            with torch.no_grad():\n                feat_val = criterion_main(main_model(xi), yi).item()\n            # clone & step\n            clone = PretrainModel().to(device)\n            clone.load_state_dict(base_state)\n            opt_c = torch.optim.Adam(clone.parameters(), lr=1e-2)\n            clone.eval()\n            with torch.no_grad():\n                L0 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            clone.train()\n            loss_ci = criterion_main(clone(xi), yi).mean()\n            opt_c.zero_grad()\n            loss_ci.backward()\n            opt_c.step()\n            clone.eval()\n            with torch.no_grad():\n                L1 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            contr = L0 - L1\n            features_list.append([feat_val])\n            contr_list.append([contr])\n        feats = torch.tensor(features_list, dtype=torch.float32).to(device)\n        contrs = torch.tensor(contr_list, dtype=torch.float32).to(device)\n        # train DVN\n        for _ in range(5):\n            dvn_model.train()\n            pred_c = dvn_model(feats)\n            dvn_loss = criterion_dvn(pred_c, contrs)\n            optimizer_dvn.zero_grad()\n            dvn_loss.backward()\n            optimizer_dvn.step()\n        # eval DVN\n        dvn_model.eval()\n        with torch.no_grad():\n            preds_np = dvn_model(feats).cpu().numpy().flatten()\n        true_np = contrs.cpu().numpy().flatten()\n        corr = spearman_corr(preds_np, true_np)\n        run_corrs.append(corr)\n        run_preds.append(preds_np)\n        run_truth.append(true_np)\n        print(f\"[E={EPOCHS}] Epoch {epoch}: Spearman Corr={corr:.4f}\")\n\n    # record this hyperparam run\n    sd = experiment_data[\"hyperparam_tuning_type_1\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(run_train_losses)\n    sd[\"metrics\"][\"val\"].append(run_val_losses)\n    sd[\"losses\"][\"train\"].append(run_train_losses)\n    sd[\"losses\"][\"val\"].append(run_val_losses)\n    sd[\"correlations\"].append(run_corrs)\n    sd[\"predictions\"].append(run_preds)\n    sd[\"ground_truth\"].append(run_truth)\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 0.1165', '\\n', 'Epoch\n0: Contribution Prediction Correlation = -0.5564', '\\n', 'Epoch 1:\nvalidation_loss = 0.0612', '\\n', 'Epoch 1: Contribution Prediction Correlation =\n0.2241', '\\n', 'Epoch 2: validation_loss = 0.0397', '\\n', 'Epoch 2: Contribution\nPrediction Correlation = -0.2902', '\\n', 'Epoch 3: validation_loss = 0.0210',\n'\\n', 'Epoch 3: Contribution Prediction Correlation = 0.1113', '\\n', 'Epoch 4:\nvalidation_loss = 0.0191', '\\n', 'Epoch 4: Contribution Prediction Correlation =\n0.1654', '\\n', 'Execution time: a second seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '[E=5] Epoch 0: train_loss=0.1885,\nval_loss=0.1165', '\\n', '[E=5] Epoch 0: Spearman Corr=-0.5564', '\\n', '[E=5]\nEpoch 1: train_loss=0.1002, val_loss=0.0612', '\\n', '[E=5] Epoch 1: Spearman\nCorr=0.2241', '\\n', '[E=5] Epoch 2: train_loss=0.0552, val_loss=0.0397', '\\n',\n'[E=5] Epoch 2: Spearman Corr=-0.2902', '\\n', '[E=5] Epoch 3: train_loss=0.0329,\nval_loss=0.0210', '\\n', '[E=5] Epoch 3: Spearman Corr=0.1113', '\\n', '[E=5]\nEpoch 4: train_loss=0.0218, val_loss=0.0191', '\\n', '[E=5] Epoch 4: Spearman\nCorr=0.1654', '\\n', '[E=20] Epoch 0: train_loss=0.2307, val_loss=0.1476', '\\n',\n'[E=20] Epoch 0: Spearman Corr=-0.6075', '\\n', '[E=20] Epoch 1:\ntrain_loss=0.1226, val_loss=0.0923', '\\n', '[E=20] Epoch 1: Spearman\nCorr=-0.2256', '\\n', '[E=20] Epoch 2: train_loss=0.0852, val_loss=0.0554', '\\n',\n'[E=20] Epoch 2: Spearman Corr=0.0241', '\\n', '[E=20] Epoch 3:\ntrain_loss=0.0535, val_loss=0.0392', '\\n', '[E=20] Epoch 3: Spearman\nCorr=0.2782', '\\n', '[E=20] Epoch 4: train_loss=0.0367, val_loss=0.0239', '\\n',\n'[E=20] Epoch 4: Spearman Corr=0.0617', '\\n', '[E=20] Epoch 5:\ntrain_loss=0.0244, val_loss=0.0220', '\\n', '[E=20] Epoch 5: Spearman\nCorr=0.1609', '\\n', '[E=20] Epoch 6: train_loss=0.0210, val_loss=0.0184', '\\n',\n'[E=20] Epoch 6: Spearman Corr=0.3895', '\\n', '[E=20] Epoch 7:\ntrain_loss=0.0183, val_loss=0.0153', '\\n', '[E=20] Epoch 7: Spearman\nCorr=0.2361', '\\n', '[E=20] Epoch 8: train_loss=0.0159, val_loss=0.0148', '\\n',\n'[E=20] Epoch 8: Spearman Corr=0.1098', '\\n', '[E=20] Epoch 9:\ntrain_loss=0.0141, val_loss=0.0131', '\\n', '[E=20] Epoch 9: Spearman\nCorr=-0.0571', '\\n', '[E=20] Epoch 10: train_loss=0.0136, val_loss=0.0122',\n'\\n', '[E=20] Epoch 10: Spearman Corr=0.2195', '\\n', '[E=20] Epoch 11:\ntrain_loss=0.0138, val_loss=0.0133', '\\n', '[E=20] Epoch 11: Spearman\nCorr=0.2617', '\\n', '[E=20] Epoch 12: train_loss=0.0130, val_loss=0.0118', '\\n',\n'[E=20] Epoch 12: Spearman Corr=-0.1143', '\\n', '[E=20] Epoch 13:\ntrain_loss=0.0127, val_loss=0.0121', '\\n', '[E=20] Epoch 13: Spearman\nCorr=-0.2045', '\\n', '[E=20] Epoch 14: train_loss=0.0125, val_loss=0.0108',\n'\\n', '[E=20] Epoch 14: Spearman Corr=0.0135', '\\n', '[E=20] Epoch 15:\ntrain_loss=0.0121, val_loss=0.0119', '\\n', '[E=20] Epoch 15: Spearman\nCorr=0.1038', '\\n', '[E=20] Epoch 16: train_loss=0.0127, val_loss=0.0115', '\\n',\n'[E=20] Epoch 16: Spearman Corr=-0.0211', '\\n', '[E=20] Epoch 17:\ntrain_loss=0.0120, val_loss=0.0108', '\\n', '[E=20] Epoch 17: Spearman\nCorr=-0.1955', '\\n', '[E=20] Epoch 18: train_loss=0.0116, val_loss=0.0107',\n'\\n', '[E=20] Epoch 18: Spearman Corr=-0.0632', '\\n', '[E=20] Epoch 19:\ntrain_loss=0.0116, val_loss=0.0105', '\\n', '[E=20] Epoch 19: Spearman\nCorr=0.1684', '\\n', '[E=50] Epoch 0: train_loss=0.2938, val_loss=0.1773', '\\n',\n'[E=50] Epoch 0: Spearman Corr=0.1053', '\\n', '[E=50] Epoch 1:\ntrain_loss=0.1909, val_loss=0.1536', '\\n', '[E=50] Epoch 1: Spearman\nCorr=0.3308', '\\n', '[E=50] Epoch 2: train_loss=0.1530, val_loss=0.1134', '\\n',\n'[E=50] Epoch 2: Spearman Corr=0.0632', '\\n', '[E=50] Epoch 3:\ntrain_loss=0.1310, val_loss=0.0994', '\\n', '[E=50] Epoch 3: Spearman\nCorr=-0.0662', '\\n', '[E=50] Epoch 4: train_loss=0.1120, val_loss=0.0888', '\\n',\n'[E=50] Epoch 4: Spearman Corr=-0.3925', '\\n', '[E=50] Epoch 5:\ntrain_loss=0.0928, val_loss=0.0687', '\\n', '[E=50] Epoch 5: Spearman\nCorr=0.3910', '\\n', '[E=50] Epoch 6: train_loss=0.0733, val_loss=0.0530', '\\n',\n'[E=50] Epoch 6: Spearman Corr=0.2015', '\\n', '[E=50] Epoch 7:\ntrain_loss=0.0589, val_loss=0.0414', '\\n', '[E=50] Epoch 7: Spearman\nCorr=0.1278', '\\n', '[E=50] Epoch 8: train_loss=0.0434, val_loss=0.0324', '\\n',\n'[E=50] Epoch 8: Spearman Corr=0.1534', '\\n', '[E=50] Epoch 9:\ntrain_loss=0.0358, val_loss=0.0274', '\\n', '[E=50] Epoch 9: Spearman\nCorr=0.5113', '\\n', '[E=50] Epoch 10: train_loss=0.0284, val_loss=0.0236', '\\n',\n'[E=50] Epoch 10: Spearman Corr=-0.0120', '\\n', '[E=50] Epoch 11:\ntrain_loss=0.0236, val_loss=0.0190', '\\n', '[E=50] Epoch 11: Spearman\nCorr=-0.1910', '\\n', '[E=50] Epoch 12: train_loss=0.0202, val_loss=0.0193',\n'\\n', '[E=50] Epoch 12: Spearman Corr=0.0286', '\\n', '[E=50] Epoch 13:\ntrain_loss=0.0180, val_loss=0.0155', '\\n', '[E=50] Epoch 13: Spearman\nCorr=-0.2842', '\\n', '[E=50] Epoch 14: train_loss=0.0181, val_loss=0.0152',\n'\\n', '[E=50] Epoch 14: Spearman Corr=0.3835', '\\n', '[E=50] Epoch 15:\ntrain_loss=0.0164, val_loss=0.0134', '\\n', '[E=50] Epoch 15: Spearman\nCorr=0.0346', '\\n', '[E=50] Epoch 16: train_loss=0.0147, val_loss=0.0139', '\\n',\n'[E=50] Epoch 16: Spearman Corr=0.4872', '\\n', '[E=50] Epoch 17:\ntrain_loss=0.0146, val_loss=0.0140', '\\n', '[E=50] Epoch 17: Spearman\nCorr=0.0075', '\\n', '[E=50] Epoch 18: train_loss=0.0136, val_loss=0.0136', '\\n',\n'[E=50] Epoch 18: Spearman Corr=-0.1128', '\\n', '[E=50] Epoch 19:\ntrain_loss=0.0142, val_loss=0.0131', '\\n', '[E=50] Epoch 19: Spearman\nCorr=0.3368', '\\n', '[E=50] Epoch 20: train_loss=0.0134, val_loss=0.0115', '\\n',\n'[E=50] Epoch 20: Spearman Corr=0.1263', '\\n', '[E=50] Epoch 21:\ntrain_loss=0.0122, val_loss=0.0109', '\\n', '[E=50] Epoch 21: Spearman\nCorr=-0.4376', '\\n', '[E=50] Epoch 22: train_loss=0.0121, val_loss=0.0112',\n'\\n', '[E=50] Epoch 22: Spearman Corr=-0.2737', '\\n', '[E=50] Epoch 23:\ntrain_loss=0.0131, val_loss=0.0120', '\\n', '[E=50] Epoch 23: Spearman\nCorr=-0.2737', '\\n', '[E=50] Epoch 24: train_loss=0.0120, val_loss=0.0105',\n'\\n', '[E=50] Epoch 24: Spearman Corr=-0.1835', '\\n', '[E=50] Epoch 25:\ntrain_loss=0.0118, val_loss=0.0104', '\\n', '[E=50] Epoch 25: Spearman\nCorr=-0.1925', '\\n', '[E=50] Epoch 26: train_loss=0.0116, val_loss=0.0101',\n'\\n', '[E=50] Epoch 26: Spearman Corr=-0.2917', '\\n', '[E=50] Epoch 27:\ntrain_loss=0.0114, val_loss=0.0118', '\\n', '[E=50] Epoch 27: Spearman\nCorr=-0.0827', '\\n', '[E=50] Epoch 28: train_loss=0.0115, val_loss=0.0109',\n'\\n', '[E=50] Epoch 28: Spearman Corr=-0.2947', '\\n', '[E=50] Epoch 29:\ntrain_loss=0.0121, val_loss=0.0120', '\\n', '[E=50] Epoch 29: Spearman\nCorr=0.0256', '\\n', '[E=50] Epoch 30: train_loss=0.0119, val_loss=0.0128', '\\n',\n'[E=50] Epoch 30: Spearman Corr=0.1895', '\\n', '[E=50] Epoch 31:\ntrain_loss=0.0126, val_loss=0.0113', '\\n', '[E=50] Epoch 31: Spearman\nCorr=-0.0180', '\\n', '[E=50] Epoch 32: train_loss=0.0115, val_loss=0.0101',\n'\\n', '[E=50] Epoch 32: Spearman Corr=-0.0632', '\\n', '[E=50] Epoch 33:\ntrain_loss=0.0112, val_loss=0.0103', '\\n', '[E=50] Epoch 33: Spearman\nCorr=-0.0917', '\\n', '[E=50] Epoch 34: train_loss=0.0110, val_loss=0.0104',\n'\\n', '[E=50] Epoch 34: Spearman Corr=0.4436', '\\n', '[E=50] Epoch 35:\ntrain_loss=0.0120, val_loss=0.0112', '\\n', '[E=50] Epoch 35: Spearman\nCorr=0.2541', '\\n', '[E=50] Epoch 36: train_loss=0.0117, val_loss=0.0101', '\\n',\n'[E=50] Epoch 36: Spearman Corr=0.0526', '\\n', '[E=50] Epoch 37:\ntrain_loss=0.0117, val_loss=0.0134', '\\n', '[E=50] Epoch 37: Spearman\nCorr=0.0421', '\\n', '[E=50] Epoch 38: train_loss=0.0119, val_loss=0.0111', '\\n',\n'[E=50] Epoch 38: Spearman Corr=0.3955', '\\n', '[E=50] Epoch 39:\ntrain_loss=0.0116, val_loss=0.0133', '\\n', '[E=50] Epoch 39: Spearman\nCorr=-0.0090', '\\n', '[E=50] Epoch 40: train_loss=0.0114, val_loss=0.0099',\n'\\n', '[E=50] Epoch 40: Spearman Corr=-0.0045', '\\n', '[E=50] Epoch 41:\ntrain_loss=0.0110, val_loss=0.0097', '\\n', '[E=50] Epoch 41: Spearman\nCorr=0.0346', '\\n', '[E=50] Epoch 42: train_loss=0.0121, val_loss=0.0128', '\\n',\n'[E=50] Epoch 42: Spearman Corr=0.3158', '\\n', '[E=50] Epoch 43:\ntrain_loss=0.0114, val_loss=0.0101', '\\n', '[E=50] Epoch 43: Spearman\nCorr=-0.0556', '\\n', '[E=50] Epoch 44: train_loss=0.0107, val_loss=0.0102',\n'\\n', '[E=50] Epoch 44: Spearman Corr=-0.0226', '\\n', '[E=50] Epoch 45:\ntrain_loss=0.0113, val_loss=0.0098', '\\n', '[E=50] Epoch 45: Spearman\nCorr=-0.1308', '\\n', '[E=50] Epoch 46: train_loss=0.0113, val_loss=0.0098',\n'\\n', '[E=50] Epoch 46: Spearman Corr=0.3805', '\\n', '[E=50] Epoch 47:\ntrain_loss=0.0114, val_loss=0.0103', '\\n', '[E=50] Epoch 47: Spearman\nCorr=-0.0692', '\\n', '[E=50] Epoch 48: train_loss=0.0122, val_loss=0.0132',\n'\\n', '[E=50] Epoch 48: Spearman Corr=-0.1684', '\\n', '[E=50] Epoch 49:\ntrain_loss=0.0113, val_loss=0.0100', '\\n', '[E=50] Epoch 49: Spearman\nCorr=0.3038', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 7\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Tuning main_model_lr = 0.001 ===', '\\n',\n'lr=0.001 Epoch 0: val_loss=0.2049', '\\n', 'lr=0.001 Epoch 0: Spearman\nCorr=-0.4421', '\\n', 'lr=0.001 Epoch 1: val_loss=0.1652', '\\n', 'lr=0.001 Epoch\n1: Spearman Corr=-0.3549', '\\n', 'lr=0.001 Epoch 2: val_loss=0.1476', '\\n',\n'lr=0.001 Epoch 2: Spearman Corr=-0.4000', '\\n', 'lr=0.001 Epoch 3:\nval_loss=0.1386', '\\n', 'lr=0.001 Epoch 3: Spearman Corr=0.2752', '\\n',\n'lr=0.001 Epoch 4: val_loss=0.1305', '\\n', 'lr=0.001 Epoch 4: Spearman\nCorr=0.3504', '\\n', '\\n=== Tuning main_model_lr = 0.005 ===', '\\n', 'lr=0.005\nEpoch 0: val_loss=0.1522', '\\n', 'lr=0.005 Epoch 0: Spearman Corr=-0.2195',\n'\\n', 'lr=0.005 Epoch 1: val_loss=0.1188', '\\n', 'lr=0.005 Epoch 1: Spearman\nCorr=-0.4827', '\\n', 'lr=0.005 Epoch 2: val_loss=0.0903', '\\n', 'lr=0.005 Epoch\n2: Spearman Corr=-0.1053', '\\n', 'lr=0.005 Epoch 3: val_loss=0.0685', '\\n',\n'lr=0.005 Epoch 3: Spearman Corr=0.1158', '\\n', 'lr=0.005 Epoch 4:\nval_loss=0.0531', '\\n', 'lr=0.005 Epoch 4: Spearman Corr=-0.1639', '\\n', '\\n===\nTuning main_model_lr = 0.01 ===', '\\n', 'lr=0.01 Epoch 0: val_loss=0.2079',\n'\\n', 'lr=0.01 Epoch 0: Spearman Corr=0.0526', '\\n', 'lr=0.01 Epoch 1:\nval_loss=0.1049', '\\n', 'lr=0.01 Epoch 1: Spearman Corr=-0.2782', '\\n', 'lr=0.01\nEpoch 2: val_loss=0.0661', '\\n', 'lr=0.01 Epoch 2: Spearman Corr=-0.0992', '\\n',\n'lr=0.01 Epoch 3: val_loss=0.0474', '\\n', 'lr=0.01 Epoch 3: Spearman\nCorr=0.1609', '\\n', 'lr=0.01 Epoch 4: val_loss=0.0325', '\\n', 'lr=0.01 Epoch 4:\nSpearman Corr=0.0541', '\\n', '\\n=== Tuning main_model_lr = 0.05 ===', '\\n',\n'lr=0.05 Epoch 0: val_loss=0.1306', '\\n', 'lr=0.05 Epoch 0: Spearman\nCorr=-0.4752', '\\n', 'lr=0.05 Epoch 1: val_loss=0.0509', '\\n', 'lr=0.05 Epoch 1:\nSpearman Corr=-0.4000', '\\n', 'lr=0.05 Epoch 2: val_loss=0.0210', '\\n', 'lr=0.05\nEpoch 2: Spearman Corr=-0.2466', '\\n', 'lr=0.05 Epoch 3: val_loss=0.0210', '\\n',\n'lr=0.05 Epoch 3: Spearman Corr=-0.1910', '\\n', 'lr=0.05 Epoch 4:\nval_loss=0.0140', '\\n', 'lr=0.05 Epoch 4: Spearman Corr=-0.2466', '\\n', '\\n===\nTuning main_model_lr = 0.1 ===', '\\n', 'lr=0.1 Epoch 0: val_loss=0.5682', '\\n',\n'lr=0.1 Epoch 0: Spearman Corr=-0.3955', '\\n', 'lr=0.1 Epoch 1:\nval_loss=0.2168', '\\n', 'lr=0.1 Epoch 1: Spearman Corr=0.1895', '\\n', 'lr=0.1\nEpoch 2: val_loss=0.1095', '\\n', 'lr=0.1 Epoch 2: Spearman Corr=-0.0947', '\\n',\n'lr=0.1 Epoch 3: val_loss=0.0919', '\\n', 'lr=0.1 Epoch 3: Spearman Corr=0.1113',\n'\\n', 'lr=0.1 Epoch 4: val_loss=0.0813', '\\n', 'lr=0.1 Epoch 4: Spearman\nCorr=0.3068', '\\n', 'Execution time: 3 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', '\\n=== Tuning batch size = 16 ===', '\\n', 'Batch 16\nEpoch 0: train=0.1061, val=0.0317', '\\n', 'Batch 16 Epoch 0: Spearman corr =\n-0.0075', '\\n', 'Batch 16 Epoch 1: train=0.0256, val=0.0145', '\\n', 'Batch 16\nEpoch 1: Spearman corr = 0.4466', '\\n', 'Batch 16 Epoch 2: train=0.0159,\nval=0.0129', '\\n', 'Batch 16 Epoch 2: Spearman corr = -0.0647', '\\n', 'Batch 16\nEpoch 3: train=0.0152, val=0.0119', '\\n', 'Batch 16 Epoch 3: Spearman corr =\n-0.3820', '\\n', 'Batch 16 Epoch 4: train=0.0139, val=0.0181', '\\n', 'Batch 16\nEpoch 4: Spearman corr = -0.3519', '\\n', '\\n=== Tuning batch size = 32 ===',\n'\\n', 'Batch 32 Epoch 0: train=0.1926, val=0.1047', '\\n', 'Batch 32 Epoch 0:\nSpearman corr = 0.1534', '\\n', 'Batch 32 Epoch 1: train=0.0829, val=0.0507',\n'\\n', 'Batch 32 Epoch 1: Spearman corr = -0.1504', '\\n', 'Batch 32 Epoch 2:\ntrain=0.0450, val=0.0267', '\\n', 'Batch 32 Epoch 2: Spearman corr = 0.2421',\n'\\n', 'Batch 32 Epoch 3: train=0.0242, val=0.0183', '\\n', 'Batch 32 Epoch 3:\nSpearman corr = 0.4556', '\\n', 'Batch 32 Epoch 4: train=0.0181, val=0.0157',\n'\\n', 'Batch 32 Epoch 4: Spearman corr = -0.2150', '\\n', '\\n=== Tuning batch\nsize = 64 ===', '\\n', 'Batch 64 Epoch 0: train=0.3364, val=0.2079', '\\n', 'Batch\n64 Epoch 0: Spearman corr = 0.0526', '\\n', 'Batch 64 Epoch 1: train=0.1761,\nval=0.1049', '\\n', 'Batch 64 Epoch 1: Spearman corr = -0.2782', '\\n', 'Batch 64\nEpoch 2: train=0.1076, val=0.0661', '\\n', 'Batch 64 Epoch 2: Spearman corr =\n-0.0992', '\\n', 'Batch 64 Epoch 3: train=0.0677, val=0.0474', '\\n', 'Batch 64\nEpoch 3: Spearman corr = 0.1609', '\\n', 'Batch 64 Epoch 4: train=0.0476,\nval=0.0325', '\\n', 'Batch 64 Epoch 4: Spearman corr = 0.0541', '\\n', '\\n===\nTuning batch size = 128 ===', '\\n', 'Batch 128 Epoch 0: train=0.2630,\nval=0.1862', '\\n', 'Batch 128 Epoch 0: Spearman corr = -0.3639', '\\n', 'Batch\n128 Epoch 1: train=0.1768, val=0.1235', '\\n', 'Batch 128 Epoch 1: Spearman corr\n= -0.1850', '\\n', 'Batch 128 Epoch 2: train=0.1251, val=0.1185', '\\n', 'Batch\n128 Epoch 2: Spearman corr = -0.6662', '\\n', 'Batch 128 Epoch 3: train=0.1141,\nval=0.0773', '\\n', 'Batch 128 Epoch 3: Spearman corr = -0.3188', '\\n', 'Batch\n128 Epoch 4: train=0.0807, val=0.0622', '\\n', 'Batch 128 Epoch 4: Spearman corr\n= -0.0617', '\\n', '\\nSaved experiment_data to /data/chenhui/AI-Scientist-\nv2/experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/0-\nrun/process_ForkProcess-7/working/experiment_data.npy', '\\n', 'Execution time: 3\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Running with temperature T = 0.1 ===', '\\n',\n' T=0.1, Epoch 0 | Train 0.1437 | Val 0.1149', '\\n', ' T=0.1, Epoch 0 | DVN\nSpearman = -0.4737', '\\n', ' T=0.1, Epoch 1 | Train 0.0715 | Val 0.0622', '\\n',\n' T=0.1, Epoch 1 | DVN Spearman = -0.3789', '\\n', ' T=0.1, Epoch 2 | Train\n0.0386 | Val 0.0393', '\\n', ' T=0.1, Epoch 2 | DVN Spearman = -0.0481', '\\n', '\nT=0.1, Epoch 3 | Train 0.0279 | Val 0.0206', '\\n', ' T=0.1, Epoch 3 | DVN\nSpearman = 0.0586', '\\n', ' T=0.1, Epoch 4 | Train 0.0199 | Val 0.0192', '\\n', '\nT=0.1, Epoch 4 | DVN Spearman = 0.1113', '\\n', '\\n=== Running with temperature T\n= 0.5 ===', '\\n', ' T=0.5, Epoch 0 | Train 0.2087 | Val 0.1559', '\\n', ' T=0.5,\nEpoch 0 | DVN Spearman = -0.6692', '\\n', ' T=0.5, Epoch 1 | Train 0.1174 | Val\n0.0908', '\\n', ' T=0.5, Epoch 1 | DVN Spearman = -0.2496', '\\n', ' T=0.5, Epoch\n2 | Train 0.0817 | Val 0.0528', '\\n', ' T=0.5, Epoch 2 | DVN Spearman = 0.0211',\n'\\n', ' T=0.5, Epoch 3 | Train 0.0507 | Val 0.0375', '\\n', ' T=0.5, Epoch 3 |\nDVN Spearman = 0.3218', '\\n', ' T=0.5, Epoch 4 | Train 0.0344 | Val 0.0228',\n'\\n', ' T=0.5, Epoch 4 | DVN Spearman = 0.2060', '\\n', '\\n=== Running with\ntemperature T = 1.0 ===', '\\n', ' T=1.0, Epoch 0 | Train 0.3364 | Val 0.2079',\n'\\n', ' T=1.0, Epoch 0 | DVN Spearman = 0.0526', '\\n', ' T=1.0, Epoch 1 | Train\n0.1761 | Val 0.1049', '\\n', ' T=1.0, Epoch 1 | DVN Spearman = -0.2782', '\\n', '\nT=1.0, Epoch 2 | Train 0.1076 | Val 0.0661', '\\n', ' T=1.0, Epoch 2 | DVN\nSpearman = -0.0992', '\\n', ' T=1.0, Epoch 3 | Train 0.0677 | Val 0.0474', '\\n',\n' T=1.0, Epoch 3 | DVN Spearman = 0.1609', '\\n', ' T=1.0, Epoch 4 | Train 0.0476\n| Val 0.0325', '\\n', ' T=1.0, Epoch 4 | DVN Spearman = 0.0541', '\\n', '\\n===\nRunning with temperature T = 2.0 ===', '\\n', ' T=2.0, Epoch 0 | Train 0.2234 |\nVal 0.1246', '\\n', ' T=2.0, Epoch 0 | DVN Spearman = -0.3068', '\\n', ' T=2.0,\nEpoch 1 | Train 0.1158 | Val 0.0759', '\\n', ' T=2.0, Epoch 1 | DVN Spearman =\n-0.0346', '\\n', ' T=2.0, Epoch 2 | Train 0.0718 | Val 0.0471', '\\n', ' T=2.0,\nEpoch 2 | DVN Spearman = -0.2150', '\\n', ' T=2.0, Epoch 3 | Train 0.0439 | Val\n0.0273', '\\n', ' T=2.0, Epoch 3 | DVN Spearman = 0.3383', '\\n', ' T=2.0, Epoch 4\n| Train 0.0279 | Val 0.0222', '\\n', ' T=2.0, Epoch 4 | DVN Spearman = -0.3263',\n'\\n', 'Execution time: 3 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'LR 0.001 Epoch 0: val_loss = 0.1165', '\\n', 'LR\n0.001 Epoch 0: Spearman corr = -0.5564', '\\n', 'LR 0.001 Epoch 1: val_loss =\n0.0611', '\\n', 'LR 0.001 Epoch 1: Spearman corr = 0.2045', '\\n', 'LR 0.001 Epoch\n2: val_loss = 0.0396', '\\n', 'LR 0.001 Epoch 2: Spearman corr = -0.2481', '\\n',\n'LR 0.001 Epoch 3: val_loss = 0.0210', '\\n', 'LR 0.001 Epoch 3: Spearman corr =\n0.1113', '\\n', 'LR 0.001 Epoch 4: val_loss = 0.0191', '\\n', 'LR 0.001 Epoch 4:\nSpearman corr = 0.1398', '\\n', 'LR 0.0005 Epoch 0: val_loss = 0.1476', '\\n', 'LR\n0.0005 Epoch 0: Spearman corr = -0.6075', '\\n', 'LR 0.0005 Epoch 1: val_loss =\n0.0920', '\\n', 'LR 0.0005 Epoch 1: Spearman corr = -0.1759', '\\n', 'LR 0.0005\nEpoch 2: val_loss = 0.0558', '\\n', 'LR 0.0005 Epoch 2: Spearman corr = 0.0556',\n'\\n', 'LR 0.0005 Epoch 3: val_loss = 0.0392', '\\n', 'LR 0.0005 Epoch 3: Spearman\ncorr = 0.2857', '\\n', 'LR 0.0005 Epoch 4: val_loss = 0.0240', '\\n', 'LR 0.0005\nEpoch 4: Spearman corr = 0.0541', '\\n', 'LR 0.0001 Epoch 0: val_loss = 0.2079',\n'\\n', 'LR 0.0001 Epoch 0: Spearman corr = -0.0256', '\\n', 'LR 0.0001 Epoch 1:\nval_loss = 0.1040', '\\n', 'LR 0.0001 Epoch 1: Spearman corr = -0.2195', '\\n',\n'LR 0.0001 Epoch 2: val_loss = 0.0659', '\\n', 'LR 0.0001 Epoch 2: Spearman corr\n= -0.2602', '\\n', 'LR 0.0001 Epoch 3: val_loss = 0.0470', '\\n', 'LR 0.0001 Epoch\n3: Spearman corr = 0.0692', '\\n', 'LR 0.0001 Epoch 4: val_loss = 0.0322', '\\n',\n'LR 0.0001 Epoch 4: Spearman corr = -0.0271', '\\n', 'Execution time: 2 seconds\nseconds (time limit is an hour).']", "['=== wd_main=0.0, wd_dvn=0.0 ===', '\\n', 'Run(wd_m=0.0,wd_d=0.0) Epoch 0:\nval_loss=0.1165', '\\n', 'Epoch 0: Spearman corr = -0.5564', '\\n',\n'Run(wd_m=0.0,wd_d=0.0) Epoch 1: val_loss=0.0612', '\\n', 'Epoch 1: Spearman corr\n= 0.2241', '\\n', 'Run(wd_m=0.0,wd_d=0.0) Epoch 2: val_loss=0.0397', '\\n', 'Epoch\n2: Spearman corr = -0.2902', '\\n', 'Run(wd_m=0.0,wd_d=0.0) Epoch 3:\nval_loss=0.0210', '\\n', 'Epoch 3: Spearman corr = 0.1113', '\\n',\n'Run(wd_m=0.0,wd_d=0.0) Epoch 4: val_loss=0.0191', '\\n', 'Epoch 4: Spearman corr\n= 0.1654', '\\n', '=== wd_main=0.0, wd_dvn=1e-05 ===', '\\n',\n'Run(wd_m=0.0,wd_d=1e-05) Epoch 0: val_loss=0.1476', '\\n', 'Epoch 0: Spearman\ncorr = -0.6075', '\\n', 'Run(wd_m=0.0,wd_d=1e-05) Epoch 1: val_loss=0.0923',\n'\\n', 'Epoch 1: Spearman corr = -0.2256', '\\n', 'Run(wd_m=0.0,wd_d=1e-05) Epoch\n2: val_loss=0.0554', '\\n', 'Epoch 2: Spearman corr = 0.0211', '\\n',\n'Run(wd_m=0.0,wd_d=1e-05) Epoch 3: val_loss=0.0392', '\\n', 'Epoch 3: Spearman\ncorr = 0.2812', '\\n', 'Run(wd_m=0.0,wd_d=1e-05) Epoch 4: val_loss=0.0239', '\\n',\n'Epoch 4: Spearman corr = 0.0677', '\\n', '=== wd_main=0.0, wd_dvn=0.0001 ===',\n'\\n', 'Run(wd_m=0.0,wd_d=0.0001) Epoch 0: val_loss=0.2079', '\\n', 'Epoch 0:\nSpearman corr = 0.0526', '\\n', 'Run(wd_m=0.0,wd_d=0.0001) Epoch 1:\nval_loss=0.1049', '\\n', 'Epoch 1: Spearman corr = -0.2782', '\\n',\n'Run(wd_m=0.0,wd_d=0.0001) Epoch 2: val_loss=0.0661', '\\n', 'Epoch 2: Spearman\ncorr = -0.2887', '\\n', 'Run(wd_m=0.0,wd_d=0.0001) Epoch 3: val_loss=0.0474',\n'\\n', 'Epoch 3: Spearman corr = 0.1188', '\\n', 'Run(wd_m=0.0,wd_d=0.0001) Epoch\n4: val_loss=0.0325', '\\n', 'Epoch 4: Spearman corr = 0.0000', '\\n', '===\nwd_main=0.0, wd_dvn=0.001 ===', '\\n', 'Run(wd_m=0.0,wd_d=0.001) Epoch 0:\nval_loss=0.1243', '\\n', 'Epoch 0: Spearman corr = -0.2767', '\\n',\n'Run(wd_m=0.0,wd_d=0.001) Epoch 1: val_loss=0.0763', '\\n', 'Epoch 1: Spearman\ncorr = -0.1038', '\\n', 'Run(wd_m=0.0,wd_d=0.001) Epoch 2: val_loss=0.0467',\n'\\n', 'Epoch 2: Spearman corr = -0.1158', '\\n', 'Run(wd_m=0.0,wd_d=0.001) Epoch\n3: val_loss=0.0274', '\\n', 'Epoch 3: Spearman corr = 0.2872', '\\n',\n'Run(wd_m=0.0,wd_d=0.001) Epoch 4: val_loss=0.0221', '\\n', 'Epoch 4: Spearman\ncorr = -0.2571', '\\n', '=== wd_main=1e-05, wd_dvn=0.0 ===', '\\n',\n'Run(wd_m=1e-05,wd_d=0.0) Epoch 0: val_loss=0.1563', '\\n', 'Epoch 0: Spearman\ncorr = 0.0211', '\\n', 'Run(wd_m=1e-05,wd_d=0.0) Epoch 1: val_loss=0.0848', '\\n',\n'Epoch 1: Spearman corr = 0.4361', '\\n', 'Run(wd_m=1e-05,wd_d=0.0) Epoch 2:\nval_loss=0.0473', '\\n', 'Epoch 2: Spearman corr = 0.0707', '\\n',\n'Run(wd_m=1e-05,wd_d=0.0) Epoch 3: val_loss=0.0356', '\\n', 'Epoch 3: Spearman\ncorr = 0.3173', '\\n', 'Run(wd_m=1e-05,wd_d=0.0) Epoch 4: val_loss=0.0282', '\\n',\n'Epoch 4: Spearman corr = 0.5459', '\\n', '=== wd_main=1e-05, wd_dvn=1e-05 ===',\n'\\n', 'Run(wd_m=1e-05,wd_d=1e-05) Epoch 0: val_loss=0.2459', '\\n', 'Epoch 0:\nSpearman corr = 0.2496', '\\n', 'Run(wd_m=1e-05,wd_d=1e-05) Epoch 1:\nval_loss=0.1836', '\\n', 'Epoch 1: Spearman corr = -0.1323', '\\n',\n'Run(wd_m=1e-05,wd_d=1e-05) Epoch 2: val_loss=0.1596', '\\n', 'Epoch 2: Spearman\ncorr = 0.6195', '\\n', 'Run(wd_m=1e-05,wd_d=1e-05) Epoch 3: val_loss=0.1303',\n'\\n', 'Epoch 3: Spearman corr = 0.1774', '\\n', 'Run(wd_m=1e-05,wd_d=1e-05) Epoch\n4: val_loss=0.1113', '\\n', 'Epoch 4: Spearman corr = -0.2331', '\\n', '===\nwd_main=1e-05, wd_dvn=0.0001 ===', '\\n', 'Run(wd_m=1e-05,wd_d=0.0001) Epoch 0:\nval_loss=0.1446', '\\n', 'Epoch 0: Spearman corr = -0.3203', '\\n',\n'Run(wd_m=1e-05,wd_d=0.0001) Epoch 1: val_loss=0.0972', '\\n', 'Epoch 1: Spearman\ncorr = -0.3278', '\\n', 'Run(wd_m=1e-05,wd_d=0.0001) Epoch 2: val_loss=0.0629',\n'\\n', 'Epoch 2: Spearman corr = -0.2060', '\\n', 'Run(wd_m=1e-05,wd_d=0.0001)\nEpoch 3: val_loss=0.0443', '\\n', 'Epoch 3: Spearman corr = -0.3504', '\\n',\n'Run(wd_m=1e-05,wd_d=0.0001) Epoch 4: val_loss=0.0288', '\\n', 'Epoch 4: Spearman\ncorr = 0.6932', '\\n', '=== wd_main=1e-05, wd_dvn=0.001 ===', '\\n',\n'Run(wd_m=1e-05,wd_d=0.001) Epoch 0: val_loss=0.1359', '\\n', 'Epoch 0: Spearman\ncorr = 0.5714', '\\n', 'Run(wd_m=1e-05,wd_d=0.001) Epoch 1: val_loss=0.0848',\n'\\n', 'Epoch 1: Spearman corr = 0.2165', '\\n', 'Run(wd_m=1e-05,wd_d=0.001) Epoch\n2: val_loss=0.0517', '\\n', 'Epoch 2: Spearman corr = -0.0135', '\\n',\n'Run(wd_m=1e-05,wd_d=0.001) Epoch 3: val_loss=0.0310', '\\n', 'Epoch 3: Spearman\ncorr = 0.1368', '\\n', 'Run(wd_m=1e-05,wd_d=0.001) Epoch 4: val_loss=0.0213',\n'\\n', 'Epoch 4: Spearman corr = -0.1263', '\\n', '=== wd_main=0.0001, wd_dvn=0.0\n===', '\\n', 'Run(wd_m=0.0001,wd_d=0.0) Epoch 0: val_loss=0.1764', '\\n', 'Epoch\n0: Spearman corr = -0.2677', '\\n', 'Run(wd_m=0.0001,wd_d=0.0) Epoch 1:\nval_loss=0.1111', '\\n', 'Epoch 1: Spearman corr = -0.2421', '\\n',\n'Run(wd_m=0.0001,wd_d=0.0) Epoch 2: val_loss=0.0676', '\\n', 'Epoch 2: Spearman\ncorr = -0.4722', '\\n', 'Run(wd_m=0.0001,wd_d=0.0) Epoch 3: val_loss=0.0424',\n'\\n', 'Epoch 3: Spearman corr = 0.0376', '\\n', 'Run(wd_m=0.0001,wd_d=0.0) Epoch\n4: val_loss=0.0292', '\\n', 'Epoch 4: Spearman corr = 0.0827', '\\n', '===\nwd_main=0.0001, wd_dvn=1e-05 ===', '\\n', 'Run(wd_m=0.0001,wd_d=1e-05) Epoch 0:\nval_loss=0.2001', '\\n', 'Epoch 0: Spearman corr = 0.0947', '\\n',\n'Run(wd_m=0.0001,wd_d=1e-05) Epoch 1: val_loss=0.1129', '\\n', 'Epoch 1: Spearman\ncorr = -0.0902', '\\n', 'Run(wd_m=0.0001,wd_d=1e-05) Epoch 2: val_loss=0.0986',\n'\\n', 'Epoch 2: Spearman corr = -0.2466', '\\n', 'Run(wd_m=0.0001,wd_d=1e-05)\nEpoch 3: val_loss=0.0793', '\\n', 'Epoch 3: Spearman corr = 0.0602', '\\n',\n'Run(wd_m=0.0001,wd_d=1e-05) Epoch 4: val_loss=0.0588', '\\n', 'Epoch 4: Spearman\ncorr = -0.1038', '\\n', '=== wd_main=0.0001, wd_dvn=0.0001 ===', '\\n',\n'Run(wd_m=0.0001,wd_d=0.0001) Epoch 0: val_loss=0.0813', '\\n', 'Epoch 0:\nSpearman corr = 0.1684', '\\n', 'Run(wd_m=0.0001,wd_d=0.0001) Epoch 1:\nval_loss=0.0348', '\\n', 'Epoch 1: Spearman corr = -0.2526', '\\n',\n'Run(wd_m=0.0001,wd_d=0.0001) Epoch 2: val_loss=0.0224', '\\n', 'Epoch 2:\nSpearman corr = -0.1128', '\\n', 'Run(wd_m=0.0001,wd_d=0.0001) Epoch 3:\nval_loss=0.0204', '\\n', 'Epoch 3: Spearman corr = 0.2647', '\\n',\n'Run(wd_m=0.0001,wd_d=0.0001) Epoch 4: val_loss=0.0142', '\\n', 'Epoch 4:\nSpearman corr = -0.0526', '\\n', '=== wd_main=0.0001, wd_dvn=0.001 ===', '\\n',\n'Run(wd_m=0.0001,wd_d=0.001) Epoch 0: val_loss=0.1341', '\\n', 'Epoch 0: Spearman\ncorr = -0.0511', '\\n', 'Run(wd_m=0.0001,wd_d=0.001) Epoch 1: val_loss=0.0793',\n'\\n', 'Epoch 1: Spearman corr = 0.1398', '\\n', 'Run(wd_m=0.0001,wd_d=0.001)\nEpoch 2: val_loss=0.0542', '\\n', 'Epoch 2: Spearman corr = -0.0902', '\\n',\n'Run(wd_m=0.0001,wd_d=0.001) Epoch 3: val_loss=0.0362', '\\n', 'Epoch 3: Spearman\ncorr = -0.1158', '\\n', 'Run(wd_m=0.0001,wd_d=0.001) Epoch 4: val_loss=0.0259',\n'\\n', 'Epoch 4: Spearman corr = -0.1985', '\\n', '=== wd_main=0.001, wd_dvn=0.0\n===', '\\n', 'Run(wd_m=0.001,wd_d=0.0) Epoch 0: val_loss=0.1281', '\\n', 'Epoch 0:\nSpearman corr = -0.1910', '\\n', 'Run(wd_m=0.001,wd_d=0.0) Epoch 1:\nval_loss=0.1022', '\\n', 'Epoch 1: Spearman corr = -0.2165', '\\n',\n'Run(wd_m=0.001,wd_d=0.0) Epoch 2: val_loss=0.0815', '\\n', 'Epoch 2: Spearman\ncorr = 0.0391', '\\n', 'Run(wd_m=0.001,wd_d=0.0) Epoch 3: val_loss=0.0562', '\\n',\n'Epoch 3: Spearman corr = -0.0932', '\\n', 'Run(wd_m=0.001,wd_d=0.0) Epoch 4:\nval_loss=0.0417', '\\n', 'Epoch 4: Spearman corr = -0.1865', '\\n', '===\nwd_main=0.001, wd_dvn=1e-05 ===', '\\n', 'Run(wd_m=0.001,wd_d=1e-05) Epoch 0:\nval_loss=0.2541', '\\n', 'Epoch 0: Spearman corr = -0.4150', '\\n',\n'Run(wd_m=0.001,wd_d=1e-05) Epoch 1: val_loss=0.1626', '\\n', 'Epoch 1: Spearman\ncorr = -0.6436', '\\n', 'Run(wd_m=0.001,wd_d=1e-05) Epoch 2: val_loss=0.1117',\n'\\n', 'Epoch 2: Spearman corr = 0.0271', '\\n', 'Run(wd_m=0.001,wd_d=1e-05) Epoch\n3: val_loss=0.0768', '\\n', 'Epoch 3: Spearman corr = 0.3173', '\\n',\n'Run(wd_m=0.001,wd_d=1e-05) Epoch 4: val_loss=0.0539', '\\n', 'Epoch 4: Spearman\ncorr = -0.0346', '\\n', '=== wd_main=0.001, wd_dvn=0.0001 ===', '\\n',\n'Run(wd_m=0.001,wd_d=0.0001) Epoch 0: val_loss=0.1975', '\\n', 'Epoch 0: Spearman\ncorr = -0.4677', '\\n', 'Run(wd_m=0.001,wd_d=0.0001) Epoch 1: val_loss=0.0882',\n'\\n', 'Epoch 1: Spearman corr = 0.0361', '\\n', 'Run(wd_m=0.001,wd_d=0.0001)\nEpoch 2: val_loss=0.0660', '\\n', 'Epoch 2: Spearman corr = -0.4617', '\\n',\n'Run(wd_m=0.001,wd_d=0.0001) Epoch 3: val_loss=0.0494', '\\n', 'Epoch 3: Spearman\ncorr = -0.3489', '\\n', 'Run(wd_m=0.001,wd_d=0.0001) Epoch 4: val_loss=0.0354',\n'\\n', 'Epoch 4: Spearman corr = 0.0060', '\\n', '=== wd_main=0.001, wd_dvn=0.001\n===', '\\n', 'Run(wd_m=0.001,wd_d=0.001) Epoch 0: val_loss=0.2571', '\\n', 'Epoch\n0: Spearman corr = -0.4541', '\\n', 'Run(wd_m=0.001,wd_d=0.001) Epoch 1:\nval_loss=0.1129', '\\n', 'Epoch 1: Spearman corr = 0.0045', '\\n',\n'Run(wd_m=0.001,wd_d=0.001) Epoch 2: val_loss=0.0723', '\\n', 'Epoch 2: Spearman\ncorr = 0.5053', '\\n', 'Run(wd_m=0.001,wd_d=0.001) Epoch 3: val_loss=0.0517',\n'\\n', 'Epoch 3: Spearman corr = -0.1218', '\\n', 'Run(wd_m=0.001,wd_d=0.001)\nEpoch 4: val_loss=0.0366', '\\n', 'Epoch 4: Spearman corr = 0.1609', '\\n',\n'Execution time: 7 seconds seconds (time limit is an hour).']", "['=== Running META_SAMPLE = 5 ===', '\\n', 'META_SAMPLE=5 Epoch=0 Val\nLoss=0.1165', '\\n', 'Epoch=0 Spearman Corr=-0.3000', '\\n', 'META_SAMPLE=5\nEpoch=1 Val Loss=0.0603', '\\n', 'Epoch=1 Spearman Corr=0.6000', '\\n',\n'META_SAMPLE=5 Epoch=2 Val Loss=0.0332', '\\n', 'Epoch=2 Spearman Corr=0.0000',\n'\\n', 'META_SAMPLE=5 Epoch=3 Val Loss=0.0222', '\\n', 'Epoch=3 Spearman\nCorr=-0.2000', '\\n', 'META_SAMPLE=5 Epoch=4 Val Loss=0.0185', '\\n', 'Epoch=4\nSpearman Corr=0.5000', '\\n', '=== Running META_SAMPLE = 10 ===', '\\n',\n'META_SAMPLE=10 Epoch=0 Val Loss=0.1963', '\\n', 'Epoch=0 Spearman Corr=-0.8545',\n'\\n', 'META_SAMPLE=10 Epoch=1 Val Loss=0.0950', '\\n', 'Epoch=1 Spearman\nCorr=-0.6848', '\\n', 'META_SAMPLE=10 Epoch=2 Val Loss=0.0687', '\\n', 'Epoch=2\nSpearman Corr=-0.4061', '\\n', 'META_SAMPLE=10 Epoch=3 Val Loss=0.0471', '\\n',\n'Epoch=3 Spearman Corr=0.4424', '\\n', 'META_SAMPLE=10 Epoch=4 Val Loss=0.0374',\n'\\n', 'Epoch=4 Spearman Corr=-0.0182', '\\n', '=== Running META_SAMPLE = 20 ===',\n'\\n', 'META_SAMPLE=20 Epoch=0 Val Loss=0.1690', '\\n', 'Epoch=0 Spearman\nCorr=-0.8436', '\\n', 'META_SAMPLE=20 Epoch=1 Val Loss=0.0919', '\\n', 'Epoch=1\nSpearman Corr=-0.4226', '\\n', 'META_SAMPLE=20 Epoch=2 Val Loss=0.0576', '\\n',\n'Epoch=2 Spearman Corr=-0.1594', '\\n', 'META_SAMPLE=20 Epoch=3 Val Loss=0.0392',\n'\\n', 'Epoch=3 Spearman Corr=0.5218', '\\n', 'META_SAMPLE=20 Epoch=4 Val\nLoss=0.0290', '\\n', 'Epoch=4 Spearman Corr=-0.2842', '\\n', '=== Running\nMETA_SAMPLE = 50 ===', '\\n', 'META_SAMPLE=50 Epoch=0 Val Loss=0.2081', '\\n',\n'Epoch=0 Spearman Corr=-0.5114', '\\n', 'META_SAMPLE=50 Epoch=1 Val Loss=0.1324',\n'\\n', 'Epoch=1 Spearman Corr=-0.0348', '\\n', 'META_SAMPLE=50 Epoch=2 Val\nLoss=0.0931', '\\n', 'Epoch=2 Spearman Corr=0.0412', '\\n', 'META_SAMPLE=50\nEpoch=3 Val Loss=0.0630', '\\n', 'Epoch=3 Spearman Corr=0.2032', '\\n',\n'META_SAMPLE=50 Epoch=4 Val Loss=0.0442', '\\n', 'Epoch=4 Spearman Corr=-0.1584',\n'\\n', '=== Running META_SAMPLE = 100 ===', '\\n', 'META_SAMPLE=100 Epoch=0 Val\nLoss=0.1440', '\\n', 'Epoch=0 Spearman Corr=-0.3340', '\\n', 'META_SAMPLE=100\nEpoch=1 Val Loss=0.0787', '\\n', 'Epoch=1 Spearman Corr=-0.1001', '\\n',\n'META_SAMPLE=100 Epoch=2 Val Loss=0.0510', '\\n', 'Epoch=2 Spearman\nCorr=-0.3024', '\\n', 'META_SAMPLE=100 Epoch=3 Val Loss=0.0427', '\\n', 'Epoch=3\nSpearman Corr=-0.1692', '\\n', 'META_SAMPLE=100 Epoch=4 Val Loss=0.0200', '\\n',\n'Epoch=4 Spearman Corr=0.2453', '\\n', 'Execution time: 4 seconds seconds (time\nlimit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Running DVN_STEPS_PER_EPOCH = 1 ===', '\\n',\n'Hyper 1 Epoch 0 | val_loss=0.1165', '\\n', 'Hyper 1 Epoch 0 | Spearman=-0.5564',\n'\\n', 'Hyper 1 Epoch 1 | val_loss=0.0611', '\\n', 'Hyper 1 Epoch 1 |\nSpearman=0.1880', '\\n', 'Hyper 1 Epoch 2 | val_loss=0.0396', '\\n', 'Hyper 1\nEpoch 2 | Spearman=-0.2496', '\\n', 'Hyper 1 Epoch 3 | val_loss=0.0211', '\\n',\n'Hyper 1 Epoch 3 | Spearman=0.1113', '\\n', 'Hyper 1 Epoch 4 | val_loss=0.0191',\n'\\n', 'Hyper 1 Epoch 4 | Spearman=0.1429', '\\n', '\\n=== Running\nDVN_STEPS_PER_EPOCH = 5 ===', '\\n', 'Hyper 5 Epoch 0 | val_loss=0.1476', '\\n',\n'Hyper 5 Epoch 0 | Spearman=-0.6075', '\\n', 'Hyper 5 Epoch 1 | val_loss=0.0923',\n'\\n', 'Hyper 5 Epoch 1 | Spearman=-0.2256', '\\n', 'Hyper 5 Epoch 2 |\nval_loss=0.0554', '\\n', 'Hyper 5 Epoch 2 | Spearman=0.0241', '\\n', 'Hyper 5\nEpoch 3 | val_loss=0.0392', '\\n', 'Hyper 5 Epoch 3 | Spearman=0.2782', '\\n',\n'Hyper 5 Epoch 4 | val_loss=0.0239', '\\n', 'Hyper 5 Epoch 4 | Spearman=0.0617',\n'\\n', '\\n=== Running DVN_STEPS_PER_EPOCH = 10 ===', '\\n', 'Hyper 10 Epoch 0 |\nval_loss=0.2079', '\\n', 'Hyper 10 Epoch 0 | Spearman=0.0632', '\\n', 'Hyper 10\nEpoch 1 | val_loss=0.1048', '\\n', 'Hyper 10 Epoch 1 | Spearman=-0.2677', '\\n',\n'Hyper 10 Epoch 2 | val_loss=0.0661', '\\n', 'Hyper 10 Epoch 2 |\nSpearman=-0.2887', '\\n', 'Hyper 10 Epoch 3 | val_loss=0.0472', '\\n', 'Hyper 10\nEpoch 3 | Spearman=0.0617', '\\n', 'Hyper 10 Epoch 4 | val_loss=0.0324', '\\n',\n'Hyper 10 Epoch 4 | Spearman=0.0331', '\\n', '\\n=== Running DVN_STEPS_PER_EPOCH =\n20 ===', '\\n', 'Hyper 20 Epoch 0 | val_loss=0.1243', '\\n', 'Hyper 20 Epoch 0 |\nSpearman=-0.1880', '\\n', 'Hyper 20 Epoch 1 | val_loss=0.0762', '\\n', 'Hyper 20\nEpoch 1 | Spearman=0.0677', '\\n', 'Hyper 20 Epoch 2 | val_loss=0.0477', '\\n',\n'Hyper 20 Epoch 2 | Spearman=0.3278', '\\n', 'Hyper 20 Epoch 3 |\nval_loss=0.0273', '\\n', 'Hyper 20 Epoch 3 | Spearman=-0.3023', '\\n', 'Hyper 20\nEpoch 4 | val_loss=0.0223', '\\n', 'Hyper 20 Epoch 4 | Spearman=0.3955', '\\n',\n'Execution time: 2 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\u03b21=0.8 Epoch 0: val_loss=0.1061', '\\n', '\u03b21=0.8\nEpoch 0: Spearman Corr=-0.2752', '\\n', '\u03b21=0.8 Epoch 1: val_loss=0.0576', '\\n',\n'\u03b21=0.8 Epoch 1: Spearman Corr=-0.5985', '\\n', '\u03b21=0.8 Epoch 2:\nval_loss=0.0329', '\\n', '\u03b21=0.8 Epoch 2: Spearman Corr=-0.1805', '\\n', '\u03b21=0.8\nEpoch 3: val_loss=0.0225', '\\n', '\u03b21=0.8 Epoch 3: Spearman Corr=-0.0932', '\\n',\n'\u03b21=0.8 Epoch 4: val_loss=0.0177', '\\n', '\u03b21=0.8 Epoch 4: Spearman Corr=0.3594',\n'\\n', '\u03b21=0.9 Epoch 0: val_loss=0.1476', '\\n', '\u03b21=0.9 Epoch 0: Spearman\nCorr=-0.6075', '\\n', '\u03b21=0.9 Epoch 1: val_loss=0.0923', '\\n', '\u03b21=0.9 Epoch 1:\nSpearman Corr=-0.2256', '\\n', '\u03b21=0.9 Epoch 2: val_loss=0.0554', '\\n', '\u03b21=0.9\nEpoch 2: Spearman Corr=0.0241', '\\n', '\u03b21=0.9 Epoch 3: val_loss=0.0392', '\\n',\n'\u03b21=0.9 Epoch 3: Spearman Corr=0.2782', '\\n', '\u03b21=0.9 Epoch 4: val_loss=0.0239',\n'\\n', '\u03b21=0.9 Epoch 4: Spearman Corr=0.0617', '\\n', '\u03b21=0.99 Epoch 0:\nval_loss=0.2921', '\\n', '\u03b21=0.99 Epoch 0: Spearman Corr=0.2647', '\\n', '\u03b21=0.99\nEpoch 1: val_loss=0.1255', '\\n', '\u03b21=0.99 Epoch 1: Spearman Corr=0.3970', '\\n',\n'\u03b21=0.99 Epoch 2: val_loss=0.1462', '\\n', '\u03b21=0.99 Epoch 2: Spearman\nCorr=-0.6030', '\\n', '\u03b21=0.99 Epoch 3: val_loss=0.1356', '\\n', '\u03b21=0.99 Epoch 3:\nSpearman Corr=-0.2436', '\\n', '\u03b21=0.99 Epoch 4: val_loss=0.0562', '\\n', '\u03b21=0.99\nEpoch 4: Spearman Corr=0.2496', '\\n', 'Execution time: 2 seconds seconds (time\nlimit is an hour).']", "['\\n=== Tuning META_UPDATE_LR = 1.0e-04 ===', '\\n', 'MetaLR 1.0e-04, Epoch 0:\nVal Loss = 0.1165', '\\n', 'MetaLR 1.0e-04, Epoch 0: Spearman = -0.5835', '\\n',\n'MetaLR 1.0e-04, Epoch 1: Val Loss = 0.0612', '\\n', 'MetaLR 1.0e-04, Epoch 1:\nSpearman = 0.1850', '\\n', 'MetaLR 1.0e-04, Epoch 2: Val Loss = 0.0397', '\\n',\n'MetaLR 1.0e-04, Epoch 2: Spearman = -0.5008', '\\n', 'MetaLR 1.0e-04, Epoch 3:\nVal Loss = 0.0210', '\\n', 'MetaLR 1.0e-04, Epoch 3: Spearman = -0.1368', '\\n',\n'MetaLR 1.0e-04, Epoch 4: Val Loss = 0.0191', '\\n', 'MetaLR 1.0e-04, Epoch 4:\nSpearman = -0.1218', '\\n', '\\n=== Tuning META_UPDATE_LR = 4.0e-04 ===', '\\n',\n'MetaLR 4.0e-04, Epoch 0: Val Loss = 0.1476', '\\n', 'MetaLR 4.0e-04, Epoch 0:\nSpearman = -0.6165', '\\n', 'MetaLR 4.0e-04, Epoch 1: Val Loss = 0.0923', '\\n',\n'MetaLR 4.0e-04, Epoch 1: Spearman = -0.2000', '\\n', 'MetaLR 4.0e-04, Epoch 2:\nVal Loss = 0.0555', '\\n', 'MetaLR 4.0e-04, Epoch 2: Spearman = 0.0165', '\\n',\n'MetaLR 4.0e-04, Epoch 3: Val Loss = 0.0392', '\\n', 'MetaLR 4.0e-04, Epoch 3:\nSpearman = 0.3368', '\\n', 'MetaLR 4.0e-04, Epoch 4: Val Loss = 0.0239', '\\n',\n'MetaLR 4.0e-04, Epoch 4: Spearman = 0.0421', '\\n', '\\n=== Tuning META_UPDATE_LR\n= 1.6e-03 ===', '\\n', 'MetaLR 1.6e-03, Epoch 0: Val Loss = 0.2079', '\\n',\n'MetaLR 1.6e-03, Epoch 0: Spearman = 0.1023', '\\n', 'MetaLR 1.6e-03, Epoch 1:\nVal Loss = 0.1048', '\\n', 'MetaLR 1.6e-03, Epoch 1: Spearman = -0.0586', '\\n',\n'MetaLR 1.6e-03, Epoch 2: Val Loss = 0.0660', '\\n', 'MetaLR 1.6e-03, Epoch 2:\nSpearman = 0.1143', '\\n', 'MetaLR 1.6e-03, Epoch 3: Val Loss = 0.0473', '\\n',\n'MetaLR 1.6e-03, Epoch 3: Spearman = -0.2466', '\\n', 'MetaLR 1.6e-03, Epoch 4:\nVal Loss = 0.0325', '\\n', 'MetaLR 1.6e-03, Epoch 4: Spearman = 0.1203', '\\n',\n'\\n=== Tuning META_UPDATE_LR = 6.3e-03 ===', '\\n', 'MetaLR 6.3e-03, Epoch 0: Val\nLoss = 0.1243', '\\n', 'MetaLR 6.3e-03, Epoch 0: Spearman = -0.2857', '\\n',\n'MetaLR 6.3e-03, Epoch 1: Val Loss = 0.0763', '\\n', 'MetaLR 6.3e-03, Epoch 1:\nSpearman = -0.1699', '\\n', 'MetaLR 6.3e-03, Epoch 2: Val Loss = 0.0467', '\\n',\n'MetaLR 6.3e-03, Epoch 2: Spearman = -0.2571', '\\n', 'MetaLR 6.3e-03, Epoch 3:\nVal Loss = 0.0274', '\\n', 'MetaLR 6.3e-03, Epoch 3: Spearman = 0.2511', '\\n',\n'MetaLR 6.3e-03, Epoch 4: Val Loss = 0.0221', '\\n', 'MetaLR 6.3e-03, Epoch 4:\nSpearman = -0.3098', '\\n', '\\n=== Tuning META_UPDATE_LR = 2.5e-02 ===', '\\n',\n'MetaLR 2.5e-02, Epoch 0: Val Loss = 0.1563', '\\n', 'MetaLR 2.5e-02, Epoch 0:\nSpearman = 0.0090', '\\n', 'MetaLR 2.5e-02, Epoch 1: Val Loss = 0.0848', '\\n',\n'MetaLR 2.5e-02, Epoch 1: Spearman = 0.3669', '\\n', 'MetaLR 2.5e-02, Epoch 2:\nVal Loss = 0.0473', '\\n', 'MetaLR 2.5e-02, Epoch 2: Spearman = 0.0180', '\\n',\n'MetaLR 2.5e-02, Epoch 3: Val Loss = 0.0357', '\\n', 'MetaLR 2.5e-02, Epoch 3:\nSpearman = 0.4496', '\\n', 'MetaLR 2.5e-02, Epoch 4: Val Loss = 0.0282', '\\n',\n'MetaLR 2.5e-02, Epoch 4: Spearman = 0.5579', '\\n', '\\n=== Tuning META_UPDATE_LR\n= 1.0e-01 ===', '\\n', 'MetaLR 1.0e-01, Epoch 0: Val Loss = 0.2459', '\\n',\n'MetaLR 1.0e-01, Epoch 0: Spearman = -0.0797', '\\n', 'MetaLR 1.0e-01, Epoch 1:\nVal Loss = 0.1905', '\\n', 'MetaLR 1.0e-01, Epoch 1: Spearman = -0.5173', '\\n',\n'MetaLR 1.0e-01, Epoch 2: Val Loss = 0.1590', '\\n', 'MetaLR 1.0e-01, Epoch 2:\nSpearman = -0.0602', '\\n', 'MetaLR 1.0e-01, Epoch 3: Val Loss = 0.1310', '\\n',\n'MetaLR 1.0e-01, Epoch 3: Spearman = -0.3955', '\\n', 'MetaLR 1.0e-01, Epoch 4:\nVal Loss = 0.1139', '\\n', 'MetaLR 1.0e-01, Epoch 4: Spearman = 0.0872', '\\n',\n'Execution time: 3 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Starting sweep for DVN hidden size = 8', '\\n',\n'Size 8 Epoch 0: val_loss = 0.1166', '\\n', 'Size 8 Epoch 0: Spearman corr =\n-0.4165', '\\n', 'Size 8 Epoch 1: val_loss = 0.0659', '\\n', 'Size 8 Epoch 1:\nSpearman corr = -0.5955', '\\n', 'Size 8 Epoch 2: val_loss = 0.0459', '\\n', 'Size\n8 Epoch 2: Spearman corr = -0.2662', '\\n', 'Size 8 Epoch 3: val_loss = 0.0280',\n'\\n', 'Size 8 Epoch 3: Spearman corr = -0.0827', '\\n', 'Size 8 Epoch 4: val_loss\n= 0.0195', '\\n', 'Size 8 Epoch 4: Spearman corr = 0.2301', '\\n', 'Starting sweep\nfor DVN hidden size = 16', '\\n', 'Size 16 Epoch 0: val_loss = 0.1148', '\\n',\n'Size 16 Epoch 0: Spearman corr = 0.3023', '\\n', 'Size 16 Epoch 1: val_loss =\n0.0668', '\\n', 'Size 16 Epoch 1: Spearman corr = -0.3820', '\\n', 'Size 16 Epoch\n2: val_loss = 0.0368', '\\n', 'Size 16 Epoch 2: Spearman corr = 0.0060', '\\n',\n'Size 16 Epoch 3: val_loss = 0.0224', '\\n', 'Size 16 Epoch 3: Spearman corr =\n-0.3368', '\\n', 'Size 16 Epoch 4: val_loss = 0.0188', '\\n', 'Size 16 Epoch 4:\nSpearman corr = 0.0902', '\\n', 'Starting sweep for DVN hidden size = 32', '\\n',\n'Size 32 Epoch 0: val_loss = 0.0764', '\\n', 'Size 32 Epoch 0: Spearman corr =\n0.4782', '\\n', 'Size 32 Epoch 1: val_loss = 0.0396', '\\n', 'Size 32 Epoch 1:\nSpearman corr = 0.4902', '\\n', 'Size 32 Epoch 2: val_loss = 0.0262', '\\n', 'Size\n32 Epoch 2: Spearman corr = 0.1910', '\\n', 'Size 32 Epoch 3: val_loss = 0.0227',\n'\\n', 'Size 32 Epoch 3: Spearman corr = 0.0030', '\\n', 'Size 32 Epoch 4:\nval_loss = 0.0192', '\\n', 'Size 32 Epoch 4: Spearman corr = 0.0917', '\\n',\n'Starting sweep for DVN hidden size = 64', '\\n', 'Size 64 Epoch 0: val_loss =\n0.2320', '\\n', 'Size 64 Epoch 0: Spearman corr = 0.4887', '\\n', 'Size 64 Epoch\n1: val_loss = 0.1190', '\\n', 'Size 64 Epoch 1: Spearman corr = 0.2015', '\\n',\n'Size 64 Epoch 2: val_loss = 0.0874', '\\n', 'Size 64 Epoch 2: Spearman corr =\n0.0526', '\\n', 'Size 64 Epoch 3: val_loss = 0.0635', '\\n', 'Size 64 Epoch 3:\nSpearman corr = 0.0887', '\\n', 'Size 64 Epoch 4: val_loss = 0.0454', '\\n', 'Size\n64 Epoch 4: Spearman corr = -0.0526', '\\n', 'Execution time: 3 seconds seconds\n(time limit is an hour).']", "['Using device: cuda', '\\n', '[beta2_0.9] Epoch 0 Train 0.3546 Val 0.3644',\n'\\n', '[beta2_0.9] Epoch 0 Spearman Corr: 0.2241', '\\n', '[beta2_0.9] Epoch 1\nTrain 0.1999 Val 0.1739', '\\n', '[beta2_0.9] Epoch 1 Spearman Corr: -0.0977',\n'\\n', '[beta2_0.9] Epoch 2 Train 0.1324 Val 0.1374', '\\n', '[beta2_0.9] Epoch 2\nSpearman Corr: 0.0150', '\\n', '[beta2_0.9] Epoch 3 Train 0.0870 Val 0.0759',\n'\\n', '[beta2_0.9] Epoch 3 Spearman Corr: -0.0526', '\\n', '[beta2_0.9] Epoch 4\nTrain 0.0501 Val 0.0452', '\\n', '[beta2_0.9] Epoch 4 Spearman Corr: 0.2752',\n'\\n', '[beta2_0.98] Epoch 0 Train 0.2766 Val 0.2236', '\\n', '[beta2_0.98] Epoch\n0 Spearman Corr: -0.5218', '\\n', '[beta2_0.98] Epoch 1 Train 0.1432 Val 0.1413',\n'\\n', '[beta2_0.98] Epoch 1 Spearman Corr: -0.6015', '\\n', '[beta2_0.98] Epoch 2\nTrain 0.0949 Val 0.0815', '\\n', '[beta2_0.98] Epoch 2 Spearman Corr: -0.0511',\n'\\n', '[beta2_0.98] Epoch 3 Train 0.0522 Val 0.0577', '\\n', '[beta2_0.98] Epoch\n3 Spearman Corr: -0.1895', '\\n', '[beta2_0.98] Epoch 4 Train 0.0316 Val 0.0356',\n'\\n', '[beta2_0.98] Epoch 4 Spearman Corr: -0.1850', '\\n', '[beta2_0.999] Epoch\n0 Train 0.3700 Val 0.1834', '\\n', '[beta2_0.999] Epoch 0 Spearman Corr: 0.2346',\n'\\n', '[beta2_0.999] Epoch 1 Train 0.1428 Val 0.1653', '\\n', '[beta2_0.999]\nEpoch 1 Spearman Corr: -0.4346', '\\n', '[beta2_0.999] Epoch 2 Train 0.0912 Val\n0.1123', '\\n', '[beta2_0.999] Epoch 2 Spearman Corr: -0.1248', '\\n',\n'[beta2_0.999] Epoch 3 Train 0.0773 Val 0.0832', '\\n', '[beta2_0.999] Epoch 3\nSpearman Corr: -0.2932', '\\n', '[beta2_0.999] Epoch 4 Train 0.0575 Val 0.0627',\n'\\n', '[beta2_0.999] Epoch 4 Spearman Corr: 0.0030', '\\n', 'Execution time: 2\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '[E=5] Epoch 0: train_loss=0.3487,\nval_loss=0.3365', '\\n', '[E=5] Epoch 0: Spearman Corr=0.4150', '\\n', '[E=5]\nEpoch 1: train_loss=0.1902, val_loss=0.1624', '\\n', '[E=5] Epoch 1: Spearman\nCorr=-0.0887', '\\n', '[E=5] Epoch 2: train_loss=0.1317, val_loss=0.1312', '\\n',\n'[E=5] Epoch 2: Spearman Corr=-0.0767', '\\n', '[E=5] Epoch 3: train_loss=0.0938,\nval_loss=0.0887', '\\n', '[E=5] Epoch 3: Spearman Corr=-0.0992', '\\n', '[E=5]\nEpoch 4: train_loss=0.0635, val_loss=0.0659', '\\n', '[E=5] Epoch 4: Spearman\nCorr=-0.2677', '\\n', '[E=20] Epoch 0: train_loss=0.2759, val_loss=0.2223', '\\n',\n'[E=20] Epoch 0: Spearman Corr=-0.5053', '\\n', '[E=20] Epoch 1:\ntrain_loss=0.1427, val_loss=0.1415', '\\n', '[E=20] Epoch 1: Spearman\nCorr=-0.6120', '\\n', '[E=20] Epoch 2: train_loss=0.0948, val_loss=0.0836', '\\n',\n'[E=20] Epoch 2: Spearman Corr=0.0812', '\\n', '[E=20] Epoch 3:\ntrain_loss=0.0533, val_loss=0.0610', '\\n', '[E=20] Epoch 3: Spearman\nCorr=-0.0647', '\\n', '[E=20] Epoch 4: train_loss=0.0333, val_loss=0.0372', '\\n',\n'[E=20] Epoch 4: Spearman Corr=-0.0947', '\\n', '[E=20] Epoch 5:\ntrain_loss=0.0233, val_loss=0.0271', '\\n', '[E=20] Epoch 5: Spearman\nCorr=-0.2496', '\\n', '[E=20] Epoch 6: train_loss=0.0187, val_loss=0.0199', '\\n',\n'[E=20] Epoch 6: Spearman Corr=-0.0466', '\\n', '[E=20] Epoch 7:\ntrain_loss=0.0161, val_loss=0.0178', '\\n', '[E=20] Epoch 7: Spearman\nCorr=0.2421', '\\n', '[E=20] Epoch 8: train_loss=0.0144, val_loss=0.0165', '\\n',\n'[E=20] Epoch 8: Spearman Corr=0.1624', '\\n', '[E=20] Epoch 9:\ntrain_loss=0.0141, val_loss=0.0173', '\\n', '[E=20] Epoch 9: Spearman\nCorr=0.0797', '\\n', '[E=20] Epoch 10: train_loss=0.0125, val_loss=0.0135', '\\n',\n'[E=20] Epoch 10: Spearman Corr=0.2872', '\\n', '[E=20] Epoch 11:\ntrain_loss=0.0130, val_loss=0.0129', '\\n', '[E=20] Epoch 11: Spearman\nCorr=-0.2075', '\\n', '[E=20] Epoch 12: train_loss=0.0115, val_loss=0.0145',\n'\\n', '[E=20] Epoch 12: Spearman Corr=-0.1774', '\\n', '[E=20] Epoch 13:\ntrain_loss=0.0126, val_loss=0.0120', '\\n', '[E=20] Epoch 13: Spearman\nCorr=-0.0256', '\\n', '[E=20] Epoch 14: train_loss=0.0112, val_loss=0.0125',\n'\\n', '[E=20] Epoch 14: Spearman Corr=-0.0586', '\\n', '[E=20] Epoch 15:\ntrain_loss=0.0123, val_loss=0.0149', '\\n', '[E=20] Epoch 15: Spearman\nCorr=0.2331', '\\n', '[E=20] Epoch 16: train_loss=0.0118, val_loss=0.0129', '\\n',\n'[E=20] Epoch 16: Spearman Corr=-0.0135', '\\n', '[E=20] Epoch 17:\ntrain_loss=0.0130, val_loss=0.0118', '\\n', '[E=20] Epoch 17: Spearman\nCorr=0.3143', '\\n', '[E=20] Epoch 18: train_loss=0.0127, val_loss=0.0154', '\\n',\n'[E=20] Epoch 18: Spearman Corr=0.0000', '\\n', '[E=20] Epoch 19:\ntrain_loss=0.0123, val_loss=0.0121', '\\n', '[E=20] Epoch 19: Spearman\nCorr=-0.1564', '\\n', '[E=50] Epoch 0: train_loss=0.3454, val_loss=0.2251', '\\n',\n'[E=50] Epoch 0: Spearman Corr=0.4511', '\\n', '[E=50] Epoch 1:\ntrain_loss=0.1393, val_loss=0.1486', '\\n', '[E=50] Epoch 1: Spearman\nCorr=0.5699', '\\n', '[E=50] Epoch 2: train_loss=0.0907, val_loss=0.0953', '\\n',\n'[E=50] Epoch 2: Spearman Corr=0.7053', '\\n', '[E=50] Epoch 3:\ntrain_loss=0.0684, val_loss=0.0692', '\\n', '[E=50] Epoch 3: Spearman\nCorr=0.1008', '\\n', '[E=50] Epoch 4: train_loss=0.0478, val_loss=0.0508', '\\n',\n'[E=50] Epoch 4: Spearman Corr=-0.5820', '\\n', '[E=50] Epoch 5:\ntrain_loss=0.0364, val_loss=0.0422', '\\n', '[E=50] Epoch 5: Spearman\nCorr=0.0977', '\\n', '[E=50] Epoch 6: train_loss=0.0284, val_loss=0.0344', '\\n',\n'[E=50] Epoch 6: Spearman Corr=-0.0301', '\\n', '[E=50] Epoch 7:\ntrain_loss=0.0242, val_loss=0.0337', '\\n', '[E=50] Epoch 7: Spearman\nCorr=0.1203', '\\n', '[E=50] Epoch 8: train_loss=0.0220, val_loss=0.0278', '\\n',\n'[E=50] Epoch 8: Spearman Corr=-0.0135', '\\n', '[E=50] Epoch 9:\ntrain_loss=0.0202, val_loss=0.0248', '\\n', '[E=50] Epoch 9: Spearman\nCorr=-0.0571', '\\n', '[E=50] Epoch 10: train_loss=0.0180, val_loss=0.0233',\n'\\n', '[E=50] Epoch 10: Spearman Corr=-0.0120', '\\n', '[E=50] Epoch 11:\ntrain_loss=0.0158, val_loss=0.0219', '\\n', '[E=50] Epoch 11: Spearman\nCorr=0.1383', '\\n', '[E=50] Epoch 12: train_loss=0.0156, val_loss=0.0185', '\\n',\n'[E=50] Epoch 12: Spearman Corr=0.1970', '\\n', '[E=50] Epoch 13:\ntrain_loss=0.0151, val_loss=0.0178', '\\n', '[E=50] Epoch 13: Spearman\nCorr=0.2752', '\\n', '[E=50] Epoch 14: train_loss=0.0133, val_loss=0.0163', '\\n',\n'[E=50] Epoch 14: Spearman Corr=-0.0511', '\\n', '[E=50] Epoch 15:\ntrain_loss=0.0132, val_loss=0.0189', '\\n', '[E=50] Epoch 15: Spearman\nCorr=0.0000', '\\n', '[E=50] Epoch 16: train_loss=0.0132, val_loss=0.0160', '\\n',\n'[E=50] Epoch 16: Spearman Corr=0.1669', '\\n', '[E=50] Epoch 17:\ntrain_loss=0.0131, val_loss=0.0147', '\\n', '[E=50] Epoch 17: Spearman\nCorr=-0.4226', '\\n', '[E=50] Epoch 18: train_loss=0.0118, val_loss=0.0141',\n'\\n', '[E=50] Epoch 18: Spearman Corr=-0.1038', '\\n', '[E=50] Epoch 19:\ntrain_loss=0.0116, val_loss=0.0150', '\\n', '[E=50] Epoch 19: Spearman\nCorr=0.2331', '\\n', '[E=50] Epoch 20: train_loss=0.0113, val_loss=0.0132', '\\n',\n'[E=50] Epoch 20: Spearman Corr=0.0692', '\\n', '[E=50] Epoch 21:\ntrain_loss=0.0110, val_loss=0.0131', '\\n', '[E=50] Epoch 21: Spearman\nCorr=0.1038', '\\n', '[E=50] Epoch 22: train_loss=0.0107, val_loss=0.0141', '\\n',\n'[E=50] Epoch 22: Spearman Corr=0.0241', '\\n', '[E=50] Epoch 23:\ntrain_loss=0.0111, val_loss=0.0142', '\\n', '[E=50] Epoch 23: Spearman\nCorr=-0.1008', '\\n', '[E=50] Epoch 24: train_loss=0.0117, val_loss=0.0132',\n'\\n', '[E=50] Epoch 24: Spearman Corr=0.1955', '\\n', '[E=50] Epoch 25:\ntrain_loss=0.0113, val_loss=0.0124', '\\n', '[E=50] Epoch 25: Spearman\nCorr=0.2556', '\\n', '[E=50] Epoch 26: train_loss=0.0105, val_loss=0.0121', '\\n',\n'[E=50] Epoch 26: Spearman Corr=-0.6707', '\\n', '[E=50] Epoch 27:\ntrain_loss=0.0110, val_loss=0.0119', '\\n', '[E=50] Epoch 27: Spearman\nCorr=0.0466', '\\n', '[E=50] Epoch 28: train_loss=0.0111, val_loss=0.0141', '\\n',\n'[E=50] Epoch 28: Spearman Corr=-0.0135', '\\n', '[E=50] Epoch 29:\ntrain_loss=0.0118, val_loss=0.0122', '\\n', '[E=50] Epoch 29: Spearman\nCorr=-0.0842', '\\n', '[E=50] Epoch 30: train_loss=0.0114, val_loss=0.0118',\n'\\n', '[E=50] Epoch 30: Spearman Corr=0.2301', '\\n', '[E=50] Epoch 31:\ntrain_loss=0.0106, val_loss=0.0120', '\\n', '[E=50] Epoch 31: Spearman\nCorr=-0.2571', '\\n', '[E=50] Epoch 32: train_loss=0.0104, val_loss=0.0115',\n'\\n', '[E=50] Epoch 32: Spearman Corr=-0.3955', '\\n', '[E=50] Epoch 33:\ntrain_loss=0.0108, val_loss=0.0138', '\\n', '[E=50] Epoch 33: Spearman\nCorr=0.5669', '\\n', '[E=50] Epoch 34: train_loss=0.0101, val_loss=0.0123', '\\n',\n'[E=50] Epoch 34: Spearman Corr=-0.3143', '\\n', '[E=50] Epoch 35:\ntrain_loss=0.0112, val_loss=0.0124', '\\n', '[E=50] Epoch 35: Spearman\nCorr=-0.0075', '\\n', '[E=50] Epoch 36: train_loss=0.0109, val_loss=0.0116',\n'\\n', '[E=50] Epoch 36: Spearman Corr=-0.2917', '\\n', '[E=50] Epoch 37:\ntrain_loss=0.0104, val_loss=0.0114', '\\n', '[E=50] Epoch 37: Spearman\nCorr=-0.0331', '\\n', '[E=50] Epoch 38: train_loss=0.0098, val_loss=0.0114',\n'\\n', '[E=50] Epoch 38: Spearman Corr=-0.0662', '\\n', '[E=50] Epoch 39:\ntrain_loss=0.0097, val_loss=0.0114', '\\n', '[E=50] Epoch 39: Spearman\nCorr=0.1504', '\\n', '[E=50] Epoch 40: train_loss=0.0096, val_loss=0.0122', '\\n',\n'[E=50] Epoch 40: Spearman Corr=0.1865', '\\n', '[E=50] Epoch 41:\ntrain_loss=0.0110, val_loss=0.0130', '\\n', '[E=50] Epoch 41: Spearman\nCorr=0.1729', '\\n', '[E=50] Epoch 42: train_loss=0.0107, val_loss=0.0128', '\\n',\n'[E=50] Epoch 42: Spearman Corr=0.1639', '\\n', '[E=50] Epoch 43:\ntrain_loss=0.0108, val_loss=0.0122', '\\n', '[E=50] Epoch 43: Spearman\nCorr=-0.1865', '\\n', '[E=50] Epoch 44: train_loss=0.0102, val_loss=0.0110',\n'\\n', '[E=50] Epoch 44: Spearman Corr=-0.4135', '\\n', '[E=50] Epoch 45:\ntrain_loss=0.0097, val_loss=0.0113', '\\n', '[E=50] Epoch 45: Spearman\nCorr=-0.1564', '\\n', '[E=50] Epoch 46: train_loss=0.0100, val_loss=0.0123',\n'\\n', '[E=50] Epoch 46: Spearman Corr=-0.1323', '\\n', '[E=50] Epoch 47:\ntrain_loss=0.0103, val_loss=0.0114', '\\n', '[E=50] Epoch 47: Spearman\nCorr=-0.0782', '\\n', '[E=50] Epoch 48: train_loss=0.0101, val_loss=0.0113',\n'\\n', '[E=50] Epoch 48: Spearman Corr=0.2842', '\\n', '[E=50] Epoch 49:\ntrain_loss=0.0112, val_loss=0.0135', '\\n', '[E=50] Epoch 49: Spearman\nCorr=-0.0947', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 7\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '[E=5] Epoch 0: train_loss=0.3164,\nval_loss=0.1677', '\\n', '[E=5] Epoch 0: Spearman Corr=-0.1083', '\\n', '[E=5]\nEpoch 1: train_loss=0.1441, val_loss=0.1191', '\\n', '[E=5] Epoch 1: Spearman\nCorr=0.1383', '\\n', '[E=5] Epoch 2: train_loss=0.0991, val_loss=0.0806', '\\n',\n'[E=5] Epoch 2: Spearman Corr=0.2286', '\\n', '[E=5] Epoch 3: train_loss=0.0691,\nval_loss=0.0639', '\\n', '[E=5] Epoch 3: Spearman Corr=0.5023', '\\n', '[E=5]\nEpoch 4: train_loss=0.0464, val_loss=0.0406', '\\n', '[E=5] Epoch 4: Spearman\nCorr=0.2556', '\\n', '[E=20] Epoch 0: train_loss=0.1758, val_loss=0.1491', '\\n',\n'[E=20] Epoch 0: Spearman Corr=0.2647', '\\n', '[E=20] Epoch 1:\ntrain_loss=0.0979, val_loss=0.1008', '\\n', '[E=20] Epoch 1: Spearman\nCorr=0.3639', '\\n', '[E=20] Epoch 2: train_loss=0.0618, val_loss=0.0517', '\\n',\n'[E=20] Epoch 2: Spearman Corr=0.2662', '\\n', '[E=20] Epoch 3:\ntrain_loss=0.0379, val_loss=0.0334', '\\n', '[E=20] Epoch 3: Spearman\nCorr=-0.4211', '\\n', '[E=20] Epoch 4: train_loss=0.0280, val_loss=0.0263', '\\n',\n'[E=20] Epoch 4: Spearman Corr=0.1519', '\\n', '[E=20] Epoch 5:\ntrain_loss=0.0208, val_loss=0.0231', '\\n', '[E=20] Epoch 5: Spearman\nCorr=-0.0842', '\\n', '[E=20] Epoch 6: train_loss=0.0176, val_loss=0.0193', '\\n',\n'[E=20] Epoch 6: Spearman Corr=0.1068', '\\n', '[E=20] Epoch 7:\ntrain_loss=0.0166, val_loss=0.0169', '\\n', '[E=20] Epoch 7: Spearman\nCorr=0.4857', '\\n', '[E=20] Epoch 8: train_loss=0.0139, val_loss=0.0171', '\\n',\n'[E=20] Epoch 8: Spearman Corr=0.0030', '\\n', '[E=20] Epoch 9:\ntrain_loss=0.0129, val_loss=0.0167', '\\n', '[E=20] Epoch 9: Spearman\nCorr=0.2075', '\\n', '[E=20] Epoch 10: train_loss=0.0128, val_loss=0.0158', '\\n',\n'[E=20] Epoch 10: Spearman Corr=0.1609', '\\n', '[E=20] Epoch 11:\ntrain_loss=0.0119, val_loss=0.0166', '\\n', '[E=20] Epoch 11: Spearman\nCorr=0.2571', '\\n', '[E=20] Epoch 12: train_loss=0.0119, val_loss=0.0162', '\\n',\n'[E=20] Epoch 12: Spearman Corr=0.0872', '\\n', '[E=20] Epoch 13:\ntrain_loss=0.0112, val_loss=0.0157', '\\n', '[E=20] Epoch 13: Spearman\nCorr=-0.1323', '\\n', '[E=20] Epoch 14: train_loss=0.0118, val_loss=0.0138',\n'\\n', '[E=20] Epoch 14: Spearman Corr=-0.0947', '\\n', '[E=20] Epoch 15:\ntrain_loss=0.0112, val_loss=0.0129', '\\n', '[E=20] Epoch 15: Spearman\nCorr=0.2150', '\\n', '[E=20] Epoch 16: train_loss=0.0121, val_loss=0.0118', '\\n',\n'[E=20] Epoch 16: Spearman Corr=-0.2211', '\\n', '[E=20] Epoch 17:\ntrain_loss=0.0107, val_loss=0.0128', '\\n', '[E=20] Epoch 17: Spearman\nCorr=0.1113', '\\n', '[E=20] Epoch 18: train_loss=0.0106, val_loss=0.0167', '\\n',\n'[E=20] Epoch 18: Spearman Corr=0.3263', '\\n', '[E=20] Epoch 19:\ntrain_loss=0.0117, val_loss=0.0164', '\\n', '[E=20] Epoch 19: Spearman\nCorr=0.0586', '\\n', '[E=50] Epoch 0: train_loss=0.2935, val_loss=0.1890', '\\n',\n'[E=50] Epoch 0: Spearman Corr=-0.1323', '\\n', '[E=50] Epoch 1:\ntrain_loss=0.1509, val_loss=0.1171', '\\n', '[E=50] Epoch 1: Spearman\nCorr=0.1729', '\\n', '[E=50] Epoch 2: train_loss=0.1024, val_loss=0.0972', '\\n',\n'[E=50] Epoch 2: Spearman Corr=0.0015', '\\n', '[E=50] Epoch 3:\ntrain_loss=0.0723, val_loss=0.0708', '\\n', '[E=50] Epoch 3: Spearman\nCorr=-0.0165', '\\n', '[E=50] Epoch 4: train_loss=0.0510, val_loss=0.0497', '\\n',\n'[E=50] Epoch 4: Spearman Corr=-0.1564', '\\n', '[E=50] Epoch 5:\ntrain_loss=0.0335, val_loss=0.0336', '\\n', '[E=50] Epoch 5: Spearman\nCorr=0.1895', '\\n', '[E=50] Epoch 6: train_loss=0.0253, val_loss=0.0260', '\\n',\n'[E=50] Epoch 6: Spearman Corr=-0.3459', '\\n', '[E=50] Epoch 7:\ntrain_loss=0.0208, val_loss=0.0227', '\\n', '[E=50] Epoch 7: Spearman\nCorr=0.3083', '\\n', '[E=50] Epoch 8: train_loss=0.0180, val_loss=0.0244', '\\n',\n'[E=50] Epoch 8: Spearman Corr=-0.1729', '\\n', '[E=50] Epoch 9:\ntrain_loss=0.0162, val_loss=0.0192', '\\n', '[E=50] Epoch 9: Spearman\nCorr=-0.3008', '\\n', '[E=50] Epoch 10: train_loss=0.0143, val_loss=0.0156',\n'\\n', '[E=50] Epoch 10: Spearman Corr=0.1820', '\\n', '[E=50] Epoch 11:\ntrain_loss=0.0133, val_loss=0.0171', '\\n', '[E=50] Epoch 11: Spearman\nCorr=-0.0857', '\\n', '[E=50] Epoch 12: train_loss=0.0125, val_loss=0.0150',\n'\\n', '[E=50] Epoch 12: Spearman Corr=0.1263', '\\n', '[E=50] Epoch 13:\ntrain_loss=0.0119, val_loss=0.0147', '\\n', '[E=50] Epoch 13: Spearman\nCorr=-0.0947', '\\n', '[E=50] Epoch 14: train_loss=0.0123, val_loss=0.0137',\n'\\n', '[E=50] Epoch 14: Spearman Corr=-0.4361', '\\n', '[E=50] Epoch 15:\ntrain_loss=0.0114, val_loss=0.0143', '\\n', '[E=50] Epoch 15: Spearman\nCorr=-0.1474', '\\n', '[E=50] Epoch 16: train_loss=0.0113, val_loss=0.0151',\n'\\n', '[E=50] Epoch 16: Spearman Corr=-0.2767', '\\n', '[E=50] Epoch 17:\ntrain_loss=0.0113, val_loss=0.0152', '\\n', '[E=50] Epoch 17: Spearman\nCorr=-0.2977', '\\n', '[E=50] Epoch 18: train_loss=0.0107, val_loss=0.0134',\n'\\n', '[E=50] Epoch 18: Spearman Corr=0.1489', '\\n', '[E=50] Epoch 19:\ntrain_loss=0.0105, val_loss=0.0151', '\\n', '[E=50] Epoch 19: Spearman\nCorr=0.2767', '\\n', '[E=50] Epoch 20: train_loss=0.0110, val_loss=0.0112', '\\n',\n'[E=50] Epoch 20: Spearman Corr=-0.1805', '\\n', '[E=50] Epoch 21:\ntrain_loss=0.0109, val_loss=0.0111', '\\n', '[E=50] Epoch 21: Spearman\nCorr=-0.0331', '\\n', '[E=50] Epoch 22: train_loss=0.0107, val_loss=0.0131',\n'\\n', '[E=50] Epoch 22: Spearman Corr=-0.0932', '\\n', '[E=50] Epoch 23:\ntrain_loss=0.0104, val_loss=0.0112', '\\n', '[E=50] Epoch 23: Spearman\nCorr=0.2752', '\\n', '[E=50] Epoch 24: train_loss=0.0105, val_loss=0.0107', '\\n',\n'[E=50] Epoch 24: Spearman Corr=-0.0887', '\\n', '[E=50] Epoch 25:\ntrain_loss=0.0101, val_loss=0.0114', '\\n', '[E=50] Epoch 25: Spearman\nCorr=-0.3068', '\\n', '[E=50] Epoch 26: train_loss=0.0101, val_loss=0.0108',\n'\\n', '[E=50] Epoch 26: Spearman Corr=0.2737', '\\n', '[E=50] Epoch 27:\ntrain_loss=0.0101, val_loss=0.0106', '\\n', '[E=50] Epoch 27: Spearman\nCorr=-0.3353', '\\n', '[E=50] Epoch 28: train_loss=0.0098, val_loss=0.0133',\n'\\n', '[E=50] Epoch 28: Spearman Corr=-0.2722', '\\n', '[E=50] Epoch 29:\ntrain_loss=0.0109, val_loss=0.0105', '\\n', '[E=50] Epoch 29: Spearman\nCorr=-0.5880', '\\n', '[E=50] Epoch 30: train_loss=0.0101, val_loss=0.0105',\n'\\n', '[E=50] Epoch 30: Spearman Corr=-0.0857', '\\n', '[E=50] Epoch 31:\ntrain_loss=0.0097, val_loss=0.0114', '\\n', '[E=50] Epoch 31: Spearman\nCorr=-0.2135', '\\n', '[E=50] Epoch 32: train_loss=0.0096, val_loss=0.0107',\n'\\n', '[E=50] Epoch 32: Spearman Corr=-0.1504', '\\n', '[E=50] Epoch 33:\ntrain_loss=0.0103, val_loss=0.0110', '\\n', '[E=50] Epoch 33: Spearman\nCorr=0.0962', '\\n', '[E=50] Epoch 34: train_loss=0.0102, val_loss=0.0107', '\\n',\n'[E=50] Epoch 34: Spearman Corr=0.3233', '\\n', '[E=50] Epoch 35:\ntrain_loss=0.0095, val_loss=0.0113', '\\n', '[E=50] Epoch 35: Spearman\nCorr=-0.1023', '\\n', '[E=50] Epoch 36: train_loss=0.0098, val_loss=0.0113',\n'\\n', '[E=50] Epoch 36: Spearman Corr=0.0301', '\\n', '[E=50] Epoch 37:\ntrain_loss=0.0105, val_loss=0.0119', '\\n', '[E=50] Epoch 37: Spearman\nCorr=-0.2045', '\\n', '[E=50] Epoch 38: train_loss=0.0109, val_loss=0.0102',\n'\\n', '[E=50] Epoch 38: Spearman Corr=0.4226', '\\n', '[E=50] Epoch 39:\ntrain_loss=0.0100, val_loss=0.0107', '\\n', '[E=50] Epoch 39: Spearman\nCorr=0.2075', '\\n', '[E=50] Epoch 40: train_loss=0.0097, val_loss=0.0123', '\\n',\n'[E=50] Epoch 40: Spearman Corr=-0.0406', '\\n', '[E=50] Epoch 41:\ntrain_loss=0.0101, val_loss=0.0102', '\\n', '[E=50] Epoch 41: Spearman\nCorr=0.0857', '\\n', '[E=50] Epoch 42: train_loss=0.0096, val_loss=0.0115', '\\n',\n'[E=50] Epoch 42: Spearman Corr=0.0872', '\\n', '[E=50] Epoch 43:\ntrain_loss=0.0102, val_loss=0.0106', '\\n', '[E=50] Epoch 43: Spearman\nCorr=-0.1128', '\\n', '[E=50] Epoch 44: train_loss=0.0098, val_loss=0.0108',\n'\\n', '[E=50] Epoch 44: Spearman Corr=0.0707', '\\n', '[E=50] Epoch 45:\ntrain_loss=0.0099, val_loss=0.0114', '\\n', '[E=50] Epoch 45: Spearman\nCorr=-0.2135', '\\n', '[E=50] Epoch 46: train_loss=0.0111, val_loss=0.0102',\n'\\n', '[E=50] Epoch 46: Spearman Corr=-0.0361', '\\n', '[E=50] Epoch 47:\ntrain_loss=0.0103, val_loss=0.0119', '\\n', '[E=50] Epoch 47: Spearman\nCorr=-0.1233', '\\n', '[E=50] Epoch 48: train_loss=0.0099, val_loss=0.0117',\n'\\n', '[E=50] Epoch 48: Spearman Corr=-0.2526', '\\n', '[E=50] Epoch 49:\ntrain_loss=0.0107, val_loss=0.0146', '\\n', '[E=50] Epoch 49: Spearman\nCorr=-0.2105', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 7\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '[E=5] Epoch 0: train_loss=0.3929,\nval_loss=0.2949', '\\n', '[E=5] Epoch 0: Spearman Corr=-0.2887', '\\n', '[E=5]\nEpoch 1: train_loss=0.1712, val_loss=0.1280', '\\n', '[E=5] Epoch 1: Spearman\nCorr=-0.1053', '\\n', '[E=5] Epoch 2: train_loss=0.1198, val_loss=0.1059', '\\n',\n'[E=5] Epoch 2: Spearman Corr=0.0481', '\\n', '[E=5] Epoch 3: train_loss=0.0846,\nval_loss=0.0681', '\\n', '[E=5] Epoch 3: Spearman Corr=0.0602', '\\n', '[E=5]\nEpoch 4: train_loss=0.0604, val_loss=0.0527', '\\n', '[E=5] Epoch 4: Spearman\nCorr=0.2692', '\\n', '[E=20] Epoch 0: train_loss=0.3327, val_loss=0.3138', '\\n',\n'[E=20] Epoch 0: Spearman Corr=-0.3624', '\\n', '[E=20] Epoch 1:\ntrain_loss=0.1817, val_loss=0.1452', '\\n', '[E=20] Epoch 1: Spearman\nCorr=-0.2406', '\\n', '[E=20] Epoch 2: train_loss=0.1284, val_loss=0.1138', '\\n',\n'[E=20] Epoch 2: Spearman Corr=-0.0977', '\\n', '[E=20] Epoch 3:\ntrain_loss=0.0913, val_loss=0.0727', '\\n', '[E=20] Epoch 3: Spearman\nCorr=0.3053', '\\n', '[E=20] Epoch 4: train_loss=0.0636, val_loss=0.0509', '\\n',\n'[E=20] Epoch 4: Spearman Corr=0.0827', '\\n', '[E=20] Epoch 5:\ntrain_loss=0.0451, val_loss=0.0386', '\\n', '[E=20] Epoch 5: Spearman\nCorr=-0.2241', '\\n', '[E=20] Epoch 6: train_loss=0.0334, val_loss=0.0276', '\\n',\n'[E=20] Epoch 6: Spearman Corr=0.5263', '\\n', '[E=20] Epoch 7:\ntrain_loss=0.0247, val_loss=0.0244', '\\n', '[E=20] Epoch 7: Spearman\nCorr=-0.0947', '\\n', '[E=20] Epoch 8: train_loss=0.0215, val_loss=0.0218', '\\n',\n'[E=20] Epoch 8: Spearman Corr=0.0962', '\\n', '[E=20] Epoch 9:\ntrain_loss=0.0202, val_loss=0.0222', '\\n', '[E=20] Epoch 9: Spearman\nCorr=-0.0436', '\\n', '[E=20] Epoch 10: train_loss=0.0197, val_loss=0.0179',\n'\\n', '[E=20] Epoch 10: Spearman Corr=0.5474', '\\n', '[E=20] Epoch 11:\ntrain_loss=0.0168, val_loss=0.0176', '\\n', '[E=20] Epoch 11: Spearman\nCorr=-0.1820', '\\n', '[E=20] Epoch 12: train_loss=0.0147, val_loss=0.0152',\n'\\n', '[E=20] Epoch 12: Spearman Corr=-0.2015', '\\n', '[E=20] Epoch 13:\ntrain_loss=0.0138, val_loss=0.0162', '\\n', '[E=20] Epoch 13: Spearman\nCorr=-0.1835', '\\n', '[E=20] Epoch 14: train_loss=0.0130, val_loss=0.0146',\n'\\n', '[E=20] Epoch 14: Spearman Corr=-0.5835', '\\n', '[E=20] Epoch 15:\ntrain_loss=0.0132, val_loss=0.0157', '\\n', '[E=20] Epoch 15: Spearman\nCorr=0.0241', '\\n', '[E=20] Epoch 16: train_loss=0.0133, val_loss=0.0130', '\\n',\n'[E=20] Epoch 16: Spearman Corr=0.2030', '\\n', '[E=20] Epoch 17:\ntrain_loss=0.0131, val_loss=0.0129', '\\n', '[E=20] Epoch 17: Spearman\nCorr=0.2376', '\\n', '[E=20] Epoch 18: train_loss=0.0123, val_loss=0.0120', '\\n',\n'[E=20] Epoch 18: Spearman Corr=-0.1759', '\\n', '[E=20] Epoch 19:\ntrain_loss=0.0136, val_loss=0.0162', '\\n', '[E=20] Epoch 19: Spearman\nCorr=-0.1128', '\\n', '[E=50] Epoch 0: train_loss=0.2189, val_loss=0.1671', '\\n',\n'[E=50] Epoch 0: Spearman Corr=0.1444', '\\n', '[E=50] Epoch 1:\ntrain_loss=0.1336, val_loss=0.1077', '\\n', '[E=50] Epoch 1: Spearman\nCorr=0.3233', '\\n', '[E=50] Epoch 2: train_loss=0.0969, val_loss=0.0795', '\\n',\n'[E=50] Epoch 2: Spearman Corr=0.1910', '\\n', '[E=50] Epoch 3:\ntrain_loss=0.0655, val_loss=0.0578', '\\n', '[E=50] Epoch 3: Spearman\nCorr=0.1789', '\\n', '[E=50] Epoch 4: train_loss=0.0476, val_loss=0.0375', '\\n',\n'[E=50] Epoch 4: Spearman Corr=0.0301', '\\n', '[E=50] Epoch 5:\ntrain_loss=0.0335, val_loss=0.0326', '\\n', '[E=50] Epoch 5: Spearman\nCorr=-0.1564', '\\n', '[E=50] Epoch 6: train_loss=0.0262, val_loss=0.0244', '\\n',\n'[E=50] Epoch 6: Spearman Corr=-0.2767', '\\n', '[E=50] Epoch 7:\ntrain_loss=0.0229, val_loss=0.0216', '\\n', '[E=50] Epoch 7: Spearman\nCorr=0.1865', '\\n', '[E=50] Epoch 8: train_loss=0.0210, val_loss=0.0195', '\\n',\n'[E=50] Epoch 8: Spearman Corr=-0.0511', '\\n', '[E=50] Epoch 9:\ntrain_loss=0.0193, val_loss=0.0179', '\\n', '[E=50] Epoch 9: Spearman\nCorr=0.0301', '\\n', '[E=50] Epoch 10: train_loss=0.0160, val_loss=0.0164', '\\n',\n'[E=50] Epoch 10: Spearman Corr=-0.3068', '\\n', '[E=50] Epoch 11:\ntrain_loss=0.0151, val_loss=0.0159', '\\n', '[E=50] Epoch 11: Spearman\nCorr=-0.1684', '\\n', '[E=50] Epoch 12: train_loss=0.0142, val_loss=0.0142',\n'\\n', '[E=50] Epoch 12: Spearman Corr=-0.0571', '\\n', '[E=50] Epoch 13:\ntrain_loss=0.0135, val_loss=0.0147', '\\n', '[E=50] Epoch 13: Spearman\nCorr=-0.2962', '\\n', '[E=50] Epoch 14: train_loss=0.0132, val_loss=0.0130',\n'\\n', '[E=50] Epoch 14: Spearman Corr=-0.6015', '\\n', '[E=50] Epoch 15:\ntrain_loss=0.0121, val_loss=0.0153', '\\n', '[E=50] Epoch 15: Spearman\nCorr=-0.3459', '\\n', '[E=50] Epoch 16: train_loss=0.0132, val_loss=0.0133',\n'\\n', '[E=50] Epoch 16: Spearman Corr=-0.0075', '\\n', '[E=50] Epoch 17:\ntrain_loss=0.0120, val_loss=0.0120', '\\n', '[E=50] Epoch 17: Spearman\nCorr=0.1759', '\\n', '[E=50] Epoch 18: train_loss=0.0111, val_loss=0.0124', '\\n',\n'[E=50] Epoch 18: Spearman Corr=-0.1233', '\\n', '[E=50] Epoch 19:\ntrain_loss=0.0116, val_loss=0.0117', '\\n', '[E=50] Epoch 19: Spearman\nCorr=0.1669', '\\n', '[E=50] Epoch 20: train_loss=0.0113, val_loss=0.0115', '\\n',\n'[E=50] Epoch 20: Spearman Corr=0.0917', '\\n', '[E=50] Epoch 21:\ntrain_loss=0.0121, val_loss=0.0173', '\\n', '[E=50] Epoch 21: Spearman\nCorr=-0.2090', '\\n', '[E=50] Epoch 22: train_loss=0.0125, val_loss=0.0121',\n'\\n', '[E=50] Epoch 22: Spearman Corr=-0.0451', '\\n', '[E=50] Epoch 23:\ntrain_loss=0.0115, val_loss=0.0109', '\\n', '[E=50] Epoch 23: Spearman\nCorr=-0.1654', '\\n', '[E=50] Epoch 24: train_loss=0.0108, val_loss=0.0117',\n'\\n', '[E=50] Epoch 24: Spearman Corr=-0.1203', '\\n', '[E=50] Epoch 25:\ntrain_loss=0.0111, val_loss=0.0109', '\\n', '[E=50] Epoch 25: Spearman\nCorr=0.1263', '\\n', '[E=50] Epoch 26: train_loss=0.0111, val_loss=0.0115', '\\n',\n'[E=50] Epoch 26: Spearman Corr=0.3699', '\\n', '[E=50] Epoch 27:\ntrain_loss=0.0109, val_loss=0.0113', '\\n', '[E=50] Epoch 27: Spearman\nCorr=0.3729', '\\n', '[E=50] Epoch 28: train_loss=0.0116, val_loss=0.0116', '\\n',\n'[E=50] Epoch 28: Spearman Corr=-0.1023', '\\n', '[E=50] Epoch 29:\ntrain_loss=0.0104, val_loss=0.0123', '\\n', '[E=50] Epoch 29: Spearman\nCorr=0.0000', '\\n', '[E=50] Epoch 30: train_loss=0.0103, val_loss=0.0128', '\\n',\n'[E=50] Epoch 30: Spearman Corr=-0.3068', '\\n', '[E=50] Epoch 31:\ntrain_loss=0.0106, val_loss=0.0111', '\\n', '[E=50] Epoch 31: Spearman\nCorr=0.2797', '\\n', '[E=50] Epoch 32: train_loss=0.0111, val_loss=0.0119', '\\n',\n'[E=50] Epoch 32: Spearman Corr=0.0421', '\\n', '[E=50] Epoch 33:\ntrain_loss=0.0106, val_loss=0.0117', '\\n', '[E=50] Epoch 33: Spearman\nCorr=-0.0887', '\\n', '[E=50] Epoch 34: train_loss=0.0102, val_loss=0.0107',\n'\\n', '[E=50] Epoch 34: Spearman Corr=-0.1459', '\\n', '[E=50] Epoch 35:\ntrain_loss=0.0103, val_loss=0.0107', '\\n', '[E=50] Epoch 35: Spearman\nCorr=0.1970', '\\n', '[E=50] Epoch 36: train_loss=0.0110, val_loss=0.0114', '\\n',\n'[E=50] Epoch 36: Spearman Corr=-0.1203', '\\n', '[E=50] Epoch 37:\ntrain_loss=0.0103, val_loss=0.0117', '\\n', '[E=50] Epoch 37: Spearman\nCorr=0.1895', '\\n', '[E=50] Epoch 38: train_loss=0.0102, val_loss=0.0110', '\\n',\n'[E=50] Epoch 38: Spearman Corr=0.0090', '\\n', '[E=50] Epoch 39:\ntrain_loss=0.0103, val_loss=0.0109', '\\n', '[E=50] Epoch 39: Spearman\nCorr=-0.2962', '\\n', '[E=50] Epoch 40: train_loss=0.0101, val_loss=0.0109',\n'\\n', '[E=50] Epoch 40: Spearman Corr=0.1143', '\\n', '[E=50] Epoch 41:\ntrain_loss=0.0101, val_loss=0.0111', '\\n', '[E=50] Epoch 41: Spearman\nCorr=0.3248', '\\n', '[E=50] Epoch 42: train_loss=0.0101, val_loss=0.0111', '\\n',\n'[E=50] Epoch 42: Spearman Corr=-0.1414', '\\n', '[E=50] Epoch 43:\ntrain_loss=0.0104, val_loss=0.0105', '\\n', '[E=50] Epoch 43: Spearman\nCorr=-0.1353', '\\n', '[E=50] Epoch 44: train_loss=0.0102, val_loss=0.0114',\n'\\n', '[E=50] Epoch 44: Spearman Corr=0.4256', '\\n', '[E=50] Epoch 45:\ntrain_loss=0.0103, val_loss=0.0109', '\\n', '[E=50] Epoch 45: Spearman\nCorr=0.1820', '\\n', '[E=50] Epoch 46: train_loss=0.0111, val_loss=0.0136', '\\n',\n'[E=50] Epoch 46: Spearman Corr=0.0376', '\\n', '[E=50] Epoch 47:\ntrain_loss=0.0115, val_loss=0.0137', '\\n', '[E=50] Epoch 47: Spearman\nCorr=0.2662', '\\n', '[E=50] Epoch 48: train_loss=0.0129, val_loss=0.0119', '\\n',\n'[E=50] Epoch 48: Spearman Corr=0.0376', '\\n', '[E=50] Epoch 49:\ntrain_loss=0.0117, val_loss=0.0142', '\\n', '[E=50] Epoch 49: Spearman\nCorr=-0.0045', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 7\nseconds seconds (time limit is an hour).']", ""], "analysis": ["", "", "", "", "Script executed successfully without any errors or crashes. Training and\nvalidation losses consistently decreased across all temperature settings,\nindicating the weighted sampling scheme works. The DVN\u2019s Spearman correlation\nbetween predicted and actual sample contributions started negative but improved\nover training, peaking around 0.34, though overall correlation remains modest.\nThis suggests the DVN learns some signal but could benefit from richer features,\nmore meta-update steps, or larger meta-sample sizes to achieve stronger and more\nconsistent data valuation.", "", "", "", "", "", "", "The experiment code ran without runtime errors but did not fulfill the Sub-stage\n1 goal of adding two HuggingFace datasets for evaluation. All results currently\ncome from the synthetic sine task. Furthermore, the DVN\u2019s Spearman correlations\nremain low or unstable, indicating that the current one-dimensional loss feature\nalone may be insufficient for reliable contribution predictions.  Proposed fix:\nintegrate two real regression datasets (e.g., the UCI Housing and Concrete\nStrength datasets from HuggingFace via `datasets.load_dataset`), build\nDataLoaders for them, and append their validation curves to `experiment_data`.\nTo strengthen the DVN, enrich the per-sample feature vector (add gradient norms,\nembedding diversity metrics), increase or adapt the meta-update schedule (more\nDVN training steps or tuned learning rates), and reevaluate Spearman\ncorrelations on both synthetic and real data.", "", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.0218, "best_value": 0.0218}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.0191, "best_value": 0.0191}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss on the synthetic dataset", "data": [{"dataset_name": "Synthetic Dataset", "final_value": 0.0113, "best_value": 0.0113}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss on the synthetic dataset", "data": [{"dataset_name": "Synthetic Dataset", "final_value": 0.01, "best_value": 0.01}]}, {"metric_name": "Spearman correlation", "lower_is_better": false, "description": "Final Spearman correlation on the synthetic dataset", "data": [{"dataset_name": "Synthetic Dataset", "final_value": 0.3038, "best_value": 0.3038}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "synthetic", "final_value": 0.0153, "best_value": 0.0153}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "synthetic", "final_value": 0.014, "best_value": 0.014}]}]}, {"metric_names": [{"metric_name": "final training loss", "lower_is_better": true, "description": "Final training loss on synthetic dataset for each batch size configuration", "data": [{"dataset_name": "synthetic (batch size 16)", "final_value": 0.0139, "best_value": 0.0139}, {"dataset_name": "synthetic (batch size 32)", "final_value": 0.0181, "best_value": 0.0181}, {"dataset_name": "synthetic (batch size 64)", "final_value": 0.0476, "best_value": 0.0476}, {"dataset_name": "synthetic (batch size 128)", "final_value": 0.0807, "best_value": 0.0807}]}, {"metric_name": "final validation loss", "lower_is_better": true, "description": "Final validation loss on synthetic dataset for each batch size configuration", "data": [{"dataset_name": "synthetic (batch size 16)", "final_value": 0.0181, "best_value": 0.0181}, {"dataset_name": "synthetic (batch size 32)", "final_value": 0.0157, "best_value": 0.0157}, {"dataset_name": "synthetic (batch size 64)", "final_value": 0.0325, "best_value": 0.0325}, {"dataset_name": "synthetic (batch size 128)", "final_value": 0.0622, "best_value": 0.0622}]}]}, {"metric_names": [{"metric_name": "training loss (MSE)", "lower_is_better": true, "description": "Final training loss (MSE)", "data": [{"dataset_name": "synthetic (temp 0.1)", "final_value": 0.0199, "best_value": 0.0199}, {"dataset_name": "synthetic (temp 0.5)", "final_value": 0.0344, "best_value": 0.0344}, {"dataset_name": "synthetic (temp 1.0)", "final_value": 0.0476, "best_value": 0.0476}, {"dataset_name": "synthetic (temp 2.0)", "final_value": 0.0279, "best_value": 0.0279}]}, {"metric_name": "validation loss (MSE)", "lower_is_better": true, "description": "Final validation loss (MSE)", "data": [{"dataset_name": "synthetic (temp 0.1)", "final_value": 0.0192, "best_value": 0.0192}, {"dataset_name": "synthetic (temp 0.5)", "final_value": 0.0228, "best_value": 0.0228}, {"dataset_name": "synthetic (temp 1.0)", "final_value": 0.0325, "best_value": 0.0325}, {"dataset_name": "synthetic (temp 2.0)", "final_value": 0.0222, "best_value": 0.0222}]}, {"metric_name": "DVN Spearman correlation", "lower_is_better": false, "description": "Final DVN Spearman correlation", "data": [{"dataset_name": "synthetic (temp 0.1)", "final_value": 0.1113, "best_value": 0.1113}, {"dataset_name": "synthetic (temp 0.5)", "final_value": 0.206, "best_value": 0.206}, {"dataset_name": "synthetic (temp 1.0)", "final_value": 0.0541, "best_value": 0.0541}, {"dataset_name": "synthetic (temp 2.0)", "final_value": -0.3263, "best_value": -0.3263}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final loss on the training dataset after training.", "data": [{"dataset_name": "synthetic", "final_value": 0.0471, "best_value": 0.0217}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final loss on the validation dataset after training.", "data": [{"dataset_name": "synthetic", "final_value": 0.0322, "best_value": 0.0191}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final loss on the synthetic dataset for the training set.", "data": [{"dataset_name": "synthetic dataset", "final_value": 0.0204, "best_value": 0.0204}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final loss on the synthetic dataset for the validation set.", "data": [{"dataset_name": "synthetic dataset", "final_value": 0.0142, "best_value": 0.0142}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss on synthetic dataset for various meta-sample sizes", "data": [{"dataset_name": "synthetic (meta-sample size 5)", "final_value": 0.0218, "best_value": 0.0218}, {"dataset_name": "synthetic (meta-sample size 10)", "final_value": 0.0477, "best_value": 0.0477}, {"dataset_name": "synthetic (meta-sample size 20)", "final_value": 0.0444, "best_value": 0.0444}, {"dataset_name": "synthetic (meta-sample size 50)", "final_value": 0.0693, "best_value": 0.0693}, {"dataset_name": "synthetic (meta-sample size 100)", "final_value": 0.037, "best_value": 0.037}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss on synthetic dataset for various meta-sample sizes", "data": [{"dataset_name": "synthetic (meta-sample size 5)", "final_value": 0.0185, "best_value": 0.0185}, {"dataset_name": "synthetic (meta-sample size 10)", "final_value": 0.0374, "best_value": 0.0374}, {"dataset_name": "synthetic (meta-sample size 20)", "final_value": 0.029, "best_value": 0.029}, {"dataset_name": "synthetic (meta-sample size 50)", "final_value": 0.0442, "best_value": 0.0442}, {"dataset_name": "synthetic (meta-sample size 100)", "final_value": 0.02, "best_value": 0.02}]}, {"metric_name": "Spearman correlation", "lower_is_better": false, "description": "Final Spearman rank correlation on synthetic dataset for various meta-sample sizes", "data": [{"dataset_name": "synthetic (meta-sample size 5)", "final_value": 0.5, "best_value": 0.5}, {"dataset_name": "synthetic (meta-sample size 10)", "final_value": -0.0182, "best_value": -0.0182}, {"dataset_name": "synthetic (meta-sample size 20)", "final_value": -0.2842, "best_value": -0.2842}, {"dataset_name": "synthetic (meta-sample size 50)", "final_value": -0.1584, "best_value": -0.1584}, {"dataset_name": "synthetic (meta-sample size 100)", "final_value": 0.2453, "best_value": 0.2453}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "synthetic (DVN_STEPS_PER_EPOCH = 1)", "final_value": 0.0217, "best_value": 0.0217}, {"dataset_name": "synthetic (DVN_STEPS_PER_EPOCH = 5)", "final_value": 0.0367, "best_value": 0.0367}, {"dataset_name": "synthetic (DVN_STEPS_PER_EPOCH = 10)", "final_value": 0.0475, "best_value": 0.0475}, {"dataset_name": "synthetic (DVN_STEPS_PER_EPOCH = 20)", "final_value": 0.0281, "best_value": 0.0281}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "synthetic (DVN_STEPS_PER_EPOCH = 1)", "final_value": 0.0191, "best_value": 0.0191}, {"dataset_name": "synthetic (DVN_STEPS_PER_EPOCH = 5)", "final_value": 0.0239, "best_value": 0.0239}, {"dataset_name": "synthetic (DVN_STEPS_PER_EPOCH = 10)", "final_value": 0.0324, "best_value": 0.0324}, {"dataset_name": "synthetic (DVN_STEPS_PER_EPOCH = 20)", "final_value": 0.0223, "best_value": 0.0223}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "synthetic", "final_value": 0.1074, "best_value": 0.0218}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "synthetic", "final_value": 0.0562, "best_value": 0.0177}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "synthetic", "final_value": 0.0218, "best_value": 0.0218}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "synthetic", "final_value": 0.0191, "best_value": 0.0191}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss on the synthetic dataset (best across hidden sizes)", "data": [{"dataset_name": "synthetic", "final_value": 0.0214, "best_value": 0.0214}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss on the synthetic dataset (best across hidden sizes)", "data": [{"dataset_name": "synthetic", "final_value": 0.0188, "best_value": 0.0188}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Loss on training dataset", "data": [{"dataset_name": "beta2_0.9 synthetic", "final_value": 0.0501, "best_value": 0.0501}, {"dataset_name": "beta2_0.98 synthetic", "final_value": 0.0316, "best_value": 0.0316}, {"dataset_name": "beta2_0.999 synthetic", "final_value": 0.0575, "best_value": 0.0575}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on validation dataset", "data": [{"dataset_name": "beta2_0.9 synthetic", "final_value": 0.0452, "best_value": 0.0452}, {"dataset_name": "beta2_0.98 synthetic", "final_value": 0.0356, "best_value": 0.0356}, {"dataset_name": "beta2_0.999 synthetic", "final_value": 0.0627, "best_value": 0.0627}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss on the Synthetic dataset", "data": [{"dataset_name": "Synthetic Dataset", "final_value": 0.0123, "best_value": 0.0123}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss on the Synthetic dataset", "data": [{"dataset_name": "Synthetic Dataset", "final_value": 0.0121, "best_value": 0.0121}]}, {"metric_name": "spearman correlation", "lower_is_better": false, "description": "Final Spearman correlation on the Synthetic dataset", "data": [{"dataset_name": "Synthetic Dataset", "final_value": -0.1564, "best_value": -0.1564}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "Synthetic Dataset", "final_value": 0.0107, "best_value": 0.0107}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "Synthetic Dataset", "final_value": 0.0146, "best_value": 0.0146}]}, {"metric_name": "Spearman correlation", "lower_is_better": false, "description": "Final Spearman correlation", "data": [{"dataset_name": "Synthetic Dataset", "final_value": -0.2105, "best_value": -0.2105}]}]}, {"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training split of the synthetic dataset", "data": [{"dataset_name": "Synthetic", "final_value": 0.0117, "best_value": 0.0117}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation split of the synthetic dataset", "data": [{"dataset_name": "Synthetic", "final_value": 0.0142, "best_value": 0.0142}]}, {"metric_name": "spearman correlation", "lower_is_better": false, "description": "Spearman correlation between predictions and ground truth on the synthetic dataset", "data": [{"dataset_name": "Synthetic", "final_value": -0.0045, "best_value": -0.0045}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_f09c1e34663f40bb80e664bd5be20968_proc_231372/synthetic_contribution_dist_epoch_4.png", "../../logs/0-run/experiment_results/experiment_f09c1e34663f40bb80e664bd5be20968_proc_231372/synthetic_loss_curve.png", "../../logs/0-run/experiment_results/experiment_f09c1e34663f40bb80e664bd5be20968_proc_231372/synthetic_correlation_curve.png"], ["../../logs/0-run/experiment_results/experiment_57ddd97f0ac5481ab77c3607a38a5106_proc_234628/synthetic_spearman_corr.png", "../../logs/0-run/experiment_results/experiment_57ddd97f0ac5481ab77c3607a38a5106_proc_234628/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_train_val_loss_lr_0.001.png", "../../logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_train_val_loss_lr_0.01.png", "../../logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_train_val_loss_lr_0.05.png", "../../logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_spearman_correlation.png", "../../logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_train_val_loss_lr_0.005.png", "../../logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_train_val_loss_lr_0.1.png"], ["../../logs/0-run/experiment_results/experiment_03b634473eb14fd9a1c263bed3ec2aff_proc_234630/synthetic_spearman_corr.png", "../../logs/0-run/experiment_results/experiment_03b634473eb14fd9a1c263bed3ec2aff_proc_234630/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_85b7845cc59145c798f1b6ee4a3dbd6d_proc_234629/synthetic_dvn_correlation_all_T.png", "../../logs/0-run/experiment_results/experiment_85b7845cc59145c798f1b6ee4a3dbd6d_proc_234629/synthetic_pred_vs_gt_last_epoch.png", "../../logs/0-run/experiment_results/experiment_85b7845cc59145c798f1b6ee4a3dbd6d_proc_234629/synthetic_loss_curves_all_T.png"], ["../../logs/0-run/experiment_results/experiment_a53c308ae97045a087fa1a3c1bc88290_proc_234628/synthetic_spearman_per_epoch.png", "../../logs/0-run/experiment_results/experiment_a53c308ae97045a087fa1a3c1bc88290_proc_234628/synthetic_pred_vs_gt_last_epoch.png", "../../logs/0-run/experiment_results/experiment_a53c308ae97045a087fa1a3c1bc88290_proc_234628/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_1322edafb06145daa344bf28b0ba67f0_proc_234630/synthetic_spearman_corr.png", "../../logs/0-run/experiment_results/experiment_1322edafb06145daa344bf28b0ba67f0_proc_234630/synthetic_val_loss_heatmap.png", "../../logs/0-run/experiment_results/experiment_1322edafb06145daa344bf28b0ba67f0_proc_234630/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_51bb196406374bc1abec3d14f0c6557e_proc_234628/synthetic_pred_vs_truth_meta100.png", "../../logs/0-run/experiment_results/experiment_51bb196406374bc1abec3d14f0c6557e_proc_234628/synthetic_spearman_corr.png", "../../logs/0-run/experiment_results/experiment_51bb196406374bc1abec3d14f0c6557e_proc_234628/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_f9b0ead5ec0048479d28c69856233ffd_proc_234629/synthetic_scatter_steps_10.png", "../../logs/0-run/experiment_results/experiment_f9b0ead5ec0048479d28c69856233ffd_proc_234629/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f9b0ead5ec0048479d28c69856233ffd_proc_234629/synthetic_spearman_curves.png"], ["../../logs/0-run/experiment_results/experiment_f3b2904d2a2242ee915e6e479c2673f5_proc_234630/synthetic_training_val_loss.png", "../../logs/0-run/experiment_results/experiment_f3b2904d2a2242ee915e6e479c2673f5_proc_234630/synthetic_dvn_scatter_final_epoch.png"], ["../../logs/0-run/experiment_results/experiment_f16c84b289e047268afd7f78289790b9_proc_234628/synthetic_final_val_loss_vs_metalr.png", "../../logs/0-run/experiment_results/experiment_f16c84b289e047268afd7f78289790b9_proc_234628/synthetic_spearman_vs_epoch.png", "../../logs/0-run/experiment_results/experiment_f16c84b289e047268afd7f78289790b9_proc_234628/synthetic_loss_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_d567f933516647d4802894dec54be6df_proc_234630/synthetic_scatter_beta2_0.999.png", "../../logs/0-run/experiment_results/experiment_d567f933516647d4802894dec54be6df_proc_234630/synthetic_spearman_curve.png", "../../logs/0-run/experiment_results/experiment_d567f933516647d4802894dec54be6df_proc_234630/synthetic_loss_curve.png", "../../logs/0-run/experiment_results/experiment_d567f933516647d4802894dec54be6df_proc_234630/synthetic_scatter_beta2_0.9.png", "../../logs/0-run/experiment_results/experiment_d567f933516647d4802894dec54be6df_proc_234630/synthetic_scatter_beta2_0.98.png"], ["../../logs/0-run/experiment_results/experiment_fce85537044142eea1d3340b1d92973a_proc_234629/synthetic_spearman_corr.png", "../../logs/0-run/experiment_results/experiment_fce85537044142eea1d3340b1d92973a_proc_234629/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_99ea040080fe490e8ba4afac99d6ae99_proc_234630/synthetic_spearman_corr.png", "../../logs/0-run/experiment_results/experiment_99ea040080fe490e8ba4afac99d6ae99_proc_234630/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_b78f5529dcd3453f9ebfa65101d0f2a0_proc_234628/synthetic_spearman_corr.png", "../../logs/0-run/experiment_results/experiment_b78f5529dcd3453f9ebfa65101d0f2a0_proc_234628/synthetic_loss_curves.png"], []], "plot_paths": [["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f09c1e34663f40bb80e664bd5be20968_proc_231372/synthetic_contribution_dist_epoch_4.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f09c1e34663f40bb80e664bd5be20968_proc_231372/synthetic_loss_curve.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f09c1e34663f40bb80e664bd5be20968_proc_231372/synthetic_correlation_curve.png"], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_57ddd97f0ac5481ab77c3607a38a5106_proc_234628/synthetic_spearman_corr.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_57ddd97f0ac5481ab77c3607a38a5106_proc_234628/synthetic_loss_curves.png"], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_train_val_loss_lr_0.001.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_train_val_loss_lr_0.01.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_train_val_loss_lr_0.05.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_spearman_correlation.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_train_val_loss_lr_0.005.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_train_val_loss_lr_0.1.png"], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_03b634473eb14fd9a1c263bed3ec2aff_proc_234630/synthetic_spearman_corr.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_03b634473eb14fd9a1c263bed3ec2aff_proc_234630/synthetic_loss_curves.png"], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_85b7845cc59145c798f1b6ee4a3dbd6d_proc_234629/synthetic_dvn_correlation_all_T.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_85b7845cc59145c798f1b6ee4a3dbd6d_proc_234629/synthetic_pred_vs_gt_last_epoch.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_85b7845cc59145c798f1b6ee4a3dbd6d_proc_234629/synthetic_loss_curves_all_T.png"], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_a53c308ae97045a087fa1a3c1bc88290_proc_234628/synthetic_spearman_per_epoch.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_a53c308ae97045a087fa1a3c1bc88290_proc_234628/synthetic_pred_vs_gt_last_epoch.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_a53c308ae97045a087fa1a3c1bc88290_proc_234628/synthetic_loss_curves.png"], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_1322edafb06145daa344bf28b0ba67f0_proc_234630/synthetic_spearman_corr.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_1322edafb06145daa344bf28b0ba67f0_proc_234630/synthetic_val_loss_heatmap.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_1322edafb06145daa344bf28b0ba67f0_proc_234630/synthetic_loss_curves.png"], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_51bb196406374bc1abec3d14f0c6557e_proc_234628/synthetic_pred_vs_truth_meta100.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_51bb196406374bc1abec3d14f0c6557e_proc_234628/synthetic_spearman_corr.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_51bb196406374bc1abec3d14f0c6557e_proc_234628/synthetic_loss_curves.png"], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f9b0ead5ec0048479d28c69856233ffd_proc_234629/synthetic_scatter_steps_10.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f9b0ead5ec0048479d28c69856233ffd_proc_234629/synthetic_loss_curves.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f9b0ead5ec0048479d28c69856233ffd_proc_234629/synthetic_spearman_curves.png"], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f3b2904d2a2242ee915e6e479c2673f5_proc_234630/synthetic_training_val_loss.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f3b2904d2a2242ee915e6e479c2673f5_proc_234630/synthetic_dvn_scatter_final_epoch.png"], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f16c84b289e047268afd7f78289790b9_proc_234628/synthetic_final_val_loss_vs_metalr.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f16c84b289e047268afd7f78289790b9_proc_234628/synthetic_spearman_vs_epoch.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f16c84b289e047268afd7f78289790b9_proc_234628/synthetic_loss_curves.png"], [], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_d567f933516647d4802894dec54be6df_proc_234630/synthetic_scatter_beta2_0.999.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_d567f933516647d4802894dec54be6df_proc_234630/synthetic_spearman_curve.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_d567f933516647d4802894dec54be6df_proc_234630/synthetic_loss_curve.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_d567f933516647d4802894dec54be6df_proc_234630/synthetic_scatter_beta2_0.9.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_d567f933516647d4802894dec54be6df_proc_234630/synthetic_scatter_beta2_0.98.png"], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_fce85537044142eea1d3340b1d92973a_proc_234629/synthetic_spearman_corr.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_fce85537044142eea1d3340b1d92973a_proc_234629/synthetic_loss_curves.png"], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_99ea040080fe490e8ba4afac99d6ae99_proc_234630/synthetic_spearman_corr.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_99ea040080fe490e8ba4afac99d6ae99_proc_234630/synthetic_loss_curves.png"], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_b78f5529dcd3453f9ebfa65101d0f2a0_proc_234628/synthetic_spearman_corr.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_b78f5529dcd3453f9ebfa65101d0f2a0_proc_234628/synthetic_loss_curves.png"], []], "plot_analyses": [[{"analysis": "Contribution Distribution histograms at epoch 4 reveal a clear mismatch: ground-truth contributions concentrate between roughly \u20130.025 and \u20130.005, with a cluster near \u20130.01, whereas DVN-predicted values lie in a more negative, narrower band around \u20130.045 to \u20130.035 and include an extreme tail near \u20130.09. This systematic shift indicates the sampler is overly pessimistic about sample value and exhibits higher variance outliers. Calibration and feature engineering or additional fine-tuning of the DVN loss could reduce bias and tighten alignment with true contributions.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f09c1e34663f40bb80e664bd5be20968_proc_231372/synthetic_contribution_dist_epoch_4.png"}, {"analysis": "Training and validation losses steadily decline from epoch 1 to epoch 5 (train: ~0.19\u21920.02, validation: ~0.12\u21920.018), with validation closely tracking or slightly undercutting training loss by the end. This smooth downward trend and lack of divergence indicate stable convergence without overfitting, confirming that the basic implementation of the training loop and loss computation is correct.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f09c1e34663f40bb80e664bd5be20968_proc_231372/synthetic_loss_curve.png"}, {"analysis": "Spearman correlation between predicted and ground-truth contributions fluctuates: strongly negative at epoch 1 (~\u20130.55), peaks positively at epoch 2 (~0.23), drops again at epoch 3 (~\u20130.29), then recovers to moderate positive values by epochs 4 (~0.11) and 5 (~0.17). These oscillations imply instability in the meta-learning updates\u2014initial uncalibrated weights, a successful adjustment, followed by an overcorrection and partial recovery. More frequent or regularized ground-truth contribution measurements, smoother update rules, or stronger feature normalization may stabilize ranking performance.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f09c1e34663f40bb80e664bd5be20968_proc_231372/synthetic_correlation_curve.png"}], [{"analysis": "", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_57ddd97f0ac5481ab77c3607a38a5106_proc_234628/synthetic_spearman_corr.png"}], [{"analysis": "Loss curves for a relatively easy synthetic setup show a smooth decline in both training and validation loss from epoch 1 to 5. Training loss drops from ~0.285 to ~0.155, while validation moves from ~0.205 to ~0.13. Validation loss remains below training throughout, hinting at cleaner validation data or some regularization effect. The persistent but narrow gap suggests stable learning with no obvious overfitting by epoch 5.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_train_val_loss_lr_0.001.png"}, {"analysis": "More aggressive convergence appears here. Training loss plummets from ~0.34 to ~0.048, validation from ~0.205 to ~0.03 in five epochs. The gap closes almost entirely by the final epoch, indicating very fast fitting. This might come from a simpler synthetic distribution or higher learning rate on the sampler features.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_train_val_loss_lr_0.01.png"}, {"analysis": "Yet another synthetic variant achieves near-zero losses by epoch 5: training from ~0.265 to ~0.015, validation from ~0.13 to ~0.013. The curves converge around epoch 3\u20134. Such rapid, deep minimization could indicate a model overshooting subtle held-out-data structure or a dataset with very low intrinsic difficulty.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_train_val_loss_lr_0.05.png"}, {"analysis": "Spearman correlations between DVN predictions and true contribution scores reveal strong sensitivity to meta\u2010learning rate. An lr of 0.001 starts negative (~\u20130.45) but transitions to robust positive correlation (~+0.30) by epochs 4\u20135. Learning rates 0.005 and 0.01 fluctuate around zero, never achieving reliable positive alignment. A high lr of 0.1 also ends with a positive correlation (~+0.30) but shows volatility earlier. An lr of 0.05 remains negative throughout. The most stable signal is obtained with lr = 0.001, compromising between stability and eventual positive alignment.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_spearman_correlation.png"}, {"analysis": "Another loss trajectory shows training dropping from ~0.25 to ~0.07 and validation from ~0.155 to ~0.05. Both curves descend smoothly and maintain a small gap (~0.02 at epoch 5), confirming good generalization and no visible overfitting across five epochs.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_train_val_loss_lr_0.005.png"}, {"analysis": "A final run begins with a high training loss (~2.65) and a validation loss (~0.55), then exhibits steep decreases to ~0.08 and ~0.05 respectively by epoch 5. The large initial discrepancy suggests either unnormalized inputs or greater complexity in this synthetic split. Rapid convergence post epoch 2 implies the model quickly adapts once difficulty barriers are overcome.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e4d2a253b1d045e29d412f644c97e735_proc_234629/synthetic_train_val_loss_lr_0.1.png"}], [{"analysis": "Spearman correlation between DVN meta-sample predictions and true held-out improvement varies significantly with batch size and epoch. With batch size 16, correlation rises sharply to about 0.45 at epoch 2, then degrades into negative territory by epoch 4 (around \u20130.38) and remains low at epoch 5. Batch size 32 shows moderate positive correlation at epochs 1\u20132 (0.15 down to \u20130.15), a rebound to 0.25 at epoch 3, and peaks at 0.46 in epoch 4 before dropping at epoch 5. Batch size 64 starts weakly positive (0.05), dips into negatives around epoch 2 (\u20130.28) and epoch 3 (\u20130.1), then recovers to 0.16 at epoch 4 and 0.05 at epoch 5. At batch size 128, correlation is consistently negative\u2014worsening to \u20130.68 by epoch 3\u2014before recovering slightly to \u20130.07 by epoch 5. These trends suggest: \n\u2022 Smaller batches (16, 32) yield stronger early DVN calibration but risk overfitting the valuation network over time.  \n\u2022 Medium batch size (32) offers a balance: highest peak correlation at epoch 4 and more stable recovery after dips.  \n\u2022 Very large batches hamper the DVN\u2019s ability to predict sample contribution, leading to persistently poor or misleading signals.  \n\u2022 Optimal update frequency or regularization of the DVN may differ by batch size: smaller batches need early-stopping or stronger smoothing, larger batches require more frequent ground-truth refreshes to avoid drift.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_03b634473eb14fd9a1c263bed3ec2aff_proc_234630/synthetic_spearman_corr.png"}, {"analysis": "Across batch sizes, training and validation losses consistently decrease with epoch, confirming stable optimization and no severe overfitting. With batch size 16, loss drops very rapidly: training loss falls from ~0.10 to ~0.01 by epoch 2 and levels off near 0.005; validation loss follows suit, reaching ~0.005 early. Batch size 32 starts at ~0.19 (train) and ~0.10 (val), descending to ~0.02 and ~0.015 respectively by epoch 5. Batch sizes 64 and 128 show slower convergence: initial training losses of ~0.34 and ~0.26, reaching ~0.05/0.08 by epoch 5, and validation losses of ~0.21/0.17 down to ~0.07/0.06. Key takeaways:  \n\u2022 Smaller batches accelerate convergence and attain lower final losses on both train and val.  \n\u2022 Larger batches converge slower but remain stable\u2014no divergence between train and val\u2014indicating good generalization with longer training.  \n\u2022 Trade-offs emerge: batch size 16 yields fastest convergence but DVN predictions degrade rapidly; batch size 32 offers a compromise between reasonable convergence speed, robust generalization, and strong DVN signal.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_03b634473eb14fd9a1c263bed3ec2aff_proc_234630/synthetic_loss_curves.png"}], [{"analysis": "Spearman correlation of DVN scores versus real contributions shows that lower temperature settings (T=0.1 and T=0.5) start with negative or weak correlations but quickly climb positive, with T=0.5 peaking highest (\u22480.33) at epoch 4. Mid\u2010temperature (T=1.0) yields moderate but stable improvement, while high temperature (T=2.0) is erratic\u2014initially negative, spikes at epoch 4, then collapses by epoch 5. This suggests that a temperature around 0.5 provides the best alignment between predicted and true sample importance, accelerating calibration of the sampler over training.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_85b7845cc59145c798f1b6ee4a3dbd6d_proc_234629/synthetic_dvn_correlation_all_T.png"}, {"analysis": "At epoch 5, the DVN prediction scatter against true contributions reveals that T=0.5 predictions hug the identity line tightly (low bias, low variance), indicating accurate sample valuation. T=0.1 systematically underestimates contributions (points below the diagonal). T=1.0 shows moderate dispersion with slight underestimation, while T=2.0 exhibits the largest spread and bias. Again, T=0.5 yields the most reliable per-sample predictions at convergence.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_85b7845cc59145c798f1b6ee4a3dbd6d_proc_234629/synthetic_pred_vs_gt_last_epoch.png"}, {"analysis": "Training and validation loss curves for the synthetic dataset demonstrate that T=0.1 and T=0.5 yield the fastest convergence and lowest final losses (\u223c0.02 at epoch 5), with minimal train/val gap, indicating good generalization. T=1.0 converges slowest with higher end-point loss (~0.04), and T=2.0 sits in between. Combined with the correlation and prediction results, T=0.5 stands out as the best balance of stability, predictive accuracy, and optimization efficiency.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_85b7845cc59145c798f1b6ee4a3dbd6d_proc_234629/synthetic_loss_curves_all_T.png"}], [], [{"analysis": "On the Synthetic Dataset - DVN Spearman Corr vs Epoch curve, Spearman correlation starts strongly negative (\u20130.15 at epoch 1), indicating that the DVN\u2019s initial ranking of sample contributions is inversely aligned with actual held-out performance. Over epochs 2 and 3, the correlation steadily rises toward zero (\u20130.09, \u20130.07), suggesting the DVN is learning useful patterns. By epoch 4, it crosses into positive territory (\u22480.07), demonstrating the network now effectively predicts which samples accelerate convergence. The slight drop at epoch 5 (\u22480.03) hints at potential over-regularization or insufficient update frequency; refining update intervals or tuning weight decay could help preserve peak alignment.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_1322edafb06145daa344bf28b0ba67f0_proc_234630/synthetic_spearman_corr.png"}, {"analysis": "In the Synthetic Dataset - Final Validation Loss Heatmap, the grid of main\u2010model vs. DVN weight decay values shows a clear optimum at (main WD=1e-5, DVN WD=1e-5), yielding the lowest validation loss (~0.011). Zero or very large decay on either component increases loss, and extreme DVN decay (1e-3) leads to underfitting. This indicates matched moderate regularization on both the foundation model and the DVN is key to stable, accurate data valuation.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_1322edafb06145daa344bf28b0ba67f0_proc_234630/synthetic_val_loss_heatmap.png"}, {"analysis": "The Synthetic Dataset - Training and Validation Loss Curves reveal a rapid decline in MSE: training loss falls from 0.34 to 0.05, and validation loss from 0.17 to 0.035 across five epochs. The parallel shape and narrowing train/validation gap indicate strong generalization without overfitting. These trends support extending training for incremental gains and introducing a learning-rate scheduling (e.g., halving LR after epoch 3) or slight batch size adjustments to further lower loss.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_1322edafb06145daa344bf28b0ba67f0_proc_234630/synthetic_loss_curves.png"}], [{"analysis": "Predictions cluster in a narrow band around 0.014\u20130.016 despite ground\u2010truth \u0394val spanning \u22120.032 to \u22120.012. The learned DVN at meta=100 is underfitting the contribution signal: it fails to track the true mini\u2010batch contribution magnitudes or ordering. This bias toward a constant prediction makes it impossible to prioritize high\u2010impact samples correctly.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_51bb196406374bc1abec3d14f0c6557e_proc_234628/synthetic_pred_vs_truth_meta100.png"}, {"analysis": "All meta\u2010sample sizes start with negative Spearman \u03c1 (poor ranking) and improve over epochs, but behavior splits by scale:\n\u00b7 Smaller meta sizes (meta=5,10,20) recover ranking faster and reach peaks of \u03c1\u22480.5\u20130.6 by epoch 1\u20133; however, they oscillate and sometimes degrade by epoch 4.\n\u00b7 Larger meta sizes (meta=50,100) show more muted improvements (peak \u03c1\u22480.2\u20130.25) and more stable but lower-quality rankings.\nThis suggests a trade\u2010off: smaller meta batches yield quicker, stronger meta\u2010learning signals but suffer greater variance; larger meta batches produce steadier but weaker updates.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_51bb196406374bc1abec3d14f0c6557e_proc_234628/synthetic_spearman_corr.png"}, {"analysis": "Training and validation losses both drop monotonically for all meta sizes. meta=5 achieves the fastest convergence and lowest final loss, followed by meta=10 and meta=20. meta=50 learns slowest and retains the highest loss, with meta=100 in between. Validation curves mirror the training trends, indicating that the gains are not pure overfitting: meta=5 consistently generalizes best, then meta=10/20, then meta=100, and meta=50 last. \nOverall, a very small meta\u2010sample size (5\u201320) yields the strongest learning signals, fastest pre\u2010training progress, and best held\u2010out performance; too large a meta batch dilutes the meta\u2010gradient and slows convergence.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_51bb196406374bc1abec3d14f0c6557e_proc_234628/synthetic_loss_curves.png"}], [{"analysis": "Scatter of true vs predicted contributions shows that the DVN approximates the narrow band of influence values fairly consistently but underestimates the extremes. True contributions range roughly from \u20130.05 to \u20130.01, while predictions lie in a tighter interval (\u20130.014 to \u20130.008). A handful of points with more negative true values (around \u20130.04 to \u20130.05) have predicted contributions clustered around \u20130.008, indicating the DVN struggles to distinguish the very lowest-contribution samples. Conversely, high-contribution points (closer to \u20130.01) are slightly overestimated toward the upper bound of the prediction range. Overall there is a positive trend, but the limited dynamic range suggests calibration and model capacity issues in capturing full signal amplitude.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f9b0ead5ec0048479d28c69856233ffd_proc_234629/synthetic_scatter_steps_10.png"}, {"analysis": "All curves descend steadily over five epochs. Models with more frequent DVN updates (1-step, 5-step intervals) begin with lower training and validation loss and converge faster. In particular, the 1-step update setting achieves the lowest train and val loss by epoch\u20095 (<0.02), followed closely by 5-step. Less frequent update regimes (10-step, 20-step) start at much higher loss (0.17\u20130.33) and maintain a noticeable gap above the frequent-update curves through epoch\u20095 (ending around 0.04\u20130.05 validation loss for 20-step). Training/validation gaps are small across all settings, suggesting no severe overfitting, but slower DVN updates clearly hurt pre-training efficiency.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f9b0ead5ec0048479d28c69856233ffd_proc_234629/synthetic_loss_curves.png"}, {"analysis": "Spearman \u03c1 shows correlation between DVN\u2019s predicted and true contributions evolving over epochs. For 1-step updates, \u03c1 climbs from \u20130.56 at epoch\u20091 to a peak ~0.20 at epoch\u20092, dips at epoch\u20093, then recovers to ~0.14 by epoch\u20095. The 5-step curve starts at \u20130.62, rises steadily to ~0.28 at epoch\u20094 before settling at ~0.06. The 10-step setting hovers around zero with a slight positive blip at epoch\u20091 and weak signals thereafter. The 20-step schedule yields a mixed signal: moderate negative initially (\u20130.20), peaks ~0.32 at epoch\u20093, drops sharply at epoch\u20094 (\u20130.30), and then ends high (~0.40). Frequent DVN updates (1 and 5 steps) produce more stable positive correlations after warm-up, whereas infrequent updates cause erratic swings despite occasionally high peaks. This suggests a trade-off: too sparse updating destabilizes learned valuations, while very frequent updating yields more consistent alignment with true contributions.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f9b0ead5ec0048479d28c69856233ffd_proc_234629/synthetic_spearman_curves.png"}], [{"analysis": "Synthetic Dataset Training & Validation Loss: Three \u03b21 settings are compared (0.8, 0.9, 0.99) over five epochs. \u03b21=0.8 yields the fastest and smoothest decline in both training (0.18\u21920.015) and validation loss (0.105\u21920.02). \u03b21=0.9 converges a bit more slowly (train 0.23\u21920.03; val 0.15\u21920.015) but remains stable, with no signs of over\u2010 or under\u2010shooting. Setting \u03b21=0.99 shows unstable behavior: initial loss drop is strong (train 0.345\u21920.135; val 0.28\u21920.12 by epoch 1) but then training and validation losses oscillate (spikes at epoch 3) before descending again. Overall, lower momentum (\u03b21=0.8) leads to faster, steadier convergence, \u03b21=0.9 is a reasonable trade\u2010off, while \u03b21=0.99 is too sluggish and prone to loss fluctuations.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f3b2904d2a2242ee915e6e479c2673f5_proc_234630/synthetic_training_val_loss.png"}, {"analysis": "Synthetic Dataset DVN Influence Prediction (Final Epoch): Scatter plots of true vs. predicted influence under the three \u03b21 settings yield Spearman correlations of 0.36 (\u03b21=0.8), 0.06 (\u03b21=0.9), and 0.25 (\u03b21=0.99). With \u03b21=0.8, the DVN ranks samples moderately well despite a bias toward positive predicted values even though true influences lie in the negative range. \u03b21=0.9 almost completely fails to capture any rank ordering. \u03b21=0.99 recovers some ordering ability but predicted influences are tightly clustered. This suggests that a momentum parameter of 0.8 both stabilizes training and yields the best sample\u2010valuation performance, while too much momentum (0.99) or an intermediate setting (0.9) degrade the DVN\u2019s final influence estimates.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f3b2904d2a2242ee915e6e479c2673f5_proc_234630/synthetic_dvn_scatter_final_epoch.png"}], [{"analysis": "Validation loss exhibits a non-monotonic relationship with meta-learning rate. At the lowest MetaLR (~1e-4), it achieves the smallest final loss (~0.02). As MetaLR increases toward ~1e-3, loss rises modestly (~0.03) before dipping slightly around ~1e-2 (~0.022). Extreme MetaLR (0.1) leads to a dramatic spike in loss (~0.11), indicating divergence or instability. This suggests an optimal MetaLR in the lower range (1e-4\u20131e-2) with caution against overly aggressive values.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f16c84b289e047268afd7f78289790b9_proc_234628/synthetic_final_val_loss_vs_metalr.png"}, {"analysis": "Spearman correlation between DVN predictions and true per-sample contributions varies strongly with MetaLR and training time. Low MetaLRs (1e-4, 4e-4) start with negative or near-zero correlations and only reach mild positive values by epoch 4. Mid-range MetaLRs (1.6e-3, 6.3e-3) show oscillatory, unstable correlations. MetaLR=2.5e-2 yields consistently positive and increasing correlation across epochs, peaking at ~0.56 by epoch 5. Very high MetaLR (1e-1) again produces mostly negative correlations until a late slight recovery. This highlights that a MetaLR around 2.5e-2 best aligns DVN predictions with actual influence signals.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f16c84b289e047268afd7f78289790b9_proc_234628/synthetic_spearman_vs_epoch.png"}, {"analysis": "Training and validation loss curves across epochs confirm the stability\u2013convergence trade-off. Small MetaLR (1e-4) gives slow but steady loss reduction (train from ~0.84 to ~0.14, val from ~0.25 to ~0.12). Moderate MetaLRs (4e-4 through 2.5e-2) accelerate convergence substantially, with train/val losses dropping below ~0.05/~0.03 by epoch 5 for the two fastest rates (6.3e-3, 2.5e-2). The largest MetaLR (1e-1) shows a sharp early drop followed by plateau (~0.12/0.08 by epoch 5), indicating overstepping the stable regime. Overall, MetaLR\u22432.5e-2 achieves the best compromise of rapid convergence and stable loss trajectories.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_f16c84b289e047268afd7f78289790b9_proc_234628/synthetic_loss_curves.png"}], [], [{"analysis": "Scatter points are widely dispersed around the diagonal and do not lie close to a y=x line. Predicted values occupy a narrower, shifted range (around \u20130.005 to \u20130.02) compared to ground truth (\u20130.045 to 0.005). Little to no linear alignment suggests that with beta2=0.999, the DVN\u2019s output lacks fidelity in ranking true contributions on this synthetic data.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_d567f933516647d4802894dec54be6df_proc_234630/synthetic_scatter_beta2_0.999.png"}, {"analysis": "Beta2=0.9 starts with a positive Spearman (~0.23), dips to slightly negative around epoch 1, then returns strongly positive by epoch 4 (~0.28). Beta2=0.98 remains consistently negative (\u20130.6 to \u20130.18), indicating reverse ranking. Beta2=0.999 fluctuates between moderate positive at epoch 0 (0.23), dips (\u20130.44 at epoch 1), and recovers only marginally by epoch 4 (~0.01). Lower beta2 yields more stable and improving rank correlation over training.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_d567f933516647d4802894dec54be6df_proc_234630/synthetic_spearman_curve.png"}, {"analysis": "All three beta2 settings drive loss down smoothly. Beta2=0.98 achieves the fastest convergence: by epoch 4, train ~0.03 and val ~0.04, slightly ahead of beta2=0.999 (train ~0.05, val ~0.06) and beta2=0.9 (train ~0.05, val ~0.05). Minimal train\u2010val gap across settings indicates no severe overfitting; beta2=0.98 gives the steepest early drop and lowest final loss.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_d567f933516647d4802894dec54be6df_proc_234630/synthetic_loss_curve.png"}, {"analysis": "Predicted meta-feedback values occupy a tight band (roughly \u20130.026 to \u20130.02) while ground truth spans \u20130.045 to 0.002. Points show almost horizontal alignment, reflecting very low sensitivity to true variation. With beta2=0.9, the DVN severely underestimates dynamic range, yielding poor ranking performance at this snapshot.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_d567f933516647d4802894dec54be6df_proc_234630/synthetic_scatter_beta2_0.9.png"}, {"analysis": "Predictions are clustered in an even narrower window (around \u20130.04) with an outlier near \u20130.065, despite ground truth covering \u20130.09 to \u20130.01. The collapsed output distribution under beta2=0.98 indicates the DVN is nearly blind to true contribution differences, mirroring the strongly negative Spearman correlation seen earlier.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_d567f933516647d4802894dec54be6df_proc_234630/synthetic_scatter_beta2_0.98.png"}], [{"analysis": "Spearman correlation of DVN predictions against true sample contributions reveals distinct behaviors at different DVN training budgets. With a 5-epoch DVN, correlation is modestly positive around epoch 1 (~0.4), then collapses into negative territory by epoch 2 and remains unstable. At 20 epochs, correlation fluctuates around zero with occasional modest peaks (~0.3) but also dips below \u20130.2, indicating noisy and unreliable contribution estimates. A 50-epoch DVN shows the highest initial peak (~0.7 at epoch 2) but also extreme volatility, plunging below \u20130.6 at certain points and oscillating throughout. This suggests that while extended DVN training can yield stronger short-term alignment to true contributions, it suffers from stability issues and diminishing consistency over long horizons.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_fce85537044142eea1d3340b1d92973a_proc_234629/synthetic_spearman_corr.png"}, {"analysis": "Training and validation loss curves for the synthetic dataset converge rapidly for all DVN training budgets. Within the first 10 epochs, losses drop from ~0.35 to near zero, with minimal differences between the 5-, 20-, and 50-epoch settings. Beyond ~15 epochs, all loss curves overlap and plateau around 0.01 on both training and validation sets, with no pronounced overfitting even at 50 epochs. This indicates that increasing epoch budget beyond ~20 yields negligible further reduction in loss, and that the model generalizes well on the validation set across all budgets.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_fce85537044142eea1d3340b1d92973a_proc_234629/synthetic_loss_curves.png"}], [{"analysis": "Spearman correlation trajectories reveal that shorter tuning runs (5 epochs) yield more stable and positive predictive performance of the DVN, peaking around 0.5 correlation early on, but plateau quickly. Increasing epochs to 20 extends the positive correlation window slightly but introduces more volatility after epoch 10, with occasional dips below zero. The 50-epoch run is markedly unstable: correlations oscillate widely, even reaching strong negative values (around \u20130.6), suggesting overfitting or that the DVN\u2019s predictions grow misaligned with true contributions when allowed too many updates without additional regularization or fresh held-out measurements.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_99ea040080fe490e8ba4afac99d6ae99_proc_234630/synthetic_spearman_corr.png"}, {"analysis": "Training and validation losses across the three epoch configurations converge rapidly within the first 10\u201315 epochs, with diminishing returns thereafter. All configurations reach near-zero training and validation loss by epoch 20. Notably, the 50-epoch curve starts higher and takes marginally longer to align with the others, indicating that the larger number of tuning epochs may initially hamper optimization speed. Validation loss tracks training loss closely, showing no severe overfitting even at 50 epochs, though the noisy correlation results imply over-specialization of the DVN to spurious features rather than overall predictive quality.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_99ea040080fe490e8ba4afac99d6ae99_proc_234630/synthetic_loss_curves.png"}], [{"analysis": "Synthetic Dataset Spearman Correlation:\nDVN\u2019s ability to predict per-sample contributions improves rapidly in early training but then becomes noisy as epochs accumulate. With only 5 epochs of meta-learning, Spearman correlation rises from around \u20130.3 to about +0.3 by epoch 5, showing quick signal acquisition but limited ceiling. Extending to 20 epochs yields a clear peak correlation above +0.5 at mid-training (around epochs 6\u201312), followed by a gradual decline to around +0.2\u2013+0.3 by epoch 20. Pushing to 50 epochs further increases volatility: correlations oscillate between modest positives (+0.2\u2013+0.4) and negatives (down to \u20130.4), with few sustained peaks beyond +0.3. This suggests an optimal window around 10\u201320 epochs for updating the DVN: enough time to learn reliable sample-value mappings, but not so long that overfitting, drift, or feature saturation degrade its precision.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_b78f5529dcd3453f9ebfa65101d0f2a0_proc_234628/synthetic_spearman_corr.png"}, {"analysis": "Synthetic Dataset Training/Validation Loss:\nAll configurations follow a similar exponential decay, with training and validation losses nearly overlapping throughout, indicating minimal generalization gap. A 5-epoch run drops loss from ~0.39 to ~0.05 but remains above its long-run minimum, whereas both 20- and 50-epoch runs converge to near-zero loss by epoch ~15\u201320. Beyond that point, additional epochs yield vanishing gains. The close alignment of solid (training) and dashed (validation) curves across 20 and 50 epochs confirms absence of overfitting despite extended training. These dynamics explain why DVN correlation peaks around when loss plateau is reached: once the base model has largely converged, there is less informative signal for the DVN to learn from.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_b78f5529dcd3453f9ebfa65101d0f2a0_proc_234628/synthetic_loss_curves.png"}], []], "vlm_feedback_summary": ["Plots confirm functional convergence (loss curves) but reveal calibration and\nstability issues in the DVN\u2019s ranking ability: predicted contributions are\nbiased low and ranking correlation oscillates before modest recovery. Refining\nmeta-update schedule, loss weighting, and feature representation should improve\nalignment with true sample value.", "Correlations improve markedly with more meta\u2010training epochs but suffer\nintermittent declines; tuning update frequency and meta\u2010learning rate schedules\nshould stabilize DVN performance.", "All loss curves across six synthetic configurations show monotonic decreases\nwithout overfitting, though initial scales and convergence speeds vary. DVN\nmeta\u2010learning is most effective at lr=0.001, with lr=0.1 as a secondary\ncandidate but more erratic. Midrange rates underperform. For the next tuning\nstage, fix the DVN meta\u2010lr at 0.001, consider a mild warm-up on the hardest\nsplit, and maintain batch size. Two Hugging Face datasets to introduce for\nfurther testing: \u201cwikitext-103\u201d for standard language modeling and \u201copenwebtext\u201d\nto mirror WebText diversity.", "DVN calibration is strongest early in training with small-to-medium batch sizes,\nbut small batches risk overfitting the valuation network over epochs. Loss\ncurves confirm smaller batches converge fastest and achieve lowest loss, while\nlarger batches train more slowly yet stably. Batch size 32 appears to balance\nconvergence speed, generalization, and DVN prediction quality.", "Results consistently favor a temperature of ~0.5 for the DVN: it aligns best\nwith true contributions, produces accurate per-sample predictions, and drives\nthe fastest loss reduction without overfitting. To further improve performance\nin baseline tuning, we recommend: 1) modestly increase learning rate (e.g. from\n1e-4 to 2\u20133e-4) under the T=0.5 setting; 2) enlarge batch size (e.g. from 32 to\n64) to stabilize gradient estimates; 3) extend pre\u2010training to ~8 epochs to\nexploit the strong early\u2010stage gains. For more rigorous downstream evaluation,\nintroduce two additional Hugging Face corpora: wikitext-103 (for\nperplexity/generalization on open-domain text) and cc_news (to test robustness\non newswire style). These additions will stress-test generalization and fairness\nunder realistic, varied distributions.", "[]", "The DVN transitions from poor initial rankings to reliable contribution\npredictions by epoch 4, validating the meta-learning approach. The\nhyperparameter sweep highlights weight decay at 1e-5 for both the main model and\nDVN as optimal; future runs should fix these and explore finer learning-rate\nschedules or batch-size scaling. To establish real\u2010world robustness, it is\nrecommended to pre-train and evaluate on WikiText-103 (a standard\nlanguage\u2010modeling benchmark) and the C4 corpus (a large, diverse web\u2010scraped\ndataset) from HuggingFace. These additions will demonstrate scalability,\ngeneralization, and practical efficiency gains of the adaptive sampler on\nnatural text distributions.", "DVN with small meta\u2010sample sizes (5\u201320) produces stronger, faster learning and\nbetter ranking early on but exhibits epoch\u2010wise variability; meta=5 achieves the\nlowest loss and highest Spearman peaks. Meta=100 underfits the contribution\nvalues, leading to nearly constant predicted \u0394val and poor ranking.", "DVN update frequency critically affects both convergence speed and valuation\naccuracy. Frequent updates (every 1\u20135 steps) yield faster loss reduction and\nmore stable, positive Spearman correlation with ground truth. Sparse updates\n(10\u201320 steps) slow down training and produce noisy correlation signals, even if\nthey peak occasionally. Predicted contributions cover a too-narrow range and\ntend to regress extreme samples toward the mean, indicating a need for capacity\nor calibration improvements.", "Lower momentum (\u03b21=0.8) provides best convergence and highest Spearman for\ninfluence prediction; \u03b21=0.9 underperforms on DVN correlation; \u03b21=0.99 shows\nunstable loss and moderate correlation. Momentum setting is crucial for sampler\nquality.", "Plots indicate that meta-learning rate selection critically affects final\nvalidation loss, alignment of DVN predictions (Spearman correlation), and\nconvergence speed. A MetaLR around 2.5e-2 offers the best balance between rapid\nlearning and reliable contribution estimates, while very low rates are stable\nbut slow, and very high rates lead to instability.", "[]", "The DVN\u2019s ability to predict sample contributions is highly sensitive to the\noptimizer\u2019s momentum beta2. Lower beta2 (0.9) recovers better rank correlation\nby late training but still shows compressed outputs; beta2=0.98 yields the worst\nranking and fastest loss convergence; beta2=0.999 falls in between. Overall,\ntuning beta2 is critical for balancing optimization speed and accurate data\nvaluation.", "The DVN\u2019s ability to predict sample contributions benefits from moderate\ntraining (around 20 epochs) in terms of stability, while extreme budgets (5 or\n50 epochs) either underfit or overfit the contribution signal, leading to noise.\nLoss trajectories plateau early, showing that additional DVN training epochs\nprovide little extra utility. For future tuning, consider regularizing the DVN\nupdate, employing early stopping on contribution correlation, or adaptive\nlearning-rate schedules to balance stability and alignment.", "Short DVN tuning runs (5\u201320 epochs) strike a better balance between stability\nand predictive accuracy; extending to 50 epochs destabilizes correlation without\noverfitting loss. Future hyperparameter tuning should focus on selecting an\nintermediate epoch budget (around 10\u201320) and possibly adding regularization or\nmore frequent held-out contribution updates. To further validate generality and\nfairness impact, testing on additional large-scale unlabeled corpora such as\n\u2018allenai/c4\u2019 and \u2018bookcorpus\u2019 from Hugging Face is recommended.", "DVN meta-learning yields the best sample-value predictions after roughly 10\u201320\nepochs. Base-model loss plateaus by epoch ~15\u201320 without overfitting, aligning\nwith the window of maximal DVN correlation. Training fewer than 10 epochs\nunderfits the DVN, while much longer runs cause noisy valuation signals.", "[]"], "exec_time": [1.8638930320739746, 7.528475761413574, 3.48099684715271, 3.7417984008789062, 3.1211588382720947, 2.6712207794189453, 7.927788496017456, 4.196411371231079, 2.7488574981689453, 2.8272528648376465, 3.930365562438965, 3.157353162765503, 2.851449728012085, 7.963127136230469, 7.746835708618164, 7.680660963058472, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["['synthetic']"], ["[synthetic]"], ["[\"synthetic\"]"], ["[\"synthetic\"]"], ["['synthetic']"], [], ["['synthetic']"], ["['synthetic']"], ["['synthetic']"], ["['synthetic']"], ["[\u201csynthetic\u201d]"], [], [""], ["[]"], ["[synthetic]"], ["['synthetic']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n# Load experiment data\ntry:\n    experiment_data = np.load(data_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    # Extract metrics\n    train_losses = experiment_data[\"synthetic\"][\"metrics\"][\"train\"]\n    val_losses = experiment_data[\"synthetic\"][\"metrics\"][\"val\"]\n    print(\"Train losses per epoch:\", train_losses)\n    print(\"Validation losses per epoch:\", val_losses)\n    # Compute Spearman correlations\n    preds_list = experiment_data[\"synthetic\"][\"predictions\"]\n    true_list = experiment_data[\"synthetic\"][\"ground_truth\"]\n\n    def spearman_corr(a, b):\n        a_rank = np.argsort(np.argsort(a))\n        b_rank = np.argsort(np.argsort(b))\n        return np.corrcoef(a_rank, b_rank)[0, 1]\n\n    corrs = [spearman_corr(p, t) for p, t in zip(preds_list, true_list)]\n    print(\"Spearman correlations per epoch:\", corrs)\n\n    # Plot 1: Loss curves\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(train_losses) + 1)\n        plt.plot(epochs, train_losses, label=\"Train Loss\")\n        plt.plot(epochs, val_losses, label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"Synthetic Dataset: Training and Validation Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve: {e}\")\n        plt.close()\n\n    # Plot 2: Spearman correlation per epoch\n    try:\n        plt.figure()\n        plt.plot(epochs, corrs, marker=\"o\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Spearman Correlation\")\n        plt.title(\"Synthetic Dataset: DVN Prediction Correlation per Epoch\")\n        plt.savefig(os.path.join(working_dir, \"synthetic_correlation_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating correlation curve: {e}\")\n        plt.close()\n\n    # Plot 3: Histogram of last-epoch contributions\n    try:\n        last_idx = len(preds_list) - 1\n        true_vals = true_list[last_idx]\n        pred_vals = preds_list[last_idx]\n        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n        axes[0].hist(true_vals, bins=10)\n        axes[0].set_title(\"Ground Truth\")\n        axes[1].hist(pred_vals, bins=10)\n        axes[1].set_title(\"Predicted Samples\")\n        fig.suptitle(\n            f\"Synthetic Dataset: Contribution Distribution (Epoch {last_idx})\\n\"\n            \"Left: Ground Truth, Right: Predicted Samples\"\n        )\n        plt.savefig(\n            os.path.join(\n                working_dir, f\"synthetic_contribution_dist_epoch_{last_idx}.png\"\n            )\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating distribution plot: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    syn = experiment_data[\"hyperparam_tuning_type_1\"][\"synthetic\"]\n    param_values = syn[\"param_values\"]\n    loss_train = syn[\"losses\"][\"train\"]\n    loss_val = syn[\"losses\"][\"val\"]\n    corrs = syn[\"correlations\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot training/validation loss curves\ntry:\n    plt.figure()\n    for idx, p in enumerate(param_values):\n        epochs = np.arange(1, len(loss_train[idx]) + 1)\n        plt.plot(epochs, loss_train[idx], label=f\"{p} epochs train\")\n        plt.plot(epochs, loss_val[idx], linestyle=\"--\", label=f\"{p} epochs val\")\n    plt.suptitle(\"Synthetic Dataset Training/Validation Loss\")\n    plt.title(\"Solid: Training Loss, Dashed: Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# Plot Spearman correlation curves\ntry:\n    plt.figure()\n    for idx, p in enumerate(param_values):\n        epochs = np.arange(1, len(corrs[idx]) + 1)\n        plt.plot(epochs, corrs[idx], marker=\"o\", label=f\"{p} epochs\")\n    plt.suptitle(\"Synthetic Dataset Spearman Correlation\")\n    plt.title(\"Correlation of DVN Predictions vs True Contributions\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Spearman Corr\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_spearman_corr.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Spearman correlation plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# Helper for Spearman correlation\ndef spearman_corr(a, b):\n    ar = np.argsort(np.argsort(a))\n    br = np.argsort(np.argsort(b))\n    return np.corrcoef(ar, br)[0, 1]\n\n\n# Plot train/val loss per learning rate (up to 5)\nlr_data = experiment_data.get(\"hyperparam_tuning_main_model_lr\", {}).get(\n    \"synthetic\", {}\n)\nfor idx, (lr_key, data) in enumerate(lr_data.items()):\n    if idx >= 5:\n        break\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(data[\"losses\"][\"train\"]) + 1)\n        plt.plot(epochs, data[\"losses\"][\"train\"], label=\"Train Loss\")\n        plt.plot(epochs, data[\"losses\"][\"val\"], label=\"Validation Loss\")\n        plt.title(\"synthetic dataset Loss Curves\\nTrain vs Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        fname = os.path.join(working_dir, f\"synthetic_train_val_loss_{lr_key}.png\")\n        plt.savefig(fname)\n    except Exception as e:\n        print(f\"Error creating loss plot for {lr_key}: {e}\")\n    finally:\n        plt.close()\n\n# Plot Spearman correlation vs epoch for all learning rates\ntry:\n    plt.figure()\n    for lr_key, data in lr_data.items():\n        corrs = [\n            spearman_corr(p, t)\n            for p, t in zip(data.get(\"predictions\", []), data.get(\"ground_truth\", []))\n        ]\n        epochs = np.arange(1, len(corrs) + 1)\n        plt.plot(epochs, corrs, label=lr_key)\n    plt.title(\n        \"synthetic dataset Spearman Correlation\\nDVN Predictions vs True Contrasts\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Spearman Corr\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"synthetic_spearman_correlation.png\")\n    plt.savefig(fname)\nexcept Exception as e:\n    print(f\"Error creating Spearman correlation plot: {e}\")\nfinally:\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Extract synthetic dataset results\nsd = experiment_data.get(\"batch_size\", {}).get(\"synthetic\", {})\nbatch_sizes = sd.get(\"batch_sizes\", [])\ntrain_metrics = sd.get(\"metrics\", {}).get(\"train\", [])\nval_metrics = sd.get(\"metrics\", {}).get(\"val\", [])\npredictions = sd.get(\"predictions\", [])\nground_truth = sd.get(\"ground_truth\", [])\n\n# Plot training and validation loss curves\ntry:\n    plt.figure()\n    epochs = range(1, len(train_metrics[0]) + 1) if train_metrics else []\n    for i, bsz in enumerate(batch_sizes):\n        plt.plot(epochs, train_metrics[i], label=f\"Train bs={bsz}\")\n        plt.plot(epochs, val_metrics[i], \"--\", label=f\"Val bs={bsz}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Synthetic Dataset Loss Curves\\nTraining and Validation Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"synthetic_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\n\n# Define Spearman correlation\ndef spearman_corr(a, b):\n    ar = np.argsort(np.argsort(a))\n    br = np.argsort(np.argsort(b))\n    return np.corrcoef(ar, br)[0, 1]\n\n\n# Plot DVN Spearman correlations\ntry:\n    plt.figure()\n    epochs = range(1, len(predictions[0]) + 1) if predictions else []\n    for i, bsz in enumerate(batch_sizes):\n        corrs = []\n        for ep in range(len(predictions[i])):\n            preds = np.array(predictions[i][ep])\n            trues = np.array(ground_truth[i][ep])\n            corrs.append(spearman_corr(preds, trues))\n        plt.plot(epochs, corrs, marker=\"o\", label=f\"bs={bsz}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Spearman Correlation\")\n    plt.title(\n        \"Synthetic Dataset DVN Spearman Correlation\\nMeta-sample Predictions vs True Improvement\"\n    )\n    plt.legend()\n    fname = os.path.join(working_dir, \"synthetic_spearman_corr.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data and summarize\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    sd = exp[\"softmax_temperature\"][\"synthetic\"]\n    T_vals = sd[\"T_values\"]\n    train_losses = sd[\"losses\"][\"train\"]\n    val_losses = sd[\"losses\"][\"val\"]\n    correlations = sd[\"correlation\"]\n    preds = sd[\"predictions\"]\n    gts = sd[\"ground_truth\"]\n    print(\"Final metrics by T:\")\n    for T, tr, va, corr in zip(T_vals, train_losses, val_losses, correlations):\n        print(\n            f\" T={T}: Train Loss={tr[-1]:.4f}, Val Loss={va[-1]:.4f}, DVN Spearman={corr[-1]:.4f}\"\n        )\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot train/val loss curves\ntry:\n    plt.figure()\n    for T, tr, va in zip(T_vals, train_losses, val_losses):\n        epochs = range(1, len(tr) + 1)\n        plt.plot(epochs, tr, label=f\"Train T={T}\")\n        plt.plot(epochs, va, \"--\", label=f\"Val T={T}\")\n    plt.title(\"Synthetic dataset Loss Curves (Train & Val)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"synthetic_loss_curves_all_T.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Plot DVN Spearman correlation curves\ntry:\n    plt.figure()\n    for T, corr in zip(T_vals, correlations):\n        epochs = range(1, len(corr) + 1)\n        plt.plot(epochs, corr, marker=\"o\", label=f\"T={T}\")\n    plt.title(\"Synthetic dataset DVN Spearman Correlation vs Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Spearman Corr\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"synthetic_dvn_correlation_all_T.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating correlation plot: {e}\")\n    plt.close()\n\n# Scatter of DVN predictions vs ground truth at final epoch\ntry:\n    plt.figure()\n    for T, p_list, g_list in zip(T_vals, preds, gts):\n        p_last = p_list[-1]\n        g_last = g_list[-1]\n        plt.scatter(g_last, p_last, label=f\"T={T}\", alpha=0.7)\n    plt.title(\n        \"Synthetic dataset DVN Predictions vs Ground Truth (Epoch {})\".format(\n            len(train_losses[0])\n        )\n    )\n    plt.xlabel(\"True Contribution\")\n    plt.ylabel(\"Predicted Contribution\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"synthetic_pred_vs_gt_last_epoch.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating pred vs gt scatter: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# 1) Loss curves\ntry:\n    plt.figure()\n    syn = experiment_data[\"DVN_LR\"][\"synthetic\"]\n    for lr, exp in syn.items():\n        epochs = np.arange(len(exp[\"metrics\"][\"train\"]))\n        plt.plot(epochs, exp[\"metrics\"][\"train\"], label=f\"{lr}-train\")\n        plt.plot(epochs, exp[\"metrics\"][\"val\"], label=f\"{lr}-val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Synthetic dataset - Loss Curves\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n\n# 2) Spearman correlation per epoch\ndef spearman_corr(a, b):\n    ar, br = np.argsort(np.argsort(a)), np.argsort(np.argsort(b))\n    return np.corrcoef(ar, br)[0, 1]\n\n\ntry:\n    plt.figure()\n    for lr, exp in syn.items():\n        corrs = [\n            spearman_corr(p, g) for p, g in zip(exp[\"predictions\"], exp[\"ground_truth\"])\n        ]\n        plt.plot(range(len(corrs)), corrs, label=f\"LR {lr}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Spearman Correlation\")\n    plt.title(\"Synthetic dataset - Spearman Correlation per Epoch\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_spearman_per_epoch.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating spearman correlation plot: {e}\")\n    plt.close()\n\n# 3) Scatter of predictions vs ground truth at last epoch\ntry:\n    lrs = list(syn.keys())\n    fig, axes = plt.subplots(1, len(lrs), figsize=(5 * len(lrs), 5))\n    for ax, lr in zip(np.atleast_1d(axes).flatten(), lrs):\n        preds = syn[lr][\"predictions\"][-1]\n        gt = syn[lr][\"ground_truth\"][-1]\n        ax.scatter(gt, preds)\n        ax.set_xlabel(\"Ground Truth Contrast\")\n        ax.set_ylabel(\"Predicted Contrast\")\n        ax.set_title(f\"LR {lr}\")\n    fig.suptitle(\n        f\"Synthetic dataset - Predictions vs Ground Truth (Last Epoch)\\n\"\n        f\"Subplots: Left: LR {lrs[0]}, Center: LR {lrs[1]}, Right: LR {lrs[-1]}\"\n        if len(lrs) == 3\n        else \"\"\n    )\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(os.path.join(working_dir, \"synthetic_pred_vs_gt_last_epoch.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating predictions vs ground truth plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# load data\ntry:\n    ed = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# 1) Plot avg train/val loss curves\ntry:\n    mdata = ed[\"weight_decay\"][\"synthetic\"][\"metrics\"]\n    train = np.array(mdata[\"train\"])  # shape (runs, epochs)\n    val = np.array(mdata[\"val\"])\n    epochs = np.arange(1, train.shape[1] + 1)\n    mean_train = train.mean(axis=0)\n    mean_val = val.mean(axis=0)\n    plt.figure()\n    plt.plot(epochs, mean_train, \"-o\")\n    plt.plot(epochs, mean_val, \"-o\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"MSE Loss\")\n    plt.title(\"Synthetic Dataset - Training and Validation Loss Curves\")\n    plt.legend([\"Train Loss\", \"Val Loss\"])\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# 2) Heatmap of final validation loss over weight decay grid\ntry:\n    wd_main = np.array(ed[\"weight_decay\"][\"synthetic\"][\"weight_decay_main\"], float)\n    wd_dvn = np.array(ed[\"weight_decay\"][\"synthetic\"][\"weight_decay_dvn\"], float)\n    final_val = np.array(ed[\"weight_decay\"][\"synthetic\"][\"metrics\"][\"val\"])[:, -1]\n    um = np.unique(wd_main)\n    ud = np.unique(wd_dvn)\n    mat = final_val.reshape(len(um), len(ud))\n    plt.figure()\n    im = plt.imshow(mat, aspect=\"auto\", interpolation=\"nearest\")\n    plt.colorbar(im)\n    plt.xticks(np.arange(len(ud)), [str(d) for d in ud])\n    plt.yticks(np.arange(len(um)), [str(m) for m in um])\n    plt.xlabel(\"Weight Decay DVN\")\n    plt.ylabel(\"Weight Decay Main\")\n    plt.title(\"Synthetic Dataset - Final Validation Loss Heatmap\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_val_loss_heatmap.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating heatmap: {e}\")\n    plt.close()\n\n# 3) Spearman correlation vs epoch\ntry:\n    preds = np.array(\n        ed[\"weight_decay\"][\"synthetic\"][\"predictions\"]\n    )  # (runs, epochs, samples)\n    gts = np.array(ed[\"weight_decay\"][\"synthetic\"][\"ground_truth\"])\n    runs, epochs_n, _ = preds.shape\n\n    def spearman(a, b):\n        ar = np.argsort(np.argsort(a))\n        br = np.argsort(np.argsort(b))\n        return np.corrcoef(ar, br)[0, 1]\n\n    corr_mat = np.zeros((runs, epochs_n))\n    for i in range(runs):\n        for j in range(epochs_n):\n            corr_mat[i, j] = spearman(preds[i, j], gts[i, j])\n    mean_corr = corr_mat.mean(axis=0)\n    plt.figure()\n    plt.plot(np.arange(1, epochs_n + 1), mean_corr, \"-o\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Spearman Correlation\")\n    plt.title(\"Synthetic Dataset - DVN Spearman Corr vs Epoch\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_spearman_corr.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Spearman plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    ds = data[\"meta_sample_tuning\"][\"synthetic\"]\n    metas = ds[\"meta_sample_values\"]\n    train_losses = ds[\"metrics\"][\"train\"]\n    val_losses = ds[\"metrics\"][\"val\"]\n    corrs = ds[\"corr\"]\n    preds = ds[\"predictions\"]\n    truths = ds[\"ground_truth\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    train_losses = val_losses = corrs = preds = truths = metas = []\n\n# Plot training & validation loss curves\ntry:\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 2, 1)\n    for m, tl in zip(metas, train_losses):\n        plt.plot(range(len(tl)), tl, label=f\"meta={m}\")\n    plt.title(\"Training Loss Curves\\nSynthetic Dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    for m, vl in zip(metas, val_losses):\n        plt.plot(range(len(vl)), vl, label=f\"meta={m}\")\n    plt.title(\"Validation Loss Curves\\nSynthetic Dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n\n    plt.suptitle(\"Loss vs Epoch for Meta-sample Tuning\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Plot Spearman correlation curves\ntry:\n    plt.figure()\n    for m, cr in zip(metas, corrs):\n        plt.plot(range(len(cr)), cr, marker=\"o\", label=f\"meta={m}\")\n    plt.title(\"Spearman Correlation vs Epoch\\nSynthetic Dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Spearman \u03c1\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_spearman_corr.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating spearman correlation plot: {e}\")\n    plt.close()\n\n# Scatter predicted vs ground truth at final epoch for largest meta-sample\ntry:\n    if metas:\n        idx = len(metas) - 1\n        pred_last = preds[idx][-1]\n        truth_last = truths[idx][-1]\n        plt.figure()\n        plt.scatter(truth_last, pred_last, alpha=0.7)\n        plt.title(f\"Pred vs Truth at Final Epoch\\nSynthetic Dataset, meta={metas[idx]}\")\n        plt.xlabel(\"Ground Truth \u0394Val\")\n        plt.ylabel(\"Predicted \u0394Val\")\n        fname = f\"synthetic_pred_vs_truth_meta{metas[idx]}.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating scatter plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n\n# Helper for Spearman correlation\ndef spearmanr(a, b):\n    ar = np.argsort(np.argsort(a))\n    br = np.argsort(np.argsort(b))\n    return np.corrcoef(ar, br)[0, 1]\n\n\n# Plot 1: training & validation loss curves\ntry:\n    plt.figure()\n    hyp = experiment_data.get(\"hyperparam_tuning_type_1\", {})\n    for steps, cfg in hyp.items():\n        m = cfg[\"synthetic\"][\"metrics\"]\n        epochs = np.arange(1, len(m[\"train\"]) + 1)\n        plt.plot(epochs, m[\"train\"], label=f\"{steps} steps train\", linestyle=\"-\")\n        plt.plot(epochs, m[\"val\"], label=f\"{steps} steps val\", linestyle=\"--\")\n    plt.title(\"Synthetic Dataset: Training & Validation Loss\\nSolid=train, Dashed=val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\n# Plot 2: Spearman correlation over epochs\ntry:\n    plt.figure()\n    for steps, cfg in hyp.items():\n        preds = cfg[\"synthetic\"][\"predictions\"]\n        trues = cfg[\"synthetic\"][\"ground_truth\"]\n        cors = [spearmanr(p, t) for p, t in zip(preds, trues)]\n        plt.plot(np.arange(1, len(cors) + 1), cors, label=f\"{steps} steps\")\n    plt.title(\n        \"Synthetic Dataset: Spearman Correlation over Epochs\\nDVN pred vs true contribution\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Spearman \u03c1\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_spearman_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n\n# Plot 3: scatter predicted vs ground truth at final epoch for 10 steps\ntry:\n    key = \"10\"\n    cfg = hyp.get(key, {})\n    p_final = np.array(cfg[\"synthetic\"][\"predictions\"][-1])\n    t_final = np.array(cfg[\"synthetic\"][\"ground_truth\"][-1])\n    plt.figure()\n    plt.scatter(t_final, p_final, alpha=0.7)\n    plt.title(\n        \"Synthetic Dataset: Pred vs GT Contributions (10 Steps)\\nX: Ground Truth, Y: Predicted\"\n    )\n    plt.xlabel(\"True Contribution\")\n    plt.ylabel(\"Predicted Contribution\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_scatter_steps_10.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot3: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Plot training and validation loss curves\ntry:\n    beta_data = experiment_data.get(\"ADAM_BETA1\", {})\n    plt.figure()\n    for beta1, info in sorted(beta_data.items(), key=lambda x: float(x[0])):\n        m = info[\"synthetic\"][\"metrics\"]\n        epochs = np.arange(len(m[\"train\"]))\n        plt.plot(epochs, m[\"train\"], label=f\"\u03b21={beta1} Train\")\n        plt.plot(epochs, m[\"val\"], \"--\", label=f\"\u03b21={beta1} Val\")\n    plt.title(\"Synthetic Dataset Training & Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    path1 = os.path.join(working_dir, \"synthetic_training_val_loss.png\")\n    plt.savefig(path1)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\n# Plot final\u2010epoch DVN predictions vs ground truth\ncors = {}\ntry:\n    beta_data = experiment_data.get(\"ADAM_BETA1\", {})\n    n = len(beta_data)\n    fig, axes = plt.subplots(1, n, figsize=(5 * n, 4))\n    axes = np.atleast_1d(axes)\n    for ax, (beta1, info) in zip(\n        axes, sorted(beta_data.items(), key=lambda x: float(x[0]))\n    ):\n        preds = np.array(info[\"synthetic\"][\"predictions\"][-1])\n        truth = np.array(info[\"synthetic\"][\"ground_truth\"][-1])\n        corr = spearman_corr(preds, truth)\n        cors[beta1] = corr\n        ax.scatter(truth, preds, alpha=0.7)\n        ax.set_title(f\"\u03b21={beta1}\\nSpearman={corr:.2f}\")\n        ax.set_xlabel(\"True Influence\")\n        ax.set_ylabel(\"Predicted Influence\")\n    fig.suptitle(\"Synthetic Dataset DVN Influence Prediction (Final Epoch)\")\n    path2 = os.path.join(working_dir, \"synthetic_dvn_scatter_final_epoch.png\")\n    plt.savefig(path2)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n\n# Print final\u2010epoch Spearman correlations\nfor beta1, corr in sorted(cors.items(), key=lambda x: float(x[0])):\n    print(f\"Final epoch Spearman Corr for \u03b21={beta1}: {corr:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# Load and process experiment data\ntry:\n    experiment_data = np.load(data_path, allow_pickle=True).item()\n    d = experiment_data[\"meta_update_lr_tuning\"][\"synthetic\"]\n    meta_lrs = d[\"meta_lrs\"]\n    train_losses = d[\"losses\"][\"train\"]\n    val_losses = d[\"losses\"][\"val\"]\n    preds = d[\"predictions\"]\n    truths = d[\"ground_truth\"]\n\n    def spearman_corr(a, b):\n        ar = np.argsort(np.argsort(a))\n        br = np.argsort(np.argsort(b))\n        return np.corrcoef(ar, br)[0, 1]\n\n    spearman = [\n        [\n            spearman_corr(np.array(preds[i][j]), np.array(truths[i][j]))\n            for j in range(len(preds[i]))\n        ]\n        for i in range(len(meta_lrs))\n    ]\nexcept Exception as e:\n    print(f\"Error loading or processing experiment data: {e}\")\n\n# Plot 1: Loss curves for all meta_lrs\ntry:\n    plt.figure()\n    for i, lr in enumerate(meta_lrs):\n        epochs = np.arange(1, len(train_losses[i]) + 1)\n        plt.plot(epochs, train_losses[i], label=f\"Train lr={lr:.1e}\")\n        plt.plot(epochs, val_losses[i], linestyle=\"--\", label=f\"Val lr={lr:.1e}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Synthetic Dataset - Train & Val Loss Curves\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Plot 2: Final validation loss vs meta_lr\ntry:\n    final_val = [vl[-1] for vl in val_losses]\n    plt.figure()\n    plt.semilogx(meta_lrs, final_val, marker=\"o\")\n    plt.xlabel(\"MetaLR\")\n    plt.ylabel(\"Final Validation Loss\")\n    plt.title(\"Synthetic Dataset - Final Val Loss vs MetaLR\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_final_val_loss_vs_metalr.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final val loss vs meta_lr plot: {e}\")\n    plt.close()\n\n# Plot 3: Spearman correlation vs epoch for each meta_lr\ntry:\n    plt.figure()\n    for i, lr in enumerate(meta_lrs):\n        epochs = np.arange(1, len(spearman[i]) + 1)\n        plt.plot(epochs, spearman[i], marker=\"o\", label=f\"lr={lr:.1e}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Spearman Correlation\")\n    plt.title(\"Synthetic Dataset - Spearman Corr vs Epoch\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_spearman_vs_epoch.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Spearman correlation plot: {e}\")\n    plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nexp = experiment_data.get(\"ADAM_BETA2\", {})\ndataset = \"synthetic\"\n\n# Plot 1: Loss curves\ntry:\n    plt.figure()\n    for key, val in exp.items():\n        b2 = key.split(\"_\", 1)[1]\n        train = val[dataset][\"losses\"][\"train\"]\n        val_loss = val[dataset][\"losses\"][\"val\"]\n        epochs = range(len(train))\n        plt.plot(epochs, train, label=f\"beta2={b2} train\")\n        plt.plot(epochs, val_loss, linestyle=\"--\", label=f\"beta2={b2} val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\n        \"Training vs Validation Loss (Synthetic Dataset)\\nTrain (solid) and Val (dashed)\"\n    )\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n\n# Spearman helper\ndef spearman_corr(a, b):\n    ar = np.argsort(np.argsort(a))\n    br = np.argsort(np.argsort(b))\n    return np.corrcoef(ar, br)[0, 1]\n\n\n# Plot 2: Spearman correlation\ntry:\n    plt.figure()\n    for key, val in exp.items():\n        b2 = key.split(\"_\", 1)[1]\n        preds = val[dataset][\"predictions\"]\n        truths = val[dataset][\"ground_truth\"]\n        corrs = [spearman_corr(p, t) for p, t in zip(preds, truths)]\n        plt.plot(range(len(corrs)), corrs, marker=\"o\", label=f\"beta2={b2}\")\n        print(f\"Final Spearman (beta2={b2}): {corrs[-1]:.4f}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Spearman Corr\")\n    plt.title(\"Spearman Correlation vs Epoch (Synthetic Dataset)\\nHigher is better\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_spearman_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating spearman plot: {e}\")\n    plt.close()\n\n# Plots 3\u20135: Scatter at final epoch\nfor key, val in exp.items():\n    b2 = key.split(\"_\", 1)[1]\n    try:\n        plt.figure()\n        preds = np.array(val[dataset][\"predictions\"][-1])\n        truths = np.array(val[dataset][\"ground_truth\"][-1])\n        plt.scatter(truths, preds, alpha=0.6)\n        plt.xlabel(\"Ground Truth\")\n        plt.ylabel(\"Predicted Meta-feedback\")\n        plt.title(\n            f\"Pred vs True Scatter (Synthetic Dataset, beta2={b2})\\nScatter of model outputs\"\n        )\n        plt.savefig(os.path.join(working_dir, f\"synthetic_scatter_beta2_{b2}.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating scatter_beta2_{b2}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    syn = experiment_data[\"hyperparam_tuning_type_1\"][\"synthetic\"]\n    param_values = syn[\"param_values\"]\n    loss_train = syn[\"losses\"][\"train\"]\n    loss_val = syn[\"losses\"][\"val\"]\n    corrs = syn[\"correlations\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot training/validation loss curves\ntry:\n    plt.figure()\n    for idx, p in enumerate(param_values):\n        epochs = np.arange(1, len(loss_train[idx]) + 1)\n        plt.plot(epochs, loss_train[idx], label=f\"{p} epochs train\")\n        plt.plot(epochs, loss_val[idx], linestyle=\"--\", label=f\"{p} epochs val\")\n    plt.suptitle(\"Synthetic Dataset Training/Validation Loss\")\n    plt.title(\"Solid: Training Loss, Dashed: Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# Plot Spearman correlation curves\ntry:\n    plt.figure()\n    for idx, p in enumerate(param_values):\n        epochs = np.arange(1, len(corrs[idx]) + 1)\n        plt.plot(epochs, corrs[idx], marker=\"o\", label=f\"{p} epochs\")\n    plt.suptitle(\"Synthetic Dataset Spearman Correlation\")\n    plt.title(\"Correlation of DVN Predictions vs True Contributions\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Spearman Corr\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_spearman_corr.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Spearman correlation plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    syn = experiment_data[\"hyperparam_tuning_type_1\"][\"synthetic\"]\n    param_values = syn[\"param_values\"]\n    loss_train = syn[\"losses\"][\"train\"]\n    loss_val = syn[\"losses\"][\"val\"]\n    corrs = syn[\"correlations\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot training/validation loss curves\ntry:\n    plt.figure()\n    for idx, p in enumerate(param_values):\n        epochs = np.arange(1, len(loss_train[idx]) + 1)\n        plt.plot(epochs, loss_train[idx], label=f\"{p} epochs train\")\n        plt.plot(epochs, loss_val[idx], linestyle=\"--\", label=f\"{p} epochs val\")\n    plt.suptitle(\"Synthetic Dataset Training/Validation Loss\")\n    plt.title(\"Solid: Training Loss, Dashed: Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# Plot Spearman correlation curves\ntry:\n    plt.figure()\n    for idx, p in enumerate(param_values):\n        epochs = np.arange(1, len(corrs[idx]) + 1)\n        plt.plot(epochs, corrs[idx], marker=\"o\", label=f\"{p} epochs\")\n    plt.suptitle(\"Synthetic Dataset Spearman Correlation\")\n    plt.title(\"Correlation of DVN Predictions vs True Contributions\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Spearman Corr\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_spearman_corr.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Spearman correlation plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    syn = experiment_data[\"hyperparam_tuning_type_1\"][\"synthetic\"]\n    param_values = syn[\"param_values\"]\n    loss_train = syn[\"losses\"][\"train\"]\n    loss_val = syn[\"losses\"][\"val\"]\n    corrs = syn[\"correlations\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot training/validation loss curves\ntry:\n    plt.figure()\n    for idx, p in enumerate(param_values):\n        epochs = np.arange(1, len(loss_train[idx]) + 1)\n        plt.plot(epochs, loss_train[idx], label=f\"{p} epochs train\")\n        plt.plot(epochs, loss_val[idx], linestyle=\"--\", label=f\"{p} epochs val\")\n    plt.suptitle(\"Synthetic Dataset Training/Validation Loss\")\n    plt.title(\"Solid: Training Loss, Dashed: Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# Plot Spearman correlation curves\ntry:\n    plt.figure()\n    for idx, p in enumerate(param_values):\n        epochs = np.arange(1, len(corrs[idx]) + 1)\n        plt.plot(epochs, corrs[idx], marker=\"o\", label=f\"{p} epochs\")\n    plt.suptitle(\"Synthetic Dataset Spearman Correlation\")\n    plt.title(\"Correlation of DVN Predictions vs True Contributions\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Spearman Corr\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_spearman_corr.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Spearman correlation plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_data_path_list = [\n    \"experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_fce85537044142eea1d3340b1d92973a_proc_234629/experiment_data.npy\",\n    \"experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_99ea040080fe490e8ba4afac99d6ae99_proc_234630/experiment_data.npy\",\n    \"experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_b78f5529dcd3453f9ebfa65101d0f2a0_proc_234628/experiment_data.npy\",\n]\n\n# Load synthetic results from each run\nall_synthetic = []\ntry:\n    for path in experiment_data_path_list:\n        data = np.load(\n            os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\"), path), allow_pickle=True\n        ).item()\n        syn = data[\"hyperparam_tuning_type_1\"][\"synthetic\"]\n        all_synthetic.append(syn)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Extract parameters and number of runs\nparam_values = all_synthetic[0][\"param_values\"]\nn_runs = len(all_synthetic)\n\n# Stack metrics across runs: shape (runs, params, epochs)\nloss_train_runs = np.array([syn[\"losses\"][\"train\"] for syn in all_synthetic])\nloss_val_runs = np.array([syn[\"losses\"][\"val\"] for syn in all_synthetic])\ncorrs_runs = np.array([syn[\"correlations\"] for syn in all_synthetic])\n\n# Compute mean and SEM\nmean_train = np.mean(loss_train_runs, axis=0)\nsem_train = np.std(loss_train_runs, axis=0) / np.sqrt(n_runs)\nmean_val = np.mean(loss_val_runs, axis=0)\nsem_val = np.std(loss_val_runs, axis=0) / np.sqrt(n_runs)\nmean_corr = np.mean(corrs_runs, axis=0)\nsem_corr = np.std(corrs_runs, axis=0) / np.sqrt(n_runs)\n\n# Plot aggregated training/validation loss curves\ntry:\n    plt.figure()\n    for idx, p in enumerate(param_values):\n        epochs = np.arange(1, mean_train.shape[1] + 1)\n        plt.plot(epochs, mean_train[idx], label=f\"{p} epochs train\")\n        plt.fill_between(\n            epochs,\n            mean_train[idx] - sem_train[idx],\n            mean_train[idx] + sem_train[idx],\n            alpha=0.2,\n        )\n        plt.plot(epochs, mean_val[idx], linestyle=\"--\", label=f\"{p} epochs val\")\n        plt.fill_between(\n            epochs,\n            mean_val[idx] - sem_val[idx],\n            mean_val[idx] + sem_val[idx],\n            alpha=0.2,\n        )\n    plt.suptitle(\"Synthetic Dataset Training/Validation Loss (Mean \u00b1 SE)\")\n    plt.title(\"Solid: Mean Train Loss, Dashed: Mean Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves_aggregated.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated loss curves: {e}\")\n    plt.close()\n\n# Plot aggregated Spearman correlation curves\ntry:\n    plt.figure()\n    for idx, p in enumerate(param_values):\n        epochs = np.arange(1, mean_corr.shape[1] + 1)\n        plt.plot(epochs, mean_corr[idx], marker=\"o\", label=f\"{p} epochs\")\n        plt.fill_between(\n            epochs,\n            mean_corr[idx] - sem_corr[idx],\n            mean_corr[idx] + sem_corr[idx],\n            alpha=0.2,\n        )\n    plt.suptitle(\"Synthetic Dataset Spearman Correlation (Mean \u00b1 SE)\")\n    plt.title(\"Correlation of DVN Predictions vs True Contributions\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Spearman Corr\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_spearman_corr_aggregated.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating aggregated Spearman correlation plot: {e}\")\n    plt.close()\n\n# Print final epoch evaluation metrics\ntry:\n    for idx, p in enumerate(param_values):\n        final_mean = mean_corr[idx, -1]\n        final_se = sem_corr[idx, -1]\n        print(f\"Param {p}: Final Spearman Corr = {final_mean:.3f} \u00b1 {final_se:.3f}\")\nexcept Exception as e:\n    print(f\"Error printing evaluation metrics: {e}\")\n"], "plot_plan": ["We load the saved experiment data, extract and print training/validation losses, then compute and print per-epoch Spearman correlations between DVN predictions and true contributions. We wrap each plot in its own try-except block, ensure that we close figures after saving, and save plots to the working directory with descriptive filenames. The first figure shows the training and validation loss curves; the second shows the DVN prediction correlation per epoch; the third is a two-panel histogram comparing ground truth vs. predicted contributions for the last epoch. All plotting uses basic matplotlib and clear titles/subtitles indicating the dataset and subplot roles.", "We will load the saved experiment_data.npy from working_dir and select the synthetic dataset results. Then for each hyperparameter value (EPOCHS), we extract the per-epoch training and validation losses and Spearman correlations. We create two plots: (1) Training and validation loss curves for each EPOCHS run on a single figure, using solid lines for training and dashed lines for validation, (2) Spearman correlation versus epoch for each EPOCHS value with distinct markers. Each figure is generated inside a try-except block to catch errors, includes a main title specifying \"Synthetic Dataset\" and a clear subtitle, and is saved to working_dir with descriptive filenames. We add legends, axis labels, and include both suptitle and title for clarity. After saving, we ensure the figure is closed whether the plot succeeded or an exception occurred. This approach provides a concise visual summary of the training dynamics and DVN performance across hyperparameter sweeps.", "We first load the experiment data from the working directory with error handling. Then for each of the first five learning rates under the synthetic dataset, we plot the training and validation losses versus epochs in separate try-except blocks, titling each figure with the dataset and a subtitle indicating train vs validation, and saving them under descriptive filenames. Next, we compute the Spearman rank correlation at each epoch between the DVN predictions and the true contrasts using a NumPy-based function, and plot all five correlation curves together in a single figure with a legend, again in a try-except block. Each plot uses basic matplotlib, saves to working_dir, and is closed after saving or on error. We include clear titles and subtitles for both loss curves and correlation plots, and filenames that specify the dataset and plot type.", "We first load the saved experiment_data.npy file, catching any load errors. Next, we extract batch sizes, training and validation losses, and DVN meta-sample predictions and ground truths for the synthetic dataset. We create a training/validation loss curve figure, plotting both curves for each batch size across epochs, and save it under a descriptive filename. We then define a simple Spearman correlation function in NumPy, compute the correlation between predicted and true meta-sample improvements for each batch size and epoch, and plot these curves. Each plotting routine is wrapped in its own try-except block to handle any runtime issues, and all figures are closed after saving. Titles explicitly mention the synthetic dataset and include subtitles describing the content. All plots are saved to the working directory. We use only basic matplotlib calls for clarity and reproducibility.", "We first load the saved experiment data and extract for the synthetic dataset the temperature values, per-epoch training and validation losses, DVN Spearman correlations, and predictions vs. ground truth meta\u2010contributions. We then print a summary of the final train/val losses and DVN correlations for each temperature to verify our metrics. Next, we make three separate plots in isolated try-except blocks: (1) a combined training and validation loss curve over epochs for all temperatures; (2) a combined DVN Spearman correlation curve over epochs for all temperatures; and (3) a combined scatter of DVN predictions versus true meta-contributions at the final epoch for each temperature. Each plot includes a clear title specifying the synthetic dataset, proper axis labels, and a legend, and is saved to the working directory with descriptive filenames. Figures are closed immediately after saving to manage resources. Error handling ensures any figure creation errors are logged without interrupting other plots. All plotting uses basic matplotlib calls and only data actually present in the .npy file.", "I will load the saved experiment_data from the working directory and then create three sets of plots, each wrapped in its own try-except block. First, I\u2019ll plot training and validation losses across epochs for each DVN learning rate, labeling curves, axes, and saving with a descriptive filename. Second, I'll recompute Spearman correlations epoch\u2010by\u2010epoch from stored predictions and ground truths, plot these per\u2010lr curves, label and save. Third, I will make a multi\u2010panel scatter of predicted vs true meta\u2010loss differences at the last epoch for each lr, using clear per\u2010subplot titles and a suptitle that specifies dataset and left/center/right placement. Each block catches errors, prints a message if it fails, and always closes figures to avoid resource leaks. Finally, I only use matplotlib and numpy and ensure filenames include the dataset name and plot type.", "I will load the saved experiment_data and then produce three figures: first, the average training and validation loss curves across all weight decay settings for the synthetic dataset; second, a heatmap of final validation loss over the grid of main and DVN weight decay values; and third, the average Spearman correlation of DVN predictions versus true improvements across epochs. Each plot is generated in its own try-except block to handle errors, and figures are saved in the working directory with descriptive names. The code uses basic matplotlib, closes every figure after saving, and includes clear titles and axis labels.", "We will load the saved experiment_data.npy from the working directory and extract the synthetic dataset results.  Then we will plot three figures within separate try-except blocks: (1) a combined training/validation loss curve figure with two side\u2010by\u2010side subplots, showing loss vs epoch for each meta\u2010sample value; (2) a Spearman correlation vs epoch curve for each meta\u2010sample value; and (3) a scatter plot of predicted vs ground\u2010truth contributions at the final epoch for the largest meta\u2010sample setting.  Each plot will have clear titles and axis labels specifying the synthetic dataset, and all figures will be saved to the working directory with descriptive filenames.  We ensure to close each figure after saving or upon catching errors, and we use only data present in experiment_data.npy.", "We will load the saved experiment data and then produce three plots inside separate try-except blocks. First, we plot training and validation loss curves over epochs for each DVN step setting on the synthetic dataset, using solid lines for training and dashed for validation. Next, we compute the Spearman rank correlation between DVN predictions and true contribution values at each epoch for each hyperparameter, and plot these correlation curves. Finally, we create a scatter plot of predicted versus ground truth contributions at the last epoch for the 10-step configuration, with an appropriate title and subtitle. Each plot is saved into the working directory with a descriptive filename, and figures are closed after saving or on error. We include an inline function to compute Spearman correlation ranks. This approach yields concise diagnostics of loss behavior and DVN prediction fidelity across different update frequencies.", "We will load the saved experiment data for the synthetic dataset under the ADAM_BETA1 sweep and extract all training and validation losses per \u03b21. These will be plotted together in a single figure with clear legends and saved to working_dir. Next, we pull out the stored DVN predictions and ground truth influence values from the final epoch for each \u03b21, compute Spearman rank correlations via a NumPy utility, and scatter\u2010plot predicted vs true influences in subplots, annotating each with its \u03b21 and correlation. Both figures live in separate try\u2010except blocks with appropriate titles, axis labels, and descriptive filenames. Figures are closed after saving, and we finally print out the computed final\u2010epoch Spearman correlations. This approach ensures we only visualize existing data, respect the \u201cmax 5 figures\u201d guideline by combining curves and subplots, and avoid any fabricated values.", "We first load the saved experiment_data dictionary from the working directory and extract the synthetic dataset entries including meta-learning rates, training/validation losses, and per-epoch predictions and ground truths. A custom spearman_corr function is defined to compute the Spearman rank correlation for each combination of meta_lr and epoch. We then assemble an array of Spearman correlations over epochs for each meta_lr. Next, we create three separate figures: (1) loss curves plotting both training and validation losses across all epochs for each meta_lr, (2) a hyperparameter tuning curve showing final validation loss versus meta_lr on a logarithmic x-axis, and (3) Spearman correlation across epochs for each meta_lr. Each plot is generated within its own try-except block to safely handle errors, uses a descriptive title including the dataset name, and saves the figure to the working directory with a clear filename before closing the figure.", null, "I will load the saved experiment data from the working directory and extract the synthetic dataset results for each beta2 setting. First, I plot training vs validation loss curves over epochs in a single figure, distinguishing each beta2 with unique line styles. Next, I compute and plot the Spearman correlation between DVN predictions and ground\u2013truth meta-feedback at every epoch, again overlaying the three beta2 curves. I then generate scatter plots of predicted vs true meta-feedback at the final epoch for each beta2 to visualize model calibration. Each plotting step is wrapped in a try-except block, with descriptive titles and subtitles specifying the synthetic dataset, and all figures are saved with informative filenames and closed afterward to free resources.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "EPOCHS", "MAIN_MODEL_LR", "BATCH_SIZE", "softmax_temperature", "DVN_LR", "weight_decay", "META_SAMPLE", "DVN_STEPS_PER_EPOCH", "ADAM_BETA1", "META_UPDATE_LR", "DVN_HIDDEN_SIZE", "ADAM_BETA2", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["I will load the saved NumPy file from the working directory, extract each\ndataset\u2019s final training and validation loss values, and then print them with\nclear labels. The script iterates over all datasets in the loaded dictionary,\nretrieves the last entry of the \u201ctrain\u201d and \u201cval\u201d lists under \u201cmetrics,\u201d and\nprints them as \u201cfinal train loss\u201d and \u201cfinal validation loss.\u201d No execution\nguard or plotting is used\u2014everything runs immediately at the global scope.", "The script begins by loading the saved `experiment_data.npy` from the \u201cworking\u201d\ndirectory and extracting the `synthetic` dataset under\n`hyperparam_tuning_type_1`. It then computes the final training loss, validation\nloss, and Spearman correlation for each hyperparameter setting, selects the run\nwith the lowest final validation loss, and prints out the dataset name, the\nchosen number of epochs, and the corresponding final metric values with clear\nlabels. The code runs immediately at global scope and does not require any\nspecial entry point.", "Here\u2019s a script that loads the saved experiment data from the working directory,\nextracts the final training and validation losses for each learning rate on the\nsynthetic dataset, and prints them with clear metric names and dataset labeling:", "I will load the saved experiment_data.npy from the working directory and extract\nthe \u201csynthetic\u201d data under the \u201cbatch_size\u201d key. Then I\u2019ll pull out the list of\nbatch sizes along with their per-epoch training and validation loss lists. The\nscript prints \u201cDataset: synthetic\u201d once and then iterates through each batch\nsize, reporting the final epoch training loss and final epoch validation loss\nwith explicit metric labels. No plotting or `__main__` entry point guards are\nused\u2014this runs immediately when executed.", "I will load the saved experiment data from the working directory and iterate\nover each dataset (here \u201csynthetic\u201d). For each temperature setting, I\u2019ll extract\nthe final training and validation MSE as well as the final DVN Spearman\ncorrelation, and print them with clear labels. The script runs immediately at\nglobal scope without any `if __name__ == \"__main__\":` guard.", "The script first constructs the path to the working directory and loads the\nsaved experiment data. It then iterates over each dataset (here \u201csynthetic\u201d) and\neach DVN learning rate setting, printing the dataset name, the DVN LR, and the\nfinal training and final validation losses with clear labels. The code runs\nimmediately at the global scope without any guarded entry point.", "The following script loads the saved `experiment_data.npy` from the working\ndirectory, iterates over each dataset under the \u201cweight_decay\u201d key, and computes\nthe final epoch training and validation losses for each run. It then selects the\nrun with the lowest final validation loss and prints the dataset name along with\ndescriptive metric labels for its final training and validation loss. The code\nexecutes immediately at the global scope without any `if __name__ ==\n\"__main__\":` guard.", "The following script immediately loads the saved `experiment_data.npy` from the\n`working` directory, iterates over each dataset under the `\"meta_sample_tuning\"`\nkey, and for each meta\u2010sample size prints the final training loss, final\nvalidation loss, and final Spearman correlation computed in the last epoch. All\ncode runs at the global scope with no entry\u2010point guard.", "The following script loads the saved experiment data from the working directory\nand iterates over each DVN steps setting, extracting the final training and\nvalidation loss for the synthetic dataset. It then prints the dataset name with\nthe corresponding hyperparameter and clearly labeled metric values.", "Here\u2019s a script that immediately loads the saved experiment data from the\nworking directory, iterates through each \u03b21 setting under the ADAM_BETA1 key,\nand for each dataset prints its name followed by the final train and validation\nlosses labeled clearly. The code runs at global scope without any entry\u2010point\nguard and avoids plotting.", "The following script loads the saved experiment data from the working directory,\niterates over each dataset under the meta\u2010update learning rate tuning section,\nand prints the dataset name followed by the final training and validation loss\nat the last epoch for each meta\u2010learning rate. It uses clear metric labels like\n\u201cfinal training loss\u201d and \u201cfinal validation loss\u201d and prints each meta\u2010learning\nrate in scientific notation. No plotting or entry\u2010point guards are used, so the\ncode executes immediately.", "I will import os and numpy to construct the working directory path and load the\nsaved `experiment_data.npy`. Then I\u2019ll navigate into the\n`\"DVN_hidden_size_tuning\"` \u2192 `\"synthetic\"` section and iterate over each hidden\nsize configuration. For each one, I will extract the final reported training and\nvalidation loss values from the stored metrics. Finally, the script will print\nthe dataset name followed by clearly labeled final training and final validation\nlosses for each hidden size.", "The following script locates the working directory, loads the saved experiments,\nand iterates over each \u03b2\u2082 setting under the \u201cADAM_BETA2\u201d group. For each\nsynthetic dataset run it retrieves the last\u2010epoch metrics, printing the dataset\nidentifier followed by clearly labeled train and validation losses. This\nexecutes immediately when run without any special entry point.", "The script begins by loading the saved `experiment_data.npy` from the \u201cworking\u201d\ndirectory and extracting the `synthetic` dataset under\n`hyperparam_tuning_type_1`. It then computes the final training loss, validation\nloss, and Spearman correlation for each hyperparameter setting, selects the run\nwith the lowest final validation loss, and prints out the dataset name, the\nchosen number of epochs, and the corresponding final metric values with clear\nlabels. The code runs immediately at global scope and does not require any\nspecial entry point.", "The script begins by loading the saved `experiment_data.npy` from the \u201cworking\u201d\ndirectory and extracting the `synthetic` dataset under\n`hyperparam_tuning_type_1`. It then computes the final training loss, validation\nloss, and Spearman correlation for each hyperparameter setting, selects the run\nwith the lowest final validation loss, and prints out the dataset name, the\nchosen number of epochs, and the corresponding final metric values with clear\nlabels. The code runs immediately at global scope and does not require any\nspecial entry point.", "The script begins by loading the saved `experiment_data.npy` from the \u201cworking\u201d\ndirectory and extracting the `synthetic` dataset under\n`hyperparam_tuning_type_1`. It then computes the final training loss, validation\nloss, and Spearman correlation for each hyperparameter setting, selects the run\nwith the lowest final validation loss, and prints out the dataset name, the\nchosen number of epochs, and the corresponding final metric values with clear\nlabels. The code runs immediately at global scope and does not require any\nspecial entry point.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Load experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print the final metrics\nfor dataset_name, dataset_data in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n    train_metrics = dataset_data.get(\"metrics\", {}).get(\"train\", [])\n    val_metrics = dataset_data.get(\"metrics\", {}).get(\"val\", [])\n\n    if train_metrics:\n        final_train_loss = train_metrics[-1]\n        print(f\"final train loss: {final_train_loss:.4f}\")\n    else:\n        print(\"final train loss: N/A\")\n\n    if val_metrics:\n        final_val_loss = val_metrics[-1]\n        print(f\"final validation loss: {final_val_loss:.4f}\")\n    else:\n        print(\"final validation loss: N/A\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Extract the synthetic dataset entry\nsynthetic = experiment_data[\"hyperparam_tuning_type_1\"][\"synthetic\"]\nparam_name = synthetic[\"param_name\"]\nparam_values = synthetic[\"param_values\"]\ntrain_losses = synthetic[\"metrics\"][\"train\"]\nval_losses = synthetic[\"metrics\"][\"val\"]\ncorrelations = synthetic[\"correlations\"]\n\n# Compute the final values for each run\nfinal_train = [losses[-1] for losses in train_losses]\nfinal_val = [losses[-1] for losses in val_losses]\nfinal_corr = [corrs[-1] for corrs in correlations]\n\n# Select the run with the lowest final validation loss\nbest_idx = int(np.argmin(final_val))\nbest_epochs = param_values[best_idx]\nbest_train_loss = final_train[best_idx]\nbest_val_loss = final_val[best_idx]\nbest_spearman = final_corr[best_idx]\n\n# Print out the results\nprint(\"Synthetic Dataset:\")\nprint(f\"Hyperparameter {param_name}: {best_epochs}\")\nprint(f\"Final Training Loss: {best_train_loss:.4f}\")\nprint(f\"Final Validation Loss: {best_val_loss:.4f}\")\nprint(f\"Final Spearman Correlation: {best_spearman:.4f}\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Extract synthetic dataset results\nsynthetic_results = experiment_data[\"hyperparam_tuning_main_model_lr\"][\"synthetic\"]\n\n# Print dataset name\nprint(\"Dataset: synthetic\")\n\n# Iterate over learning rates in numeric order and print final metrics\nfor lr_key in sorted(synthetic_results.keys(), key=lambda k: float(k.split(\"_\")[1])):\n    lr_value = lr_key.split(\"_\")[1]\n    metrics = synthetic_results[lr_key][\"metrics\"]\n    final_train_loss = metrics[\"train\"][-1]\n    final_val_loss = metrics[\"val\"][-1]\n\n    print(f\"\\nLearning rate: {lr_value}\")\n    print(f\"train loss: {final_train_loss:.4f}\")\n    print(f\"validation loss: {final_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract synthetic dataset information\nsynthetic_data = experiment_data[\"batch_size\"][\"synthetic\"]\nbatch_sizes = synthetic_data[\"batch_sizes\"]\ntrain_metrics = synthetic_data[\"metrics\"][\"train\"]\nval_metrics = synthetic_data[\"metrics\"][\"val\"]\n\n# Print final losses for each batch size\nprint(\"Dataset: synthetic\")\nfor bs, train_losses, val_losses in zip(batch_sizes, train_metrics, val_metrics):\n    final_train_loss = train_losses[-1]\n    final_val_loss = val_losses[-1]\n    print(\n        f\"Batch size {bs} - final training loss: {final_train_loss:.4f}, final validation loss: {final_val_loss:.4f}\"\n    )\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each experimental condition and dataset\nfor condition, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        T_values = data[\"T_values\"]\n        train_metrics_list = data[\"metrics\"][\"train\"]\n        val_metrics_list = data[\"metrics\"][\"val\"]\n        corr_list = data[\"correlation\"]\n\n        # Print final metrics for each temperature\n        for T, train_metrics, val_metrics, corrs in zip(\n            T_values, train_metrics_list, val_metrics_list, corr_list\n        ):\n            final_train_loss = train_metrics[-1]\n            final_val_loss = val_metrics[-1]\n            final_corr = corrs[-1]\n            print(f\"  Temperature {T}:\")\n            print(f\"    Final Training Loss (MSE): {final_train_loss:.4f}\")\n            print(f\"    Final Validation Loss (MSE): {final_val_loss:.4f}\")\n            print(f\"    Final DVN Spearman Correlation: {final_corr:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each dataset under DVN_LR\nfor dataset_name, lr_dict in experiment_data.get(\"DVN_LR\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    for dvn_lr, exp in lr_dict.items():\n        # Extract the final values from the recorded metrics\n        train_losses = exp[\"metrics\"][\"train\"]\n        val_losses = exp[\"metrics\"][\"val\"]\n        final_train_loss = train_losses[-1]\n        final_val_loss = val_losses[-1]\n        # Print the settings and corresponding final metrics\n        print(f\"DVN learning rate = {dvn_lr}\")\n        print(f\"Final training loss: {final_train_loss:.4f}\")\n        print(f\"Final validation loss: {final_val_loss:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each dataset under the \"weight_decay\" key\nfor dataset_name, dataset_info in experiment_data[\"weight_decay\"].items():\n    print(f\"{dataset_name} dataset\")\n\n    # Extract per-epoch metrics for all runs\n    train_runs = dataset_info[\"metrics\"][\"train\"]\n    val_runs = dataset_info[\"metrics\"][\"val\"]\n\n    # Compute the final epoch loss for each run\n    final_train_losses = [run_metrics[-1] for run_metrics in train_runs]\n    final_val_losses = [run_metrics[-1] for run_metrics in val_runs]\n\n    # Select the run with the lowest final validation loss\n    best_idx = int(np.argmin(final_val_losses))\n    best_train_loss = final_train_losses[best_idx]\n    best_val_loss = final_val_losses[best_idx]\n\n    # Print the results with descriptive metric names\n    print(f\"Final training loss: {best_train_loss:.4f}\")\n    print(f\"Final validation loss: {best_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# Load experiment data from working directory\ndata_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset in meta_sample_tuning\nfor dataset_name, ds in experiment_data[\"meta_sample_tuning\"].items():\n    print(f\"Dataset: {dataset_name}\")\n    meta_values = ds[\"meta_sample_values\"]\n    train_metrics = ds[\"metrics\"][\"train\"]\n    val_metrics = ds[\"metrics\"][\"val\"]\n    correlations = ds[\"corr\"]\n\n    # Print final metrics for each meta_sample size\n    for i, meta in enumerate(meta_values):\n        final_train_loss = train_metrics[i][-1]\n        final_val_loss = val_metrics[i][-1]\n        final_corr = correlations[i][-1]\n        print(f\"Meta-sample size: {meta}\")\n        print(f\"Final training loss: {final_train_loss:.4f}\")\n        print(f\"Final validation loss: {final_val_loss:.4f}\")\n        print(f\"Final Spearman correlation: {final_corr:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Navigate to the hyperparameter tuning results\nhpt_results = experiment_data.get(\"hyperparam_tuning_type_1\", {})\n\n# For each DVN_STEPS_PER_EPOCH setting, extract and print final metrics\nfor steps, result in hpt_results.items():\n    synthetic_data = result.get(\"synthetic\", {})\n    train_history = synthetic_data.get(\"metrics\", {}).get(\"train\", [])\n    val_history = synthetic_data.get(\"metrics\", {}).get(\"val\", [])\n\n    # Get the final losses if available\n    final_train_loss = train_history[-1] if train_history else None\n    final_val_loss = val_history[-1] if val_history else None\n\n    print(f\"Dataset: synthetic (DVN_STEPS_PER_EPOCH = {steps})\")\n    if final_train_loss is not None:\n        print(f\"Final training loss: {final_train_loss:.4f}\")\n    if final_val_loss is not None:\n        print(f\"Final validation loss: {final_val_loss:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Load the experiment data dictionary\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Navigate to the ADAM_BETA1 experiments\nadam_experiments = experiment_data.get(\"ADAM_BETA1\", {})\n\n# Print final metrics for each beta1 and each dataset\nfor beta1_str, exp_dict in sorted(adam_experiments.items(), key=lambda x: float(x[0])):\n    print(f\"Optimizer hyperparameter beta1 = {beta1_str}\")\n    for dataset_name, dataset_info in exp_dict.items():\n        print(f\"Dataset: {dataset_name}\")\n        metrics = dataset_info.get(\"metrics\", {})\n        train_losses = metrics.get(\"train\", [])\n        val_losses = metrics.get(\"val\", [])\n        if train_losses:\n            print(f\"  Train loss: {train_losses[-1]:.4f}\")\n        if val_losses:\n            print(f\"  Validation loss: {val_losses[-1]:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Extract and print final metrics for each dataset\ntuning_results = experiment_data[\"meta_update_lr_tuning\"]\nfor dataset_name, dataset_results in tuning_results.items():\n    print(f\"Dataset: {dataset_name}\")\n    meta_lrs = dataset_results[\"meta_lrs\"]\n    train_metrics = dataset_results[\"metrics\"][\"train\"]\n    val_metrics = dataset_results[\"metrics\"][\"val\"]\n\n    for lr, train_list, val_list in zip(meta_lrs, train_metrics, val_metrics):\n        final_train_loss = train_list[-1]\n        final_val_loss = val_list[-1]\n        print(\n            f\"  Meta learning rate {lr:.1e}: final training loss = {final_train_loss:.4f}, final validation loss = {final_val_loss:.4f}\"\n        )\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Navigate to the DVN hidden size tuning results\ntuning = experiment_data.get(\"DVN_hidden_size_tuning\", {})\n\n# Iterate over each dataset (e.g., \"synthetic\")\nfor dataset_name, size_dict in tuning.items():\n    print(f\"Dataset: {dataset_name}\")\n    # For each hidden size configuration, extract final metrics\n    for hidden_size, container in size_dict.items():\n        train_losses = container.get(\"metrics\", {}).get(\"train\", [])\n        val_losses = container.get(\"metrics\", {}).get(\"val\", [])\n\n        if train_losses:\n            final_train_loss = train_losses[-1]\n        else:\n            final_train_loss = float(\"nan\")\n\n        if val_losses:\n            final_val_loss = val_losses[-1]\n        else:\n            final_val_loss = float(\"nan\")\n\n        print(f\" Hidden size {hidden_size}:\")\n        print(f\"  Final training loss: {final_train_loss:.4f}\")\n        print(f\"  Final validation loss: {final_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# Determine working directory and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(experiment_path, allow_pickle=True).item()\n\n# Iterate through each beta2 experiment and print final metrics\nfor beta_key, results in experiment_data[\"ADAM_BETA2\"].items():\n    synthetic_data = results[\"synthetic\"]\n    train_metrics = synthetic_data[\"metrics\"][\"train\"]\n    val_metrics = synthetic_data[\"metrics\"][\"val\"]\n    final_train_loss = train_metrics[-1]\n    final_validation_loss = val_metrics[-1]\n\n    print(f\"Dataset: {beta_key} synthetic\")\n    print(f\"train loss: {final_train_loss:.4f}\")\n    print(f\"validation loss: {final_validation_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Extract the synthetic dataset entry\nsynthetic = experiment_data[\"hyperparam_tuning_type_1\"][\"synthetic\"]\nparam_name = synthetic[\"param_name\"]\nparam_values = synthetic[\"param_values\"]\ntrain_losses = synthetic[\"metrics\"][\"train\"]\nval_losses = synthetic[\"metrics\"][\"val\"]\ncorrelations = synthetic[\"correlations\"]\n\n# Compute the final values for each run\nfinal_train = [losses[-1] for losses in train_losses]\nfinal_val = [losses[-1] for losses in val_losses]\nfinal_corr = [corrs[-1] for corrs in correlations]\n\n# Select the run with the lowest final validation loss\nbest_idx = int(np.argmin(final_val))\nbest_epochs = param_values[best_idx]\nbest_train_loss = final_train[best_idx]\nbest_val_loss = final_val[best_idx]\nbest_spearman = final_corr[best_idx]\n\n# Print out the results\nprint(\"Synthetic Dataset:\")\nprint(f\"Hyperparameter {param_name}: {best_epochs}\")\nprint(f\"Final Training Loss: {best_train_loss:.4f}\")\nprint(f\"Final Validation Loss: {best_val_loss:.4f}\")\nprint(f\"Final Spearman Correlation: {best_spearman:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Extract the synthetic dataset entry\nsynthetic = experiment_data[\"hyperparam_tuning_type_1\"][\"synthetic\"]\nparam_name = synthetic[\"param_name\"]\nparam_values = synthetic[\"param_values\"]\ntrain_losses = synthetic[\"metrics\"][\"train\"]\nval_losses = synthetic[\"metrics\"][\"val\"]\ncorrelations = synthetic[\"correlations\"]\n\n# Compute the final values for each run\nfinal_train = [losses[-1] for losses in train_losses]\nfinal_val = [losses[-1] for losses in val_losses]\nfinal_corr = [corrs[-1] for corrs in correlations]\n\n# Select the run with the lowest final validation loss\nbest_idx = int(np.argmin(final_val))\nbest_epochs = param_values[best_idx]\nbest_train_loss = final_train[best_idx]\nbest_val_loss = final_val[best_idx]\nbest_spearman = final_corr[best_idx]\n\n# Print out the results\nprint(\"Synthetic Dataset:\")\nprint(f\"Hyperparameter {param_name}: {best_epochs}\")\nprint(f\"Final Training Loss: {best_train_loss:.4f}\")\nprint(f\"Final Validation Loss: {best_val_loss:.4f}\")\nprint(f\"Final Spearman Correlation: {best_spearman:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Extract the synthetic dataset entry\nsynthetic = experiment_data[\"hyperparam_tuning_type_1\"][\"synthetic\"]\nparam_name = synthetic[\"param_name\"]\nparam_values = synthetic[\"param_values\"]\ntrain_losses = synthetic[\"metrics\"][\"train\"]\nval_losses = synthetic[\"metrics\"][\"val\"]\ncorrelations = synthetic[\"correlations\"]\n\n# Compute the final values for each run\nfinal_train = [losses[-1] for losses in train_losses]\nfinal_val = [losses[-1] for losses in val_losses]\nfinal_corr = [corrs[-1] for corrs in correlations]\n\n# Select the run with the lowest final validation loss\nbest_idx = int(np.argmin(final_val))\nbest_epochs = param_values[best_idx]\nbest_train_loss = final_train[best_idx]\nbest_val_loss = final_val[best_idx]\nbest_spearman = final_corr[best_idx]\n\n# Print out the results\nprint(\"Synthetic Dataset:\")\nprint(f\"Hyperparameter {param_name}: {best_epochs}\")\nprint(f\"Final Training Loss: {best_train_loss:.4f}\")\nprint(f\"Final Validation Loss: {best_val_loss:.4f}\")\nprint(f\"Final Spearman Correlation: {best_spearman:.4f}\")\n", ""], "parse_term_out": ["['Dataset: synthetic', '\\n', 'final train loss: 0.0218', '\\n', 'final validation\nloss: 0.0191', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Synthetic Dataset:', '\\n', 'Hyperparameter EPOCHS: 50', '\\n', 'Final Training\nLoss: 0.0113', '\\n', 'Final Validation Loss: 0.0100', '\\n', 'Final Spearman\nCorrelation: 0.3038', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Dataset: synthetic', '\\n', '\\nLearning rate: 0.001', '\\n', 'train loss:\n0.1550', '\\n', 'validation loss: 0.1305', '\\n', '\\nLearning rate: 0.005', '\\n',\n'train loss: 0.0717', '\\n', 'validation loss: 0.0531', '\\n', '\\nLearning rate:\n0.01', '\\n', 'train loss: 0.0476', '\\n', 'validation loss: 0.0325', '\\n',\n'\\nLearning rate: 0.05', '\\n', 'train loss: 0.0153', '\\n', 'validation loss:\n0.0140', '\\n', '\\nLearning rate: 0.1', '\\n', 'train loss: 0.1044', '\\n',\n'validation loss: 0.0813', '\\n', 'Execution time: a moment seconds (time limit\nis an hour).']", "['Dataset: synthetic', '\\n', 'Batch size 16 - final training loss: 0.0139, final\nvalidation loss: 0.0181', '\\n', 'Batch size 32 - final training loss: 0.0181,\nfinal validation loss: 0.0157', '\\n', 'Batch size 64 - final training loss:\n0.0476, final validation loss: 0.0325', '\\n', 'Batch size 128 - final training\nloss: 0.0807, final validation loss: 0.0622', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', '  Temperature 0.1:', '\\n', '    Final Training\nLoss (MSE): 0.0199', '\\n', '    Final Validation Loss (MSE): 0.0192', '\\n', '\nFinal DVN Spearman Correlation: 0.1113', '\\n', '  Temperature 0.5:', '\\n', '\nFinal Training Loss (MSE): 0.0344', '\\n', '    Final Validation Loss (MSE):\n0.0228', '\\n', '    Final DVN Spearman Correlation: 0.2060', '\\n', '\nTemperature 1.0:', '\\n', '    Final Training Loss (MSE): 0.0476', '\\n', '\nFinal Validation Loss (MSE): 0.0325', '\\n', '    Final DVN Spearman Correlation:\n0.0541', '\\n', '  Temperature 2.0:', '\\n', '    Final Training Loss (MSE):\n0.0279', '\\n', '    Final Validation Loss (MSE): 0.0222', '\\n', '    Final DVN\nSpearman Correlation: -0.3263', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "['Dataset: synthetic', '\\n', 'DVN learning rate = 0.001', '\\n', 'Final training\nloss: 0.0217', '\\n', 'Final validation loss: 0.0191\\n', '\\n', 'DVN learning rate\n= 0.0005', '\\n', 'Final training loss: 0.0358', '\\n', 'Final validation loss:\n0.0240\\n', '\\n', 'DVN learning rate = 0.0001', '\\n', 'Final training loss:\n0.0471', '\\n', 'Final validation loss: 0.0322\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['synthetic dataset', '\\n', 'Final training loss: 0.0204', '\\n', 'Final\nvalidation loss: 0.0142', '\\n', 'Execution time: a moment seconds (time limit is\nan hour).']", "['Dataset: synthetic', '\\n', 'Meta-sample size: 5', '\\n', 'Final training loss:\n0.0218', '\\n', 'Final validation loss: 0.0185', '\\n', 'Final Spearman\ncorrelation: 0.5000', '\\n', 'Meta-sample size: 10', '\\n', 'Final training loss:\n0.0477', '\\n', 'Final validation loss: 0.0374', '\\n', 'Final Spearman\ncorrelation: -0.0182', '\\n', 'Meta-sample size: 20', '\\n', 'Final training loss:\n0.0444', '\\n', 'Final validation loss: 0.0290', '\\n', 'Final Spearman\ncorrelation: -0.2842', '\\n', 'Meta-sample size: 50', '\\n', 'Final training loss:\n0.0693', '\\n', 'Final validation loss: 0.0442', '\\n', 'Final Spearman\ncorrelation: -0.1584', '\\n', 'Meta-sample size: 100', '\\n', 'Final training\nloss: 0.0370', '\\n', 'Final validation loss: 0.0200', '\\n', 'Final Spearman\ncorrelation: 0.2453', '\\n', '\\n', 'Execution time: a moment seconds (time limit\nis an hour).']", "['Dataset: synthetic (DVN_STEPS_PER_EPOCH = 1)', '\\n', 'Final training loss:\n0.0217', '\\n', 'Final validation loss: 0.0191', '\\n', '\\n', 'Dataset: synthetic\n(DVN_STEPS_PER_EPOCH = 5)', '\\n', 'Final training loss: 0.0367', '\\n', 'Final\nvalidation loss: 0.0239', '\\n', '\\n', 'Dataset: synthetic (DVN_STEPS_PER_EPOCH =\n10)', '\\n', 'Final training loss: 0.0475', '\\n', 'Final validation loss:\n0.0324', '\\n', '\\n', 'Dataset: synthetic (DVN_STEPS_PER_EPOCH = 20)', '\\n',\n'Final training loss: 0.0281', '\\n', 'Final validation loss: 0.0223', '\\n',\n'\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Optimizer hyperparameter beta1 = 0.8', '\\n', 'Dataset: synthetic', '\\n', '\nTrain loss: 0.0218', '\\n', '  Validation loss: 0.0177', '\\n', '\\n', 'Optimizer\nhyperparameter beta1 = 0.9', '\\n', 'Dataset: synthetic', '\\n', '  Train loss:\n0.0367', '\\n', '  Validation loss: 0.0239', '\\n', '\\n', 'Optimizer\nhyperparameter beta1 = 0.99', '\\n', 'Dataset: synthetic', '\\n', '  Train loss:\n0.1074', '\\n', '  Validation loss: 0.0562', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', '  Meta learning rate 1.0e-04: final training loss\n= 0.0218, final validation loss = 0.0191', '\\n', '  Meta learning rate 4.0e-04:\nfinal training loss = 0.0366, final validation loss = 0.0239', '\\n', '  Meta\nlearning rate 1.6e-03: final training loss = 0.0476, final validation loss =\n0.0325', '\\n', '  Meta learning rate 6.3e-03: final training loss = 0.0279,\nfinal validation loss = 0.0221', '\\n', '  Meta learning rate 2.5e-02: final\ntraining loss = 0.0436, final validation loss = 0.0282', '\\n', '  Meta learning\nrate 1.0e-01: final training loss = 0.1357, final validation loss = 0.1139',\n'\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', ' Hidden size 8:', '\\n', '  Final training loss:\n0.0255', '\\n', '  Final validation loss: 0.0195', '\\n', ' Hidden size 16:',\n'\\n', '  Final training loss: 0.0268', '\\n', '  Final validation loss: 0.0188',\n'\\n', ' Hidden size 32:', '\\n', '  Final training loss: 0.0214', '\\n', '  Final\nvalidation loss: 0.0192', '\\n', ' Hidden size 64:', '\\n', '  Final training\nloss: 0.0627', '\\n', '  Final validation loss: 0.0454', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['Dataset: beta2_0.9 synthetic', '\\n', 'train loss: 0.0501', '\\n', 'validation\nloss: 0.0452', '\\n', 'Dataset: beta2_0.98 synthetic', '\\n', 'train loss:\n0.0316', '\\n', 'validation loss: 0.0356', '\\n', 'Dataset: beta2_0.999\nsynthetic', '\\n', 'train loss: 0.0575', '\\n', 'validation loss: 0.0627', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Synthetic Dataset:', '\\n', 'Hyperparameter EPOCHS: 20', '\\n', 'Final Training\nLoss: 0.0123', '\\n', 'Final Validation Loss: 0.0121', '\\n', 'Final Spearman\nCorrelation: -0.1564', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Synthetic Dataset:', '\\n', 'Hyperparameter EPOCHS: 50', '\\n', 'Final Training\nLoss: 0.0107', '\\n', 'Final Validation Loss: 0.0146', '\\n', 'Final Spearman\nCorrelation: -0.2105', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Synthetic Dataset:', '\\n', 'Hyperparameter EPOCHS: 50', '\\n', 'Final Training\nLoss: 0.0117', '\\n', 'Final Validation Loss: 0.0142', '\\n', 'Final Spearman\nCorrelation: -0.0045', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]}
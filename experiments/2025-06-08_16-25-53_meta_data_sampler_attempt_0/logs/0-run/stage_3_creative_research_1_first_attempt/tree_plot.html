<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 1], [0, 2], [0, 3], [1, 10], [2, 12], [3, 7], [3, 4], [3, 5], [3, 6], [5, 9], [6, 8], [8, 11], [9, 16], [9, 17], [9, 15], [9, 14], [10, 13]], "layout": [[0.2692307692307692, 0.0], [0.0, 0.25], [0.15384615384615385, 0.25], [0.6538461538461539, 0.25], [0.3076923076923077, 0.5], [0.46153846153846156, 0.5], [0.8461538461538461, 0.5], [1.0, 0.5], [0.8461538461538461, 0.75], [0.46153846153846156, 0.75], [0.0, 0.5], [0.8461538461538461, 1.0], [0.15384615384615385, 0.5], [0.0, 0.75], [0.23076923076923078, 1.0], [0.38461538461538464, 1.0], [0.5384615384615384, 1.0], [0.6923076923076923, 1.0]], "plan": ["Hyperparam tuning name: EPOCHS. We wrap the training and DVN meta-update in a\nloop over different EPOCHS settings (e.g. [5, 20, 50]), reinitializing models\nand optimizers for each.  For each run, we record per-epoch train/val losses,\nSpearman correlations, DVN predictions and true contributions, and store them\nunder `hyperparam_tuning_type_1 -> synthetic`, along with the parameter name and\nvalues.  At the end, we save everything into `experiment_data.npy`.", "We extend our DVN sampler to multi\u2010corpus pre\u2010training by running the same\npipeline on three distinct HuggingFace corpora\u2014Wikitext-2, BookCorpus, and\nOpenWebText\u2014each subsampled for speed.  We tokenize and chunk each dataset into\nfixed 64\u2010token blocks, split into train/val, and fine\u2010tune GPT-2 small with\nper\u2010sample loss weighting from a lightweight DVN.  Every epoch we meta\u2010update\nthe DVN on 20 randomly selected training samples by measuring their true\nheld\u2010out loss reduction after a one\u2010step fine\u2010tune, and regress these\ncontributions from their current loss to train the DVN.  We track Spearman rank\ncorrelation between DVN predictions and true contributions to assess ranking\nquality.  At each epoch we log train+validation losses (and perplexities) and\nthe DVN\u2019s contribution correlation.  We repeat this for all three corpora to\ntest generality and save all metrics, DVN predictions, and ground truth arrays\nto disk for later analysis.", "We extend the DVN with embedding\u2010norm features and pre\u2010train on three\nHuggingFace corpora (OpenWebText, BookCorpus, Wikitext\u20102) sampled at 1K texts\neach to simulate a multi\u2010domain setting. Sequences are tokenized with GPT2\ntokenizer (padded/truncated to 128 tokens) and split into train/val with a\n100\u2010sample held\u2010out set for meta\u2010updates. We use distilgpt2 to reduce clone\noverhead and maintain foundation\u2010model behavior. Each epoch applies the DVN\n(taking per\u2010sample loss and embedding L2 norm) to softmax\u2010weight the\ncross\u2010entropy loss for main\u2010model updates. At epoch end, we compute ground\u2010truth\nsingle\u2010sample contributions on the held\u2010out set via small clones, train the DVN\nfor several steps, and record Spearman correlation. We print validation loss,\nperplexity, and contribution correlation each epoch. All metrics are saved to\nworking/experiment_data.npy for further analysis.", "We propose a simplified DVN pipeline that weighs per\u2010sample losses instead of\nre\u2010sampling and evaluate it on three HuggingFace text datasets (AG News, Yelp\nPolarity, and DBpedia) using a small MLP classifier on TF-IDF features.\nPer\u2010sample features combine current loss and input entropy, and every 10 update\nsteps we meta\u2010train the DVN by measuring held\u2010out loss reductions on 20 random\nsamples via single\u2010sample clone fine\u2010tuning. We track Spearman correlation of\nDVN predictions vs. true contributions at each meta-update and log validation\nloss and accuracy each epoch. All training runs on GPU if available, and metrics\nare saved as numpy arrays in the working directory. This multi\u2010dataset setup\nexamines DVN\u2019s ability to generalize data valuation across diverse text domains.", "We extend the DVN framework by adding per-sample gradient norm as an extra\nfeature alongside loss and entropy to capture local model sensitivity. We also\nrestructure the data valuation process to perform meta\u2010updates only at the end\nof each epoch on a larger subset (K_meta) of samples, stabilizing the DVN\ntraining while reducing overhead. The DVN network is deepened to two hidden\nlayers for richer representation learning from our three per\u2010sample features. We\ntest this on three HuggingFace text classification datasets (AG News, Yelp\nPolarity, DBpedia\u201014) using small TF-IDF features for the main model. Each epoch\nwe compute true contributions via one\u2010step fine\u2010tuning clones, update the DVN\nwith MSE loss, and measure Spearman rank correlation. The main model then uses\nthe updated DVN to adaptively weight samples during training. We log and print\nvalidation loss, accuracy, and contribution prediction correlation at each\nepoch, and save all recorded metrics into working/experiment_data.npy. This\ndesign balances computational cost and ranking accuracy for efficient and fair\npre-training of foundation models.", "Our improved approach extends feature set with representation norm from the\nMLP\u2019s penultimate layer, augmenting loss and entropy measures to better capture\nsample diversity. We introduce an adaptive meta-update schedule for the DVN,\ndoubling its interval when ranking correlation improves and halving it when\nperformance stalls, thus balancing valuation accuracy with computational\noverhead. We maintain experiments on three HuggingFace datasets (ag_news,\nyelp_polarity, dbpedia_14), sampling 1000 train and 200 test examples to keep\nruns efficient. Each epoch records validation loss, accuracy, and Spearman\ncorrelation between DVN predictions and actual contribution measured via held-\nout loss reduction. The DVN now accepts three features and is updated every\nN_meta steps, with N_meta evolving dynamically per dataset based on observed\ncorrelations. We log the history of N_meta per meta-update to analyze how\nscheduling impacts DVN stability. All metrics and scheduling histories are\nstored in a dictionary and saved at the end of the run in the working directory.\nGPU usage is enforced throughout, moving all models and tensors to the selected\ndevice. The script prints validation metrics per epoch, Spearman correlation at\neach meta-update, and then saves the collected experiment_data.", "We enhance the DVN by adding a simple representation\u2010diversity feature\u2014each\nsample\u2019s L2 distance from the dataset mean in tfidf space\u2014alongside per\u2010sample\nloss, giving the DVN a richer two\u2010dimensional input. An adaptive meta\u2010update\nschedule automatically tightens or relaxes the DVN update frequency based on the\ncurrent Spearman correlation to a held\u2010out set, so we spend more compute where\nranking quality lags and less when it\u2019s strong. We benchmark on three\nHuggingFace corpora (AG News, Yelp Polarity, DBpedia) using a lightweight MLP\nmain model to isolate the sampler\u2019s effect. Each epoch logs and saves training\nloss, validation loss, train/val accuracy, Spearman sample\u2010importance\ncorrelation, raw DVN predictions, and ground\u2010truth contributions. All metrics\nand predictions are saved in a structured dict and dumped via numpy. By running\non GPU where available and modularizing the DVN inputs, this sketch should\nreveal whether diversity signals plus adaptive scheduling improve sample\nranking, convergence speed, and fairness stability over a loss\u2010only baseline.", "We propose a dynamic scheduler for the DVN\u2019s meta-update interval that adapts\nN_meta online based on its Spearman correlation, increasing updates when ranking\nquality dips and reducing frequency when the DVN is reliable. The DVN now\ningests three lightweight per-sample features\u2014current cross-entropy loss, tf-idf\nentropy, and model output uncertainty\u2014providing richer signals for data\nvaluation. We test on three Hugging Face classification corpora (AG News, Yelp\nPolarity, DBpedia), training a lightweight MLP classifier with DVN-guided loss\nweighting. At each epoch we record and print both validation loss and the DVN\u2019s\naverage contribution prediction Spearman correlation. For the Yelp dataset we\nalso calculate class-specific accuracies to assess fairness disparity between\npositive and negative reviews. All metrics (losses, accuracies, Spearman\ncorrelations, fairness measures) are logged per epoch and saved as numpy arrays\nin the working directory. Models, tensors, and optimizers are moved to GPU when\navailable, and the code dynamically adjusts sampling weights based on DVN\noutputs. This pipeline will reveal how adaptive meta-update scheduling and\nricher feature sets impact training efficiency and fairness.", "We enrich the DVN with gradient\u2010norm features computed via input gradients\nalongside per-sample loss and representation\u2010diversity, and replace the shallow\nDVN with a deeper 2\u2010layer MLP to capture more complex valuation patterns. We\nincrease the meta\u2010batch size (K_meta=30) and meta\u2010update steps (meta_steps=5),\nand adopt an adaptive meta\u2010update schedule that halves or doubles frequency\nbased on Spearman rank thresholds (0.3/0.7). We evaluate on three HuggingFace\ndatasets (AG News, DBpedia, Yelp Polarities), using TFIDF features for\nlightweight embedding diversity. During training, weighted sampling uses a\nsoftmax over DVN predictions to bias SGD toward high\u2010value samples. Every\nmeta\u2010update we measure true contribution by single\u2010step clones on held\u2010out data,\ntrain the DVN, then record its Spearman correlation. We track and print\nvalidation loss and all metrics each epoch, and save structured experiment\nresults (losses, accuracies, and correlation scores) as an npy at the end.", "We need to include the missing rep-norm feature when building the meta-training\nfeatures so that the DVN gets the same 3\u2010D inputs it expects. In the meta\u2010update\nloop, we now compute the representation norm for each sampled point using the\nfrozen main model, and append this as the third feature alongside loss and\nentropy. This ensures `feats_meta` has shape `(K_meta, 3)` and fixes the matmul\nsize mismatch error. All other code remains the same and will now execute\nwithout shape errors.", "The error arises because `AdamW` is no longer exported from `transformers`;\ninstead we should import it from `torch.optim`. By replacing the faulty import\nwith `from torch.optim import AdamW`, the script can locate and use the correct\noptimizer implementation without further changes. All other model, data loading,\nand training logic remain intact.", "I fixed the meta\u2010update bug by ensuring that each sampled example `xi` is\ndetached, cloned, moved to the device, and marked with `requires_grad_(True)`\nbefore computing its gradient. This way, `torch.autograd.grad` can properly\ntrace and compute gradients w.r.t. `xi` during the DVN ground\u2010truth contribution\nmeasurement. All other components (data loading, main training, DVN updates,\nmetric logging, and saving) remain unchanged.", "We can fix the indexing error by converting the 0\u2010dim PyTorch tensors returned\nby torch.randperm into Python ints\u2014e.g., by calling .tolist() before\niteration\u2014so that train_dataset[idx] receives a proper integer index. All other\ndevice moves and evaluation steps remain the same.", "We increase the dataset fraction to 10%, bump epochs to 10, and switch from\nGPT2-small to GPT2-medium to better utilize the available GPU time. All models\nand tensors are properly moved to `device`, training & validation losses and\nSpearman correlations are tracked each epoch, and the final `experiment_data` is\nsaved to disk. This will ensure the script runs longer and provides more robust\ninsights.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data\nN = 1000\nx = torch.rand(N, 1) * 6 - 3\ny = torch.sin(x) + 0.1 * torch.randn_like(x)\nx_train, y_train = x[:800], y[:800]\nx_val, y_val = x[800:], y[800:]\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=64, shuffle=True)\nx_val_tensor, y_val_tensor = x_val.to(device), y_val.to(device)\n\n\n# models\nclass PretrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# spearman correlation\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\n# hyperparameter sweep over EPOCHS\nepoch_values = [5, 20, 50]\nexperiment_data = {\n    \"hyperparam_tuning_type_1\": {\n        \"synthetic\": {\n            \"param_name\": \"EPOCHS\",\n            \"param_values\": epoch_values,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"correlations\": [],\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor EPOCHS in epoch_values:\n    # storage for this run\n    run_train_losses, run_val_losses = [], []\n    run_corrs, run_preds, run_truth = [], [], []\n    # initialize models & optimizers\n    main_model = PretrainModel().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-2)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-2)\n    criterion_main = nn.MSELoss(reduction=\"none\").to(device)\n    criterion_dvn = nn.MSELoss(reduction=\"mean\").to(device)\n\n    for epoch in range(EPOCHS):\n        # train main model\n        main_model.train()\n        running_loss = 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = main_model(xb)\n            loss_i = criterion_main(preds, yb)  # per-sample\n            feats = loss_i.detach().unsqueeze(1)\n            scores = dvn_model(feats).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n            running_loss += loss.item()\n        train_loss = running_loss / len(train_loader)\n        run_train_losses.append(train_loss)\n\n        # validation\n        main_model.eval()\n        with torch.no_grad():\n            val_preds = main_model(x_val_tensor)\n            val_loss = criterion_main(val_preds, y_val_tensor).mean().item()\n        run_val_losses.append(val_loss)\n        print(\n            f\"[E={EPOCHS}] Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\"\n        )\n\n        # meta-update DVN\n        features_list, contr_list = [], []\n        base_state = main_model.state_dict()\n        # sample\n        for idx in np.random.choice(len(x_train), 20, replace=False):\n            xi = x_train[idx].unsqueeze(0).to(device)\n            yi = y_train[idx].unsqueeze(0).to(device)\n            # feature\n            with torch.no_grad():\n                feat_val = criterion_main(main_model(xi), yi).item()\n            # clone & step\n            clone = PretrainModel().to(device)\n            clone.load_state_dict(base_state)\n            opt_c = torch.optim.Adam(clone.parameters(), lr=1e-2)\n            clone.eval()\n            with torch.no_grad():\n                L0 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            clone.train()\n            loss_ci = criterion_main(clone(xi), yi).mean()\n            opt_c.zero_grad()\n            loss_ci.backward()\n            opt_c.step()\n            clone.eval()\n            with torch.no_grad():\n                L1 = criterion_main(clone(x_val_tensor), y_val_tensor).mean().item()\n            contr = L0 - L1\n            features_list.append([feat_val])\n            contr_list.append([contr])\n        feats = torch.tensor(features_list, dtype=torch.float32).to(device)\n        contrs = torch.tensor(contr_list, dtype=torch.float32).to(device)\n        # train DVN\n        for _ in range(5):\n            dvn_model.train()\n            pred_c = dvn_model(feats)\n            dvn_loss = criterion_dvn(pred_c, contrs)\n            optimizer_dvn.zero_grad()\n            dvn_loss.backward()\n            optimizer_dvn.step()\n        # eval DVN\n        dvn_model.eval()\n        with torch.no_grad():\n            preds_np = dvn_model(feats).cpu().numpy().flatten()\n        true_np = contrs.cpu().numpy().flatten()\n        corr = spearman_corr(preds_np, true_np)\n        run_corrs.append(corr)\n        run_preds.append(preds_np)\n        run_truth.append(true_np)\n        print(f\"[E={EPOCHS}] Epoch {epoch}: Spearman Corr={corr:.4f}\")\n\n    # record this hyperparam run\n    sd = experiment_data[\"hyperparam_tuning_type_1\"][\"synthetic\"]\n    sd[\"metrics\"][\"train\"].append(run_train_losses)\n    sd[\"metrics\"][\"val\"].append(run_val_losses)\n    sd[\"losses\"][\"train\"].append(run_train_losses)\n    sd[\"losses\"][\"val\"].append(run_val_losses)\n    sd[\"correlations\"].append(run_corrs)\n    sd[\"predictions\"].append(run_preds)\n    sd[\"ground_truth\"].append(run_truth)\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport copy\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\n# feature network: DVN\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# PyTorch wrapper for HF dataset\nclass TextDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.ids = hf_dataset[\"input_ids\"]\n        self.attn = hf_dataset[\"attention_mask\"]\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, i):\n        return {\n            \"input_ids\": torch.tensor(self.ids[i], dtype=torch.long),\n            \"attention_mask\": torch.tensor(self.attn[i], dtype=torch.long),\n            \"labels\": torch.tensor(self.ids[i], dtype=torch.long),\n        }\n\n\n# initialize tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\nblock_size = 64\n\n# main experiment data storage\nexperiment_data = {}\n\n# run on three corpora\nfor ds_name, name, cfg in [\n    (\"wikitext2\", \"wikitext\", \"wikitext-2-raw-v1\"),\n    (\"bookcorpus\", \"bookcorpus\", None),\n    (\"openwebtext\", \"openwebtext\", None),\n]:\n    print(f\"*** Dataset: {ds_name}\")\n    # load 1% for speed\n    if cfg:\n        raw = load_dataset(name, cfg, split=\"train[:1%]\")\n    else:\n        raw = load_dataset(name, split=\"train[:1%]\")\n\n    # tokenize\n    def tok(ex):\n        return tokenizer(ex[\"text\"])\n\n    tokenized = raw.map(tok, batched=True, remove_columns=[\"text\"])\n\n    # group into fixed blocks\n    def group(ex):\n        all_ids = sum(ex[\"input_ids\"], [])\n        total = (len(all_ids) // block_size) * block_size\n        chunks = [all_ids[i : i + block_size] for i in range(0, total, block_size)]\n        return {\n            \"input_ids\": chunks,\n            \"attention_mask\": [[1] * block_size for _ in chunks],\n        }\n\n    lm_ds = tokenized.map(\n        group,\n        batched=True,\n        batch_size=1000,\n        remove_columns=[\"input_ids\", \"attention_mask\"],\n    )\n    split = lm_ds.train_test_split(test_size=0.1)\n    train_ds, val_ds = split[\"train\"], split[\"test\"]\n    train_loader = DataLoader(TextDataset(train_ds), batch_size=8, shuffle=True)\n    val_loader = DataLoader(TextDataset(val_ds), batch_size=8)\n\n    # models & optimizers\n    model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n    dvn = DVN().to(device)\n    opt_main = AdamW(model.parameters(), lr=5e-5)\n    opt_dvn = torch.optim.Adam(dvn.parameters(), lr=1e-3)\n\n    # logs\n    experiment_data[ds_name] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"correlations\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    EPOCHS = 3\n    for epoch in range(EPOCHS):\n        # === main training ===\n        model.train()\n        tot_loss = 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            bs = batch[\"input_ids\"].size(0)\n            # per-sample feature: current model loss\n            feats = []\n            with torch.no_grad():\n                for i in range(bs):\n                    out = model(\n                        input_ids=batch[\"input_ids\"][i].unsqueeze(0),\n                        attention_mask=batch[\"attention_mask\"][i].unsqueeze(0),\n                        labels=batch[\"labels\"][i].unsqueeze(0),\n                    )\n                    feats.append(\n                        (out.loss.item() * batch[\"attention_mask\"][i].sum().item())\n                    )\n            feats = torch.tensor(feats, dtype=torch.float, device=device).unsqueeze(1)\n            scores = dvn(feats).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            # compute weighted cross-entropy\n            outputs = model(\n                input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n            )\n            logits = outputs.logits\n            shift_logits = logits[:, :-1, :].contiguous()\n            shift_labels = batch[\"labels\"][:, 1:].contiguous()\n            loss_tokens = F.cross_entropy(\n                shift_logits.view(-1, shift_logits.size(-1)),\n                shift_labels.view(-1),\n                reduction=\"none\",\n            )\n            loss_tok = loss_tokens.view(bs, -1).sum(dim=1)\n            loss = (weights * loss_tok).sum()\n            opt_main.zero_grad()\n            loss.backward()\n            opt_main.step()\n            tot_loss += loss.item()\n        train_loss = tot_loss / len(train_loader)\n        experiment_data[ds_name][\"metrics\"][\"train\"].append(train_loss)\n\n        # === validation ===\n        model.eval()\n        vlosses = []\n        with torch.no_grad():\n            for vb in val_loader:\n                vb = {k: v.to(device) for k, v in vb.items()}\n                out = model(\n                    input_ids=vb[\"input_ids\"],\n                    attention_mask=vb[\"attention_mask\"],\n                    labels=vb[\"labels\"],\n                )\n                vlosses.append(out.loss.item())\n        val_loss = float(np.mean(vlosses))\n        experiment_data[ds_name][\"metrics\"][\"val\"].append(val_loss)\n        print(f\"Dataset {ds_name} Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n        # === meta-update DVN ===\n        feats_list, ctr_list = [], []\n        idxs = np.random.choice(len(train_ds), min(20, len(train_ds)), replace=False)\n        for idx in idxs:\n            item = train_ds[int(idx)]\n            bi = {\n                k: torch.tensor(item[k], dtype=torch.long).unsqueeze(0).to(device)\n                for k in (\"input_ids\", \"attention_mask\")\n            }\n            bi[\"labels\"] = bi[\"input_ids\"]\n            with torch.no_grad():\n                fval = model(**bi).loss.item() * bi[\"attention_mask\"].sum().item()\n            # clone & measure L0\n            clone = copy.deepcopy(model).to(device).eval()\n            l0 = []\n            with torch.no_grad():\n                for vb in val_loader:\n                    vb = {k: v.to(device) for k, v in vb.items()}\n                    l0.append(\n                        clone(\n                            input_ids=vb[\"input_ids\"],\n                            attention_mask=vb[\"attention_mask\"],\n                            labels=vb[\"labels\"],\n                        ).loss.item()\n                    )\n            L0 = float(np.mean(l0))\n            # one-step update\n            clone.train()\n            opt_c = AdamW(clone.parameters(), lr=5e-5)\n            out_c = clone(**bi)\n            opt_c.zero_grad()\n            out_c.loss.backward()\n            opt_c.step()\n            # measure L1\n            clone.eval()\n            l1 = []\n            with torch.no_grad():\n                for vb in val_loader:\n                    vb = {k: v.to(device) for k, v in vb.items()}\n                    l1.append(\n                        clone(\n                            input_ids=vb[\"input_ids\"],\n                            attention_mask=vb[\"attention_mask\"],\n                            labels=vb[\"labels\"],\n                        ).loss.item()\n                    )\n            L1 = float(np.mean(l1))\n            feats_list.append([fval])\n            ctr_list.append([L0 - L1])\n        # train DVN\n        F_feats = torch.tensor(feats_list, dtype=torch.float32, device=device)\n        F_ctrs = torch.tensor(ctr_list, dtype=torch.float32, device=device)\n        for _ in range(5):\n            dvn.train()\n            pred = dvn(F_feats)\n            l_dvn = F.mse_loss(pred, F_ctrs)\n            opt_dvn.zero_grad()\n            l_dvn.backward()\n            opt_dvn.step()\n        # eval DVN\n        dvn.eval()\n        with torch.no_grad():\n            p_np = dvn(F_feats).cpu().numpy().flatten()\n        t_np = F_ctrs.cpu().numpy().flatten()\n        corr = spearman_corr(p_np, t_np)\n        experiment_data[ds_name][\"correlations\"].append(corr)\n        experiment_data[ds_name][\"predictions\"].append(p_np)\n        experiment_data[ds_name][\"ground_truth\"].append(t_np)\n        print(f\"Dataset {ds_name} Epoch {epoch}: Spearman Corr={corr:.4f}\")\n\n# save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, concatenate_datasets\nfrom transformers import GPT2TokenizerFast, GPT2LMHeadModel\nfrom torch.nn import CrossEntropyLoss, MSELoss\nimport torch.nn as nn\n\n\ndef spearman_corr(a, b):\n    a, b = np.array(a), np.array(b)\n    ar, br = np.argsort(np.argsort(a)), np.argsort(np.argsort(b))\n    return np.corrcoef(ar, br)[0, 1]\n\n\n# load and subsample 3 corpora\nd1 = load_dataset(\"openwebtext\", split=\"train[:1000]\")\nd2 = load_dataset(\"bookcorpus\", split=\"train[:1000]\")\nd3 = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1000]\")\ntokenizer = GPT2TokenizerFast.from_pretrained(\"distilgpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\n\ndef tokenize(ex):\n    return tokenizer(ex[\"text\"], truncation=True, max_length=128, padding=\"max_length\")\n\n\nds = [d.map(tokenize, batched=True, remove_columns=[\"text\"]) for d in (d1, d2, d3)]\ndataset = concatenate_datasets(ds)\ndataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\nsplit = dataset.train_test_split(test_size=0.1, seed=42)\ntrain_dataset, val_dataset = split[\"train\"], split[\"test\"]\nn_hold = min(100, len(val_dataset))\nheldout_dataset = val_dataset.select(range(n_hold))\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8)\nheldout_loader = DataLoader(heldout_dataset, batch_size=16)\n\n# device and models\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nmain_model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\nmain_model.config.output_hidden_states = True\nmain_model.to(device)\noptimizer_main = torch.optim.Adam(main_model.parameters(), lr=5e-5)\ndvn = nn.Sequential(nn.Linear(2, 32), nn.ReLU(), nn.Linear(32, 1)).to(device)\noptimizer_dvn = torch.optim.Adam(dvn.parameters(), lr=1e-3)\nce_loss = CrossEntropyLoss(reduction=\"none\")\nmse_loss = MSELoss()\n\n# experiment logging\nexperiment_data = {\n    \"metrics\": {\"train_loss\": [], \"val_loss\": [], \"val_ppl\": [], \"contrib_corr\": []}\n}\nEPOCHS, M, DVN_STEPS = 3, 4, 10\nvocab_size = tokenizer.vocab_size\n\nfor epoch in range(EPOCHS):\n    main_model.train()\n    total_train = 0.0\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        inp, att = batch[\"input_ids\"], batch[\"attention_mask\"]\n        out = main_model(input_ids=inp, attention_mask=att, output_hidden_states=True)\n        logits, hid = out.logits, out.hidden_states[-1]\n        sl, sh = logits[..., :-1, :].contiguous(), inp[..., 1:].contiguous()\n        B, L = sh.shape\n        loss_all = ce_loss(sl.view(-1, vocab_size), sh.view(-1))\n        loss_per = loss_all.view(B, L).mean(dim=1)\n        emb_norms = hid.mean(dim=1).norm(dim=1)\n        feats = torch.stack([loss_per.detach(), emb_norms.detach()], dim=1)\n        scores = dvn(feats).squeeze(1)\n        weights = torch.softmax(scores, dim=0)\n        loss = (weights * loss_per).sum()\n        optimizer_main.zero_grad()\n        loss.backward()\n        optimizer_main.step()\n        total_train += loss.item()\n    avg_train = total_train / len(train_loader)\n    experiment_data[\"metrics\"][\"train_loss\"].append(avg_train)\n\n    # validation\n    main_model.eval()\n    tot_val = 0.0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = main_model(\n                input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n            )\n            lg = out.logits\n            sl = lg[..., :-1, :].contiguous()\n            sh = batch[\"input_ids\"][..., 1:].contiguous()\n            la = ce_loss(sl.view(-1, vocab_size), sh.view(-1))\n            sp = la.view(batch[\"input_ids\"].size(0), -1).mean(dim=1)\n            tot_val += sp.mean().item()\n    avg_val = tot_val / len(val_loader)\n    ppl = float(np.exp(avg_val))\n    experiment_data[\"metrics\"][\"val_loss\"].append(avg_val)\n    experiment_data[\"metrics\"][\"val_ppl\"].append(ppl)\n    print(f\"Epoch {epoch}: validation_loss = {avg_val:.4f}\")\n\n    # DVN meta\u2010update\n    main_model.eval()\n    feats_list, ctr_list = [], []\n    for idx in torch.randperm(len(train_dataset))[:M]:\n        sample = train_dataset[idx]\n        xi = sample[\"input_ids\"].unsqueeze(0).to(device)\n        mi = sample[\"attention_mask\"].unsqueeze(0).to(device)\n        with torch.no_grad():\n            o = main_model(input_ids=xi, attention_mask=mi, output_hidden_states=True)\n            llog = o.logits[..., :-1, :].contiguous()\n            labs = xi[..., 1:].contiguous()\n            li = ce_loss(llog.view(-1, vocab_size), labs.view(-1)).mean().item()\n            ei = o.hidden_states[-1].mean(dim=1).squeeze(0).norm().item()\n        # clone evaluate\n        clone = GPT2LMHeadModel(main_model.config).to(device)\n        clone.load_state_dict(main_model.state_dict())\n        optc = torch.optim.Adam(clone.parameters(), lr=5e-5)\n        clone.eval()\n        L0 = 0.0\n        with torch.no_grad():\n            for b in heldout_loader:\n                b = {k: v.to(device) for k, v in b.items()}\n                o0 = clone(input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"])\n                l0 = o0.logits[..., :-1, :].contiguous()\n                s0 = b[\"input_ids\"][..., 1:].contiguous()\n                L0 += (\n                    ce_loss(l0.view(-1, vocab_size), s0.view(-1))\n                    .view(b[\"input_ids\"].size(0), -1)\n                    .mean()\n                    .item()\n                )\n        L0 /= len(heldout_loader)\n        clone.train()\n        optc.zero_grad()\n        oc = clone(input_ids=xi, attention_mask=mi, labels=xi)\n        oc.loss.backward()\n        optc.step()\n        clone.eval()\n        L1 = 0.0\n        with torch.no_grad():\n            for b in heldout_loader:\n                b = {k: v.to(device) for k, v in b.items()}\n                o1 = clone(input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"])\n                l1 = o1.logits[..., :-1, :].contiguous()\n                s1 = b[\"input_ids\"][..., 1:].contiguous()\n                L1 += (\n                    ce_loss(l1.view(-1, vocab_size), s1.view(-1))\n                    .view(b[\"input_ids\"].size(0), -1)\n                    .mean()\n                    .item()\n                )\n        L1 /= len(heldout_loader)\n        ctr = L0 - L1\n        feats_list.append([li, ei])\n        ctr_list.append(ctr)\n        del clone\n        torch.cuda.empty_cache()\n    Fts = torch.tensor(feats_list, dtype=torch.float32).to(device)\n    Trs = torch.tensor(ctr_list, dtype=torch.float32).to(device)\n    for _ in range(DVN_STEPS):\n        preds = dvn(Fts).squeeze(1)\n        ldv = mse_loss(preds, Trs)\n        optimizer_dvn.zero_grad()\n        ldv.backward()\n        optimizer_dvn.step()\n    dvn.eval()\n    with torch.no_grad():\n        p_np = dvn(Fts).squeeze(1).cpu().numpy()\n    t_np = Trs.cpu().numpy()\n    corr = spearman_corr(p_np, t_np)\n    experiment_data[\"metrics\"][\"contrib_corr\"].append(corr)\n    print(f\"Epoch {epoch}: contribution_corr = {corr:.4f}\")\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom datasets import load_dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import spearmanr\nimport random\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Datasets to test\nhf_datasets = {\"ag_news\": \"ag_news\", \"yelp\": \"yelp_polarity\", \"dbpedia\": \"dbpedia_14\"}\nexperiment_data = {}\n\nfor name, hf_name in hf_datasets.items():\n    ds = load_dataset(hf_name)\n    train = ds[\"train\"].shuffle(seed=42).select(range(1000))\n    test = ds[\"test\"].shuffle(seed=42).select(range(200))\n    text_col = \"text\" if \"text\" in train.column_names else \"content\"\n    train_texts, test_texts = train[text_col], test[text_col]\n    y_train, y_test = train[\"label\"], test[\"label\"]\n\n    # TF-IDF features and entropy\n    tfidf = TfidfVectorizer(max_features=500, norm=\"l2\")\n    tfidf.fit(train_texts + test_texts)\n    X_train_np = tfidf.transform(train_texts).toarray()\n    X_test_np = tfidf.transform(test_texts).toarray()\n    ent_train_np = -np.sum(X_train_np * np.log(X_train_np + 1e-10), axis=1)\n\n    # Tensors\n    X_train = torch.tensor(X_train_np, dtype=torch.float32)\n    ent_train = torch.tensor(ent_train_np, dtype=torch.float32).unsqueeze(1)\n    y_train_t = torch.tensor(y_train, dtype=torch.long)\n    X_test = torch.tensor(X_test_np, dtype=torch.float32).to(device)\n    y_test_t = torch.tensor(y_test, dtype=torch.long).to(device)\n\n    train_ds = TensorDataset(X_train, ent_train, y_train_t)\n    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n\n    input_dim = X_train.shape[1]\n    num_classes = len(set(y_train))\n\n    class MLP(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(input_dim, 128), nn.ReLU(), nn.Linear(128, num_classes)\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    class DVN(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Sequential(nn.Linear(2, 32), nn.ReLU(), nn.Linear(32, 1))\n\n        def forward(self, x):\n            return self.net(x)\n\n    main_model = MLP().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-3)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-3)\n    criterion_main = nn.CrossEntropyLoss(reduction=\"none\")\n    criterion_dvn = nn.MSELoss()\n\n    experiment_data[name] = {\"val_loss\": [], \"val_acc\": [], \"corrs\": []}\n\n    N_meta, K_meta, epochs = 10, 20, 3\n    for epoch in range(epochs):\n        main_model.train()\n        step = 0\n        for Xb, entb, yb in train_loader:\n            Xb, entb, yb = Xb.to(device), entb.to(device), yb.to(device)\n            logits = main_model(Xb)\n            loss_i = criterion_main(logits, yb)\n            feats = torch.cat([loss_i.detach().unsqueeze(1), entb], dim=1)\n            w = torch.softmax(dvn_model(feats).squeeze(1), dim=0)\n            loss = (w * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n\n            if step % N_meta == 0:\n                main_model.eval()\n                with torch.no_grad():\n                    base_loss = nn.CrossEntropyLoss(reduction=\"mean\")(\n                        main_model(X_test), y_test_t\n                    ).item()\n                feats_list, contr_list = [], []\n                base_state = main_model.state_dict()\n                for idx in random.sample(range(len(X_train)), K_meta):\n                    xi = X_train[idx].unsqueeze(0).to(device)\n                    yi = torch.tensor([y_train_t[idx]], dtype=torch.long).to(device)\n                    with torch.no_grad():\n                        li = criterion_main(main_model(xi), yi).item()\n                    ent_i = ent_train[idx].item()\n                    feats_list.append([li, ent_i])\n\n                    clone = MLP().to(device)\n                    clone.load_state_dict(base_state)\n                    opt_c = torch.optim.Adam(clone.parameters(), lr=1e-3)\n                    clone.train()\n                    out = clone(xi)\n                    lci = criterion_main(out, yi).mean()\n                    opt_c.zero_grad()\n                    lci.backward()\n                    opt_c.step()\n                    clone.eval()\n                    with torch.no_grad():\n                        new_loss = nn.CrossEntropyLoss(reduction=\"mean\")(\n                            clone(X_test), y_test_t\n                        ).item()\n                    contr_list.append([base_loss - new_loss])\n\n                feats_meta = torch.tensor(feats_list, dtype=torch.float32).to(device)\n                contr_meta = torch.tensor(contr_list, dtype=torch.float32).to(device)\n                for _ in range(5):\n                    dvn_model.train()\n                    p = dvn_model(feats_meta)\n                    dvn_loss = criterion_dvn(p, contr_meta)\n                    optimizer_dvn.zero_grad()\n                    dvn_loss.backward()\n                    optimizer_dvn.step()\n                dvn_model.eval()\n                with torch.no_grad():\n                    preds_c = dvn_model(feats_meta).cpu().numpy().flatten()\n                true_c = contr_meta.cpu().numpy().flatten()\n                corr = spearmanr(preds_c, true_c).correlation\n                experiment_data[name][\"corrs\"].append(corr)\n                print(f\"[{name}] Step {step}: Spearman Corr={corr:.4f}\")\n                main_model.train()\n            step += 1\n\n        main_model.eval()\n        with torch.no_grad():\n            logits_val = main_model(X_test)\n            val_loss = nn.CrossEntropyLoss(reduction=\"mean\")(\n                logits_val, y_test_t\n            ).item()\n            acc = (logits_val.argmax(dim=1) == y_test_t).float().mean().item()\n        experiment_data[name][\"val_loss\"].append(val_loss)\n        experiment_data[name][\"val_acc\"].append(acc)\n        print(f\"[{name}] Epoch {epoch}: val_loss={val_loss:.4f}, val_acc={acc:.4f}\")\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom datasets import load_dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import spearmanr\nimport random\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nhf_datasets = {\"ag_news\": \"ag_news\", \"yelp\": \"yelp_polarity\", \"dbpedia\": \"dbpedia_14\"}\nexperiment_data = {}\n\nfor name, hf_name in hf_datasets.items():\n    ds = load_dataset(hf_name)\n    train = ds[\"train\"].shuffle(seed=42).select(range(1000))\n    test = ds[\"test\"].shuffle(seed=42).select(range(200))\n    text_col = \"text\" if \"text\" in train.column_names else \"content\"\n    train_texts, test_texts = train[text_col], test[text_col]\n    y_train, y_test = train[\"label\"], test[\"label\"]\n\n    tfidf = TfidfVectorizer(max_features=500, norm=\"l2\")\n    tfidf.fit(train_texts + test_texts)\n    X_train_np = tfidf.transform(train_texts).toarray()\n    X_test_np = tfidf.transform(test_texts).toarray()\n    ent_train_np = -np.sum(X_train_np * np.log(X_train_np + 1e-10), axis=1)\n\n    X_train = torch.tensor(X_train_np, dtype=torch.float32)\n    ent_train = torch.tensor(ent_train_np, dtype=torch.float32).unsqueeze(1)\n    y_train_t = torch.tensor(y_train, dtype=torch.long)\n    X_test = torch.tensor(X_test_np, dtype=torch.float32).to(device)\n    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n\n    train_ds = TensorDataset(X_train, ent_train, y_train_t)\n    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n\n    input_dim = X_train.shape[1]\n\n    class MLP(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(input_dim, 128),\n                nn.ReLU(),\n                nn.Linear(128, 128),\n                nn.ReLU(),\n                nn.Linear(128, len(set(y_train))),\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    class DVN(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(3, 64),\n                nn.ReLU(),\n                nn.Linear(64, 32),\n                nn.ReLU(),\n                nn.Linear(32, 1),\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    main_model = MLP().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-3)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-3)\n    criterion_main = nn.CrossEntropyLoss(reduction=\"none\")\n    criterion_eval = nn.CrossEntropyLoss()\n    criterion_dvn = nn.MSELoss()\n\n    epochs = 5\n    K_meta = 50\n    dvn_steps = 10\n    experiment_data[name] = {\n        \"metrics\": {\"val_loss\": [], \"val_acc\": [], \"corr\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    for epoch in range(epochs):\n        # Meta-update at epoch end\n        main_model.eval()\n        with torch.no_grad():\n            base_loss = criterion_eval(main_model(X_test), y_test).item()\n        feats_l, contr_l = [], []\n        idxs = random.sample(range(len(X_train)), K_meta)\n        for idx in idxs:\n            xi = (\n                X_train[idx]\n                .unsqueeze(0)\n                .clone()\n                .detach()\n                .requires_grad_(True)\n                .to(device)\n            )\n            yi = torch.tensor([y_train[idx]], dtype=torch.long).to(device)\n            logits_i = main_model(xi)\n            loss_i = criterion_main(logits_i, yi).mean()\n            grads = torch.autograd.grad(loss_i, xi)[0]\n            grad_norm_i = grads.norm().item()\n            ent_i = ent_train[idx].item()\n            feats_l.append([loss_i.item(), ent_i, grad_norm_i])\n            clone = MLP().to(device)\n            clone.load_state_dict(main_model.state_dict())\n            opt_clone = torch.optim.Adam(clone.parameters(), lr=1e-3)\n            clone.train()\n            out = clone(xi)\n            lci = criterion_main(out, yi).mean()\n            opt_clone.zero_grad()\n            lci.backward()\n            opt_clone.step()\n            clone.eval()\n            new_loss = criterion_eval(clone(X_test), y_test).item()\n            contr_l.append(base_loss - new_loss)\n\n        feats_meta = torch.tensor(feats_l, dtype=torch.float32).to(device)\n        contr_meta = torch.tensor(contr_l, dtype=torch.float32).to(device)\n        for _ in range(dvn_steps):\n            dvn_model.train()\n            pred = dvn_model(feats_meta).squeeze()\n            dvn_loss = criterion_dvn(pred, contr_meta)\n            optimizer_dvn.zero_grad()\n            dvn_loss.backward()\n            optimizer_dvn.step()\n\n        dvn_model.eval()\n        with torch.no_grad():\n            preds = dvn_model(feats_meta).squeeze().cpu().numpy()\n            trues = contr_meta.cpu().numpy()\n            corr = spearmanr(preds, trues).correlation\n        experiment_data[name][\"metrics\"][\"corr\"].append(corr)\n        experiment_data[name][\"predictions\"].append(preds)\n        experiment_data[name][\"ground_truth\"].append(trues)\n        print(f\"Epoch {epoch}: Pre-training DVN Spearman Corr={corr:.4f}\")\n\n        # Main model training\n        main_model.train()\n        total_train = 0.0\n        for Xb_cpu, entb_cpu, yb_cpu in train_loader:\n            Xb = Xb_cpu.clone().detach().to(device).requires_grad_(True)\n            entb = entb_cpu.to(device)\n            yb = yb_cpu.to(device)\n            logits = main_model(Xb)\n            loss_i = criterion_main(logits, yb)\n            total_train += loss_i.mean().item() * Xb.size(0)\n            grads = torch.autograd.grad(loss_i.sum(), Xb)[0]\n            grad_norm = grads.norm(dim=1).detach()\n            feats = torch.cat(\n                [loss_i.detach().unsqueeze(1), entb, grad_norm.unsqueeze(1)], dim=1\n            )\n            w = torch.softmax(dvn_model(feats).squeeze(), dim=0)\n            loss = (loss_i * w).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n\n        avg_train_loss = total_train / len(train_loader.dataset)\n        experiment_data[name][\"losses\"][\"train\"].append(avg_train_loss)\n\n        # Validation\n        main_model.eval()\n        with torch.no_grad():\n            logits_val = main_model(X_test)\n            val_loss = criterion_eval(logits_val, y_test).item()\n            val_acc = (logits_val.argmax(dim=1) == y_test).float().mean().item()\n        experiment_data[name][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[name][\"metrics\"][\"val_loss\"].append(val_loss)\n        experiment_data[name][\"metrics\"][\"val_acc\"].append(val_acc)\n        print(\n            f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, val_acc = {val_acc:.4f}\"\n        )\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom datasets import load_dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import spearmanr\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nhf_datasets = {\"ag_news\": \"ag_news\", \"yelp\": \"yelp_polarity\", \"dbpedia\": \"dbpedia_14\"}\nexperiment_data = {}\n\nfor name, hf_name in hf_datasets.items():\n    ds = load_dataset(hf_name)\n    train = ds[\"train\"].shuffle(seed=42).select(range(1000))\n    test = ds[\"test\"].shuffle(seed=42).select(range(200))\n    text_col = \"text\" if \"text\" in train.column_names else \"content\"\n    tr_txt, te_txt = train[text_col], test[text_col]\n    y_tr, y_te = train[\"label\"], test[\"label\"]\n\n    tfidf = TfidfVectorizer(max_features=500, norm=\"l2\")\n    tfidf.fit(tr_txt + te_txt)\n    X_tr_np = tfidf.transform(tr_txt).toarray()\n    X_te_np = tfidf.transform(te_txt).toarray()\n    ent_tr = -np.sum(X_tr_np * np.log(X_tr_np + 1e-10), axis=1, keepdims=True)\n\n    X_train = torch.tensor(X_tr_np, dtype=torch.float32)\n    ent_train = torch.tensor(ent_tr, dtype=torch.float32)\n    y_train_t = torch.tensor(y_tr, dtype=torch.long)\n    X_test = torch.tensor(X_te_np, dtype=torch.float32).to(device)\n    y_test_t = torch.tensor(y_te, dtype=torch.long).to(device)\n\n    train_ds = TensorDataset(X_train, ent_train, y_train_t)\n    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n\n    input_dim = X_train.shape[1]\n    num_classes = len(set(y_tr))\n\n    class MLP(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(input_dim, 128), nn.ReLU(), nn.Linear(128, num_classes)\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    class DVN(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Sequential(nn.Linear(3, 32), nn.ReLU(), nn.Linear(32, 1))\n\n        def forward(self, x):\n            return self.net(x)\n\n    main_model = MLP().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-3)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-3)\n    crit_main = nn.CrossEntropyLoss(reduction=\"none\")\n    crit_dvn = nn.MSELoss()\n\n    experiment_data[name] = {\n        \"val_loss\": [],\n        \"val_acc\": [],\n        \"corrs\": [],\n        \"N_meta_history\": [],\n    }\n    N_meta, prev_corr = 10, None\n    K_meta, epochs = 20, 3\n\n    for epoch in range(epochs):\n        main_model.train()\n        step = 0\n        for Xb, entb, yb in train_loader:\n            batch = {\"X\": Xb.to(device), \"ent\": entb.to(device), \"y\": yb.to(device)}\n            logits = main_model(batch[\"X\"])\n            loss_i = crit_main(logits, batch[\"y\"])\n            # representation norm feature\n            reps = main_model.net[1](main_model.net[0](batch[\"X\"]))\n            rep_norm = torch.norm(reps, dim=1, keepdim=True)\n            feats = torch.cat(\n                [loss_i.detach().unsqueeze(1), batch[\"ent\"], rep_norm], dim=1\n            )\n            weights = torch.softmax(dvn_model(feats).squeeze(1), dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n\n            if step % N_meta == 0:\n                main_model.eval()\n                with torch.no_grad():\n                    base_loss = nn.CrossEntropyLoss()(\n                        main_model(X_test), y_test_t\n                    ).item()\n                feats_list, contr_list = [], []\n                base_state = main_model.state_dict()\n                for idx in random.sample(range(len(X_train)), K_meta):\n                    xi = X_train[idx].unsqueeze(0).to(device)\n                    yi = torch.tensor([y_train_t[idx]], dtype=torch.long).to(device)\n                    with torch.no_grad():\n                        li = crit_main(main_model(xi), yi).item()\n                        ent_i = ent_train[idx].item()\n                    feats_list.append([li, ent_i])\n                    clone = MLP().to(device)\n                    clone.load_state_dict(base_state)\n                    opt_c = torch.optim.Adam(clone.parameters(), lr=1e-3)\n                    clone.train()\n                    out = clone(xi)\n                    lc = crit_main(out, yi).mean()\n                    opt_c.zero_grad()\n                    lc.backward()\n                    opt_c.step()\n                    clone.eval()\n                    with torch.no_grad():\n                        new_loss = nn.CrossEntropyLoss()(clone(X_test), y_test_t).item()\n                    contr_list.append([base_loss - new_loss])\n                feats_meta = torch.tensor(feats_list, dtype=torch.float32).to(device)\n                contr_meta = torch.tensor(contr_list, dtype=torch.float32).to(device)\n                for _ in range(5):\n                    dvn_model.train()\n                    p = dvn_model(feats_meta)\n                    dvn_loss = crit_dvn(p, contr_meta)\n                    optimizer_dvn.zero_grad()\n                    dvn_loss.backward()\n                    optimizer_dvn.step()\n                dvn_model.eval()\n                with torch.no_grad():\n                    preds = dvn_model(feats_meta).cpu().numpy().flatten()\n                true = contr_meta.cpu().numpy().flatten()\n                corr = spearmanr(preds, true).correlation\n                experiment_data[name][\"corrs\"].append(corr)\n                # adaptive N_meta\n                if prev_corr is not None:\n                    if corr > prev_corr:\n                        N_meta = min(50, N_meta * 2)\n                    else:\n                        N_meta = max(1, N_meta // 2)\n                experiment_data[name][\"N_meta_history\"].append(N_meta)\n                prev_corr = corr\n                print(\n                    f\"[{name}] Step {step}: Spearman Corr={corr:.4f}, N_meta={N_meta}\"\n                )\n                main_model.train()\n            step += 1\n\n        main_model.eval()\n        with torch.no_grad():\n            logits_val = main_model(X_test)\n            val_loss = nn.CrossEntropyLoss()(logits_val, y_test_t).item()\n            acc = (logits_val.argmax(dim=1) == y_test_t).float().mean().item()\n        experiment_data[name][\"val_loss\"].append(val_loss)\n        experiment_data[name][\"val_acc\"].append(acc)\n        print(f\"[{name}] Epoch {epoch}: val_loss={val_loss:.4f}, val_acc={acc:.4f}\")\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom datasets import load_dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import spearmanr\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nhf_datasets = {\"ag_news\": \"ag_news\", \"yelp\": \"yelp_polarity\", \"dbpedia\": \"dbpedia_14\"}\nexperiment_data = {}\n\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 128), nn.ReLU(), nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(2, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nfor name, hf_name in hf_datasets.items():\n    ds = load_dataset(hf_name)\n    train = ds[\"train\"].shuffle(seed=42).select(range(1000))\n    test = ds[\"test\"].shuffle(seed=42).select(range(200))\n    text_col = \"text\" if \"text\" in train.column_names else \"content\"\n    train_texts, test_texts = train[text_col], test[text_col]\n    y_train, y_test = train[\"label\"], test[\"label\"]\n\n    tfidf = TfidfVectorizer(max_features=500, norm=\"l2\")\n    tfidf.fit(train_texts + test_texts)\n    X_train_np = tfidf.transform(train_texts).toarray()\n    X_test_np = tfidf.transform(test_texts).toarray()\n    mean_vec = X_train_np.mean(axis=0, keepdims=True)\n    div_train_np = np.linalg.norm(X_train_np - mean_vec, axis=1)\n\n    X_train_tensor = torch.tensor(X_train_np, dtype=torch.float32)\n    div_train_tensor = torch.tensor(div_train_np, dtype=torch.float32).unsqueeze(1)\n    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n    X_test = torch.tensor(X_test_np, dtype=torch.float32).to(device)\n    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n\n    train_ds = TensorDataset(X_train_tensor, div_train_tensor, y_train_tensor)\n    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n\n    input_dim = X_train_np.shape[1]\n    num_classes = len(set(y_train))\n    main_model = MLP(input_dim, num_classes).to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-3)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-3)\n    criterion_main = nn.CrossEntropyLoss(reduction=\"none\")\n    criterion_main_mean = nn.CrossEntropyLoss(reduction=\"mean\")\n    criterion_dvn = nn.MSELoss()\n\n    meta_freq, N_min, N_max = 10, 1, 50\n    low_th, high_th = 0.2, 0.8\n    K_meta, meta_steps, epochs = 20, 3, 3\n\n    experiment_data[name] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"corrs\": [],\n    }\n\n    for epoch in range(epochs):\n        main_model.train()\n        train_loss_sum, batches = 0.0, 0\n        step = 0\n        for Xb, divb, yb in train_loader:\n            Xb, divb, yb = Xb.to(device), divb.to(device), yb.to(device)\n            logits = main_model(Xb)\n            loss_i = criterion_main(logits, yb)\n            feats_i = torch.cat([loss_i.detach().unsqueeze(1), divb], dim=1)\n            scores = dvn_model(feats_i).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            weighted_loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            weighted_loss.backward()\n            optimizer_main.step()\n            train_loss_sum += weighted_loss.item()\n            batches += 1\n\n            if step % meta_freq == 0:\n                main_model.eval()\n                with torch.no_grad():\n                    base_loss = criterion_main_mean(main_model(X_test), y_test).item()\n                feats_list, contr_list = [], []\n                base_state = main_model.state_dict()\n                for idx in random.sample(range(len(X_train_tensor)), K_meta):\n                    xi = X_train_tensor[idx].unsqueeze(0).to(device)\n                    yi = y_train_tensor[idx].unsqueeze(0).to(device)\n                    with torch.no_grad():\n                        li = criterion_main(main_model(xi), yi).item()\n                    div_i = div_train_tensor[idx].item()\n                    feats_list.append([li, div_i])\n                    clone = MLP(input_dim, num_classes).to(device)\n                    clone.load_state_dict(base_state)\n                    opt_c = torch.optim.Adam(clone.parameters(), lr=1e-3)\n                    clone.train()\n                    out = clone(xi)\n                    lci = criterion_main(out, yi).mean()\n                    opt_c.zero_grad()\n                    lci.backward()\n                    opt_c.step()\n                    clone.eval()\n                    with torch.no_grad():\n                        new_loss = criterion_main_mean(clone(X_test), y_test).item()\n                    contr_list.append([base_loss - new_loss])\n                feats_meta = torch.tensor(feats_list, dtype=torch.float32).to(device)\n                contr_meta = torch.tensor(contr_list, dtype=torch.float32).to(device)\n                for _ in range(meta_steps):\n                    dvn_model.train()\n                    pred = dvn_model(feats_meta)\n                    dvn_loss = criterion_dvn(pred, contr_meta)\n                    optimizer_dvn.zero_grad()\n                    dvn_loss.backward()\n                    optimizer_dvn.step()\n                dvn_model.eval()\n                with torch.no_grad():\n                    preds_c = dvn_model(feats_meta).cpu().numpy().flatten()\n                true_c = contr_meta.cpu().numpy().flatten()\n                corr = spearmanr(preds_c, true_c).correlation\n                experiment_data[name][\"predictions\"].extend(preds_c.tolist())\n                experiment_data[name][\"ground_truth\"].extend(true_c.tolist())\n                experiment_data[name][\"corrs\"].append(corr)\n                if corr < low_th:\n                    meta_freq = max(N_min, meta_freq // 2)\n                elif corr > high_th:\n                    meta_freq = min(N_max, meta_freq * 2)\n                print(f\"[{name}] Step {step}: Spearman Corr={corr:.4f}\")\n                main_model.train()\n            step += 1\n\n        main_model.eval()\n        with torch.no_grad():\n            tr_logits = main_model(X_train_tensor.to(device))\n            tr_loss = criterion_main_mean(tr_logits, y_train_tensor.to(device)).item()\n            tr_acc = (\n                (tr_logits.argmax(dim=1) == y_train_tensor.to(device))\n                .float()\n                .mean()\n                .item()\n            )\n            val_logits = main_model(X_test)\n            val_loss = criterion_main_mean(val_logits, y_test).item()\n            val_acc = (val_logits.argmax(dim=1) == y_test).float().mean().item()\n        experiment_data[name][\"losses\"][\"train\"].append(tr_loss)\n        experiment_data[name][\"metrics\"][\"train\"].append(tr_acc)\n        experiment_data[name][\"losses\"][\"val\"].append(val_loss)\n        experiment_data[name][\"metrics\"][\"val\"].append(val_acc)\n        print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom datasets import load_dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import spearmanr\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nhf_datasets = {\"ag_news\": \"ag_news\", \"yelp\": \"yelp_polarity\", \"dbpedia\": \"dbpedia_14\"}\nexperiment_data = {}\n\nfor name, hf_name in hf_datasets.items():\n    ds = load_dataset(hf_name)\n    train = ds[\"train\"].shuffle(seed=42).select(range(1000))\n    test = ds[\"test\"].shuffle(seed=42).select(range(200))\n    text_col = \"text\" if \"text\" in train.column_names else \"content\"\n    train_texts, y_train = train[text_col], train[\"label\"]\n    test_texts, y_test = test[text_col], test[\"label\"]\n\n    tfidf = TfidfVectorizer(max_features=500, norm=\"l2\")\n    tfidf.fit(train_texts + test_texts)\n    X_train_np = tfidf.transform(train_texts).toarray()\n    X_test_np = tfidf.transform(test_texts).toarray()\n    ent_train_np = -np.sum(X_train_np * np.log(X_train_np + 1e-10), axis=1)\n\n    X_train = torch.tensor(X_train_np, dtype=torch.float32)\n    ent_train = torch.tensor(ent_train_np, dtype=torch.float32).unsqueeze(1)\n    y_train_t = torch.tensor(y_train, dtype=torch.long)\n    X_test = torch.tensor(X_test_np, dtype=torch.float32).to(device)\n    y_test_t = torch.tensor(y_test, dtype=torch.long).to(device)\n\n    train_ds = TensorDataset(X_train, ent_train, y_train_t)\n    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n\n    input_dim = X_train.shape[1]\n    num_classes = len(set(y_train))\n\n    class MLP(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(input_dim, 128), nn.ReLU(), nn.Linear(128, num_classes)\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    class DVN(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Sequential(nn.Linear(3, 32), nn.ReLU(), nn.Linear(32, 1))\n\n        def forward(self, x):\n            return self.net(x)\n\n    main_model = MLP().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-3)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-3)\n    crit_main = nn.CrossEntropyLoss(reduction=\"none\")\n    crit_eval = nn.CrossEntropyLoss()\n    crit_meta = nn.MSELoss()\n\n    epoch_metrics = {\"val_loss\": [], \"val_acc\": [], \"corr\": [], \"fairness\": []}\n    N_meta, K_meta, epochs = 20, 10, 5\n\n    for epoch in range(epochs):\n        main_model.train()\n        corr_list = []\n        step = 0\n        for Xb, entb, yb in train_loader:\n            Xb, entb, yb = Xb.to(device), entb.to(device), yb.to(device)\n            logits = main_model(Xb)\n            loss_i = crit_main(logits, yb)\n            p = nn.functional.softmax(logits, dim=1)\n            ent_mod = -(p * torch.log(p + 1e-10)).sum(dim=1, keepdim=True)\n            feats = torch.cat(\n                [loss_i.detach().unsqueeze(1), entb, ent_mod.detach()], dim=1\n            )\n            w = torch.softmax(dvn_model(feats).squeeze(1), dim=0)\n            loss = (w * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n\n            if step % N_meta == 0:\n                main_model.eval()\n                with torch.no_grad():\n                    base_loss = crit_eval(main_model(X_test), y_test_t).item()\n                feats_meta, contr = [], []\n                base_state = main_model.state_dict()\n\n                for idx in random.sample(range(len(X_train)), K_meta):\n                    xi = X_train[idx].unsqueeze(0).to(device)\n                    yi = y_train_t[idx].unsqueeze(0).to(device)\n                    with torch.no_grad():\n                        logits_i = main_model(xi)\n                        li = crit_main(logits_i, yi).item()\n                        pi = nn.functional.softmax(logits_i, dim=1)\n                        ent_i = -(pi * torch.log(pi + 1e-10)).sum().item()\n                    clone = MLP().to(device)\n                    clone.load_state_dict(base_state)\n                    opt_c = torch.optim.Adam(clone.parameters(), lr=1e-3)\n                    clone.train()\n                    out = clone(xi)\n                    loss_c = crit_main(out, yi).mean()\n                    opt_c.zero_grad()\n                    loss_c.backward()\n                    opt_c.step()\n                    clone.eval()\n                    with torch.no_grad():\n                        new_loss = crit_eval(clone(X_test), y_test_t).item()\n                    feats_meta.append([li, ent_train[idx].item(), ent_i])\n                    contr.append(base_loss - new_loss)\n\n                feats_meta = torch.tensor(feats_meta, dtype=torch.float32).to(device)\n                contr = torch.tensor(contr, dtype=torch.float32).unsqueeze(1).to(device)\n                for _ in range(5):\n                    dvn_model.train()\n                    loss_d = crit_meta(dvn_model(feats_meta), contr)\n                    optimizer_dvn.zero_grad()\n                    loss_d.backward()\n                    optimizer_dvn.step()\n                dvn_model.eval()\n                preds = dvn_model(feats_meta).detach().cpu().numpy().flatten()\n                trues = contr.cpu().numpy().flatten()\n                corr = spearmanr(preds, trues).correlation\n                corr_list.append(corr)\n                if corr < 0.3:\n                    N_meta = max(5, N_meta // 2)\n                elif corr > 0.7:\n                    N_meta = min(100, N_meta * 2)\n                main_model.train()\n            step += 1\n\n        main_model.eval()\n        with torch.no_grad():\n            logits_val = main_model(X_test)\n            val_loss = crit_eval(logits_val, y_test_t).item()\n            val_acc = (logits_val.argmax(dim=1) == y_test_t).float().mean().item()\n        avg_corr = float(np.mean(corr_list)) if corr_list else 0.0\n        print(\n            f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, Spearman Corr = {avg_corr:.4f}\"\n        )\n        epoch_metrics[\"val_loss\"].append(val_loss)\n        epoch_metrics[\"val_acc\"].append(val_acc)\n        epoch_metrics[\"corr\"].append(avg_corr)\n\n        if name == \"yelp\":\n            labels = y_test_t.cpu().numpy()\n            preds_np = logits_val.argmax(dim=1).cpu().numpy()\n            acc0 = (preds_np[labels == 0] == 0).mean()\n            acc1 = (preds_np[labels == 1] == 1).mean()\n            disp = abs(acc0 - acc1)\n            epoch_metrics[\"fairness\"].append(disp)\n            print(f\"Yelp fairness disparity = {disp:.4f}\")\n        else:\n            epoch_metrics[\"fairness\"].append(None)\n\n    experiment_data[name] = epoch_metrics\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom datasets import load_dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import spearmanr\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nhf_datasets = {\"ag_news\": \"ag_news\", \"dbpedia\": \"dbpedia_14\", \"yelp\": \"yelp_polarity\"}\nexperiment_data = {}\n\n\nclass MLP(nn.Module):\n    def __init__(self, inp, out):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(inp, 128), nn.ReLU(), nn.Linear(128, out))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, 1)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nfor name, hf in hf_datasets.items():\n    ds = load_dataset(hf)\n    tr = ds[\"train\"].shuffle(seed=42).select(range(1000))\n    te = ds[\"test\"].shuffle(seed=42).select(range(200))\n    text_col = \"text\" if \"text\" in tr.column_names else \"content\"\n    tv = TfidfVectorizer(max_features=500, norm=\"l2\")\n    tv.fit(tr[text_col] + te[text_col])\n    Xtr = tv.transform(tr[text_col]).toarray()\n    Xte = tv.transform(te[text_col]).toarray()\n    mean_vec = Xtr.mean(0, keepdims=True)\n    div_tr = np.linalg.norm(Xtr - mean_vec, axis=1)\n    X_train = torch.tensor(Xtr, dtype=torch.float32)\n    div_train = torch.tensor(div_tr, dtype=torch.float32).unsqueeze(1)\n    y_train = torch.tensor(tr[\"label\"], dtype=torch.long)\n    X_test = torch.tensor(Xte, dtype=torch.float32).to(device)\n    y_test = torch.tensor(te[\"label\"], dtype=torch.long).to(device)\n\n    train_ds = TensorDataset(X_train, div_train, y_train)\n    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n\n    input_dim = X_train.shape[1]\n    num_classes = len(set(tr[\"label\"]))\n    main_model = MLP(input_dim, num_classes).to(device)\n    dvn_model = DVN().to(device)\n    opt_main = torch.optim.Adam(main_model.parameters(), lr=1e-3)\n    opt_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-3)\n    crit_main = nn.CrossEntropyLoss(reduction=\"none\")\n    crit_main_mean = nn.CrossEntropyLoss(reduction=\"mean\")\n    crit_dvn = nn.MSELoss()\n\n    meta_freq, N_min, N_max = 50, 10, 200\n    low_th, high_th = 0.3, 0.7\n    K_meta, meta_steps, epochs = 30, 5, 3\n\n    experiment_data[name] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"corrs\": [],\n    }\n\n    for epoch in range(epochs):\n        main_model.train()\n        step = 0\n        for Xb, divb, yb in train_loader:\n            Xb, divb, yb = Xb.to(device), divb.to(device), yb.to(device)\n            Xb.requires_grad_(True)\n            logits = main_model(Xb)\n            loss_i = crit_main(logits, yb)\n            grads = torch.autograd.grad(loss_i.sum(), Xb, retain_graph=True)[0]\n            grad_norm = grads.norm(dim=1).detach().unsqueeze(1)\n            feats = torch.cat([loss_i.detach().unsqueeze(1), grad_norm, divb], dim=1)\n            scores = dvn_model(feats).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            wloss = (weights * loss_i).sum()\n            opt_main.zero_grad()\n            wloss.backward()\n            opt_main.step()\n\n            if step % meta_freq == 0:\n                main_model.eval()\n                with torch.no_grad():\n                    base_loss = crit_main_mean(main_model(X_test), y_test).item()\n                feats_list, contr_list = [], []\n                base_state = main_model.state_dict()\n                idxs = random.sample(range(len(X_train)), K_meta)\n                for idx in idxs:\n                    xi = X_train[idx : idx + 1].to(device)\n                    yi = y_train[idx : idx + 1].to(device)\n                    li = crit_main(main_model(xi), yi).item()\n                    gi = (\n                        torch.autograd.grad(crit_main(main_model(xi), yi).sum(), xi)[0]\n                        .norm()\n                        .item()\n                    )\n                    di = div_train[idx].item()\n                    feats_list.append([li, gi, di])\n                    clone = MLP(input_dim, num_classes).to(device)\n                    clone.load_state_dict(base_state)\n                    oc = torch.optim.Adam(clone.parameters(), lr=1e-3)\n                    oc.zero_grad()\n                    lci = crit_main(clone(xi), yi).sum()\n                    lci.backward()\n                    oc.step()\n                    clone.eval()\n                    with torch.no_grad():\n                        new_loss = crit_main_mean(clone(X_test), y_test).item()\n                    contr_list.append(base_loss - new_loss)\n                feats_m = torch.tensor(feats_list, dtype=torch.float32).to(device)\n                true_c = (\n                    torch.tensor(contr_list, dtype=torch.float32)\n                    .unsqueeze(1)\n                    .to(device)\n                )\n                for _ in range(meta_steps):\n                    dvn_model.train()\n                    pred_c = dvn_model(feats_m)\n                    loss_d = crit_dvn(pred_c, true_c)\n                    opt_dvn.zero_grad()\n                    loss_d.backward()\n                    opt_dvn.step()\n                dvn_model.eval()\n                with torch.no_grad():\n                    pc = dvn_model(feats_m).cpu().numpy().flatten()\n                tc = true_c.cpu().numpy().flatten()\n                corr = spearmanr(pc, tc).correlation\n                experiment_data[name][\"corrs\"].append(corr)\n                print(f\"[{name}] Step {step}: Spearman Corr={corr:.4f}\")\n                if corr < low_th:\n                    meta_freq = max(N_min, meta_freq // 2)\n                elif corr > high_th:\n                    meta_freq = min(N_max, meta_freq * 2)\n                main_model.train()\n            step += 1\n\n        main_model.eval()\n        with torch.no_grad():\n            tr_l = crit_main_mean(\n                main_model(X_train.to(device)), y_train.to(device)\n            ).item()\n            tr_acc = (\n                (main_model(X_train.to(device)).argmax(1) == y_train.to(device))\n                .float()\n                .mean()\n                .item()\n            )\n            va_l = crit_main_mean(main_model(X_test), y_test).item()\n            va_acc = (main_model(X_test).argmax(1) == y_test).float().mean().item()\n        experiment_data[name][\"losses\"][\"train\"].append(tr_l)\n        experiment_data[name][\"metrics\"][\"train\"].append(tr_acc)\n        experiment_data[name][\"losses\"][\"val\"].append(va_l)\n        experiment_data[name][\"metrics\"][\"val\"].append(va_acc)\n        print(f\"Epoch {epoch}: validation_loss = {va_l:.4f}\")\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom datasets import load_dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import spearmanr\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nhf_datasets = {\"ag_news\": \"ag_news\", \"yelp\": \"yelp_polarity\", \"dbpedia\": \"dbpedia_14\"}\nexperiment_data = {}\n\nfor name, hf_name in hf_datasets.items():\n    ds = load_dataset(hf_name)\n    train = ds[\"train\"].shuffle(seed=42).select(range(1000))\n    test = ds[\"test\"].shuffle(seed=42).select(range(200))\n    text_col = \"text\" if \"text\" in train.column_names else \"content\"\n    tr_txt, te_txt = train[text_col], test[text_col]\n    y_tr, y_te = train[\"label\"], test[\"label\"]\n\n    tfidf = TfidfVectorizer(max_features=500, norm=\"l2\")\n    tfidf.fit(tr_txt + te_txt)\n    X_tr_np = tfidf.transform(tr_txt).toarray()\n    X_te_np = tfidf.transform(te_txt).toarray()\n    ent_tr = -np.sum(X_tr_np * np.log(X_tr_np + 1e-10), axis=1, keepdims=True)\n\n    X_train = torch.tensor(X_tr_np, dtype=torch.float32)\n    ent_train = torch.tensor(ent_tr, dtype=torch.float32)\n    y_train_t = torch.tensor(y_tr, dtype=torch.long)\n    X_test = torch.tensor(X_te_np, dtype=torch.float32).to(device)\n    y_test_t = torch.tensor(y_te, dtype=torch.long).to(device)\n\n    train_ds = TensorDataset(X_train, ent_train, y_train_t)\n    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n\n    input_dim = X_train.shape[1]\n    num_classes = len(set(y_tr))\n\n    class MLP(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(input_dim, 128),\n                nn.ReLU(),\n                nn.Linear(128, num_classes),\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    class DVN(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(3, 32),\n                nn.ReLU(),\n                nn.Linear(32, 1),\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    main_model = MLP().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-3)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-3)\n    crit_main = nn.CrossEntropyLoss(reduction=\"none\")\n    crit_dvn = nn.MSELoss()\n\n    experiment_data[name] = {\n        \"val_loss\": [],\n        \"val_acc\": [],\n        \"corrs\": [],\n        \"N_meta_history\": [],\n    }\n    N_meta, prev_corr = 10, None\n    K_meta, epochs = 20, 3\n\n    for epoch in range(epochs):\n        main_model.train()\n        step = 0\n        for Xb, entb, yb in train_loader:\n            batch = {\"X\": Xb.to(device), \"ent\": entb.to(device), \"y\": yb.to(device)}\n            logits = main_model(batch[\"X\"])\n            loss_i = crit_main(logits, batch[\"y\"])\n            # representation norm feature\n            reps = main_model.net[1](main_model.net[0](batch[\"X\"]))\n            rep_norm = torch.norm(reps, dim=1, keepdim=True)\n            feats = torch.cat(\n                [loss_i.detach().unsqueeze(1), batch[\"ent\"], rep_norm], dim=1\n            )\n            weights = torch.softmax(dvn_model(feats).squeeze(1), dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n\n            if step % N_meta == 0:\n                main_model.eval()\n                with torch.no_grad():\n                    base_loss = nn.CrossEntropyLoss()(\n                        main_model(X_test), y_test_t\n                    ).item()\n                feats_list, contr_list = [], []\n                base_state = main_model.state_dict()\n                for idx in random.sample(range(len(X_train)), K_meta):\n                    xi = X_train[idx].unsqueeze(0).to(device)\n                    yi = y_train_t[idx].unsqueeze(0).to(device)\n                    with torch.no_grad():\n                        li = crit_main(main_model(xi), yi).item()\n                        ent_i = ent_train[idx].item()\n                        # compute rep norm for meta feature\n                        rep_i = main_model.net[1](main_model.net[0](xi))\n                        rep_norm_i = torch.norm(rep_i, dim=1).item()\n                    feats_list.append([li, ent_i, rep_norm_i])\n                    clone = MLP().to(device)\n                    clone.load_state_dict(base_state)\n                    opt_c = torch.optim.Adam(clone.parameters(), lr=1e-3)\n                    clone.train()\n                    lc = crit_main(clone(xi), yi).mean()\n                    opt_c.zero_grad()\n                    lc.backward()\n                    opt_c.step()\n                    clone.eval()\n                    with torch.no_grad():\n                        new_loss = nn.CrossEntropyLoss()(clone(X_test), y_test_t).item()\n                    contr_list.append([base_loss - new_loss])\n                feats_meta = torch.tensor(feats_list, dtype=torch.float32).to(device)\n                contr_meta = torch.tensor(contr_list, dtype=torch.float32).to(device)\n                for _ in range(5):\n                    dvn_model.train()\n                    p = dvn_model(feats_meta)\n                    dvn_loss = crit_dvn(p, contr_meta)\n                    optimizer_dvn.zero_grad()\n                    dvn_loss.backward()\n                    optimizer_dvn.step()\n                dvn_model.eval()\n                with torch.no_grad():\n                    preds = dvn_model(feats_meta).cpu().numpy().flatten()\n                true = contr_meta.cpu().numpy().flatten()\n                corr = spearmanr(preds, true).correlation\n                experiment_data[name][\"corrs\"].append(corr)\n                if prev_corr is not None:\n                    N_meta = (\n                        min(50, N_meta * 2) if corr > prev_corr else max(1, N_meta // 2)\n                    )\n                experiment_data[name][\"N_meta_history\"].append(N_meta)\n                prev_corr = corr\n                print(\n                    f\"[{name}] Step {step}: Spearman Corr={corr:.4f}, N_meta={N_meta}\"\n                )\n                main_model.train()\n            step += 1\n\n        main_model.eval()\n        with torch.no_grad():\n            logits_val = main_model(X_test)\n            val_loss = nn.CrossEntropyLoss()(logits_val, y_test_t).item()\n            acc = (logits_val.argmax(1) == y_test_t).float().mean().item()\n        experiment_data[name][\"val_loss\"].append(val_loss)\n        experiment_data[name][\"val_acc\"].append(acc)\n        print(f\"[{name}] Epoch {epoch}: val_loss={val_loss:.4f}, val_acc={acc:.4f}\")\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport copy\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom torch.optim import AdamW\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass TextDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.ids = hf_dataset[\"input_ids\"]\n        self.attn = hf_dataset[\"attention_mask\"]\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, i):\n        return {\n            \"input_ids\": torch.tensor(self.ids[i], dtype=torch.long),\n            \"attention_mask\": torch.tensor(self.attn[i], dtype=torch.long),\n            \"labels\": torch.tensor(self.ids[i], dtype=torch.long),\n        }\n\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\nblock_size = 64\nexperiment_data = {}\n\nfor ds_name, name, cfg in [\n    (\"wikitext2\", \"wikitext\", \"wikitext-2-raw-v1\"),\n    (\"bookcorpus\", \"bookcorpus\", None),\n    (\"openwebtext\", \"openwebtext\", None),\n]:\n    print(f\"*** Dataset: {ds_name}\")\n    if cfg:\n        raw = load_dataset(name, cfg, split=\"train[:1%]\")\n    else:\n        raw = load_dataset(name, split=\"train[:1%]\")\n\n    def tok(ex):\n        return tokenizer(ex[\"text\"])\n\n    tokenized = raw.map(tok, batched=True, remove_columns=[\"text\"])\n\n    def group(ex):\n        all_ids = sum(ex[\"input_ids\"], [])\n        total = (len(all_ids) // block_size) * block_size\n        chunks = [all_ids[i : i + block_size] for i in range(0, total, block_size)]\n        return {\n            \"input_ids\": chunks,\n            \"attention_mask\": [[1] * block_size for _ in chunks],\n        }\n\n    lm_ds = tokenized.map(\n        group,\n        batched=True,\n        batch_size=1000,\n        remove_columns=[\"input_ids\", \"attention_mask\"],\n    )\n    split = lm_ds.train_test_split(test_size=0.1)\n    train_ds, val_ds = split[\"train\"], split[\"test\"]\n    train_loader = DataLoader(TextDataset(train_ds), batch_size=8, shuffle=True)\n    val_loader = DataLoader(TextDataset(val_ds), batch_size=8)\n\n    model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n    dvn = DVN().to(device)\n    opt_main = AdamW(model.parameters(), lr=5e-5)\n    opt_dvn = torch.optim.Adam(dvn.parameters(), lr=1e-3)\n\n    experiment_data[ds_name] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"correlations\": [],\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    EPOCHS = 3\n    for epoch in range(EPOCHS):\n        model.train()\n        tot_loss = 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            bs = batch[\"input_ids\"].size(0)\n            feats = []\n            with torch.no_grad():\n                for i in range(bs):\n                    out = model(\n                        input_ids=batch[\"input_ids\"][i].unsqueeze(0),\n                        attention_mask=batch[\"attention_mask\"][i].unsqueeze(0),\n                        labels=batch[\"labels\"][i].unsqueeze(0),\n                    )\n                    feats.append(\n                        out.loss.item() * batch[\"attention_mask\"][i].sum().item()\n                    )\n            feats = torch.tensor(feats, dtype=torch.float, device=device).unsqueeze(1)\n            scores = dvn(feats).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            outputs = model(\n                input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n            )\n            logits = outputs.logits\n            shift_logits = logits[:, :-1, :].contiguous()\n            shift_labels = batch[\"labels\"][:, 1:].contiguous()\n            loss_tokens = F.cross_entropy(\n                shift_logits.view(-1, shift_logits.size(-1)),\n                shift_labels.view(-1),\n                reduction=\"none\",\n            )\n            loss_tok = loss_tokens.view(bs, -1).sum(dim=1)\n            loss = (weights * loss_tok).sum()\n            opt_main.zero_grad()\n            loss.backward()\n            opt_main.step()\n            tot_loss += loss.item()\n        train_loss = tot_loss / len(train_loader)\n        experiment_data[ds_name][\"metrics\"][\"train\"].append(train_loss)\n\n        model.eval()\n        vlosses = []\n        with torch.no_grad():\n            for vb in val_loader:\n                vb = {k: v.to(device) for k, v in vb.items()}\n                out = model(\n                    input_ids=vb[\"input_ids\"],\n                    attention_mask=vb[\"attention_mask\"],\n                    labels=vb[\"labels\"],\n                )\n                vlosses.append(out.loss.item())\n        val_loss = float(np.mean(vlosses))\n        experiment_data[ds_name][\"metrics\"][\"val\"].append(val_loss)\n        print(f\"Dataset {ds_name} Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n        feats_list, ctr_list = [], []\n        idxs = np.random.choice(len(train_ds), min(20, len(train_ds)), replace=False)\n        for idx in idxs:\n            item = train_ds[int(idx)]\n            bi = {\n                k: torch.tensor(item[k], dtype=torch.long).unsqueeze(0).to(device)\n                for k in (\"input_ids\", \"attention_mask\")\n            }\n            bi[\"labels\"] = bi[\"input_ids\"]\n            with torch.no_grad():\n                fval = model(**bi).loss.item() * bi[\"attention_mask\"].sum().item()\n            clone = copy.deepcopy(model).to(device).eval()\n            l0 = []\n            with torch.no_grad():\n                for vb in val_loader:\n                    vb = {k: v.to(device) for k, v in vb.items()}\n                    l0.append(\n                        clone(\n                            input_ids=vb[\"input_ids\"],\n                            attention_mask=vb[\"attention_mask\"],\n                            labels=vb[\"labels\"],\n                        ).loss.item()\n                    )\n            L0 = float(np.mean(l0))\n            clone.train()\n            opt_c = AdamW(clone.parameters(), lr=5e-5)\n            out_c = clone(**bi)\n            opt_c.zero_grad()\n            out_c.loss.backward()\n            opt_c.step()\n            clone.eval()\n            l1 = []\n            with torch.no_grad():\n                for vb in val_loader:\n                    vb = {k: v.to(device) for k, v in vb.items()}\n                    l1.append(\n                        clone(\n                            input_ids=vb[\"input_ids\"],\n                            attention_mask=vb[\"attention_mask\"],\n                            labels=vb[\"labels\"],\n                        ).loss.item()\n                    )\n            L1 = float(np.mean(l1))\n            feats_list.append([fval])\n            ctr_list.append([L0 - L1])\n        F_feats = torch.tensor(feats_list, dtype=torch.float32, device=device)\n        F_ctrs = torch.tensor(ctr_list, dtype=torch.float32, device=device)\n        for _ in range(5):\n            dvn.train()\n            pred = dvn(F_feats)\n            l_dvn = F.mse_loss(pred, F_ctrs)\n            opt_dvn.zero_grad()\n            l_dvn.backward()\n            opt_dvn.step()\n        dvn.eval()\n        with torch.no_grad():\n            p_np = dvn(F_feats).cpu().numpy().flatten()\n        t_np = F_ctrs.cpu().numpy().flatten()\n        corr = spearman_corr(p_np, t_np)\n        experiment_data[ds_name][\"correlations\"].append(corr)\n        experiment_data[ds_name][\"predictions\"].append(p_np)\n        experiment_data[ds_name][\"ground_truth\"].append(t_np)\n        print(f\"Dataset {ds_name} Epoch {epoch}: Spearman Corr={corr:.4f}\")\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom datasets import load_dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import spearmanr\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nhf_datasets = {\"ag_news\": \"ag_news\", \"dbpedia\": \"dbpedia_14\", \"yelp\": \"yelp_polarity\"}\nexperiment_data = {}\n\n\nclass MLP(nn.Module):\n    def __init__(self, inp, out):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(inp, 128), nn.ReLU(), nn.Linear(128, out))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, 1)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nfor name, hf in hf_datasets.items():\n    ds = load_dataset(hf)\n    tr = ds[\"train\"].shuffle(seed=42).select(range(1000))\n    te = ds[\"test\"].shuffle(seed=42).select(range(200))\n    text_col = \"text\" if \"text\" in tr.column_names else \"content\"\n    tv = TfidfVectorizer(max_features=500, norm=\"l2\")\n    tv.fit(tr[text_col] + te[text_col])\n    Xtr = tv.transform(tr[text_col]).toarray()\n    Xte = tv.transform(te[text_col]).toarray()\n    mean_vec = Xtr.mean(0, keepdims=True)\n    div_tr = np.linalg.norm(Xtr - mean_vec, axis=1)\n    X_train = torch.tensor(Xtr, dtype=torch.float32)\n    div_train = torch.tensor(div_tr, dtype=torch.float32).unsqueeze(1)\n    y_train = torch.tensor(tr[\"label\"], dtype=torch.long)\n    X_test = torch.tensor(Xte, dtype=torch.float32).to(device)\n    y_test = torch.tensor(te[\"label\"], dtype=torch.long).to(device)\n\n    train_ds = TensorDataset(X_train, div_train, y_train)\n    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n\n    input_dim = X_train.shape[1]\n    num_classes = len(set(tr[\"label\"]))\n    main_model = MLP(input_dim, num_classes).to(device)\n    dvn_model = DVN().to(device)\n    opt_main = torch.optim.Adam(main_model.parameters(), lr=1e-3)\n    opt_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-3)\n    crit_main = nn.CrossEntropyLoss(reduction=\"none\")\n    crit_main_mean = nn.CrossEntropyLoss(reduction=\"mean\")\n    crit_dvn = nn.MSELoss()\n\n    meta_freq, N_min, N_max = 50, 10, 200\n    low_th, high_th = 0.3, 0.7\n    K_meta, meta_steps, epochs = 30, 5, 3\n\n    experiment_data[name] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"corrs\": [],\n    }\n\n    for epoch in range(epochs):\n        main_model.train()\n        step = 0\n        for Xb, divb, yb in train_loader:\n            Xb, divb, yb = Xb.to(device), divb.to(device), yb.to(device)\n            Xb.requires_grad_(True)\n            logits = main_model(Xb)\n            loss_i = crit_main(logits, yb)\n            grads = torch.autograd.grad(loss_i.sum(), Xb, retain_graph=True)[0]\n            grad_norm = grads.norm(dim=1).detach().unsqueeze(1)\n            feats = torch.cat([loss_i.detach().unsqueeze(1), grad_norm, divb], dim=1)\n            scores = dvn_model(feats).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            wloss = (weights * loss_i).sum()\n            opt_main.zero_grad()\n            wloss.backward()\n            opt_main.step()\n\n            if step % meta_freq == 0:\n                main_model.eval()\n                with torch.no_grad():\n                    base_loss = crit_main_mean(main_model(X_test), y_test).item()\n                feats_list, contr_list = [], []\n                base_state = main_model.state_dict()\n                idxs = random.sample(range(len(X_train)), K_meta)\n                for idx in idxs:\n                    # clone sample and enable gradient tracking\n                    xi = (\n                        X_train[idx : idx + 1]\n                        .clone()\n                        .detach()\n                        .to(device)\n                        .requires_grad_(True)\n                    )\n                    yi = y_train[idx : idx + 1].to(device)\n                    li = crit_main(main_model(xi), yi).item()\n                    gi = (\n                        torch.autograd.grad(crit_main(main_model(xi), yi).sum(), xi)[0]\n                        .norm()\n                        .item()\n                    )\n                    di = div_train[idx].item()\n                    feats_list.append([li, gi, di])\n\n                    # compute one-step influence by cloning model\n                    clone = MLP(input_dim, num_classes).to(device)\n                    clone.load_state_dict(base_state)\n                    oc = torch.optim.Adam(clone.parameters(), lr=1e-3)\n                    oc.zero_grad()\n                    lci = crit_main(clone(xi), yi).sum()\n                    lci.backward()\n                    oc.step()\n                    clone.eval()\n                    with torch.no_grad():\n                        new_loss = crit_main_mean(clone(X_test), y_test).item()\n                    contr_list.append(base_loss - new_loss)\n\n                feats_m = torch.tensor(feats_list, dtype=torch.float32).to(device)\n                true_c = (\n                    torch.tensor(contr_list, dtype=torch.float32)\n                    .unsqueeze(1)\n                    .to(device)\n                )\n                for _ in range(meta_steps):\n                    dvn_model.train()\n                    pred_c = dvn_model(feats_m)\n                    loss_d = crit_dvn(pred_c, true_c)\n                    opt_dvn.zero_grad()\n                    loss_d.backward()\n                    opt_dvn.step()\n                dvn_model.eval()\n                with torch.no_grad():\n                    pc = dvn_model(feats_m).cpu().numpy().flatten()\n                tc = true_c.cpu().numpy().flatten()\n                corr = spearmanr(pc, tc).correlation\n                experiment_data[name][\"corrs\"].append(corr)\n                print(f\"[{name}] Step {step}: Spearman Corr={corr:.4f}\")\n                if corr < low_th:\n                    meta_freq = max(N_min, meta_freq // 2)\n                elif corr > high_th:\n                    meta_freq = min(N_max, meta_freq * 2)\n                main_model.train()\n            step += 1\n\n        main_model.eval()\n        with torch.no_grad():\n            tr_l = crit_main_mean(\n                main_model(X_train.to(device)), y_train.to(device)\n            ).item()\n            tr_acc = (\n                (main_model(X_train.to(device)).argmax(1) == y_train.to(device))\n                .float()\n                .mean()\n                .item()\n            )\n            va_l = crit_main_mean(main_model(X_test), y_test).item()\n            va_acc = (main_model(X_test).argmax(1) == y_test).float().mean().item()\n        experiment_data[name][\"losses\"][\"train\"].append(tr_l)\n        experiment_data[name][\"metrics\"][\"train\"].append(tr_acc)\n        experiment_data[name][\"losses\"][\"val\"].append(va_l)\n        experiment_data[name][\"metrics\"][\"val\"].append(va_acc)\n        print(f\"Epoch {epoch}: validation_loss = {va_l:.4f}\")\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset, concatenate_datasets\nfrom transformers import GPT2TokenizerFast, GPT2LMHeadModel\nfrom torch.nn import CrossEntropyLoss, MSELoss\nimport torch.nn as nn\n\n\ndef spearman_corr(a, b):\n    a, b = np.array(a), np.array(b)\n    ar, br = np.argsort(np.argsort(a)), np.argsort(np.argsort(b))\n    return np.corrcoef(ar, br)[0, 1]\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# load and subsample 3 corpora\nd1 = load_dataset(\"openwebtext\", split=\"train[:1000]\")\nd2 = load_dataset(\"bookcorpus\", split=\"train[:1000]\")\nd3 = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1000]\")\ntokenizer = GPT2TokenizerFast.from_pretrained(\"distilgpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\n\ndef tokenize(ex):\n    return tokenizer(ex[\"text\"], truncation=True, max_length=128, padding=\"max_length\")\n\n\nds = [d.map(tokenize, batched=True, remove_columns=[\"text\"]) for d in (d1, d2, d3)]\ndataset = concatenate_datasets(ds)\ndataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\nsplit = dataset.train_test_split(test_size=0.1, seed=42)\ntrain_dataset, val_dataset = split[\"train\"], split[\"test\"]\nn_hold = min(100, len(val_dataset))\nheldout_dataset = val_dataset.select(range(n_hold))\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8)\nheldout_loader = DataLoader(heldout_dataset, batch_size=16)\n\n# models, losses, optimizers\nmain_model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\nmain_model.config.output_hidden_states = True\nmain_model.to(device)\noptimizer_main = torch.optim.Adam(main_model.parameters(), lr=5e-5)\n\ndvn = nn.Sequential(nn.Linear(2, 32), nn.ReLU(), nn.Linear(32, 1)).to(device)\noptimizer_dvn = torch.optim.Adam(dvn.parameters(), lr=1e-3)\n\nce_loss = CrossEntropyLoss(reduction=\"none\")\nmse_loss = MSELoss()\n\nexperiment_data = {\n    \"metrics\": {\"train_loss\": [], \"val_loss\": [], \"val_ppl\": [], \"contrib_corr\": []}\n}\nEPOCHS, M, DVN_STEPS = 3, 4, 10\nvocab_size = tokenizer.vocab_size\n\nfor epoch in range(EPOCHS):\n    # -- training --\n    main_model.train()\n    total_train = 0.0\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        out = main_model(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            output_hidden_states=True,\n        )\n        logits, hid = out.logits, out.hidden_states[-1]\n        sl = logits[..., :-1, :].contiguous()\n        sh = batch[\"input_ids\"][..., 1:].contiguous()\n        B, L = sh.shape\n        loss_all = ce_loss(sl.view(-1, vocab_size), sh.view(-1))\n        loss_per = loss_all.view(B, L).mean(dim=1)\n        emb_norms = hid.mean(dim=1).norm(dim=1)\n        feats = torch.stack([loss_per.detach(), emb_norms.detach()], dim=1)\n        scores = dvn(feats).squeeze(1)\n        weights = torch.softmax(scores, dim=0)\n        loss = (weights * loss_per).sum()\n        optimizer_main.zero_grad()\n        loss.backward()\n        optimizer_main.step()\n        total_train += loss.item()\n    avg_train = total_train / len(train_loader)\n    experiment_data[\"metrics\"][\"train_loss\"].append(avg_train)\n\n    # -- validation --\n    main_model.eval()\n    tot_val = 0.0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = main_model(\n                input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n            )\n            sl = out.logits[..., :-1, :].contiguous()\n            sh = batch[\"input_ids\"][..., 1:].contiguous()\n            la = ce_loss(sl.view(-1, vocab_size), sh.view(-1))\n            tot_val += la.view(batch[\"input_ids\"].size(0), -1).mean(dim=1).mean().item()\n    avg_val = tot_val / len(val_loader)\n    ppl = float(np.exp(avg_val))\n    experiment_data[\"metrics\"][\"val_loss\"].append(avg_val)\n    experiment_data[\"metrics\"][\"val_ppl\"].append(ppl)\n    print(f\"Epoch {epoch}: validation_loss = {avg_val:.4f}\")\n\n    # -- DVN meta\u2010update --\n    main_model.eval()\n    feats_list, ctr_list = [], []\n    indices = torch.randperm(len(train_dataset))[:M].tolist()\n    for idx in indices:\n        sample = train_dataset[idx]\n        xi = sample[\"input_ids\"].unsqueeze(0).to(device)\n        mi = sample[\"attention_mask\"].unsqueeze(0).to(device)\n        with torch.no_grad():\n            o = main_model(input_ids=xi, attention_mask=mi, output_hidden_states=True)\n            li = (\n                ce_loss(\n                    o.logits[..., :-1, :].view(-1, vocab_size),\n                    xi[..., 1:].contiguous().view(-1),\n                )\n                .mean()\n                .item()\n            )\n            ei = o.hidden_states[-1].mean(dim=1).squeeze(0).norm().item()\n        clone = GPT2LMHeadModel(main_model.config).to(device)\n        clone.load_state_dict(main_model.state_dict())\n        optc = torch.optim.Adam(clone.parameters(), lr=5e-5)\n        clone.eval()\n        L0 = 0.0\n        with torch.no_grad():\n            for b in heldout_loader:\n                b = {k: v.to(device) for k, v in b.items()}\n                o0 = clone(input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"])\n                l0 = ce_loss(\n                    o0.logits[..., :-1, :].view(-1, vocab_size),\n                    b[\"input_ids\"][..., 1:].contiguous().view(-1),\n                )\n                L0 += l0.view(b[\"input_ids\"].size(0), -1).mean().item()\n        L0 /= len(heldout_loader)\n        clone.train()\n        optc.zero_grad()\n        oc = clone(input_ids=xi, attention_mask=mi, labels=xi)\n        oc.loss.backward()\n        optc.step()\n        clone.eval()\n        L1 = 0.0\n        with torch.no_grad():\n            for b in heldout_loader:\n                b = {k: v.to(device) for k, v in b.items()}\n                o1 = clone(input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"])\n                l1 = ce_loss(\n                    o1.logits[..., :-1, :].view(-1, vocab_size),\n                    b[\"input_ids\"][..., 1:].contiguous().view(-1),\n                )\n                L1 += l1.view(b[\"input_ids\"].size(0), -1).mean().item()\n        L1 /= len(heldout_loader)\n        ctr = L0 - L1\n        feats_list.append([li, ei])\n        ctr_list.append(ctr)\n        del clone\n        torch.cuda.empty_cache()\n\n    Fts = torch.tensor(feats_list, dtype=torch.float32).to(device)\n    Trs = torch.tensor(ctr_list, dtype=torch.float32).to(device)\n    for _ in range(DVN_STEPS):\n        preds = dvn(Fts).squeeze(1)\n        ldv = mse_loss(preds, Trs)\n        optimizer_dvn.zero_grad()\n        ldv.backward()\n        optimizer_dvn.step()\n    dvn.eval()\n    with torch.no_grad():\n        corr = spearman_corr(dvn(Fts).squeeze(1).cpu().numpy(), Trs.cpu().numpy())\n    experiment_data[\"metrics\"][\"contrib_corr\"].append(corr)\n    print(f\"Epoch {epoch}: contribution_corr = {corr:.4f}\")\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport copy\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom torch.optim import AdamW\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# utilities\ndef spearman_corr(a, b):\n    a_rank = np.argsort(np.argsort(a))\n    b_rank = np.argsort(np.argsort(b))\n    return np.corrcoef(a_rank, b_rank)[0, 1]\n\n\nclass DVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass TextDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.ids = hf_dataset[\"input_ids\"]\n        self.attn = hf_dataset[\"attention_mask\"]\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, i):\n        return {\n            \"input_ids\": torch.tensor(self.ids[i], dtype=torch.long),\n            \"attention_mask\": torch.tensor(self.attn[i], dtype=torch.long),\n            \"labels\": torch.tensor(self.ids[i], dtype=torch.long),\n        }\n\n\n# tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\ntokenizer.pad_token = tokenizer.eos_token\nblock_size = 64\n\n# prepare experiment data container\nexperiment_data = {}\n\n# iterate datasets\nfor ds_name, name, cfg in [\n    (\"wikitext2\", \"wikitext\", \"wikitext-2-raw-v1\"),\n    (\"bookcorpus\", \"bookcorpus\", None),\n    (\"openwebtext\", \"openwebtext\", None),\n]:\n    print(f\"*** Dataset: {ds_name}\")\n    # load 10% subset\n    split_str = \"train[:10%]\"\n    if cfg:\n        raw = load_dataset(name, cfg, split=split_str)\n    else:\n        raw = load_dataset(name, split=split_str)\n\n    # tokenize\n    def tok(ex):\n        return tokenizer(ex[\"text\"], padding=False, truncation=False)\n\n    tokenized = raw.map(tok, batched=True, remove_columns=[\"text\"])\n\n    # group into blocks\n    def group(ex):\n        all_ids = sum(ex[\"input_ids\"], [])\n        total = (len(all_ids) // block_size) * block_size\n        chunks = [all_ids[i : i + block_size] for i in range(0, total, block_size)]\n        return {\n            \"input_ids\": chunks,\n            \"attention_mask\": [[1] * block_size for _ in chunks],\n        }\n\n    lm_ds = tokenized.map(\n        group,\n        batched=True,\n        batch_size=1000,\n        remove_columns=[\"input_ids\", \"attention_mask\"],\n    )\n    split = lm_ds.train_test_split(test_size=0.1)\n    train_ds, val_ds = split[\"train\"], split[\"test\"]\n    train_loader = DataLoader(TextDataset(train_ds), batch_size=8, shuffle=True)\n    val_loader = DataLoader(TextDataset(val_ds), batch_size=8)\n\n    # models & optimizers\n    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\").to(device)\n    dvn = DVN().to(device)\n    opt_main = AdamW(model.parameters(), lr=5e-5)\n    opt_dvn = torch.optim.Adam(dvn.parameters(), lr=1e-3)\n\n    # init experiment data\n    experiment_data[ds_name] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"correlations\": [],\n    }\n\n    EPOCHS = 10\n    for epoch in range(EPOCHS):\n        # training\n        model.train()\n        tot_loss = 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            bs = batch[\"input_ids\"].size(0)\n            # compute per-sample feature: current loss*length\n            feats = []\n            with torch.no_grad():\n                for i in range(bs):\n                    out = model(\n                        input_ids=batch[\"input_ids\"][i].unsqueeze(0),\n                        attention_mask=batch[\"attention_mask\"][i].unsqueeze(0),\n                        labels=batch[\"labels\"][i].unsqueeze(0),\n                    )\n                    feats.append(\n                        out.loss.item() * batch[\"attention_mask\"][i].sum().item()\n                    )\n            feats = torch.tensor(feats, dtype=torch.float, device=device).unsqueeze(1)\n            scores = dvn(feats).squeeze(1)\n            weights = torch.softmax(scores, dim=0)\n            # compute weighted token loss\n            outputs = model(\n                input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n            )\n            logits = outputs.logits\n            shift_logits = logits[:, :-1, :].contiguous()\n            shift_labels = batch[\"labels\"][:, 1:].contiguous()\n            loss_tokens = F.cross_entropy(\n                shift_logits.view(-1, shift_logits.size(-1)),\n                shift_labels.view(-1),\n                reduction=\"none\",\n            )\n            loss_tok = loss_tokens.view(bs, -1).sum(dim=1)\n            loss = (weights * loss_tok).sum()\n            opt_main.zero_grad()\n            loss.backward()\n            opt_main.step()\n            tot_loss += loss.item()\n        train_loss = tot_loss / len(train_loader)\n        experiment_data[ds_name][\"metrics\"][\"train\"].append(train_loss)\n        experiment_data[ds_name][\"losses\"][\"train\"].append(train_loss)\n\n        # validation\n        model.eval()\n        vlosses = []\n        with torch.no_grad():\n            for vb in val_loader:\n                vb = {k: v.to(device) for k, v in vb.items()}\n                out = model(\n                    input_ids=vb[\"input_ids\"],\n                    attention_mask=vb[\"attention_mask\"],\n                    labels=vb[\"labels\"],\n                )\n                vlosses.append(out.loss.item())\n        val_loss = float(np.mean(vlosses))\n        experiment_data[ds_name][\"metrics\"][\"val\"].append(val_loss)\n        experiment_data[ds_name][\"losses\"][\"val\"].append(val_loss)\n        print(f\"Dataset {ds_name} Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n        # ground-truth contributions & DVN update\n        feats_list, ctr_list = [], []\n        idxs = np.random.choice(len(train_ds), min(20, len(train_ds)), replace=False)\n        for idx in idxs:\n            item = train_ds[int(idx)]\n            bi = {\n                k: torch.tensor(item[k], dtype=torch.long).unsqueeze(0).to(device)\n                for k in (\"input_ids\", \"attention_mask\")\n            }\n            bi[\"labels\"] = bi[\"input_ids\"]\n            with torch.no_grad():\n                fval = model(**bi).loss.item() * bi[\"attention_mask\"].sum().item()\n            clone = copy.deepcopy(model).to(device).eval()\n            # baseline val loss\n            l0 = []\n            with torch.no_grad():\n                for vb in val_loader:\n                    vb = {k: v.to(device) for k, v in vb.items()}\n                    l0.append(clone(**vb).loss.item())\n            L0 = float(np.mean(l0))\n            # one-step finetune\n            clone.train()\n            opt_c = AdamW(clone.parameters(), lr=5e-5)\n            out_c = clone(**bi)\n            opt_c.zero_grad()\n            out_c.loss.backward()\n            opt_c.step()\n            clone.eval()\n            l1 = []\n            with torch.no_grad():\n                for vb in val_loader:\n                    vb = {k: v.to(device) for k, v in vb.items()}\n                    l1.append(clone(**vb).loss.item())\n            L1 = float(np.mean(l1))\n            feats_list.append([fval])\n            ctr_list.append([L0 - L1])\n\n        F_feats = torch.tensor(feats_list, dtype=torch.float, device=device)\n        F_ctrs = torch.tensor(ctr_list, dtype=torch.float, device=device)\n        # train DVN\n        for _ in range(5):\n            dvn.train()\n            pred = dvn(F_feats)\n            l_dvn = F.mse_loss(pred, F_ctrs)\n            opt_dvn.zero_grad()\n            l_dvn.backward()\n            opt_dvn.step()\n        dvn.eval()\n        with torch.no_grad():\n            p_np = dvn(F_feats).cpu().numpy().flatten()\n        t_np = F_ctrs.cpu().numpy().flatten()\n        corr = spearman_corr(p_np, t_np)\n        experiment_data[ds_name][\"correlations\"].append(corr)\n        experiment_data[ds_name][\"predictions\"].append(p_np)\n        experiment_data[ds_name][\"ground_truth\"].append(t_np)\n        print(f\"Dataset {ds_name} Epoch {epoch}: Spearman Corr={corr:.4f}\")\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom datasets import load_dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import spearmanr\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nhf_datasets = {\"ag_news\": \"ag_news\", \"yelp\": \"yelp_polarity\", \"dbpedia\": \"dbpedia_14\"}\nexperiment_data = {}\n\nfor name, hf_name in hf_datasets.items():\n    ds = load_dataset(hf_name)\n    train = ds[\"train\"].shuffle(seed=42).select(range(1000))\n    test = ds[\"test\"].shuffle(seed=42).select(range(200))\n    text_col = \"text\" if \"text\" in train.column_names else \"content\"\n    tr_txt, te_txt = train[text_col], test[text_col]\n    y_tr, y_te = train[\"label\"], test[\"label\"]\n\n    tfidf = TfidfVectorizer(max_features=500, norm=\"l2\")\n    tfidf.fit(tr_txt + te_txt)\n    X_tr_np = tfidf.transform(tr_txt).toarray()\n    X_te_np = tfidf.transform(te_txt).toarray()\n    ent_tr = -np.sum(X_tr_np * np.log(X_tr_np + 1e-10), axis=1, keepdims=True)\n\n    X_train = torch.tensor(X_tr_np, dtype=torch.float32)\n    ent_train = torch.tensor(ent_tr, dtype=torch.float32)\n    y_train_t = torch.tensor(y_tr, dtype=torch.long)\n    X_test = torch.tensor(X_te_np, dtype=torch.float32).to(device)\n    y_test_t = torch.tensor(y_te, dtype=torch.long).to(device)\n\n    train_ds = TensorDataset(X_train, ent_train, y_train_t)\n    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n\n    input_dim = X_train.shape[1]\n    num_classes = len(set(y_tr))\n\n    class MLP(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(input_dim, 128),\n                nn.ReLU(),\n                nn.Linear(128, num_classes),\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    class DVN(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(3, 32),\n                nn.ReLU(),\n                nn.Linear(32, 1),\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    main_model = MLP().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-3)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-3)\n    crit_main = nn.CrossEntropyLoss(reduction=\"none\")\n    crit_dvn = nn.MSELoss()\n\n    experiment_data[name] = {\n        \"val_loss\": [],\n        \"val_acc\": [],\n        \"corrs\": [],\n        \"N_meta_history\": [],\n    }\n    N_meta, prev_corr = 10, None\n    K_meta, epochs = 20, 3\n\n    for epoch in range(epochs):\n        main_model.train()\n        step = 0\n        for Xb, entb, yb in train_loader:\n            batch = {\"X\": Xb.to(device), \"ent\": entb.to(device), \"y\": yb.to(device)}\n            logits = main_model(batch[\"X\"])\n            loss_i = crit_main(logits, batch[\"y\"])\n            # representation norm feature\n            reps = main_model.net[1](main_model.net[0](batch[\"X\"]))\n            rep_norm = torch.norm(reps, dim=1, keepdim=True)\n            feats = torch.cat(\n                [loss_i.detach().unsqueeze(1), batch[\"ent\"], rep_norm], dim=1\n            )\n            weights = torch.softmax(dvn_model(feats).squeeze(1), dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n\n            if step % N_meta == 0:\n                main_model.eval()\n                with torch.no_grad():\n                    base_loss = nn.CrossEntropyLoss()(\n                        main_model(X_test), y_test_t\n                    ).item()\n                feats_list, contr_list = [], []\n                base_state = main_model.state_dict()\n                for idx in random.sample(range(len(X_train)), K_meta):\n                    xi = X_train[idx].unsqueeze(0).to(device)\n                    yi = y_train_t[idx].unsqueeze(0).to(device)\n                    with torch.no_grad():\n                        li = crit_main(main_model(xi), yi).item()\n                        ent_i = ent_train[idx].item()\n                        # compute rep norm for meta feature\n                        rep_i = main_model.net[1](main_model.net[0](xi))\n                        rep_norm_i = torch.norm(rep_i, dim=1).item()\n                    feats_list.append([li, ent_i, rep_norm_i])\n                    clone = MLP().to(device)\n                    clone.load_state_dict(base_state)\n                    opt_c = torch.optim.Adam(clone.parameters(), lr=1e-3)\n                    clone.train()\n                    lc = crit_main(clone(xi), yi).mean()\n                    opt_c.zero_grad()\n                    lc.backward()\n                    opt_c.step()\n                    clone.eval()\n                    with torch.no_grad():\n                        new_loss = nn.CrossEntropyLoss()(clone(X_test), y_test_t).item()\n                    contr_list.append([base_loss - new_loss])\n                feats_meta = torch.tensor(feats_list, dtype=torch.float32).to(device)\n                contr_meta = torch.tensor(contr_list, dtype=torch.float32).to(device)\n                for _ in range(5):\n                    dvn_model.train()\n                    p = dvn_model(feats_meta)\n                    dvn_loss = crit_dvn(p, contr_meta)\n                    optimizer_dvn.zero_grad()\n                    dvn_loss.backward()\n                    optimizer_dvn.step()\n                dvn_model.eval()\n                with torch.no_grad():\n                    preds = dvn_model(feats_meta).cpu().numpy().flatten()\n                true = contr_meta.cpu().numpy().flatten()\n                corr = spearmanr(preds, true).correlation\n                experiment_data[name][\"corrs\"].append(corr)\n                if prev_corr is not None:\n                    N_meta = (\n                        min(50, N_meta * 2) if corr > prev_corr else max(1, N_meta // 2)\n                    )\n                experiment_data[name][\"N_meta_history\"].append(N_meta)\n                prev_corr = corr\n                print(\n                    f\"[{name}] Step {step}: Spearman Corr={corr:.4f}, N_meta={N_meta}\"\n                )\n                main_model.train()\n            step += 1\n\n        main_model.eval()\n        with torch.no_grad():\n            logits_val = main_model(X_test)\n            val_loss = nn.CrossEntropyLoss()(logits_val, y_test_t).item()\n            acc = (logits_val.argmax(1) == y_test_t).float().mean().item()\n        experiment_data[name][\"val_loss\"].append(val_loss)\n        experiment_data[name][\"val_acc\"].append(acc)\n        print(f\"[{name}] Epoch {epoch}: val_loss={val_loss:.4f}, val_acc={acc:.4f}\")\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom datasets import load_dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import spearmanr\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nhf_datasets = {\"ag_news\": \"ag_news\", \"yelp\": \"yelp_polarity\", \"dbpedia\": \"dbpedia_14\"}\nexperiment_data = {}\n\nfor name, hf_name in hf_datasets.items():\n    ds = load_dataset(hf_name)\n    train = ds[\"train\"].shuffle(seed=42).select(range(1000))\n    test = ds[\"test\"].shuffle(seed=42).select(range(200))\n    text_col = \"text\" if \"text\" in train.column_names else \"content\"\n    tr_txt, te_txt = train[text_col], test[text_col]\n    y_tr, y_te = train[\"label\"], test[\"label\"]\n\n    tfidf = TfidfVectorizer(max_features=500, norm=\"l2\")\n    tfidf.fit(tr_txt + te_txt)\n    X_tr_np = tfidf.transform(tr_txt).toarray()\n    X_te_np = tfidf.transform(te_txt).toarray()\n    ent_tr = -np.sum(X_tr_np * np.log(X_tr_np + 1e-10), axis=1, keepdims=True)\n\n    X_train = torch.tensor(X_tr_np, dtype=torch.float32)\n    ent_train = torch.tensor(ent_tr, dtype=torch.float32)\n    y_train_t = torch.tensor(y_tr, dtype=torch.long)\n    X_test = torch.tensor(X_te_np, dtype=torch.float32).to(device)\n    y_test_t = torch.tensor(y_te, dtype=torch.long).to(device)\n\n    train_ds = TensorDataset(X_train, ent_train, y_train_t)\n    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n\n    input_dim = X_train.shape[1]\n    num_classes = len(set(y_tr))\n\n    class MLP(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(input_dim, 128),\n                nn.ReLU(),\n                nn.Linear(128, num_classes),\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    class DVN(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(3, 32),\n                nn.ReLU(),\n                nn.Linear(32, 1),\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    main_model = MLP().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-3)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-3)\n    crit_main = nn.CrossEntropyLoss(reduction=\"none\")\n    crit_dvn = nn.MSELoss()\n\n    experiment_data[name] = {\n        \"val_loss\": [],\n        \"val_acc\": [],\n        \"corrs\": [],\n        \"N_meta_history\": [],\n    }\n    N_meta, prev_corr = 10, None\n    K_meta, epochs = 20, 3\n\n    for epoch in range(epochs):\n        main_model.train()\n        step = 0\n        for Xb, entb, yb in train_loader:\n            batch = {\"X\": Xb.to(device), \"ent\": entb.to(device), \"y\": yb.to(device)}\n            logits = main_model(batch[\"X\"])\n            loss_i = crit_main(logits, batch[\"y\"])\n            # representation norm feature\n            reps = main_model.net[1](main_model.net[0](batch[\"X\"]))\n            rep_norm = torch.norm(reps, dim=1, keepdim=True)\n            feats = torch.cat(\n                [loss_i.detach().unsqueeze(1), batch[\"ent\"], rep_norm], dim=1\n            )\n            weights = torch.softmax(dvn_model(feats).squeeze(1), dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n\n            if step % N_meta == 0:\n                main_model.eval()\n                with torch.no_grad():\n                    base_loss = nn.CrossEntropyLoss()(\n                        main_model(X_test), y_test_t\n                    ).item()\n                feats_list, contr_list = [], []\n                base_state = main_model.state_dict()\n                for idx in random.sample(range(len(X_train)), K_meta):\n                    xi = X_train[idx].unsqueeze(0).to(device)\n                    yi = y_train_t[idx].unsqueeze(0).to(device)\n                    with torch.no_grad():\n                        li = crit_main(main_model(xi), yi).item()\n                        ent_i = ent_train[idx].item()\n                        # compute rep norm for meta feature\n                        rep_i = main_model.net[1](main_model.net[0](xi))\n                        rep_norm_i = torch.norm(rep_i, dim=1).item()\n                    feats_list.append([li, ent_i, rep_norm_i])\n                    clone = MLP().to(device)\n                    clone.load_state_dict(base_state)\n                    opt_c = torch.optim.Adam(clone.parameters(), lr=1e-3)\n                    clone.train()\n                    lc = crit_main(clone(xi), yi).mean()\n                    opt_c.zero_grad()\n                    lc.backward()\n                    opt_c.step()\n                    clone.eval()\n                    with torch.no_grad():\n                        new_loss = nn.CrossEntropyLoss()(clone(X_test), y_test_t).item()\n                    contr_list.append([base_loss - new_loss])\n                feats_meta = torch.tensor(feats_list, dtype=torch.float32).to(device)\n                contr_meta = torch.tensor(contr_list, dtype=torch.float32).to(device)\n                for _ in range(5):\n                    dvn_model.train()\n                    p = dvn_model(feats_meta)\n                    dvn_loss = crit_dvn(p, contr_meta)\n                    optimizer_dvn.zero_grad()\n                    dvn_loss.backward()\n                    optimizer_dvn.step()\n                dvn_model.eval()\n                with torch.no_grad():\n                    preds = dvn_model(feats_meta).cpu().numpy().flatten()\n                true = contr_meta.cpu().numpy().flatten()\n                corr = spearmanr(preds, true).correlation\n                experiment_data[name][\"corrs\"].append(corr)\n                if prev_corr is not None:\n                    N_meta = (\n                        min(50, N_meta * 2) if corr > prev_corr else max(1, N_meta // 2)\n                    )\n                experiment_data[name][\"N_meta_history\"].append(N_meta)\n                prev_corr = corr\n                print(\n                    f\"[{name}] Step {step}: Spearman Corr={corr:.4f}, N_meta={N_meta}\"\n                )\n                main_model.train()\n            step += 1\n\n        main_model.eval()\n        with torch.no_grad():\n            logits_val = main_model(X_test)\n            val_loss = nn.CrossEntropyLoss()(logits_val, y_test_t).item()\n            acc = (logits_val.argmax(1) == y_test_t).float().mean().item()\n        experiment_data[name][\"val_loss\"].append(val_loss)\n        experiment_data[name][\"val_acc\"].append(acc)\n        print(f\"[{name}] Epoch {epoch}: val_loss={val_loss:.4f}, val_acc={acc:.4f}\")\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom datasets import load_dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import spearmanr\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nhf_datasets = {\"ag_news\": \"ag_news\", \"yelp\": \"yelp_polarity\", \"dbpedia\": \"dbpedia_14\"}\nexperiment_data = {}\n\nfor name, hf_name in hf_datasets.items():\n    ds = load_dataset(hf_name)\n    train = ds[\"train\"].shuffle(seed=42).select(range(1000))\n    test = ds[\"test\"].shuffle(seed=42).select(range(200))\n    text_col = \"text\" if \"text\" in train.column_names else \"content\"\n    tr_txt, te_txt = train[text_col], test[text_col]\n    y_tr, y_te = train[\"label\"], test[\"label\"]\n\n    tfidf = TfidfVectorizer(max_features=500, norm=\"l2\")\n    tfidf.fit(tr_txt + te_txt)\n    X_tr_np = tfidf.transform(tr_txt).toarray()\n    X_te_np = tfidf.transform(te_txt).toarray()\n    ent_tr = -np.sum(X_tr_np * np.log(X_tr_np + 1e-10), axis=1, keepdims=True)\n\n    X_train = torch.tensor(X_tr_np, dtype=torch.float32)\n    ent_train = torch.tensor(ent_tr, dtype=torch.float32)\n    y_train_t = torch.tensor(y_tr, dtype=torch.long)\n    X_test = torch.tensor(X_te_np, dtype=torch.float32).to(device)\n    y_test_t = torch.tensor(y_te, dtype=torch.long).to(device)\n\n    train_ds = TensorDataset(X_train, ent_train, y_train_t)\n    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n\n    input_dim = X_train.shape[1]\n    num_classes = len(set(y_tr))\n\n    class MLP(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(input_dim, 128),\n                nn.ReLU(),\n                nn.Linear(128, num_classes),\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    class DVN(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(3, 32),\n                nn.ReLU(),\n                nn.Linear(32, 1),\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    main_model = MLP().to(device)\n    dvn_model = DVN().to(device)\n    optimizer_main = torch.optim.Adam(main_model.parameters(), lr=1e-3)\n    optimizer_dvn = torch.optim.Adam(dvn_model.parameters(), lr=1e-3)\n    crit_main = nn.CrossEntropyLoss(reduction=\"none\")\n    crit_dvn = nn.MSELoss()\n\n    experiment_data[name] = {\n        \"val_loss\": [],\n        \"val_acc\": [],\n        \"corrs\": [],\n        \"N_meta_history\": [],\n    }\n    N_meta, prev_corr = 10, None\n    K_meta, epochs = 20, 3\n\n    for epoch in range(epochs):\n        main_model.train()\n        step = 0\n        for Xb, entb, yb in train_loader:\n            batch = {\"X\": Xb.to(device), \"ent\": entb.to(device), \"y\": yb.to(device)}\n            logits = main_model(batch[\"X\"])\n            loss_i = crit_main(logits, batch[\"y\"])\n            # representation norm feature\n            reps = main_model.net[1](main_model.net[0](batch[\"X\"]))\n            rep_norm = torch.norm(reps, dim=1, keepdim=True)\n            feats = torch.cat(\n                [loss_i.detach().unsqueeze(1), batch[\"ent\"], rep_norm], dim=1\n            )\n            weights = torch.softmax(dvn_model(feats).squeeze(1), dim=0)\n            loss = (weights * loss_i).sum()\n            optimizer_main.zero_grad()\n            loss.backward()\n            optimizer_main.step()\n\n            if step % N_meta == 0:\n                main_model.eval()\n                with torch.no_grad():\n                    base_loss = nn.CrossEntropyLoss()(\n                        main_model(X_test), y_test_t\n                    ).item()\n                feats_list, contr_list = [], []\n                base_state = main_model.state_dict()\n                for idx in random.sample(range(len(X_train)), K_meta):\n                    xi = X_train[idx].unsqueeze(0).to(device)\n                    yi = y_train_t[idx].unsqueeze(0).to(device)\n                    with torch.no_grad():\n                        li = crit_main(main_model(xi), yi).item()\n                        ent_i = ent_train[idx].item()\n                        # compute rep norm for meta feature\n                        rep_i = main_model.net[1](main_model.net[0](xi))\n                        rep_norm_i = torch.norm(rep_i, dim=1).item()\n                    feats_list.append([li, ent_i, rep_norm_i])\n                    clone = MLP().to(device)\n                    clone.load_state_dict(base_state)\n                    opt_c = torch.optim.Adam(clone.parameters(), lr=1e-3)\n                    clone.train()\n                    lc = crit_main(clone(xi), yi).mean()\n                    opt_c.zero_grad()\n                    lc.backward()\n                    opt_c.step()\n                    clone.eval()\n                    with torch.no_grad():\n                        new_loss = nn.CrossEntropyLoss()(clone(X_test), y_test_t).item()\n                    contr_list.append([base_loss - new_loss])\n                feats_meta = torch.tensor(feats_list, dtype=torch.float32).to(device)\n                contr_meta = torch.tensor(contr_list, dtype=torch.float32).to(device)\n                for _ in range(5):\n                    dvn_model.train()\n                    p = dvn_model(feats_meta)\n                    dvn_loss = crit_dvn(p, contr_meta)\n                    optimizer_dvn.zero_grad()\n                    dvn_loss.backward()\n                    optimizer_dvn.step()\n                dvn_model.eval()\n                with torch.no_grad():\n                    preds = dvn_model(feats_meta).cpu().numpy().flatten()\n                true = contr_meta.cpu().numpy().flatten()\n                corr = spearmanr(preds, true).correlation\n                experiment_data[name][\"corrs\"].append(corr)\n                if prev_corr is not None:\n                    N_meta = (\n                        min(50, N_meta * 2) if corr > prev_corr else max(1, N_meta // 2)\n                    )\n                experiment_data[name][\"N_meta_history\"].append(N_meta)\n                prev_corr = corr\n                print(\n                    f\"[{name}] Step {step}: Spearman Corr={corr:.4f}, N_meta={N_meta}\"\n                )\n                main_model.train()\n            step += 1\n\n        main_model.eval()\n        with torch.no_grad():\n            logits_val = main_model(X_test)\n            val_loss = nn.CrossEntropyLoss()(logits_val, y_test_t).item()\n            acc = (logits_val.argmax(1) == y_test_t).float().mean().item()\n        experiment_data[name][\"val_loss\"].append(val_loss)\n        experiment_data[name][\"val_acc\"].append(acc)\n        print(f\"[{name}] Epoch {epoch}: val_loss={val_loss:.4f}, val_acc={acc:.4f}\")\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', '[E=5] Epoch 0: train_loss=0.1885,\nval_loss=0.1165', '\\n', '[E=5] Epoch 0: Spearman Corr=-0.5564', '\\n', '[E=5]\nEpoch 1: train_loss=0.1002, val_loss=0.0612', '\\n', '[E=5] Epoch 1: Spearman\nCorr=0.2241', '\\n', '[E=5] Epoch 2: train_loss=0.0552, val_loss=0.0397', '\\n',\n'[E=5] Epoch 2: Spearman Corr=-0.2902', '\\n', '[E=5] Epoch 3: train_loss=0.0329,\nval_loss=0.0210', '\\n', '[E=5] Epoch 3: Spearman Corr=0.1113', '\\n', '[E=5]\nEpoch 4: train_loss=0.0218, val_loss=0.0191', '\\n', '[E=5] Epoch 4: Spearman\nCorr=0.1654', '\\n', '[E=20] Epoch 0: train_loss=0.2307, val_loss=0.1476', '\\n',\n'[E=20] Epoch 0: Spearman Corr=-0.6075', '\\n', '[E=20] Epoch 1:\ntrain_loss=0.1226, val_loss=0.0923', '\\n', '[E=20] Epoch 1: Spearman\nCorr=-0.2256', '\\n', '[E=20] Epoch 2: train_loss=0.0852, val_loss=0.0554', '\\n',\n'[E=20] Epoch 2: Spearman Corr=0.0241', '\\n', '[E=20] Epoch 3:\ntrain_loss=0.0535, val_loss=0.0392', '\\n', '[E=20] Epoch 3: Spearman\nCorr=0.2782', '\\n', '[E=20] Epoch 4: train_loss=0.0367, val_loss=0.0239', '\\n',\n'[E=20] Epoch 4: Spearman Corr=0.0617', '\\n', '[E=20] Epoch 5:\ntrain_loss=0.0244, val_loss=0.0220', '\\n', '[E=20] Epoch 5: Spearman\nCorr=0.1609', '\\n', '[E=20] Epoch 6: train_loss=0.0210, val_loss=0.0184', '\\n',\n'[E=20] Epoch 6: Spearman Corr=0.3895', '\\n', '[E=20] Epoch 7:\ntrain_loss=0.0183, val_loss=0.0153', '\\n', '[E=20] Epoch 7: Spearman\nCorr=0.2361', '\\n', '[E=20] Epoch 8: train_loss=0.0159, val_loss=0.0148', '\\n',\n'[E=20] Epoch 8: Spearman Corr=0.1098', '\\n', '[E=20] Epoch 9:\ntrain_loss=0.0141, val_loss=0.0131', '\\n', '[E=20] Epoch 9: Spearman\nCorr=-0.0571', '\\n', '[E=20] Epoch 10: train_loss=0.0136, val_loss=0.0122',\n'\\n', '[E=20] Epoch 10: Spearman Corr=0.2195', '\\n', '[E=20] Epoch 11:\ntrain_loss=0.0138, val_loss=0.0133', '\\n', '[E=20] Epoch 11: Spearman\nCorr=0.2617', '\\n', '[E=20] Epoch 12: train_loss=0.0130, val_loss=0.0118', '\\n',\n'[E=20] Epoch 12: Spearman Corr=-0.1143', '\\n', '[E=20] Epoch 13:\ntrain_loss=0.0127, val_loss=0.0121', '\\n', '[E=20] Epoch 13: Spearman\nCorr=-0.2045', '\\n', '[E=20] Epoch 14: train_loss=0.0125, val_loss=0.0108',\n'\\n', '[E=20] Epoch 14: Spearman Corr=0.0135', '\\n', '[E=20] Epoch 15:\ntrain_loss=0.0121, val_loss=0.0119', '\\n', '[E=20] Epoch 15: Spearman\nCorr=0.1038', '\\n', '[E=20] Epoch 16: train_loss=0.0127, val_loss=0.0115', '\\n',\n'[E=20] Epoch 16: Spearman Corr=-0.0211', '\\n', '[E=20] Epoch 17:\ntrain_loss=0.0120, val_loss=0.0108', '\\n', '[E=20] Epoch 17: Spearman\nCorr=-0.1955', '\\n', '[E=20] Epoch 18: train_loss=0.0116, val_loss=0.0107',\n'\\n', '[E=20] Epoch 18: Spearman Corr=-0.0632', '\\n', '[E=20] Epoch 19:\ntrain_loss=0.0116, val_loss=0.0105', '\\n', '[E=20] Epoch 19: Spearman\nCorr=0.1684', '\\n', '[E=50] Epoch 0: train_loss=0.2938, val_loss=0.1773', '\\n',\n'[E=50] Epoch 0: Spearman Corr=0.1053', '\\n', '[E=50] Epoch 1:\ntrain_loss=0.1909, val_loss=0.1536', '\\n', '[E=50] Epoch 1: Spearman\nCorr=0.3308', '\\n', '[E=50] Epoch 2: train_loss=0.1530, val_loss=0.1134', '\\n',\n'[E=50] Epoch 2: Spearman Corr=0.0632', '\\n', '[E=50] Epoch 3:\ntrain_loss=0.1310, val_loss=0.0994', '\\n', '[E=50] Epoch 3: Spearman\nCorr=-0.0662', '\\n', '[E=50] Epoch 4: train_loss=0.1120, val_loss=0.0888', '\\n',\n'[E=50] Epoch 4: Spearman Corr=-0.3925', '\\n', '[E=50] Epoch 5:\ntrain_loss=0.0928, val_loss=0.0687', '\\n', '[E=50] Epoch 5: Spearman\nCorr=0.3910', '\\n', '[E=50] Epoch 6: train_loss=0.0733, val_loss=0.0530', '\\n',\n'[E=50] Epoch 6: Spearman Corr=0.2015', '\\n', '[E=50] Epoch 7:\ntrain_loss=0.0589, val_loss=0.0414', '\\n', '[E=50] Epoch 7: Spearman\nCorr=0.1278', '\\n', '[E=50] Epoch 8: train_loss=0.0434, val_loss=0.0324', '\\n',\n'[E=50] Epoch 8: Spearman Corr=0.1534', '\\n', '[E=50] Epoch 9:\ntrain_loss=0.0358, val_loss=0.0274', '\\n', '[E=50] Epoch 9: Spearman\nCorr=0.5113', '\\n', '[E=50] Epoch 10: train_loss=0.0284, val_loss=0.0236', '\\n',\n'[E=50] Epoch 10: Spearman Corr=-0.0120', '\\n', '[E=50] Epoch 11:\ntrain_loss=0.0236, val_loss=0.0190', '\\n', '[E=50] Epoch 11: Spearman\nCorr=-0.1910', '\\n', '[E=50] Epoch 12: train_loss=0.0202, val_loss=0.0193',\n'\\n', '[E=50] Epoch 12: Spearman Corr=0.0286', '\\n', '[E=50] Epoch 13:\ntrain_loss=0.0180, val_loss=0.0155', '\\n', '[E=50] Epoch 13: Spearman\nCorr=-0.2842', '\\n', '[E=50] Epoch 14: train_loss=0.0181, val_loss=0.0152',\n'\\n', '[E=50] Epoch 14: Spearman Corr=0.3835', '\\n', '[E=50] Epoch 15:\ntrain_loss=0.0164, val_loss=0.0134', '\\n', '[E=50] Epoch 15: Spearman\nCorr=0.0346', '\\n', '[E=50] Epoch 16: train_loss=0.0147, val_loss=0.0139', '\\n',\n'[E=50] Epoch 16: Spearman Corr=0.4872', '\\n', '[E=50] Epoch 17:\ntrain_loss=0.0146, val_loss=0.0140', '\\n', '[E=50] Epoch 17: Spearman\nCorr=0.0075', '\\n', '[E=50] Epoch 18: train_loss=0.0136, val_loss=0.0136', '\\n',\n'[E=50] Epoch 18: Spearman Corr=-0.1128', '\\n', '[E=50] Epoch 19:\ntrain_loss=0.0142, val_loss=0.0131', '\\n', '[E=50] Epoch 19: Spearman\nCorr=0.3368', '\\n', '[E=50] Epoch 20: train_loss=0.0134, val_loss=0.0115', '\\n',\n'[E=50] Epoch 20: Spearman Corr=0.1263', '\\n', '[E=50] Epoch 21:\ntrain_loss=0.0122, val_loss=0.0109', '\\n', '[E=50] Epoch 21: Spearman\nCorr=-0.4376', '\\n', '[E=50] Epoch 22: train_loss=0.0121, val_loss=0.0112',\n'\\n', '[E=50] Epoch 22: Spearman Corr=-0.2737', '\\n', '[E=50] Epoch 23:\ntrain_loss=0.0131, val_loss=0.0120', '\\n', '[E=50] Epoch 23: Spearman\nCorr=-0.2737', '\\n', '[E=50] Epoch 24: train_loss=0.0120, val_loss=0.0105',\n'\\n', '[E=50] Epoch 24: Spearman Corr=-0.1835', '\\n', '[E=50] Epoch 25:\ntrain_loss=0.0118, val_loss=0.0104', '\\n', '[E=50] Epoch 25: Spearman\nCorr=-0.1925', '\\n', '[E=50] Epoch 26: train_loss=0.0116, val_loss=0.0101',\n'\\n', '[E=50] Epoch 26: Spearman Corr=-0.2917', '\\n', '[E=50] Epoch 27:\ntrain_loss=0.0114, val_loss=0.0118', '\\n', '[E=50] Epoch 27: Spearman\nCorr=-0.0827', '\\n', '[E=50] Epoch 28: train_loss=0.0115, val_loss=0.0109',\n'\\n', '[E=50] Epoch 28: Spearman Corr=-0.2947', '\\n', '[E=50] Epoch 29:\ntrain_loss=0.0121, val_loss=0.0120', '\\n', '[E=50] Epoch 29: Spearman\nCorr=0.0256', '\\n', '[E=50] Epoch 30: train_loss=0.0119, val_loss=0.0128', '\\n',\n'[E=50] Epoch 30: Spearman Corr=0.1895', '\\n', '[E=50] Epoch 31:\ntrain_loss=0.0126, val_loss=0.0113', '\\n', '[E=50] Epoch 31: Spearman\nCorr=-0.0180', '\\n', '[E=50] Epoch 32: train_loss=0.0115, val_loss=0.0101',\n'\\n', '[E=50] Epoch 32: Spearman Corr=-0.0632', '\\n', '[E=50] Epoch 33:\ntrain_loss=0.0112, val_loss=0.0103', '\\n', '[E=50] Epoch 33: Spearman\nCorr=-0.0917', '\\n', '[E=50] Epoch 34: train_loss=0.0110, val_loss=0.0104',\n'\\n', '[E=50] Epoch 34: Spearman Corr=0.4436', '\\n', '[E=50] Epoch 35:\ntrain_loss=0.0120, val_loss=0.0112', '\\n', '[E=50] Epoch 35: Spearman\nCorr=0.2541', '\\n', '[E=50] Epoch 36: train_loss=0.0117, val_loss=0.0101', '\\n',\n'[E=50] Epoch 36: Spearman Corr=0.0526', '\\n', '[E=50] Epoch 37:\ntrain_loss=0.0117, val_loss=0.0134', '\\n', '[E=50] Epoch 37: Spearman\nCorr=0.0421', '\\n', '[E=50] Epoch 38: train_loss=0.0119, val_loss=0.0111', '\\n',\n'[E=50] Epoch 38: Spearman Corr=0.3955', '\\n', '[E=50] Epoch 39:\ntrain_loss=0.0116, val_loss=0.0133', '\\n', '[E=50] Epoch 39: Spearman\nCorr=-0.0090', '\\n', '[E=50] Epoch 40: train_loss=0.0114, val_loss=0.0099',\n'\\n', '[E=50] Epoch 40: Spearman Corr=-0.0045', '\\n', '[E=50] Epoch 41:\ntrain_loss=0.0110, val_loss=0.0097', '\\n', '[E=50] Epoch 41: Spearman\nCorr=0.0346', '\\n', '[E=50] Epoch 42: train_loss=0.0121, val_loss=0.0128', '\\n',\n'[E=50] Epoch 42: Spearman Corr=0.3158', '\\n', '[E=50] Epoch 43:\ntrain_loss=0.0114, val_loss=0.0101', '\\n', '[E=50] Epoch 43: Spearman\nCorr=-0.0556', '\\n', '[E=50] Epoch 44: train_loss=0.0107, val_loss=0.0102',\n'\\n', '[E=50] Epoch 44: Spearman Corr=-0.0226', '\\n', '[E=50] Epoch 45:\ntrain_loss=0.0113, val_loss=0.0098', '\\n', '[E=50] Epoch 45: Spearman\nCorr=-0.1308', '\\n', '[E=50] Epoch 46: train_loss=0.0113, val_loss=0.0098',\n'\\n', '[E=50] Epoch 46: Spearman Corr=0.3805', '\\n', '[E=50] Epoch 47:\ntrain_loss=0.0114, val_loss=0.0103', '\\n', '[E=50] Epoch 47: Spearman\nCorr=-0.0692', '\\n', '[E=50] Epoch 48: train_loss=0.0122, val_loss=0.0132',\n'\\n', '[E=50] Epoch 48: Spearman Corr=-0.1684', '\\n', '[E=50] Epoch 49:\ntrain_loss=0.0113, val_loss=0.0100', '\\n', '[E=50] Epoch 49: Spearman\nCorr=0.3038', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 7\nseconds seconds (time limit is an hour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 9, in <module>\\n\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\\nImportError:\ncannot import name \\'AdamW\\' from \\'transformers\\'\n(/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/__init__.py)\\n', 'Execution time: 2 seconds seconds (time\nlimit is an hour).']", "['\\rMap:   0%|          | 0/1000 [00:00<?, ? examples/s]', '\\rMap:\n100%|##########| 1000/1000 [00:00<00:00, 1675.18 examples/s]', '', '\\rMap:\n100%|##########| 1000/1000 [00:00<00:00, 1666.13 examples/s]', '\\n', '\\rMap:\n0%|          | 0/1000 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########|\n1000/1000 [00:00<00:00, 14928.47 examples/s]', '\\n', '\\rMap:   0%|          |\n0/1000 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 1000/1000\n[00:00<00:00, 10817.14 examples/s]', '\\n', 'Using device: cuda', '\\n', 'Epoch 0:\nvalidation_loss = 1.8912', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 112, in <module>\\n    sample = train_dataset[idx]\\n\n~~~~~~~~~~~~~^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/arrow_dataset.py\", line 2777, in __getitem__\\n    return\nself._getitem(key)\\n           ^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/arrow_dataset.py\", line 2761, in _getitem\\n    pa_subtable =\nquery_table(self._data, key, indices=self._indices)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/formatting/formatting.py\", line 607, in query_table\\n\n_check_valid_index_key(key, size)\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/formatting/formatting.py\", line 556, in\n_check_valid_index_key\\n    if len(key) > 0:\\n       ^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/_tensor.py\", line 1087, in __len__\\n    raise TypeError(\"len() of\na 0-d tensor\")\\nTypeError: len() of a 0-d tensor\\n', 'Execution time: 36 seconds\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', '[ag_news] Step 0: Spearman Corr=-0.3564', '\\n',\n'[ag_news] Step 10: Spearman Corr=-0.0602', '\\n', '[ag_news] Epoch 0:\nval_loss=1.3676, val_acc=0.5400', '\\n', '[ag_news] Step 0: Spearman\nCorr=0.0752', '\\n', '[ag_news] Step 10: Spearman Corr=-0.2872', '\\n', '[ag_news]\nEpoch 1: val_loss=1.3330, val_acc=0.4950', '\\n', '[ag_news] Step 0: Spearman\nCorr=-0.4421', '\\n', '[ag_news] Step 10: Spearman Corr=-0.2932', '\\n',\n'[ag_news] Epoch 2: val_loss=1.2654, val_acc=0.5800', '\\n', '[yelp] Step 0:\nSpearman Corr=0.2602', '\\n', '[yelp] Step 10: Spearman Corr=0.0256', '\\n',\n'[yelp] Epoch 0: val_loss=0.6731, val_acc=0.7750', '\\n', '[yelp] Step 0:\nSpearman Corr=0.2301', '\\n', '[yelp] Step 10: Spearman Corr=-0.4977', '\\n',\n'[yelp] Epoch 1: val_loss=0.6262, val_acc=0.8150', '\\n', '[yelp] Step 0:\nSpearman Corr=-0.3308', '\\n', '[yelp] Step 10: Spearman Corr=-0.4075', '\\n',\n'[yelp] Epoch 2: val_loss=0.5525, val_acc=0.8150', '\\n', '[dbpedia] Step 0:\nSpearman Corr=-0.1218', '\\n', '[dbpedia] Step 10: Spearman Corr=0.0910', '\\n',\n'[dbpedia] Epoch 0: val_loss=2.6003, val_acc=0.3500', '\\n', '[dbpedia] Step 0:\nSpearman Corr=-0.1083', '\\n', '[dbpedia] Step 10: Spearman Corr=0.0511', '\\n',\n'[dbpedia] Epoch 1: val_loss=2.5306, val_acc=0.6350', '\\n', '[dbpedia] Step 0:\nSpearman Corr=-0.3233', '\\n', '[dbpedia] Step 10: Spearman Corr=0.0932', '\\n',\n'[dbpedia] Epoch 2: val_loss=2.4078, val_acc=0.7250', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 28 seconds seconds (time limit is\nan hour).']", "['Using device: cuda', '\\n', 'Epoch 0: Pre-training DVN Spearman Corr=-0.0566',\n'\\n', 'Traceback (most recent call last):\\n  File \"runfile.py\", line 165, in\n<module>\\n    loss.backward()\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/_tensor.py\", line 581, in backward\\n\ntorch.autograd.backward(\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/autograd/__init__.py\", line 347, in backward\\n\n_engine_run_backward(\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/autograd/graph.py\", line 825, in _engine_run_backward\\n    return\nVariable._execution_engine.run_backward(  # Calls into the C++ engine to run the\nbackward pass\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: Trying to backward\nthrough the graph a second time (or directly access saved tensors after they\nhave already been freed). Saved intermediate values of the graph are freed when\nyou call .backward() or autograd.grad(). Specify retain_graph=True if you need\nto backward through the graph a second time or if you need to access saved\ntensors after calling backward.\\n', 'Execution time: 11 seconds seconds (time\nlimit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 131, in <module>\\n    p = dvn_model(feats_meta)\\n\n^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"runfile.py\", line 62, in forward\\n    return self.net(x)\\n\n^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/container.py\", line 250, in forward\\n    input =\nmodule(input)\\n            ^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/linear.py\", line 125, in forward\\n    return\nF.linear(input, self.weight, self.bias)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: mat1 and mat2 shapes\ncannot be multiplied (20x2 and 3x32)\\n', 'Execution time: 11 seconds seconds\n(time limit is an hour).']", "['Using device: cuda', '\\n', '[ag_news] Step 0: Spearman Corr=0.6256', '\\n',\n'[ag_news] Step 10: Spearman Corr=0.6408', '\\n', 'Epoch 0: validation_loss =\n1.3646', '\\n', '[ag_news] Step 0: Spearman Corr=0.5774', '\\n', '[ag_news] Step\n10: Spearman Corr=0.3293', '\\n', 'Epoch 1: validation_loss = 1.3237', '\\n',\n'[ag_news] Step 0: Spearman Corr=0.1579', '\\n', '[ag_news] Step 5: Spearman\nCorr=0.2406', '\\n', '[ag_news] Step 10: Spearman Corr=0.0617', '\\n', '[ag_news]\nStep 12: Spearman Corr=0.0737', '\\n', '[ag_news] Step 13: Spearman Corr=0.0075',\n'\\n', '[ag_news] Step 14: Spearman Corr=-0.0406', '\\n', '[ag_news] Step 15:\nSpearman Corr=-0.2030', '\\n', 'Epoch 2: validation_loss = 1.2470', '\\n', '[yelp]\nStep 0: Spearman Corr=0.2211', '\\n', '[yelp] Step 10: Spearman Corr=-0.2526',\n'\\n', '[yelp] Step 15: Spearman Corr=0.3850', '\\n', 'Epoch 0: validation_loss =\n0.6735', '\\n', '[yelp] Step 0: Spearman Corr=0.4150', '\\n', '[yelp] Step 5:\nSpearman Corr=0.4571', '\\n', '[yelp] Step 10: Spearman Corr=0.2000', '\\n',\n'[yelp] Step 12: Spearman Corr=0.5233', '\\n', '[yelp] Step 14: Spearman\nCorr=0.4135', '\\n', 'Epoch 1: validation_loss = 0.6234', '\\n', '[yelp] Step 0:\nSpearman Corr=0.6632', '\\n', '[yelp] Step 2: Spearman Corr=0.8150', '\\n',\n'[yelp] Step 4: Spearman Corr=0.7744', '\\n', '[yelp] Step 8: Spearman\nCorr=0.3338', '\\n', '[yelp] Step 12: Spearman Corr=0.4030', '\\n', 'Epoch 2:\nvalidation_loss = 0.5444', '\\n', '[dbpedia] Step 0: Spearman Corr=-0.2361',\n'\\n', '[dbpedia] Step 5: Spearman Corr=-0.5053', '\\n', '[dbpedia] Step 6:\nSpearman Corr=-0.2571', '\\n', '[dbpedia] Step 7: Spearman Corr=0.1068', '\\n',\n'[dbpedia] Step 8: Spearman Corr=-0.7158', '\\n', '[dbpedia] Step 9: Spearman\nCorr=-0.5970', '\\n', '[dbpedia] Step 10: Spearman Corr=0.0647', '\\n', '[dbpedia]\nStep 11: Spearman Corr=-0.1444', '\\n', '[dbpedia] Step 12: Spearman\nCorr=-0.3098', '\\n', '[dbpedia] Step 13: Spearman Corr=0.0466', '\\n', '[dbpedia]\nStep 14: Spearman Corr=-0.0180', '\\n', '[dbpedia] Step 15: Spearman\nCorr=-0.1820', '\\n', 'Epoch 0: validation_loss = 2.6022', '\\n', '[dbpedia] Step\n0: Spearman Corr=-0.0887', '\\n', '[dbpedia] Step 1: Spearman Corr=0.1068', '\\n',\n'[dbpedia] Step 2: Spearman Corr=-0.3684', '\\n', '[dbpedia] Step 3: Spearman\nCorr=-0.1278', '\\n', '[dbpedia] Step 4: Spearman Corr=0.0947', '\\n', '[dbpedia]\nStep 5: Spearman Corr=0.0767', '\\n', '[dbpedia] Step 6: Spearman Corr=-0.2767',\n'\\n', '[dbpedia] Step 7: Spearman Corr=-0.3880', '\\n', '[dbpedia] Step 8:\nSpearman Corr=0.3053', '\\n', '[dbpedia] Step 9: Spearman Corr=-0.1940', '\\n',\n'[dbpedia] Step 10: Spearman Corr=-0.4526', '\\n', '[dbpedia] Step 11: Spearman\nCorr=-0.5519', '\\n', '[dbpedia] Step 12: Spearman Corr=0.2211', '\\n', '[dbpedia]\nStep 13: Spearman Corr=-0.1008', '\\n', '[dbpedia] Step 14: Spearman\nCorr=-0.1489', '\\n', '[dbpedia] Step 15: Spearman Corr=-0.3212', '\\n', 'Epoch 1:\nvalidation_loss = 2.5235', '\\n', '[dbpedia] Step 0: Spearman Corr=-0.5203',\n'\\n', '[dbpedia] Step 1: Spearman Corr=-0.1308', '\\n', '[dbpedia] Step 2:\nSpearman Corr=-0.2346', '\\n', '[dbpedia] Step 3: Spearman Corr=-0.2842', '\\n',\n'[dbpedia] Step 4: Spearman Corr=-0.2992', '\\n', '[dbpedia] Step 5: Spearman\nCorr=-0.3128', '\\n', '[dbpedia] Step 6: Spearman Corr=-0.5519', '\\n', '[dbpedia]\nStep 7: Spearman Corr=-0.4045', '\\n', '[dbpedia] Step 8: Spearman Corr=0.0857',\n'\\n', '[dbpedia] Step 9: Spearman Corr=-0.2632', '\\n', '[dbpedia] Step 10:\nSpearman Corr=-0.0962', '\\n', '[dbpedia] Step 11: Spearman Corr=-0.6346', '\\n',\n'[dbpedia] Step 12: Spearman Corr=-0.4872', '\\n', '[dbpedia] Step 13: Spearman\nCorr=-0.4015', '\\n', '[dbpedia] Step 14: Spearman Corr=-0.3684', '\\n',\n'[dbpedia] Step 15: Spearman Corr=0.0030', '\\n', 'Epoch 2: validation_loss =\n2.3937', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 30 seconds\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 1.3646, Spearman Corr =\n-0.2121', '\\n', 'Epoch 1: validation_loss = 1.3235, Spearman Corr = 0.0606',\n'\\n', 'Epoch 2: validation_loss = 1.2501, Spearman Corr = 0.2606', '\\n', 'Epoch\n3: validation_loss = 1.1395, Spearman Corr = 0.2061', '\\n', 'Epoch 4:\nvalidation_loss = 1.0158, Spearman Corr = 0.0818', '\\n', 'Epoch 0:\nvalidation_loss = 0.6747, Spearman Corr = 0.0727', '\\n', 'Yelp fairness\ndisparity = 0.8500', '\\n', 'Epoch 1: validation_loss = 0.6417, Spearman Corr =\n-0.2212', '\\n', 'Yelp fairness disparity = 0.7500', '\\n', 'Epoch 2:\nvalidation_loss = 0.5746, Spearman Corr = -0.1333', '\\n', 'Yelp fairness\ndisparity = 0.1300', '\\n', 'Epoch 3: validation_loss = 0.4971, Spearman Corr =\n-0.2182', '\\n', 'Yelp fairness disparity = 0.0200', '\\n', 'Epoch 4:\nvalidation_loss = 0.4314, Spearman Corr = -0.1667', '\\n', 'Yelp fairness\ndisparity = 0.0000', '\\n', 'Epoch 0: validation_loss = 2.5970, Spearman Corr =\n0.1111', '\\n', 'Epoch 1: validation_loss = 2.5254, Spearman Corr = -0.1667',\n'\\n', 'Epoch 2: validation_loss = 2.3985, Spearman Corr = -0.1182', '\\n', 'Epoch\n3: validation_loss = 2.2203, Spearman Corr = -0.6303', '\\n', 'Epoch 4:\nvalidation_loss = 1.9922, Spearman Corr = -0.3970', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 28 seconds seconds (time limit is\nan hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 112, in <module>\\n\ntorch.autograd.grad(crit_main(main_model(xi), yi).sum(), xi)[0]\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/autograd/__init__.py\", line 496, in grad\\n    result =\n_engine_run_backward(\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/autograd/graph.py\", line 825, in _engine_run_backward\\n    return\nVariable._execution_engine.run_backward(  # Calls into the C++ engine to run the\nbackward pass\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: One of the\ndifferentiated Tensors does not require grad\\n', 'Execution time: 10 seconds\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', '[ag_news] Step 0: Spearman Corr=-0.4331,\nN_meta=10', '\\n', '[ag_news] Step 10: Spearman Corr=0.1850, N_meta=20', '\\n',\n'[ag_news] Epoch 0: val_loss=1.3645, val_acc=0.5400', '\\n', '[ag_news] Step 0:\nSpearman Corr=0.1519, N_meta=10', '\\n', '[ag_news] Step 10: Spearman\nCorr=-0.0767, N_meta=5', '\\n', '[ag_news] Step 15: Spearman Corr=0.0647,\nN_meta=10', '\\n', '[ag_news] Epoch 1: val_loss=1.3231, val_acc=0.7000', '\\n',\n'[ag_news] Step 0: Spearman Corr=0.4301, N_meta=20', '\\n', '[ag_news] Epoch 2:\nval_loss=1.2479, val_acc=0.7000', '\\n', '[yelp] Step 0: Spearman Corr=0.2211,\nN_meta=10', '\\n', '[yelp] Step 10: Spearman Corr=-0.2105, N_meta=5', '\\n',\n'[yelp] Step 15: Spearman Corr=-0.3188, N_meta=2', '\\n', '[yelp] Epoch 0:\nval_loss=0.6676, val_acc=0.6450', '\\n', '[yelp] Step 0: Spearman Corr=0.2361,\nN_meta=4', '\\n', '[yelp] Step 4: Spearman Corr=0.2647, N_meta=8', '\\n', '[yelp]\nStep 8: Spearman Corr=0.1474, N_meta=4', '\\n', '[yelp] Step 12: Spearman\nCorr=0.4226, N_meta=8', '\\n', '[yelp] Epoch 1: val_loss=0.6186, val_acc=0.7950',\n'\\n', '[yelp] Step 0: Spearman Corr=0.3805, N_meta=4', '\\n', '[yelp] Step 4:\nSpearman Corr=0.2827, N_meta=2', '\\n', '[yelp] Step 6: Spearman Corr=0.3789,\nN_meta=4', '\\n', '[yelp] Step 8: Spearman Corr=0.1925, N_meta=2', '\\n', '[yelp]\nStep 10: Spearman Corr=0.3865, N_meta=4', '\\n', '[yelp] Step 12: Spearman\nCorr=0.4286, N_meta=8', '\\n', '[yelp] Epoch 2: val_loss=0.5452, val_acc=0.8350',\n'\\n', '[dbpedia] Step 0: Spearman Corr=-0.1669, N_meta=10', '\\n', '[dbpedia]\nStep 10: Spearman Corr=-0.0752, N_meta=20', '\\n', '[dbpedia] Epoch 0:\nval_loss=2.6020, val_acc=0.3300', '\\n', '[dbpedia] Step 0: Spearman Corr=0.5233,\nN_meta=40', '\\n', '[dbpedia] Epoch 1: val_loss=2.5291, val_acc=0.6300', '\\n',\n'[dbpedia] Step 0: Spearman Corr=0.1038, N_meta=20', '\\n', '[dbpedia] Epoch 2:\nval_loss=2.4056, val_acc=0.6250', '\\n', 'Saved experiment_data.npy', '\\n',\n'Execution time: 32 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '*** Dataset: wikitext2', '\\n', '`loss_type=None`\nwas set in the config but it is unrecognised.Using the default loss:\n`ForCausalLMLoss`.\\n', 'Dataset wikitext2 Epoch 0: validation_loss = 3.8086',\n'\\n', 'Dataset wikitext2 Epoch 0: Spearman Corr=-0.1263', '\\n', 'Dataset\nwikitext2 Epoch 1: validation_loss = 3.6474', '\\n', 'Dataset wikitext2 Epoch 1:\nSpearman Corr=-0.5308', '\\n', 'Dataset wikitext2 Epoch 2: validation_loss =\n3.5265', '\\n', 'Dataset wikitext2 Epoch 2: Spearman Corr=0.3549', '\\n', '***\nDataset: bookcorpus', '\\n', 'Dataset bookcorpus Epoch 0: validation_loss =\n3.2902', '\\n', 'Dataset bookcorpus Epoch 0: Spearman Corr=-0.3624', '\\n',\n'Traceback (most recent call last):\\n  File \"runfile.py\", line 114, in\n<module>\\n    out = model(\\n          ^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/models/gpt2/modeling_gpt2.py\", line 1210, in forward\\n\ntransformer_outputs = self.transformer(\\n\n^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/models/gpt2/modeling_gpt2.py\", line 939, in forward\\n\noutputs = block(\\n              ^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/utils/deprecation.py\", line 172, in wrapped_func\\n\nreturn func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/models/gpt2/modeling_gpt2.py\", line 439, in forward\\n\nfeed_forward_hidden_states = self.mlp(hidden_states)\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/models/gpt2/modeling_gpt2.py\", line 364, in forward\\n\nhidden_states = self.c_fc(hidden_states)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/pytorch_utils.py\", line 117, in forward\\n    size_out =\nx.size()[:-1] + (self.nf,)\\n               ^^^^^^^^\\nKeyboardInterrupt\\n',\n'TimeoutError: Execution exceeded the time limit of an hour']", "['Using device: cuda', '\\n', '[ag_news] Step 0: Spearman Corr=0.7089', '\\n',\n'Epoch 0: validation_loss = 1.3633', '\\n', '[ag_news] Step 0: Spearman\nCorr=0.2000', '\\n', 'Epoch 1: validation_loss = 1.3227', '\\n', '[ag_news] Step\n0: Spearman Corr=0.1884', '\\n', 'Epoch 2: validation_loss = 1.2461', '\\n',\n'[dbpedia] Step 0: Spearman Corr=-0.0283', '\\n', 'Epoch 0: validation_loss =\n2.5999', '\\n', '[dbpedia] Step 0: Spearman Corr=0.2374', '\\n', '[dbpedia] Step\n12: Spearman Corr=0.1871', '\\n', 'Epoch 1: validation_loss = 2.5196', '\\n',\n'[dbpedia] Step 0: Spearman Corr=0.5230', '\\n', '[dbpedia] Step 10: Spearman\nCorr=0.5261', '\\n', 'Epoch 2: validation_loss = 2.3858', '\\n', '[yelp] Step 0:\nSpearman Corr=-0.4474', '\\n', 'Epoch 0: validation_loss = 0.6670', '\\n', '[yelp]\nStep 0: Spearman Corr=0.0590', '\\n', '[yelp] Step 12: Spearman Corr=-0.2618',\n'\\n', 'Epoch 1: validation_loss = 0.6116', '\\n', '[yelp] Step 0: Spearman\nCorr=-0.1439', '\\n', '[yelp] Step 10: Spearman Corr=-0.2957', '\\n', 'Epoch 2:\nvalidation_loss = 0.5335', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution\ntime: 28 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\rMap:   0%|          | 0/1000 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 1000/1000 [00:00<00:00, 11037.32\nexamples/s]', '\\n', 'Epoch 0: validation_loss = 1.9072', '\\n', \"The following\ngeneration flags are not valid and may be ignored: ['output_hidden_states']. Set\n`TRANSFORMERS_VERBOSITY=info` for more details.\\n\", \"The following generation\nflags are not valid and may be ignored: ['output_hidden_states']. Set\n`TRANSFORMERS_VERBOSITY=info` for more details.\\n\", 'Traceback (most recent call\nlast):\\n  File \"runfile.py\", line 142, in <module>\\n    o0.logits[..., :-1,\n:].view(-1, vocab_size),\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: view size is not\ncompatible with input tensor\\'s size and stride (at least one dimension spans\nacross two contiguous subspaces). Use .reshape(...) instead.\\n', 'Execution\ntime: 37 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '*** Dataset: wikitext2', '\\n', '\\rMap:   0%|\n| 0/3672 [00:00<?, ? examples/s]', '\\rMap:  27%|##7       | 1000/3672\n[00:00<00:01, 1874.83 examples/s]', '\\rMap:  54%|#####4    | 2000/3672\n[00:01<00:00, 1857.17 examples/s]', '\\rMap:  82%|########1 | 3000/3672\n[00:01<00:00, 1670.71 examples/s]', '\\rMap: 100%|##########| 3672/3672\n[00:02<00:00, 1813.73 examples/s]', '', '\\rMap: 100%|##########| 3672/3672\n[00:02<00:00, 1795.21 examples/s]', '\\n', '\\rMap:   0%|          | 0/3672\n[00:00<?, ? examples/s]', '\\rMap:  27%|##7       | 1000/3672 [00:00<00:00,\n8394.57 examples/s]', '\\rMap:  54%|#####4    | 2000/3672 [00:00<00:00, 7267.88\nexamples/s]', '\\rMap:  82%|########1 | 3000/3672 [00:00<00:00, 7367.95\nexamples/s]', '', '\\rMap: 100%|##########| 3672/3672 [00:00<00:00, 7806.78\nexamples/s]', '\\n', '`loss_type=None` was set in the config but it is\nunrecognised.Using the default loss: `ForCausalLMLoss`.\\n', 'Dataset wikitext2\nEpoch 0: validation_loss = 3.4705', '\\n', 'Dataset wikitext2 Epoch 0: Spearman\nCorr=-0.3098', '\\n', 'Dataset wikitext2 Epoch 1: validation_loss = 3.3726',\n'\\n', 'Dataset wikitext2 Epoch 1: Spearman Corr=0.0602', '\\n', 'Dataset\nwikitext2 Epoch 2: validation_loss = 3.3131', '\\n', 'Dataset wikitext2 Epoch 2:\nSpearman Corr=0.0301', '\\n', 'Dataset wikitext2 Epoch 3: validation_loss =\n3.2730', '\\n', 'Dataset wikitext2 Epoch 3: Spearman Corr=0.3534', '\\n', 'Dataset\nwikitext2 Epoch 4: validation_loss = 3.2624', '\\n', 'Dataset wikitext2 Epoch 4:\nSpearman Corr=0.2376', '\\n', 'Dataset wikitext2 Epoch 5: validation_loss =\n3.2537', '\\n', 'Dataset wikitext2 Epoch 5: Spearman Corr=0.7925', '\\n', 'Dataset\nwikitext2 Epoch 6: validation_loss = 3.2680', '\\n', 'Dataset wikitext2 Epoch 6:\nSpearman Corr=0.6451', '\\n', 'Dataset wikitext2 Epoch 7: validation_loss =\n3.2925', '\\n', 'Dataset wikitext2 Epoch 7: Spearman Corr=0.5925', '\\n', 'Dataset\nwikitext2 Epoch 8: validation_loss = 3.3871', '\\n', 'Dataset wikitext2 Epoch 8:\nSpearman Corr=0.6767', '\\n', 'Dataset wikitext2 Epoch 9: validation_loss =\n3.4745', '\\n', 'Dataset wikitext2 Epoch 9: Spearman Corr=-0.0947', '\\n', '***\nDataset: bookcorpus', '\\n', '\\rMap:   0%|          | 0/7400423 [00:00<?, ?\nexamples/s]', '\\rMap:   0%|          | 1000/7400423 [00:00<17:27, 7061.27\nexamples/s]', '\\rMap:   0%|          | 2000/7400423 [00:00<15:33, 7922.58\nexamples/s]', '\\rMap:   0%|          | 3000/7400423 [00:00<14:47, 8330.97\nexamples/s]', '\\rMap:   0%|          | 4000/7400423 [00:00<14:02, 8775.06\nexamples/s]', '\\rMap:   0%|          | 5000/7400423 [00:00<13:45, 8961.50\nexamples/s]', '\\rMap:   0%|          | 7000/7400423 [00:00<12:51, 9580.93\nexamples/s]', '\\rMap:   0%|          | 8000/7400423 [00:00<12:49, 9604.82\nexamples/s]', '\\rMap:   0%|          | 9000/7400423 [00:01<13:36, 9051.84\nexamples/s]', '\\rMap:   0%|          | 10000/7400423 [00:01<14:40, 8397.46\nexamples/s]', '\\rMap:   0%|          | 11000/7400423 [00:01<14:38, 8408.00\nexamples/s]', '\\rMap:   0%|          | 12000/7400423 [00:01<14:26, 8527.86\nexamples/s]', '\\rMap:   0%|          | 13000/7400423 [00:01<14:09, 8698.37\nexamples/s]', '\\rMap:   0%|          | 14000/7400423 [00:01<14:00, 8789.69\nexamples/s]', '\\rMap:   0%|          | 15000/7400423 [00:01<13:50, 8897.99\nexamples/s]', '\\rMap:   0%|          | 16000/7400423 [00:01<14:13, 8656.81\nexamples/s]', '\\rMap:   0%|          | 17000/7400423 [00:01<14:02, 8761.94\nexamples/s]', '\\rMap:   0%|          | 18000/7400423 [00:02<14:18, 8602.03\nexamples/s]', '\\rMap:   0%|          | 19000/7400423 [00:02<14:22, 8554.96\nexamples/s]', '\\rMap:   0%|          | 20000/7400423 [00:02<14:31, 8466.91\nexamples/s]', '\\rMap:   0%|          | 21000/7400423 [00:02<14:30, 8475.83\nexamples/s]', '\\rMap:   0%|          | 22000/7400423 [00:02<14:22, 8554.66\nexamples/s]', '\\rMap:   0%|          | 23000/7400423 [00:02<14:10, 8670.56\nexamples/s]', '\\rMap:   0%|          | 24000/7400423 [00:02<14:18, 8592.11\nexamples/s]', '\\rMap:   0%|          | 25000/7400423 [00:02<14:52, 8259.30\nexamples/s]', '\\rMap:   0%|          | 26000/7400423 [00:03<14:59, 8194.44\nexamples/s]', '\\rMap:   0%|          | 27000/7400423 [00:03<15:04, 8155.70\nexamples/s]', '\\rMap:   0%|          | 28000/7400423 [00:03<14:58, 8201.22\nexamples/s]', '\\rMap:   0%|          | 29000/7400423 [00:03<15:10, 8096.62\nexamples/s]', '\\rMap:   0%|          | 30000/7400423 [00:03<14:45, 8324.92\nexamples/s]', '\\rMap:   0%|          | 31000/7400423 [00:03<14:38, 8383.99\nexamples/s]', '\\rMap:   0%|          | 32000/7400423 [00:03<14:28, 8482.30\nexamples/s]', '\\rMap:   0%|          | 33000/7400423 [00:03<13:59, 8772.04\nexamples/s]', '\\rMap:   0%|          | 34000/7400423 [00:03<14:05, 8710.99\nexamples/s]', '\\rMap:   0%|          | 35000/7400423 [00:04<14:21, 8547.12\nexamples/s]', '\\rMap:   0%|          | 36000/7400423 [00:04<14:35, 8416.28\nexamples/s]', '\\rMap:   0%|          | 37000/7400423 [00:04<14:30, 8463.27\nexamples/s]', '\\rMap:   1%|          | 38000/7400423 [00:04<14:46, 8308.84\nexamples/s]', '\\rMap:   1%|          | 39000/7400423 [00:04<14:47, 8293.18\nexamples/s]', '\\rMap:   1%|          | 40000/7400423 [00:04<14:37, 8386.22\nexamples/s]', '\\rMap:   1%|          | 41000/7400423 [00:04<14:16, 8589.95\nexamples/s]', '\\rMap:   1%|          | 42000/7400423 [00:04<13:51, 8848.03\nexamples/s]', '\\rMap:   1%|          | 43000/7400423 [00:05<13:59, 8766.15\nexamples/s]', '\\rMap:   1%|          | 44000/7400423 [00:05<13:55, 8807.29\nexamples/s]', '\\rMap:   1%|          | 45000/7400423 [00:05<13:45, 8914.56\nexamples/s]', '\\rMap:   1%|          | 46000/7400423 [00:05<13:34, 9024.19\nexamples/s]', '\\rMap:   1%|          | 47000/7400423 [00:05<13:25, 9130.53\nexamples/s]', '\\rMap:   1%|          | 48000/7400423 [00:05<13:35, 9011.00\nexamples/s]', '\\rMap:   1%|          | 49000/7400423 [00:05<13:49, 8866.09\nexamples/s]', '\\rMap:   1%|          | 50000/7400423 [00:05<13:52, 8834.43\nexamples/s]', '\\rMap:   1%|          | 51000/7400423 [00:05<13:59, 8753.38\nexamples/s]', '\\rMap:   1%|          | 52000/7400423 [00:06<13:59, 8751.37\nexamples/s]', '\\rMap:   1%|          | 53000/7400423 [00:06<13:51, 8836.94\nexamples/s]', '\\rMap:   1%|          | 54000/7400423 [00:06<13:58, 8765.99\nexamples/s]', '\\rMap:   1%|          | 55000/7400423 [00:06<14:00, 8737.71\nexamples/s]', '\\rMap:   1%|          | 56000/7400423 [00:06<22:07, 5533.51\nexamples/s]', '\\rMap:   1%|          | 57000/7400423 [00:06<19:41, 6214.57\nexamples/s]', '\\rMap:   1%|          | 58000/7400423 [00:06<17:46, 6882.17\nexamples/s]', '\\rMap:   1%|          | 59000/7400423 [00:07<16:42, 7323.96\nexamples/s]', '\\rMap:   1%|          | 60000/7400423 [00:07<15:42, 7790.06\nexamples/s]', '\\rMap:   1%|          | 61000/7400423 [00:07<14:51, 8232.33\nexamples/s]', '\\rMap:   1%|          | 62000/7400423 [00:07<14:19, 8539.24\nexamples/s]', '\\rMap:   1%|          | 63000/7400423 [00:07<13:59, 8741.23\nexamples/s]', '\\rMap:   1%|          | 64000/7400423 [00:07<13:43, 8910.34\nexamples/s]', '\\rMap:   1%|          | 65000/7400423 [00:07<13:33, 9015.75\nexamples/s]', '\\rMap:   1%|          | 66000/7400423 [00:07<13:14, 9228.58\nexamples/s]', '\\rMap:   1%|          | 67000/7400423 [00:07<13:01, 9388.17\nexamples/s]', '\\rMap:   1%|          | 68000/7400423 [00:08<13:08, 9301.90\nexamples/s]', '\\rMap:   1%|          | 69000/7400423 [00:08<12:56, 9441.65\nexamples/s]', '\\rMap:   1%|          | 70000/7400423 [00:08<12:58, 9416.12\nexamples/s]', '\\rMap:   1%|          | 72000/7400423 [00:08<12:18, 9925.68\nexamples/s]', '\\rMap:   1%|          | 74000/7400423 [00:08<12:14, 9970.15\nexamples/s]', '\\rMap:   1%|1         | 75000/7400423 [00:08<12:16, 9949.00\nexamples/s]', '\\rMap:   1%|1         | 77000/7400423 [00:08<12:16, 9949.10\nexamples/s]', '\\rMap:   1%|1         | 78000/7400423 [00:09<12:18, 9915.49\nexamples/s]', '\\rMap:   1%|1         | 79000/7400423 [00:09<12:45, 9566.61\nexamples/s]', '\\rMap:   1%|1         | 80000/7400423 [00:09<12:49, 9515.70\nexamples/s]', '\\rMap:   1%|1         | 81000/7400423 [00:09<12:59, 9394.96\nexamples/s]', '\\rMap:   1%|1         | 82000/7400423 [00:09<13:15, 9202.82\nexamples/s]', '\\rMap:   1%|1         | 83000/7400423 [00:09<13:08, 9275.41\nexamples/s]', '\\rMap:   1%|1         | 84000/7400423 [00:09<12:53, 9456.52\nexamples/s]', '\\rMap:   1%|1         | 85000/7400423 [00:09<13:11, 9243.53\nexamples/s]', '\\rMap:   1%|1         | 86000/7400423 [00:09<13:00, 9365.64\nexamples/s]', '\\rMap:   1%|1         | 87000/7400423 [00:09<12:46, 9536.68\nexamples/s]', '\\rMap:   1%|1         | 89000/7400423 [00:10<12:19, 9885.04\nexamples/s]', '\\rMap:   1%|1         | 91000/7400423 [00:10<11:40, 10436.19\nexamples/s]', '\\rMap:   1%|1         | 93000/7400423 [00:10<11:59, 10154.98\nexamples/s]', '\\rMap:   1%|1         | 95000/7400423 [00:10<11:43, 10380.50\nexamples/s]', '\\rMap:   1%|1         | 97000/7400423 [00:10<11:33, 10534.11\nexamples/s]', '\\rMap:   1%|1         | 99000/7400423 [00:11<11:35, 10496.73\nexamples/s]', '\\rMap:   1%|1         | 101000/7400423 [00:11<11:30, 10565.87\nexamples/s]', '\\rMap:   1%|1         | 103000/7400423 [00:11<11:26, 10635.51\nexamples/s]', '\\rMap:   1%|1         | 105000/7400423 [00:11<11:34, 10508.63\nexamples/s]', '\\rMap:   1%|1         | 107000/7400423 [00:11<11:26, 10616.64\nexamples/s]', '\\rMap:   1%|1         | 109000/7400423 [00:12<11:15, 10795.41\nexamples/s]', '\\rMap:   1%|1         | 111000/7400423 [00:12<11:13, 10819.17\nexamples/s]', '\\rMap:   2%|1         | 113000/7400423 [00:12<11:30, 10552.11\nexamples/s]', '\\rMap:   2%|1         | 115000/7400423 [00:12<11:53, 10210.20\nexamples/s]', '\\rMap:   2%|1         | 117000/7400423 [00:12<12:25, 9767.64\nexamples/s] ', '\\rMap:   2%|1         | 118000/7400423 [00:12<12:27, 9736.21\nexamples/s]', '\\rMap:   2%|1         | 119000/7400423 [00:13<12:33, 9668.85\nexamples/s]', '\\rMap:   2%|1         | 120000/7400423 [00:13<12:50, 9445.79\nexamples/s]', '\\rMap:   2%|1         | 121000/7400423 [00:13<12:55, 9389.51\nexamples/s]', '\\rMap:   2%|1         | 122000/7400423 [00:13<12:55, 9385.43\nexamples/s]', '\\rMap:   2%|1         | 123000/7400423 [00:13<12:57, 9359.36\nexamples/s]', '\\rMap:   2%|1         | 124000/7400423 [00:13<13:24, 9044.26\nexamples/s]', '\\rMap:   2%|1         | 125000/7400423 [00:13<13:29, 8985.32\nexamples/s]', '\\rMap:   2%|1         | 126000/7400423 [00:13<13:21, 9079.79\nexamples/s]', '\\rMap:   2%|1         | 127000/7400423 [00:13<13:21, 9078.38\nexamples/s]', '\\rMap:   2%|1         | 128000/7400423 [00:14<13:20, 9080.52\nexamples/s]', '\\rMap:   2%|1         | 129000/7400423 [00:14<13:08, 9221.19\nexamples/s]', '\\rMap:   2%|1         | 130000/7400423 [00:14<13:04, 9262.78\nexamples/s]', '\\rMap:   2%|1         | 131000/7400423 [00:14<13:11, 9180.44\nexamples/s]', '\\rMap:   2%|1         | 132000/7400423 [00:14<13:15, 9138.34\nexamples/s]', '\\rMap:   2%|1         | 133000/7400423 [00:14<13:20, 9079.78\nexamples/s]', '\\rMap:   2%|1         | 134000/7400423 [00:14<13:15, 9133.23\nexamples/s]', '\\rMap:   2%|1         | 135000/7400423 [00:14<13:22, 9054.28\nexamples/s]', '\\rMap:   2%|1         | 136000/7400423 [00:14<13:30, 8958.92\nexamples/s]', '\\rMap:   2%|1         | 137000/7400423 [00:15<13:29, 8970.84\nexamples/s]', '\\rMap:   2%|1         | 138000/7400423 [00:15<13:16, 9123.25\nexamples/s]', '\\rMap:   2%|1         | 139000/7400423 [00:15<12:55, 9360.11\nexamples/s]', '\\rMap:   2%|1         | 140000/7400423 [00:15<12:45, 9481.94\nexamples/s]', '\\rMap:   2%|1         | 141000/7400423 [00:15<12:37, 9581.45\nexamples/s]', '\\rMap:   2%|1         | 142000/7400423 [00:15<12:42, 9517.26\nexamples/s]', '\\rMap:   2%|1         | 143000/7400423 [00:15<19:49, 6098.98\nexamples/s]', '\\rMap:   2%|1         | 145000/7400423 [00:16<16:17, 7420.15\nexamples/s]', '\\rMap:   2%|1         | 146000/7400423 [00:16<15:16, 7918.77\nexamples/s]', '\\rMap:   2%|1         | 147000/7400423 [00:16<14:30, 8328.72\nexamples/s]', '\\rMap:   2%|1         | 148000/7400423 [00:16<13:55, 8676.75\nexamples/s]', '\\rMap:   2%|2         | 149000/7400423 [00:16<13:31, 8935.82\nexamples/s]', '\\rMap:   2%|2         | 150000/7400423 [00:16<13:15, 9109.41\nexamples/s]', '\\rMap:   2%|2         | 152000/7400423 [00:16<12:43, 9495.51\nexamples/s]', '\\rMap:   2%|2         | 153000/7400423 [00:16<12:38, 9554.87\nexamples/s]', '\\rMap:   2%|2         | 154000/7400423 [00:17<12:49, 9413.87\nexamples/s]', '\\rMap:   2%|2         | 155000/7400423 [00:17<12:45, 9464.76\nexamples/s]', '\\rMap:   2%|2         | 156000/7400423 [00:17<12:44, 9479.66\nexamples/s]', '\\rMap:   2%|2         | 158000/7400423 [00:17<12:26, 9696.83\nexamples/s]', '\\rMap:   2%|2         | 160000/7400423 [00:17<12:24, 9726.50\nexamples/s]', '\\rMap:   2%|2         | 161000/7400423 [00:17<12:23, 9730.57\nexamples/s]', '\\rMap:   2%|2         | 163000/7400423 [00:17<12:09, 9918.96\nexamples/s]', '\\rMap:   2%|2         | 164000/7400423 [00:18<12:15, 9834.80\nexamples/s]', '\\rMap:   2%|2         | 166000/7400423 [00:18<12:06, 9959.36\nexamples/s]', '\\rMap:   2%|2         | 167000/7400423 [00:18<12:21, 9754.67\nexamples/s]', '\\rMap:   2%|2         | 168000/7400423 [00:18<12:25, 9697.65\nexamples/s]', '\\rMap:   2%|2         | 170000/7400423 [00:18<12:06, 9950.23\nexamples/s]', '\\rMap:   2%|2         | 172000/7400423 [00:18<12:03, 9996.51\nexamples/s]', '\\rMap:   2%|2         | 174000/7400423 [00:19<11:56, 10081.85\nexamples/s]', '\\rMap:   2%|2         | 176000/7400423 [00:19<11:53, 10121.88\nexamples/s]', '\\rMap:   2%|2         | 178000/7400423 [00:19<12:01, 10004.84\nexamples/s]', '\\rMap:   2%|2         | 179000/7400423 [00:19<12:07, 9930.11\nexamples/s] ', '\\rMap:   2%|2         | 181000/7400423 [00:19<12:06, 9943.85\nexamples/s]', '\\rMap:   2%|2         | 183000/7400423 [00:19<11:52, 10133.20\nexamples/s]', '\\rMap:   2%|2         | 185000/7400423 [00:20<11:51, 10142.30\nexamples/s]', '\\rMap:   3%|2         | 187000/7400423 [00:20<11:51, 10133.33\nexamples/s]', '\\rMap:   3%|2         | 189000/7400423 [00:20<11:48, 10180.42\nexamples/s]', '\\rMap:   3%|2         | 191000/7400423 [00:20<11:56, 10061.58\nexamples/s]', '\\rMap:   3%|2         | 193000/7400423 [00:20<12:00, 10000.89\nexamples/s]', '\\rMap:   3%|2         | 195000/7400423 [00:21<12:00, 10000.27\nexamples/s]', '\\rMap:   3%|2         | 197000/7400423 [00:21<11:51, 10123.90\nexamples/s]', '\\rMap:   3%|2         | 199000/7400423 [00:21<11:50, 10130.22\nexamples/s]', '\\rMap:   3%|2         | 201000/7400423 [00:21<11:49, 10143.68\nexamples/s]', '\\rMap:   3%|2         | 203000/7400423 [00:21<12:03, 9951.37\nexamples/s] ', '\\rMap:   3%|2         | 204000/7400423 [00:22<12:04, 9937.92\nexamples/s]', '\\rMap:   3%|2         | 206000/7400423 [00:22<11:57, 10020.71\nexamples/s]', '\\rMap:   3%|2         | 208000/7400423 [00:22<12:04, 9921.50\nexamples/s] ', '\\rMap:   3%|2         | 209000/7400423 [00:22<12:17, 9748.19\nexamples/s]', '\\rMap:   3%|2         | 210000/7400423 [00:22<12:14, 9795.51\nexamples/s]', '\\rMap:   3%|2         | 212000/7400423 [00:22<12:05, 9911.26\nexamples/s]', '\\rMap:   3%|2         | 214000/7400423 [00:23<11:59, 9991.14\nexamples/s]', '\\rMap:   3%|2         | 216000/7400423 [00:23<11:50, 10111.28\nexamples/s]', '\\rMap:   3%|2         | 218000/7400423 [00:23<11:45, 10187.46\nexamples/s]', '\\rMap:   3%|2         | 220000/7400423 [00:23<15:23, 7778.91\nexamples/s] ', '\\rMap:   3%|2         | 221000/7400423 [00:23<14:51, 8057.58\nexamples/s]', '\\rMap:   3%|3         | 223000/7400423 [00:24<13:52, 8621.25\nexamples/s]', '\\rMap:   3%|3         | 225000/7400423 [00:24<13:11, 9064.74\nexamples/s]', '\\rMap:   3%|3         | 226000/7400423 [00:24<12:59, 9202.44\nexamples/s]', '\\rMap:   3%|3         | 227000/7400423 [00:24<12:46, 9359.22\nexamples/s]', '\\rMap:   3%|3         | 228000/7400423 [00:24<12:53, 9270.39\nexamples/s]', '\\rMap:   3%|3         | 229000/7400423 [00:24<12:49, 9320.93\nexamples/s]', '\\rMap:   3%|3         | 230000/7400423 [00:24<12:41, 9410.25\nexamples/s]', '\\rMap:   3%|3         | 232000/7400423 [00:25<12:14, 9756.71\nexamples/s]', '\\rMap:   3%|3         | 234000/7400423 [00:25<11:56, 10002.48\nexamples/s]', '\\rMap:   3%|3         | 235000/7400423 [00:25<12:15, 9746.85\nexamples/s] ', '\\rMap:   3%|3         | 236000/7400423 [00:25<12:32, 9517.79\nexamples/s]', '\\rMap:   3%|3         | 237000/7400423 [00:25<12:53, 9258.64\nexamples/s]', '\\rMap:   3%|3         | 238000/7400423 [00:25<13:03, 9143.38\nexamples/s]', '\\rMap:   3%|3         | 239000/7400423 [00:25<13:02, 9150.68\nexamples/s]', '\\rMap:   3%|3         | 240000/7400423 [00:25<12:45, 9359.39\nexamples/s]', '\\rMap:   3%|3         | 242000/7400423 [00:26<12:15, 9738.03\nexamples/s]', '\\rMap:   3%|3         | 243000/7400423 [00:26<12:12, 9765.27\nexamples/s]', '\\rMap:   3%|3         | 244000/7400423 [00:26<12:21, 9657.14\nexamples/s]', '\\rMap:   3%|3         | 246000/7400423 [00:26<11:53, 10024.37\nexamples/s]', '\\rMap:   3%|3         | 248000/7400423 [00:26<11:54, 10004.27\nexamples/s]', '\\rMap:   3%|3         | 249000/7400423 [00:26<11:55, 9988.11\nexamples/s] ', '\\rMap:   3%|3         | 250000/7400423 [00:26<11:59, 9939.11\nexamples/s]', '\\rMap:   3%|3         | 252000/7400423 [00:27<11:48, 10088.53\nexamples/s]', '\\rMap:   3%|3         | 254000/7400423 [00:27<11:42, 10167.01\nexamples/s]', '\\rMap:   3%|3         | 256000/7400423 [00:27<11:33, 10296.20\nexamples/s]', '\\rMap:   3%|3         | 258000/7400423 [00:27<11:36, 10253.23\nexamples/s]', '\\rMap:   4%|3         | 260000/7400423 [00:27<11:36, 10258.72\nexamples/s]', '\\rMap:   4%|3         | 262000/7400423 [00:28<11:44, 10136.41\nexamples/s]', '\\rMap:   4%|3         | 264000/7400423 [00:28<11:35, 10267.53\nexamples/s]', '\\rMap:   4%|3         | 266000/7400423 [00:28<11:34, 10276.84\nexamples/s]', '\\rMap:   4%|3         | 268000/7400423 [00:28<11:43, 10141.98\nexamples/s]', '\\rMap:   4%|3         | 270000/7400423 [00:28<11:44, 10126.47\nexamples/s]', '\\rMap:   4%|3         | 272000/7400423 [00:29<11:32, 10291.26\nexamples/s]', '\\rMap:   4%|3         | 274000/7400423 [00:29<11:36, 10226.38\nexamples/s]', '\\rMap:   4%|3         | 276000/7400423 [00:29<11:47, 10069.95\nexamples/s]', '\\rMap:   4%|3         | 278000/7400423 [00:29<12:00, 9889.52\nexamples/s] ', '\\rMap:   4%|3         | 279000/7400423 [00:29<12:03, 9842.92\nexamples/s]', '\\rMap:   4%|3         | 280000/7400423 [00:29<12:06, 9803.64\nexamples/s]', '\\rMap:   4%|3         | 282000/7400423 [00:30<12:21, 9606.04\nexamples/s]', '\\rMap:   4%|3         | 283000/7400423 [00:30<12:30, 9485.95\nexamples/s]', '\\rMap:   4%|3         | 284000/7400423 [00:30<12:48, 9254.70\nexamples/s]', '\\rMap:   4%|3         | 285000/7400423 [00:30<13:15, 8946.10\nexamples/s]', '\\rMap:   4%|3         | 286000/7400423 [00:30<13:21, 8875.18\nexamples/s]', '\\rMap:   4%|3         | 287000/7400423 [00:30<13:40, 8664.47\nexamples/s]', '\\rMap:   4%|3         | 288000/7400423 [00:30<13:49, 8571.58\nexamples/s]', '\\rMap:   4%|3         | 289000/7400423 [00:30<14:07, 8394.40\nexamples/s]', '\\rMap:   4%|3         | 290000/7400423 [00:30<14:02, 8442.11\nexamples/s]', '\\rMap:   4%|3         | 291000/7400423 [00:31<14:10, 8356.76\nexamples/s]', '\\rMap:   4%|3         | 292000/7400423 [00:31<14:08, 8382.07\nexamples/s]', '\\rMap:   4%|3         | 293000/7400423 [00:31<13:56, 8498.61\nexamples/s]', '\\rMap:   4%|3         | 294000/7400423 [00:31<14:02, 8437.62\nexamples/s]', '\\rMap:   4%|3         | 295000/7400423 [00:31<13:51, 8542.28\nexamples/s]', '\\rMap:   4%|3         | 296000/7400423 [00:31<13:27, 8798.40\nexamples/s]', '\\rMap:   4%|4         | 297000/7400423 [00:31<13:53, 8519.24\nexamples/s]', '\\rMap:   4%|4         | 298000/7400423 [00:31<13:48, 8568.83\nexamples/s]', '\\rMap:   4%|4         | 299000/7400423 [00:32<13:45, 8600.88\nexamples/s]', '\\rMap:   4%|4         | 300000/7400423 [00:32<13:47, 8583.26\nexamples/s]', '\\rMap:   4%|4         | 301000/7400423 [00:32<13:29, 8771.77\nexamples/s]', '\\rMap:   4%|4         | 302000/7400423 [00:32<13:00, 9096.86\nexamples/s]', '\\rMap:   4%|4         | 304000/7400423 [00:32<11:54, 9934.85\nexamples/s]', '\\rMap:   4%|4         | 306000/7400423 [00:32<11:29, 10290.67\nexamples/s]', '\\rMap:   4%|4         | 308000/7400423 [00:32<11:26, 10336.84\nexamples/s]', '\\rMap:   4%|4         | 310000/7400423 [00:33<11:11, 10557.80\nexamples/s]', '\\rMap:   4%|4         | 312000/7400423 [00:33<11:07, 10622.17\nexamples/s]', '\\rMap:   4%|4         | 314000/7400423 [00:33<11:12, 10545.07\nexamples/s]', '\\rMap:   4%|4         | 316000/7400423 [00:33<11:21, 10393.74\nexamples/s]', '\\rMap:   4%|4         | 318000/7400423 [00:33<11:33, 10210.40\nexamples/s]', '\\rMap:   4%|4         | 320000/7400423 [00:34<11:22, 10374.17\nexamples/s]', '\\rMap:   4%|4         | 322000/7400423 [00:34<11:08, 10591.27\nexamples/s]', '\\rMap:   4%|4         | 324000/7400423 [00:34<11:06, 10623.08\nexamples/s]', '\\rMap:   4%|4         | 326000/7400423 [00:34<10:56, 10769.91\nexamples/s]', '\\rMap:   4%|4         | 328000/7400423 [00:34<10:47, 10927.93\nexamples/s]', '\\rMap:   4%|4         | 330000/7400423 [00:34<10:45, 10953.53\nexamples/s]', '\\rMap:   4%|4         | 332000/7400423 [00:35<10:45, 10951.40\nexamples/s]', '\\rMap:   5%|4         | 334000/7400423 [00:35<10:46, 10922.32\nexamples/s]', '\\rMap:   5%|4         | 336000/7400423 [00:35<11:12, 10505.28\nexamples/s]', '\\rMap:   5%|4         | 338000/7400423 [00:35<11:54, 9888.27\nexamples/s] ', '\\rMap:   5%|4         | 339000/7400423 [00:35<12:14, 9616.05\nexamples/s]', '\\rMap:   5%|4         | 340000/7400423 [00:36<12:35, 9345.02\nexamples/s]', '\\rMap:   5%|4         | 341000/7400423 [00:36<12:48, 9188.17\nexamples/s]', '\\rMap:   5%|4         | 342000/7400423 [00:36<12:57, 9082.53\nexamples/s]', '\\rMap:   5%|4         | 343000/7400423 [00:36<12:52, 9133.07\nexamples/s]', '\\rMap:   5%|4         | 344000/7400423 [00:36<12:40, 9274.26\nexamples/s]', '\\rMap:   5%|4         | 345000/7400423 [00:36<19:25, 6051.31\nexamples/s]', '\\rMap:   5%|4         | 346000/7400423 [00:36<18:00, 6528.86\nexamples/s]', '\\rMap:   5%|4         | 347000/7400423 [00:37<16:56, 6935.65\nexamples/s]', '\\rMap:   5%|4         | 348000/7400423 [00:37<16:06, 7296.17\nexamples/s]', '\\rMap:   5%|4         | 349000/7400423 [00:37<15:14, 7710.44\nexamples/s]', '\\rMap:   5%|4         | 350000/7400423 [00:37<14:34, 8062.43\nexamples/s]', '\\rMap:   5%|4         | 351000/7400423 [00:37<14:16, 8226.11\nexamples/s]', '\\rMap:   5%|4         | 352000/7400423 [00:37<14:24, 8155.20\nexamples/s]', '\\rMap:   5%|4         | 353000/7400423 [00:37<14:10, 8288.26\nexamples/s]', '\\rMap:   5%|4         | 354000/7400423 [00:37<14:09, 8295.17\nexamples/s]', '\\rMap:   5%|4         | 355000/7400423 [00:37<14:09, 8296.86\nexamples/s]', '\\rMap:   5%|4         | 356000/7400423 [00:38<13:56, 8416.36\nexamples/s]', '\\rMap:   5%|4         | 357000/7400423 [00:38<13:45, 8531.18\nexamples/s]', '\\rMap:   5%|4         | 358000/7400423 [00:38<13:22, 8771.10\nexamples/s]', '\\rMap:   5%|4         | 359000/7400423 [00:38<13:21, 8785.82\nexamples/s]', '\\rMap:   5%|4         | 360000/7400423 [00:38<12:58, 9048.41\nexamples/s]', '\\rMap:   5%|4         | 361000/7400423 [00:38<13:21, 8781.61\nexamples/s]', '\\rMap:   5%|4         | 362000/7400423 [00:38<13:21, 8780.84\nexamples/s]', '\\rMap:   5%|4         | 363000/7400423 [00:38<13:06, 8948.21\nexamples/s]', '\\rMap:   5%|4         | 364000/7400423 [00:38<12:47, 9172.15\nexamples/s]', '\\rMap:   5%|4         | 365000/7400423 [00:39<12:47, 9165.34\nexamples/s]', '\\rMap:   5%|4         | 366000/7400423 [00:39<13:01, 8996.98\nexamples/s]', '\\rMap:   5%|4         | 367000/7400423 [00:39<12:56, 9054.41\nexamples/s]', '\\rMap:   5%|4         | 368000/7400423 [00:39<13:12, 8870.69\nexamples/s]', '\\rMap:   5%|4         | 369000/7400423 [00:39<13:20, 8784.41\nexamples/s]', '\\rMap:   5%|4         | 370000/7400423 [00:39<13:37, 8595.29\nexamples/s]', '\\rMap:   5%|5         | 371000/7400423 [00:39<13:56, 8400.44\nexamples/s]', '\\rMap:   5%|5         | 372000/7400423 [00:39<13:57, 8393.13\nexamples/s]', '\\rMap:   5%|5         | 373000/7400423 [00:40<13:33, 8634.94\nexamples/s]', '\\rMap:   5%|5         | 374000/7400423 [00:40<13:29, 8677.62\nexamples/s]', '\\rMap:   5%|5         | 375000/7400423 [00:40<13:32, 8647.06\nexamples/s]', '\\rMap:   5%|5         | 376000/7400423 [00:40<13:29, 8677.06\nexamples/s]', '\\rMap:   5%|5         | 377000/7400423 [00:40<13:29, 8678.02\nexamples/s]', '\\rMap:   5%|5         | 378000/7400423 [00:40<13:30, 8667.85\nexamples/s]', '\\rMap:   5%|5         | 379000/7400423 [00:40<13:30, 8659.74\nexamples/s]', '\\rMap:   5%|5         | 380000/7400423 [00:40<13:45, 8504.93\nexamples/s]', '\\rMap:   5%|5         | 381000/7400423 [00:40<13:35, 8603.47\nexamples/s]', '\\rMap:   5%|5         | 382000/7400423 [00:41<13:42, 8530.23\nexamples/s]', '\\rMap:   5%|5         | 383000/7400423 [00:41<13:44, 8506.07\nexamples/s]', '\\rMap:   5%|5         | 384000/7400423 [00:41<13:50, 8446.51\nexamples/s]', '\\rMap:   5%|5         | 385000/7400423 [00:41<14:01, 8338.08\nexamples/s]', '\\rMap:   5%|5         | 386000/7400423 [00:41<13:52, 8425.62\nexamples/s]', '\\rMap:   5%|5         | 387000/7400423 [00:41<13:37, 8578.79\nexamples/s]', '\\rMap:   5%|5         | 388000/7400423 [00:41<13:40, 8543.21\nexamples/s]', '\\rMap:   5%|5         | 389000/7400423 [00:41<13:38, 8563.54\nexamples/s]', '\\rMap:   5%|5         | 390000/7400423 [00:41<13:31, 8635.89\nexamples/s]', '\\rMap:   5%|5         | 391000/7400423 [00:42<13:35, 8594.97\nexamples/s]', '\\rMap:   5%|5         | 392000/7400423 [00:42<13:48, 8458.39\nexamples/s]', '\\rMap:   5%|5         | 393000/7400423 [00:42<13:45, 8491.49\nexamples/s]', '\\rMap:   5%|5         | 394000/7400423 [00:42<13:34, 8605.84\nexamples/s]', '\\rMap:   5%|5         | 395000/7400423 [00:42<13:11, 8852.51\nexamples/s]', '\\rMap:   5%|5         | 396000/7400423 [00:42<13:00, 8973.19\nexamples/s]', '\\rMap:   5%|5         | 397000/7400423 [00:42<13:00, 8973.31\nexamples/s]', '\\rMap:   5%|5         | 398000/7400423 [00:42<12:46, 9138.43\nexamples/s]', '\\rMap:   5%|5         | 399000/7400423 [00:42<12:38, 9235.80\nexamples/s]', '\\rMap:   5%|5         | 400000/7400423 [00:43<12:32, 9300.38\nexamples/s]', '\\rMap:   5%|5         | 402000/7400423 [00:43<12:07, 9614.52\nexamples/s]', '\\rMap:   5%|5         | 404000/7400423 [00:43<12:03, 9675.65\nexamples/s]', '\\rMap:   5%|5         | 405000/7400423 [00:43<12:09, 9587.08\nexamples/s]', '\\rMap:   5%|5         | 406000/7400423 [00:43<12:04, 9654.80\nexamples/s]', '\\rMap:   5%|5         | 407000/7400423 [00:43<12:08, 9606.33\nexamples/s]', '\\rMap:   6%|5         | 408000/7400423 [00:43<12:38, 9224.07\nexamples/s]', '\\rMap:   6%|5         | 409000/7400423 [00:44<12:50, 9071.89\nexamples/s]', '\\rMap:   6%|5         | 410000/7400423 [00:44<12:59, 8966.67\nexamples/s]', '\\rMap:   6%|5         | 411000/7400423 [00:44<13:14, 8797.38\nexamples/s]', '\\rMap:   6%|5         | 412000/7400423 [00:44<13:40, 8516.58\nexamples/s]', '\\rMap:   6%|5         | 413000/7400423 [00:44<13:40, 8517.32\nexamples/s]', '\\rMap:   6%|5         | 414000/7400423 [00:44<13:53, 8383.46\nexamples/s]', '\\rMap:   6%|5         | 415000/7400423 [00:44<13:54, 8369.63\nexamples/s]', '\\rMap:   6%|5         | 416000/7400423 [00:44<13:41, 8501.93\nexamples/s]', '\\rMap:   6%|5         | 417000/7400423 [00:44<13:21, 8716.09\nexamples/s]', '\\rMap:   6%|5         | 418000/7400423 [00:45<12:55, 9001.10\nexamples/s]', '\\rMap:   6%|5         | 420000/7400423 [00:45<12:13, 9517.87\nexamples/s]', '\\rMap:   6%|5         | 421000/7400423 [00:45<12:20, 9419.51\nexamples/s]', '\\rMap:   6%|5         | 422000/7400423 [00:45<12:30, 9293.83\nexamples/s]', '\\rMap:   6%|5         | 424000/7400423 [00:45<12:09, 9565.08\nexamples/s]', '\\rMap:   6%|5         | 425000/7400423 [00:45<12:10, 9552.53\nexamples/s]', '\\rMap:   6%|5         | 427000/7400423 [00:46<12:10, 9542.98\nexamples/s]', '\\rMap:   6%|5         | 428000/7400423 [00:46<12:04, 9618.37\nexamples/s]', '\\rMap:   6%|5         | 429000/7400423 [00:46<12:03, 9639.73\nexamples/s]', '\\rMap:   6%|5         | 430000/7400423 [00:46<11:57, 9712.18\nexamples/s]', '\\rMap:   6%|5         | 431000/7400423 [00:46<11:58, 9696.65\nexamples/s]', '\\rMap:   6%|5         | 432000/7400423 [00:46<11:55, 9741.47\nexamples/s]', '\\rMap:   6%|5         | 433000/7400423 [00:46<12:00, 9670.03\nexamples/s]', '\\rMap:   6%|5         | 434000/7400423 [00:46<12:04, 9613.20\nexamples/s]', '\\rMap:   6%|5         | 435000/7400423 [00:46<12:09, 9554.39\nexamples/s]', '\\rMap:   6%|5         | 437000/7400423 [00:47<11:48, 9828.24\nexamples/s]', '\\rMap:   6%|5         | 438000/7400423 [00:47<11:48, 9827.55\nexamples/s]', '\\rMap:   6%|5         | 439000/7400423 [00:47<11:52, 9767.83\nexamples/s]', '\\rMap:   6%|5         | 441000/7400423 [00:47<11:47, 9842.96\nexamples/s]', '\\rMap:   6%|5         | 442000/7400423 [00:47<11:48, 9824.03\nexamples/s]', '\\rMap:   6%|5         | 443000/7400423 [00:47<11:46, 9853.15\nexamples/s]', '\\rMap:   6%|5         | 444000/7400423 [00:47<18:05, 6406.62\nexamples/s]', '\\rMap:   6%|6         | 445000/7400423 [00:48<16:47, 6900.97\nexamples/s]', '\\rMap:   6%|6         | 446000/7400423 [00:48<15:47, 7341.89\nexamples/s]', '\\rMap:   6%|6         | 447000/7400423 [00:48<14:46, 7843.23\nexamples/s]', '\\rMap:   6%|6         | 448000/7400423 [00:48<14:06, 8215.26\nexamples/s]', '\\rMap:   6%|6         | 450000/7400423 [00:48<12:57, 8935.38\nexamples/s]', '\\rMap:   6%|6         | 452000/7400423 [00:48<12:18, 9404.36\nexamples/s]', '\\rMap:   6%|6         | 453000/7400423 [00:48<12:32, 9227.40\nexamples/s]', '\\rMap:   6%|6         | 454000/7400423 [00:49<12:36, 9179.66\nexamples/s]', '\\rMap:   6%|6         | 455000/7400423 [00:49<12:30, 9258.24\nexamples/s]', '\\rMap:   6%|6         | 456000/7400423 [00:49<12:29, 9264.80\nexamples/s]', '\\rMap:   6%|6         | 457000/7400423 [00:49<12:42, 9106.15\nexamples/s]', '\\rMap:   6%|6         | 458000/7400423 [00:49<12:27, 9288.96\nexamples/s]', '\\rMap:   6%|6         | 459000/7400423 [00:49<12:30, 9249.35\nexamples/s]', '\\rMap:   6%|6         | 460000/7400423 [00:49<12:45, 9070.00\nexamples/s]', '\\rMap:   6%|6         | 461000/7400423 [00:49<12:46, 9050.68\nexamples/s]', '\\rMap:   6%|6         | 462000/7400423 [00:49<13:02, 8870.28\nexamples/s]', '\\rMap:   6%|6         | 463000/7400423 [00:50<12:56, 8939.49\nexamples/s]', '\\rMap:   6%|6         | 464000/7400423 [00:50<12:54, 8960.21\nexamples/s]', '\\rMap:   6%|6         | 465000/7400423 [00:50<12:50, 9001.83\nexamples/s]', '\\rMap:   6%|6         | 466000/7400423 [00:50<12:39, 9128.92\nexamples/s]', '\\rMap:   6%|6         | 467000/7400423 [00:50<12:23, 9320.96\nexamples/s]', '\\rMap:   6%|6         | 468000/7400423 [00:50<12:26, 9288.62\nexamples/s]', '\\rMap:   6%|6         | 469000/7400423 [00:50<12:14, 9439.80\nexamples/s]', '\\rMap:   6%|6         | 470000/7400423 [00:50<12:05, 9549.09\nexamples/s]', '\\rMap:   6%|6         | 472000/7400423 [00:50<11:46, 9810.00\nexamples/s]', '\\rMap:   6%|6         | 474000/7400423 [00:51<11:26, 10094.28\nexamples/s]', '\\rMap:   6%|6         | 476000/7400423 [00:51<11:14, 10271.58\nexamples/s]', '\\rMap:   6%|6         | 478000/7400423 [00:51<11:05, 10401.86\nexamples/s]', '\\rMap:   6%|6         | 480000/7400423 [00:51<10:56, 10546.49\nexamples/s]', '\\rMap:   7%|6         | 482000/7400423 [00:51<10:45, 10720.10\nexamples/s]', '\\rMap:   7%|6         | 484000/7400423 [00:52<10:42, 10759.04\nexamples/s]', '\\rMap:   7%|6         | 486000/7400423 [00:52<10:43, 10747.32\nexamples/s]', '\\rMap:   7%|6         | 488000/7400423 [00:52<10:41, 10768.37\nexamples/s]', '\\rMap:   7%|6         | 490000/7400423 [00:52<10:46, 10684.94\nexamples/s]', '\\rMap:   7%|6         | 492000/7400423 [00:52<10:47, 10669.43\nexamples/s]', '\\rMap:   7%|6         | 494000/7400423 [00:53<10:51, 10607.93\nexamples/s]', '\\rMap:   7%|6         | 496000/7400423 [00:53<10:50, 10608.70\nexamples/s]', '\\rMap:   7%|6         | 498000/7400423 [00:53<10:50, 10613.22\nexamples/s]', '\\rMap:   7%|6         | 500000/7400423 [00:53<10:42, 10743.17\nexamples/s]', '\\rMap:   7%|6         | 502000/7400423 [00:53<10:39, 10784.46\nexamples/s]', '\\rMap:   7%|6         | 504000/7400423 [00:53<10:47, 10648.65\nexamples/s]', '\\rMap:   7%|6         | 506000/7400423 [00:54<11:05, 10363.18\nexamples/s]', '\\rMap:   7%|6         | 508000/7400423 [00:54<10:59, 10449.25\nexamples/s]', '\\rMap:   7%|6         | 510000/7400423 [00:54<10:50, 10595.13\nexamples/s]', '\\rMap:   7%|6         | 512000/7400423 [00:54<10:35, 10837.51\nexamples/s]', '\\rMap:   7%|6         | 514000/7400423 [00:54<10:47, 10632.99\nexamples/s]', '\\rMap:   7%|6         | 516000/7400423 [00:55<11:01, 10403.34\nexamples/s]', '\\rMap:   7%|6         | 518000/7400423 [00:55<11:13, 10223.84\nexamples/s]', '\\rMap:   7%|7         | 520000/7400423 [00:55<11:27, 10012.08\nexamples/s]', '\\rMap:   7%|7         | 522000/7400423 [00:55<11:21, 10098.54\nexamples/s]', '\\rMap:   7%|7         | 524000/7400423 [00:55<11:30, 9963.26\nexamples/s] ', '\\rMap:   7%|7         | 525000/7400423 [00:56<11:35, 9892.26\nexamples/s]', '\\rMap:   7%|7         | 526000/7400423 [00:56<11:39, 9821.23\nexamples/s]', '\\rMap:   7%|7         | 527000/7400423 [00:56<11:41, 9800.32\nexamples/s]', '\\rMap:   7%|7         | 528000/7400423 [00:56<11:45, 9738.39\nexamples/s]', '\\rMap:   7%|7         | 529000/7400423 [00:56<11:46, 9719.86\nexamples/s]', '\\rMap:   7%|7         | 530000/7400423 [00:56<11:45, 9733.72\nexamples/s]', '\\rMap:   7%|7         | 531000/7400423 [00:56<18:09, 6306.41\nexamples/s]', '\\rMap:   7%|7         | 532000/7400423 [00:56<16:23, 6982.58\nexamples/s]', '\\rMap:   7%|7         | 533000/7400423 [00:57<15:08, 7554.97\nexamples/s]', '\\rMap:   7%|7         | 534000/7400423 [00:57<14:15, 8029.46\nexamples/s]', '\\rMap:   7%|7         | 535000/7400423 [00:57<13:29, 8485.55\nexamples/s]', '\\rMap:   7%|7         | 536000/7400423 [00:57<12:55, 8856.44\nexamples/s]', '\\rMap:   7%|7         | 537000/7400423 [00:57<12:30, 9147.40\nexamples/s]', '\\rMap:   7%|7         | 539000/7400423 [00:57<11:44, 9742.65\nexamples/s]', '\\rMap:   7%|7         | 541000/7400423 [00:57<11:19, 10101.08\nexamples/s]', '\\rMap:   7%|7         | 543000/7400423 [00:58<11:06, 10289.81\nexamples/s]', '\\rMap:   7%|7         | 545000/7400423 [00:58<11:09, 10234.64\nexamples/s]', '\\rMap:   7%|7         | 547000/7400423 [00:58<10:52, 10503.19\nexamples/s]', '\\rMap:   7%|7         | 549000/7400423 [00:58<11:03, 10320.48\nexamples/s]', '\\rMap:   7%|7         | 551000/7400423 [00:58<11:03, 10322.69\nexamples/s]', '\\rMap:   7%|7         | 553000/7400423 [00:58<10:56, 10429.30\nexamples/s]', '\\rMap:   7%|7         | 555000/7400423 [00:59<11:15, 10136.52\nexamples/s]', '\\rMap:   8%|7         | 557000/7400423 [00:59<11:42, 9743.46\nexamples/s] ', '\\rMap:   8%|7         | 559000/7400423 [00:59<11:42, 9736.19\nexamples/s]', '\\rMap:   8%|7         | 560000/7400423 [00:59<11:48, 9657.01\nexamples/s]', '\\rMap:   8%|7         | 561000/7400423 [00:59<11:48, 9655.76\nexamples/s]', '\\rMap:   8%|7         | 562000/7400423 [00:59<11:44, 9702.03\nexamples/s]', '\\rMap:   8%|7         | 563000/7400423 [01:00<11:50, 9617.87\nexamples/s]', '\\rMap:   8%|7         | 565000/7400423 [01:00<11:04, 10282.14\nexamples/s]', '\\rMap:   8%|7         | 567000/7400423 [01:00<10:42, 10627.41\nexamples/s]', '\\rMap:   8%|7         | 569000/7400423 [01:00<10:12, 11154.84\nexamples/s]', '\\rMap:   8%|7         | 571000/7400423 [01:00<10:21, 10993.46\nexamples/s]', '\\rMap:   8%|7         | 573000/7400423 [01:00<10:25, 10922.46\nexamples/s]', '\\rMap:   8%|7         | 575000/7400423 [01:01<10:34, 10754.89\nexamples/s]', '\\rMap:   8%|7         | 577000/7400423 [01:01<10:16, 11069.33\nexamples/s]', '\\rMap:   8%|7         | 579000/7400423 [01:01<10:09, 11199.66\nexamples/s]', '\\rMap:   8%|7         | 581000/7400423 [01:01<10:17, 11049.50\nexamples/s]', '\\rMap:   8%|7         | 583000/7400423 [01:01<10:21, 10973.65\nexamples/s]', '\\rMap:   8%|7         | 585000/7400423 [01:02<10:15, 11076.81\nexamples/s]', '\\rMap:   8%|7         | 587000/7400423 [01:02<10:15, 11075.41\nexamples/s]', '\\rMap:   8%|7         | 589000/7400423 [01:02<10:10, 11156.27\nexamples/s]', '\\rMap:   8%|7         | 591000/7400423 [01:02<10:12, 11108.50\nexamples/s]', '\\rMap:   8%|8         | 593000/7400423 [01:02<10:17, 11022.95\nexamples/s]', '\\rMap:   8%|8         | 595000/7400423 [01:02<10:18, 10999.13\nexamples/s]', '\\rMap:   8%|8         | 597000/7400423 [01:03<10:16, 11032.67\nexamples/s]', '\\rMap:   8%|8         | 599000/7400423 [01:03<10:25, 10867.15\nexamples/s]', '\\rMap:   8%|8         | 601000/7400423 [01:03<10:24, 10891.19\nexamples/s]', '\\rMap:   8%|8         | 603000/7400423 [01:03<10:24, 10892.00\nexamples/s]', '\\rMap:   8%|8         | 605000/7400423 [01:03<10:23, 10893.97\nexamples/s]', '\\rMap:   8%|8         | 607000/7400423 [01:04<13:39, 8291.14\nexamples/s] ', '\\rMap:   8%|8         | 608000/7400423 [01:04<13:30, 8382.86\nexamples/s]', '\\rMap:   8%|8         | 609000/7400423 [01:04<13:09, 8599.43\nexamples/s]', '\\rMap:   8%|8         | 610000/7400423 [01:04<13:03, 8661.97\nexamples/s]', '\\rMap:   8%|8         | 611000/7400423 [01:04<12:51, 8802.30\nexamples/s]', '\\rMap:   8%|8         | 612000/7400423 [01:04<13:14, 8544.96\nexamples/s]', '\\rMap:   8%|8         | 613000/7400423 [01:04<13:28, 8395.44\nexamples/s]', '\\rMap:   8%|8         | 614000/7400423 [01:05<13:15, 8527.73\nexamples/s]', '\\rMap:   8%|8         | 616000/7400423 [01:05<12:24, 9112.33\nexamples/s]', '\\rMap:   8%|8         | 617000/7400423 [01:05<12:21, 9154.31\nexamples/s]', '\\rMap:   8%|8         | 618000/7400423 [01:05<12:07, 9329.06\nexamples/s]', '\\rMap:   8%|8         | 619000/7400423 [01:05<11:57, 9452.94\nexamples/s]', '\\rMap:   8%|8         | 621000/7400423 [01:05<11:07, 10159.54\nexamples/s]', '\\rMap:   8%|8         | 623000/7400423 [01:05<11:09, 10125.46\nexamples/s]', '\\rMap:   8%|8         | 625000/7400423 [01:06<11:36, 9730.54\nexamples/s] ', '\\rMap:   8%|8         | 626000/7400423 [01:06<11:43, 9627.02\nexamples/s]', '\\rMap:   8%|8         | 627000/7400423 [01:06<12:07, 9309.11\nexamples/s]', '\\rMap:   8%|8         | 628000/7400423 [01:06<12:45, 8848.12\nexamples/s]', '\\rMap:   8%|8         | 629000/7400423 [01:06<12:54, 8739.75\nexamples/s]', '\\rMap:   9%|8         | 630000/7400423 [01:06<12:43, 8866.55\nexamples/s]', '\\rMap:   9%|8         | 631000/7400423 [01:06<12:25, 9077.72\nexamples/s]', '\\rMap:   9%|8         | 632000/7400423 [01:06<12:26, 9069.28\nexamples/s]', '\\rMap:   9%|8         | 633000/7400423 [01:07<12:21, 9123.88\nexamples/s]', '\\rMap:   9%|8         | 634000/7400423 [01:07<12:42, 8869.13\nexamples/s]', '\\rMap:   9%|8         | 635000/7400423 [01:07<12:43, 8865.25\nexamples/s]', '\\rMap:   9%|8         | 636000/7400423 [01:07<12:54, 8735.62\nexamples/s]', '\\rMap:   9%|8         | 637000/7400423 [01:07<12:42, 8866.63\nexamples/s]', '\\rMap:   9%|8         | 638000/7400423 [01:07<12:36, 8942.59\nexamples/s]', '\\rMap:   9%|8         | 639000/7400423 [01:07<12:33, 8977.89\nexamples/s]', '\\rMap:   9%|8         | 640000/7400423 [01:07<12:30, 9012.41\nexamples/s]', '\\rMap:   9%|8         | 641000/7400423 [01:07<12:11, 9238.79\nexamples/s]', '\\rMap:   9%|8         | 643000/7400423 [01:08<11:52, 9486.60\nexamples/s]', '\\rMap:   9%|8         | 645000/7400423 [01:08<11:43, 9607.37\nexamples/s]', '\\rMap:   9%|8         | 646000/7400423 [01:08<12:07, 9282.28\nexamples/s]', '\\rMap:   9%|8         | 647000/7400423 [01:08<12:06, 9294.28\nexamples/s]', '\\rMap:   9%|8         | 648000/7400423 [01:08<12:15, 9180.12\nexamples/s]', '\\rMap:   9%|8         | 649000/7400423 [01:08<12:18, 9138.64\nexamples/s]', '\\rMap:   9%|8         | 650000/7400423 [01:08<12:17, 9157.67\nexamples/s]', '\\rMap:   9%|8         | 651000/7400423 [01:09<12:15, 9175.28\nexamples/s]', '\\rMap:   9%|8         | 652000/7400423 [01:09<12:16, 9159.67\nexamples/s]', '\\rMap:   9%|8         | 653000/7400423 [01:09<12:16, 9158.83\nexamples/s]', '\\rMap:   9%|8         | 654000/7400423 [01:09<12:17, 9142.49\nexamples/s]', '\\rMap:   9%|8         | 655000/7400423 [01:09<12:21, 9102.75\nexamples/s]', '\\rMap:   9%|8         | 656000/7400423 [01:09<12:22, 9080.23\nexamples/s]', '\\rMap:   9%|8         | 657000/7400423 [01:09<12:20, 9111.79\nexamples/s]', '\\rMap:   9%|8         | 658000/7400423 [01:09<12:35, 8923.89\nexamples/s]', '\\rMap:   9%|8         | 659000/7400423 [01:09<12:31, 8965.65\nexamples/s]', '\\rMap:   9%|8         | 660000/7400423 [01:10<12:48, 8775.70\nexamples/s]', '\\rMap:   9%|8         | 661000/7400423 [01:10<12:47, 8784.84\nexamples/s]', '\\rMap:   9%|8         | 662000/7400423 [01:10<12:56, 8681.76\nexamples/s]', '\\rMap:   9%|8         | 663000/7400423 [01:10<12:45, 8804.96\nexamples/s]', '\\rMap:   9%|8         | 664000/7400423 [01:10<12:48, 8766.44\nexamples/s]', '\\rMap:   9%|8         | 665000/7400423 [01:10<12:35, 8917.29\nexamples/s]', '\\rMap:   9%|8         | 666000/7400423 [01:10<12:30, 8976.32\nexamples/s]', '\\rMap:   9%|9         | 667000/7400423 [01:10<12:20, 9098.09\nexamples/s]', '\\rMap:   9%|9         | 668000/7400423 [01:10<12:27, 9004.87\nexamples/s]', '\\rMap:   9%|9         | 669000/7400423 [01:11<12:44, 8810.17\nexamples/s]', '\\rMap:   9%|9         | 670000/7400423 [01:11<12:41, 8836.48\nexamples/s]', '\\rMap:   9%|9         | 671000/7400423 [01:11<12:46, 8774.54\nexamples/s]', '\\rMap:   9%|9         | 672000/7400423 [01:11<12:41, 8841.42\nexamples/s]', '\\rMap:   9%|9         | 673000/7400423 [01:11<12:35, 8899.26\nexamples/s]', '\\rMap:   9%|9         | 674000/7400423 [01:11<12:23, 9047.68\nexamples/s]', '\\rMap:   9%|9         | 675000/7400423 [01:11<12:17, 9124.63\nexamples/s]', '\\rMap:   9%|9         | 676000/7400423 [01:11<12:06, 9253.71\nexamples/s]', '\\rMap:   9%|9         | 678000/7400423 [01:11<11:13, 9976.36\nexamples/s]', '\\rMap:   9%|9         | 680000/7400423 [01:12<10:35, 10579.60\nexamples/s]', '\\rMap:   9%|9         | 682000/7400423 [01:12<10:13, 10953.80\nexamples/s]', '\\rMap:   9%|9         | 684000/7400423 [01:12<10:03, 11125.43\nexamples/s]', '\\rMap:   9%|9         | 686000/7400423 [01:12<09:58, 11215.95\nexamples/s]', '\\rMap:   9%|9         | 688000/7400423 [01:12<09:44, 11482.85\nexamples/s]', '\\rMap:   9%|9         | 690000/7400423 [01:13<09:38, 11593.83\nexamples/s]', '\\rMap:   9%|9         | 692000/7400423 [01:13<09:36, 11630.17\nexamples/s]', '\\rMap:   9%|9         | 694000/7400423 [01:13<09:53, 11293.22\nexamples/s]', '\\rMap:   9%|9         | 696000/7400423 [01:13<09:56, 11239.36\nexamples/s]', '\\rMap:   9%|9         | 698000/7400423 [01:13<09:47, 11414.54\nexamples/s]', '\\rMap:   9%|9         | 700000/7400423 [01:13<10:00, 11149.49\nexamples/s]', '\\rMap:   9%|9         | 702000/7400423 [01:14<09:46, 11427.49\nexamples/s]', '\\rMap:  10%|9         | 704000/7400423 [01:14<10:02, 11121.89\nexamples/s]', '\\rMap:  10%|9         | 706000/7400423 [01:14<10:26, 10693.00\nexamples/s]', '\\rMap:  10%|9         | 708000/7400423 [01:14<10:40, 10441.23\nexamples/s]', '\\rMap:  10%|9         | 710000/7400423 [01:14<10:47, 10327.28\nexamples/s]', '\\rMap:  10%|9         | 712000/7400423 [01:15<10:54, 10220.55\nexamples/s]', '\\rMap:  10%|9         | 714000/7400423 [01:15<11:01, 10111.95\nexamples/s]', '\\rMap:  10%|9         | 716000/7400423 [01:15<11:05, 10045.73\nexamples/s]', '\\rMap:  10%|9         | 718000/7400423 [01:15<11:10, 9969.07\nexamples/s] ', '\\rMap:  10%|9         | 720000/7400423 [01:15<11:02, 10078.07\nexamples/s]', '\\rMap:  10%|9         | 722000/7400423 [01:16<10:51, 10250.39\nexamples/s]', '\\rMap:  10%|9         | 724000/7400423 [01:16<10:47, 10310.63\nexamples/s]', '\\rMap:  10%|9         | 726000/7400423 [01:16<11:00, 10108.61\nexamples/s]', '\\rMap:  10%|9         | 728000/7400423 [01:16<11:17, 9848.69\nexamples/s] ', '\\rMap:  10%|9         | 730000/7400423 [01:16<11:13, 9898.31\nexamples/s]', '\\rMap:  10%|9         | 731000/7400423 [01:16<11:13, 9905.37\nexamples/s]', '\\rMap:  10%|9         | 732000/7400423 [01:17<11:16, 9855.10\nexamples/s]', '\\rMap:  10%|9         | 733000/7400423 [01:17<16:05, 6907.02\nexamples/s]', '\\rMap:  10%|9         | 735000/7400423 [01:17<14:10, 7840.29\nexamples/s]', '\\rMap:  10%|9         | 737000/7400423 [01:17<13:00, 8538.71\nexamples/s]', '\\rMap:  10%|9         | 739000/7400423 [01:17<12:17, 9031.15\nexamples/s]', '\\rMap:  10%|#         | 741000/7400423 [01:18<11:56, 9290.55\nexamples/s]', '\\rMap:  10%|#         | 742000/7400423 [01:18<11:47, 9413.71\nexamples/s]', '\\rMap:  10%|#         | 744000/7400423 [01:18<11:19, 9791.38\nexamples/s]', '\\rMap:  10%|#         | 745000/7400423 [01:18<11:19, 9793.98\nexamples/s]', '\\rMap:  10%|#         | 747000/7400423 [01:18<11:05, 10000.01\nexamples/s]', '\\rMap:  10%|#         | 749000/7400423 [01:18<10:52, 10187.04\nexamples/s]', '\\rMap:  10%|#         | 751000/7400423 [01:19<10:53, 10174.54\nexamples/s]', '\\rMap:  10%|#         | 753000/7400423 [01:19<10:54, 10157.61\nexamples/s]', '\\rMap:  10%|#         | 755000/7400423 [01:19<10:53, 10167.44\nexamples/s]', '\\rMap:  10%|#         | 757000/7400423 [01:19<10:45, 10285.28\nexamples/s]', '\\rMap:  10%|#         | 759000/7400423 [01:19<10:48, 10241.15\nexamples/s]', '\\rMap:  10%|#         | 761000/7400423 [01:20<10:53, 10166.06\nexamples/s]', '\\rMap:  10%|#         | 763000/7400423 [01:20<10:47, 10252.08\nexamples/s]', '\\rMap:  10%|#         | 765000/7400423 [01:20<10:48, 10226.53\nexamples/s]', '\\rMap:  10%|#         | 767000/7400423 [01:20<10:48, 10231.20\nexamples/s]', '\\rMap:  10%|#         | 769000/7400423 [01:20<10:52, 10155.97\nexamples/s]', '\\rMap:  10%|#         | 771000/7400423 [01:21<11:30, 9598.22\nexamples/s] ', '\\rMap:  10%|#         | 772000/7400423 [01:21<12:22, 8929.15\nexamples/s]', '\\rMap:  10%|#         | 773000/7400423 [01:21<13:22, 8253.67\nexamples/s]', '\\rMap:  10%|#         | 774000/7400423 [01:21<14:03, 7855.26\nexamples/s]', '\\rMap:  10%|#         | 775000/7400423 [01:21<14:54, 7409.56\nexamples/s]', '\\rMap:  10%|#         | 776000/7400423 [01:21<15:23, 7172.26\nexamples/s]', '\\rMap:  10%|#         | 777000/7400423 [01:22<15:25, 7154.45\nexamples/s]', '\\rMap:  11%|#         | 778000/7400423 [01:22<15:47, 6985.72\nexamples/s]', '\\rMap:  11%|#         | 779000/7400423 [01:22<15:30, 7112.75\nexamples/s]', '\\rMap:  11%|#         | 780000/7400423 [01:22<15:05, 7313.52\nexamples/s]', '\\rMap:  11%|#         | 781000/7400423 [01:22<14:48, 7450.57\nexamples/s]', '\\rMap:  11%|#         | 782000/7400423 [01:22<14:43, 7492.10\nexamples/s]', '\\rMap:  11%|#         | 783000/7400423 [01:22<14:28, 7620.36\nexamples/s]', '\\rMap:  11%|#         | 784000/7400423 [01:22<14:23, 7658.35\nexamples/s]', '\\rMap:  11%|#         | 785000/7400423 [01:23<14:23, 7657.50\nexamples/s]', '\\rMap:  11%|#         | 786000/7400423 [01:23<14:13, 7746.43\nexamples/s]', '\\rMap:  11%|#         | 787000/7400423 [01:23<14:01, 7860.26\nexamples/s]', '\\rMap:  11%|#         | 788000/7400423 [01:23<14:17, 7714.19\nexamples/s]', '\\rMap:  11%|#         | 789000/7400423 [01:23<14:19, 7688.63\nexamples/s]', '\\rMap:  11%|#         | 790000/7400423 [01:23<15:10, 7259.27\nexamples/s]', '\\rMap:  11%|#         | 791000/7400423 [01:23<15:11, 7249.90\nexamples/s]', '\\rMap:  11%|#         | 792000/7400423 [01:24<15:48, 6970.04\nexamples/s]', '\\rMap:  11%|#         | 793000/7400423 [01:24<15:41, 7019.00\nexamples/s]', '\\rMap:  11%|#         | 794000/7400423 [01:24<16:03, 6853.77\nexamples/s]', '\\rMap:  11%|#         | 795000/7400423 [01:24<15:27, 7118.23\nexamples/s]', '\\rMap:  11%|#         | 796000/7400423 [01:24<15:25, 7133.60\nexamples/s]', '\\rMap:  11%|#         | 797000/7400423 [01:24<15:20, 7174.76\nexamples/s]', '\\rMap:  11%|#         | 798000/7400423 [01:24<15:23, 7148.07\nexamples/s]', '\\rMap:  11%|#         | 799000/7400423 [01:25<15:16, 7205.80\nexamples/s]', '\\rMap:  11%|#         | 800000/7400423 [01:25<14:59, 7339.08\nexamples/s]', '\\rMap:  11%|#         | 801000/7400423 [01:25<15:01, 7320.52\nexamples/s]', '\\rMap:  11%|#         | 802000/7400423 [01:25<15:19, 7172.40\nexamples/s]', '\\rMap:  11%|#         | 803000/7400423 [01:25<15:35, 7049.62\nexamples/s]', '\\rMap:  11%|#         | 804000/7400423 [01:25<15:19, 7175.14\nexamples/s]', '\\rMap:  11%|#         | 805000/7400423 [01:25<14:56, 7357.92\nexamples/s]', '\\rMap:  11%|#         | 806000/7400423 [01:25<14:54, 7372.08\nexamples/s]', '\\rMap:  11%|#         | 807000/7400423 [01:26<14:52, 7386.87\nexamples/s]', '\\rMap:  11%|#         | 808000/7400423 [01:26<14:51, 7392.30\nexamples/s]', '\\rMap:  11%|#         | 809000/7400423 [01:26<14:46, 7436.81\nexamples/s]', '\\rMap:  11%|#         | 810000/7400423 [01:26<14:24, 7623.27\nexamples/s]', '\\rMap:  11%|#         | 811000/7400423 [01:26<14:25, 7610.75\nexamples/s]', '\\rMap:  11%|#         | 812000/7400423 [01:26<14:36, 7515.16\nexamples/s]', '\\rMap:  11%|#         | 813000/7400423 [01:26<15:14, 7204.61\nexamples/s]', '\\rMap:  11%|#         | 814000/7400423 [01:27<15:41, 6998.37\nexamples/s]', '\\rMap:  11%|#1        | 815000/7400423 [01:27<15:48, 6942.81\nexamples/s]', '\\rMap:  11%|#1        | 816000/7400423 [01:27<15:44, 6972.45\nexamples/s]', '\\rMap:  11%|#1        | 817000/7400423 [01:27<15:58, 6868.40\nexamples/s]', '\\rMap:  11%|#1        | 818000/7400423 [01:27<15:46, 6951.49\nexamples/s]', '\\rMap:  11%|#1        | 819000/7400423 [01:27<15:26, 7100.38\nexamples/s]', '\\rMap:  11%|#1        | 820000/7400423 [01:27<15:10, 7227.68\nexamples/s]', '\\rMap:  11%|#1        | 821000/7400423 [01:28<14:56, 7341.77\nexamples/s]', '\\rMap:  11%|#1        | 822000/7400423 [01:28<14:39, 7483.95\nexamples/s]', '\\rMap:  11%|#1        | 823000/7400423 [01:28<14:26, 7588.04\nexamples/s]', '\\rMap:  11%|#1        | 824000/7400423 [01:28<14:35, 7508.84\nexamples/s]', '\\rMap:  11%|#1        | 825000/7400423 [01:28<14:16, 7676.67\nexamples/s]', '\\rMap:  11%|#1        | 826000/7400423 [01:28<14:26, 7587.12\nexamples/s]', '\\rMap:  11%|#1        | 827000/7400423 [01:28<14:42, 7450.65\nexamples/s]', '\\rMap:  11%|#1        | 828000/7400423 [01:29<15:05, 7259.84\nexamples/s]', '\\rMap:  11%|#1        | 829000/7400423 [01:29<15:06, 7253.04\nexamples/s]', '\\rMap:  11%|#1        | 830000/7400423 [01:29<14:56, 7330.02\nexamples/s]', '\\rMap:  11%|#1        | 831000/7400423 [01:29<14:25, 7591.52\nexamples/s]', '\\rMap:  11%|#1        | 832000/7400423 [01:29<21:09, 5173.16\nexamples/s]', '\\rMap:  11%|#1        | 833000/7400423 [01:29<19:03, 5742.71\nexamples/s]', '\\rMap:  11%|#1        | 834000/7400423 [01:29<17:30, 6250.21\nexamples/s]', '\\rMap:  11%|#1        | 835000/7400423 [01:30<16:39, 6568.62\nexamples/s]', '\\rMap:  11%|#1        | 836000/7400423 [01:30<15:49, 6914.24\nexamples/s]', '\\rMap:  11%|#1        | 837000/7400423 [01:30<15:03, 7266.87\nexamples/s]', '\\rMap:  11%|#1        | 838000/7400423 [01:30<14:35, 7493.73\nexamples/s]', '\\rMap:  11%|#1        | 839000/7400423 [01:30<14:29, 7542.39\nexamples/s]', '\\rMap:  11%|#1        | 840000/7400423 [01:30<14:12, 7699.89\nexamples/s]', '\\rMap:  11%|#1        | 841000/7400423 [01:30<14:08, 7730.81\nexamples/s]', '\\rMap:  11%|#1        | 842000/7400423 [01:31<14:40, 7444.93\nexamples/s]', '\\rMap:  11%|#1        | 843000/7400423 [01:31<14:29, 7538.96\nexamples/s]', '\\rMap:  11%|#1        | 844000/7400423 [01:31<14:51, 7357.70\nexamples/s]', '\\rMap:  11%|#1        | 845000/7400423 [01:31<15:20, 7122.67\nexamples/s]', '\\rMap:  11%|#1        | 846000/7400423 [01:31<15:15, 7162.26\nexamples/s]', '\\rMap:  11%|#1        | 847000/7400423 [01:31<15:19, 7126.12\nexamples/s]', '\\rMap:  11%|#1        | 848000/7400423 [01:31<15:17, 7140.84\nexamples/s]', '\\rMap:  11%|#1        | 849000/7400423 [01:32<15:14, 7165.54\nexamples/s]', '\\rMap:  11%|#1        | 850000/7400423 [01:32<15:18, 7129.71\nexamples/s]', '\\rMap:  11%|#1        | 851000/7400423 [01:32<15:27, 7061.38\nexamples/s]', '\\rMap:  12%|#1        | 852000/7400423 [01:32<15:12, 7174.92\nexamples/s]', '\\rMap:  12%|#1        | 853000/7400423 [01:32<15:05, 7233.66\nexamples/s]', '\\rMap:  12%|#1        | 854000/7400423 [01:32<14:53, 7328.69\nexamples/s]', '\\rMap:  12%|#1        | 855000/7400423 [01:32<14:49, 7360.13\nexamples/s]', '\\rMap:  12%|#1        | 856000/7400423 [01:32<14:53, 7324.64\nexamples/s]', '\\rMap:  12%|#1        | 857000/7400423 [01:33<14:59, 7277.01\nexamples/s]', '\\rMap:  12%|#1        | 858000/7400423 [01:33<14:45, 7390.69\nexamples/s]', '\\rMap:  12%|#1        | 859000/7400423 [01:33<14:47, 7368.21\nexamples/s]', '\\rMap:  12%|#1        | 860000/7400423 [01:33<14:36, 7464.59\nexamples/s]', '\\rMap:  12%|#1        | 861000/7400423 [01:33<14:35, 7465.52\nexamples/s]', '\\rMap:  12%|#1        | 862000/7400423 [01:33<13:32, 8048.42\nexamples/s]', '\\rMap:  12%|#1        | 864000/7400423 [01:33<12:04, 9023.01\nexamples/s]', '\\rMap:  12%|#1        | 866000/7400423 [01:34<11:26, 9517.10\nexamples/s]', '\\rMap:  12%|#1        | 868000/7400423 [01:34<10:57, 9928.79\nexamples/s]', '\\rMap:  12%|#1        | 870000/7400423 [01:34<10:46, 10101.12\nexamples/s]', '\\rMap:  12%|#1        | 872000/7400423 [01:34<10:27, 10401.90\nexamples/s]', '\\rMap:  12%|#1        | 874000/7400423 [01:34<10:31, 10341.03\nexamples/s]', '\\rMap:  12%|#1        | 876000/7400423 [01:35<10:40, 10183.60\nexamples/s]', '\\rMap:  12%|#1        | 878000/7400423 [01:35<10:42, 10156.19\nexamples/s]', '\\rMap:  12%|#1        | 880000/7400423 [01:35<10:52, 9996.15\nexamples/s] ', '\\rMap:  12%|#1        | 881000/7400423 [01:35<10:53, 9976.01\nexamples/s]', '\\rMap:  12%|#1        | 882000/7400423 [01:35<10:55, 9939.27\nexamples/s]', '\\rMap:  12%|#1        | 884000/7400423 [01:35<10:54, 9961.94\nexamples/s]', '\\rMap:  12%|#1        | 886000/7400423 [01:36<10:51, 9991.98\nexamples/s]', '\\rMap:  12%|#1        | 888000/7400423 [01:36<10:44, 10109.31\nexamples/s]', '\\rMap:  12%|#2        | 890000/7400423 [01:36<10:48, 10046.14\nexamples/s]', '\\rMap:  12%|#2        | 892000/7400423 [01:36<11:06, 9768.97\nexamples/s] ', '\\rMap:  12%|#2        | 893000/7400423 [01:36<11:08, 9729.26\nexamples/s]', '\\rMap:  12%|#2        | 895000/7400423 [01:36<10:53, 9957.65\nexamples/s]', '\\rMap:  12%|#2        | 897000/7400423 [01:37<10:48, 10034.33\nexamples/s]', '\\rMap:  12%|#2        | 899000/7400423 [01:37<10:33, 10267.47\nexamples/s]', '\\rMap:  12%|#2        | 901000/7400423 [01:37<10:21, 10460.89\nexamples/s]', '\\rMap:  12%|#2        | 903000/7400423 [01:37<10:23, 10421.12\nexamples/s]', '\\rMap:  12%|#2        | 905000/7400423 [01:37<10:19, 10492.70\nexamples/s]', '\\rMap:  12%|#2        | 907000/7400423 [01:38<10:12, 10593.84\nexamples/s]', '\\rMap:  12%|#2        | 909000/7400423 [01:38<10:10, 10640.33\nexamples/s]', '\\rMap:  12%|#2        | 911000/7400423 [01:38<10:05, 10722.42\nexamples/s]', '\\rMap:  12%|#2        | 913000/7400423 [01:38<10:01, 10778.37\nexamples/s]', '\\rMap:  12%|#2        | 915000/7400423 [01:38<10:01, 10790.77\nexamples/s]', '\\rMap:  12%|#2        | 917000/7400423 [01:39<10:01, 10772.02\nexamples/s]', '\\rMap:  12%|#2        | 919000/7400423 [01:39<13:01, 8294.32\nexamples/s] ', '\\rMap:  12%|#2        | 921000/7400423 [01:39<12:13, 8837.66\nexamples/s]', '\\rMap:  12%|#2        | 923000/7400423 [01:39<11:49, 9132.84\nexamples/s]', '\\rMap:  12%|#2        | 924000/7400423 [01:39<11:40, 9246.42\nexamples/s]', '\\rMap:  13%|#2        | 926000/7400423 [01:40<11:14, 9604.21\nexamples/s]', '\\rMap:  13%|#2        | 928000/7400423 [01:40<10:59, 9811.70\nexamples/s]', '\\rMap:  13%|#2        | 930000/7400423 [01:40<10:42, 10078.11\nexamples/s]', '\\rMap:  13%|#2        | 932000/7400423 [01:40<10:29, 10275.00\nexamples/s]', '\\rMap:  13%|#2        | 934000/7400423 [01:40<10:18, 10453.41\nexamples/s]', '\\rMap:  13%|#2        | 936000/7400423 [01:41<10:24, 10355.45\nexamples/s]', '\\rMap:  13%|#2        | 938000/7400423 [01:41<10:27, 10298.70\nexamples/s]', '\\rMap:  13%|#2        | 940000/7400423 [01:41<10:43, 10044.41\nexamples/s]', '\\rMap:  13%|#2        | 942000/7400423 [01:41<11:04, 9724.33\nexamples/s] ', '\\rMap:  13%|#2        | 943000/7400423 [01:41<11:09, 9651.28\nexamples/s]', '\\rMap:  13%|#2        | 944000/7400423 [01:41<11:15, 9562.45\nexamples/s]', '\\rMap:  13%|#2        | 945000/7400423 [01:42<11:22, 9459.88\nexamples/s]', '\\rMap:  13%|#2        | 946000/7400423 [01:42<11:23, 9439.75\nexamples/s]', '\\rMap:  13%|#2        | 947000/7400423 [01:42<11:39, 9229.41\nexamples/s]', '\\rMap:  13%|#2        | 948000/7400423 [01:42<11:31, 9332.92\nexamples/s]', '\\rMap:  13%|#2        | 949000/7400423 [01:42<11:34, 9290.66\nexamples/s]', '\\rMap:  13%|#2        | 950000/7400423 [01:42<11:34, 9286.27\nexamples/s]', '\\rMap:  13%|#2        | 951000/7400423 [01:42<11:33, 9294.38\nexamples/s]', '\\rMap:  13%|#2        | 952000/7400423 [01:42<11:24, 9420.15\nexamples/s]', '\\rMap:  13%|#2        | 953000/7400423 [01:42<11:36, 9250.35\nexamples/s]', '\\rMap:  13%|#2        | 954000/7400423 [01:42<11:56, 8998.37\nexamples/s]', '\\rMap:  13%|#2        | 955000/7400423 [01:43<11:49, 9088.05\nexamples/s]', '\\rMap:  13%|#2        | 956000/7400423 [01:43<11:40, 9196.53\nexamples/s]', '\\rMap:  13%|#2        | 957000/7400423 [01:43<11:46, 9116.72\nexamples/s]', '\\rMap:  13%|#2        | 958000/7400423 [01:43<11:48, 9097.21\nexamples/s]', '\\rMap:  13%|#2        | 959000/7400423 [01:43<11:41, 9188.64\nexamples/s]', '\\rMap:  13%|#2        | 960000/7400423 [01:43<11:33, 9293.41\nexamples/s]', '\\rMap:  13%|#2        | 961000/7400423 [01:43<11:32, 9294.96\nexamples/s]', '\\rMap:  13%|#2        | 962000/7400423 [01:43<11:51, 9052.24\nexamples/s]', '\\rMap:  13%|#3        | 963000/7400423 [01:43<11:48, 9085.56\nexamples/s]', '\\rMap:  13%|#3        | 964000/7400423 [01:44<11:48, 9088.79\nexamples/s]', '\\rMap:  13%|#3        | 966000/7400423 [01:44<11:21, 9435.97\nexamples/s]', '\\rMap:  13%|#3        | 968000/7400423 [01:44<11:01, 9719.99\nexamples/s]', '\\rMap:  13%|#3        | 970000/7400423 [01:44<10:35, 10114.82\nexamples/s]', '\\rMap:  13%|#3        | 972000/7400423 [01:44<10:04, 10636.51\nexamples/s]', '\\rMap:  13%|#3        | 974000/7400423 [01:45<09:52, 10849.14\nexamples/s]', '\\rMap:  13%|#3        | 976000/7400423 [01:45<09:49, 10897.22\nexamples/s]', '\\rMap:  13%|#3        | 978000/7400423 [01:45<09:47, 10928.90\nexamples/s]', '\\rMap:  13%|#3        | 980000/7400423 [01:45<09:51, 10858.37\nexamples/s]', '\\rMap:  13%|#3        | 982000/7400423 [01:45<09:55, 10776.45\nexamples/s]', '\\rMap:  13%|#3        | 984000/7400423 [01:45<09:52, 10835.13\nexamples/s]', '\\rMap:  13%|#3        | 986000/7400423 [01:46<09:50, 10859.78\nexamples/s]', '\\rMap:  13%|#3        | 988000/7400423 [01:46<09:47, 10905.77\nexamples/s]', '\\rMap:  13%|#3        | 990000/7400423 [01:46<09:44, 10965.32\nexamples/s]', '\\rMap:  13%|#3        | 992000/7400423 [01:46<10:06, 10564.43\nexamples/s]', '\\rMap:  13%|#3        | 994000/7400423 [01:46<10:19, 10347.91\nexamples/s]', '\\rMap:  13%|#3        | 996000/7400423 [01:47<13:41, 7797.50\nexamples/s] ', '\\rMap:  13%|#3        | 997000/7400423 [01:47<13:11, 8094.40\nexamples/s]', '\\rMap:  13%|#3        | 999000/7400423 [01:47<12:17, 8679.79\nexamples/s]', '\\rMap:  14%|#3        | 1001000/7400423 [01:47<11:44, 9088.85\nexamples/s]', '\\rMap:  14%|#3        | 1003000/7400423 [01:47<11:03, 9644.24\nexamples/s]', '\\rMap:  14%|#3        | 1005000/7400423 [01:48<10:44, 9916.25\nexamples/s]', '\\rMap:  14%|#3        | 1007000/7400423 [01:48<10:22, 10278.08\nexamples/s]', '\\rMap:  14%|#3        | 1009000/7400423 [01:48<10:06, 10542.52\nexamples/s]', '\\rMap:  14%|#3        | 1011000/7400423 [01:48<09:48, 10851.98\nexamples/s]', '\\rMap:  14%|#3        | 1013000/7400423 [01:48<09:33, 11145.66\nexamples/s]', '\\rMap:  14%|#3        | 1015000/7400423 [01:49<09:33, 11131.89\nexamples/s]', '\\rMap:  14%|#3        | 1017000/7400423 [01:49<09:28, 11230.50\nexamples/s]', '\\rMap:  14%|#3        | 1019000/7400423 [01:49<09:25, 11284.61\nexamples/s]', '\\rMap:  14%|#3        | 1021000/7400423 [01:49<09:27, 11238.55\nexamples/s]', '\\rMap:  14%|#3        | 1023000/7400423 [01:49<09:16, 11460.83\nexamples/s]', '\\rMap:  14%|#3        | 1025000/7400423 [01:49<09:27, 11235.60\nexamples/s]', '\\rMap:  14%|#3        | 1027000/7400423 [01:50<09:25, 11265.83\nexamples/s]', '\\rMap:  14%|#3        | 1029000/7400423 [01:50<09:28, 11214.49\nexamples/s]', '\\rMap:  14%|#3        | 1031000/7400423 [01:50<09:24, 11280.48\nexamples/s]', '\\rMap:  14%|#3        | 1033000/7400423 [01:50<09:32, 11125.33\nexamples/s]', '\\rMap:  14%|#3        | 1035000/7400423 [01:50<09:44, 10896.95\nexamples/s]', '\\rMap:  14%|#4        | 1037000/7400423 [01:51<09:43, 10897.51\nexamples/s]', '\\rMap:  14%|#4        | 1039000/7400423 [01:51<09:49, 10789.05\nexamples/s]', '\\rMap:  14%|#4        | 1041000/7400423 [01:51<09:49, 10782.56\nexamples/s]', '\\rMap:  14%|#4        | 1043000/7400423 [01:51<09:48, 10807.89\nexamples/s]', '\\rMap:  14%|#4        | 1045000/7400423 [01:51<09:43, 10898.42\nexamples/s]', '\\rMap:  14%|#4        | 1047000/7400423 [01:51<09:39, 10956.64\nexamples/s]', '\\rMap:  14%|#4        | 1049000/7400423 [01:52<09:24, 11258.23\nexamples/s]', '\\rMap:  14%|#4        | 1051000/7400423 [01:52<09:16, 11419.48\nexamples/s]', '\\rMap:  14%|#4        | 1053000/7400423 [01:52<09:08, 11582.21\nexamples/s]', '\\rMap:  14%|#4        | 1055000/7400423 [01:52<09:05, 11637.45\nexamples/s]', '\\rMap:  14%|#4        | 1057000/7400423 [01:52<09:02, 11688.88\nexamples/s]', '\\rMap:  14%|#4        | 1059000/7400423 [01:52<09:03, 11663.65\nexamples/s]', '\\rMap:  14%|#4        | 1061000/7400423 [01:53<09:07, 11582.01\nexamples/s]', '\\rMap:  14%|#4        | 1063000/7400423 [01:53<09:14, 11432.23\nexamples/s]', '\\rMap:  14%|#4        | 1065000/7400423 [01:53<09:18, 11340.04\nexamples/s]', '\\rMap:  14%|#4        | 1067000/7400423 [01:53<09:19, 11325.91\nexamples/s]', '\\rMap:  14%|#4        | 1069000/7400423 [01:53<09:18, 11338.72\nexamples/s]', '\\rMap:  14%|#4        | 1071000/7400423 [01:54<09:15, 11399.37\nexamples/s]', '\\rMap:  14%|#4        | 1073000/7400423 [01:54<09:08, 11545.72\nexamples/s]', '\\rMap:  15%|#4        | 1075000/7400423 [01:54<09:05, 11596.14\nexamples/s]', '\\rMap:  15%|#4        | 1077000/7400423 [01:54<09:56, 10593.45\nexamples/s]', '\\rMap:  15%|#4        | 1079000/7400423 [01:54<10:31, 10008.87\nexamples/s]', '\\rMap:  15%|#4        | 1081000/7400423 [01:54<10:27, 10073.00\nexamples/s]', '\\rMap:  15%|#4        | 1083000/7400423 [01:55<10:27, 10071.33\nexamples/s]', '\\rMap:  15%|#4        | 1085000/7400423 [01:55<10:18, 10213.09\nexamples/s]', '\\rMap:  15%|#4        | 1087000/7400423 [01:55<10:18, 10205.12\nexamples/s]', '\\rMap:  15%|#4        | 1089000/7400423 [01:55<10:14, 10270.49\nexamples/s]', '\\rMap:  15%|#4        | 1091000/7400423 [01:55<10:01, 10490.32\nexamples/s]', '\\rMap:  15%|#4        | 1093000/7400423 [01:56<09:59, 10521.00\nexamples/s]', '\\rMap:  15%|#4        | 1095000/7400423 [01:56<10:05, 10418.88\nexamples/s]', '\\rMap:  15%|#4        | 1097000/7400423 [01:56<10:11, 10315.56\nexamples/s]', '\\rMap:  15%|#4        | 1099000/7400423 [01:56<10:09, 10330.81\nexamples/s]', '\\rMap:  15%|#4        | 1101000/7400423 [01:56<10:12, 10282.77\nexamples/s]', '\\rMap:  15%|#4        | 1103000/7400423 [01:57<10:23, 10096.82\nexamples/s]', '\\rMap:  15%|#4        | 1105000/7400423 [01:57<10:18, 10171.54\nexamples/s]', '\\rMap:  15%|#4        | 1107000/7400423 [01:57<10:10, 10305.05\nexamples/s]', '\\rMap:  15%|#4        | 1109000/7400423 [01:57<10:20, 10141.12\nexamples/s]', '\\rMap:  15%|#5        | 1111000/7400423 [01:57<10:16, 10195.95\nexamples/s]', '\\rMap:  15%|#5        | 1113000/7400423 [01:58<10:34, 9910.65\nexamples/s] ', '\\rMap:  15%|#5        | 1114000/7400423 [01:58<10:39, 9826.39\nexamples/s]', '\\rMap:  15%|#5        | 1116000/7400423 [01:58<10:37, 9860.86\nexamples/s]', '\\rMap:  15%|#5        | 1118000/7400423 [01:58<10:33, 9915.30\nexamples/s]', '\\rMap:  15%|#5        | 1120000/7400423 [01:58<10:29, 9975.59\nexamples/s]', '\\rMap:  15%|#5        | 1121000/7400423 [01:59<14:42, 7115.62\nexamples/s]', '\\rMap:  15%|#5        | 1122000/7400423 [01:59<13:52, 7537.76\nexamples/s]', '\\rMap:  15%|#5        | 1124000/7400423 [01:59<12:25, 8419.02\nexamples/s]', '\\rMap:  15%|#5        | 1125000/7400423 [01:59<12:01, 8699.03\nexamples/s]', '\\rMap:  15%|#5        | 1126000/7400423 [01:59<11:42, 8933.34\nexamples/s]', '\\rMap:  15%|#5        | 1128000/7400423 [01:59<10:49, 9661.65\nexamples/s]', '\\rMap:  15%|#5        | 1130000/7400423 [01:59<09:58, 10476.58\nexamples/s]', '\\rMap:  15%|#5        | 1132000/7400423 [02:00<09:32, 10942.13\nexamples/s]', '\\rMap:  15%|#5        | 1134000/7400423 [02:00<09:15, 11286.69\nexamples/s]', '\\rMap:  15%|#5        | 1136000/7400423 [02:00<09:24, 11105.28\nexamples/s]', '\\rMap:  15%|#5        | 1138000/7400423 [02:00<09:36, 10866.95\nexamples/s]', '\\rMap:  15%|#5        | 1140000/7400423 [02:00<10:01, 10403.90\nexamples/s]', '\\rMap:  15%|#5        | 1142000/7400423 [02:01<10:13, 10196.40\nexamples/s]', '\\rMap:  15%|#5        | 1144000/7400423 [02:01<10:37, 9807.75\nexamples/s] ', '\\rMap:  15%|#5        | 1145000/7400423 [02:01<10:47, 9663.32\nexamples/s]', '\\rMap:  15%|#5        | 1146000/7400423 [02:01<10:47, 9656.57\nexamples/s]', '\\rMap:  16%|#5        | 1148000/7400423 [02:01<10:24, 10006.43\nexamples/s]', '\\rMap:  16%|#5        | 1150000/7400423 [02:01<10:00, 10404.22\nexamples/s]', '\\rMap:  16%|#5        | 1152000/7400423 [02:02<09:45, 10664.75\nexamples/s]', '\\rMap:  16%|#5        | 1154000/7400423 [02:02<09:27, 11003.57\nexamples/s]', '\\rMap:  16%|#5        | 1156000/7400423 [02:02<09:24, 11056.59\nexamples/s]', '\\rMap:  16%|#5        | 1158000/7400423 [02:02<09:12, 11293.10\nexamples/s]', '\\rMap:  16%|#5        | 1160000/7400423 [02:02<09:05, 11432.36\nexamples/s]', '\\rMap:  16%|#5        | 1162000/7400423 [02:02<08:58, 11590.95\nexamples/s]', '\\rMap:  16%|#5        | 1164000/7400423 [02:03<09:26, 11006.02\nexamples/s]', '\\rMap:  16%|#5        | 1166000/7400423 [02:03<10:04, 10319.21\nexamples/s]', '\\rMap:  16%|#5        | 1168000/7400423 [02:03<10:37, 9779.31\nexamples/s] ', '\\rMap:  16%|#5        | 1169000/7400423 [02:03<10:45, 9650.40\nexamples/s]', '\\rMap:  16%|#5        | 1170000/7400423 [02:03<10:58, 9468.61\nexamples/s]', '\\rMap:  16%|#5        | 1171000/7400423 [02:03<10:59, 9448.03\nexamples/s]', '\\rMap:  16%|#5        | 1172000/7400423 [02:04<10:54, 9511.44\nexamples/s]', '\\rMap:  16%|#5        | 1173000/7400423 [02:04<10:50, 9577.44\nexamples/s]', '\\rMap:  16%|#5        | 1174000/7400423 [02:04<10:50, 9575.07\nexamples/s]', '\\rMap:  16%|#5        | 1175000/7400423 [02:04<10:44, 9659.99\nexamples/s]', '\\rMap:  16%|#5        | 1176000/7400423 [02:04<10:45, 9648.31\nexamples/s]', '\\rMap:  16%|#5        | 1177000/7400423 [02:04<10:39, 9726.33\nexamples/s]', '\\rMap:  16%|#5        | 1178000/7400423 [02:04<10:46, 9629.23\nexamples/s]', '\\rMap:  16%|#5        | 1180000/7400423 [02:04<10:52, 9535.02\nexamples/s]', '\\rMap:  16%|#5        | 1181000/7400423 [02:04<10:57, 9463.83\nexamples/s]', '\\rMap:  16%|#5        | 1182000/7400423 [02:05<10:55, 9483.92\nexamples/s]', '\\rMap:  16%|#5        | 1183000/7400423 [02:05<10:52, 9523.99\nexamples/s]', '\\rMap:  16%|#5        | 1184000/7400423 [02:05<10:52, 9532.70\nexamples/s]', '\\rMap:  16%|#6        | 1185000/7400423 [02:05<10:52, 9531.52\nexamples/s]', '\\rMap:  16%|#6        | 1186000/7400423 [02:05<10:50, 9549.22\nexamples/s]', '\\rMap:  16%|#6        | 1187000/7400423 [02:05<11:06, 9315.57\nexamples/s]', '\\rMap:  16%|#6        | 1188000/7400423 [02:05<11:07, 9310.52\nexamples/s]', '\\rMap:  16%|#6        | 1189000/7400423 [02:05<11:00, 9401.84\nexamples/s]', '\\rMap:  16%|#6        | 1190000/7400423 [02:05<10:59, 9410.20\nexamples/s]', '\\rMap:  16%|#6        | 1191000/7400423 [02:06<11:01, 9383.91\nexamples/s]', '\\rMap:  16%|#6        | 1192000/7400423 [02:06<10:57, 9445.99\nexamples/s]', '\\rMap:  16%|#6        | 1193000/7400423 [02:06<10:54, 9489.57\nexamples/s]', '\\rMap:  16%|#6        | 1194000/7400423 [02:06<11:04, 9333.84\nexamples/s]', '\\rMap:  16%|#6        | 1195000/7400423 [02:06<11:02, 9359.65\nexamples/s]', '\\rMap:  16%|#6        | 1196000/7400423 [02:06<10:52, 9505.39\nexamples/s]', '\\rMap:  16%|#6        | 1198000/7400423 [02:06<10:43, 9632.96\nexamples/s]', '\\rMap:  16%|#6        | 1199000/7400423 [02:06<10:44, 9626.89\nexamples/s]', '\\rMap:  16%|#6        | 1200000/7400423 [02:06<10:57, 9431.11\nexamples/s]', '\\rMap:  16%|#6        | 1201000/7400423 [02:07<11:02, 9360.23\nexamples/s]', '\\rMap:  16%|#6        | 1202000/7400423 [02:07<11:00, 9377.66\nexamples/s]', '\\rMap:  16%|#6        | 1203000/7400423 [02:07<11:13, 9201.13\nexamples/s]', '\\rMap:  16%|#6        | 1204000/7400423 [02:07<11:11, 9220.99\nexamples/s]', '\\rMap:  16%|#6        | 1205000/7400423 [02:07<11:32, 8944.31\nexamples/s]', '\\rMap:  16%|#6        | 1206000/7400423 [02:07<11:25, 9033.47\nexamples/s]', '\\rMap:  16%|#6        | 1207000/7400423 [02:07<11:06, 9299.00\nexamples/s]', '\\rMap:  16%|#6        | 1208000/7400423 [02:07<10:53, 9482.55\nexamples/s]', '\\rMap:  16%|#6        | 1209000/7400423 [02:07<10:42, 9631.10\nexamples/s]', '\\rMap:  16%|#6        | 1210000/7400423 [02:08<10:37, 9709.85\nexamples/s]', '\\rMap:  16%|#6        | 1211000/7400423 [02:08<10:57, 9413.31\nexamples/s]', '\\rMap:  16%|#6        | 1212000/7400423 [02:08<11:07, 9272.47\nexamples/s]', '\\rMap:  16%|#6        | 1213000/7400423 [02:08<11:18, 9123.48\nexamples/s]', '\\rMap:  16%|#6        | 1214000/7400423 [02:08<11:23, 9054.44\nexamples/s]', '\\rMap:  16%|#6        | 1215000/7400423 [02:08<11:07, 9263.43\nexamples/s]', '\\rMap:  16%|#6        | 1216000/7400423 [02:08<11:00, 9369.26\nexamples/s]', '\\rMap:  16%|#6        | 1217000/7400423 [02:08<11:00, 9355.96\nexamples/s]', '\\rMap:  16%|#6        | 1218000/7400423 [02:08<10:58, 9394.57\nexamples/s]', '\\rMap:  16%|#6        | 1219000/7400423 [02:09<10:55, 9434.31\nexamples/s]', '\\rMap:  16%|#6        | 1220000/7400423 [02:09<17:13, 5982.49\nexamples/s]', '\\rMap:  16%|#6        | 1221000/7400423 [02:09<15:19, 6720.24\nexamples/s]', '\\rMap:  17%|#6        | 1222000/7400423 [02:09<13:57, 7374.67\nexamples/s]', '\\rMap:  17%|#6        | 1223000/7400423 [02:09<13:05, 7865.70\nexamples/s]', '\\rMap:  17%|#6        | 1224000/7400423 [02:09<12:25, 8282.67\nexamples/s]', '\\rMap:  17%|#6        | 1225000/7400423 [02:09<12:09, 8463.98\nexamples/s]', '\\rMap:  17%|#6        | 1226000/7400423 [02:09<11:47, 8731.31\nexamples/s]', '\\rMap:  17%|#6        | 1227000/7400423 [02:10<11:23, 9032.86\nexamples/s]', '\\rMap:  17%|#6        | 1228000/7400423 [02:10<11:09, 9219.94\nexamples/s]', '\\rMap:  17%|#6        | 1229000/7400423 [02:10<11:03, 9296.28\nexamples/s]', '\\rMap:  17%|#6        | 1230000/7400423 [02:10<11:29, 8952.49\nexamples/s]', '\\rMap:  17%|#6        | 1231000/7400423 [02:10<11:37, 8843.92\nexamples/s]', '\\rMap:  17%|#6        | 1232000/7400423 [02:10<11:31, 8918.69\nexamples/s]', '\\rMap:  17%|#6        | 1233000/7400423 [02:10<11:18, 9083.74\nexamples/s]', '\\rMap:  17%|#6        | 1234000/7400423 [02:10<11:24, 9004.09\nexamples/s]', '\\rMap:  17%|#6        | 1235000/7400423 [02:10<11:21, 9045.97\nexamples/s]', '\\rMap:  17%|#6        | 1236000/7400423 [02:11<11:23, 9015.47\nexamples/s]', '\\rMap:  17%|#6        | 1237000/7400423 [02:11<11:20, 9051.07\nexamples/s]', '\\rMap:  17%|#6        | 1238000/7400423 [02:11<11:07, 9228.45\nexamples/s]', '\\rMap:  17%|#6        | 1239000/7400423 [02:11<11:19, 9072.64\nexamples/s]', '\\rMap:  17%|#6        | 1240000/7400423 [02:11<11:13, 9149.54\nexamples/s]', '\\rMap:  17%|#6        | 1242000/7400423 [02:11<10:52, 9443.24\nexamples/s]', '\\rMap:  17%|#6        | 1244000/7400423 [02:11<10:46, 9523.42\nexamples/s]', '\\rMap:  17%|#6        | 1245000/7400423 [02:12<10:47, 9510.81\nexamples/s]', '\\rMap:  17%|#6        | 1247000/7400423 [02:12<10:29, 9781.87\nexamples/s]', '\\rMap:  17%|#6        | 1248000/7400423 [02:12<10:25, 9828.27\nexamples/s]', '\\rMap:  17%|#6        | 1250000/7400423 [02:12<10:22, 9873.85\nexamples/s]', '\\rMap:  17%|#6        | 1251000/7400423 [02:12<10:31, 9732.45\nexamples/s]', '\\rMap:  17%|#6        | 1252000/7400423 [02:12<10:41, 9586.19\nexamples/s]', '\\rMap:  17%|#6        | 1253000/7400423 [02:12<10:45, 9520.35\nexamples/s]', '\\rMap:  17%|#6        | 1254000/7400423 [02:12<10:52, 9417.16\nexamples/s]', '\\rMap:  17%|#6        | 1255000/7400423 [02:13<10:51, 9427.04\nexamples/s]', '\\rMap:  17%|#6        | 1256000/7400423 [02:13<10:59, 9320.07\nexamples/s]', '\\rMap:  17%|#6        | 1257000/7400423 [02:13<11:06, 9221.62\nexamples/s]', '\\rMap:  17%|#6        | 1258000/7400423 [02:13<11:13, 9125.34\nexamples/s]', '\\rMap:  17%|#7        | 1259000/7400423 [02:13<11:14, 9105.27\nexamples/s]', '\\rMap:  17%|#7        | 1260000/7400423 [02:13<11:15, 9086.63\nexamples/s]', '\\rMap:  17%|#7        | 1261000/7400423 [02:13<11:29, 8898.07\nexamples/s]', '\\rMap:  17%|#7        | 1263000/7400423 [02:13<10:35, 9654.54\nexamples/s]', '\\rMap:  17%|#7        | 1265000/7400423 [02:14<10:25, 9814.96\nexamples/s]', '\\rMap:  17%|#7        | 1266000/7400423 [02:14<10:32, 9694.53\nexamples/s]', '\\rMap:  17%|#7        | 1267000/7400423 [02:14<10:30, 9720.50\nexamples/s]', '\\rMap:  17%|#7        | 1268000/7400423 [02:14<10:31, 9717.58\nexamples/s]', '\\rMap:  17%|#7        | 1269000/7400423 [02:14<10:26, 9789.82\nexamples/s]', '\\rMap:  17%|#7        | 1271000/7400423 [02:14<10:19, 9888.16\nexamples/s]', '\\rMap:  17%|#7        | 1273000/7400423 [02:14<10:04, 10137.28\nexamples/s]', '\\rMap:  17%|#7        | 1275000/7400423 [02:15<09:48, 10406.12\nexamples/s]', '\\rMap:  17%|#7        | 1277000/7400423 [02:15<09:44, 10468.92\nexamples/s]', '\\rMap:  17%|#7        | 1279000/7400423 [02:15<09:35, 10641.30\nexamples/s]', '\\rMap:  17%|#7        | 1281000/7400423 [02:15<09:35, 10625.20\nexamples/s]', '\\rMap:  17%|#7        | 1283000/7400423 [02:15<09:31, 10712.82\nexamples/s]', '\\rMap:  17%|#7        | 1285000/7400423 [02:16<09:46, 10423.72\nexamples/s]', '\\rMap:  17%|#7        | 1287000/7400423 [02:16<09:59, 10197.77\nexamples/s]', '\\rMap:  17%|#7        | 1289000/7400423 [02:16<10:07, 10064.10\nexamples/s]', '\\rMap:  17%|#7        | 1291000/7400423 [02:16<10:15, 9930.08\nexamples/s] ', '\\rMap:  17%|#7        | 1292000/7400423 [02:16<10:21, 9824.25\nexamples/s]', '\\rMap:  17%|#7        | 1294000/7400423 [02:16<10:04, 10109.93\nexamples/s]', '\\rMap:  18%|#7        | 1296000/7400423 [02:17<09:26, 10775.84\nexamples/s]', '\\rMap:  18%|#7        | 1298000/7400423 [02:17<09:05, 11178.50\nexamples/s]', '\\rMap:  18%|#7        | 1300000/7400423 [02:17<09:08, 11125.42\nexamples/s]', '\\rMap:  18%|#7        | 1302000/7400423 [02:17<09:26, 10757.63\nexamples/s]', '\\rMap:  18%|#7        | 1304000/7400423 [02:17<09:28, 10725.18\nexamples/s]', '\\rMap:  18%|#7        | 1306000/7400423 [02:18<09:23, 10811.82\nexamples/s]', '\\rMap:  18%|#7        | 1308000/7400423 [02:18<12:18, 8246.81\nexamples/s] ', '\\rMap:  18%|#7        | 1310000/7400423 [02:18<11:16, 9002.06\nexamples/s]', '\\rMap:  18%|#7        | 1312000/7400423 [02:18<10:33, 9608.80\nexamples/s]', '\\rMap:  18%|#7        | 1314000/7400423 [02:18<10:05, 10044.63\nexamples/s]', '\\rMap:  18%|#7        | 1316000/7400423 [02:19<09:45, 10399.92\nexamples/s]', '\\rMap:  18%|#7        | 1318000/7400423 [02:19<09:32, 10632.81\nexamples/s]', '\\rMap:  18%|#7        | 1320000/7400423 [02:19<09:57, 10174.05\nexamples/s]', '\\rMap:  18%|#7        | 1322000/7400423 [02:19<10:09, 9973.01\nexamples/s] ', '\\rMap:  18%|#7        | 1324000/7400423 [02:19<10:09, 9968.26\nexamples/s]', '\\rMap:  18%|#7        | 1326000/7400423 [02:20<10:25, 9715.37\nexamples/s]', '\\rMap:  18%|#7        | 1327000/7400423 [02:20<10:51, 9322.87\nexamples/s]', '\\rMap:  18%|#7        | 1328000/7400423 [02:20<11:16, 8982.30\nexamples/s]', '\\rMap:  18%|#7        | 1329000/7400423 [02:20<11:12, 9024.88\nexamples/s]', '\\rMap:  18%|#7        | 1330000/7400423 [02:20<11:12, 9032.28\nexamples/s]', '\\rMap:  18%|#7        | 1331000/7400423 [02:20<11:04, 9127.13\nexamples/s]', '\\rMap:  18%|#7        | 1332000/7400423 [02:20<11:12, 9017.32\nexamples/s]', '\\rMap:  18%|#8        | 1333000/7400423 [02:20<11:00, 9188.15\nexamples/s]', '\\rMap:  18%|#8        | 1334000/7400423 [02:21<11:02, 9158.43\nexamples/s]', '\\rMap:  18%|#8        | 1335000/7400423 [02:21<10:52, 9293.27\nexamples/s]', '\\rMap:  18%|#8        | 1336000/7400423 [02:21<10:40, 9465.55\nexamples/s]', '\\rMap:  18%|#8        | 1338000/7400423 [02:21<10:21, 9753.26\nexamples/s]', '\\rMap:  18%|#8        | 1339000/7400423 [02:21<10:36, 9525.12\nexamples/s]', '\\rMap:  18%|#8        | 1340000/7400423 [02:21<10:36, 9524.06\nexamples/s]', '\\rMap:  18%|#8        | 1341000/7400423 [02:21<10:41, 9446.93\nexamples/s]', '\\rMap:  18%|#8        | 1342000/7400423 [02:21<10:39, 9474.79\nexamples/s]', '\\rMap:  18%|#8        | 1343000/7400423 [02:21<10:33, 9556.30\nexamples/s]', '\\rMap:  18%|#8        | 1344000/7400423 [02:22<10:40, 9452.68\nexamples/s]', '\\rMap:  18%|#8        | 1346000/7400423 [02:22<10:27, 9646.27\nexamples/s]', '\\rMap:  18%|#8        | 1347000/7400423 [02:22<10:29, 9608.63\nexamples/s]', '\\rMap:  18%|#8        | 1348000/7400423 [02:22<10:26, 9657.44\nexamples/s]', '\\rMap:  18%|#8        | 1350000/7400423 [02:22<10:34, 9542.56\nexamples/s]', '\\rMap:  18%|#8        | 1351000/7400423 [02:22<10:48, 9327.39\nexamples/s]', '\\rMap:  18%|#8        | 1352000/7400423 [02:22<11:11, 9005.35\nexamples/s]', '\\rMap:  18%|#8        | 1353000/7400423 [02:23<11:13, 8977.92\nexamples/s]', '\\rMap:  18%|#8        | 1354000/7400423 [02:23<11:31, 8743.27\nexamples/s]', '\\rMap:  18%|#8        | 1355000/7400423 [02:23<11:24, 8828.79\nexamples/s]', '\\rMap:  18%|#8        | 1356000/7400423 [02:23<11:08, 9039.85\nexamples/s]', '\\rMap:  18%|#8        | 1357000/7400423 [02:23<11:02, 9118.14\nexamples/s]', '\\rMap:  18%|#8        | 1358000/7400423 [02:23<10:59, 9161.72\nexamples/s]', '\\rMap:  18%|#8        | 1359000/7400423 [02:23<10:58, 9178.78\nexamples/s]', '\\rMap:  18%|#8        | 1360000/7400423 [02:23<10:46, 9349.88\nexamples/s]', '\\rMap:  18%|#8        | 1361000/7400423 [02:23<10:43, 9389.06\nexamples/s]', '\\rMap:  18%|#8        | 1362000/7400423 [02:24<11:18, 8893.43\nexamples/s]', '\\rMap:  18%|#8        | 1363000/7400423 [02:24<11:43, 8585.02\nexamples/s]', '\\rMap:  18%|#8        | 1364000/7400423 [02:24<11:55, 8438.23\nexamples/s]', '\\rMap:  18%|#8        | 1365000/7400423 [02:24<12:01, 8362.78\nexamples/s]', '\\rMap:  18%|#8        | 1366000/7400423 [02:24<11:56, 8425.46\nexamples/s]', '\\rMap:  18%|#8        | 1368000/7400423 [02:24<10:32, 9530.37\nexamples/s]', '\\rMap:  19%|#8        | 1370000/7400423 [02:24<09:43, 10332.45\nexamples/s]', '\\rMap:  19%|#8        | 1372000/7400423 [02:25<09:13, 10887.57\nexamples/s]', '\\rMap:  19%|#8        | 1374000/7400423 [02:25<08:59, 11164.28\nexamples/s]', '\\rMap:  19%|#8        | 1376000/7400423 [02:25<08:46, 11449.13\nexamples/s]', '\\rMap:  19%|#8        | 1378000/7400423 [02:25<08:39, 11588.78\nexamples/s]', '\\rMap:  19%|#8        | 1380000/7400423 [02:25<08:30, 11785.16\nexamples/s]', '\\rMap:  19%|#8        | 1382000/7400423 [02:25<08:30, 11797.59\nexamples/s]', '\\rMap:  19%|#8        | 1384000/7400423 [02:26<11:19, 8853.12\nexamples/s] ', '\\rMap:  19%|#8        | 1386000/7400423 [02:26<10:14, 9785.67\nexamples/s]', '\\rMap:  19%|#8        | 1388000/7400423 [02:26<09:48, 10216.17\nexamples/s]', '\\rMap:  19%|#8        | 1390000/7400423 [02:26<09:25, 10619.42\nexamples/s]', '\\rMap:  19%|#8        | 1392000/7400423 [02:26<09:08, 10959.44\nexamples/s]', '\\rMap:  19%|#8        | 1394000/7400423 [02:27<08:46, 11411.20\nexamples/s]', '\\rMap:  19%|#8        | 1396000/7400423 [02:27<09:09, 10929.13\nexamples/s]', '\\rMap:  19%|#8        | 1398000/7400423 [02:27<09:44, 10278.08\nexamples/s]', '\\rMap:  19%|#8        | 1400000/7400423 [02:27<09:47, 10216.69\nexamples/s]', '\\rMap:  19%|#8        | 1402000/7400423 [02:27<09:53, 10100.92\nexamples/s]', '\\rMap:  19%|#8        | 1404000/7400423 [02:28<09:49, 10180.46\nexamples/s]', '\\rMap:  19%|#8        | 1406000/7400423 [02:28<09:55, 10072.95\nexamples/s]', '\\rMap:  19%|#9        | 1408000/7400423 [02:28<10:03, 9936.55\nexamples/s] ', '\\rMap:  19%|#9        | 1410000/7400423 [02:28<09:57, 10020.73\nexamples/s]', '\\rMap:  19%|#9        | 1412000/7400423 [02:28<09:44, 10248.39\nexamples/s]', '\\rMap:  19%|#9        | 1414000/7400423 [02:29<09:49, 10160.10\nexamples/s]', '\\rMap:  19%|#9        | 1416000/7400423 [02:29<09:59, 9974.97\nexamples/s] ', '\\rMap:  19%|#9        | 1418000/7400423 [02:29<09:54, 10061.54\nexamples/s]', '\\rMap:  19%|#9        | 1420000/7400423 [02:29<09:44, 10233.48\nexamples/s]', '\\rMap:  19%|#9        | 1422000/7400423 [02:29<09:33, 10417.73\nexamples/s]', '\\rMap:  19%|#9        | 1424000/7400423 [02:30<09:28, 10516.07\nexamples/s]', '\\rMap:  19%|#9        | 1426000/7400423 [02:30<09:20, 10661.82\nexamples/s]', '\\rMap:  19%|#9        | 1428000/7400423 [02:30<09:14, 10767.26\nexamples/s]', '\\rMap:  19%|#9        | 1430000/7400423 [02:30<09:13, 10793.27\nexamples/s]', '\\rMap:  19%|#9        | 1432000/7400423 [02:30<09:08, 10883.93\nexamples/s]', '\\rMap:  19%|#9        | 1434000/7400423 [02:30<09:08, 10885.38\nexamples/s]', '\\rMap:  19%|#9        | 1436000/7400423 [02:31<08:56, 11120.71\nexamples/s]', '\\rMap:  19%|#9        | 1438000/7400423 [02:31<08:51, 11224.29\nexamples/s]', '\\rMap:  19%|#9        | 1440000/7400423 [02:31<08:43, 11389.14\nexamples/s]', '\\rMap:  19%|#9        | 1442000/7400423 [02:31<08:37, 11505.27\nexamples/s]', '\\rMap:  20%|#9        | 1444000/7400423 [02:31<08:48, 11275.53\nexamples/s]', '\\rMap:  20%|#9        | 1446000/7400423 [02:32<09:03, 10955.63\nexamples/s]', '\\rMap:  20%|#9        | 1448000/7400423 [02:32<09:07, 10865.80\nexamples/s]', '\\rMap:  20%|#9        | 1450000/7400423 [02:32<09:16, 10694.54\nexamples/s]', '\\rMap:  20%|#9        | 1452000/7400423 [02:32<09:21, 10588.16\nexamples/s]', '\\rMap:  20%|#9        | 1454000/7400423 [02:32<09:05, 10897.15\nexamples/s]', '\\rMap:  20%|#9        | 1456000/7400423 [02:32<08:57, 11049.71\nexamples/s]', '\\rMap:  20%|#9        | 1458000/7400423 [02:33<08:51, 11171.36\nexamples/s]', '\\rMap:  20%|#9        | 1460000/7400423 [02:33<08:59, 11007.70\nexamples/s]', '\\rMap:  20%|#9        | 1462000/7400423 [02:33<09:15, 10692.44\nexamples/s]', '\\rMap:  20%|#9        | 1464000/7400423 [02:33<09:18, 10632.40\nexamples/s]', '\\rMap:  20%|#9        | 1466000/7400423 [02:33<09:14, 10697.00\nexamples/s]', '\\rMap:  20%|#9        | 1468000/7400423 [02:34<09:18, 10613.38\nexamples/s]', '\\rMap:  20%|#9        | 1470000/7400423 [02:34<09:32, 10362.17\nexamples/s]', '\\rMap:  20%|#9        | 1472000/7400423 [02:34<10:04, 9812.09\nexamples/s] ', '\\rMap:  20%|#9        | 1473000/7400423 [02:34<10:13, 9661.93\nexamples/s]', '\\rMap:  20%|#9        | 1474000/7400423 [02:34<10:22, 9520.70\nexamples/s]', '\\rMap:  20%|#9        | 1475000/7400423 [02:34<10:29, 9417.13\nexamples/s]', '\\rMap:  20%|#9        | 1476000/7400423 [02:34<10:23, 9496.22\nexamples/s]', '\\rMap:  20%|#9        | 1478000/7400423 [02:35<09:52, 9991.85\nexamples/s]', '\\rMap:  20%|#9        | 1480000/7400423 [02:35<09:22, 10517.24\nexamples/s]', '\\rMap:  20%|##        | 1482000/7400423 [02:35<09:02, 10907.54\nexamples/s]', '\\rMap:  20%|##        | 1484000/7400423 [02:35<08:54, 11063.23\nexamples/s]', '\\rMap:  20%|##        | 1486000/7400423 [02:35<08:47, 11208.98\nexamples/s]', '\\rMap:  20%|##        | 1488000/7400423 [02:36<08:49, 11161.41\nexamples/s]', '\\rMap:  20%|##        | 1490000/7400423 [02:36<08:39, 11384.97\nexamples/s]', '\\rMap:  20%|##        | 1492000/7400423 [02:36<08:39, 11382.78\nexamples/s]', '\\rMap:  20%|##        | 1494000/7400423 [02:36<08:37, 11410.89\nexamples/s]', '\\rMap:  20%|##        | 1496000/7400423 [02:36<08:44, 11249.35\nexamples/s]', '\\rMap:  20%|##        | 1498000/7400423 [02:36<09:01, 10904.50\nexamples/s]', '\\rMap:  20%|##        | 1500000/7400423 [02:37<09:13, 10654.99\nexamples/s]', '\\rMap:  20%|##        | 1502000/7400423 [02:37<09:12, 10676.17\nexamples/s]', '\\rMap:  20%|##        | 1504000/7400423 [02:37<09:15, 10622.29\nexamples/s]', '\\rMap:  20%|##        | 1506000/7400423 [02:37<09:16, 10583.03\nexamples/s]', '\\rMap:  20%|##        | 1508000/7400423 [02:37<09:22, 10469.76\nexamples/s]', '\\rMap:  20%|##        | 1510000/7400423 [02:38<12:41, 7737.37\nexamples/s] ', '\\rMap:  20%|##        | 1512000/7400423 [02:38<11:38, 8430.53\nexamples/s]', '\\rMap:  20%|##        | 1514000/7400423 [02:38<10:55, 8985.28\nexamples/s]', '\\rMap:  20%|##        | 1516000/7400423 [02:38<10:26, 9396.46\nexamples/s]', '\\rMap:  20%|##        | 1517000/7400423 [02:38<10:20, 9475.98\nexamples/s]', '\\rMap:  21%|##        | 1519000/7400423 [02:39<10:08, 9668.62\nexamples/s]', '\\rMap:  21%|##        | 1521000/7400423 [02:39<10:00, 9798.18\nexamples/s]', '\\rMap:  21%|##        | 1523000/7400423 [02:39<10:01, 9778.57\nexamples/s]', '\\rMap:  21%|##        | 1524000/7400423 [02:39<09:59, 9808.32\nexamples/s]', '\\rMap:  21%|##        | 1525000/7400423 [02:39<09:57, 9831.30\nexamples/s]', '\\rMap:  21%|##        | 1526000/7400423 [02:39<09:55, 9863.89\nexamples/s]', '\\rMap:  21%|##        | 1527000/7400423 [02:39<09:58, 9809.65\nexamples/s]', '\\rMap:  21%|##        | 1529000/7400423 [02:40<09:48, 9977.87\nexamples/s]', '\\rMap:  21%|##        | 1530000/7400423 [02:40<09:52, 9907.93\nexamples/s]', '\\rMap:  21%|##        | 1532000/7400423 [02:40<09:37, 10153.60\nexamples/s]', '\\rMap:  21%|##        | 1534000/7400423 [02:40<09:34, 10211.63\nexamples/s]', '\\rMap:  21%|##        | 1536000/7400423 [02:40<09:35, 10188.65\nexamples/s]', '\\rMap:  21%|##        | 1538000/7400423 [02:41<09:23, 10410.13\nexamples/s]', '\\rMap:  21%|##        | 1540000/7400423 [02:41<09:16, 10538.27\nexamples/s]', '\\rMap:  21%|##        | 1542000/7400423 [02:41<09:12, 10597.76\nexamples/s]', '\\rMap:  21%|##        | 1544000/7400423 [02:41<09:09, 10656.14\nexamples/s]', '\\rMap:  21%|##        | 1546000/7400423 [02:41<09:19, 10458.71\nexamples/s]', '\\rMap:  21%|##        | 1548000/7400423 [02:41<09:27, 10316.56\nexamples/s]', '\\rMap:  21%|##        | 1550000/7400423 [02:42<09:23, 10379.59\nexamples/s]', '\\rMap:  21%|##        | 1552000/7400423 [02:42<09:22, 10396.98\nexamples/s]', '\\rMap:  21%|##        | 1554000/7400423 [02:42<09:27, 10299.85\nexamples/s]', '\\rMap:  21%|##1       | 1556000/7400423 [02:42<09:28, 10288.53\nexamples/s]', '\\rMap:  21%|##1       | 1558000/7400423 [02:42<09:26, 10320.41\nexamples/s]', '\\rMap:  21%|##1       | 1560000/7400423 [02:43<09:28, 10273.61\nexamples/s]', '\\rMap:  21%|##1       | 1562000/7400423 [02:43<09:28, 10267.49\nexamples/s]', '\\rMap:  21%|##1       | 1564000/7400423 [02:43<09:33, 10174.15\nexamples/s]', '\\rMap:  21%|##1       | 1566000/7400423 [02:43<09:26, 10298.41\nexamples/s]', '\\rMap:  21%|##1       | 1568000/7400423 [02:43<09:28, 10259.30\nexamples/s]', '\\rMap:  21%|##1       | 1570000/7400423 [02:44<09:30, 10216.92\nexamples/s]', '\\rMap:  21%|##1       | 1572000/7400423 [02:44<09:20, 10403.71\nexamples/s]', '\\rMap:  21%|##1       | 1574000/7400423 [02:44<09:21, 10372.83\nexamples/s]', '\\rMap:  21%|##1       | 1576000/7400423 [02:44<09:14, 10497.32\nexamples/s]', '\\rMap:  21%|##1       | 1578000/7400423 [02:44<09:21, 10378.53\nexamples/s]', '\\rMap:  21%|##1       | 1580000/7400423 [02:45<09:27, 10262.45\nexamples/s]', '\\rMap:  21%|##1       | 1582000/7400423 [02:45<09:17, 10427.82\nexamples/s]', '\\rMap:  21%|##1       | 1584000/7400423 [02:45<09:16, 10450.34\nexamples/s]', '\\rMap:  21%|##1       | 1586000/7400423 [02:45<09:17, 10432.17\nexamples/s]', '\\rMap:  21%|##1       | 1588000/7400423 [02:45<09:40, 10014.37\nexamples/s]', '\\rMap:  21%|##1       | 1590000/7400423 [02:46<09:49, 9862.30\nexamples/s] ', '\\rMap:  21%|##1       | 1591000/7400423 [02:46<09:53, 9788.88\nexamples/s]', '\\rMap:  22%|##1       | 1592000/7400423 [02:46<09:58, 9702.86\nexamples/s]', '\\rMap:  22%|##1       | 1593000/7400423 [02:46<10:07, 9559.77\nexamples/s]', '\\rMap:  22%|##1       | 1594000/7400423 [02:46<10:06, 9566.19\nexamples/s]', '\\rMap:  22%|##1       | 1595000/7400423 [02:46<10:04, 9599.59\nexamples/s]', '\\rMap:  22%|##1       | 1597000/7400423 [02:46<09:27, 10233.71\nexamples/s]', '\\rMap:  22%|##1       | 1599000/7400423 [02:46<09:10, 10531.52\nexamples/s]', '\\rMap:  22%|##1       | 1601000/7400423 [02:47<08:48, 10966.32\nexamples/s]', '\\rMap:  22%|##1       | 1603000/7400423 [02:47<08:39, 11148.95\nexamples/s]', '\\rMap:  22%|##1       | 1605000/7400423 [02:47<08:39, 11164.34\nexamples/s]', '\\rMap:  22%|##1       | 1607000/7400423 [02:47<08:34, 11267.44\nexamples/s]', '\\rMap:  22%|##1       | 1609000/7400423 [02:48<11:46, 8192.66\nexamples/s] ', '\\rMap:  22%|##1       | 1611000/7400423 [02:48<11:02, 8732.48\nexamples/s]', '\\rMap:  22%|##1       | 1613000/7400423 [02:48<10:28, 9210.37\nexamples/s]', '\\rMap:  22%|##1       | 1615000/7400423 [02:48<09:53, 9740.08\nexamples/s]', '\\rMap:  22%|##1       | 1617000/7400423 [02:48<09:34, 10065.88\nexamples/s]', '\\rMap:  22%|##1       | 1619000/7400423 [02:48<09:28, 10169.16\nexamples/s]', '\\rMap:  22%|##1       | 1621000/7400423 [02:49<09:19, 10338.46\nexamples/s]', '\\rMap:  22%|##1       | 1623000/7400423 [02:49<09:09, 10522.20\nexamples/s]', '\\rMap:  22%|##1       | 1625000/7400423 [02:49<08:58, 10720.16\nexamples/s]', '\\rMap:  22%|##1       | 1627000/7400423 [02:49<08:52, 10839.93\nexamples/s]', '\\rMap:  22%|##2       | 1629000/7400423 [02:49<08:51, 10863.61\nexamples/s]', '\\rMap:  22%|##2       | 1631000/7400423 [02:50<08:50, 10872.79\nexamples/s]', '\\rMap:  22%|##2       | 1633000/7400423 [02:50<08:49, 10900.72\nexamples/s]', '\\rMap:  22%|##2       | 1635000/7400423 [02:50<08:55, 10773.80\nexamples/s]', '\\rMap:  22%|##2       | 1637000/7400423 [02:50<09:19, 10307.33\nexamples/s]', '\\rMap:  22%|##2       | 1639000/7400423 [02:50<09:43, 9882.06\nexamples/s] ', '\\rMap:  22%|##2       | 1640000/7400423 [02:51<09:54, 9688.96\nexamples/s]', '\\rMap:  22%|##2       | 1641000/7400423 [02:51<10:06, 9495.82\nexamples/s]', '\\rMap:  22%|##2       | 1642000/7400423 [02:51<10:14, 9367.55\nexamples/s]', '\\rMap:  22%|##2       | 1643000/7400423 [02:51<10:25, 9198.02\nexamples/s]', '\\rMap:  22%|##2       | 1644000/7400423 [02:51<10:31, 9122.66\nexamples/s]', '\\rMap:  22%|##2       | 1645000/7400423 [02:51<10:35, 9057.43\nexamples/s]', '\\rMap:  22%|##2       | 1646000/7400423 [02:51<10:47, 8892.59\nexamples/s]', '\\rMap:  22%|##2       | 1647000/7400423 [02:51<10:45, 8914.48\nexamples/s]', '\\rMap:  22%|##2       | 1648000/7400423 [02:51<10:30, 9129.68\nexamples/s]', '\\rMap:  22%|##2       | 1650000/7400423 [02:52<09:51, 9719.21\nexamples/s]', '\\rMap:  22%|##2       | 1652000/7400423 [02:52<09:26, 10150.47\nexamples/s]', '\\rMap:  22%|##2       | 1654000/7400423 [02:52<09:09, 10464.25\nexamples/s]', '\\rMap:  22%|##2       | 1656000/7400423 [02:52<09:14, 10361.02\nexamples/s]', '\\rMap:  22%|##2       | 1658000/7400423 [02:52<09:33, 10020.88\nexamples/s]', '\\rMap:  22%|##2       | 1659000/7400423 [02:52<09:33, 10003.08\nexamples/s]', '\\rMap:  22%|##2       | 1660000/7400423 [02:53<09:43, 9838.55\nexamples/s] ', '\\rMap:  22%|##2       | 1662000/7400423 [02:53<09:31, 10038.83\nexamples/s]', '\\rMap:  22%|##2       | 1664000/7400423 [02:53<09:20, 10231.13\nexamples/s]', '\\rMap:  23%|##2       | 1666000/7400423 [02:53<09:19, 10248.71\nexamples/s]', '\\rMap:  23%|##2       | 1668000/7400423 [02:53<09:16, 10291.89\nexamples/s]', '\\rMap:  23%|##2       | 1670000/7400423 [02:54<09:19, 10233.40\nexamples/s]', '\\rMap:  23%|##2       | 1672000/7400423 [02:54<09:16, 10294.57\nexamples/s]', '\\rMap:  23%|##2       | 1674000/7400423 [02:54<09:19, 10233.37\nexamples/s]', '\\rMap:  23%|##2       | 1676000/7400423 [02:54<09:19, 10237.17\nexamples/s]', '\\rMap:  23%|##2       | 1678000/7400423 [02:54<09:37, 9903.34\nexamples/s] ', '\\rMap:  23%|##2       | 1679000/7400423 [02:54<09:44, 9796.36\nexamples/s]', '\\rMap:  23%|##2       | 1680000/7400423 [02:55<09:45, 9766.22\nexamples/s]', '\\rMap:  23%|##2       | 1681000/7400423 [02:55<09:52, 9647.81\nexamples/s]', '\\rMap:  23%|##2       | 1682000/7400423 [02:55<09:55, 9604.56\nexamples/s]', '\\rMap:  23%|##2       | 1683000/7400423 [02:55<10:01, 9513.18\nexamples/s]', '\\rMap:  23%|##2       | 1684000/7400423 [02:55<10:00, 9514.01\nexamples/s]', '\\rMap:  23%|##2       | 1685000/7400423 [02:55<09:52, 9643.60\nexamples/s]', '\\rMap:  23%|##2       | 1687000/7400423 [02:55<09:40, 9837.46\nexamples/s]', '\\rMap:  23%|##2       | 1689000/7400423 [02:55<09:34, 9947.23\nexamples/s]', '\\rMap:  23%|##2       | 1691000/7400423 [02:56<09:30, 10011.78\nexamples/s]', '\\rMap:  23%|##2       | 1693000/7400423 [02:56<09:23, 10120.71\nexamples/s]', '\\rMap:  23%|##2       | 1695000/7400423 [02:56<12:22, 7683.69\nexamples/s] ', '\\rMap:  23%|##2       | 1696000/7400423 [02:56<11:52, 8010.21\nexamples/s]', '\\rMap:  23%|##2       | 1698000/7400423 [02:57<10:52, 8745.81\nexamples/s]', '\\rMap:  23%|##2       | 1700000/7400423 [02:57<10:21, 9173.65\nexamples/s]', '\\rMap:  23%|##2       | 1702000/7400423 [02:57<09:48, 9687.83\nexamples/s]', '\\rMap:  23%|##3       | 1704000/7400423 [02:57<09:24, 10085.60\nexamples/s]', '\\rMap:  23%|##3       | 1706000/7400423 [02:57<09:26, 10050.37\nexamples/s]', '\\rMap:  23%|##3       | 1708000/7400423 [02:57<09:15, 10251.04\nexamples/s]', '\\rMap:  23%|##3       | 1710000/7400423 [02:58<09:13, 10282.47\nexamples/s]', '\\rMap:  23%|##3       | 1712000/7400423 [02:58<09:15, 10231.48\nexamples/s]', '\\rMap:  23%|##3       | 1714000/7400423 [02:58<09:21, 10128.43\nexamples/s]', '\\rMap:  23%|##3       | 1716000/7400423 [02:58<09:31, 9944.72\nexamples/s] ', '\\rMap:  23%|##3       | 1717000/7400423 [02:58<09:32, 9920.77\nexamples/s]', '\\rMap:  23%|##3       | 1719000/7400423 [02:59<09:23, 10084.52\nexamples/s]', '\\rMap:  23%|##3       | 1721000/7400423 [02:59<09:14, 10249.93\nexamples/s]', '\\rMap:  23%|##3       | 1723000/7400423 [02:59<08:55, 10596.73\nexamples/s]', '\\rMap:  23%|##3       | 1725000/7400423 [02:59<08:44, 10825.80\nexamples/s]', '\\rMap:  23%|##3       | 1727000/7400423 [02:59<08:43, 10833.91\nexamples/s]', '\\rMap:  23%|##3       | 1729000/7400423 [03:00<08:53, 10637.88\nexamples/s]', '\\rMap:  23%|##3       | 1731000/7400423 [03:00<09:08, 10331.79\nexamples/s]', '\\rMap:  23%|##3       | 1733000/7400423 [03:00<09:10, 10296.04\nexamples/s]', '\\rMap:  23%|##3       | 1735000/7400423 [03:00<09:06, 10372.54\nexamples/s]', '\\rMap:  23%|##3       | 1737000/7400423 [03:00<09:04, 10401.17\nexamples/s]', '\\rMap:  23%|##3       | 1739000/7400423 [03:00<09:02, 10429.09\nexamples/s]', '\\rMap:  24%|##3       | 1741000/7400423 [03:01<09:12, 10245.90\nexamples/s]', '\\rMap:  24%|##3       | 1743000/7400423 [03:01<09:14, 10202.58\nexamples/s]', '\\rMap:  24%|##3       | 1745000/7400423 [03:01<09:18, 10133.09\nexamples/s]', '\\rMap:  24%|##3       | 1747000/7400423 [03:01<09:14, 10203.47\nexamples/s]', '\\rMap:  24%|##3       | 1749000/7400423 [03:01<09:10, 10258.48\nexamples/s]', '\\rMap:  24%|##3       | 1751000/7400423 [03:02<09:10, 10260.84\nexamples/s]', '\\rMap:  24%|##3       | 1753000/7400423 [03:02<09:11, 10242.31\nexamples/s]', '\\rMap:  24%|##3       | 1755000/7400423 [03:02<09:23, 10027.22\nexamples/s]', '\\rMap:  24%|##3       | 1757000/7400423 [03:02<09:19, 10078.60\nexamples/s]', '\\rMap:  24%|##3       | 1759000/7400423 [03:02<09:18, 10109.91\nexamples/s]', '\\rMap:  24%|##3       | 1761000/7400423 [03:03<09:10, 10235.37\nexamples/s]', '\\rMap:  24%|##3       | 1763000/7400423 [03:03<09:11, 10219.10\nexamples/s]', '\\rMap:  24%|##3       | 1765000/7400423 [03:03<09:06, 10308.37\nexamples/s]', '\\rMap:  24%|##3       | 1767000/7400423 [03:03<08:55, 10523.69\nexamples/s]', '\\rMap:  24%|##3       | 1769000/7400423 [03:03<08:59, 10432.35\nexamples/s]', '\\rMap:  24%|##3       | 1771000/7400423 [03:04<11:40, 8035.08\nexamples/s] ', '\\rMap:  24%|##3       | 1773000/7400423 [03:04<11:02, 8490.32\nexamples/s]', '\\rMap:  24%|##3       | 1774000/7400423 [03:04<11:00, 8513.54\nexamples/s]', '\\rMap:  24%|##3       | 1775000/7400423 [03:04<10:55, 8577.14\nexamples/s]', '\\rMap:  24%|##3       | 1776000/7400423 [03:04<10:51, 8634.78\nexamples/s]', '\\rMap:  24%|##4       | 1777000/7400423 [03:04<10:41, 8770.03\nexamples/s]', '\\rMap:  24%|##4       | 1779000/7400423 [03:05<09:43, 9641.83\nexamples/s]', '\\rMap:  24%|##4       | 1781000/7400423 [03:05<09:20, 10019.93\nexamples/s]', '\\rMap:  24%|##4       | 1783000/7400423 [03:05<09:04, 10317.26\nexamples/s]', '\\rMap:  24%|##4       | 1785000/7400423 [03:05<08:54, 10501.47\nexamples/s]', '\\rMap:  24%|##4       | 1787000/7400423 [03:05<08:36, 10869.62\nexamples/s]', '\\rMap:  24%|##4       | 1789000/7400423 [03:06<08:31, 10966.46\nexamples/s]', '\\rMap:  24%|##4       | 1791000/7400423 [03:06<08:30, 10986.41\nexamples/s]', '\\rMap:  24%|##4       | 1793000/7400423 [03:06<08:41, 10757.65\nexamples/s]', '\\rMap:  24%|##4       | 1795000/7400423 [03:06<09:00, 10373.06\nexamples/s]', '\\rMap:  24%|##4       | 1797000/7400423 [03:06<08:51, 10550.72\nexamples/s]', '\\rMap:  24%|##4       | 1799000/7400423 [03:06<08:54, 10480.53\nexamples/s]', '\\rMap:  24%|##4       | 1801000/7400423 [03:07<08:40, 10761.55\nexamples/s]', '\\rMap:  24%|##4       | 1803000/7400423 [03:07<08:41, 10729.73\nexamples/s]', '\\rMap:  24%|##4       | 1805000/7400423 [03:07<08:47, 10615.46\nexamples/s]', '\\rMap:  24%|##4       | 1807000/7400423 [03:07<09:07, 10217.63\nexamples/s]', '\\rMap:  24%|##4       | 1809000/7400423 [03:07<09:29, 9816.66\nexamples/s] ', '\\rMap:  24%|##4       | 1810000/7400423 [03:08<09:37, 9687.83\nexamples/s]', '\\rMap:  24%|##4       | 1811000/7400423 [03:08<09:52, 9439.70\nexamples/s]', '\\rMap:  24%|##4       | 1812000/7400423 [03:08<09:59, 9326.85\nexamples/s]', '\\rMap:  24%|##4       | 1813000/7400423 [03:08<10:10, 9158.77\nexamples/s]', '\\rMap:  25%|##4       | 1814000/7400423 [03:08<10:28, 8884.61\nexamples/s]', '\\rMap:  25%|##4       | 1815000/7400423 [03:08<10:35, 8784.43\nexamples/s]', '\\rMap:  25%|##4       | 1816000/7400423 [03:08<10:44, 8664.46\nexamples/s]', '\\rMap:  25%|##4       | 1818000/7400423 [03:08<09:57, 9350.39\nexamples/s]', '\\rMap:  25%|##4       | 1819000/7400423 [03:09<09:50, 9445.56\nexamples/s]', '\\rMap:  25%|##4       | 1820000/7400423 [03:09<09:51, 9432.24\nexamples/s]', '\\rMap:  25%|##4       | 1821000/7400423 [03:09<09:54, 9383.74\nexamples/s]', '\\rMap:  25%|##4       | 1822000/7400423 [03:09<10:08, 9173.38\nexamples/s]', '\\rMap:  25%|##4       | 1823000/7400423 [03:09<10:01, 9278.34\nexamples/s]', '\\rMap:  25%|##4       | 1824000/7400423 [03:09<10:07, 9173.03\nexamples/s]', '\\rMap:  25%|##4       | 1825000/7400423 [03:09<10:04, 9220.60\nexamples/s]', '\\rMap:  25%|##4       | 1827000/7400423 [03:09<09:50, 9443.79\nexamples/s]', '\\rMap:  25%|##4       | 1828000/7400423 [03:10<09:48, 9473.61\nexamples/s]', '\\rMap:  25%|##4       | 1830000/7400423 [03:10<09:30, 9769.84\nexamples/s]', '\\rMap:  25%|##4       | 1832000/7400423 [03:10<09:28, 9794.86\nexamples/s]', '\\rMap:  25%|##4       | 1833000/7400423 [03:10<09:35, 9667.90\nexamples/s]', '\\rMap:  25%|##4       | 1834000/7400423 [03:10<09:34, 9685.65\nexamples/s]', '\\rMap:  25%|##4       | 1835000/7400423 [03:10<09:47, 9477.58\nexamples/s]', '\\rMap:  25%|##4       | 1836000/7400423 [03:10<10:00, 9271.54\nexamples/s]', '\\rMap:  25%|##4       | 1837000/7400423 [03:10<09:59, 9277.91\nexamples/s]', '\\rMap:  25%|##4       | 1838000/7400423 [03:11<10:00, 9265.50\nexamples/s]', '\\rMap:  25%|##4       | 1839000/7400423 [03:11<09:57, 9314.79\nexamples/s]', '\\rMap:  25%|##4       | 1840000/7400423 [03:11<09:56, 9322.04\nexamples/s]', '\\rMap:  25%|##4       | 1841000/7400423 [03:11<09:57, 9298.17\nexamples/s]', '\\rMap:  25%|##4       | 1842000/7400423 [03:11<09:52, 9377.99\nexamples/s]', '\\rMap:  25%|##4       | 1843000/7400423 [03:11<09:46, 9473.75\nexamples/s]', '\\rMap:  25%|##4       | 1844000/7400423 [03:11<09:53, 9356.27\nexamples/s]', '\\rMap:  25%|##4       | 1845000/7400423 [03:11<09:45, 9485.82\nexamples/s]', '\\rMap:  25%|##4       | 1846000/7400423 [03:11<09:47, 9448.06\nexamples/s]', '\\rMap:  25%|##4       | 1847000/7400423 [03:12<09:39, 9579.54\nexamples/s]', '\\rMap:  25%|##4       | 1849000/7400423 [03:12<09:28, 9770.22\nexamples/s]', '\\rMap:  25%|##5       | 1851000/7400423 [03:12<09:18, 9932.06\nexamples/s]', '\\rMap:  25%|##5       | 1853000/7400423 [03:12<09:25, 9815.58\nexamples/s]', '\\rMap:  25%|##5       | 1854000/7400423 [03:12<09:37, 9609.29\nexamples/s]', '\\rMap:  25%|##5       | 1855000/7400423 [03:12<09:42, 9521.74\nexamples/s]', '\\rMap:  25%|##5       | 1856000/7400423 [03:12<10:02, 9209.16\nexamples/s]', '\\rMap:  25%|##5       | 1857000/7400423 [03:13<09:53, 9337.71\nexamples/s]', '\\rMap:  25%|##5       | 1858000/7400423 [03:13<09:51, 9369.13\nexamples/s]', '\\rMap:  25%|##5       | 1859000/7400423 [03:13<09:51, 9366.25\nexamples/s]', '\\rMap:  25%|##5       | 1860000/7400423 [03:13<09:42, 9510.50\nexamples/s]', '\\rMap:  25%|##5       | 1861000/7400423 [03:13<09:38, 9577.18\nexamples/s]', '\\rMap:  25%|##5       | 1862000/7400423 [03:13<09:37, 9588.15\nexamples/s]', '\\rMap:  25%|##5       | 1863000/7400423 [03:13<09:32, 9668.03\nexamples/s]', '\\rMap:  25%|##5       | 1864000/7400423 [03:13<09:27, 9754.86\nexamples/s]', '\\rMap:  25%|##5       | 1866000/7400423 [03:14<09:35, 9614.77\nexamples/s]', '\\rMap:  25%|##5       | 1867000/7400423 [03:14<09:39, 9556.28\nexamples/s]', '\\rMap:  25%|##5       | 1868000/7400423 [03:14<09:37, 9584.27\nexamples/s]', '\\rMap:  25%|##5       | 1869000/7400423 [03:14<09:46, 9438.02\nexamples/s]', '\\rMap:  25%|##5       | 1870000/7400423 [03:14<09:47, 9415.09\nexamples/s]', '\\rMap:  25%|##5       | 1871000/7400423 [03:14<09:38, 9552.69\nexamples/s]', '\\rMap:  25%|##5       | 1873000/7400423 [03:14<09:12, 9999.13\nexamples/s]', '\\rMap:  25%|##5       | 1875000/7400423 [03:14<09:11, 10012.31\nexamples/s]', '\\rMap:  25%|##5       | 1877000/7400423 [03:15<09:02, 10190.44\nexamples/s]', '\\rMap:  25%|##5       | 1879000/7400423 [03:15<09:11, 10009.33\nexamples/s]', '\\rMap:  25%|##5       | 1881000/7400423 [03:15<09:12, 9996.18\nexamples/s] ', '\\rMap:  25%|##5       | 1882000/7400423 [03:15<09:19, 9865.21\nexamples/s]', '\\rMap:  25%|##5       | 1883000/7400423 [03:15<09:23, 9785.24\nexamples/s]', '\\rMap:  25%|##5       | 1884000/7400423 [03:15<09:34, 9605.84\nexamples/s]', '\\rMap:  25%|##5       | 1885000/7400423 [03:15<09:49, 9348.73\nexamples/s]', '\\rMap:  25%|##5       | 1886000/7400423 [03:16<09:53, 9294.08\nexamples/s]', '\\rMap:  25%|##5       | 1887000/7400423 [03:16<09:57, 9231.47\nexamples/s]', '\\rMap:  26%|##5       | 1888000/7400423 [03:16<09:49, 9355.42\nexamples/s]', '\\rMap:  26%|##5       | 1890000/7400423 [03:16<09:27, 9717.45\nexamples/s]', '\\rMap:  26%|##5       | 1892000/7400423 [03:16<09:13, 9949.78\nexamples/s]', '\\rMap:  26%|##5       | 1893000/7400423 [03:16<09:16, 9895.17\nexamples/s]', '\\rMap:  26%|##5       | 1894000/7400423 [03:16<09:22, 9796.95\nexamples/s]', '\\rMap:  26%|##5       | 1895000/7400423 [03:17<09:19, 9834.70\nexamples/s]', '\\rMap:  26%|##5       | 1897000/7400423 [03:17<13:19, 6882.73\nexamples/s]', '\\rMap:  26%|##5       | 1899000/7400423 [03:17<11:52, 7716.82\nexamples/s]', '\\rMap:  26%|##5       | 1900000/7400423 [03:17<11:22, 8054.47\nexamples/s]', '\\rMap:  26%|##5       | 1901000/7400423 [03:17<10:56, 8382.90\nexamples/s]', '\\rMap:  26%|##5       | 1902000/7400423 [03:17<10:32, 8698.38\nexamples/s]', '\\rMap:  26%|##5       | 1904000/7400423 [03:18<09:56, 9213.89\nexamples/s]', '\\rMap:  26%|##5       | 1905000/7400423 [03:18<09:50, 9311.12\nexamples/s]', '\\rMap:  26%|##5       | 1906000/7400423 [03:18<09:55, 9223.92\nexamples/s]', '\\rMap:  26%|##5       | 1907000/7400423 [03:18<09:48, 9334.41\nexamples/s]', '\\rMap:  26%|##5       | 1908000/7400423 [03:18<09:52, 9275.42\nexamples/s]', '\\rMap:  26%|##5       | 1909000/7400423 [03:18<09:47, 9343.81\nexamples/s]', '\\rMap:  26%|##5       | 1910000/7400423 [03:18<09:40, 9453.46\nexamples/s]', '\\rMap:  26%|##5       | 1911000/7400423 [03:18<09:38, 9490.92\nexamples/s]', '\\rMap:  26%|##5       | 1912000/7400423 [03:18<09:32, 9587.28\nexamples/s]', '\\rMap:  26%|##5       | 1913000/7400423 [03:19<09:35, 9539.65\nexamples/s]', '\\rMap:  26%|##5       | 1915000/7400423 [03:19<09:25, 9699.56\nexamples/s]', '\\rMap:  26%|##5       | 1916000/7400423 [03:19<09:34, 9553.01\nexamples/s]', '\\rMap:  26%|##5       | 1917000/7400423 [03:19<09:27, 9654.80\nexamples/s]', '\\rMap:  26%|##5       | 1919000/7400423 [03:19<09:28, 9642.33\nexamples/s]', '\\rMap:  26%|##5       | 1920000/7400423 [03:19<09:32, 9571.78\nexamples/s]', '\\rMap:  26%|##5       | 1921000/7400423 [03:19<09:44, 9378.92\nexamples/s]', '\\rMap:  26%|##5       | 1922000/7400423 [03:20<09:36, 9495.89\nexamples/s]', '\\rMap:  26%|##5       | 1924000/7400423 [03:20<09:27, 9644.00\nexamples/s]', '\\rMap:  26%|##6       | 1926000/7400423 [03:20<09:22, 9732.63\nexamples/s]', '\\rMap:  26%|##6       | 1927000/7400423 [03:20<09:22, 9723.91\nexamples/s]', '\\rMap:  26%|##6       | 1928000/7400423 [03:20<09:20, 9768.98\nexamples/s]', '\\rMap:  26%|##6       | 1929000/7400423 [03:20<09:24, 9699.21\nexamples/s]', '\\rMap:  26%|##6       | 1930000/7400423 [03:20<09:50, 9263.49\nexamples/s]', '\\rMap:  26%|##6       | 1931000/7400423 [03:20<10:06, 9019.69\nexamples/s]', '\\rMap:  26%|##6       | 1932000/7400423 [03:21<10:20, 8809.08\nexamples/s]', '\\rMap:  26%|##6       | 1933000/7400423 [03:21<10:33, 8624.42\nexamples/s]', '\\rMap:  26%|##6       | 1934000/7400423 [03:21<10:32, 8635.96\nexamples/s]', '\\rMap:  26%|##6       | 1935000/7400423 [03:21<10:30, 8666.98\nexamples/s]', '\\rMap:  26%|##6       | 1936000/7400423 [03:21<10:28, 8695.98\nexamples/s]', '\\rMap:  26%|##6       | 1937000/7400423 [03:21<10:24, 8749.33\nexamples/s]', '\\rMap:  26%|##6       | 1938000/7400423 [03:21<10:19, 8812.48\nexamples/s]', '\\rMap:  26%|##6       | 1939000/7400423 [03:21<10:14, 8892.88\nexamples/s]', '\\rMap:  26%|##6       | 1940000/7400423 [03:22<10:25, 8734.54\nexamples/s]', '\\rMap:  26%|##6       | 1941000/7400423 [03:22<10:14, 8886.15\nexamples/s]', '\\rMap:  26%|##6       | 1942000/7400423 [03:22<10:09, 8962.08\nexamples/s]', '\\rMap:  26%|##6       | 1943000/7400423 [03:22<10:05, 9015.58\nexamples/s]', '\\rMap:  26%|##6       | 1944000/7400423 [03:22<10:04, 9027.14\nexamples/s]', '\\rMap:  26%|##6       | 1945000/7400423 [03:22<10:17, 8837.38\nexamples/s]', '\\rMap:  26%|##6       | 1946000/7400423 [03:22<10:16, 8844.46\nexamples/s]', '\\rMap:  26%|##6       | 1947000/7400423 [03:22<10:05, 8999.65\nexamples/s]', '\\rMap:  26%|##6       | 1948000/7400423 [03:22<10:06, 8990.04\nexamples/s]', '\\rMap:  26%|##6       | 1949000/7400423 [03:23<09:51, 9208.76\nexamples/s]', '\\rMap:  26%|##6       | 1950000/7400423 [03:23<09:53, 9179.98\nexamples/s]', '\\rMap:  26%|##6       | 1951000/7400423 [03:23<10:16, 8843.71\nexamples/s]', '\\rMap:  26%|##6       | 1952000/7400423 [03:23<10:19, 8801.67\nexamples/s]', '\\rMap:  26%|##6       | 1953000/7400423 [03:23<10:16, 8841.74\nexamples/s]', '\\rMap:  26%|##6       | 1954000/7400423 [03:23<10:12, 8894.69\nexamples/s]', '\\rMap:  26%|##6       | 1955000/7400423 [03:23<10:05, 8992.75\nexamples/s]', '\\rMap:  26%|##6       | 1956000/7400423 [03:23<10:15, 8852.45\nexamples/s]', '\\rMap:  26%|##6       | 1957000/7400423 [03:23<10:24, 8718.15\nexamples/s]', '\\rMap:  26%|##6       | 1958000/7400423 [03:24<10:29, 8651.28\nexamples/s]', '\\rMap:  26%|##6       | 1960000/7400423 [03:24<09:27, 9588.67\nexamples/s]', '\\rMap:  27%|##6       | 1962000/7400423 [03:24<09:19, 9717.84\nexamples/s]', '\\rMap:  27%|##6       | 1964000/7400423 [03:24<09:13, 9830.72\nexamples/s]', '\\rMap:  27%|##6       | 1965000/7400423 [03:24<09:16, 9768.08\nexamples/s]', '\\rMap:  27%|##6       | 1966000/7400423 [03:24<09:26, 9598.25\nexamples/s]', '\\rMap:  27%|##6       | 1968000/7400423 [03:25<08:54, 10162.58\nexamples/s]', '\\rMap:  27%|##6       | 1970000/7400423 [03:25<08:13, 11014.06\nexamples/s]', '\\rMap:  27%|##6       | 1972000/7400423 [03:25<07:55, 11415.34\nexamples/s]', '\\rMap:  27%|##6       | 1974000/7400423 [03:25<07:42, 11725.41\nexamples/s]', '\\rMap:  27%|##6       | 1976000/7400423 [03:25<07:55, 11416.17\nexamples/s]', '\\rMap:  27%|##6       | 1978000/7400423 [03:25<08:09, 11084.71\nexamples/s]', '\\rMap:  27%|##6       | 1980000/7400423 [03:26<08:27, 10685.26\nexamples/s]', '\\rMap:  27%|##6       | 1982000/7400423 [03:26<08:40, 10416.87\nexamples/s]', '\\rMap:  27%|##6       | 1984000/7400423 [03:26<09:16, 9736.71\nexamples/s] ', '\\rMap:  27%|##6       | 1985000/7400423 [03:26<09:29, 9514.32\nexamples/s]', '\\rMap:  27%|##6       | 1986000/7400423 [03:26<09:45, 9247.97\nexamples/s]', '\\rMap:  27%|##6       | 1987000/7400423 [03:26<10:10, 8868.54\nexamples/s]', '\\rMap:  27%|##6       | 1989000/7400423 [03:27<09:25, 9576.11\nexamples/s]', '\\rMap:  27%|##6       | 1991000/7400423 [03:27<09:08, 9866.74\nexamples/s]', '\\rMap:  27%|##6       | 1992000/7400423 [03:27<09:08, 9853.46\nexamples/s]', '\\rMap:  27%|##6       | 1993000/7400423 [03:27<09:07, 9877.46\nexamples/s]', '\\rMap:  27%|##6       | 1995000/7400423 [03:27<08:52, 10145.73\nexamples/s]', '\\rMap:  27%|##6       | 1997000/7400423 [03:28<12:03, 7467.82\nexamples/s] ', '\\rMap:  27%|##7       | 1999000/7400423 [03:28<10:56, 8223.81\nexamples/s]', '\\rMap:  27%|##7       | 2001000/7400423 [03:28<10:01, 8975.44\nexamples/s]', '\\rMap:  27%|##7       | 2003000/7400423 [03:28<09:38, 9334.31\nexamples/s]', '\\rMap:  27%|##7       | 2005000/7400423 [03:28<09:25, 9542.02\nexamples/s]', '\\rMap:  27%|##7       | 2007000/7400423 [03:29<09:12, 9767.63\nexamples/s]', '\\rMap:  27%|##7       | 2009000/7400423 [03:29<09:09, 9805.75\nexamples/s]', '\\rMap:  27%|##7       | 2010000/7400423 [03:29<09:08, 9820.32\nexamples/s]', '\\rMap:  27%|##7       | 2011000/7400423 [03:29<09:10, 9791.56\nexamples/s]', '\\rMap:  27%|##7       | 2012000/7400423 [03:29<09:14, 9726.08\nexamples/s]', '\\rMap:  27%|##7       | 2014000/7400423 [03:29<09:01, 9953.58\nexamples/s]', '\\rMap:  27%|##7       | 2016000/7400423 [03:29<08:49, 10167.62\nexamples/s]', '\\rMap:  27%|##7       | 2018000/7400423 [03:30<08:35, 10434.84\nexamples/s]', '\\rMap:  27%|##7       | 2020000/7400423 [03:30<08:55, 10044.16\nexamples/s]', '\\rMap:  27%|##7       | 2022000/7400423 [03:30<08:58, 9988.48\nexamples/s] ', '\\rMap:  27%|##7       | 2024000/7400423 [03:30<09:01, 9921.27\nexamples/s]', '\\rMap:  27%|##7       | 2025000/7400423 [03:30<09:02, 9907.24\nexamples/s]', '\\rMap:  27%|##7       | 2026000/7400423 [03:30<09:08, 9797.96\nexamples/s]', '\\rMap:  27%|##7       | 2028000/7400423 [03:31<09:05, 9855.32\nexamples/s]', '\\rMap:  27%|##7       | 2030000/7400423 [03:31<09:00, 9943.45\nexamples/s]', '\\rMap:  27%|##7       | 2031000/7400423 [03:31<09:00, 9939.57\nexamples/s]', '\\rMap:  27%|##7       | 2033000/7400423 [03:31<08:55, 10027.30\nexamples/s]', '\\rMap:  27%|##7       | 2034000/7400423 [03:31<08:58, 9962.81\nexamples/s] ', '\\rMap:  28%|##7       | 2036000/7400423 [03:31<08:54, 10038.49\nexamples/s]', '\\rMap:  28%|##7       | 2037000/7400423 [03:32<08:55, 10015.92\nexamples/s]', '\\rMap:  28%|##7       | 2039000/7400423 [03:32<08:46, 10189.14\nexamples/s]', '\\rMap:  28%|##7       | 2041000/7400423 [03:32<08:38, 10339.22\nexamples/s]', '\\rMap:  28%|##7       | 2043000/7400423 [03:32<08:36, 10374.48\nexamples/s]', '\\rMap:  28%|##7       | 2045000/7400423 [03:32<08:47, 10161.01\nexamples/s]', '\\rMap:  28%|##7       | 2047000/7400423 [03:33<09:17, 9600.06\nexamples/s] ', '\\rMap:  28%|##7       | 2048000/7400423 [03:33<09:20, 9555.76\nexamples/s]', '\\rMap:  28%|##7       | 2049000/7400423 [03:33<09:18, 9582.50\nexamples/s]', '\\rMap:  28%|##7       | 2050000/7400423 [03:33<09:31, 9364.45\nexamples/s]', '\\rMap:  28%|##7       | 2051000/7400423 [03:33<09:31, 9364.66\nexamples/s]', '\\rMap:  28%|##7       | 2052000/7400423 [03:33<09:38, 9241.27\nexamples/s]', '\\rMap:  28%|##7       | 2053000/7400423 [03:33<09:52, 9021.59\nexamples/s]', '\\rMap:  28%|##7       | 2054000/7400423 [03:33<09:52, 9016.37\nexamples/s]', '\\rMap:  28%|##7       | 2055000/7400423 [03:33<09:45, 9125.81\nexamples/s]', '\\rMap:  28%|##7       | 2056000/7400423 [03:34<09:45, 9127.32\nexamples/s]', '\\rMap:  28%|##7       | 2057000/7400423 [03:34<09:39, 9222.70\nexamples/s]', '\\rMap:  28%|##7       | 2059000/7400423 [03:34<09:02, 9851.22\nexamples/s]', '\\rMap:  28%|##7       | 2061000/7400423 [03:34<08:21, 10636.88\nexamples/s]', '\\rMap:  28%|##7       | 2063000/7400423 [03:34<08:07, 10953.66\nexamples/s]', '\\rMap:  28%|##7       | 2065000/7400423 [03:34<08:12, 10825.98\nexamples/s]', '\\rMap:  28%|##7       | 2067000/7400423 [03:35<08:11, 10856.53\nexamples/s]', '\\rMap:  28%|##7       | 2069000/7400423 [03:35<08:07, 10935.04\nexamples/s]', '\\rMap:  28%|##7       | 2071000/7400423 [03:35<08:06, 10948.82\nexamples/s]', '\\rMap:  28%|##8       | 2073000/7400423 [03:35<08:10, 10856.53\nexamples/s]', '\\rMap:  28%|##8       | 2075000/7400423 [03:35<08:14, 10767.33\nexamples/s]', '\\rMap:  28%|##8       | 2077000/7400423 [03:35<08:25, 10523.07\nexamples/s]', '\\rMap:  28%|##8       | 2079000/7400423 [03:36<08:29, 10445.00\nexamples/s]', '\\rMap:  28%|##8       | 2081000/7400423 [03:36<08:23, 10567.49\nexamples/s]', '\\rMap:  28%|##8       | 2083000/7400423 [03:36<11:15, 7868.85\nexamples/s] ', '\\rMap:  28%|##8       | 2084000/7400423 [03:36<10:51, 8157.73\nexamples/s]', '\\rMap:  28%|##8       | 2085000/7400423 [03:36<10:28, 8461.55\nexamples/s]', '\\rMap:  28%|##8       | 2086000/7400423 [03:37<10:06, 8761.59\nexamples/s]', '\\rMap:  28%|##8       | 2088000/7400423 [03:37<09:29, 9323.79\nexamples/s]', '\\rMap:  28%|##8       | 2090000/7400423 [03:37<09:04, 9751.36\nexamples/s]', '\\rMap:  28%|##8       | 2092000/7400423 [03:37<08:37, 10252.09\nexamples/s]', '\\rMap:  28%|##8       | 2094000/7400423 [03:37<08:42, 10163.65\nexamples/s]', '\\rMap:  28%|##8       | 2096000/7400423 [03:37<08:51, 9972.05\nexamples/s] ', '\\rMap:  28%|##8       | 2098000/7400423 [03:38<08:54, 9922.57\nexamples/s]', '\\rMap:  28%|##8       | 2100000/7400423 [03:38<09:07, 9678.39\nexamples/s]', '\\rMap:  28%|##8       | 2101000/7400423 [03:38<09:12, 9585.67\nexamples/s]', '\\rMap:  28%|##8       | 2102000/7400423 [03:38<09:13, 9575.21\nexamples/s]', '\\rMap:  28%|##8       | 2104000/7400423 [03:38<09:00, 9791.23\nexamples/s]', '\\rMap:  28%|##8       | 2106000/7400423 [03:39<08:51, 9956.61\nexamples/s]', '\\rMap:  28%|##8       | 2108000/7400423 [03:39<08:48, 10010.87\nexamples/s]', '\\rMap:  29%|##8       | 2110000/7400423 [03:39<08:44, 10094.44\nexamples/s]', '\\rMap:  29%|##8       | 2112000/7400423 [03:39<08:46, 10051.53\nexamples/s]', '\\rMap:  29%|##8       | 2114000/7400423 [03:39<08:43, 10106.25\nexamples/s]', '\\rMap:  29%|##8       | 2116000/7400423 [03:40<08:42, 10116.26\nexamples/s]', '\\rMap:  29%|##8       | 2118000/7400423 [03:40<08:20, 10545.51\nexamples/s]', '\\rMap:  29%|##8       | 2120000/7400423 [03:40<08:11, 10745.30\nexamples/s]', '\\rMap:  29%|##8       | 2122000/7400423 [03:40<08:05, 10866.50\nexamples/s]', '\\rMap:  29%|##8       | 2124000/7400423 [03:40<08:03, 10922.69\nexamples/s]', '\\rMap:  29%|##8       | 2126000/7400423 [03:40<07:59, 10999.98\nexamples/s]', '\\rMap:  29%|##8       | 2128000/7400423 [03:41<07:54, 11115.29\nexamples/s]', '\\rMap:  29%|##8       | 2130000/7400423 [03:41<07:49, 11221.95\nexamples/s]', '\\rMap:  29%|##8       | 2132000/7400423 [03:41<07:54, 11099.19\nexamples/s]', '\\rMap:  29%|##8       | 2134000/7400423 [03:41<07:52, 11146.09\nexamples/s]', '\\rMap:  29%|##8       | 2136000/7400423 [03:41<07:51, 11156.57\nexamples/s]', '\\rMap:  29%|##8       | 2138000/7400423 [03:41<07:54, 11081.54\nexamples/s]', '\\rMap:  29%|##8       | 2140000/7400423 [03:42<07:57, 11007.75\nexamples/s]', '\\rMap:  29%|##8       | 2142000/7400423 [03:42<07:57, 11003.85\nexamples/s]', '\\rMap:  29%|##8       | 2144000/7400423 [03:42<08:05, 10820.42\nexamples/s]', '\\rMap:  29%|##8       | 2146000/7400423 [03:42<08:11, 10696.45\nexamples/s]', '\\rMap:  29%|##9       | 2148000/7400423 [03:42<08:11, 10688.64\nexamples/s]', '\\rMap:  29%|##9       | 2150000/7400423 [03:43<08:18, 10528.33\nexamples/s]', '\\rMap:  29%|##9       | 2152000/7400423 [03:43<08:24, 10401.12\nexamples/s]', '\\rMap:  29%|##9       | 2154000/7400423 [03:43<08:25, 10382.86\nexamples/s]', '\\rMap:  29%|##9       | 2156000/7400423 [03:43<08:33, 10219.53\nexamples/s]', '\\rMap:  29%|##9       | 2158000/7400423 [03:43<08:35, 10173.62\nexamples/s]', '\\rMap:  29%|##9       | 2160000/7400423 [03:44<11:02, 7909.69\nexamples/s] ', '\\rMap:  29%|##9       | 2162000/7400423 [03:44<10:11, 8567.19\nexamples/s]', '\\rMap:  29%|##9       | 2164000/7400423 [03:44<09:27, 9231.17\nexamples/s]', '\\rMap:  29%|##9       | 2166000/7400423 [03:44<09:02, 9643.51\nexamples/s]', '\\rMap:  29%|##9       | 2168000/7400423 [03:45<08:36, 10124.37\nexamples/s]', '\\rMap:  29%|##9       | 2170000/7400423 [03:45<08:23, 10391.06\nexamples/s]', '\\rMap:  29%|##9       | 2172000/7400423 [03:45<08:14, 10567.03\nexamples/s]', '\\rMap:  29%|##9       | 2174000/7400423 [03:45<08:08, 10697.36\nexamples/s]', '\\rMap:  29%|##9       | 2176000/7400423 [03:45<08:09, 10682.92\nexamples/s]', '\\rMap:  29%|##9       | 2178000/7400423 [03:45<08:02, 10816.40\nexamples/s]', '\\rMap:  29%|##9       | 2180000/7400423 [03:46<07:56, 10963.80\nexamples/s]', '\\rMap:  29%|##9       | 2182000/7400423 [03:46<07:45, 11203.61\nexamples/s]', '\\rMap:  30%|##9       | 2184000/7400423 [03:46<07:42, 11283.62\nexamples/s]', '\\rMap:  30%|##9       | 2186000/7400423 [03:46<07:48, 11121.16\nexamples/s]', '\\rMap:  30%|##9       | 2188000/7400423 [03:46<07:51, 11064.64\nexamples/s]', '\\rMap:  30%|##9       | 2190000/7400423 [03:46<07:41, 11299.78\nexamples/s]', '\\rMap:  30%|##9       | 2192000/7400423 [03:47<07:38, 11365.54\nexamples/s]', '\\rMap:  30%|##9       | 2194000/7400423 [03:47<07:35, 11433.73\nexamples/s]', '\\rMap:  30%|##9       | 2196000/7400423 [03:47<07:24, 11715.71\nexamples/s]', '\\rMap:  30%|##9       | 2198000/7400423 [03:47<07:19, 11824.83\nexamples/s]', '\\rMap:  30%|##9       | 2200000/7400423 [03:47<07:14, 11960.82\nexamples/s]', '\\rMap:  30%|##9       | 2202000/7400423 [03:47<07:18, 11851.52\nexamples/s]', '\\rMap:  30%|##9       | 2204000/7400423 [03:48<07:18, 11863.70\nexamples/s]', '\\rMap:  30%|##9       | 2206000/7400423 [03:48<07:20, 11788.36\nexamples/s]', '\\rMap:  30%|##9       | 2208000/7400423 [03:48<07:21, 11751.44\nexamples/s]', '\\rMap:  30%|##9       | 2210000/7400423 [03:48<07:21, 11747.20\nexamples/s]', '\\rMap:  30%|##9       | 2212000/7400423 [03:48<07:24, 11661.86\nexamples/s]', '\\rMap:  30%|##9       | 2214000/7400423 [03:49<07:28, 11560.48\nexamples/s]', '\\rMap:  30%|##9       | 2216000/7400423 [03:49<07:30, 11502.05\nexamples/s]', '\\rMap:  30%|##9       | 2218000/7400423 [03:49<07:33, 11438.76\nexamples/s]', '\\rMap:  30%|##9       | 2220000/7400423 [03:49<07:37, 11328.00\nexamples/s]', '\\rMap:  30%|###       | 2222000/7400423 [03:49<07:39, 11279.04\nexamples/s]', '\\rMap:  30%|###       | 2224000/7400423 [03:49<07:41, 11217.20\nexamples/s]', '\\rMap:  30%|###       | 2226000/7400423 [03:50<07:41, 11205.79\nexamples/s]', '\\rMap:  30%|###       | 2228000/7400423 [03:50<07:46, 11078.48\nexamples/s]', '\\rMap:  30%|###       | 2230000/7400423 [03:50<07:37, 11295.33\nexamples/s]', '\\rMap:  30%|###       | 2232000/7400423 [03:50<07:29, 11499.80\nexamples/s]', '\\rMap:  30%|###       | 2234000/7400423 [03:50<07:21, 11705.32\nexamples/s]', '\\rMap:  30%|###       | 2236000/7400423 [03:50<07:16, 11832.54\nexamples/s]', '\\rMap:  30%|###       | 2238000/7400423 [03:51<07:19, 11742.21\nexamples/s]', '\\rMap:  30%|###       | 2240000/7400423 [03:51<07:17, 11789.83\nexamples/s]', '\\rMap:  30%|###       | 2242000/7400423 [03:51<07:13, 11912.30\nexamples/s]', '\\rMap:  30%|###       | 2244000/7400423 [03:51<07:16, 11804.68\nexamples/s]', '\\rMap:  30%|###       | 2246000/7400423 [03:51<07:16, 11805.01\nexamples/s]', '\\rMap:  30%|###       | 2248000/7400423 [03:51<07:11, 11930.41\nexamples/s]', '\\rMap:  30%|###       | 2250000/7400423 [03:52<07:06, 12066.45\nexamples/s]', '\\rMap:  30%|###       | 2252000/7400423 [03:52<07:10, 11971.59\nexamples/s]', '\\rMap:  30%|###       | 2254000/7400423 [03:52<07:08, 12012.84\nexamples/s]', '\\rMap:  30%|###       | 2256000/7400423 [03:52<07:07, 12033.31\nexamples/s]', '\\rMap:  31%|###       | 2258000/7400423 [03:52<07:09, 11978.02\nexamples/s]', '\\rMap:  31%|###       | 2260000/7400423 [03:52<07:09, 11955.22\nexamples/s]', '\\rMap:  31%|###       | 2262000/7400423 [03:53<07:16, 11776.57\nexamples/s]', '\\rMap:  31%|###       | 2264000/7400423 [03:53<07:16, 11759.72\nexamples/s]', '\\rMap:  31%|###       | 2266000/7400423 [03:53<07:16, 11768.60\nexamples/s]', '\\rMap:  31%|###       | 2268000/7400423 [03:53<07:17, 11732.38\nexamples/s]', '\\rMap:  31%|###       | 2270000/7400423 [03:53<07:24, 11553.29\nexamples/s]', '\\rMap:  31%|###       | 2272000/7400423 [03:53<07:26, 11484.69\nexamples/s]', '\\rMap:  31%|###       | 2274000/7400423 [03:54<07:33, 11305.36\nexamples/s]', '\\rMap:  31%|###       | 2276000/7400423 [03:54<07:32, 11320.08\nexamples/s]', '\\rMap:  31%|###       | 2278000/7400423 [03:54<07:34, 11281.80\nexamples/s]', '\\rMap:  31%|###       | 2280000/7400423 [03:54<07:34, 11276.00\nexamples/s]', '\\rMap:  31%|###       | 2282000/7400423 [03:54<07:34, 11257.66\nexamples/s]', '\\rMap:  31%|###       | 2284000/7400423 [03:55<07:31, 11344.17\nexamples/s]', '\\rMap:  31%|###       | 2286000/7400423 [03:55<09:57, 8563.94\nexamples/s] ', '\\rMap:  31%|###       | 2288000/7400423 [03:55<09:08, 9314.93\nexamples/s]', '\\rMap:  31%|###       | 2290000/7400423 [03:55<08:39, 9843.16\nexamples/s]', '\\rMap:  31%|###       | 2292000/7400423 [03:55<08:12, 10367.36\nexamples/s]', '\\rMap:  31%|###       | 2294000/7400423 [03:56<07:57, 10701.58\nexamples/s]', '\\rMap:  31%|###1      | 2296000/7400423 [03:56<07:46, 10938.52\nexamples/s]', '\\rMap:  31%|###1      | 2298000/7400423 [03:56<07:36, 11171.29\nexamples/s]', '\\rMap:  31%|###1      | 2300000/7400423 [03:56<07:29, 11334.87\nexamples/s]', '\\rMap:  31%|###1      | 2302000/7400423 [03:56<07:27, 11388.24\nexamples/s]', '\\rMap:  31%|###1      | 2304000/7400423 [03:56<07:20, 11571.74\nexamples/s]', '\\rMap:  31%|###1      | 2306000/7400423 [03:57<07:18, 11614.83\nexamples/s]', '\\rMap:  31%|###1      | 2308000/7400423 [03:57<07:13, 11733.99\nexamples/s]', '\\rMap:  31%|###1      | 2310000/7400423 [03:57<07:15, 11681.07\nexamples/s]', '\\rMap:  31%|###1      | 2312000/7400423 [03:57<07:12, 11760.86\nexamples/s]', '\\rMap:  31%|###1      | 2314000/7400423 [03:57<07:17, 11626.61\nexamples/s]', '\\rMap:  31%|###1      | 2316000/7400423 [03:57<07:14, 11695.47\nexamples/s]', '\\rMap:  31%|###1      | 2318000/7400423 [03:58<07:14, 11710.56\nexamples/s]', '\\rMap:  31%|###1      | 2320000/7400423 [03:58<07:19, 11558.90\nexamples/s]', '\\rMap:  31%|###1      | 2322000/7400423 [03:58<07:12, 11730.71\nexamples/s]', '\\rMap:  31%|###1      | 2324000/7400423 [03:58<07:12, 11724.81\nexamples/s]', '\\rMap:  31%|###1      | 2326000/7400423 [03:58<07:10, 11774.43\nexamples/s]', '\\rMap:  31%|###1      | 2328000/7400423 [03:59<07:10, 11784.35\nexamples/s]', '\\rMap:  31%|###1      | 2330000/7400423 [03:59<07:03, 11975.83\nexamples/s]', '\\rMap:  32%|###1      | 2332000/7400423 [03:59<07:06, 11876.29\nexamples/s]', '\\rMap:  32%|###1      | 2334000/7400423 [03:59<07:11, 11737.69\nexamples/s]', '\\rMap:  32%|###1      | 2336000/7400423 [03:59<07:20, 11504.04\nexamples/s]', '\\rMap:  32%|###1      | 2338000/7400423 [03:59<07:12, 11692.79\nexamples/s]', '\\rMap:  32%|###1      | 2340000/7400423 [04:00<07:18, 11546.17\nexamples/s]', '\\rMap:  32%|###1      | 2342000/7400423 [04:00<07:16, 11599.74\nexamples/s]', '\\rMap:  32%|###1      | 2344000/7400423 [04:00<07:18, 11525.22\nexamples/s]', '\\rMap:  32%|###1      | 2346000/7400423 [04:00<07:21, 11438.49\nexamples/s]', '\\rMap:  32%|###1      | 2348000/7400423 [04:00<07:21, 11452.65\nexamples/s]', '\\rMap:  32%|###1      | 2350000/7400423 [04:00<07:20, 11477.14\nexamples/s]', '\\rMap:  32%|###1      | 2352000/7400423 [04:01<07:20, 11459.95\nexamples/s]', '\\rMap:  32%|###1      | 2354000/7400423 [04:01<07:20, 11464.08\nexamples/s]', '\\rMap:  32%|###1      | 2356000/7400423 [04:01<07:20, 11445.59\nexamples/s]', '\\rMap:  32%|###1      | 2358000/7400423 [04:01<07:20, 11459.95\nexamples/s]', '\\rMap:  32%|###1      | 2360000/7400423 [04:01<07:25, 11324.95\nexamples/s]', '\\rMap:  32%|###1      | 2362000/7400423 [04:01<07:33, 11103.50\nexamples/s]', '\\rMap:  32%|###1      | 2364000/7400423 [04:02<07:37, 11013.69\nexamples/s]', '\\rMap:  32%|###1      | 2366000/7400423 [04:02<07:42, 10883.36\nexamples/s]', '\\rMap:  32%|###1      | 2368000/7400423 [04:02<07:42, 10878.09\nexamples/s]', '\\rMap:  32%|###2      | 2370000/7400423 [04:02<08:17, 10102.21\nexamples/s]', '\\rMap:  32%|###2      | 2372000/7400423 [04:02<08:37, 9724.94\nexamples/s] ', '\\rMap:  32%|###2      | 2373000/7400423 [04:03<08:43, 9603.97\nexamples/s]', '\\rMap:  32%|###2      | 2374000/7400423 [04:03<08:48, 9515.73\nexamples/s]', '\\rMap:  32%|###2      | 2375000/7400423 [04:03<08:52, 9434.98\nexamples/s]', '\\rMap:  32%|###2      | 2376000/7400423 [04:03<08:53, 9416.04\nexamples/s]', '\\rMap:  32%|###2      | 2377000/7400423 [04:03<08:58, 9327.24\nexamples/s]', '\\rMap:  32%|###2      | 2378000/7400423 [04:03<09:12, 9091.10\nexamples/s]', '\\rMap:  32%|###2      | 2379000/7400423 [04:03<09:20, 8959.04\nexamples/s]', '\\rMap:  32%|###2      | 2380000/7400423 [04:03<09:31, 8787.30\nexamples/s]', '\\rMap:  32%|###2      | 2381000/7400423 [04:04<09:29, 8820.47\nexamples/s]', '\\rMap:  32%|###2      | 2382000/7400423 [04:04<09:28, 8826.69\nexamples/s]', '\\rMap:  32%|###2      | 2383000/7400423 [04:04<09:21, 8943.57\nexamples/s]', '\\rMap:  32%|###2      | 2384000/7400423 [04:04<14:38, 5713.27\nexamples/s]', '\\rMap:  32%|###2      | 2385000/7400423 [04:04<13:07, 6366.31\nexamples/s]', '\\rMap:  32%|###2      | 2386000/7400423 [04:04<11:54, 7018.15\nexamples/s]', '\\rMap:  32%|###2      | 2387000/7400423 [04:04<11:30, 7258.58\nexamples/s]', '\\rMap:  32%|###2      | 2388000/7400423 [04:05<11:04, 7537.67\nexamples/s]', '\\rMap:  32%|###2      | 2389000/7400423 [04:05<10:45, 7769.41\nexamples/s]', '\\rMap:  32%|###2      | 2390000/7400423 [04:05<10:21, 8064.02\nexamples/s]', '\\rMap:  32%|###2      | 2391000/7400423 [04:05<10:25, 8007.16\nexamples/s]', '\\rMap:  32%|###2      | 2392000/7400423 [04:05<10:37, 7856.91\nexamples/s]', '\\rMap:  32%|###2      | 2393000/7400423 [04:05<10:55, 7641.22\nexamples/s]', '\\rMap:  32%|###2      | 2394000/7400423 [04:05<10:45, 7751.30\nexamples/s]', '\\rMap:  32%|###2      | 2395000/7400423 [04:05<10:36, 7862.51\nexamples/s]', '\\rMap:  32%|###2      | 2396000/7400423 [04:06<10:25, 7997.41\nexamples/s]', '\\rMap:  32%|###2      | 2397000/7400423 [04:06<10:07, 8242.65\nexamples/s]', '\\rMap:  32%|###2      | 2398000/7400423 [04:06<10:05, 8268.46\nexamples/s]', '\\rMap:  32%|###2      | 2399000/7400423 [04:06<10:02, 8296.77\nexamples/s]', '\\rMap:  32%|###2      | 2400000/7400423 [04:06<09:40, 8617.30\nexamples/s]', '\\rMap:  32%|###2      | 2401000/7400423 [04:06<09:25, 8836.86\nexamples/s]', '\\rMap:  32%|###2      | 2402000/7400423 [04:06<09:12, 9044.35\nexamples/s]', '\\rMap:  32%|###2      | 2403000/7400423 [04:06<09:14, 9011.12\nexamples/s]', '\\rMap:  32%|###2      | 2404000/7400423 [04:06<09:24, 8853.22\nexamples/s]', '\\rMap:  32%|###2      | 2405000/7400423 [04:07<09:24, 8852.36\nexamples/s]', '\\rMap:  33%|###2      | 2406000/7400423 [04:07<09:21, 8895.25\nexamples/s]', '\\rMap:  33%|###2      | 2407000/7400423 [04:07<09:29, 8773.06\nexamples/s]', '\\rMap:  33%|###2      | 2408000/7400423 [04:07<09:17, 8953.80\nexamples/s]', '\\rMap:  33%|###2      | 2409000/7400423 [04:07<09:06, 9135.41\nexamples/s]', '\\rMap:  33%|###2      | 2411000/7400423 [04:07<08:37, 9638.95\nexamples/s]', '\\rMap:  33%|###2      | 2413000/7400423 [04:07<08:11, 10143.33\nexamples/s]', '\\rMap:  33%|###2      | 2415000/7400423 [04:08<07:55, 10475.07\nexamples/s]', '\\rMap:  33%|###2      | 2417000/7400423 [04:08<07:45, 10713.25\nexamples/s]', '\\rMap:  33%|###2      | 2419000/7400423 [04:08<07:42, 10779.98\nexamples/s]', '\\rMap:  33%|###2      | 2421000/7400423 [04:08<07:44, 10710.11\nexamples/s]', '\\rMap:  33%|###2      | 2423000/7400423 [04:08<07:35, 10931.95\nexamples/s]', '\\rMap:  33%|###2      | 2425000/7400423 [04:08<07:34, 10938.82\nexamples/s]', '\\rMap:  33%|###2      | 2427000/7400423 [04:09<07:27, 11105.11\nexamples/s]', '\\rMap:  33%|###2      | 2429000/7400423 [04:09<07:31, 11002.00\nexamples/s]', '\\rMap:  33%|###2      | 2431000/7400423 [04:09<07:36, 10884.74\nexamples/s]', '\\rMap:  33%|###2      | 2433000/7400423 [04:09<07:33, 10950.95\nexamples/s]', '\\rMap:  33%|###2      | 2435000/7400423 [04:09<07:31, 11009.75\nexamples/s]', '\\rMap:  33%|###2      | 2437000/7400423 [04:10<07:21, 11241.07\nexamples/s]', '\\rMap:  33%|###2      | 2439000/7400423 [04:10<07:30, 11018.80\nexamples/s]', '\\rMap:  33%|###2      | 2441000/7400423 [04:10<07:27, 11079.45\nexamples/s]', '\\rMap:  33%|###3      | 2443000/7400423 [04:10<07:21, 11227.12\nexamples/s]', '\\rMap:  33%|###3      | 2445000/7400423 [04:10<07:16, 11348.45\nexamples/s]', '\\rMap:  33%|###3      | 2447000/7400423 [04:10<07:24, 11144.53\nexamples/s]', '\\rMap:  33%|###3      | 2449000/7400423 [04:11<07:32, 10941.75\nexamples/s]', '\\rMap:  33%|###3      | 2451000/7400423 [04:11<07:30, 10983.80\nexamples/s]', '\\rMap:  33%|###3      | 2453000/7400423 [04:11<07:24, 11119.24\nexamples/s]', '\\rMap:  33%|###3      | 2455000/7400423 [04:11<07:28, 11019.51\nexamples/s]', '\\rMap:  33%|###3      | 2457000/7400423 [04:11<07:23, 11141.85\nexamples/s]', '\\rMap:  33%|###3      | 2459000/7400423 [04:11<07:18, 11278.52\nexamples/s]', '\\rMap:  33%|###3      | 2461000/7400423 [04:12<07:17, 11297.45\nexamples/s]', '\\rMap:  33%|###3      | 2463000/7400423 [04:12<07:27, 11044.38\nexamples/s]', '\\rMap:  33%|###3      | 2465000/7400423 [04:12<07:27, 11032.12\nexamples/s]', '\\rMap:  33%|###3      | 2467000/7400423 [04:12<07:27, 11025.70\nexamples/s]', '\\rMap:  33%|###3      | 2469000/7400423 [04:12<07:30, 10956.92\nexamples/s]', '\\rMap:  33%|###3      | 2471000/7400423 [04:13<09:54, 8295.29\nexamples/s] ', '\\rMap:  33%|###3      | 2473000/7400423 [04:13<09:03, 9062.04\nexamples/s]', '\\rMap:  33%|###3      | 2475000/7400423 [04:13<08:37, 9520.76\nexamples/s]', '\\rMap:  33%|###3      | 2477000/7400423 [04:13<08:12, 9986.78\nexamples/s]', '\\rMap:  33%|###3      | 2479000/7400423 [04:13<07:56, 10332.04\nexamples/s]', '\\rMap:  34%|###3      | 2481000/7400423 [04:14<07:44, 10587.10\nexamples/s]', '\\rMap:  34%|###3      | 2483000/7400423 [04:14<07:45, 10569.63\nexamples/s]', '\\rMap:  34%|###3      | 2485000/7400423 [04:14<07:42, 10631.01\nexamples/s]', '\\rMap:  34%|###3      | 2487000/7400423 [04:14<07:38, 10710.48\nexamples/s]', '\\rMap:  34%|###3      | 2489000/7400423 [04:14<07:39, 10696.64\nexamples/s]', '\\rMap:  34%|###3      | 2491000/7400423 [04:15<07:42, 10615.27\nexamples/s]', '\\rMap:  34%|###3      | 2493000/7400423 [04:15<07:36, 10758.27\nexamples/s]', '\\rMap:  34%|###3      | 2495000/7400423 [04:15<07:25, 11008.94\nexamples/s]', '\\rMap:  34%|###3      | 2497000/7400423 [04:15<07:18, 11187.85\nexamples/s]', '\\rMap:  34%|###3      | 2499000/7400423 [04:15<07:28, 10940.26\nexamples/s]', '\\rMap:  34%|###3      | 2501000/7400423 [04:16<07:23, 11048.06\nexamples/s]', '\\rMap:  34%|###3      | 2503000/7400423 [04:16<07:16, 11221.33\nexamples/s]', '\\rMap:  34%|###3      | 2505000/7400423 [04:16<07:11, 11334.42\nexamples/s]', '\\rMap:  34%|###3      | 2507000/7400423 [04:16<07:10, 11376.99\nexamples/s]', '\\rMap:  34%|###3      | 2509000/7400423 [04:16<07:11, 11345.47\nexamples/s]', '\\rMap:  34%|###3      | 2511000/7400423 [04:16<07:17, 11187.61\nexamples/s]', '\\rMap:  34%|###3      | 2513000/7400423 [04:17<07:16, 11202.13\nexamples/s]', '\\rMap:  34%|###3      | 2515000/7400423 [04:17<07:18, 11141.22\nexamples/s]', '\\rMap:  34%|###4      | 2517000/7400423 [04:17<07:13, 11262.41\nexamples/s]', '\\rMap:  34%|###4      | 2519000/7400423 [04:17<07:11, 11316.56\nexamples/s]', '\\rMap:  34%|###4      | 2521000/7400423 [04:17<07:11, 11295.58\nexamples/s]', '\\rMap:  34%|###4      | 2523000/7400423 [04:17<07:15, 11206.57\nexamples/s]', '\\rMap:  34%|###4      | 2525000/7400423 [04:18<07:10, 11316.25\nexamples/s]', '\\rMap:  34%|###4      | 2527000/7400423 [04:18<07:09, 11345.11\nexamples/s]', '\\rMap:  34%|###4      | 2529000/7400423 [04:18<07:05, 11452.25\nexamples/s]', '\\rMap:  34%|###4      | 2531000/7400423 [04:18<07:05, 11442.83\nexamples/s]', '\\rMap:  34%|###4      | 2533000/7400423 [04:18<07:06, 11412.33\nexamples/s]', '\\rMap:  34%|###4      | 2535000/7400423 [04:19<07:12, 11251.76\nexamples/s]', '\\rMap:  34%|###4      | 2537000/7400423 [04:19<07:10, 11290.86\nexamples/s]', '\\rMap:  34%|###4      | 2539000/7400423 [04:19<07:13, 11224.04\nexamples/s]', '\\rMap:  34%|###4      | 2541000/7400423 [04:19<07:16, 11129.27\nexamples/s]', '\\rMap:  34%|###4      | 2543000/7400423 [04:19<07:20, 11018.31\nexamples/s]', '\\rMap:  34%|###4      | 2545000/7400423 [04:19<07:15, 11139.01\nexamples/s]', '\\rMap:  34%|###4      | 2547000/7400423 [04:20<09:44, 8305.29\nexamples/s] ', '\\rMap:  34%|###4      | 2549000/7400423 [04:20<08:58, 9009.90\nexamples/s]', '\\rMap:  34%|###4      | 2551000/7400423 [04:20<08:21, 9668.89\nexamples/s]', '\\rMap:  34%|###4      | 2553000/7400423 [04:20<07:57, 10145.06\nexamples/s]', '\\rMap:  35%|###4      | 2555000/7400423 [04:20<07:45, 10418.27\nexamples/s]', '\\rMap:  35%|###4      | 2557000/7400423 [04:21<07:27, 10819.12\nexamples/s]', '\\rMap:  35%|###4      | 2559000/7400423 [04:21<07:20, 10996.20\nexamples/s]', '\\rMap:  35%|###4      | 2561000/7400423 [04:21<07:13, 11175.13\nexamples/s]', '\\rMap:  35%|###4      | 2563000/7400423 [04:21<07:18, 11029.79\nexamples/s]', '\\rMap:  35%|###4      | 2565000/7400423 [04:21<07:17, 11060.04\nexamples/s]', '\\rMap:  35%|###4      | 2567000/7400423 [04:22<07:16, 11066.28\nexamples/s]', '\\rMap:  35%|###4      | 2569000/7400423 [04:22<07:14, 11115.01\nexamples/s]', '\\rMap:  35%|###4      | 2571000/7400423 [04:22<07:09, 11255.87\nexamples/s]', '\\rMap:  35%|###4      | 2573000/7400423 [04:22<07:13, 11123.11\nexamples/s]', '\\rMap:  35%|###4      | 2575000/7400423 [04:22<07:12, 11146.74\nexamples/s]', '\\rMap:  35%|###4      | 2577000/7400423 [04:22<07:08, 11245.01\nexamples/s]', '\\rMap:  35%|###4      | 2579000/7400423 [04:23<06:57, 11548.70\nexamples/s]', '\\rMap:  35%|###4      | 2581000/7400423 [04:23<06:58, 11522.55\nexamples/s]', '\\rMap:  35%|###4      | 2583000/7400423 [04:23<06:59, 11496.71\nexamples/s]', '\\rMap:  35%|###4      | 2585000/7400423 [04:23<07:01, 11426.35\nexamples/s]', '\\rMap:  35%|###4      | 2587000/7400423 [04:23<06:58, 11499.48\nexamples/s]', '\\rMap:  35%|###4      | 2589000/7400423 [04:23<07:10, 11184.30\nexamples/s]', '\\rMap:  35%|###5      | 2591000/7400423 [04:24<07:13, 11103.39\nexamples/s]', '\\rMap:  35%|###5      | 2593000/7400423 [04:24<07:18, 10961.80\nexamples/s]', '\\rMap:  35%|###5      | 2595000/7400423 [04:24<07:16, 10997.28\nexamples/s]', '\\rMap:  35%|###5      | 2597000/7400423 [04:24<07:15, 11017.17\nexamples/s]', '\\rMap:  35%|###5      | 2599000/7400423 [04:24<07:17, 10966.17\nexamples/s]', '\\rMap:  35%|###5      | 2601000/7400423 [04:25<07:23, 10818.27\nexamples/s]', '\\rMap:  35%|###5      | 2603000/7400423 [04:25<07:24, 10790.87\nexamples/s]', '\\rMap:  35%|###5      | 2605000/7400423 [04:25<07:41, 10388.78\nexamples/s]', '\\rMap:  35%|###5      | 2607000/7400423 [04:25<07:47, 10242.40\nexamples/s]', '\\rMap:  35%|###5      | 2609000/7400423 [04:25<07:44, 10304.68\nexamples/s]', '\\rMap:  35%|###5      | 2611000/7400423 [04:26<07:35, 10506.80\nexamples/s]', '\\rMap:  35%|###5      | 2613000/7400423 [04:26<07:25, 10746.95\nexamples/s]', '\\rMap:  35%|###5      | 2615000/7400423 [04:26<07:14, 11001.27\nexamples/s]', '\\rMap:  35%|###5      | 2617000/7400423 [04:26<07:06, 11209.02\nexamples/s]', '\\rMap:  35%|###5      | 2619000/7400423 [04:26<07:05, 11233.01\nexamples/s]', '\\rMap:  35%|###5      | 2621000/7400423 [04:26<06:57, 11439.73\nexamples/s]', '\\rMap:  35%|###5      | 2623000/7400423 [04:27<06:56, 11465.39\nexamples/s]', '\\rMap:  35%|###5      | 2625000/7400423 [04:27<06:59, 11384.60\nexamples/s]', '\\rMap:  35%|###5      | 2627000/7400423 [04:27<07:00, 11355.70\nexamples/s]', '\\rMap:  36%|###5      | 2629000/7400423 [04:27<07:00, 11337.78\nexamples/s]', '\\rMap:  36%|###5      | 2631000/7400423 [04:27<07:03, 11262.95\nexamples/s]', '\\rMap:  36%|###5      | 2633000/7400423 [04:28<07:01, 11310.09\nexamples/s]', '\\rMap:  36%|###5      | 2635000/7400423 [04:28<06:56, 11443.67\nexamples/s]', '\\rMap:  36%|###5      | 2637000/7400423 [04:28<07:04, 11220.03\nexamples/s]', '\\rMap:  36%|###5      | 2639000/7400423 [04:28<07:12, 11014.41\nexamples/s]', '\\rMap:  36%|###5      | 2641000/7400423 [04:28<07:13, 10968.95\nexamples/s]', '\\rMap:  36%|###5      | 2643000/7400423 [04:28<07:10, 11062.66\nexamples/s]', '\\rMap:  36%|###5      | 2645000/7400423 [04:29<07:10, 11039.96\nexamples/s]', '\\rMap:  36%|###5      | 2647000/7400423 [04:29<07:06, 11148.97\nexamples/s]', '\\rMap:  36%|###5      | 2649000/7400423 [04:29<07:04, 11194.39\nexamples/s]', '\\rMap:  36%|###5      | 2651000/7400423 [04:29<07:10, 11034.55\nexamples/s]', '\\rMap:  36%|###5      | 2653000/7400423 [04:29<07:11, 10999.77\nexamples/s]', '\\rMap:  36%|###5      | 2655000/7400423 [04:29<07:11, 10998.48\nexamples/s]', '\\rMap:  36%|###5      | 2657000/7400423 [04:30<07:12, 10957.50\nexamples/s]', '\\rMap:  36%|###5      | 2659000/7400423 [04:30<07:13, 10937.29\nexamples/s]', '\\rMap:  36%|###5      | 2661000/7400423 [04:30<07:11, 10973.14\nexamples/s]', '\\rMap:  36%|###5      | 2663000/7400423 [04:30<07:13, 10938.12\nexamples/s]', '\\rMap:  36%|###6      | 2665000/7400423 [04:30<07:16, 10844.39\nexamples/s]', '\\rMap:  36%|###6      | 2667000/7400423 [04:31<07:16, 10836.92\nexamples/s]', '\\rMap:  36%|###6      | 2669000/7400423 [04:31<07:11, 10962.35\nexamples/s]', '\\rMap:  36%|###6      | 2671000/7400423 [04:31<07:05, 11104.77\nexamples/s]', '\\rMap:  36%|###6      | 2673000/7400423 [04:31<09:25, 8364.03\nexamples/s] ', '\\rMap:  36%|###6      | 2675000/7400423 [04:32<08:50, 8903.84\nexamples/s]', '\\rMap:  36%|###6      | 2677000/7400423 [04:32<08:26, 9332.63\nexamples/s]', '\\rMap:  36%|###6      | 2679000/7400423 [04:32<08:07, 9685.42\nexamples/s]', '\\rMap:  36%|###6      | 2681000/7400423 [04:32<07:46, 10127.40\nexamples/s]', '\\rMap:  36%|###6      | 2683000/7400423 [04:32<07:38, 10294.51\nexamples/s]', '\\rMap:  36%|###6      | 2685000/7400423 [04:32<07:35, 10362.68\nexamples/s]', '\\rMap:  36%|###6      | 2687000/7400423 [04:33<07:36, 10331.52\nexamples/s]', '\\rMap:  36%|###6      | 2689000/7400423 [04:33<07:44, 10144.74\nexamples/s]', '\\rMap:  36%|###6      | 2691000/7400423 [04:33<07:48, 10050.44\nexamples/s]', '\\rMap:  36%|###6      | 2693000/7400423 [04:33<07:50, 9995.81\nexamples/s] ', '\\rMap:  36%|###6      | 2695000/7400423 [04:33<07:47, 10066.36\nexamples/s]', '\\rMap:  36%|###6      | 2697000/7400423 [04:34<07:55, 9895.54\nexamples/s] ', '\\rMap:  36%|###6      | 2698000/7400423 [04:34<08:00, 9792.47\nexamples/s]', '\\rMap:  36%|###6      | 2700000/7400423 [04:34<07:59, 9809.79\nexamples/s]', '\\rMap:  37%|###6      | 2702000/7400423 [04:34<07:52, 9950.19\nexamples/s]', '\\rMap:  37%|###6      | 2704000/7400423 [04:34<07:54, 9905.54\nexamples/s]', '\\rMap:  37%|###6      | 2705000/7400423 [04:34<07:56, 9852.84\nexamples/s]', '\\rMap:  37%|###6      | 2707000/7400423 [04:35<07:52, 9924.18\nexamples/s]', '\\rMap:  37%|###6      | 2709000/7400423 [04:35<07:46, 10061.85\nexamples/s]', '\\rMap:  37%|###6      | 2711000/7400423 [04:35<07:50, 9973.86\nexamples/s] ', '\\rMap:  37%|###6      | 2713000/7400423 [04:35<07:45, 10061.50\nexamples/s]', '\\rMap:  37%|###6      | 2715000/7400423 [04:35<07:41, 10145.95\nexamples/s]', '\\rMap:  37%|###6      | 2717000/7400423 [04:36<07:33, 10332.59\nexamples/s]', '\\rMap:  37%|###6      | 2719000/7400423 [04:36<07:28, 10433.72\nexamples/s]', '\\rMap:  37%|###6      | 2721000/7400423 [04:36<07:18, 10665.01\nexamples/s]', '\\rMap:  37%|###6      | 2723000/7400423 [04:36<07:09, 10882.76\nexamples/s]', '\\rMap:  37%|###6      | 2725000/7400423 [04:36<07:08, 10915.82\nexamples/s]', '\\rMap:  37%|###6      | 2727000/7400423 [04:37<07:09, 10879.53\nexamples/s]', '\\rMap:  37%|###6      | 2729000/7400423 [04:37<07:22, 10567.42\nexamples/s]', '\\rMap:  37%|###6      | 2731000/7400423 [04:37<07:26, 10447.28\nexamples/s]', '\\rMap:  37%|###6      | 2733000/7400423 [04:37<07:24, 10496.68\nexamples/s]', '\\rMap:  37%|###6      | 2735000/7400423 [04:37<07:16, 10698.85\nexamples/s]', '\\rMap:  37%|###6      | 2737000/7400423 [04:38<07:15, 10708.79\nexamples/s]', '\\rMap:  37%|###7      | 2739000/7400423 [04:38<07:20, 10588.92\nexamples/s]', '\\rMap:  37%|###7      | 2741000/7400423 [04:38<07:26, 10431.81\nexamples/s]', '\\rMap:  37%|###7      | 2743000/7400423 [04:38<07:30, 10328.98\nexamples/s]', '\\rMap:  37%|###7      | 2745000/7400423 [04:38<07:34, 10232.91\nexamples/s]', '\\rMap:  37%|###7      | 2747000/7400423 [04:39<07:46, 9976.95\nexamples/s] ', '\\rMap:  37%|###7      | 2748000/7400423 [04:39<07:57, 9742.17\nexamples/s]', '\\rMap:  37%|###7      | 2749000/7400423 [04:39<08:03, 9617.91\nexamples/s]', '\\rMap:  37%|###7      | 2750000/7400423 [04:39<08:17, 9354.24\nexamples/s]', '\\rMap:  37%|###7      | 2751000/7400423 [04:39<08:22, 9258.68\nexamples/s]', '\\rMap:  37%|###7      | 2752000/7400423 [04:39<08:23, 9240.54\nexamples/s]', '\\rMap:  37%|###7      | 2753000/7400423 [04:39<08:31, 9077.09\nexamples/s]', '\\rMap:  37%|###7      | 2754000/7400423 [04:39<08:33, 9042.65\nexamples/s]', '\\rMap:  37%|###7      | 2756000/7400423 [04:39<08:03, 9596.05\nexamples/s]', '\\rMap:  37%|###7      | 2757000/7400423 [04:40<08:11, 9449.34\nexamples/s]', '\\rMap:  37%|###7      | 2759000/7400423 [04:40<07:49, 9882.57\nexamples/s]', '\\rMap:  37%|###7      | 2761000/7400423 [04:40<07:43, 10007.03\nexamples/s]', '\\rMap:  37%|###7      | 2763000/7400423 [04:40<07:43, 10014.36\nexamples/s]', '\\rMap:  37%|###7      | 2765000/7400423 [04:40<07:32, 10249.27\nexamples/s]', '\\rMap:  37%|###7      | 2767000/7400423 [04:41<07:36, 10159.27\nexamples/s]', '\\rMap:  37%|###7      | 2769000/7400423 [04:41<07:35, 10173.87\nexamples/s]', '\\rMap:  37%|###7      | 2771000/7400423 [04:41<07:39, 10077.50\nexamples/s]', '\\rMap:  37%|###7      | 2773000/7400423 [04:41<09:55, 7775.05\nexamples/s] ', '\\rMap:  37%|###7      | 2774000/7400423 [04:41<09:37, 8016.15\nexamples/s]', '\\rMap:  38%|###7      | 2776000/7400423 [04:42<08:57, 8596.06\nexamples/s]', '\\rMap:  38%|###7      | 2777000/7400423 [04:42<08:47, 8761.96\nexamples/s]', '\\rMap:  38%|###7      | 2778000/7400423 [04:42<08:35, 8962.94\nexamples/s]', '\\rMap:  38%|###7      | 2779000/7400423 [04:42<08:29, 9073.33\nexamples/s]', '\\rMap:  38%|###7      | 2780000/7400423 [04:42<08:24, 9159.20\nexamples/s]', '\\rMap:  38%|###7      | 2781000/7400423 [04:42<08:19, 9238.88\nexamples/s]', '\\rMap:  38%|###7      | 2783000/7400423 [04:42<07:59, 9628.52\nexamples/s]', '\\rMap:  38%|###7      | 2785000/7400423 [04:43<07:53, 9751.45\nexamples/s]', '\\rMap:  38%|###7      | 2786000/7400423 [04:43<07:55, 9710.30\nexamples/s]', '\\rMap:  38%|###7      | 2787000/7400423 [04:43<07:59, 9611.43\nexamples/s]', '\\rMap:  38%|###7      | 2788000/7400423 [04:43<07:58, 9630.19\nexamples/s]', '\\rMap:  38%|###7      | 2789000/7400423 [04:43<07:54, 9716.04\nexamples/s]', '\\rMap:  38%|###7      | 2791000/7400423 [04:43<07:43, 9952.99\nexamples/s]', '\\rMap:  38%|###7      | 2792000/7400423 [04:43<07:45, 9907.49\nexamples/s]', '\\rMap:  38%|###7      | 2793000/7400423 [04:43<07:52, 9746.56\nexamples/s]', '\\rMap:  38%|###7      | 2794000/7400423 [04:44<07:52, 9741.91\nexamples/s]', '\\rMap:  38%|###7      | 2796000/7400423 [04:44<07:44, 9919.00\nexamples/s]', '\\rMap:  38%|###7      | 2798000/7400423 [04:44<07:38, 10034.31\nexamples/s]', '\\rMap:  38%|###7      | 2799000/7400423 [04:44<07:43, 9917.00\nexamples/s] ', '\\rMap:  38%|###7      | 2801000/7400423 [04:44<07:41, 9962.44\nexamples/s]', '\\rMap:  38%|###7      | 2802000/7400423 [04:44<07:43, 9921.73\nexamples/s]', '\\rMap:  38%|###7      | 2804000/7400423 [04:45<07:35, 10085.60\nexamples/s]', '\\rMap:  38%|###7      | 2806000/7400423 [04:45<07:35, 10093.65\nexamples/s]', '\\rMap:  38%|###7      | 2808000/7400423 [04:45<07:43, 9909.86\nexamples/s] ', '\\rMap:  38%|###7      | 2809000/7400423 [04:45<07:47, 9819.05\nexamples/s]', '\\rMap:  38%|###7      | 2810000/7400423 [04:45<07:47, 9810.64\nexamples/s]', '\\rMap:  38%|###7      | 2811000/7400423 [04:45<07:57, 9616.08\nexamples/s]', '\\rMap:  38%|###8      | 2813000/7400423 [04:45<07:39, 9978.22\nexamples/s]', '\\rMap:  38%|###8      | 2814000/7400423 [04:46<07:46, 9823.58\nexamples/s]', '\\rMap:  38%|###8      | 2815000/7400423 [04:46<07:50, 9751.74\nexamples/s]', '\\rMap:  38%|###8      | 2816000/7400423 [04:46<07:48, 9778.48\nexamples/s]', '\\rMap:  38%|###8      | 2817000/7400423 [04:46<07:46, 9831.95\nexamples/s]', '\\rMap:  38%|###8      | 2818000/7400423 [04:46<07:50, 9746.11\nexamples/s]', '\\rMap:  38%|###8      | 2819000/7400423 [04:46<07:54, 9659.76\nexamples/s]', '\\rMap:  38%|###8      | 2820000/7400423 [04:46<08:03, 9478.03\nexamples/s]', '\\rMap:  38%|###8      | 2821000/7400423 [04:46<08:14, 9267.72\nexamples/s]', '\\rMap:  38%|###8      | 2822000/7400423 [04:46<08:09, 9362.16\nexamples/s]', '\\rMap:  38%|###8      | 2823000/7400423 [04:46<08:12, 9289.21\nexamples/s]', '\\rMap:  38%|###8      | 2824000/7400423 [04:47<08:12, 9299.34\nexamples/s]', '\\rMap:  38%|###8      | 2826000/7400423 [04:47<07:41, 9914.07\nexamples/s]', '\\rMap:  38%|###8      | 2828000/7400423 [04:47<07:24, 10288.90\nexamples/s]', '\\rMap:  38%|###8      | 2830000/7400423 [04:47<07:17, 10458.34\nexamples/s]', '\\rMap:  38%|###8      | 2832000/7400423 [04:47<07:07, 10678.07\nexamples/s]', '\\rMap:  38%|###8      | 2834000/7400423 [04:47<06:53, 11030.28\nexamples/s]', '\\rMap:  38%|###8      | 2836000/7400423 [04:48<07:02, 10811.37\nexamples/s]', '\\rMap:  38%|###8      | 2838000/7400423 [04:48<07:07, 10660.34\nexamples/s]', '\\rMap:  38%|###8      | 2840000/7400423 [04:48<07:14, 10497.84\nexamples/s]', '\\rMap:  38%|###8      | 2842000/7400423 [04:48<07:12, 10535.78\nexamples/s]', '\\rMap:  38%|###8      | 2844000/7400423 [04:48<07:14, 10495.48\nexamples/s]', '\\rMap:  38%|###8      | 2846000/7400423 [04:49<07:09, 10598.50\nexamples/s]', '\\rMap:  38%|###8      | 2848000/7400423 [04:49<06:52, 11044.13\nexamples/s]', '\\rMap:  39%|###8      | 2850000/7400423 [04:49<06:48, 11126.34\nexamples/s]', '\\rMap:  39%|###8      | 2852000/7400423 [04:49<06:44, 11256.30\nexamples/s]', '\\rMap:  39%|###8      | 2854000/7400423 [04:49<06:43, 11267.58\nexamples/s]', '\\rMap:  39%|###8      | 2856000/7400423 [04:50<06:43, 11271.93\nexamples/s]', '\\rMap:  39%|###8      | 2858000/7400423 [04:50<06:53, 10973.92\nexamples/s]', '\\rMap:  39%|###8      | 2860000/7400423 [04:50<09:26, 8007.89\nexamples/s] ', '\\rMap:  39%|###8      | 2862000/7400423 [04:50<08:45, 8641.33\nexamples/s]', '\\rMap:  39%|###8      | 2864000/7400423 [04:50<08:11, 9221.94\nexamples/s]', '\\rMap:  39%|###8      | 2866000/7400423 [04:51<07:53, 9585.26\nexamples/s]', '\\rMap:  39%|###8      | 2868000/7400423 [04:51<07:33, 10005.16\nexamples/s]', '\\rMap:  39%|###8      | 2870000/7400423 [04:51<07:22, 10242.34\nexamples/s]', '\\rMap:  39%|###8      | 2872000/7400423 [04:51<06:58, 10810.56\nexamples/s]', '\\rMap:  39%|###8      | 2874000/7400423 [04:51<06:57, 10832.50\nexamples/s]', '\\rMap:  39%|###8      | 2876000/7400423 [04:52<06:43, 11222.13\nexamples/s]', '\\rMap:  39%|###8      | 2878000/7400423 [04:52<06:41, 11251.81\nexamples/s]', '\\rMap:  39%|###8      | 2880000/7400423 [04:52<06:51, 10993.69\nexamples/s]', '\\rMap:  39%|###8      | 2882000/7400423 [04:52<06:57, 10829.58\nexamples/s]', '\\rMap:  39%|###8      | 2884000/7400423 [04:52<07:03, 10653.41\nexamples/s]', '\\rMap:  39%|###8      | 2886000/7400423 [04:52<07:00, 10732.05\nexamples/s]', '\\rMap:  39%|###9      | 2888000/7400423 [04:53<06:55, 10873.31\nexamples/s]', '\\rMap:  39%|###9      | 2890000/7400423 [04:53<06:49, 11008.20\nexamples/s]', '\\rMap:  39%|###9      | 2892000/7400423 [04:53<06:50, 10986.62\nexamples/s]', '\\rMap:  39%|###9      | 2894000/7400423 [04:53<06:57, 10800.34\nexamples/s]', '\\rMap:  39%|###9      | 2896000/7400423 [04:53<07:08, 10519.12\nexamples/s]', '\\rMap:  39%|###9      | 2898000/7400423 [04:54<07:11, 10428.78\nexamples/s]', '\\rMap:  39%|###9      | 2900000/7400423 [04:54<07:07, 10521.30\nexamples/s]', '\\rMap:  39%|###9      | 2902000/7400423 [04:54<07:07, 10525.06\nexamples/s]', '\\rMap:  39%|###9      | 2904000/7400423 [04:54<07:10, 10434.30\nexamples/s]', '\\rMap:  39%|###9      | 2906000/7400423 [04:54<07:10, 10434.49\nexamples/s]', '\\rMap:  39%|###9      | 2908000/7400423 [04:55<07:10, 10430.53\nexamples/s]', '\\rMap:  39%|###9      | 2910000/7400423 [04:55<07:01, 10646.64\nexamples/s]', '\\rMap:  39%|###9      | 2912000/7400423 [04:55<06:57, 10747.53\nexamples/s]', '\\rMap:  39%|###9      | 2914000/7400423 [04:55<06:52, 10880.34\nexamples/s]', '\\rMap:  39%|###9      | 2916000/7400423 [04:55<06:43, 11125.07\nexamples/s]', '\\rMap:  39%|###9      | 2918000/7400423 [04:55<06:29, 11502.02\nexamples/s]', '\\rMap:  39%|###9      | 2920000/7400423 [04:56<06:25, 11636.78\nexamples/s]', '\\rMap:  39%|###9      | 2922000/7400423 [04:56<06:22, 11709.17\nexamples/s]', '\\rMap:  40%|###9      | 2924000/7400423 [04:56<06:25, 11621.34\nexamples/s]', '\\rMap:  40%|###9      | 2926000/7400423 [04:56<06:30, 11445.75\nexamples/s]', '\\rMap:  40%|###9      | 2928000/7400423 [04:56<06:30, 11453.62\nexamples/s]', '\\rMap:  40%|###9      | 2930000/7400423 [04:56<06:31, 11419.22\nexamples/s]', '\\rMap:  40%|###9      | 2932000/7400423 [04:57<06:35, 11305.02\nexamples/s]', '\\rMap:  40%|###9      | 2934000/7400423 [04:57<06:29, 11469.82\nexamples/s]', '\\rMap:  40%|###9      | 2936000/7400423 [04:57<08:42, 8546.03\nexamples/s] ', '\\rMap:  40%|###9      | 2938000/7400423 [04:57<07:58, 9319.20\nexamples/s]', '\\rMap:  40%|###9      | 2940000/7400423 [04:58<07:31, 9880.52\nexamples/s]', '\\rMap:  40%|###9      | 2942000/7400423 [04:58<07:15, 10233.87\nexamples/s]', '\\rMap:  40%|###9      | 2944000/7400423 [04:58<07:03, 10511.72\nexamples/s]', '\\rMap:  40%|###9      | 2946000/7400423 [04:58<06:54, 10744.94\nexamples/s]', '\\rMap:  40%|###9      | 2948000/7400423 [04:58<06:58, 10627.85\nexamples/s]', '\\rMap:  40%|###9      | 2950000/7400423 [04:58<06:59, 10615.03\nexamples/s]', '\\rMap:  40%|###9      | 2952000/7400423 [04:59<06:52, 10771.02\nexamples/s]', '\\rMap:  40%|###9      | 2954000/7400423 [04:59<06:44, 10992.21\nexamples/s]', '\\rMap:  40%|###9      | 2956000/7400423 [04:59<06:42, 11038.78\nexamples/s]', '\\rMap:  40%|###9      | 2958000/7400423 [04:59<06:43, 11015.51\nexamples/s]', '\\rMap:  40%|###9      | 2960000/7400423 [04:59<06:49, 10834.35\nexamples/s]', '\\rMap:  40%|####      | 2962000/7400423 [05:00<06:52, 10751.87\nexamples/s]', '\\rMap:  40%|####      | 2964000/7400423 [05:00<06:51, 10784.05\nexamples/s]', '\\rMap:  40%|####      | 2966000/7400423 [05:00<06:52, 10742.67\nexamples/s]', '\\rMap:  40%|####      | 2968000/7400423 [05:00<06:51, 10761.99\nexamples/s]', '\\rMap:  40%|####      | 2970000/7400423 [05:00<06:51, 10766.31\nexamples/s]', '\\rMap:  40%|####      | 2972000/7400423 [05:00<06:47, 10880.11\nexamples/s]', '\\rMap:  40%|####      | 2974000/7400423 [05:01<06:44, 10942.23\nexamples/s]', '\\rMap:  40%|####      | 2976000/7400423 [05:01<06:43, 10963.26\nexamples/s]', '\\rMap:  40%|####      | 2978000/7400423 [05:01<06:39, 11064.62\nexamples/s]', '\\rMap:  40%|####      | 2980000/7400423 [05:01<06:46, 10879.28\nexamples/s]', '\\rMap:  40%|####      | 2982000/7400423 [05:01<06:44, 10936.23\nexamples/s]', '\\rMap:  40%|####      | 2984000/7400423 [05:02<06:49, 10779.76\nexamples/s]', '\\rMap:  40%|####      | 2986000/7400423 [05:02<07:00, 10504.23\nexamples/s]', '\\rMap:  40%|####      | 2988000/7400423 [05:02<07:06, 10344.75\nexamples/s]', '\\rMap:  40%|####      | 2990000/7400423 [05:02<07:08, 10288.45\nexamples/s]', '\\rMap:  40%|####      | 2992000/7400423 [05:02<07:13, 10169.98\nexamples/s]', '\\rMap:  40%|####      | 2994000/7400423 [05:03<07:06, 10330.00\nexamples/s]', '\\rMap:  40%|####      | 2996000/7400423 [05:03<07:11, 10202.43\nexamples/s]', '\\rMap:  41%|####      | 2998000/7400423 [05:03<07:12, 10183.87\nexamples/s]', '\\rMap:  41%|####      | 3000000/7400423 [05:03<07:05, 10348.93\nexamples/s]', '\\rMap:  41%|####      | 3002000/7400423 [05:03<07:02, 10409.94\nexamples/s]', '\\rMap:  41%|####      | 3004000/7400423 [05:04<06:56, 10554.60\nexamples/s]', '\\rMap:  41%|####      | 3006000/7400423 [05:04<06:51, 10673.56\nexamples/s]', '\\rMap:  41%|####      | 3008000/7400423 [05:04<06:52, 10657.58\nexamples/s]', '\\rMap:  41%|####      | 3010000/7400423 [05:04<06:49, 10731.52\nexamples/s]', '\\rMap:  41%|####      | 3012000/7400423 [05:04<06:43, 10866.47\nexamples/s]', '\\rMap:  41%|####      | 3014000/7400423 [05:04<06:38, 10999.78\nexamples/s]', '\\rMap:  41%|####      | 3016000/7400423 [05:05<06:39, 10982.66\nexamples/s]', '\\rMap:  41%|####      | 3018000/7400423 [05:05<06:36, 11052.12\nexamples/s]', '\\rMap:  41%|####      | 3020000/7400423 [05:05<06:36, 11051.31\nexamples/s]', '\\rMap:  41%|####      | 3022000/7400423 [05:05<06:37, 11006.73\nexamples/s]', '\\rMap:  41%|####      | 3024000/7400423 [05:05<06:36, 11042.60\nexamples/s]', '\\rMap:  41%|####      | 3026000/7400423 [05:06<06:37, 11000.76\nexamples/s]', '\\rMap:  41%|####      | 3028000/7400423 [05:06<06:28, 11247.22\nexamples/s]', '\\rMap:  41%|####      | 3030000/7400423 [05:06<06:32, 11123.19\nexamples/s]', '\\rMap:  41%|####      | 3032000/7400423 [05:06<06:42, 10856.69\nexamples/s]', '\\rMap:  41%|####      | 3034000/7400423 [05:06<06:43, 10827.10\nexamples/s]', '\\rMap:  41%|####1     | 3036000/7400423 [05:06<06:44, 10800.10\nexamples/s]', '\\rMap:  41%|####1     | 3038000/7400423 [05:07<06:49, 10648.97\nexamples/s]', '\\rMap:  41%|####1     | 3040000/7400423 [05:07<07:02, 10332.21\nexamples/s]', '\\rMap:  41%|####1     | 3042000/7400423 [05:07<07:02, 10308.30\nexamples/s]', '\\rMap:  41%|####1     | 3044000/7400423 [05:07<07:07, 10198.34\nexamples/s]', '\\rMap:  41%|####1     | 3046000/7400423 [05:07<07:05, 10234.21\nexamples/s]', '\\rMap:  41%|####1     | 3048000/7400423 [05:08<07:04, 10246.23\nexamples/s]', '\\rMap:  41%|####1     | 3050000/7400423 [05:08<07:14, 10020.01\nexamples/s]', '\\rMap:  41%|####1     | 3052000/7400423 [05:08<07:20, 9869.91\nexamples/s] ', '\\rMap:  41%|####1     | 3054000/7400423 [05:08<07:12, 10054.44\nexamples/s]', '\\rMap:  41%|####1     | 3056000/7400423 [05:08<07:08, 10134.34\nexamples/s]', '\\rMap:  41%|####1     | 3058000/7400423 [05:09<07:12, 10036.09\nexamples/s]', '\\rMap:  41%|####1     | 3060000/7400423 [05:09<07:22, 9809.70\nexamples/s] ', '\\rMap:  41%|####1     | 3061000/7400423 [05:09<10:17, 7027.65\nexamples/s]', '\\rMap:  41%|####1     | 3062000/7400423 [05:09<10:07, 7142.82\nexamples/s]', '\\rMap:  41%|####1     | 3063000/7400423 [05:09<09:37, 7505.53\nexamples/s]', '\\rMap:  41%|####1     | 3064000/7400423 [05:10<09:05, 7948.20\nexamples/s]', '\\rMap:  41%|####1     | 3065000/7400423 [05:10<08:55, 8097.56\nexamples/s]', '\\rMap:  41%|####1     | 3066000/7400423 [05:10<09:01, 8006.94\nexamples/s]', '\\rMap:  41%|####1     | 3067000/7400423 [05:10<09:02, 7993.04\nexamples/s]', '\\rMap:  41%|####1     | 3068000/7400423 [05:10<08:55, 8093.52\nexamples/s]', '\\rMap:  41%|####1     | 3069000/7400423 [05:10<08:53, 8123.45\nexamples/s]', '\\rMap:  41%|####1     | 3070000/7400423 [05:10<09:11, 7857.17\nexamples/s]', '\\rMap:  41%|####1     | 3071000/7400423 [05:10<09:18, 7747.07\nexamples/s]', '\\rMap:  42%|####1     | 3072000/7400423 [05:11<09:15, 7796.30\nexamples/s]', '\\rMap:  42%|####1     | 3073000/7400423 [05:11<08:59, 8027.18\nexamples/s]', '\\rMap:  42%|####1     | 3074000/7400423 [05:11<08:52, 8121.82\nexamples/s]', '\\rMap:  42%|####1     | 3075000/7400423 [05:11<08:46, 8212.84\nexamples/s]', '\\rMap:  42%|####1     | 3076000/7400423 [05:11<08:38, 8347.07\nexamples/s]', '\\rMap:  42%|####1     | 3077000/7400423 [05:11<08:22, 8599.12\nexamples/s]', '\\rMap:  42%|####1     | 3078000/7400423 [05:11<08:16, 8710.52\nexamples/s]', '\\rMap:  42%|####1     | 3079000/7400423 [05:11<08:11, 8797.33\nexamples/s]', '\\rMap:  42%|####1     | 3080000/7400423 [05:11<08:08, 8842.67\nexamples/s]', '\\rMap:  42%|####1     | 3081000/7400423 [05:12<07:58, 9036.02\nexamples/s]', '\\rMap:  42%|####1     | 3082000/7400423 [05:12<07:53, 9118.73\nexamples/s]', '\\rMap:  42%|####1     | 3083000/7400423 [05:12<07:47, 9240.13\nexamples/s]', '\\rMap:  42%|####1     | 3084000/7400423 [05:12<07:38, 9409.09\nexamples/s]', '\\rMap:  42%|####1     | 3085000/7400423 [05:12<07:40, 9372.12\nexamples/s]', '\\rMap:  42%|####1     | 3086000/7400423 [05:12<07:34, 9499.44\nexamples/s]', '\\rMap:  42%|####1     | 3087000/7400423 [05:12<07:32, 9531.81\nexamples/s]', '\\rMap:  42%|####1     | 3089000/7400423 [05:12<07:28, 9618.43\nexamples/s]', '\\rMap:  42%|####1     | 3091000/7400423 [05:13<07:20, 9792.71\nexamples/s]', '\\rMap:  42%|####1     | 3092000/7400423 [05:13<07:23, 9705.16\nexamples/s]', '\\rMap:  42%|####1     | 3093000/7400423 [05:13<07:24, 9699.23\nexamples/s]', '\\rMap:  42%|####1     | 3094000/7400423 [05:13<07:22, 9731.11\nexamples/s]', '\\rMap:  42%|####1     | 3096000/7400423 [05:13<07:16, 9853.76\nexamples/s]', '\\rMap:  42%|####1     | 3097000/7400423 [05:13<07:21, 9758.06\nexamples/s]', '\\rMap:  42%|####1     | 3098000/7400423 [05:13<07:20, 9766.36\nexamples/s]', '\\rMap:  42%|####1     | 3100000/7400423 [05:13<07:12, 9953.58\nexamples/s]', '\\rMap:  42%|####1     | 3101000/7400423 [05:14<07:13, 9925.28\nexamples/s]', '\\rMap:  42%|####1     | 3103000/7400423 [05:14<07:11, 9969.13\nexamples/s]', '\\rMap:  42%|####1     | 3104000/7400423 [05:14<07:20, 9745.81\nexamples/s]', '\\rMap:  42%|####1     | 3105000/7400423 [05:14<07:26, 9615.31\nexamples/s]', '\\rMap:  42%|####1     | 3106000/7400423 [05:14<07:24, 9657.32\nexamples/s]', '\\rMap:  42%|####1     | 3108000/7400423 [05:14<07:18, 9782.09\nexamples/s]', '\\rMap:  42%|####2     | 3109000/7400423 [05:14<07:20, 9747.16\nexamples/s]', '\\rMap:  42%|####2     | 3110000/7400423 [05:15<07:22, 9689.65\nexamples/s]', '\\rMap:  42%|####2     | 3111000/7400423 [05:15<07:21, 9717.80\nexamples/s]', '\\rMap:  42%|####2     | 3112000/7400423 [05:15<07:27, 9584.56\nexamples/s]', '\\rMap:  42%|####2     | 3113000/7400423 [05:15<07:49, 9127.43\nexamples/s]', '\\rMap:  42%|####2     | 3114000/7400423 [05:15<08:07, 8790.31\nexamples/s]', '\\rMap:  42%|####2     | 3115000/7400423 [05:15<08:26, 8455.90\nexamples/s]', '\\rMap:  42%|####2     | 3116000/7400423 [05:15<08:38, 8269.29\nexamples/s]', '\\rMap:  42%|####2     | 3117000/7400423 [05:15<08:28, 8430.01\nexamples/s]', '\\rMap:  42%|####2     | 3119000/7400423 [05:16<07:43, 9230.59\nexamples/s]', '\\rMap:  42%|####2     | 3121000/7400423 [05:16<07:25, 9602.17\nexamples/s]', '\\rMap:  42%|####2     | 3123000/7400423 [05:16<07:07, 10007.06\nexamples/s]', '\\rMap:  42%|####2     | 3125000/7400423 [05:16<06:53, 10335.31\nexamples/s]', '\\rMap:  42%|####2     | 3127000/7400423 [05:16<06:53, 10329.68\nexamples/s]', '\\rMap:  42%|####2     | 3129000/7400423 [05:16<06:52, 10354.23\nexamples/s]', '\\rMap:  42%|####2     | 3131000/7400423 [05:17<07:01, 10124.82\nexamples/s]', '\\rMap:  42%|####2     | 3133000/7400423 [05:17<07:08, 9966.06\nexamples/s] ', '\\rMap:  42%|####2     | 3135000/7400423 [05:17<07:06, 9998.05\nexamples/s]', '\\rMap:  42%|####2     | 3137000/7400423 [05:17<07:01, 10116.72\nexamples/s]', '\\rMap:  42%|####2     | 3139000/7400423 [05:17<06:53, 10295.52\nexamples/s]', '\\rMap:  42%|####2     | 3141000/7400423 [05:18<06:51, 10350.73\nexamples/s]', '\\rMap:  42%|####2     | 3143000/7400423 [05:18<06:49, 10406.00\nexamples/s]', '\\rMap:  42%|####2     | 3145000/7400423 [05:18<06:45, 10498.49\nexamples/s]', '\\rMap:  43%|####2     | 3147000/7400423 [05:18<06:43, 10546.86\nexamples/s]', '\\rMap:  43%|####2     | 3149000/7400423 [05:18<06:37, 10689.45\nexamples/s]', '\\rMap:  43%|####2     | 3151000/7400423 [05:19<06:40, 10615.32\nexamples/s]', '\\rMap:  43%|####2     | 3153000/7400423 [05:19<06:37, 10685.86\nexamples/s]', '\\rMap:  43%|####2     | 3155000/7400423 [05:19<06:32, 10817.86\nexamples/s]', '\\rMap:  43%|####2     | 3157000/7400423 [05:19<06:33, 10789.99\nexamples/s]', '\\rMap:  43%|####2     | 3159000/7400423 [05:19<06:49, 10357.67\nexamples/s]', '\\rMap:  43%|####2     | 3161000/7400423 [05:20<08:55, 7916.93\nexamples/s] ', '\\rMap:  43%|####2     | 3163000/7400423 [05:20<08:16, 8531.69\nexamples/s]', '\\rMap:  43%|####2     | 3165000/7400423 [05:20<07:48, 9037.52\nexamples/s]', '\\rMap:  43%|####2     | 3167000/7400423 [05:20<07:28, 9442.57\nexamples/s]', '\\rMap:  43%|####2     | 3169000/7400423 [05:21<07:15, 9710.89\nexamples/s]', '\\rMap:  43%|####2     | 3171000/7400423 [05:21<07:01, 10038.91\nexamples/s]', '\\rMap:  43%|####2     | 3173000/7400423 [05:21<06:49, 10313.74\nexamples/s]', '\\rMap:  43%|####2     | 3175000/7400423 [05:21<06:44, 10452.74\nexamples/s]', '\\rMap:  43%|####2     | 3177000/7400423 [05:21<06:44, 10436.41\nexamples/s]', '\\rMap:  43%|####2     | 3179000/7400423 [05:21<06:37, 10606.90\nexamples/s]', '\\rMap:  43%|####2     | 3181000/7400423 [05:22<06:33, 10735.42\nexamples/s]', '\\rMap:  43%|####3     | 3183000/7400423 [05:22<06:25, 10949.11\nexamples/s]', '\\rMap:  43%|####3     | 3185000/7400423 [05:22<06:22, 11017.33\nexamples/s]', '\\rMap:  43%|####3     | 3187000/7400423 [05:22<06:27, 10883.33\nexamples/s]', '\\rMap:  43%|####3     | 3189000/7400423 [05:22<06:40, 10512.90\nexamples/s]', '\\rMap:  43%|####3     | 3191000/7400423 [05:23<07:00, 10021.30\nexamples/s]', '\\rMap:  43%|####3     | 3193000/7400423 [05:23<07:11, 9760.82\nexamples/s] ', '\\rMap:  43%|####3     | 3194000/7400423 [05:23<07:10, 9767.26\nexamples/s]', '\\rMap:  43%|####3     | 3195000/7400423 [05:23<07:13, 9691.41\nexamples/s]', '\\rMap:  43%|####3     | 3196000/7400423 [05:23<07:21, 9529.95\nexamples/s]', '\\rMap:  43%|####3     | 3197000/7400423 [05:23<07:29, 9347.84\nexamples/s]', '\\rMap:  43%|####3     | 3198000/7400423 [05:23<07:34, 9256.29\nexamples/s]', '\\rMap:  43%|####3     | 3199000/7400423 [05:23<07:30, 9318.94\nexamples/s]', '\\rMap:  43%|####3     | 3200000/7400423 [05:24<07:27, 9392.46\nexamples/s]', '\\rMap:  43%|####3     | 3201000/7400423 [05:24<07:21, 9502.93\nexamples/s]', '\\rMap:  43%|####3     | 3202000/7400423 [05:24<07:17, 9592.28\nexamples/s]', '\\rMap:  43%|####3     | 3204000/7400423 [05:24<07:08, 9795.80\nexamples/s]', '\\rMap:  43%|####3     | 3206000/7400423 [05:24<06:58, 10017.67\nexamples/s]', '\\rMap:  43%|####3     | 3208000/7400423 [05:24<06:51, 10183.64\nexamples/s]', '\\rMap:  43%|####3     | 3210000/7400423 [05:25<06:56, 10051.41\nexamples/s]', '\\rMap:  43%|####3     | 3212000/7400423 [05:25<07:02, 9910.17\nexamples/s] ', '\\rMap:  43%|####3     | 3213000/7400423 [05:25<07:03, 9887.33\nexamples/s]', '\\rMap:  43%|####3     | 3215000/7400423 [05:25<07:05, 9826.68\nexamples/s]', '\\rMap:  43%|####3     | 3216000/7400423 [05:25<07:07, 9784.10\nexamples/s]', '\\rMap:  43%|####3     | 3217000/7400423 [05:25<07:09, 9735.38\nexamples/s]', '\\rMap:  43%|####3     | 3219000/7400423 [05:25<07:06, 9811.84\nexamples/s]', '\\rMap:  44%|####3     | 3220000/7400423 [05:26<07:08, 9749.95\nexamples/s]', '\\rMap:  44%|####3     | 3222000/7400423 [05:26<07:02, 9884.55\nexamples/s]', '\\rMap:  44%|####3     | 3224000/7400423 [05:26<07:04, 9842.21\nexamples/s]', '\\rMap:  44%|####3     | 3225000/7400423 [05:26<07:14, 9613.22\nexamples/s]', '\\rMap:  44%|####3     | 3226000/7400423 [05:26<07:18, 9511.27\nexamples/s]', '\\rMap:  44%|####3     | 3228000/7400423 [05:26<07:10, 9695.39\nexamples/s]', '\\rMap:  44%|####3     | 3229000/7400423 [05:27<07:13, 9625.58\nexamples/s]', '\\rMap:  44%|####3     | 3230000/7400423 [05:27<07:11, 9671.22\nexamples/s]', '\\rMap:  44%|####3     | 3232000/7400423 [05:27<07:04, 9811.84\nexamples/s]', '\\rMap:  44%|####3     | 3233000/7400423 [05:27<07:13, 9618.48\nexamples/s]', '\\rMap:  44%|####3     | 3234000/7400423 [05:27<07:13, 9601.01\nexamples/s]', '\\rMap:  44%|####3     | 3235000/7400423 [05:27<07:11, 9661.87\nexamples/s]', '\\rMap:  44%|####3     | 3236000/7400423 [05:27<07:07, 9731.11\nexamples/s]', '\\rMap:  44%|####3     | 3237000/7400423 [05:27<07:10, 9670.87\nexamples/s]', '\\rMap:  44%|####3     | 3238000/7400423 [05:27<07:08, 9709.01\nexamples/s]', '\\rMap:  44%|####3     | 3239000/7400423 [05:28<07:16, 9537.56\nexamples/s]', '\\rMap:  44%|####3     | 3240000/7400423 [05:28<07:16, 9521.90\nexamples/s]', '\\rMap:  44%|####3     | 3241000/7400423 [05:28<07:14, 9567.05\nexamples/s]', '\\rMap:  44%|####3     | 3242000/7400423 [05:28<07:10, 9655.28\nexamples/s]', '\\rMap:  44%|####3     | 3243000/7400423 [05:28<07:06, 9750.77\nexamples/s]', '\\rMap:  44%|####3     | 3244000/7400423 [05:28<07:09, 9681.70\nexamples/s]', '\\rMap:  44%|####3     | 3245000/7400423 [05:28<07:07, 9723.80\nexamples/s]', '\\rMap:  44%|####3     | 3247000/7400423 [05:29<10:24, 6650.66\nexamples/s]', '\\rMap:  44%|####3     | 3248000/7400423 [05:29<09:38, 7179.66\nexamples/s]', '\\rMap:  44%|####3     | 3250000/7400423 [05:29<08:31, 8112.74\nexamples/s]', '\\rMap:  44%|####3     | 3252000/7400423 [05:29<07:55, 8721.83\nexamples/s]', '\\rMap:  44%|####3     | 3253000/7400423 [05:29<07:45, 8900.47\nexamples/s]', '\\rMap:  44%|####3     | 3255000/7400423 [05:29<07:27, 9264.29\nexamples/s]', '\\rMap:  44%|####3     | 3256000/7400423 [05:30<07:21, 9385.79\nexamples/s]', '\\rMap:  44%|####4     | 3257000/7400423 [05:30<07:19, 9419.39\nexamples/s]', '\\rMap:  44%|####4     | 3258000/7400423 [05:30<07:14, 9542.58\nexamples/s]', '\\rMap:  44%|####4     | 3259000/7400423 [05:30<07:11, 9600.80\nexamples/s]', '\\rMap:  44%|####4     | 3260000/7400423 [05:30<07:11, 9595.16\nexamples/s]', '\\rMap:  44%|####4     | 3262000/7400423 [05:30<07:06, 9713.64\nexamples/s]', '\\rMap:  44%|####4     | 3264000/7400423 [05:30<06:57, 9902.93\nexamples/s]', '\\rMap:  44%|####4     | 3265000/7400423 [05:30<06:58, 9882.35\nexamples/s]', '\\rMap:  44%|####4     | 3266000/7400423 [05:31<07:00, 9843.71\nexamples/s]', '\\rMap:  44%|####4     | 3267000/7400423 [05:31<06:59, 9847.10\nexamples/s]', '\\rMap:  44%|####4     | 3269000/7400423 [05:31<06:54, 9977.48\nexamples/s]', '\\rMap:  44%|####4     | 3270000/7400423 [05:31<06:54, 9957.68\nexamples/s]', '\\rMap:  44%|####4     | 3271000/7400423 [05:31<06:59, 9852.55\nexamples/s]', '\\rMap:  44%|####4     | 3272000/7400423 [05:31<07:02, 9767.50\nexamples/s]', '\\rMap:  44%|####4     | 3273000/7400423 [05:31<07:00, 9816.38\nexamples/s]', '\\rMap:  44%|####4     | 3275000/7400423 [05:31<06:56, 9901.40\nexamples/s]', '\\rMap:  44%|####4     | 3277000/7400423 [05:32<06:53, 9967.71\nexamples/s]', '\\rMap:  44%|####4     | 3279000/7400423 [05:32<06:43, 10216.34\nexamples/s]', '\\rMap:  44%|####4     | 3281000/7400423 [05:32<06:49, 10055.28\nexamples/s]', '\\rMap:  44%|####4     | 3283000/7400423 [05:32<06:46, 10136.01\nexamples/s]', '\\rMap:  44%|####4     | 3285000/7400423 [05:32<06:45, 10137.74\nexamples/s]', '\\rMap:  44%|####4     | 3287000/7400423 [05:33<06:30, 10526.13\nexamples/s]', '\\rMap:  44%|####4     | 3289000/7400423 [05:33<06:19, 10827.91\nexamples/s]', '\\rMap:  44%|####4     | 3291000/7400423 [05:33<06:12, 11017.24\nexamples/s]', '\\rMap:  44%|####4     | 3293000/7400423 [05:33<06:07, 11177.77\nexamples/s]', '\\rMap:  45%|####4     | 3295000/7400423 [05:33<06:02, 11338.74\nexamples/s]', '\\rMap:  45%|####4     | 3297000/7400423 [05:33<06:03, 11296.19\nexamples/s]', '\\rMap:  45%|####4     | 3299000/7400423 [05:34<05:56, 11494.01\nexamples/s]', '\\rMap:  45%|####4     | 3301000/7400423 [05:34<05:51, 11656.63\nexamples/s]', '\\rMap:  45%|####4     | 3303000/7400423 [05:34<05:46, 11831.79\nexamples/s]', '\\rMap:  45%|####4     | 3305000/7400423 [05:34<05:45, 11865.19\nexamples/s]', '\\rMap:  45%|####4     | 3307000/7400423 [05:34<05:46, 11804.96\nexamples/s]', '\\rMap:  45%|####4     | 3309000/7400423 [05:34<05:44, 11877.75\nexamples/s]', '\\rMap:  45%|####4     | 3311000/7400423 [05:35<05:54, 11536.40\nexamples/s]', '\\rMap:  45%|####4     | 3313000/7400423 [05:35<05:55, 11492.64\nexamples/s]', '\\rMap:  45%|####4     | 3315000/7400423 [05:35<05:56, 11447.89\nexamples/s]', '\\rMap:  45%|####4     | 3317000/7400423 [05:35<05:53, 11560.10\nexamples/s]', '\\rMap:  45%|####4     | 3319000/7400423 [05:35<05:53, 11551.07\nexamples/s]', '\\rMap:  45%|####4     | 3321000/7400423 [05:36<05:54, 11495.09\nexamples/s]', '\\rMap:  45%|####4     | 3323000/7400423 [05:36<08:00, 8485.17\nexamples/s] ', '\\rMap:  45%|####4     | 3325000/7400423 [05:36<07:22, 9204.27\nexamples/s]', '\\rMap:  45%|####4     | 3327000/7400423 [05:36<06:56, 9787.01\nexamples/s]', '\\rMap:  45%|####4     | 3329000/7400423 [05:36<06:35, 10295.49\nexamples/s]', '\\rMap:  45%|####5     | 3331000/7400423 [05:37<06:20, 10683.89\nexamples/s]', '\\rMap:  45%|####5     | 3333000/7400423 [05:37<06:08, 11034.51\nexamples/s]', '\\rMap:  45%|####5     | 3335000/7400423 [05:37<06:03, 11178.59\nexamples/s]', '\\rMap:  45%|####5     | 3337000/7400423 [05:37<05:57, 11366.07\nexamples/s]', '\\rMap:  45%|####5     | 3339000/7400423 [05:37<05:52, 11515.07\nexamples/s]', '\\rMap:  45%|####5     | 3341000/7400423 [05:37<05:56, 11391.08\nexamples/s]', '\\rMap:  45%|####5     | 3343000/7400423 [05:38<05:53, 11491.25\nexamples/s]', '\\rMap:  45%|####5     | 3345000/7400423 [05:38<05:49, 11613.59\nexamples/s]', '\\rMap:  45%|####5     | 3347000/7400423 [05:38<05:50, 11579.53\nexamples/s]', '\\rMap:  45%|####5     | 3349000/7400423 [05:38<05:50, 11549.39\nexamples/s]', '\\rMap:  45%|####5     | 3351000/7400423 [05:38<05:52, 11487.99\nexamples/s]', '\\rMap:  45%|####5     | 3353000/7400423 [05:38<05:56, 11338.21\nexamples/s]', '\\rMap:  45%|####5     | 3355000/7400423 [05:39<05:53, 11428.25\nexamples/s]', '\\rMap:  45%|####5     | 3357000/7400423 [05:39<05:54, 11396.50\nexamples/s]', '\\rMap:  45%|####5     | 3359000/7400423 [05:39<05:52, 11453.88\nexamples/s]', '\\rMap:  45%|####5     | 3361000/7400423 [05:39<05:52, 11447.06\nexamples/s]', '\\rMap:  45%|####5     | 3363000/7400423 [05:39<06:00, 11208.42\nexamples/s]', '\\rMap:  45%|####5     | 3365000/7400423 [05:40<06:01, 11148.47\nexamples/s]', '\\rMap:  45%|####5     | 3367000/7400423 [05:40<06:04, 11071.81\nexamples/s]', '\\rMap:  46%|####5     | 3369000/7400423 [05:40<06:05, 11025.50\nexamples/s]', '\\rMap:  46%|####5     | 3371000/7400423 [05:40<06:04, 11062.52\nexamples/s]', '\\rMap:  46%|####5     | 3373000/7400423 [05:40<06:02, 11109.50\nexamples/s]', '\\rMap:  46%|####5     | 3375000/7400423 [05:40<06:01, 11136.61\nexamples/s]', '\\rMap:  46%|####5     | 3377000/7400423 [05:41<05:52, 11407.73\nexamples/s]', '\\rMap:  46%|####5     | 3379000/7400423 [05:41<05:45, 11653.29\nexamples/s]', '\\rMap:  46%|####5     | 3381000/7400423 [05:41<05:41, 11784.26\nexamples/s]', '\\rMap:  46%|####5     | 3383000/7400423 [05:41<05:36, 11939.68\nexamples/s]', '\\rMap:  46%|####5     | 3385000/7400423 [05:41<05:34, 11987.14\nexamples/s]', '\\rMap:  46%|####5     | 3387000/7400423 [05:41<05:34, 11987.19\nexamples/s]', '\\rMap:  46%|####5     | 3389000/7400423 [05:42<05:43, 11687.30\nexamples/s]', '\\rMap:  46%|####5     | 3391000/7400423 [05:42<05:43, 11687.21\nexamples/s]', '\\rMap:  46%|####5     | 3393000/7400423 [05:42<05:41, 11730.14\nexamples/s]', '\\rMap:  46%|####5     | 3395000/7400423 [05:42<05:40, 11779.16\nexamples/s]', '\\rMap:  46%|####5     | 3397000/7400423 [05:42<05:41, 11725.44\nexamples/s]', '\\rMap:  46%|####5     | 3399000/7400423 [05:42<05:35, 11927.70\nexamples/s]', '\\rMap:  46%|####5     | 3401000/7400423 [05:43<05:38, 11813.74\nexamples/s]', '\\rMap:  46%|####5     | 3403000/7400423 [05:43<05:45, 11585.04\nexamples/s]', '\\rMap:  46%|####6     | 3405000/7400423 [05:43<05:42, 11674.91\nexamples/s]', '\\rMap:  46%|####6     | 3407000/7400423 [05:43<05:37, 11819.49\nexamples/s]', '\\rMap:  46%|####6     | 3409000/7400423 [05:43<05:38, 11801.55\nexamples/s]', '\\rMap:  46%|####6     | 3411000/7400423 [05:43<05:40, 11701.45\nexamples/s]', '\\rMap:  46%|####6     | 3413000/7400423 [05:44<05:45, 11551.45\nexamples/s]', '\\rMap:  46%|####6     | 3415000/7400423 [05:44<05:42, 11634.33\nexamples/s]', '\\rMap:  46%|####6     | 3417000/7400423 [05:44<05:40, 11704.16\nexamples/s]', '\\rMap:  46%|####6     | 3419000/7400423 [05:44<05:42, 11610.64\nexamples/s]', '\\rMap:  46%|####6     | 3421000/7400423 [05:44<05:40, 11694.98\nexamples/s]', '\\rMap:  46%|####6     | 3423000/7400423 [05:45<05:37, 11798.40\nexamples/s]', '\\rMap:  46%|####6     | 3425000/7400423 [05:45<05:38, 11751.94\nexamples/s]', '\\rMap:  46%|####6     | 3427000/7400423 [05:45<05:36, 11808.57\nexamples/s]', '\\rMap:  46%|####6     | 3429000/7400423 [05:45<05:32, 11929.01\nexamples/s]', '\\rMap:  46%|####6     | 3431000/7400423 [05:45<05:30, 12020.22\nexamples/s]', '\\rMap:  46%|####6     | 3433000/7400423 [05:45<05:27, 12127.71\nexamples/s]', '\\rMap:  46%|####6     | 3435000/7400423 [05:46<05:28, 12069.21\nexamples/s]', '\\rMap:  46%|####6     | 3437000/7400423 [05:46<05:39, 11659.55\nexamples/s]', '\\rMap:  46%|####6     | 3439000/7400423 [05:46<05:45, 11455.53\nexamples/s]', '\\rMap:  46%|####6     | 3441000/7400423 [05:46<05:49, 11320.54\nexamples/s]', '\\rMap:  47%|####6     | 3443000/7400423 [05:46<05:47, 11374.65\nexamples/s]', '\\rMap:  47%|####6     | 3445000/7400423 [05:46<05:47, 11396.01\nexamples/s]', '\\rMap:  47%|####6     | 3447000/7400423 [05:47<05:47, 11363.95\nexamples/s]', '\\rMap:  47%|####6     | 3449000/7400423 [05:47<07:48, 8432.10\nexamples/s] ', '\\rMap:  47%|####6     | 3451000/7400423 [05:47<07:15, 9067.87\nexamples/s]', '\\rMap:  47%|####6     | 3453000/7400423 [05:47<06:48, 9671.84\nexamples/s]', '\\rMap:  47%|####6     | 3455000/7400423 [05:47<06:25, 10238.10\nexamples/s]', '\\rMap:  47%|####6     | 3457000/7400423 [05:48<06:10, 10646.29\nexamples/s]', '\\rMap:  47%|####6     | 3459000/7400423 [05:48<06:00, 10947.46\nexamples/s]', '\\rMap:  47%|####6     | 3461000/7400423 [05:48<05:51, 11221.90\nexamples/s]', '\\rMap:  47%|####6     | 3463000/7400423 [05:48<05:40, 11547.48\nexamples/s]', '\\rMap:  47%|####6     | 3465000/7400423 [05:48<05:41, 11533.12\nexamples/s]', '\\rMap:  47%|####6     | 3467000/7400423 [05:49<05:40, 11563.69\nexamples/s]', '\\rMap:  47%|####6     | 3469000/7400423 [05:49<05:37, 11634.49\nexamples/s]', '\\rMap:  47%|####6     | 3471000/7400423 [05:49<05:34, 11763.93\nexamples/s]', '\\rMap:  47%|####6     | 3473000/7400423 [05:49<05:35, 11689.52\nexamples/s]', '\\rMap:  47%|####6     | 3475000/7400423 [05:49<05:37, 11641.89\nexamples/s]', '\\rMap:  47%|####6     | 3477000/7400423 [05:49<05:41, 11479.85\nexamples/s]', '\\rMap:  47%|####7     | 3479000/7400423 [05:50<05:40, 11502.34\nexamples/s]', '\\rMap:  47%|####7     | 3481000/7400423 [05:50<05:43, 11410.18\nexamples/s]', '\\rMap:  47%|####7     | 3483000/7400423 [05:50<05:49, 11198.87\nexamples/s]', '\\rMap:  47%|####7     | 3485000/7400423 [05:50<05:48, 11224.83\nexamples/s]', '\\rMap:  47%|####7     | 3487000/7400423 [05:50<05:44, 11354.45\nexamples/s]', '\\rMap:  47%|####7     | 3489000/7400423 [05:50<05:44, 11342.98\nexamples/s]', '\\rMap:  47%|####7     | 3491000/7400423 [05:51<05:46, 11266.37\nexamples/s]', '\\rMap:  47%|####7     | 3493000/7400423 [05:51<05:45, 11316.09\nexamples/s]', '\\rMap:  47%|####7     | 3495000/7400423 [05:51<05:41, 11432.39\nexamples/s]', '\\rMap:  47%|####7     | 3497000/7400423 [05:51<05:44, 11321.38\nexamples/s]', '\\rMap:  47%|####7     | 3499000/7400423 [05:51<05:39, 11493.41\nexamples/s]', '\\rMap:  47%|####7     | 3501000/7400423 [05:51<05:41, 11416.64\nexamples/s]', '\\rMap:  47%|####7     | 3503000/7400423 [05:52<05:50, 11114.83\nexamples/s]', '\\rMap:  47%|####7     | 3505000/7400423 [05:52<05:55, 10947.06\nexamples/s]', '\\rMap:  47%|####7     | 3507000/7400423 [05:52<05:55, 10937.42\nexamples/s]', '\\rMap:  47%|####7     | 3509000/7400423 [05:52<05:57, 10890.96\nexamples/s]', '\\rMap:  47%|####7     | 3511000/7400423 [05:52<05:55, 10931.02\nexamples/s]', '\\rMap:  47%|####7     | 3513000/7400423 [05:53<05:52, 11032.14\nexamples/s]', '\\rMap:  47%|####7     | 3515000/7400423 [05:53<05:48, 11156.33\nexamples/s]', '\\rMap:  48%|####7     | 3517000/7400423 [05:53<05:43, 11292.60\nexamples/s]', '\\rMap:  48%|####7     | 3519000/7400423 [05:53<05:41, 11381.31\nexamples/s]', '\\rMap:  48%|####7     | 3521000/7400423 [05:53<05:40, 11393.08\nexamples/s]', '\\rMap:  48%|####7     | 3523000/7400423 [05:53<05:36, 11520.02\nexamples/s]', '\\rMap:  48%|####7     | 3525000/7400423 [05:54<05:32, 11649.44\nexamples/s]', '\\rMap:  48%|####7     | 3527000/7400423 [05:54<05:31, 11678.99\nexamples/s]', '\\rMap:  48%|####7     | 3529000/7400423 [05:54<05:36, 11496.82\nexamples/s]', '\\rMap:  48%|####7     | 3531000/7400423 [05:54<05:45, 11185.49\nexamples/s]', '\\rMap:  48%|####7     | 3533000/7400423 [05:54<05:47, 11114.96\nexamples/s]', '\\rMap:  48%|####7     | 3535000/7400423 [05:55<05:45, 11181.40\nexamples/s]', '\\rMap:  48%|####7     | 3537000/7400423 [05:55<05:48, 11096.02\nexamples/s]', '\\rMap:  48%|####7     | 3539000/7400423 [05:55<05:43, 11231.44\nexamples/s]', '\\rMap:  48%|####7     | 3541000/7400423 [05:55<05:38, 11400.35\nexamples/s]', '\\rMap:  48%|####7     | 3543000/7400423 [05:55<05:37, 11442.51\nexamples/s]', '\\rMap:  48%|####7     | 3545000/7400423 [05:55<05:32, 11587.59\nexamples/s]', '\\rMap:  48%|####7     | 3547000/7400423 [05:56<05:29, 11698.92\nexamples/s]', '\\rMap:  48%|####7     | 3549000/7400423 [05:56<07:17, 8800.69\nexamples/s] ', '\\rMap:  48%|####7     | 3551000/7400423 [05:56<06:44, 9506.82\nexamples/s]', '\\rMap:  48%|####8     | 3553000/7400423 [05:56<06:20, 10120.68\nexamples/s]', '\\rMap:  48%|####8     | 3555000/7400423 [05:56<06:00, 10659.30\nexamples/s]', '\\rMap:  48%|####8     | 3557000/7400423 [05:57<05:45, 11110.33\nexamples/s]', '\\rMap:  48%|####8     | 3559000/7400423 [05:57<05:37, 11370.73\nexamples/s]', '\\rMap:  48%|####8     | 3561000/7400423 [05:57<05:29, 11635.11\nexamples/s]', '\\rMap:  48%|####8     | 3563000/7400423 [05:57<05:24, 11811.24\nexamples/s]', '\\rMap:  48%|####8     | 3565000/7400423 [05:57<05:24, 11827.80\nexamples/s]', '\\rMap:  48%|####8     | 3567000/7400423 [05:57<05:21, 11926.23\nexamples/s]', '\\rMap:  48%|####8     | 3569000/7400423 [05:58<05:24, 11815.76\nexamples/s]', '\\rMap:  48%|####8     | 3571000/7400423 [05:58<05:35, 11411.85\nexamples/s]', '\\rMap:  48%|####8     | 3573000/7400423 [05:58<05:58, 10668.52\nexamples/s]', '\\rMap:  48%|####8     | 3575000/7400423 [05:58<06:06, 10449.18\nexamples/s]', '\\rMap:  48%|####8     | 3577000/7400423 [05:58<06:07, 10413.13\nexamples/s]', '\\rMap:  48%|####8     | 3579000/7400423 [05:59<05:56, 10727.84\nexamples/s]', '\\rMap:  48%|####8     | 3581000/7400423 [05:59<05:49, 10943.53\nexamples/s]', '\\rMap:  48%|####8     | 3583000/7400423 [05:59<05:40, 11196.55\nexamples/s]', '\\rMap:  48%|####8     | 3585000/7400423 [05:59<05:34, 11412.57\nexamples/s]', '\\rMap:  48%|####8     | 3587000/7400423 [05:59<05:34, 11394.66\nexamples/s]', '\\rMap:  48%|####8     | 3589000/7400423 [05:59<05:29, 11557.52\nexamples/s]', '\\rMap:  49%|####8     | 3591000/7400423 [06:00<05:39, 11214.79\nexamples/s]', '\\rMap:  49%|####8     | 3593000/7400423 [06:00<05:43, 11075.80\nexamples/s]', '\\rMap:  49%|####8     | 3595000/7400423 [06:00<05:44, 11060.31\nexamples/s]', '\\rMap:  49%|####8     | 3597000/7400423 [06:00<05:44, 11047.77\nexamples/s]', '\\rMap:  49%|####8     | 3599000/7400423 [06:00<05:45, 11000.12\nexamples/s]', '\\rMap:  49%|####8     | 3601000/7400423 [06:01<05:51, 10805.67\nexamples/s]', '\\rMap:  49%|####8     | 3603000/7400423 [06:01<05:49, 10850.57\nexamples/s]', '\\rMap:  49%|####8     | 3605000/7400423 [06:01<05:54, 10694.31\nexamples/s]', '\\rMap:  49%|####8     | 3607000/7400423 [06:01<05:52, 10751.93\nexamples/s]', '\\rMap:  49%|####8     | 3609000/7400423 [06:01<05:56, 10650.00\nexamples/s]', '\\rMap:  49%|####8     | 3611000/7400423 [06:01<06:04, 10384.52\nexamples/s]', '\\rMap:  49%|####8     | 3613000/7400423 [06:02<06:14, 10118.55\nexamples/s]', '\\rMap:  49%|####8     | 3615000/7400423 [06:02<06:26, 9781.68\nexamples/s] ', '\\rMap:  49%|####8     | 3616000/7400423 [06:02<06:29, 9727.27\nexamples/s]', '\\rMap:  49%|####8     | 3617000/7400423 [06:02<06:29, 9724.49\nexamples/s]', '\\rMap:  49%|####8     | 3618000/7400423 [06:02<06:28, 9739.07\nexamples/s]', '\\rMap:  49%|####8     | 3619000/7400423 [06:02<06:31, 9667.09\nexamples/s]', '\\rMap:  49%|####8     | 3620000/7400423 [06:02<06:37, 9508.33\nexamples/s]', '\\rMap:  49%|####8     | 3621000/7400423 [06:03<06:39, 9450.11\nexamples/s]', '\\rMap:  49%|####8     | 3622000/7400423 [06:03<06:38, 9491.84\nexamples/s]', '\\rMap:  49%|####8     | 3623000/7400423 [06:03<06:39, 9456.53\nexamples/s]', '\\rMap:  49%|####8     | 3624000/7400423 [06:03<06:45, 9305.56\nexamples/s]', '\\rMap:  49%|####8     | 3625000/7400423 [06:03<06:44, 9342.50\nexamples/s]', '\\rMap:  49%|####8     | 3626000/7400423 [06:03<06:39, 9442.89\nexamples/s]', '\\rMap:  49%|####9     | 3628000/7400423 [06:03<06:27, 9740.95\nexamples/s]', '\\rMap:  49%|####9     | 3630000/7400423 [06:03<06:18, 9952.48\nexamples/s]', '\\rMap:  49%|####9     | 3631000/7400423 [06:04<06:18, 9949.09\nexamples/s]', '\\rMap:  49%|####9     | 3632000/7400423 [06:04<06:25, 9777.65\nexamples/s]', '\\rMap:  49%|####9     | 3634000/7400423 [06:04<06:11, 10137.02\nexamples/s]', '\\rMap:  49%|####9     | 3635000/7400423 [06:04<08:52, 7074.86\nexamples/s] ', '\\rMap:  49%|####9     | 3636000/7400423 [06:04<08:16, 7588.56\nexamples/s]', '\\rMap:  49%|####9     | 3638000/7400423 [06:04<07:25, 8438.02\nexamples/s]', '\\rMap:  49%|####9     | 3639000/7400423 [06:05<07:10, 8740.03\nexamples/s]', '\\rMap:  49%|####9     | 3641000/7400423 [06:05<06:45, 9271.79\nexamples/s]', '\\rMap:  49%|####9     | 3643000/7400423 [06:05<06:26, 9718.90\nexamples/s]', '\\rMap:  49%|####9     | 3645000/7400423 [06:05<06:17, 9947.53\nexamples/s]', '\\rMap:  49%|####9     | 3647000/7400423 [06:05<06:12, 10067.72\nexamples/s]', '\\rMap:  49%|####9     | 3649000/7400423 [06:06<06:13, 10033.50\nexamples/s]', '\\rMap:  49%|####9     | 3651000/7400423 [06:06<06:22, 9793.30\nexamples/s] ', '\\rMap:  49%|####9     | 3652000/7400423 [06:06<06:23, 9778.83\nexamples/s]', '\\rMap:  49%|####9     | 3654000/7400423 [06:06<06:08, 10161.53\nexamples/s]', '\\rMap:  49%|####9     | 3656000/7400423 [06:06<05:52, 10611.91\nexamples/s]', '\\rMap:  49%|####9     | 3658000/7400423 [06:06<05:38, 11051.89\nexamples/s]', '\\rMap:  49%|####9     | 3660000/7400423 [06:07<05:29, 11357.47\nexamples/s]', '\\rMap:  49%|####9     | 3662000/7400423 [06:07<05:28, 11385.71\nexamples/s]', '\\rMap:  50%|####9     | 3664000/7400423 [06:07<05:30, 11309.93\nexamples/s]', '\\rMap:  50%|####9     | 3666000/7400423 [06:07<05:27, 11391.21\nexamples/s]', '\\rMap:  50%|####9     | 3668000/7400423 [06:07<05:25, 11453.41\nexamples/s]', '\\rMap:  50%|####9     | 3670000/7400423 [06:07<05:24, 11482.31\nexamples/s]', '\\rMap:  50%|####9     | 3672000/7400423 [06:08<05:24, 11500.79\nexamples/s]', '\\rMap:  50%|####9     | 3674000/7400423 [06:08<05:22, 11553.91\nexamples/s]', '\\rMap:  50%|####9     | 3676000/7400423 [06:08<05:22, 11559.28\nexamples/s]', '\\rMap:  50%|####9     | 3678000/7400423 [06:08<05:19, 11655.11\nexamples/s]', '\\rMap:  50%|####9     | 3680000/7400423 [06:08<05:19, 11631.47\nexamples/s]', '\\rMap:  50%|####9     | 3682000/7400423 [06:08<05:16, 11741.13\nexamples/s]', '\\rMap:  50%|####9     | 3684000/7400423 [06:09<05:20, 11578.81\nexamples/s]', '\\rMap:  50%|####9     | 3686000/7400423 [06:09<05:17, 11701.83\nexamples/s]', '\\rMap:  50%|####9     | 3688000/7400423 [06:09<05:10, 11962.49\nexamples/s]', '\\rMap:  50%|####9     | 3690000/7400423 [06:09<05:14, 11813.84\nexamples/s]', '\\rMap:  50%|####9     | 3692000/7400423 [06:09<05:12, 11848.67\nexamples/s]', '\\rMap:  50%|####9     | 3694000/7400423 [06:09<05:17, 11690.89\nexamples/s]', '\\rMap:  50%|####9     | 3696000/7400423 [06:10<05:16, 11718.45\nexamples/s]', '\\rMap:  50%|####9     | 3698000/7400423 [06:10<05:18, 11624.92\nexamples/s]', '\\rMap:  50%|####9     | 3700000/7400423 [06:10<05:17, 11663.81\nexamples/s]', '\\rMap:  50%|#####     | 3702000/7400423 [06:10<05:12, 11822.20\nexamples/s]', '\\rMap:  50%|#####     | 3704000/7400423 [06:10<05:10, 11890.17\nexamples/s]', '\\rMap:  50%|#####     | 3706000/7400423 [06:10<05:09, 11939.44\nexamples/s]', '\\rMap:  50%|#####     | 3708000/7400423 [06:11<05:06, 12048.42\nexamples/s]', '\\rMap:  50%|#####     | 3710000/7400423 [06:11<05:12, 11821.71\nexamples/s]', '\\rMap:  50%|#####     | 3712000/7400423 [06:11<07:07, 8628.17\nexamples/s] ', '\\rMap:  50%|#####     | 3714000/7400423 [06:11<06:31, 9408.74\nexamples/s]', '\\rMap:  50%|#####     | 3716000/7400423 [06:12<06:09, 9978.36\nexamples/s]', '\\rMap:  50%|#####     | 3718000/7400423 [06:12<05:51, 10468.81\nexamples/s]', '\\rMap:  50%|#####     | 3720000/7400423 [06:12<05:42, 10732.95\nexamples/s]', '\\rMap:  50%|#####     | 3722000/7400423 [06:12<05:31, 11094.84\nexamples/s]', '\\rMap:  50%|#####     | 3724000/7400423 [06:12<05:24, 11312.26\nexamples/s]', '\\rMap:  50%|#####     | 3726000/7400423 [06:12<05:26, 11268.55\nexamples/s]', '\\rMap:  50%|#####     | 3728000/7400423 [06:13<05:24, 11319.12\nexamples/s]', '\\rMap:  50%|#####     | 3730000/7400423 [06:13<05:21, 11429.28\nexamples/s]', '\\rMap:  50%|#####     | 3732000/7400423 [06:13<05:21, 11398.60\nexamples/s]', '\\rMap:  50%|#####     | 3734000/7400423 [06:13<05:26, 11222.22\nexamples/s]', '\\rMap:  50%|#####     | 3736000/7400423 [06:13<05:35, 10930.43\nexamples/s]', '\\rMap:  51%|#####     | 3738000/7400423 [06:13<05:35, 10906.38\nexamples/s]', '\\rMap:  51%|#####     | 3740000/7400423 [06:14<05:33, 10980.91\nexamples/s]', '\\rMap:  51%|#####     | 3742000/7400423 [06:14<05:32, 10995.58\nexamples/s]', '\\rMap:  51%|#####     | 3744000/7400423 [06:14<05:26, 11184.96\nexamples/s]', '\\rMap:  51%|#####     | 3746000/7400423 [06:14<05:23, 11308.79\nexamples/s]', '\\rMap:  51%|#####     | 3748000/7400423 [06:14<05:21, 11371.92\nexamples/s]', '\\rMap:  51%|#####     | 3750000/7400423 [06:15<05:28, 11102.20\nexamples/s]', '\\rMap:  51%|#####     | 3752000/7400423 [06:15<05:37, 10805.72\nexamples/s]', '\\rMap:  51%|#####     | 3754000/7400423 [06:15<05:36, 10833.97\nexamples/s]', '\\rMap:  51%|#####     | 3756000/7400423 [06:15<05:32, 10956.85\nexamples/s]', '\\rMap:  51%|#####     | 3758000/7400423 [06:15<05:30, 11006.78\nexamples/s]', '\\rMap:  51%|#####     | 3760000/7400423 [06:15<05:31, 10974.24\nexamples/s]', '\\rMap:  51%|#####     | 3762000/7400423 [06:16<05:28, 11089.80\nexamples/s]', '\\rMap:  51%|#####     | 3764000/7400423 [06:16<05:27, 11086.99\nexamples/s]', '\\rMap:  51%|#####     | 3766000/7400423 [06:16<05:41, 10653.10\nexamples/s]', '\\rMap:  51%|#####     | 3768000/7400423 [06:16<05:49, 10403.77\nexamples/s]', '\\rMap:  51%|#####     | 3770000/7400423 [06:16<05:53, 10280.38\nexamples/s]', '\\rMap:  51%|#####     | 3772000/7400423 [06:17<05:54, 10243.51\nexamples/s]', '\\rMap:  51%|#####     | 3774000/7400423 [06:17<05:51, 10327.01\nexamples/s]', '\\rMap:  51%|#####1    | 3776000/7400423 [06:17<05:44, 10535.72\nexamples/s]', '\\rMap:  51%|#####1    | 3778000/7400423 [06:17<05:41, 10620.01\nexamples/s]', '\\rMap:  51%|#####1    | 3780000/7400423 [06:17<05:31, 10921.21\nexamples/s]', '\\rMap:  51%|#####1    | 3782000/7400423 [06:17<05:27, 11051.79\nexamples/s]', '\\rMap:  51%|#####1    | 3784000/7400423 [06:18<05:21, 11255.93\nexamples/s]', '\\rMap:  51%|#####1    | 3786000/7400423 [06:18<05:16, 11433.05\nexamples/s]', '\\rMap:  51%|#####1    | 3788000/7400423 [06:18<05:11, 11587.54\nexamples/s]', '\\rMap:  51%|#####1    | 3790000/7400423 [06:18<05:08, 11709.93\nexamples/s]', '\\rMap:  51%|#####1    | 3792000/7400423 [06:18<05:08, 11683.38\nexamples/s]', '\\rMap:  51%|#####1    | 3794000/7400423 [06:19<05:13, 11505.92\nexamples/s]', '\\rMap:  51%|#####1    | 3796000/7400423 [06:19<05:18, 11333.56\nexamples/s]', '\\rMap:  51%|#####1    | 3798000/7400423 [06:19<05:23, 11143.97\nexamples/s]', '\\rMap:  51%|#####1    | 3800000/7400423 [06:19<05:28, 10971.52\nexamples/s]', '\\rMap:  51%|#####1    | 3802000/7400423 [06:19<05:30, 10895.86\nexamples/s]', '\\rMap:  51%|#####1    | 3804000/7400423 [06:19<05:36, 10695.99\nexamples/s]', '\\rMap:  51%|#####1    | 3806000/7400423 [06:20<05:44, 10441.56\nexamples/s]', '\\rMap:  51%|#####1    | 3808000/7400423 [06:20<05:46, 10357.10\nexamples/s]', '\\rMap:  51%|#####1    | 3810000/7400423 [06:20<05:45, 10404.34\nexamples/s]', '\\rMap:  52%|#####1    | 3812000/7400423 [06:20<05:53, 10143.59\nexamples/s]', '\\rMap:  52%|#####1    | 3814000/7400423 [06:20<06:03, 9873.70\nexamples/s] ', '\\rMap:  52%|#####1    | 3815000/7400423 [06:21<06:15, 9551.17\nexamples/s]', '\\rMap:  52%|#####1    | 3816000/7400423 [06:21<06:14, 9582.99\nexamples/s]', '\\rMap:  52%|#####1    | 3818000/7400423 [06:21<06:13, 9578.71\nexamples/s]', '\\rMap:  52%|#####1    | 3819000/7400423 [06:21<06:16, 9504.61\nexamples/s]', '\\rMap:  52%|#####1    | 3820000/7400423 [06:21<06:17, 9496.87\nexamples/s]', '\\rMap:  52%|#####1    | 3821000/7400423 [06:21<06:17, 9494.36\nexamples/s]', '\\rMap:  52%|#####1    | 3822000/7400423 [06:21<06:21, 9388.70\nexamples/s]', '\\rMap:  52%|#####1    | 3823000/7400423 [06:21<06:26, 9262.94\nexamples/s]', '\\rMap:  52%|#####1    | 3824000/7400423 [06:22<06:24, 9303.79\nexamples/s]', '\\rMap:  52%|#####1    | 3825000/7400423 [06:22<06:18, 9439.90\nexamples/s]', '\\rMap:  52%|#####1    | 3826000/7400423 [06:22<06:14, 9554.56\nexamples/s]', '\\rMap:  52%|#####1    | 3827000/7400423 [06:22<06:10, 9656.56\nexamples/s]', '\\rMap:  52%|#####1    | 3828000/7400423 [06:22<06:06, 9752.08\nexamples/s]', '\\rMap:  52%|#####1    | 3829000/7400423 [06:22<06:08, 9684.07\nexamples/s]', '\\rMap:  52%|#####1    | 3830000/7400423 [06:22<06:06, 9732.87\nexamples/s]', '\\rMap:  52%|#####1    | 3831000/7400423 [06:22<06:05, 9757.27\nexamples/s]', '\\rMap:  52%|#####1    | 3833000/7400423 [06:22<06:01, 9865.59\nexamples/s]', '\\rMap:  52%|#####1    | 3835000/7400423 [06:23<05:57, 9980.52\nexamples/s]', '\\rMap:  52%|#####1    | 3836000/7400423 [06:23<06:02, 9827.74\nexamples/s]', '\\rMap:  52%|#####1    | 3837000/7400423 [06:23<08:52, 6690.97\nexamples/s]', '\\rMap:  52%|#####1    | 3838000/7400423 [06:23<08:09, 7275.35\nexamples/s]', '\\rMap:  52%|#####1    | 3840000/7400423 [06:23<07:12, 8233.90\nexamples/s]', '\\rMap:  52%|#####1    | 3842000/7400423 [06:24<06:36, 8967.76\nexamples/s]', '\\rMap:  52%|#####1    | 3843000/7400423 [06:24<06:28, 9151.21\nexamples/s]', '\\rMap:  52%|#####1    | 3845000/7400423 [06:24<06:16, 9441.87\nexamples/s]', '\\rMap:  52%|#####1    | 3846000/7400423 [06:24<06:15, 9463.49\nexamples/s]', '\\rMap:  52%|#####1    | 3848000/7400423 [06:24<05:59, 9882.09\nexamples/s]', '\\rMap:  52%|#####2    | 3850000/7400423 [06:24<05:44, 10306.58\nexamples/s]', '\\rMap:  52%|#####2    | 3852000/7400423 [06:25<06:00, 9837.78\nexamples/s] ', '\\rMap:  52%|#####2    | 3853000/7400423 [06:25<06:04, 9741.42\nexamples/s]', '\\rMap:  52%|#####2    | 3854000/7400423 [06:25<06:11, 9544.19\nexamples/s]', '\\rMap:  52%|#####2    | 3855000/7400423 [06:25<06:22, 9262.79\nexamples/s]', '\\rMap:  52%|#####2    | 3856000/7400423 [06:25<06:35, 8968.82\nexamples/s]', '\\rMap:  52%|#####2    | 3857000/7400423 [06:25<06:45, 8747.90\nexamples/s]', '\\rMap:  52%|#####2    | 3858000/7400423 [06:25<06:41, 8822.43\nexamples/s]', '\\rMap:  52%|#####2    | 3859000/7400423 [06:25<06:29, 9089.05\nexamples/s]', '\\rMap:  52%|#####2    | 3860000/7400423 [06:25<06:21, 9291.05\nexamples/s]', '\\rMap:  52%|#####2    | 3861000/7400423 [06:26<06:14, 9452.36\nexamples/s]', '\\rMap:  52%|#####2    | 3863000/7400423 [06:26<05:54, 9992.17\nexamples/s]', '\\rMap:  52%|#####2    | 3865000/7400423 [06:26<05:43, 10286.57\nexamples/s]', '\\rMap:  52%|#####2    | 3867000/7400423 [06:26<05:26, 10823.18\nexamples/s]', '\\rMap:  52%|#####2    | 3869000/7400423 [06:26<05:21, 10988.01\nexamples/s]', '\\rMap:  52%|#####2    | 3871000/7400423 [06:26<05:14, 11224.46\nexamples/s]', '\\rMap:  52%|#####2    | 3873000/7400423 [06:27<05:20, 10999.90\nexamples/s]', '\\rMap:  52%|#####2    | 3875000/7400423 [06:27<05:13, 11241.39\nexamples/s]', '\\rMap:  52%|#####2    | 3877000/7400423 [06:27<05:11, 11308.07\nexamples/s]', '\\rMap:  52%|#####2    | 3879000/7400423 [06:27<05:13, 11221.35\nexamples/s]', '\\rMap:  52%|#####2    | 3881000/7400423 [06:27<05:18, 11038.98\nexamples/s]', '\\rMap:  52%|#####2    | 3883000/7400423 [06:28<05:21, 10938.59\nexamples/s]', '\\rMap:  52%|#####2    | 3885000/7400423 [06:28<05:16, 11102.59\nexamples/s]', '\\rMap:  53%|#####2    | 3887000/7400423 [06:28<05:23, 10854.70\nexamples/s]', '\\rMap:  53%|#####2    | 3889000/7400423 [06:28<05:24, 10823.78\nexamples/s]', '\\rMap:  53%|#####2    | 3891000/7400423 [06:28<05:25, 10782.78\nexamples/s]', '\\rMap:  53%|#####2    | 3893000/7400423 [06:28<05:22, 10860.86\nexamples/s]', '\\rMap:  53%|#####2    | 3895000/7400423 [06:29<05:22, 10857.20\nexamples/s]', '\\rMap:  53%|#####2    | 3897000/7400423 [06:29<05:14, 11157.27\nexamples/s]', '\\rMap:  53%|#####2    | 3899000/7400423 [06:29<05:23, 10826.09\nexamples/s]', '\\rMap:  53%|#####2    | 3901000/7400423 [06:29<05:18, 10974.73\nexamples/s]', '\\rMap:  53%|#####2    | 3903000/7400423 [06:29<05:14, 11121.36\nexamples/s]', '\\rMap:  53%|#####2    | 3905000/7400423 [06:30<05:11, 11234.18\nexamples/s]', '\\rMap:  53%|#####2    | 3907000/7400423 [06:30<05:10, 11264.32\nexamples/s]', '\\rMap:  53%|#####2    | 3909000/7400423 [06:30<05:16, 11045.12\nexamples/s]', '\\rMap:  53%|#####2    | 3911000/7400423 [06:30<05:14, 11098.39\nexamples/s]', '\\rMap:  53%|#####2    | 3913000/7400423 [06:30<05:22, 10816.96\nexamples/s]', '\\rMap:  53%|#####2    | 3915000/7400423 [06:30<05:22, 10823.86\nexamples/s]', '\\rMap:  53%|#####2    | 3917000/7400423 [06:31<05:14, 11068.50\nexamples/s]', '\\rMap:  53%|#####2    | 3919000/7400423 [06:31<05:19, 10896.32\nexamples/s]', '\\rMap:  53%|#####2    | 3921000/7400423 [06:31<05:19, 10875.02\nexamples/s]', '\\rMap:  53%|#####3    | 3923000/7400423 [06:31<05:17, 10937.83\nexamples/s]', '\\rMap:  53%|#####3    | 3925000/7400423 [06:31<05:20, 10847.48\nexamples/s]', '\\rMap:  53%|#####3    | 3927000/7400423 [06:32<05:13, 11096.20\nexamples/s]', '\\rMap:  53%|#####3    | 3929000/7400423 [06:32<05:05, 11352.60\nexamples/s]', '\\rMap:  53%|#####3    | 3931000/7400423 [06:32<04:58, 11610.90\nexamples/s]', '\\rMap:  53%|#####3    | 3933000/7400423 [06:32<05:10, 11150.84\nexamples/s]', '\\rMap:  53%|#####3    | 3935000/7400423 [06:32<05:22, 10750.83\nexamples/s]', '\\rMap:  53%|#####3    | 3937000/7400423 [06:33<07:14, 7975.09\nexamples/s] ', '\\rMap:  53%|#####3    | 3939000/7400423 [06:33<06:41, 8619.76\nexamples/s]', '\\rMap:  53%|#####3    | 3941000/7400423 [06:33<06:09, 9364.33\nexamples/s]', '\\rMap:  53%|#####3    | 3943000/7400423 [06:33<05:45, 10007.50\nexamples/s]', '\\rMap:  53%|#####3    | 3945000/7400423 [06:33<05:31, 10428.01\nexamples/s]', '\\rMap:  53%|#####3    | 3947000/7400423 [06:34<05:31, 10409.72\nexamples/s]', '\\rMap:  53%|#####3    | 3949000/7400423 [06:34<05:29, 10477.65\nexamples/s]', '\\rMap:  53%|#####3    | 3951000/7400423 [06:34<05:26, 10562.68\nexamples/s]', '\\rMap:  53%|#####3    | 3953000/7400423 [06:34<05:28, 10502.58\nexamples/s]', '\\rMap:  53%|#####3    | 3955000/7400423 [06:34<05:29, 10447.07\nexamples/s]', '\\rMap:  53%|#####3    | 3957000/7400423 [06:35<05:24, 10617.40\nexamples/s]', '\\rMap:  53%|#####3    | 3959000/7400423 [06:35<05:15, 10903.04\nexamples/s]', '\\rMap:  54%|#####3    | 3961000/7400423 [06:35<05:14, 10934.20\nexamples/s]', '\\rMap:  54%|#####3    | 3963000/7400423 [06:35<05:17, 10837.67\nexamples/s]', '\\rMap:  54%|#####3    | 3965000/7400423 [06:35<05:14, 10927.83\nexamples/s]', '\\rMap:  54%|#####3    | 3967000/7400423 [06:35<05:12, 10995.58\nexamples/s]', '\\rMap:  54%|#####3    | 3969000/7400423 [06:36<05:19, 10730.53\nexamples/s]', '\\rMap:  54%|#####3    | 3971000/7400423 [06:36<05:23, 10589.45\nexamples/s]', '\\rMap:  54%|#####3    | 3973000/7400423 [06:36<05:24, 10550.02\nexamples/s]', '\\rMap:  54%|#####3    | 3975000/7400423 [06:36<05:26, 10482.77\nexamples/s]', '\\rMap:  54%|#####3    | 3977000/7400423 [06:36<05:34, 10239.81\nexamples/s]', '\\rMap:  54%|#####3    | 3979000/7400423 [06:37<05:37, 10145.71\nexamples/s]', '\\rMap:  54%|#####3    | 3981000/7400423 [06:37<05:41, 10019.84\nexamples/s]', '\\rMap:  54%|#####3    | 3983000/7400423 [06:37<05:44, 9920.38\nexamples/s] ', '\\rMap:  54%|#####3    | 3985000/7400423 [06:37<05:43, 9938.36\nexamples/s]', '\\rMap:  54%|#####3    | 3986000/7400423 [06:37<05:47, 9828.45\nexamples/s]', '\\rMap:  54%|#####3    | 3987000/7400423 [06:37<05:50, 9741.55\nexamples/s]', '\\rMap:  54%|#####3    | 3988000/7400423 [06:38<05:48, 9787.83\nexamples/s]', '\\rMap:  54%|#####3    | 3990000/7400423 [06:38<05:41, 10001.24\nexamples/s]', '\\rMap:  54%|#####3    | 3991000/7400423 [06:38<05:45, 9879.77\nexamples/s] ', '\\rMap:  54%|#####3    | 3992000/7400423 [06:38<05:45, 9853.76\nexamples/s]', '\\rMap:  54%|#####3    | 3993000/7400423 [06:38<06:00, 9443.90\nexamples/s]', '\\rMap:  54%|#####3    | 3994000/7400423 [06:38<05:57, 9520.47\nexamples/s]', '\\rMap:  54%|#####3    | 3995000/7400423 [06:38<05:58, 9508.45\nexamples/s]', '\\rMap:  54%|#####3    | 3996000/7400423 [06:38<06:06, 9283.17\nexamples/s]', '\\rMap:  54%|#####4    | 3997000/7400423 [06:38<06:03, 9367.57\nexamples/s]', '\\rMap:  54%|#####4    | 3998000/7400423 [06:39<06:14, 9077.63\nexamples/s]', '\\rMap:  54%|#####4    | 3999000/7400423 [06:39<06:23, 8864.57\nexamples/s]', '\\rMap:  54%|#####4    | 4000000/7400423 [06:39<06:20, 8933.06\nexamples/s]', '\\rMap:  54%|#####4    | 4001000/7400423 [06:39<06:27, 8761.65\nexamples/s]', '\\rMap:  54%|#####4    | 4002000/7400423 [06:39<06:18, 8987.97\nexamples/s]', '\\rMap:  54%|#####4    | 4003000/7400423 [06:39<06:19, 8943.38\nexamples/s]', '\\rMap:  54%|#####4    | 4004000/7400423 [06:39<06:23, 8853.42\nexamples/s]', '\\rMap:  54%|#####4    | 4005000/7400423 [06:39<06:19, 8953.12\nexamples/s]', '\\rMap:  54%|#####4    | 4006000/7400423 [06:39<06:21, 8899.57\nexamples/s]', '\\rMap:  54%|#####4    | 4007000/7400423 [06:40<06:24, 8832.70\nexamples/s]', '\\rMap:  54%|#####4    | 4008000/7400423 [06:40<06:12, 9095.92\nexamples/s]', '\\rMap:  54%|#####4    | 4009000/7400423 [06:40<06:06, 9263.44\nexamples/s]', '\\rMap:  54%|#####4    | 4010000/7400423 [06:40<06:00, 9413.92\nexamples/s]', '\\rMap:  54%|#####4    | 4011000/7400423 [06:40<06:12, 9089.00\nexamples/s]', '\\rMap:  54%|#####4    | 4012000/7400423 [06:40<06:04, 9292.55\nexamples/s]', '\\rMap:  54%|#####4    | 4013000/7400423 [06:40<05:58, 9453.05\nexamples/s]', '\\rMap:  54%|#####4    | 4014000/7400423 [06:40<05:52, 9604.74\nexamples/s]', '\\rMap:  54%|#####4    | 4015000/7400423 [06:40<05:53, 9584.84\nexamples/s]', '\\rMap:  54%|#####4    | 4017000/7400423 [06:41<05:45, 9802.10\nexamples/s]', '\\rMap:  54%|#####4    | 4018000/7400423 [06:41<05:47, 9739.97\nexamples/s]', '\\rMap:  54%|#####4    | 4019000/7400423 [06:41<05:50, 9644.68\nexamples/s]', '\\rMap:  54%|#####4    | 4021000/7400423 [06:41<05:51, 9618.80\nexamples/s]', '\\rMap:  54%|#####4    | 4022000/7400423 [06:41<05:49, 9676.25\nexamples/s]', '\\rMap:  54%|#####4    | 4023000/7400423 [06:41<08:40, 6493.94\nexamples/s]', '\\rMap:  54%|#####4    | 4024000/7400423 [06:42<08:09, 6890.93\nexamples/s]', '\\rMap:  54%|#####4    | 4025000/7400423 [06:42<07:37, 7370.19\nexamples/s]', '\\rMap:  54%|#####4    | 4026000/7400423 [06:42<07:09, 7854.04\nexamples/s]', '\\rMap:  54%|#####4    | 4027000/7400423 [06:42<06:54, 8142.05\nexamples/s]', '\\rMap:  54%|#####4    | 4028000/7400423 [06:42<06:46, 8298.32\nexamples/s]', '\\rMap:  54%|#####4    | 4029000/7400423 [06:42<06:42, 8378.26\nexamples/s]', '\\rMap:  54%|#####4    | 4030000/7400423 [06:42<06:44, 8335.16\nexamples/s]', '\\rMap:  54%|#####4    | 4031000/7400423 [06:42<06:35, 8528.94\nexamples/s]', '\\rMap:  54%|#####4    | 4032000/7400423 [06:42<06:35, 8517.18\nexamples/s]', '\\rMap:  54%|#####4    | 4033000/7400423 [06:43<06:29, 8650.08\nexamples/s]', '\\rMap:  55%|#####4    | 4034000/7400423 [06:43<06:30, 8630.92\nexamples/s]', '\\rMap:  55%|#####4    | 4035000/7400423 [06:43<06:28, 8652.52\nexamples/s]', '\\rMap:  55%|#####4    | 4036000/7400423 [06:43<06:17, 8921.83\nexamples/s]', '\\rMap:  55%|#####4    | 4038000/7400423 [06:43<05:51, 9557.63\nexamples/s]', '\\rMap:  55%|#####4    | 4039000/7400423 [06:43<05:48, 9646.80\nexamples/s]', '\\rMap:  55%|#####4    | 4040000/7400423 [06:43<05:47, 9671.63\nexamples/s]', '\\rMap:  55%|#####4    | 4041000/7400423 [06:43<05:50, 9593.04\nexamples/s]', '\\rMap:  55%|#####4    | 4042000/7400423 [06:44<05:55, 9433.94\nexamples/s]', '\\rMap:  55%|#####4    | 4043000/7400423 [06:44<05:54, 9473.72\nexamples/s]', '\\rMap:  55%|#####4    | 4044000/7400423 [06:44<05:56, 9425.91\nexamples/s]', '\\rMap:  55%|#####4    | 4045000/7400423 [06:44<05:52, 9520.72\nexamples/s]', '\\rMap:  55%|#####4    | 4047000/7400423 [06:44<05:44, 9738.00\nexamples/s]', '\\rMap:  55%|#####4    | 4048000/7400423 [06:44<05:47, 9649.56\nexamples/s]', '\\rMap:  55%|#####4    | 4050000/7400423 [06:44<05:39, 9859.93\nexamples/s]', '\\rMap:  55%|#####4    | 4052000/7400423 [06:45<05:35, 9975.64\nexamples/s]', '\\rMap:  55%|#####4    | 4054000/7400423 [06:45<05:30, 10128.35\nexamples/s]', '\\rMap:  55%|#####4    | 4056000/7400423 [06:45<05:28, 10175.67\nexamples/s]', '\\rMap:  55%|#####4    | 4058000/7400423 [06:45<05:25, 10262.94\nexamples/s]', '\\rMap:  55%|#####4    | 4060000/7400423 [06:45<05:24, 10278.23\nexamples/s]', '\\rMap:  55%|#####4    | 4062000/7400423 [06:46<05:25, 10241.82\nexamples/s]', '\\rMap:  55%|#####4    | 4064000/7400423 [06:46<05:15, 10570.06\nexamples/s]', '\\rMap:  55%|#####4    | 4066000/7400423 [06:46<05:09, 10784.51\nexamples/s]', '\\rMap:  55%|#####4    | 4068000/7400423 [06:46<05:09, 10753.83\nexamples/s]', '\\rMap:  55%|#####4    | 4070000/7400423 [06:46<05:10, 10742.46\nexamples/s]', '\\rMap:  55%|#####5    | 4072000/7400423 [06:46<05:13, 10605.98\nexamples/s]', '\\rMap:  55%|#####5    | 4074000/7400423 [06:47<05:15, 10537.90\nexamples/s]', '\\rMap:  55%|#####5    | 4076000/7400423 [06:47<05:13, 10620.85\nexamples/s]', '\\rMap:  55%|#####5    | 4078000/7400423 [06:47<05:11, 10655.21\nexamples/s]', '\\rMap:  55%|#####5    | 4080000/7400423 [06:47<05:13, 10582.62\nexamples/s]', '\\rMap:  55%|#####5    | 4082000/7400423 [06:47<05:16, 10478.10\nexamples/s]', '\\rMap:  55%|#####5    | 4084000/7400423 [06:48<05:15, 10501.29\nexamples/s]', '\\rMap:  55%|#####5    | 4086000/7400423 [06:48<05:17, 10441.12\nexamples/s]', '\\rMap:  55%|#####5    | 4088000/7400423 [06:48<05:21, 10309.63\nexamples/s]', '\\rMap:  55%|#####5    | 4090000/7400423 [06:48<05:25, 10182.02\nexamples/s]', '\\rMap:  55%|#####5    | 4092000/7400423 [06:48<05:22, 10270.03\nexamples/s]', '\\rMap:  55%|#####5    | 4094000/7400423 [06:49<05:17, 10399.12\nexamples/s]', '\\rMap:  55%|#####5    | 4096000/7400423 [06:49<05:26, 10112.62\nexamples/s]', '\\rMap:  55%|#####5    | 4098000/7400423 [06:49<05:28, 10042.18\nexamples/s]', '\\rMap:  55%|#####5    | 4100000/7400423 [06:49<07:06, 7733.83\nexamples/s] ', '\\rMap:  55%|#####5    | 4102000/7400423 [06:50<06:37, 8304.62\nexamples/s]', '\\rMap:  55%|#####5    | 4104000/7400423 [06:50<06:15, 8769.60\nexamples/s]', '\\rMap:  55%|#####5    | 4105000/7400423 [06:50<06:07, 8960.86\nexamples/s]', '\\rMap:  55%|#####5    | 4107000/7400423 [06:50<05:53, 9323.35\nexamples/s]', '\\rMap:  56%|#####5    | 4109000/7400423 [06:50<05:44, 9557.97\nexamples/s]', '\\rMap:  56%|#####5    | 4110000/7400423 [06:50<05:42, 9596.30\nexamples/s]', '\\rMap:  56%|#####5    | 4112000/7400423 [06:51<05:36, 9785.88\nexamples/s]', '\\rMap:  56%|#####5    | 4114000/7400423 [06:51<05:35, 9808.66\nexamples/s]', '\\rMap:  56%|#####5    | 4115000/7400423 [06:51<05:34, 9817.35\nexamples/s]', '\\rMap:  56%|#####5    | 4116000/7400423 [06:51<05:35, 9779.85\nexamples/s]', '\\rMap:  56%|#####5    | 4118000/7400423 [06:51<05:28, 9994.39\nexamples/s]', '\\rMap:  56%|#####5    | 4120000/7400423 [06:51<05:27, 10014.11\nexamples/s]', '\\rMap:  56%|#####5    | 4122000/7400423 [06:52<05:28, 9994.29\nexamples/s] ', '\\rMap:  56%|#####5    | 4124000/7400423 [06:52<05:26, 10044.21\nexamples/s]', '\\rMap:  56%|#####5    | 4126000/7400423 [06:52<05:24, 10088.40\nexamples/s]', '\\rMap:  56%|#####5    | 4128000/7400423 [06:52<05:22, 10143.02\nexamples/s]', '\\rMap:  56%|#####5    | 4130000/7400423 [06:52<05:23, 10113.41\nexamples/s]', '\\rMap:  56%|#####5    | 4132000/7400423 [06:53<05:23, 10112.32\nexamples/s]', '\\rMap:  56%|#####5    | 4134000/7400423 [06:53<05:24, 10077.79\nexamples/s]', '\\rMap:  56%|#####5    | 4136000/7400423 [06:53<05:19, 10230.75\nexamples/s]', '\\rMap:  56%|#####5    | 4138000/7400423 [06:53<05:18, 10237.49\nexamples/s]', '\\rMap:  56%|#####5    | 4140000/7400423 [06:53<05:17, 10266.84\nexamples/s]', '\\rMap:  56%|#####5    | 4142000/7400423 [06:54<05:22, 10088.44\nexamples/s]', '\\rMap:  56%|#####5    | 4144000/7400423 [06:54<05:23, 10067.91\nexamples/s]', '\\rMap:  56%|#####6    | 4146000/7400423 [06:54<05:19, 10183.66\nexamples/s]', '\\rMap:  56%|#####6    | 4148000/7400423 [06:54<05:19, 10169.04\nexamples/s]', '\\rMap:  56%|#####6    | 4150000/7400423 [06:54<05:14, 10323.16\nexamples/s]', '\\rMap:  56%|#####6    | 4152000/7400423 [06:54<05:12, 10392.17\nexamples/s]', '\\rMap:  56%|#####6    | 4154000/7400423 [06:55<05:13, 10351.97\nexamples/s]', '\\rMap:  56%|#####6    | 4156000/7400423 [06:55<05:12, 10366.80\nexamples/s]', '\\rMap:  56%|#####6    | 4158000/7400423 [06:55<05:11, 10405.94\nexamples/s]', '\\rMap:  56%|#####6    | 4160000/7400423 [06:55<05:11, 10388.15\nexamples/s]', '\\rMap:  56%|#####6    | 4162000/7400423 [06:55<05:16, 10231.43\nexamples/s]', '\\rMap:  56%|#####6    | 4164000/7400423 [06:56<05:18, 10148.59\nexamples/s]', '\\rMap:  56%|#####6    | 4166000/7400423 [06:56<05:23, 10013.56\nexamples/s]', '\\rMap:  56%|#####6    | 4168000/7400423 [06:56<05:19, 10118.29\nexamples/s]', '\\rMap:  56%|#####6    | 4170000/7400423 [06:56<05:14, 10265.77\nexamples/s]', '\\rMap:  56%|#####6    | 4172000/7400423 [06:56<05:13, 10285.86\nexamples/s]', '\\rMap:  56%|#####6    | 4174000/7400423 [06:57<05:15, 10227.72\nexamples/s]', '\\rMap:  56%|#####6    | 4176000/7400423 [06:57<05:14, 10240.40\nexamples/s]', '\\rMap:  56%|#####6    | 4178000/7400423 [06:57<05:16, 10188.71\nexamples/s]', '\\rMap:  56%|#####6    | 4180000/7400423 [06:57<05:16, 10174.63\nexamples/s]', '\\rMap:  57%|#####6    | 4182000/7400423 [06:57<05:19, 10068.43\nexamples/s]', '\\rMap:  57%|#####6    | 4184000/7400423 [06:58<05:25, 9873.56\nexamples/s] ', '\\rMap:  57%|#####6    | 4185000/7400423 [06:58<05:26, 9852.77\nexamples/s]', '\\rMap:  57%|#####6    | 4187000/7400423 [06:58<05:28, 9794.14\nexamples/s]', '\\rMap:  57%|#####6    | 4188000/7400423 [06:58<05:27, 9819.12\nexamples/s]', '\\rMap:  57%|#####6    | 4189000/7400423 [06:58<05:27, 9818.64\nexamples/s]', '\\rMap:  57%|#####6    | 4190000/7400423 [06:58<05:28, 9764.10\nexamples/s]', '\\rMap:  57%|#####6    | 4191000/7400423 [06:58<05:30, 9704.64\nexamples/s]', '\\rMap:  57%|#####6    | 4192000/7400423 [06:58<05:31, 9682.59\nexamples/s]', '\\rMap:  57%|#####6    | 4194000/7400423 [06:59<05:25, 9856.13\nexamples/s]', '\\rMap:  57%|#####6    | 4196000/7400423 [06:59<05:20, 9996.90\nexamples/s]', '\\rMap:  57%|#####6    | 4198000/7400423 [06:59<05:19, 10011.47\nexamples/s]', '\\rMap:  57%|#####6    | 4200000/7400423 [06:59<05:16, 10097.51\nexamples/s]', '\\rMap:  57%|#####6    | 4202000/7400423 [06:59<05:14, 10173.26\nexamples/s]', '\\rMap:  57%|#####6    | 4204000/7400423 [07:00<05:17, 10073.28\nexamples/s]', '\\rMap:  57%|#####6    | 4206000/7400423 [07:00<05:15, 10116.37\nexamples/s]', '\\rMap:  57%|#####6    | 4208000/7400423 [07:00<05:11, 10235.17\nexamples/s]', '\\rMap:  57%|#####6    | 4210000/7400423 [07:00<05:10, 10258.62\nexamples/s]', '\\rMap:  57%|#####6    | 4212000/7400423 [07:00<05:11, 10224.65\nexamples/s]', '\\rMap:  57%|#####6    | 4214000/7400423 [07:01<05:08, 10324.70\nexamples/s]', '\\rMap:  57%|#####6    | 4216000/7400423 [07:01<05:02, 10509.65\nexamples/s]', '\\rMap:  57%|#####6    | 4218000/7400423 [07:01<05:00, 10576.94\nexamples/s]', '\\rMap:  57%|#####7    | 4220000/7400423 [07:01<04:55, 10774.20\nexamples/s]', '\\rMap:  57%|#####7    | 4222000/7400423 [07:01<04:49, 10967.70\nexamples/s]', '\\rMap:  57%|#####7    | 4224000/7400423 [07:02<04:45, 11129.80\nexamples/s]', '\\rMap:  57%|#####7    | 4226000/7400423 [07:02<06:44, 7845.54\nexamples/s] ', '\\rMap:  57%|#####7    | 4228000/7400423 [07:02<06:14, 8475.23\nexamples/s]', '\\rMap:  57%|#####7    | 4230000/7400423 [07:02<05:52, 8985.85\nexamples/s]', '\\rMap:  57%|#####7    | 4231000/7400423 [07:02<05:46, 9146.07\nexamples/s]', '\\rMap:  57%|#####7    | 4233000/7400423 [07:03<05:29, 9608.07\nexamples/s]', '\\rMap:  57%|#####7    | 4235000/7400423 [07:03<05:28, 9632.71\nexamples/s]', '\\rMap:  57%|#####7    | 4236000/7400423 [07:03<05:28, 9634.05\nexamples/s]', '\\rMap:  57%|#####7    | 4238000/7400423 [07:03<05:23, 9782.18\nexamples/s]', '\\rMap:  57%|#####7    | 4239000/7400423 [07:03<05:24, 9739.00\nexamples/s]', '\\rMap:  57%|#####7    | 4241000/7400423 [07:03<05:16, 9991.68\nexamples/s]', '\\rMap:  57%|#####7    | 4243000/7400423 [07:04<05:09, 10203.06\nexamples/s]', '\\rMap:  57%|#####7    | 4245000/7400423 [07:04<05:04, 10371.25\nexamples/s]', '\\rMap:  57%|#####7    | 4247000/7400423 [07:04<04:58, 10563.62\nexamples/s]', '\\rMap:  57%|#####7    | 4249000/7400423 [07:04<05:04, 10335.40\nexamples/s]', '\\rMap:  57%|#####7    | 4251000/7400423 [07:04<05:05, 10292.27\nexamples/s]', '\\rMap:  57%|#####7    | 4253000/7400423 [07:05<05:05, 10297.80\nexamples/s]', '\\rMap:  57%|#####7    | 4255000/7400423 [07:05<05:18, 9880.43\nexamples/s] ', '\\rMap:  58%|#####7    | 4256000/7400423 [07:05<05:18, 9864.25\nexamples/s]', '\\rMap:  58%|#####7    | 4257000/7400423 [07:05<05:23, 9713.85\nexamples/s]', '\\rMap:  58%|#####7    | 4258000/7400423 [07:05<05:24, 9676.36\nexamples/s]', '\\rMap:  58%|#####7    | 4260000/7400423 [07:05<05:23, 9718.89\nexamples/s]', '\\rMap:  58%|#####7    | 4262000/7400423 [07:06<05:16, 9922.09\nexamples/s]', '\\rMap:  58%|#####7    | 4264000/7400423 [07:06<05:13, 10008.08\nexamples/s]', '\\rMap:  58%|#####7    | 4265000/7400423 [07:06<05:17, 9888.91\nexamples/s] ', '\\rMap:  58%|#####7    | 4266000/7400423 [07:06<05:23, 9693.01\nexamples/s]', '\\rMap:  58%|#####7    | 4267000/7400423 [07:06<05:26, 9586.13\nexamples/s]', '\\rMap:  58%|#####7    | 4268000/7400423 [07:06<05:26, 9602.12\nexamples/s]', '\\rMap:  58%|#####7    | 4270000/7400423 [07:06<05:14, 9940.91\nexamples/s]', '\\rMap:  58%|#####7    | 4272000/7400423 [07:07<05:08, 10155.72\nexamples/s]', '\\rMap:  58%|#####7    | 4274000/7400423 [07:07<05:09, 10092.50\nexamples/s]', '\\rMap:  58%|#####7    | 4276000/7400423 [07:07<05:02, 10313.01\nexamples/s]', '\\rMap:  58%|#####7    | 4278000/7400423 [07:07<05:01, 10365.00\nexamples/s]', '\\rMap:  58%|#####7    | 4280000/7400423 [07:07<04:52, 10678.33\nexamples/s]', '\\rMap:  58%|#####7    | 4282000/7400423 [07:07<04:47, 10856.53\nexamples/s]', '\\rMap:  58%|#####7    | 4284000/7400423 [07:08<04:45, 10897.98\nexamples/s]', '\\rMap:  58%|#####7    | 4286000/7400423 [07:08<04:44, 10934.81\nexamples/s]', '\\rMap:  58%|#####7    | 4288000/7400423 [07:08<04:42, 11004.93\nexamples/s]', '\\rMap:  58%|#####7    | 4290000/7400423 [07:08<04:43, 10971.17\nexamples/s]', '\\rMap:  58%|#####7    | 4292000/7400423 [07:08<04:39, 11122.81\nexamples/s]', '\\rMap:  58%|#####8    | 4294000/7400423 [07:09<04:38, 11158.18\nexamples/s]', '\\rMap:  58%|#####8    | 4296000/7400423 [07:09<04:36, 11225.58\nexamples/s]', '\\rMap:  58%|#####8    | 4298000/7400423 [07:09<04:33, 11328.54\nexamples/s]', '\\rMap:  58%|#####8    | 4300000/7400423 [07:09<04:32, 11378.12\nexamples/s]', '\\rMap:  58%|#####8    | 4302000/7400423 [07:09<04:37, 11184.84\nexamples/s]', '\\rMap:  58%|#####8    | 4304000/7400423 [07:09<04:41, 11009.15\nexamples/s]', '\\rMap:  58%|#####8    | 4306000/7400423 [07:10<04:43, 10899.39\nexamples/s]', '\\rMap:  58%|#####8    | 4308000/7400423 [07:10<04:43, 10898.88\nexamples/s]', '\\rMap:  58%|#####8    | 4310000/7400423 [07:10<04:40, 11018.11\nexamples/s]', '\\rMap:  58%|#####8    | 4312000/7400423 [07:10<04:39, 11048.08\nexamples/s]', '\\rMap:  58%|#####8    | 4314000/7400423 [07:10<04:41, 10977.79\nexamples/s]', '\\rMap:  58%|#####8    | 4316000/7400423 [07:11<04:36, 11161.82\nexamples/s]', '\\rMap:  58%|#####8    | 4318000/7400423 [07:11<04:40, 10979.44\nexamples/s]', '\\rMap:  58%|#####8    | 4320000/7400423 [07:11<04:37, 11094.55\nexamples/s]', '\\rMap:  58%|#####8    | 4322000/7400423 [07:11<04:34, 11207.51\nexamples/s]', '\\rMap:  58%|#####8    | 4324000/7400423 [07:11<06:06, 8387.98\nexamples/s] ', '\\rMap:  58%|#####8    | 4326000/7400423 [07:12<05:37, 9117.69\nexamples/s]', '\\rMap:  58%|#####8    | 4328000/7400423 [07:12<05:24, 9459.00\nexamples/s]', '\\rMap:  59%|#####8    | 4330000/7400423 [07:12<05:18, 9644.08\nexamples/s]', '\\rMap:  59%|#####8    | 4332000/7400423 [07:12<05:11, 9864.48\nexamples/s]', '\\rMap:  59%|#####8    | 4334000/7400423 [07:12<05:06, 10001.40\nexamples/s]', '\\rMap:  59%|#####8    | 4336000/7400423 [07:13<05:00, 10190.07\nexamples/s]', '\\rMap:  59%|#####8    | 4338000/7400423 [07:13<04:51, 10509.61\nexamples/s]', '\\rMap:  59%|#####8    | 4340000/7400423 [07:13<04:49, 10561.50\nexamples/s]', '\\rMap:  59%|#####8    | 4342000/7400423 [07:13<04:45, 10713.42\nexamples/s]', '\\rMap:  59%|#####8    | 4344000/7400423 [07:13<04:42, 10818.06\nexamples/s]', '\\rMap:  59%|#####8    | 4346000/7400423 [07:13<04:41, 10840.61\nexamples/s]', '\\rMap:  59%|#####8    | 4348000/7400423 [07:14<04:38, 10949.76\nexamples/s]', '\\rMap:  59%|#####8    | 4350000/7400423 [07:14<04:40, 10861.42\nexamples/s]', '\\rMap:  59%|#####8    | 4352000/7400423 [07:14<04:46, 10637.97\nexamples/s]', '\\rMap:  59%|#####8    | 4354000/7400423 [07:14<04:42, 10785.67\nexamples/s]', '\\rMap:  59%|#####8    | 4356000/7400423 [07:14<04:51, 10459.97\nexamples/s]', '\\rMap:  59%|#####8    | 4358000/7400423 [07:15<05:04, 9995.81\nexamples/s] ', '\\rMap:  59%|#####8    | 4360000/7400423 [07:15<05:04, 9991.06\nexamples/s]', '\\rMap:  59%|#####8    | 4362000/7400423 [07:15<05:02, 10044.68\nexamples/s]', '\\rMap:  59%|#####8    | 4364000/7400423 [07:15<05:01, 10077.72\nexamples/s]', '\\rMap:  59%|#####8    | 4366000/7400423 [07:15<05:00, 10099.25\nexamples/s]', '\\rMap:  59%|#####9    | 4368000/7400423 [07:16<05:15, 9616.38\nexamples/s] ', '\\rMap:  59%|#####9    | 4369000/7400423 [07:16<05:17, 9540.12\nexamples/s]', '\\rMap:  59%|#####9    | 4370000/7400423 [07:16<05:22, 9396.21\nexamples/s]', '\\rMap:  59%|#####9    | 4371000/7400423 [07:16<05:28, 9225.10\nexamples/s]', '\\rMap:  59%|#####9    | 4372000/7400423 [07:16<05:29, 9187.79\nexamples/s]', '\\rMap:  59%|#####9    | 4373000/7400423 [07:16<05:33, 9089.97\nexamples/s]', '\\rMap:  59%|#####9    | 4374000/7400423 [07:16<05:38, 8944.06\nexamples/s]', '\\rMap:  59%|#####9    | 4375000/7400423 [07:16<05:42, 8839.71\nexamples/s]', '\\rMap:  59%|#####9    | 4376000/7400423 [07:17<05:45, 8743.61\nexamples/s]', '\\rMap:  59%|#####9    | 4377000/7400423 [07:17<05:34, 9027.41\nexamples/s]', '\\rMap:  59%|#####9    | 4378000/7400423 [07:17<05:28, 9210.40\nexamples/s]', '\\rMap:  59%|#####9    | 4380000/7400423 [07:17<05:24, 9303.14\nexamples/s]', '\\rMap:  59%|#####9    | 4381000/7400423 [07:17<05:32, 9067.80\nexamples/s]', '\\rMap:  59%|#####9    | 4382000/7400423 [07:17<05:38, 8906.52\nexamples/s]', '\\rMap:  59%|#####9    | 4383000/7400423 [07:17<05:46, 8698.71\nexamples/s]', '\\rMap:  59%|#####9    | 4384000/7400423 [07:17<05:52, 8546.31\nexamples/s]', '\\rMap:  59%|#####9    | 4385000/7400423 [07:18<05:47, 8675.47\nexamples/s]', '\\rMap:  59%|#####9    | 4386000/7400423 [07:18<05:45, 8731.58\nexamples/s]', '\\rMap:  59%|#####9    | 4387000/7400423 [07:18<05:42, 8786.79\nexamples/s]', '\\rMap:  59%|#####9    | 4388000/7400423 [07:18<05:41, 8831.71\nexamples/s]', '\\rMap:  59%|#####9    | 4389000/7400423 [07:18<05:45, 8717.43\nexamples/s]', '\\rMap:  59%|#####9    | 4390000/7400423 [07:18<05:37, 8923.76\nexamples/s]', '\\rMap:  59%|#####9    | 4392000/7400423 [07:18<05:10, 9693.13\nexamples/s]', '\\rMap:  59%|#####9    | 4394000/7400423 [07:19<05:05, 9848.45\nexamples/s]', '\\rMap:  59%|#####9    | 4396000/7400423 [07:19<05:06, 9791.27\nexamples/s]', '\\rMap:  59%|#####9    | 4398000/7400423 [07:19<04:58, 10044.88\nexamples/s]', '\\rMap:  59%|#####9    | 4400000/7400423 [07:19<04:54, 10182.19\nexamples/s]', '\\rMap:  59%|#####9    | 4402000/7400423 [07:19<04:54, 10183.48\nexamples/s]', '\\rMap:  60%|#####9    | 4404000/7400423 [07:19<04:47, 10414.87\nexamples/s]', '\\rMap:  60%|#####9    | 4406000/7400423 [07:20<04:51, 10277.37\nexamples/s]', '\\rMap:  60%|#####9    | 4408000/7400423 [07:20<04:56, 10089.24\nexamples/s]', '\\rMap:  60%|#####9    | 4410000/7400423 [07:20<04:59, 9979.46\nexamples/s] ', '\\rMap:  60%|#####9    | 4411000/7400423 [07:20<06:46, 7362.36\nexamples/s]', '\\rMap:  60%|#####9    | 4412000/7400423 [07:21<06:25, 7755.21\nexamples/s]', '\\rMap:  60%|#####9    | 4413000/7400423 [07:21<06:09, 8076.38\nexamples/s]', '\\rMap:  60%|#####9    | 4414000/7400423 [07:21<05:56, 8366.46\nexamples/s]', '\\rMap:  60%|#####9    | 4415000/7400423 [07:21<05:52, 8475.79\nexamples/s]', '\\rMap:  60%|#####9    | 4416000/7400423 [07:21<05:42, 8706.85\nexamples/s]', '\\rMap:  60%|#####9    | 4417000/7400423 [07:21<05:32, 8967.79\nexamples/s]', '\\rMap:  60%|#####9    | 4419000/7400423 [07:21<05:16, 9421.36\nexamples/s]', '\\rMap:  60%|#####9    | 4421000/7400423 [07:21<05:07, 9686.05\nexamples/s]', '\\rMap:  60%|#####9    | 4422000/7400423 [07:22<05:07, 9682.37\nexamples/s]', '\\rMap:  60%|#####9    | 4424000/7400423 [07:22<05:01, 9879.75\nexamples/s]', '\\rMap:  60%|#####9    | 4426000/7400423 [07:22<05:03, 9805.69\nexamples/s]', '\\rMap:  60%|#####9    | 4428000/7400423 [07:22<05:01, 9862.18\nexamples/s]', '\\rMap:  60%|#####9    | 4430000/7400423 [07:22<04:57, 9985.34\nexamples/s]', '\\rMap:  60%|#####9    | 4431000/7400423 [07:22<05:02, 9824.97\nexamples/s]', '\\rMap:  60%|#####9    | 4433000/7400423 [07:23<04:57, 9967.47\nexamples/s]', '\\rMap:  60%|#####9    | 4434000/7400423 [07:23<04:57, 9964.77\nexamples/s]', '\\rMap:  60%|#####9    | 4435000/7400423 [07:23<05:01, 9838.35\nexamples/s]', '\\rMap:  60%|#####9    | 4437000/7400423 [07:23<04:56, 9981.82\nexamples/s]', '\\rMap:  60%|#####9    | 4438000/7400423 [07:23<04:59, 9882.88\nexamples/s]', '\\rMap:  60%|#####9    | 4439000/7400423 [07:23<04:59, 9888.44\nexamples/s]', '\\rMap:  60%|#####9    | 4440000/7400423 [07:23<05:07, 9612.62\nexamples/s]', '\\rMap:  60%|######    | 4441000/7400423 [07:23<05:21, 9213.27\nexamples/s]', '\\rMap:  60%|######    | 4443000/7400423 [07:24<05:12, 9473.96\nexamples/s]', '\\rMap:  60%|######    | 4444000/7400423 [07:24<05:22, 9166.19\nexamples/s]', '\\rMap:  60%|######    | 4445000/7400423 [07:24<05:31, 8909.03\nexamples/s]', '\\rMap:  60%|######    | 4446000/7400423 [07:24<05:29, 8969.55\nexamples/s]', '\\rMap:  60%|######    | 4447000/7400423 [07:24<05:33, 8850.26\nexamples/s]', '\\rMap:  60%|######    | 4448000/7400423 [07:24<05:29, 8956.32\nexamples/s]', '\\rMap:  60%|######    | 4449000/7400423 [07:24<05:22, 9156.59\nexamples/s]', '\\rMap:  60%|######    | 4450000/7400423 [07:24<05:18, 9259.78\nexamples/s]', '\\rMap:  60%|######    | 4452000/7400423 [07:25<05:08, 9545.35\nexamples/s]', '\\rMap:  60%|######    | 4453000/7400423 [07:25<05:14, 9367.34\nexamples/s]', '\\rMap:  60%|######    | 4455000/7400423 [07:25<05:01, 9755.07\nexamples/s]', '\\rMap:  60%|######    | 4456000/7400423 [07:25<05:02, 9739.28\nexamples/s]', '\\rMap:  60%|######    | 4457000/7400423 [07:25<05:14, 9346.14\nexamples/s]', '\\rMap:  60%|######    | 4458000/7400423 [07:25<05:21, 9144.26\nexamples/s]', '\\rMap:  60%|######    | 4459000/7400423 [07:25<05:21, 9153.45\nexamples/s]', '\\rMap:  60%|######    | 4460000/7400423 [07:26<05:22, 9125.42\nexamples/s]', '\\rMap:  60%|######    | 4461000/7400423 [07:26<05:18, 9224.23\nexamples/s]', '\\rMap:  60%|######    | 4462000/7400423 [07:26<05:17, 9266.77\nexamples/s]', '\\rMap:  60%|######    | 4463000/7400423 [07:26<05:20, 9165.43\nexamples/s]', '\\rMap:  60%|######    | 4464000/7400423 [07:26<05:21, 9119.94\nexamples/s]', '\\rMap:  60%|######    | 4465000/7400423 [07:26<05:28, 8923.31\nexamples/s]', '\\rMap:  60%|######    | 4466000/7400423 [07:26<05:24, 9035.09\nexamples/s]', '\\rMap:  60%|######    | 4468000/7400423 [07:26<05:11, 9412.43\nexamples/s]', '\\rMap:  60%|######    | 4469000/7400423 [07:27<05:13, 9342.85\nexamples/s]', '\\rMap:  60%|######    | 4470000/7400423 [07:27<05:13, 9343.49\nexamples/s]', '\\rMap:  60%|######    | 4471000/7400423 [07:27<05:08, 9504.26\nexamples/s]', '\\rMap:  60%|######    | 4473000/7400423 [07:27<04:56, 9865.00\nexamples/s]', '\\rMap:  60%|######    | 4474000/7400423 [07:27<05:01, 9705.35\nexamples/s]', '\\rMap:  60%|######    | 4475000/7400423 [07:27<05:03, 9649.37\nexamples/s]', '\\rMap:  60%|######    | 4476000/7400423 [07:27<05:13, 9325.13\nexamples/s]', '\\rMap:  60%|######    | 4477000/7400423 [07:27<05:18, 9193.05\nexamples/s]', '\\rMap:  61%|######    | 4478000/7400423 [07:27<05:15, 9274.14\nexamples/s]', '\\rMap:  61%|######    | 4479000/7400423 [07:28<05:18, 9181.04\nexamples/s]', '\\rMap:  61%|######    | 4480000/7400423 [07:28<05:13, 9313.39\nexamples/s]', '\\rMap:  61%|######    | 4481000/7400423 [07:28<05:17, 9205.49\nexamples/s]', '\\rMap:  61%|######    | 4482000/7400423 [07:28<05:15, 9241.67\nexamples/s]', '\\rMap:  61%|######    | 4483000/7400423 [07:28<05:10, 9392.63\nexamples/s]', '\\rMap:  61%|######    | 4484000/7400423 [07:28<05:16, 9222.65\nexamples/s]', '\\rMap:  61%|######    | 4485000/7400423 [07:28<05:27, 8900.64\nexamples/s]', '\\rMap:  61%|######    | 4486000/7400423 [07:28<05:32, 8772.97\nexamples/s]', '\\rMap:  61%|######    | 4487000/7400423 [07:29<08:22, 5793.31\nexamples/s]', '\\rMap:  61%|######    | 4488000/7400423 [07:29<07:26, 6520.42\nexamples/s]', '\\rMap:  61%|######    | 4489000/7400423 [07:29<06:46, 7156.86\nexamples/s]', '\\rMap:  61%|######    | 4490000/7400423 [07:29<06:16, 7728.65\nexamples/s]', '\\rMap:  61%|######    | 4491000/7400423 [07:29<05:53, 8226.07\nexamples/s]', '\\rMap:  61%|######    | 4493000/7400423 [07:29<05:23, 8984.57\nexamples/s]', '\\rMap:  61%|######    | 4494000/7400423 [07:29<05:22, 9025.44\nexamples/s]', '\\rMap:  61%|######    | 4495000/7400423 [07:30<05:21, 9032.54\nexamples/s]', '\\rMap:  61%|######    | 4496000/7400423 [07:30<05:18, 9117.70\nexamples/s]', '\\rMap:  61%|######    | 4497000/7400423 [07:30<05:15, 9204.38\nexamples/s]', '\\rMap:  61%|######    | 4498000/7400423 [07:30<05:10, 9343.54\nexamples/s]', '\\rMap:  61%|######    | 4500000/7400423 [07:30<05:02, 9594.20\nexamples/s]', '\\rMap:  61%|######    | 4501000/7400423 [07:30<04:59, 9674.30\nexamples/s]', '\\rMap:  61%|######    | 4502000/7400423 [07:30<05:02, 9589.71\nexamples/s]', '\\rMap:  61%|######    | 4503000/7400423 [07:30<05:01, 9610.29\nexamples/s]', '\\rMap:  61%|######    | 4505000/7400423 [07:31<04:51, 9934.46\nexamples/s]', '\\rMap:  61%|######    | 4506000/7400423 [07:31<05:02, 9563.23\nexamples/s]', '\\rMap:  61%|######    | 4507000/7400423 [07:31<05:17, 9124.19\nexamples/s]', '\\rMap:  61%|######    | 4508000/7400423 [07:31<05:25, 8889.23\nexamples/s]', '\\rMap:  61%|######    | 4509000/7400423 [07:31<05:37, 8558.52\nexamples/s]', '\\rMap:  61%|######    | 4510000/7400423 [07:31<05:45, 8376.74\nexamples/s]', '\\rMap:  61%|######    | 4511000/7400423 [07:31<05:46, 8329.32\nexamples/s]', '\\rMap:  61%|######    | 4512000/7400423 [07:31<05:57, 8089.07\nexamples/s]', '\\rMap:  61%|######    | 4513000/7400423 [07:32<06:03, 7944.44\nexamples/s]', '\\rMap:  61%|######    | 4514000/7400423 [07:32<06:02, 7966.00\nexamples/s]', '\\rMap:  61%|######1   | 4515000/7400423 [07:32<06:18, 7614.28\nexamples/s]', '\\rMap:  61%|######1   | 4516000/7400423 [07:32<06:16, 7662.85\nexamples/s]', '\\rMap:  61%|######1   | 4517000/7400423 [07:32<05:59, 8011.37\nexamples/s]', '\\rMap:  61%|######1   | 4518000/7400423 [07:32<05:50, 8225.20\nexamples/s]', '\\rMap:  61%|######1   | 4519000/7400423 [07:32<05:41, 8447.36\nexamples/s]', '\\rMap:  61%|######1   | 4520000/7400423 [07:32<05:35, 8586.62\nexamples/s]', '\\rMap:  61%|######1   | 4521000/7400423 [07:32<05:31, 8684.65\nexamples/s]', '\\rMap:  61%|######1   | 4522000/7400423 [07:33<05:32, 8669.22\nexamples/s]', '\\rMap:  61%|######1   | 4523000/7400423 [07:33<05:28, 8758.00\nexamples/s]', '\\rMap:  61%|######1   | 4524000/7400423 [07:33<05:35, 8582.79\nexamples/s]', '\\rMap:  61%|######1   | 4525000/7400423 [07:33<05:32, 8658.16\nexamples/s]', '\\rMap:  61%|######1   | 4526000/7400423 [07:33<05:33, 8623.82\nexamples/s]', '\\rMap:  61%|######1   | 4527000/7400423 [07:33<05:19, 8982.85\nexamples/s]', '\\rMap:  61%|######1   | 4528000/7400423 [07:33<05:19, 8987.69\nexamples/s]', '\\rMap:  61%|######1   | 4530000/7400423 [07:33<04:58, 9603.32\nexamples/s]', '\\rMap:  61%|######1   | 4532000/7400423 [07:34<04:44, 10088.74\nexamples/s]', '\\rMap:  61%|######1   | 4533000/7400423 [07:34<04:48, 9943.76\nexamples/s] ', '\\rMap:  61%|######1   | 4535000/7400423 [07:34<04:42, 10137.74\nexamples/s]', '\\rMap:  61%|######1   | 4536000/7400423 [07:34<04:47, 9962.25\nexamples/s] ', '\\rMap:  61%|######1   | 4537000/7400423 [07:34<04:52, 9800.30\nexamples/s]', '\\rMap:  61%|######1   | 4538000/7400423 [07:34<04:59, 9549.08\nexamples/s]', '\\rMap:  61%|######1   | 4539000/7400423 [07:34<05:03, 9417.31\nexamples/s]', '\\rMap:  61%|######1   | 4540000/7400423 [07:34<05:04, 9384.34\nexamples/s]', '\\rMap:  61%|######1   | 4541000/7400423 [07:35<05:03, 9407.73\nexamples/s]', '\\rMap:  61%|######1   | 4542000/7400423 [07:35<05:04, 9396.30\nexamples/s]', '\\rMap:  61%|######1   | 4543000/7400423 [07:35<05:01, 9492.61\nexamples/s]', '\\rMap:  61%|######1   | 4544000/7400423 [07:35<04:58, 9572.69\nexamples/s]', '\\rMap:  61%|######1   | 4545000/7400423 [07:35<04:55, 9654.59\nexamples/s]', '\\rMap:  61%|######1   | 4546000/7400423 [07:35<04:54, 9696.00\nexamples/s]', '\\rMap:  61%|######1   | 4547000/7400423 [07:35<04:54, 9686.51\nexamples/s]', '\\rMap:  61%|######1   | 4548000/7400423 [07:35<04:55, 9640.36\nexamples/s]', '\\rMap:  61%|######1   | 4550000/7400423 [07:36<04:50, 9796.53\nexamples/s]', '\\rMap:  61%|######1   | 4551000/7400423 [07:36<04:51, 9775.89\nexamples/s]', '\\rMap:  62%|######1   | 4552000/7400423 [07:36<04:49, 9823.31\nexamples/s]', '\\rMap:  62%|######1   | 4553000/7400423 [07:36<04:50, 9817.08\nexamples/s]', '\\rMap:  62%|######1   | 4555000/7400423 [07:36<04:38, 10209.15\nexamples/s]', '\\rMap:  62%|######1   | 4557000/7400423 [07:36<04:50, 9784.42\nexamples/s] ', '\\rMap:  62%|######1   | 4558000/7400423 [07:36<04:58, 9522.77\nexamples/s]', '\\rMap:  62%|######1   | 4559000/7400423 [07:36<05:08, 9199.24\nexamples/s]', '\\rMap:  62%|######1   | 4560000/7400423 [07:37<05:13, 9069.99\nexamples/s]', '\\rMap:  62%|######1   | 4561000/7400423 [07:37<05:12, 9095.55\nexamples/s]', '\\rMap:  62%|######1   | 4562000/7400423 [07:37<05:16, 8974.41\nexamples/s]', '\\rMap:  62%|######1   | 4563000/7400423 [07:37<05:13, 9046.24\nexamples/s]', '\\rMap:  62%|######1   | 4564000/7400423 [07:37<05:10, 9126.56\nexamples/s]', '\\rMap:  62%|######1   | 4565000/7400423 [07:37<05:10, 9129.03\nexamples/s]', '\\rMap:  62%|######1   | 4566000/7400423 [07:37<05:18, 8889.54\nexamples/s]', '\\rMap:  62%|######1   | 4567000/7400423 [07:37<05:17, 8914.36\nexamples/s]', '\\rMap:  62%|######1   | 4568000/7400423 [07:37<05:29, 8583.60\nexamples/s]', '\\rMap:  62%|######1   | 4569000/7400423 [07:38<05:43, 8247.24\nexamples/s]', '\\rMap:  62%|######1   | 4570000/7400423 [07:38<05:42, 8258.56\nexamples/s]', '\\rMap:  62%|######1   | 4571000/7400423 [07:38<05:35, 8428.41\nexamples/s]', '\\rMap:  62%|######1   | 4572000/7400423 [07:38<05:39, 8320.97\nexamples/s]', '\\rMap:  62%|######1   | 4573000/7400423 [07:38<05:37, 8371.69\nexamples/s]', '\\rMap:  62%|######1   | 4574000/7400423 [07:38<05:40, 8299.11\nexamples/s]', '\\rMap:  62%|######1   | 4575000/7400423 [07:38<05:42, 8255.26\nexamples/s]', '\\rMap:  62%|######1   | 4576000/7400423 [07:38<05:40, 8296.58\nexamples/s]', '\\rMap:  62%|######1   | 4577000/7400423 [07:39<05:53, 7996.20\nexamples/s]', '\\rMap:  62%|######1   | 4578000/7400423 [07:39<05:59, 7842.66\nexamples/s]', '\\rMap:  62%|######1   | 4579000/7400423 [07:39<06:00, 7819.68\nexamples/s]', '\\rMap:  62%|######1   | 4580000/7400423 [07:39<05:54, 7961.65\nexamples/s]', '\\rMap:  62%|######1   | 4581000/7400423 [07:39<05:46, 8147.60\nexamples/s]', '\\rMap:  62%|######1   | 4582000/7400423 [07:39<05:41, 8242.03\nexamples/s]', '\\rMap:  62%|######1   | 4583000/7400423 [07:39<05:46, 8120.04\nexamples/s]', '\\rMap:  62%|######1   | 4584000/7400423 [07:39<05:35, 8396.10\nexamples/s]', '\\rMap:  62%|######1   | 4585000/7400423 [07:40<05:40, 8258.72\nexamples/s]', '\\rMap:  62%|######1   | 4586000/7400423 [07:40<05:40, 8275.09\nexamples/s]', '\\rMap:  62%|######1   | 4587000/7400423 [07:40<05:34, 8398.69\nexamples/s]', '\\rMap:  62%|######1   | 4588000/7400423 [07:40<05:27, 8583.32\nexamples/s]', '\\rMap:  62%|######2   | 4589000/7400423 [07:40<05:29, 8524.65\nexamples/s]', '\\rMap:  62%|######2   | 4590000/7400423 [07:40<05:28, 8542.72\nexamples/s]', '\\rMap:  62%|######2   | 4591000/7400423 [07:40<05:15, 8907.21\nexamples/s]', '\\rMap:  62%|######2   | 4592000/7400423 [07:40<05:06, 9167.40\nexamples/s]', '\\rMap:  62%|######2   | 4593000/7400423 [07:40<05:03, 9235.04\nexamples/s]', '\\rMap:  62%|######2   | 4595000/7400423 [07:41<04:58, 9400.92\nexamples/s]', '\\rMap:  62%|######2   | 4597000/7400423 [07:41<04:47, 9763.21\nexamples/s]', '\\rMap:  62%|######2   | 4598000/7400423 [07:41<04:52, 9565.05\nexamples/s]', '\\rMap:  62%|######2   | 4599000/7400423 [07:41<05:00, 9324.23\nexamples/s]', '\\rMap:  62%|######2   | 4600000/7400423 [07:41<05:03, 9230.12\nexamples/s]', '\\rMap:  62%|######2   | 4601000/7400423 [07:41<05:02, 9255.71\nexamples/s]', '\\rMap:  62%|######2   | 4602000/7400423 [07:41<05:01, 9296.14\nexamples/s]', '\\rMap:  62%|######2   | 4603000/7400423 [07:42<05:04, 9195.23\nexamples/s]', '\\rMap:  62%|######2   | 4604000/7400423 [07:42<05:04, 9181.96\nexamples/s]', '\\rMap:  62%|######2   | 4605000/7400423 [07:42<05:10, 8996.77\nexamples/s]', '\\rMap:  62%|######2   | 4606000/7400423 [07:42<05:08, 9056.08\nexamples/s]', '\\rMap:  62%|######2   | 4607000/7400423 [07:42<05:04, 9158.88\nexamples/s]', '\\rMap:  62%|######2   | 4608000/7400423 [07:42<05:03, 9209.60\nexamples/s]', '\\rMap:  62%|######2   | 4609000/7400423 [07:42<05:01, 9258.59\nexamples/s]', '\\rMap:  62%|######2   | 4610000/7400423 [07:42<05:00, 9272.56\nexamples/s]', '\\rMap:  62%|######2   | 4611000/7400423 [07:42<04:55, 9449.84\nexamples/s]', '\\rMap:  62%|######2   | 4612000/7400423 [07:43<04:53, 9502.14\nexamples/s]', '\\rMap:  62%|######2   | 4613000/7400423 [07:43<08:06, 5723.92\nexamples/s]', '\\rMap:  62%|######2   | 4615000/7400423 [07:43<06:28, 7177.42\nexamples/s]', '\\rMap:  62%|######2   | 4616000/7400423 [07:43<06:01, 7701.24\nexamples/s]', '\\rMap:  62%|######2   | 4618000/7400423 [07:43<05:26, 8525.13\nexamples/s]', '\\rMap:  62%|######2   | 4619000/7400423 [07:43<05:18, 8740.28\nexamples/s]', '\\rMap:  62%|######2   | 4621000/7400423 [07:44<04:57, 9346.14\nexamples/s]', '\\rMap:  62%|######2   | 4623000/7400423 [07:44<04:44, 9762.75\nexamples/s]', '\\rMap:  62%|######2   | 4625000/7400423 [07:44<04:44, 9760.74\nexamples/s]', '\\rMap:  63%|######2   | 4627000/7400423 [07:44<04:34, 10089.09\nexamples/s]', '\\rMap:  63%|######2   | 4629000/7400423 [07:44<04:31, 10204.76\nexamples/s]', '\\rMap:  63%|######2   | 4631000/7400423 [07:45<04:30, 10225.82\nexamples/s]', '\\rMap:  63%|######2   | 4633000/7400423 [07:45<04:28, 10313.21\nexamples/s]', '\\rMap:  63%|######2   | 4635000/7400423 [07:45<04:27, 10356.67\nexamples/s]', '\\rMap:  63%|######2   | 4637000/7400423 [07:45<04:29, 10248.18\nexamples/s]', '\\rMap:  63%|######2   | 4639000/7400423 [07:45<04:27, 10316.62\nexamples/s]', '\\rMap:  63%|######2   | 4641000/7400423 [07:46<04:25, 10390.79\nexamples/s]', '\\rMap:  63%|######2   | 4643000/7400423 [07:46<04:22, 10510.43\nexamples/s]', '\\rMap:  63%|######2   | 4645000/7400423 [07:46<04:16, 10762.72\nexamples/s]', '\\rMap:  63%|######2   | 4647000/7400423 [07:46<04:13, 10847.27\nexamples/s]', '\\rMap:  63%|######2   | 4649000/7400423 [07:46<04:15, 10776.98\nexamples/s]', '\\rMap:  63%|######2   | 4651000/7400423 [07:46<04:17, 10657.96\nexamples/s]', '\\rMap:  63%|######2   | 4653000/7400423 [07:47<04:16, 10695.16\nexamples/s]', '\\rMap:  63%|######2   | 4655000/7400423 [07:47<04:17, 10654.89\nexamples/s]', '\\rMap:  63%|######2   | 4657000/7400423 [07:47<04:15, 10720.87\nexamples/s]', '\\rMap:  63%|######2   | 4659000/7400423 [07:47<04:15, 10714.07\nexamples/s]', '\\rMap:  63%|######2   | 4661000/7400423 [07:47<04:12, 10830.67\nexamples/s]', '\\rMap:  63%|######3   | 4663000/7400423 [07:48<04:11, 10867.83\nexamples/s]', '\\rMap:  63%|######3   | 4665000/7400423 [07:48<04:28, 10191.47\nexamples/s]', '\\rMap:  63%|######3   | 4667000/7400423 [07:48<04:25, 10313.66\nexamples/s]', '\\rMap:  63%|######3   | 4669000/7400423 [07:48<04:24, 10326.66\nexamples/s]', '\\rMap:  63%|######3   | 4671000/7400423 [07:48<04:21, 10422.40\nexamples/s]', '\\rMap:  63%|######3   | 4673000/7400423 [07:49<04:27, 10212.88\nexamples/s]', '\\rMap:  63%|######3   | 4675000/7400423 [07:49<04:27, 10189.46\nexamples/s]', '\\rMap:  63%|######3   | 4677000/7400423 [07:49<04:26, 10226.65\nexamples/s]', '\\rMap:  63%|######3   | 4679000/7400423 [07:49<04:22, 10359.48\nexamples/s]', '\\rMap:  63%|######3   | 4681000/7400423 [07:49<04:22, 10374.47\nexamples/s]', '\\rMap:  63%|######3   | 4683000/7400423 [07:50<04:21, 10374.66\nexamples/s]', '\\rMap:  63%|######3   | 4685000/7400423 [07:50<04:25, 10208.42\nexamples/s]', '\\rMap:  63%|######3   | 4687000/7400423 [07:50<04:25, 10203.05\nexamples/s]', '\\rMap:  63%|######3   | 4689000/7400423 [07:50<04:27, 10128.59\nexamples/s]', '\\rMap:  63%|######3   | 4691000/7400423 [07:50<04:19, 10443.52\nexamples/s]', '\\rMap:  63%|######3   | 4693000/7400423 [07:51<04:23, 10292.04\nexamples/s]', '\\rMap:  63%|######3   | 4695000/7400423 [07:51<04:33, 9907.45\nexamples/s] ', '\\rMap:  63%|######3   | 4696000/7400423 [07:51<04:34, 9836.82\nexamples/s]', '\\rMap:  63%|######3   | 4697000/7400423 [07:51<04:35, 9812.85\nexamples/s]', '\\rMap:  63%|######3   | 4699000/7400423 [07:51<04:34, 9824.83\nexamples/s]', '\\rMap:  64%|######3   | 4700000/7400423 [07:51<04:41, 9588.98\nexamples/s]', '\\rMap:  64%|######3   | 4701000/7400423 [07:51<04:39, 9648.92\nexamples/s]', '\\rMap:  64%|######3   | 4702000/7400423 [07:51<04:44, 9493.00\nexamples/s]', '\\rMap:  64%|######3   | 4704000/7400423 [07:52<04:42, 9558.44\nexamples/s]', '\\rMap:  64%|######3   | 4705000/7400423 [07:52<04:39, 9630.69\nexamples/s]', '\\rMap:  64%|######3   | 4706000/7400423 [07:52<04:37, 9707.48\nexamples/s]', '\\rMap:  64%|######3   | 4707000/7400423 [07:52<04:35, 9779.30\nexamples/s]', '\\rMap:  64%|######3   | 4708000/7400423 [07:52<04:38, 9671.50\nexamples/s]', '\\rMap:  64%|######3   | 4709000/7400423 [07:52<04:37, 9706.60\nexamples/s]', '\\rMap:  64%|######3   | 4711000/7400423 [07:52<04:30, 9935.94\nexamples/s]', '\\rMap:  64%|######3   | 4712000/7400423 [07:53<06:44, 6641.48\nexamples/s]', '\\rMap:  64%|######3   | 4714000/7400423 [07:53<05:42, 7843.29\nexamples/s]', '\\rMap:  64%|######3   | 4716000/7400423 [07:53<05:14, 8542.30\nexamples/s]', '\\rMap:  64%|######3   | 4718000/7400423 [07:53<04:53, 9153.73\nexamples/s]', '\\rMap:  64%|######3   | 4720000/7400423 [07:53<04:37, 9662.90\nexamples/s]', '\\rMap:  64%|######3   | 4722000/7400423 [07:54<04:26, 10033.44\nexamples/s]', '\\rMap:  64%|######3   | 4724000/7400423 [07:54<04:16, 10433.89\nexamples/s]', '\\rMap:  64%|######3   | 4726000/7400423 [07:54<04:08, 10744.02\nexamples/s]', '\\rMap:  64%|######3   | 4728000/7400423 [07:54<04:04, 10927.16\nexamples/s]', '\\rMap:  64%|######3   | 4730000/7400423 [07:54<04:00, 11100.24\nexamples/s]', '\\rMap:  64%|######3   | 4732000/7400423 [07:55<03:59, 11152.19\nexamples/s]', '\\rMap:  64%|######3   | 4734000/7400423 [07:55<04:02, 11009.60\nexamples/s]', '\\rMap:  64%|######3   | 4736000/7400423 [07:55<04:05, 10849.28\nexamples/s]', '\\rMap:  64%|######4   | 4738000/7400423 [07:55<04:07, 10742.40\nexamples/s]', '\\rMap:  64%|######4   | 4740000/7400423 [07:55<04:10, 10599.83\nexamples/s]', '\\rMap:  64%|######4   | 4742000/7400423 [07:55<04:07, 10739.64\nexamples/s]', '\\rMap:  64%|######4   | 4744000/7400423 [07:56<04:07, 10711.67\nexamples/s]', '\\rMap:  64%|######4   | 4746000/7400423 [07:56<04:06, 10761.56\nexamples/s]', '\\rMap:  64%|######4   | 4748000/7400423 [07:56<04:04, 10858.36\nexamples/s]', '\\rMap:  64%|######4   | 4750000/7400423 [07:56<03:57, 11171.07\nexamples/s]', '\\rMap:  64%|######4   | 4752000/7400423 [07:56<03:50, 11498.24\nexamples/s]', '\\rMap:  64%|######4   | 4754000/7400423 [07:57<03:45, 11718.69\nexamples/s]', '\\rMap:  64%|######4   | 4756000/7400423 [07:57<03:42, 11908.71\nexamples/s]', '\\rMap:  64%|######4   | 4758000/7400423 [07:57<03:41, 11951.00\nexamples/s]', '\\rMap:  64%|######4   | 4760000/7400423 [07:57<03:37, 12134.91\nexamples/s]', '\\rMap:  64%|######4   | 4762000/7400423 [07:57<03:34, 12316.38\nexamples/s]', '\\rMap:  64%|######4   | 4764000/7400423 [07:57<03:34, 12272.55\nexamples/s]', '\\rMap:  64%|######4   | 4766000/7400423 [07:57<03:34, 12281.22\nexamples/s]', '\\rMap:  64%|######4   | 4768000/7400423 [07:58<03:34, 12289.51\nexamples/s]', '\\rMap:  64%|######4   | 4770000/7400423 [07:58<03:34, 12238.29\nexamples/s]', '\\rMap:  64%|######4   | 4772000/7400423 [07:58<03:40, 11910.78\nexamples/s]', '\\rMap:  65%|######4   | 4774000/7400423 [07:58<03:44, 11716.47\nexamples/s]', '\\rMap:  65%|######4   | 4776000/7400423 [07:58<03:46, 11605.32\nexamples/s]', '\\rMap:  65%|######4   | 4778000/7400423 [07:59<03:43, 11726.31\nexamples/s]', '\\rMap:  65%|######4   | 4780000/7400423 [07:59<03:47, 11515.70\nexamples/s]', '\\rMap:  65%|######4   | 4782000/7400423 [07:59<03:48, 11451.42\nexamples/s]', '\\rMap:  65%|######4   | 4784000/7400423 [07:59<03:44, 11669.96\nexamples/s]', '\\rMap:  65%|######4   | 4786000/7400423 [07:59<03:41, 11786.46\nexamples/s]', '\\rMap:  65%|######4   | 4788000/7400423 [07:59<03:39, 11912.85\nexamples/s]', '\\rMap:  65%|######4   | 4790000/7400423 [08:00<03:41, 11810.51\nexamples/s]', '\\rMap:  65%|######4   | 4792000/7400423 [08:00<03:42, 11738.42\nexamples/s]', '\\rMap:  65%|######4   | 4794000/7400423 [08:00<03:43, 11679.81\nexamples/s]', '\\rMap:  65%|######4   | 4796000/7400423 [08:00<03:46, 11476.59\nexamples/s]', '\\rMap:  65%|######4   | 4798000/7400423 [08:00<03:49, 11360.34\nexamples/s]', '\\rMap:  65%|######4   | 4800000/7400423 [08:01<05:04, 8540.26\nexamples/s] ', '\\rMap:  65%|######4   | 4802000/7400423 [08:01<04:44, 9133.86\nexamples/s]', '\\rMap:  65%|######4   | 4803000/7400423 [08:01<04:41, 9217.92\nexamples/s]', '\\rMap:  65%|######4   | 4805000/7400423 [08:01<04:32, 9514.54\nexamples/s]', '\\rMap:  65%|######4   | 4807000/7400423 [08:01<04:28, 9669.73\nexamples/s]', '\\rMap:  65%|######4   | 4809000/7400423 [08:01<04:25, 9764.57\nexamples/s]', '\\rMap:  65%|######4   | 4810000/7400423 [08:02<04:24, 9786.10\nexamples/s]', '\\rMap:  65%|######5   | 4812000/7400423 [08:02<04:19, 9983.63\nexamples/s]', '\\rMap:  65%|######5   | 4814000/7400423 [08:02<04:11, 10302.88\nexamples/s]', '\\rMap:  65%|######5   | 4816000/7400423 [08:02<04:00, 10736.85\nexamples/s]', '\\rMap:  65%|######5   | 4818000/7400423 [08:02<03:53, 11048.47\nexamples/s]', '\\rMap:  65%|######5   | 4820000/7400423 [08:02<03:51, 11155.49\nexamples/s]', '\\rMap:  65%|######5   | 4822000/7400423 [08:03<03:47, 11332.20\nexamples/s]', '\\rMap:  65%|######5   | 4824000/7400423 [08:03<03:47, 11305.30\nexamples/s]', '\\rMap:  65%|######5   | 4826000/7400423 [08:03<03:42, 11552.54\nexamples/s]', '\\rMap:  65%|######5   | 4828000/7400423 [08:03<03:40, 11670.60\nexamples/s]', '\\rMap:  65%|######5   | 4830000/7400423 [08:03<03:38, 11781.55\nexamples/s]', '\\rMap:  65%|######5   | 4832000/7400423 [08:04<03:45, 11397.32\nexamples/s]', '\\rMap:  65%|######5   | 4834000/7400423 [08:04<03:55, 10914.74\nexamples/s]', '\\rMap:  65%|######5   | 4836000/7400423 [08:04<04:01, 10628.23\nexamples/s]', '\\rMap:  65%|######5   | 4838000/7400423 [08:04<04:07, 10347.68\nexamples/s]', '\\rMap:  65%|######5   | 4840000/7400423 [08:04<04:10, 10214.04\nexamples/s]', '\\rMap:  65%|######5   | 4842000/7400423 [08:05<04:15, 9996.15\nexamples/s] ', '\\rMap:  65%|######5   | 4844000/7400423 [08:05<04:15, 10017.72\nexamples/s]', '\\rMap:  65%|######5   | 4846000/7400423 [08:05<04:14, 10043.84\nexamples/s]', '\\rMap:  66%|######5   | 4848000/7400423 [08:05<04:17, 9922.77\nexamples/s] ', '\\rMap:  66%|######5   | 4849000/7400423 [08:05<04:17, 9902.89\nexamples/s]', '\\rMap:  66%|######5   | 4850000/7400423 [08:05<04:21, 9762.11\nexamples/s]', '\\rMap:  66%|######5   | 4851000/7400423 [08:05<04:21, 9765.46\nexamples/s]', '\\rMap:  66%|######5   | 4853000/7400423 [08:06<04:16, 9937.97\nexamples/s]', '\\rMap:  66%|######5   | 4854000/7400423 [08:06<04:17, 9906.73\nexamples/s]', '\\rMap:  66%|######5   | 4856000/7400423 [08:06<04:06, 10338.84\nexamples/s]', '\\rMap:  66%|######5   | 4858000/7400423 [08:06<03:56, 10751.06\nexamples/s]', '\\rMap:  66%|######5   | 4860000/7400423 [08:06<03:49, 11080.68\nexamples/s]', '\\rMap:  66%|######5   | 4862000/7400423 [08:06<03:42, 11398.79\nexamples/s]', '\\rMap:  66%|######5   | 4864000/7400423 [08:07<03:39, 11540.17\nexamples/s]', '\\rMap:  66%|######5   | 4866000/7400423 [08:07<03:39, 11550.15\nexamples/s]', '\\rMap:  66%|######5   | 4868000/7400423 [08:07<03:39, 11560.08\nexamples/s]', '\\rMap:  66%|######5   | 4870000/7400423 [08:07<03:35, 11751.29\nexamples/s]', '\\rMap:  66%|######5   | 4872000/7400423 [08:07<03:34, 11780.41\nexamples/s]', '\\rMap:  66%|######5   | 4874000/7400423 [08:07<03:35, 11742.00\nexamples/s]', '\\rMap:  66%|######5   | 4876000/7400423 [08:08<04:49, 8715.94\nexamples/s] ', '\\rMap:  66%|######5   | 4878000/7400423 [08:08<04:29, 9350.93\nexamples/s]', '\\rMap:  66%|######5   | 4880000/7400423 [08:08<04:17, 9795.93\nexamples/s]', '\\rMap:  66%|######5   | 4882000/7400423 [08:08<04:05, 10249.22\nexamples/s]', '\\rMap:  66%|######5   | 4884000/7400423 [08:09<04:00, 10477.35\nexamples/s]', '\\rMap:  66%|######6   | 4886000/7400423 [08:09<03:54, 10727.57\nexamples/s]', '\\rMap:  66%|######6   | 4888000/7400423 [08:09<03:47, 11025.97\nexamples/s]', '\\rMap:  66%|######6   | 4890000/7400423 [08:09<03:44, 11159.23\nexamples/s]', '\\rMap:  66%|######6   | 4892000/7400423 [08:09<03:43, 11244.08\nexamples/s]', '\\rMap:  66%|######6   | 4894000/7400423 [08:09<03:44, 11178.00\nexamples/s]', '\\rMap:  66%|######6   | 4896000/7400423 [08:10<03:40, 11351.94\nexamples/s]', '\\rMap:  66%|######6   | 4898000/7400423 [08:10<03:38, 11443.93\nexamples/s]', '\\rMap:  66%|######6   | 4900000/7400423 [08:10<03:38, 11431.04\nexamples/s]', '\\rMap:  66%|######6   | 4902000/7400423 [08:10<03:37, 11492.60\nexamples/s]', '\\rMap:  66%|######6   | 4904000/7400423 [08:10<03:38, 11447.01\nexamples/s]', '\\rMap:  66%|######6   | 4906000/7400423 [08:10<03:51, 10781.78\nexamples/s]', '\\rMap:  66%|######6   | 4908000/7400423 [08:11<04:03, 10247.03\nexamples/s]', '\\rMap:  66%|######6   | 4910000/7400423 [08:11<04:09, 9988.63\nexamples/s] ', '\\rMap:  66%|######6   | 4912000/7400423 [08:11<04:18, 9643.03\nexamples/s]', '\\rMap:  66%|######6   | 4913000/7400423 [08:11<04:21, 9515.21\nexamples/s]', '\\rMap:  66%|######6   | 4914000/7400423 [08:11<04:26, 9336.29\nexamples/s]', '\\rMap:  66%|######6   | 4915000/7400423 [08:11<04:31, 9155.21\nexamples/s]', '\\rMap:  66%|######6   | 4916000/7400423 [08:12<04:31, 9154.88\nexamples/s]', '\\rMap:  66%|######6   | 4917000/7400423 [08:12<04:34, 9048.56\nexamples/s]', '\\rMap:  66%|######6   | 4919000/7400423 [08:12<04:21, 9499.34\nexamples/s]', '\\rMap:  66%|######6   | 4920000/7400423 [08:12<04:18, 9588.10\nexamples/s]', '\\rMap:  67%|######6   | 4922000/7400423 [08:12<04:11, 9843.22\nexamples/s]', '\\rMap:  67%|######6   | 4924000/7400423 [08:12<04:07, 10019.19\nexamples/s]', '\\rMap:  67%|######6   | 4925000/7400423 [08:12<04:08, 9973.35\nexamples/s] ', '\\rMap:  67%|######6   | 4927000/7400423 [08:13<04:02, 10179.85\nexamples/s]', '\\rMap:  67%|######6   | 4929000/7400423 [08:13<03:59, 10332.38\nexamples/s]', '\\rMap:  67%|######6   | 4931000/7400423 [08:13<03:57, 10404.36\nexamples/s]', '\\rMap:  67%|######6   | 4933000/7400423 [08:13<03:53, 10552.24\nexamples/s]', '\\rMap:  67%|######6   | 4935000/7400423 [08:13<03:53, 10569.59\nexamples/s]', '\\rMap:  67%|######6   | 4937000/7400423 [08:14<03:53, 10568.51\nexamples/s]', '\\rMap:  67%|######6   | 4939000/7400423 [08:14<03:52, 10600.65\nexamples/s]', '\\rMap:  67%|######6   | 4941000/7400423 [08:14<03:54, 10472.56\nexamples/s]', '\\rMap:  67%|######6   | 4943000/7400423 [08:14<03:55, 10441.70\nexamples/s]', '\\rMap:  67%|######6   | 4945000/7400423 [08:14<03:52, 10573.49\nexamples/s]', '\\rMap:  67%|######6   | 4947000/7400423 [08:15<03:57, 10314.61\nexamples/s]', '\\rMap:  67%|######6   | 4949000/7400423 [08:15<04:00, 10206.20\nexamples/s]', '\\rMap:  67%|######6   | 4951000/7400423 [08:15<04:01, 10145.01\nexamples/s]', '\\rMap:  67%|######6   | 4953000/7400423 [08:15<04:01, 10121.16\nexamples/s]', '\\rMap:  67%|######6   | 4955000/7400423 [08:15<04:05, 9947.14\nexamples/s] ', '\\rMap:  67%|######6   | 4957000/7400423 [08:16<04:04, 9983.33\nexamples/s]', '\\rMap:  67%|######7   | 4959000/7400423 [08:16<03:59, 10178.39\nexamples/s]', '\\rMap:  67%|######7   | 4961000/7400423 [08:16<03:55, 10345.13\nexamples/s]', '\\rMap:  67%|######7   | 4963000/7400423 [08:16<03:49, 10604.38\nexamples/s]', '\\rMap:  67%|######7   | 4965000/7400423 [08:16<03:41, 10990.17\nexamples/s]', '\\rMap:  67%|######7   | 4967000/7400423 [08:16<03:40, 11021.77\nexamples/s]', '\\rMap:  67%|######7   | 4969000/7400423 [08:17<03:39, 11055.75\nexamples/s]', '\\rMap:  67%|######7   | 4971000/7400423 [08:17<03:41, 10968.11\nexamples/s]', '\\rMap:  67%|######7   | 4973000/7400423 [08:17<03:42, 10928.87\nexamples/s]', '\\rMap:  67%|######7   | 4975000/7400423 [08:17<03:43, 10858.46\nexamples/s]', '\\rMap:  67%|######7   | 4977000/7400423 [08:17<03:42, 10882.88\nexamples/s]', '\\rMap:  67%|######7   | 4979000/7400423 [08:18<03:42, 10873.64\nexamples/s]', '\\rMap:  67%|######7   | 4981000/7400423 [08:18<03:43, 10834.64\nexamples/s]', '\\rMap:  67%|######7   | 4983000/7400423 [08:18<03:42, 10851.92\nexamples/s]', '\\rMap:  67%|######7   | 4985000/7400423 [08:18<03:43, 10817.04\nexamples/s]', '\\rMap:  67%|######7   | 4987000/7400423 [08:18<03:47, 10613.64\nexamples/s]', '\\rMap:  67%|######7   | 4989000/7400423 [08:19<03:50, 10462.40\nexamples/s]', '\\rMap:  67%|######7   | 4991000/7400423 [08:19<03:51, 10403.80\nexamples/s]', '\\rMap:  67%|######7   | 4993000/7400423 [08:19<03:52, 10349.21\nexamples/s]', '\\rMap:  67%|######7   | 4995000/7400423 [08:19<03:58, 10073.51\nexamples/s]', '\\rMap:  68%|######7   | 4997000/7400423 [08:19<03:58, 10092.87\nexamples/s]', '\\rMap:  68%|######7   | 4999000/7400423 [08:20<03:59, 10038.86\nexamples/s]', '\\rMap:  68%|######7   | 5001000/7400423 [08:20<05:16, 7591.91\nexamples/s] ', '\\rMap:  68%|######7   | 5003000/7400423 [08:20<04:51, 8229.76\nexamples/s]', '\\rMap:  68%|######7   | 5005000/7400423 [08:20<04:30, 8839.47\nexamples/s]', '\\rMap:  68%|######7   | 5007000/7400423 [08:21<04:21, 9164.04\nexamples/s]', '\\rMap:  68%|######7   | 5009000/7400423 [08:21<04:12, 9461.62\nexamples/s]', '\\rMap:  68%|######7   | 5010000/7400423 [08:21<04:10, 9525.41\nexamples/s]', '\\rMap:  68%|######7   | 5012000/7400423 [08:21<04:04, 9778.63\nexamples/s]', '\\rMap:  68%|######7   | 5014000/7400423 [08:21<04:00, 9919.89\nexamples/s]', '\\rMap:  68%|######7   | 5016000/7400423 [08:21<04:00, 9911.70\nexamples/s]', '\\rMap:  68%|######7   | 5018000/7400423 [08:22<03:53, 10204.69\nexamples/s]', '\\rMap:  68%|######7   | 5020000/7400423 [08:22<03:51, 10289.33\nexamples/s]', '\\rMap:  68%|######7   | 5022000/7400423 [08:22<03:52, 10224.00\nexamples/s]', '\\rMap:  68%|######7   | 5024000/7400423 [08:22<03:52, 10199.68\nexamples/s]', '\\rMap:  68%|######7   | 5026000/7400423 [08:22<03:52, 10193.67\nexamples/s]', '\\rMap:  68%|######7   | 5028000/7400423 [08:23<03:51, 10249.50\nexamples/s]', '\\rMap:  68%|######7   | 5030000/7400423 [08:23<03:53, 10170.31\nexamples/s]', '\\rMap:  68%|######7   | 5032000/7400423 [08:23<03:53, 10146.85\nexamples/s]', '\\rMap:  68%|######8   | 5034000/7400423 [08:23<03:50, 10283.04\nexamples/s]', '\\rMap:  68%|######8   | 5036000/7400423 [08:23<03:47, 10375.73\nexamples/s]', '\\rMap:  68%|######8   | 5038000/7400423 [08:24<03:47, 10362.92\nexamples/s]', '\\rMap:  68%|######8   | 5040000/7400423 [08:24<03:48, 10336.84\nexamples/s]', '\\rMap:  68%|######8   | 5042000/7400423 [08:24<03:48, 10313.44\nexamples/s]', '\\rMap:  68%|######8   | 5044000/7400423 [08:24<03:44, 10484.22\nexamples/s]', '\\rMap:  68%|######8   | 5046000/7400423 [08:24<03:41, 10615.62\nexamples/s]', '\\rMap:  68%|######8   | 5048000/7400423 [08:25<03:42, 10583.87\nexamples/s]', '\\rMap:  68%|######8   | 5050000/7400423 [08:25<03:42, 10573.64\nexamples/s]', '\\rMap:  68%|######8   | 5052000/7400423 [08:25<03:41, 10598.47\nexamples/s]', '\\rMap:  68%|######8   | 5054000/7400423 [08:25<03:42, 10558.14\nexamples/s]', '\\rMap:  68%|######8   | 5056000/7400423 [08:25<03:39, 10664.17\nexamples/s]', '\\rMap:  68%|######8   | 5058000/7400423 [08:25<03:40, 10622.59\nexamples/s]', '\\rMap:  68%|######8   | 5060000/7400423 [08:26<03:44, 10423.53\nexamples/s]', '\\rMap:  68%|######8   | 5062000/7400423 [08:26<03:45, 10388.98\nexamples/s]', '\\rMap:  68%|######8   | 5064000/7400423 [08:26<03:47, 10264.23\nexamples/s]', '\\rMap:  68%|######8   | 5066000/7400423 [08:26<03:46, 10321.68\nexamples/s]', '\\rMap:  68%|######8   | 5068000/7400423 [08:26<03:46, 10318.79\nexamples/s]', '\\rMap:  69%|######8   | 5070000/7400423 [08:27<03:51, 10071.63\nexamples/s]', '\\rMap:  69%|######8   | 5072000/7400423 [08:27<03:52, 10002.47\nexamples/s]', '\\rMap:  69%|######8   | 5074000/7400423 [08:27<03:51, 10042.04\nexamples/s]', '\\rMap:  69%|######8   | 5076000/7400423 [08:27<03:52, 9997.11\nexamples/s] ', '\\rMap:  69%|######8   | 5078000/7400423 [08:27<03:57, 9778.94\nexamples/s]', '\\rMap:  69%|######8   | 5079000/7400423 [08:28<03:58, 9726.66\nexamples/s]', '\\rMap:  69%|######8   | 5080000/7400423 [08:28<03:58, 9727.39\nexamples/s]', '\\rMap:  69%|######8   | 5081000/7400423 [08:28<03:57, 9745.91\nexamples/s]', '\\rMap:  69%|######8   | 5083000/7400423 [08:28<03:53, 9908.20\nexamples/s]', '\\rMap:  69%|######8   | 5084000/7400423 [08:28<03:54, 9894.67\nexamples/s]', '\\rMap:  69%|######8   | 5086000/7400423 [08:28<03:53, 9933.02\nexamples/s]', '\\rMap:  69%|######8   | 5087000/7400423 [08:28<03:54, 9881.78\nexamples/s]', '\\rMap:  69%|######8   | 5088000/7400423 [08:28<03:56, 9781.57\nexamples/s]', '\\rMap:  69%|######8   | 5089000/7400423 [08:29<03:55, 9800.30\nexamples/s]', '\\rMap:  69%|######8   | 5091000/7400423 [08:29<03:56, 9770.17\nexamples/s]', '\\rMap:  69%|######8   | 5093000/7400423 [08:29<03:54, 9847.66\nexamples/s]', '\\rMap:  69%|######8   | 5094000/7400423 [08:29<03:55, 9788.25\nexamples/s]', '\\rMap:  69%|######8   | 5095000/7400423 [08:29<03:57, 9713.63\nexamples/s]', '\\rMap:  69%|######8   | 5096000/7400423 [08:29<03:56, 9755.75\nexamples/s]', '\\rMap:  69%|######8   | 5097000/7400423 [08:29<03:55, 9793.27\nexamples/s]', '\\rMap:  69%|######8   | 5098000/7400423 [08:29<03:57, 9701.52\nexamples/s]', '\\rMap:  69%|######8   | 5099000/7400423 [08:30<04:00, 9586.44\nexamples/s]', '\\rMap:  69%|######8   | 5100000/7400423 [08:30<06:12, 6179.94\nexamples/s]', '\\rMap:  69%|######8   | 5101000/7400423 [08:30<05:33, 6896.54\nexamples/s]', '\\rMap:  69%|######8   | 5102000/7400423 [08:30<05:03, 7576.89\nexamples/s]', '\\rMap:  69%|######8   | 5103000/7400423 [08:30<04:41, 8148.30\nexamples/s]', '\\rMap:  69%|######8   | 5104000/7400423 [08:30<04:27, 8597.87\nexamples/s]', '\\rMap:  69%|######8   | 5105000/7400423 [08:30<04:18, 8871.42\nexamples/s]', '\\rMap:  69%|######9   | 5107000/7400423 [08:31<04:03, 9424.64\nexamples/s]', '\\rMap:  69%|######9   | 5109000/7400423 [08:31<03:49, 9970.99\nexamples/s]', '\\rMap:  69%|######9   | 5111000/7400423 [08:31<03:47, 10065.90\nexamples/s]', '\\rMap:  69%|######9   | 5113000/7400423 [08:31<03:41, 10310.58\nexamples/s]', '\\rMap:  69%|######9   | 5115000/7400423 [08:31<03:40, 10370.75\nexamples/s]', '\\rMap:  69%|######9   | 5117000/7400423 [08:32<03:43, 10239.36\nexamples/s]', '\\rMap:  69%|######9   | 5119000/7400423 [08:32<03:43, 10227.87\nexamples/s]', '\\rMap:  69%|######9   | 5121000/7400423 [08:32<03:40, 10331.84\nexamples/s]', '\\rMap:  69%|######9   | 5123000/7400423 [08:32<03:39, 10353.28\nexamples/s]', '\\rMap:  69%|######9   | 5125000/7400423 [08:32<03:33, 10655.80\nexamples/s]', '\\rMap:  69%|######9   | 5127000/7400423 [08:33<03:31, 10726.48\nexamples/s]', '\\rMap:  69%|######9   | 5129000/7400423 [08:33<03:29, 10847.78\nexamples/s]', '\\rMap:  69%|######9   | 5131000/7400423 [08:33<03:24, 11080.72\nexamples/s]', '\\rMap:  69%|######9   | 5133000/7400423 [08:33<03:20, 11301.94\nexamples/s]', '\\rMap:  69%|######9   | 5135000/7400423 [08:33<03:15, 11568.31\nexamples/s]', '\\rMap:  69%|######9   | 5137000/7400423 [08:33<03:11, 11793.58\nexamples/s]', '\\rMap:  69%|######9   | 5139000/7400423 [08:34<03:15, 11577.17\nexamples/s]', '\\rMap:  69%|######9   | 5141000/7400423 [08:34<03:37, 10410.49\nexamples/s]', '\\rMap:  69%|######9   | 5143000/7400423 [08:34<03:51, 9766.62\nexamples/s] ', '\\rMap:  70%|######9   | 5145000/7400423 [08:34<04:03, 9256.68\nexamples/s]', '\\rMap:  70%|######9   | 5146000/7400423 [08:34<04:18, 8733.93\nexamples/s]', '\\rMap:  70%|######9   | 5147000/7400423 [08:35<04:25, 8487.98\nexamples/s]', '\\rMap:  70%|######9   | 5148000/7400423 [08:35<04:33, 8234.79\nexamples/s]', '\\rMap:  70%|######9   | 5149000/7400423 [08:35<04:37, 8105.05\nexamples/s]', '\\rMap:  70%|######9   | 5150000/7400423 [08:35<04:48, 7812.26\nexamples/s]', '\\rMap:  70%|######9   | 5151000/7400423 [08:35<04:49, 7767.00\nexamples/s]', '\\rMap:  70%|######9   | 5152000/7400423 [08:35<04:47, 7819.12\nexamples/s]', '\\rMap:  70%|######9   | 5153000/7400423 [08:35<04:52, 7693.63\nexamples/s]', '\\rMap:  70%|######9   | 5154000/7400423 [08:35<04:54, 7640.31\nexamples/s]', '\\rMap:  70%|######9   | 5155000/7400423 [08:36<04:49, 7761.44\nexamples/s]', '\\rMap:  70%|######9   | 5156000/7400423 [08:36<04:50, 7713.51\nexamples/s]', '\\rMap:  70%|######9   | 5157000/7400423 [08:36<04:52, 7673.49\nexamples/s]', '\\rMap:  70%|######9   | 5158000/7400423 [08:36<04:50, 7716.12\nexamples/s]', '\\rMap:  70%|######9   | 5159000/7400423 [08:36<04:47, 7807.36\nexamples/s]', '\\rMap:  70%|######9   | 5160000/7400423 [08:36<04:47, 7787.53\nexamples/s]', '\\rMap:  70%|######9   | 5161000/7400423 [08:36<04:48, 7772.13\nexamples/s]', '\\rMap:  70%|######9   | 5162000/7400423 [08:36<04:53, 7634.11\nexamples/s]', '\\rMap:  70%|######9   | 5163000/7400423 [08:37<04:54, 7598.45\nexamples/s]', '\\rMap:  70%|######9   | 5164000/7400423 [08:37<04:53, 7612.48\nexamples/s]', '\\rMap:  70%|######9   | 5165000/7400423 [08:37<04:54, 7591.48\nexamples/s]', '\\rMap:  70%|######9   | 5166000/7400423 [08:37<04:48, 7754.21\nexamples/s]', '\\rMap:  70%|######9   | 5167000/7400423 [08:37<04:57, 7517.99\nexamples/s]', '\\rMap:  70%|######9   | 5168000/7400423 [08:37<05:01, 7412.43\nexamples/s]', '\\rMap:  70%|######9   | 5169000/7400423 [08:37<04:59, 7456.83\nexamples/s]', '\\rMap:  70%|######9   | 5170000/7400423 [08:38<04:52, 7619.72\nexamples/s]', '\\rMap:  70%|######9   | 5171000/7400423 [08:38<04:44, 7823.98\nexamples/s]', '\\rMap:  70%|######9   | 5172000/7400423 [08:38<04:41, 7907.20\nexamples/s]', '\\rMap:  70%|######9   | 5173000/7400423 [08:38<04:40, 7927.80\nexamples/s]', '\\rMap:  70%|######9   | 5174000/7400423 [08:38<04:34, 8099.38\nexamples/s]', '\\rMap:  70%|######9   | 5175000/7400423 [08:38<04:32, 8172.87\nexamples/s]', '\\rMap:  70%|######9   | 5176000/7400423 [08:38<04:28, 8272.52\nexamples/s]', '\\rMap:  70%|######9   | 5177000/7400423 [08:38<04:32, 8163.50\nexamples/s]', '\\rMap:  70%|######9   | 5178000/7400423 [08:39<04:33, 8128.83\nexamples/s]', '\\rMap:  70%|######9   | 5179000/7400423 [08:39<04:29, 8240.75\nexamples/s]', '\\rMap:  70%|######9   | 5180000/7400423 [08:39<04:27, 8300.61\nexamples/s]', '\\rMap:  70%|#######   | 5181000/7400423 [08:39<04:25, 8348.16\nexamples/s]', '\\rMap:  70%|#######   | 5182000/7400423 [08:39<04:23, 8406.15\nexamples/s]', '\\rMap:  70%|#######   | 5183000/7400423 [08:39<04:19, 8550.29\nexamples/s]', '\\rMap:  70%|#######   | 5184000/7400423 [08:39<04:15, 8666.26\nexamples/s]', '\\rMap:  70%|#######   | 5185000/7400423 [08:39<04:18, 8579.73\nexamples/s]', '\\rMap:  70%|#######   | 5186000/7400423 [08:39<04:16, 8645.68\nexamples/s]', '\\rMap:  70%|#######   | 5187000/7400423 [08:40<06:37, 5572.20\nexamples/s]', '\\rMap:  70%|#######   | 5188000/7400423 [08:40<05:51, 6293.26\nexamples/s]', '\\rMap:  70%|#######   | 5189000/7400423 [08:40<05:24, 6822.28\nexamples/s]', '\\rMap:  70%|#######   | 5190000/7400423 [08:40<05:08, 7160.02\nexamples/s]', '\\rMap:  70%|#######   | 5191000/7400423 [08:40<04:57, 7423.13\nexamples/s]', '\\rMap:  70%|#######   | 5192000/7400423 [08:40<04:49, 7623.11\nexamples/s]', '\\rMap:  70%|#######   | 5193000/7400423 [08:40<04:40, 7867.69\nexamples/s]', '\\rMap:  70%|#######   | 5194000/7400423 [08:41<04:36, 7974.52\nexamples/s]', '\\rMap:  70%|#######   | 5195000/7400423 [08:41<04:22, 8411.82\nexamples/s]', '\\rMap:  70%|#######   | 5196000/7400423 [08:41<04:15, 8619.37\nexamples/s]', '\\rMap:  70%|#######   | 5197000/7400423 [08:41<04:09, 8814.31\nexamples/s]', '\\rMap:  70%|#######   | 5198000/7400423 [08:41<04:12, 8717.27\nexamples/s]', '\\rMap:  70%|#######   | 5199000/7400423 [08:41<04:08, 8842.12\nexamples/s]', '\\rMap:  70%|#######   | 5200000/7400423 [08:41<04:04, 8992.04\nexamples/s]', '\\rMap:  70%|#######   | 5201000/7400423 [08:41<03:58, 9214.22\nexamples/s]', '\\rMap:  70%|#######   | 5202000/7400423 [08:41<03:54, 9365.48\nexamples/s]', '\\rMap:  70%|#######   | 5203000/7400423 [08:42<03:52, 9446.73\nexamples/s]', '\\rMap:  70%|#######   | 5205000/7400423 [08:42<03:39, 9992.91\nexamples/s]', '\\rMap:  70%|#######   | 5207000/7400423 [08:42<03:39, 9983.52\nexamples/s]', '\\rMap:  70%|#######   | 5208000/7400423 [08:42<03:44, 9785.55\nexamples/s]', '\\rMap:  70%|#######   | 5210000/7400423 [08:42<03:45, 9698.87\nexamples/s]', '\\rMap:  70%|#######   | 5211000/7400423 [08:42<03:49, 9547.69\nexamples/s]', '\\rMap:  70%|#######   | 5212000/7400423 [08:42<03:49, 9521.97\nexamples/s]', '\\rMap:  70%|#######   | 5213000/7400423 [08:43<03:53, 9377.62\nexamples/s]', '\\rMap:  70%|#######   | 5214000/7400423 [08:43<03:57, 9192.90\nexamples/s]', '\\rMap:  70%|#######   | 5215000/7400423 [08:43<03:57, 9182.89\nexamples/s]', '\\rMap:  70%|#######   | 5216000/7400423 [08:43<03:59, 9117.81\nexamples/s]', '\\rMap:  70%|#######   | 5217000/7400423 [08:43<04:01, 9035.83\nexamples/s]', '\\rMap:  71%|#######   | 5218000/7400423 [08:43<04:02, 8987.73\nexamples/s]', '\\rMap:  71%|#######   | 5219000/7400423 [08:43<04:00, 9066.75\nexamples/s]', '\\rMap:  71%|#######   | 5220000/7400423 [08:43<03:57, 9190.94\nexamples/s]', '\\rMap:  71%|#######   | 5221000/7400423 [08:43<03:56, 9229.04\nexamples/s]', '\\rMap:  71%|#######   | 5222000/7400423 [08:44<03:58, 9144.32\nexamples/s]', '\\rMap:  71%|#######   | 5223000/7400423 [08:44<03:55, 9255.20\nexamples/s]', '\\rMap:  71%|#######   | 5224000/7400423 [08:44<03:52, 9358.52\nexamples/s]', '\\rMap:  71%|#######   | 5226000/7400423 [08:44<03:47, 9578.31\nexamples/s]', '\\rMap:  71%|#######   | 5228000/7400423 [08:44<03:42, 9753.11\nexamples/s]', '\\rMap:  71%|#######   | 5230000/7400423 [08:44<03:42, 9737.12\nexamples/s]', '\\rMap:  71%|#######   | 5231000/7400423 [08:45<03:45, 9620.45\nexamples/s]', '\\rMap:  71%|#######   | 5232000/7400423 [08:45<03:50, 9392.21\nexamples/s]', '\\rMap:  71%|#######   | 5233000/7400423 [08:45<03:49, 9457.23\nexamples/s]', '\\rMap:  71%|#######   | 5235000/7400423 [08:45<03:44, 9644.07\nexamples/s]', '\\rMap:  71%|#######   | 5237000/7400423 [08:45<03:45, 9608.79\nexamples/s]', '\\rMap:  71%|#######   | 5238000/7400423 [08:45<03:44, 9621.92\nexamples/s]', '\\rMap:  71%|#######   | 5239000/7400423 [08:45<03:47, 9499.00\nexamples/s]', '\\rMap:  71%|#######   | 5240000/7400423 [08:45<03:49, 9431.82\nexamples/s]', '\\rMap:  71%|#######   | 5241000/7400423 [08:46<03:57, 9093.03\nexamples/s]', '\\rMap:  71%|#######   | 5242000/7400423 [08:46<04:03, 8856.58\nexamples/s]', '\\rMap:  71%|#######   | 5243000/7400423 [08:46<03:57, 9080.01\nexamples/s]', '\\rMap:  71%|#######   | 5244000/7400423 [08:46<03:57, 9063.45\nexamples/s]', '\\rMap:  71%|#######   | 5245000/7400423 [08:46<03:58, 9026.93\nexamples/s]', '\\rMap:  71%|#######   | 5246000/7400423 [08:46<04:02, 8897.18\nexamples/s]', '\\rMap:  71%|#######   | 5247000/7400423 [08:46<03:58, 9032.85\nexamples/s]', '\\rMap:  71%|#######   | 5248000/7400423 [08:46<03:59, 8984.09\nexamples/s]', '\\rMap:  71%|#######   | 5249000/7400423 [08:46<03:55, 9146.01\nexamples/s]', '\\rMap:  71%|#######   | 5250000/7400423 [08:47<03:50, 9329.03\nexamples/s]', '\\rMap:  71%|#######   | 5251000/7400423 [08:47<03:49, 9379.01\nexamples/s]', '\\rMap:  71%|#######   | 5252000/7400423 [08:47<03:47, 9446.60\nexamples/s]', '\\rMap:  71%|#######   | 5253000/7400423 [08:47<03:45, 9514.00\nexamples/s]', '\\rMap:  71%|#######   | 5254000/7400423 [08:47<03:44, 9549.98\nexamples/s]', '\\rMap:  71%|#######1  | 5255000/7400423 [08:47<03:52, 9239.83\nexamples/s]', '\\rMap:  71%|#######1  | 5256000/7400423 [08:47<03:56, 9067.96\nexamples/s]', '\\rMap:  71%|#######1  | 5257000/7400423 [08:47<03:51, 9248.36\nexamples/s]', '\\rMap:  71%|#######1  | 5258000/7400423 [08:47<03:51, 9249.89\nexamples/s]', '\\rMap:  71%|#######1  | 5259000/7400423 [08:48<03:47, 9406.59\nexamples/s]', '\\rMap:  71%|#######1  | 5260000/7400423 [08:48<03:48, 9355.28\nexamples/s]', '\\rMap:  71%|#######1  | 5262000/7400423 [08:48<03:43, 9553.82\nexamples/s]', '\\rMap:  71%|#######1  | 5263000/7400423 [08:48<05:26, 6544.58\nexamples/s]', '\\rMap:  71%|#######1  | 5264000/7400423 [08:48<05:02, 7055.79\nexamples/s]', '\\rMap:  71%|#######1  | 5265000/7400423 [08:48<04:45, 7480.52\nexamples/s]', '\\rMap:  71%|#######1  | 5266000/7400423 [08:48<04:28, 7958.22\nexamples/s]', '\\rMap:  71%|#######1  | 5268000/7400423 [08:49<03:59, 8904.36\nexamples/s]', '\\rMap:  71%|#######1  | 5270000/7400423 [08:49<03:44, 9488.42\nexamples/s]', '\\rMap:  71%|#######1  | 5272000/7400423 [08:49<03:33, 9959.09\nexamples/s]', '\\rMap:  71%|#######1  | 5274000/7400423 [08:49<03:23, 10455.02\nexamples/s]', '\\rMap:  71%|#######1  | 5276000/7400423 [08:49<03:20, 10591.08\nexamples/s]', '\\rMap:  71%|#######1  | 5278000/7400423 [08:50<03:15, 10876.46\nexamples/s]', '\\rMap:  71%|#######1  | 5280000/7400423 [08:50<03:13, 10949.48\nexamples/s]', '\\rMap:  71%|#######1  | 5282000/7400423 [08:50<03:12, 10997.46\nexamples/s]', '\\rMap:  71%|#######1  | 5284000/7400423 [08:50<03:10, 11085.39\nexamples/s]', '\\rMap:  71%|#######1  | 5286000/7400423 [08:50<03:05, 11400.17\nexamples/s]', '\\rMap:  71%|#######1  | 5288000/7400423 [08:50<03:03, 11490.17\nexamples/s]', '\\rMap:  71%|#######1  | 5290000/7400423 [08:51<03:02, 11574.44\nexamples/s]', '\\rMap:  72%|#######1  | 5292000/7400423 [08:51<03:01, 11646.79\nexamples/s]', '\\rMap:  72%|#######1  | 5294000/7400423 [08:51<03:02, 11540.76\nexamples/s]', '\\rMap:  72%|#######1  | 5296000/7400423 [08:51<03:04, 11399.95\nexamples/s]', '\\rMap:  72%|#######1  | 5298000/7400423 [08:51<03:04, 11419.61\nexamples/s]', '\\rMap:  72%|#######1  | 5300000/7400423 [08:51<03:03, 11439.91\nexamples/s]', '\\rMap:  72%|#######1  | 5302000/7400423 [08:52<03:06, 11231.89\nexamples/s]', '\\rMap:  72%|#######1  | 5304000/7400423 [08:52<03:10, 11015.96\nexamples/s]', '\\rMap:  72%|#######1  | 5306000/7400423 [08:52<03:09, 11035.08\nexamples/s]', '\\rMap:  72%|#######1  | 5308000/7400423 [08:52<03:09, 11029.65\nexamples/s]', '\\rMap:  72%|#######1  | 5310000/7400423 [08:52<03:07, 11173.98\nexamples/s]', '\\rMap:  72%|#######1  | 5312000/7400423 [08:53<03:06, 11202.08\nexamples/s]', '\\rMap:  72%|#######1  | 5314000/7400423 [08:53<03:06, 11162.46\nexamples/s]', '\\rMap:  72%|#######1  | 5316000/7400423 [08:53<03:06, 11156.46\nexamples/s]', '\\rMap:  72%|#######1  | 5318000/7400423 [08:53<03:07, 11094.43\nexamples/s]', '\\rMap:  72%|#######1  | 5320000/7400423 [08:53<03:08, 11051.53\nexamples/s]', '\\rMap:  72%|#######1  | 5322000/7400423 [08:53<03:04, 11264.65\nexamples/s]', '\\rMap:  72%|#######1  | 5324000/7400423 [08:54<02:58, 11627.91\nexamples/s]', '\\rMap:  72%|#######1  | 5326000/7400423 [08:54<02:55, 11851.83\nexamples/s]', '\\rMap:  72%|#######1  | 5328000/7400423 [08:54<02:55, 11813.67\nexamples/s]', '\\rMap:  72%|#######2  | 5330000/7400423 [08:54<02:55, 11818.05\nexamples/s]', '\\rMap:  72%|#######2  | 5332000/7400423 [08:54<02:58, 11589.77\nexamples/s]', '\\rMap:  72%|#######2  | 5334000/7400423 [08:55<03:05, 11143.48\nexamples/s]', '\\rMap:  72%|#######2  | 5336000/7400423 [08:55<03:17, 10431.98\nexamples/s]', '\\rMap:  72%|#######2  | 5338000/7400423 [08:55<03:18, 10411.49\nexamples/s]', '\\rMap:  72%|#######2  | 5340000/7400423 [08:55<03:12, 10717.96\nexamples/s]', '\\rMap:  72%|#######2  | 5342000/7400423 [08:55<03:08, 10924.21\nexamples/s]', '\\rMap:  72%|#######2  | 5344000/7400423 [08:55<03:01, 11306.90\nexamples/s]', '\\rMap:  72%|#######2  | 5346000/7400423 [08:56<02:57, 11569.98\nexamples/s]', '\\rMap:  72%|#######2  | 5348000/7400423 [08:56<03:00, 11375.67\nexamples/s]', '\\rMap:  72%|#######2  | 5350000/7400423 [08:56<03:00, 11354.21\nexamples/s]', '\\rMap:  72%|#######2  | 5352000/7400423 [08:56<02:57, 11537.25\nexamples/s]', '\\rMap:  72%|#######2  | 5354000/7400423 [08:56<02:54, 11705.70\nexamples/s]', '\\rMap:  72%|#######2  | 5356000/7400423 [08:56<02:56, 11591.89\nexamples/s]', '\\rMap:  72%|#######2  | 5358000/7400423 [08:57<02:53, 11765.55\nexamples/s]', '\\rMap:  72%|#######2  | 5360000/7400423 [08:57<02:52, 11817.75\nexamples/s]', '\\rMap:  72%|#######2  | 5362000/7400423 [08:57<02:53, 11765.21\nexamples/s]', '\\rMap:  72%|#######2  | 5364000/7400423 [08:57<02:53, 11769.80\nexamples/s]', '\\rMap:  73%|#######2  | 5366000/7400423 [08:57<02:51, 11845.38\nexamples/s]', '\\rMap:  73%|#######2  | 5368000/7400423 [08:57<02:53, 11737.53\nexamples/s]', '\\rMap:  73%|#######2  | 5370000/7400423 [08:58<02:53, 11674.12\nexamples/s]', '\\rMap:  73%|#######2  | 5372000/7400423 [08:58<02:53, 11697.71\nexamples/s]', '\\rMap:  73%|#######2  | 5374000/7400423 [08:58<02:52, 11764.54\nexamples/s]', '\\rMap:  73%|#######2  | 5376000/7400423 [08:58<02:50, 11873.46\nexamples/s]', '\\rMap:  73%|#######2  | 5378000/7400423 [08:58<02:49, 11959.33\nexamples/s]', '\\rMap:  73%|#######2  | 5380000/7400423 [08:58<02:46, 12126.69\nexamples/s]', '\\rMap:  73%|#######2  | 5382000/7400423 [08:59<02:45, 12207.20\nexamples/s]', '\\rMap:  73%|#######2  | 5384000/7400423 [08:59<02:43, 12330.73\nexamples/s]', '\\rMap:  73%|#######2  | 5386000/7400423 [08:59<02:40, 12525.40\nexamples/s]', '\\rMap:  73%|#######2  | 5388000/7400423 [08:59<02:41, 12486.32\nexamples/s]', '\\rMap:  73%|#######2  | 5390000/7400423 [08:59<03:37, 9263.88\nexamples/s] ', '\\rMap:  73%|#######2  | 5392000/7400423 [09:00<03:22, 9942.08\nexamples/s]', '\\rMap:  73%|#######2  | 5394000/7400423 [09:00<03:10, 10526.77\nexamples/s]', '\\rMap:  73%|#######2  | 5396000/7400423 [09:00<03:01, 11026.88\nexamples/s]', '\\rMap:  73%|#######2  | 5398000/7400423 [09:00<03:01, 11046.43\nexamples/s]', '\\rMap:  73%|#######2  | 5400000/7400423 [09:00<03:01, 11036.56\nexamples/s]', '\\rMap:  73%|#######2  | 5402000/7400423 [09:00<02:57, 11273.48\nexamples/s]', '\\rMap:  73%|#######3  | 5404000/7400423 [09:01<02:56, 11292.76\nexamples/s]', '\\rMap:  73%|#######3  | 5406000/7400423 [09:01<02:55, 11353.22\nexamples/s]', '\\rMap:  73%|#######3  | 5408000/7400423 [09:01<02:55, 11383.07\nexamples/s]', '\\rMap:  73%|#######3  | 5410000/7400423 [09:01<02:53, 11486.09\nexamples/s]', '\\rMap:  73%|#######3  | 5412000/7400423 [09:01<02:48, 11780.12\nexamples/s]', '\\rMap:  73%|#######3  | 5414000/7400423 [09:02<02:48, 11823.19\nexamples/s]', '\\rMap:  73%|#######3  | 5416000/7400423 [09:02<02:45, 11982.16\nexamples/s]', '\\rMap:  73%|#######3  | 5418000/7400423 [09:02<02:45, 12002.16\nexamples/s]', '\\rMap:  73%|#######3  | 5420000/7400423 [09:02<02:43, 12082.32\nexamples/s]', '\\rMap:  73%|#######3  | 5422000/7400423 [09:02<02:46, 11911.59\nexamples/s]', '\\rMap:  73%|#######3  | 5424000/7400423 [09:02<02:45, 11927.52\nexamples/s]', '\\rMap:  73%|#######3  | 5426000/7400423 [09:03<02:48, 11714.45\nexamples/s]', '\\rMap:  73%|#######3  | 5428000/7400423 [09:03<02:57, 11129.40\nexamples/s]', '\\rMap:  73%|#######3  | 5430000/7400423 [09:03<03:02, 10778.45\nexamples/s]', '\\rMap:  73%|#######3  | 5432000/7400423 [09:03<03:06, 10574.88\nexamples/s]', '\\rMap:  73%|#######3  | 5434000/7400423 [09:03<03:08, 10441.12\nexamples/s]', '\\rMap:  73%|#######3  | 5436000/7400423 [09:04<03:10, 10326.43\nexamples/s]', '\\rMap:  73%|#######3  | 5438000/7400423 [09:04<03:14, 10071.26\nexamples/s]', '\\rMap:  74%|#######3  | 5440000/7400423 [09:04<03:17, 9943.74\nexamples/s] ', '\\rMap:  74%|#######3  | 5442000/7400423 [09:04<03:17, 9904.17\nexamples/s]', '\\rMap:  74%|#######3  | 5443000/7400423 [09:04<03:21, 9732.65\nexamples/s]', '\\rMap:  74%|#######3  | 5444000/7400423 [09:04<03:22, 9664.09\nexamples/s]', '\\rMap:  74%|#######3  | 5445000/7400423 [09:04<03:22, 9677.67\nexamples/s]', '\\rMap:  74%|#######3  | 5446000/7400423 [09:05<03:22, 9640.98\nexamples/s]', '\\rMap:  74%|#######3  | 5447000/7400423 [09:05<03:22, 9644.00\nexamples/s]', '\\rMap:  74%|#######3  | 5448000/7400423 [09:05<03:22, 9643.36\nexamples/s]', '\\rMap:  74%|#######3  | 5450000/7400423 [09:05<03:15, 10000.59\nexamples/s]', '\\rMap:  74%|#######3  | 5452000/7400423 [09:05<03:13, 10088.70\nexamples/s]', '\\rMap:  74%|#######3  | 5454000/7400423 [09:05<03:12, 10086.71\nexamples/s]', '\\rMap:  74%|#######3  | 5456000/7400423 [09:06<03:07, 10349.47\nexamples/s]', '\\rMap:  74%|#######3  | 5458000/7400423 [09:06<03:00, 10783.56\nexamples/s]', '\\rMap:  74%|#######3  | 5460000/7400423 [09:06<02:55, 11069.46\nexamples/s]', '\\rMap:  74%|#######3  | 5462000/7400423 [09:06<02:57, 10938.22\nexamples/s]', '\\rMap:  74%|#######3  | 5464000/7400423 [09:06<02:58, 10834.89\nexamples/s]', '\\rMap:  74%|#######3  | 5466000/7400423 [09:06<02:58, 10807.32\nexamples/s]', '\\rMap:  74%|#######3  | 5468000/7400423 [09:07<02:57, 10882.28\nexamples/s]', '\\rMap:  74%|#######3  | 5470000/7400423 [09:07<02:56, 10939.91\nexamples/s]', '\\rMap:  74%|#######3  | 5472000/7400423 [09:07<03:01, 10643.51\nexamples/s]', '\\rMap:  74%|#######3  | 5474000/7400423 [09:07<03:03, 10516.02\nexamples/s]', '\\rMap:  74%|#######3  | 5476000/7400423 [09:07<03:04, 10413.97\nexamples/s]', '\\rMap:  74%|#######4  | 5478000/7400423 [09:08<03:04, 10434.29\nexamples/s]', '\\rMap:  74%|#######4  | 5480000/7400423 [09:08<03:04, 10381.24\nexamples/s]', '\\rMap:  74%|#######4  | 5482000/7400423 [09:08<03:03, 10439.27\nexamples/s]', '\\rMap:  74%|#######4  | 5484000/7400423 [09:08<03:02, 10476.84\nexamples/s]', '\\rMap:  74%|#######4  | 5486000/7400423 [09:08<03:03, 10405.50\nexamples/s]', '\\rMap:  74%|#######4  | 5488000/7400423 [09:09<04:05, 7791.51\nexamples/s] ', '\\rMap:  74%|#######4  | 5489000/7400423 [09:09<03:57, 8039.98\nexamples/s]', '\\rMap:  74%|#######4  | 5491000/7400423 [09:09<03:39, 8683.46\nexamples/s]', '\\rMap:  74%|#######4  | 5493000/7400423 [09:09<03:26, 9223.53\nexamples/s]', '\\rMap:  74%|#######4  | 5495000/7400423 [09:09<03:19, 9558.03\nexamples/s]', '\\rMap:  74%|#######4  | 5497000/7400423 [09:10<03:11, 9945.90\nexamples/s]', '\\rMap:  74%|#######4  | 5499000/7400423 [09:10<03:06, 10211.11\nexamples/s]', '\\rMap:  74%|#######4  | 5501000/7400423 [09:10<03:06, 10176.87\nexamples/s]', '\\rMap:  74%|#######4  | 5503000/7400423 [09:10<03:02, 10375.84\nexamples/s]', '\\rMap:  74%|#######4  | 5505000/7400423 [09:10<02:59, 10550.10\nexamples/s]', '\\rMap:  74%|#######4  | 5507000/7400423 [09:11<02:56, 10734.19\nexamples/s]', '\\rMap:  74%|#######4  | 5509000/7400423 [09:11<02:56, 10723.83\nexamples/s]', '\\rMap:  74%|#######4  | 5511000/7400423 [09:11<02:58, 10571.41\nexamples/s]', '\\rMap:  74%|#######4  | 5513000/7400423 [09:11<03:01, 10398.20\nexamples/s]', '\\rMap:  75%|#######4  | 5515000/7400423 [09:11<02:59, 10500.64\nexamples/s]', '\\rMap:  75%|#######4  | 5517000/7400423 [09:12<02:57, 10593.89\nexamples/s]', '\\rMap:  75%|#######4  | 5519000/7400423 [09:12<02:57, 10569.90\nexamples/s]', '\\rMap:  75%|#######4  | 5521000/7400423 [09:12<02:55, 10683.46\nexamples/s]', '\\rMap:  75%|#######4  | 5523000/7400423 [09:12<03:01, 10332.02\nexamples/s]', '\\rMap:  75%|#######4  | 5525000/7400423 [09:12<03:05, 10094.95\nexamples/s]', '\\rMap:  75%|#######4  | 5527000/7400423 [09:12<03:04, 10167.15\nexamples/s]', '\\rMap:  75%|#######4  | 5529000/7400423 [09:13<03:01, 10307.19\nexamples/s]', '\\rMap:  75%|#######4  | 5531000/7400423 [09:13<03:00, 10350.48\nexamples/s]', '\\rMap:  75%|#######4  | 5533000/7400423 [09:13<02:59, 10375.58\nexamples/s]', '\\rMap:  75%|#######4  | 5535000/7400423 [09:13<02:58, 10442.16\nexamples/s]', '\\rMap:  75%|#######4  | 5537000/7400423 [09:13<02:54, 10654.37\nexamples/s]', '\\rMap:  75%|#######4  | 5539000/7400423 [09:14<02:54, 10678.13\nexamples/s]', '\\rMap:  75%|#######4  | 5541000/7400423 [09:14<02:55, 10591.44\nexamples/s]', '\\rMap:  75%|#######4  | 5543000/7400423 [09:14<02:55, 10571.65\nexamples/s]', '\\rMap:  75%|#######4  | 5545000/7400423 [09:14<02:56, 10493.01\nexamples/s]', '\\rMap:  75%|#######4  | 5547000/7400423 [09:14<02:54, 10619.94\nexamples/s]', '\\rMap:  75%|#######4  | 5549000/7400423 [09:15<02:53, 10675.15\nexamples/s]', '\\rMap:  75%|#######5  | 5551000/7400423 [09:15<02:51, 10807.05\nexamples/s]', '\\rMap:  75%|#######5  | 5553000/7400423 [09:15<02:51, 10794.74\nexamples/s]', '\\rMap:  75%|#######5  | 5555000/7400423 [09:15<02:51, 10751.92\nexamples/s]', '\\rMap:  75%|#######5  | 5557000/7400423 [09:15<02:50, 10809.97\nexamples/s]', '\\rMap:  75%|#######5  | 5559000/7400423 [09:15<02:49, 10847.69\nexamples/s]', '\\rMap:  75%|#######5  | 5561000/7400423 [09:16<02:49, 10869.22\nexamples/s]', '\\rMap:  75%|#######5  | 5563000/7400423 [09:16<02:50, 10774.58\nexamples/s]', '\\rMap:  75%|#######5  | 5565000/7400423 [09:16<02:50, 10783.62\nexamples/s]', '\\rMap:  75%|#######5  | 5567000/7400423 [09:16<02:49, 10793.92\nexamples/s]', '\\rMap:  75%|#######5  | 5569000/7400423 [09:16<02:48, 10884.68\nexamples/s]', '\\rMap:  75%|#######5  | 5571000/7400423 [09:17<02:47, 10920.88\nexamples/s]', '\\rMap:  75%|#######5  | 5573000/7400423 [09:17<02:46, 10973.43\nexamples/s]', '\\rMap:  75%|#######5  | 5575000/7400423 [09:17<03:40, 8279.10\nexamples/s] ', '\\rMap:  75%|#######5  | 5577000/7400423 [09:17<03:26, 8846.01\nexamples/s]', '\\rMap:  75%|#######5  | 5579000/7400423 [09:18<03:14, 9368.52\nexamples/s]', '\\rMap:  75%|#######5  | 5581000/7400423 [09:18<03:07, 9711.61\nexamples/s]', '\\rMap:  75%|#######5  | 5583000/7400423 [09:18<03:01, 9990.71\nexamples/s]', '\\rMap:  75%|#######5  | 5585000/7400423 [09:18<02:58, 10178.74\nexamples/s]', '\\rMap:  75%|#######5  | 5587000/7400423 [09:18<02:58, 10165.60\nexamples/s]', '\\rMap:  76%|#######5  | 5589000/7400423 [09:18<02:55, 10313.28\nexamples/s]', '\\rMap:  76%|#######5  | 5591000/7400423 [09:19<02:52, 10510.25\nexamples/s]', '\\rMap:  76%|#######5  | 5593000/7400423 [09:19<02:49, 10665.75\nexamples/s]', '\\rMap:  76%|#######5  | 5595000/7400423 [09:19<02:48, 10736.62\nexamples/s]', '\\rMap:  76%|#######5  | 5597000/7400423 [09:19<02:46, 10818.75\nexamples/s]', '\\rMap:  76%|#######5  | 5599000/7400423 [09:19<02:48, 10721.72\nexamples/s]', '\\rMap:  76%|#######5  | 5601000/7400423 [09:20<02:47, 10715.14\nexamples/s]', '\\rMap:  76%|#######5  | 5603000/7400423 [09:20<02:48, 10689.06\nexamples/s]', '\\rMap:  76%|#######5  | 5605000/7400423 [09:20<02:47, 10710.56\nexamples/s]', '\\rMap:  76%|#######5  | 5607000/7400423 [09:20<02:47, 10732.28\nexamples/s]', '\\rMap:  76%|#######5  | 5609000/7400423 [09:20<02:45, 10850.57\nexamples/s]', '\\rMap:  76%|#######5  | 5611000/7400423 [09:20<02:46, 10735.02\nexamples/s]', '\\rMap:  76%|#######5  | 5613000/7400423 [09:21<02:46, 10756.79\nexamples/s]', '\\rMap:  76%|#######5  | 5615000/7400423 [09:21<02:42, 10962.42\nexamples/s]', '\\rMap:  76%|#######5  | 5617000/7400423 [09:21<02:41, 11054.57\nexamples/s]', '\\rMap:  76%|#######5  | 5619000/7400423 [09:21<02:42, 10985.69\nexamples/s]', '\\rMap:  76%|#######5  | 5621000/7400423 [09:21<02:43, 10878.53\nexamples/s]', '\\rMap:  76%|#######5  | 5623000/7400423 [09:22<02:42, 10914.28\nexamples/s]', '\\rMap:  76%|#######6  | 5625000/7400423 [09:22<02:42, 10903.87\nexamples/s]', '\\rMap:  76%|#######6  | 5627000/7400423 [09:22<02:44, 10777.90\nexamples/s]', '\\rMap:  76%|#######6  | 5629000/7400423 [09:22<02:43, 10836.58\nexamples/s]', '\\rMap:  76%|#######6  | 5631000/7400423 [09:22<02:43, 10846.66\nexamples/s]', '\\rMap:  76%|#######6  | 5633000/7400423 [09:23<02:43, 10838.37\nexamples/s]', '\\rMap:  76%|#######6  | 5635000/7400423 [09:23<02:41, 10913.36\nexamples/s]', '\\rMap:  76%|#######6  | 5637000/7400423 [09:23<02:40, 10975.24\nexamples/s]', '\\rMap:  76%|#######6  | 5639000/7400423 [09:23<02:40, 10947.60\nexamples/s]', '\\rMap:  76%|#######6  | 5641000/7400423 [09:23<02:39, 11048.37\nexamples/s]', '\\rMap:  76%|#######6  | 5643000/7400423 [09:23<02:41, 10849.98\nexamples/s]', '\\rMap:  76%|#######6  | 5645000/7400423 [09:24<02:40, 10908.74\nexamples/s]', '\\rMap:  76%|#######6  | 5647000/7400423 [09:24<02:40, 10930.24\nexamples/s]', '\\rMap:  76%|#######6  | 5649000/7400423 [09:24<02:39, 10957.74\nexamples/s]', '\\rMap:  76%|#######6  | 5651000/7400423 [09:24<03:33, 8196.73\nexamples/s] ', '\\rMap:  76%|#######6  | 5653000/7400423 [09:25<03:16, 8887.77\nexamples/s]', '\\rMap:  76%|#######6  | 5655000/7400423 [09:25<03:04, 9475.55\nexamples/s]', '\\rMap:  76%|#######6  | 5657000/7400423 [09:25<02:57, 9805.60\nexamples/s]', '\\rMap:  76%|#######6  | 5659000/7400423 [09:25<02:53, 10051.69\nexamples/s]', '\\rMap:  76%|#######6  | 5661000/7400423 [09:25<02:49, 10266.74\nexamples/s]', '\\rMap:  77%|#######6  | 5663000/7400423 [09:25<02:45, 10524.59\nexamples/s]', '\\rMap:  77%|#######6  | 5665000/7400423 [09:26<02:44, 10551.91\nexamples/s]', '\\rMap:  77%|#######6  | 5667000/7400423 [09:26<02:42, 10646.10\nexamples/s]', '\\rMap:  77%|#######6  | 5669000/7400423 [09:26<02:47, 10321.85\nexamples/s]', '\\rMap:  77%|#######6  | 5671000/7400423 [09:26<02:46, 10369.72\nexamples/s]', '\\rMap:  77%|#######6  | 5673000/7400423 [09:26<02:45, 10465.42\nexamples/s]', '\\rMap:  77%|#######6  | 5675000/7400423 [09:27<02:44, 10478.10\nexamples/s]', '\\rMap:  77%|#######6  | 5677000/7400423 [09:27<02:43, 10568.14\nexamples/s]', '\\rMap:  77%|#######6  | 5679000/7400423 [09:27<02:44, 10450.89\nexamples/s]', '\\rMap:  77%|#######6  | 5681000/7400423 [09:27<02:45, 10417.83\nexamples/s]', '\\rMap:  77%|#######6  | 5683000/7400423 [09:27<02:43, 10496.46\nexamples/s]', '\\rMap:  77%|#######6  | 5685000/7400423 [09:28<02:42, 10546.20\nexamples/s]', '\\rMap:  77%|#######6  | 5687000/7400423 [09:28<02:42, 10527.03\nexamples/s]', '\\rMap:  77%|#######6  | 5689000/7400423 [09:28<02:40, 10681.41\nexamples/s]', '\\rMap:  77%|#######6  | 5691000/7400423 [09:28<02:39, 10696.20\nexamples/s]', '\\rMap:  77%|#######6  | 5693000/7400423 [09:28<02:37, 10854.62\nexamples/s]', '\\rMap:  77%|#######6  | 5695000/7400423 [09:28<02:38, 10758.53\nexamples/s]', '\\rMap:  77%|#######6  | 5697000/7400423 [09:29<02:36, 10918.45\nexamples/s]', '\\rMap:  77%|#######7  | 5699000/7400423 [09:29<02:39, 10661.86\nexamples/s]', '\\rMap:  77%|#######7  | 5701000/7400423 [09:29<02:42, 10488.71\nexamples/s]', '\\rMap:  77%|#######7  | 5703000/7400423 [09:29<02:41, 10489.43\nexamples/s]', '\\rMap:  77%|#######7  | 5705000/7400423 [09:29<02:40, 10540.06\nexamples/s]', '\\rMap:  77%|#######7  | 5707000/7400423 [09:30<02:44, 10323.28\nexamples/s]', '\\rMap:  77%|#######7  | 5709000/7400423 [09:30<02:45, 10239.54\nexamples/s]', '\\rMap:  77%|#######7  | 5711000/7400423 [09:30<02:44, 10282.39\nexamples/s]', '\\rMap:  77%|#######7  | 5713000/7400423 [09:30<02:43, 10296.30\nexamples/s]', '\\rMap:  77%|#######7  | 5715000/7400423 [09:30<02:43, 10295.44\nexamples/s]', '\\rMap:  77%|#######7  | 5717000/7400423 [09:31<02:43, 10285.68\nexamples/s]', '\\rMap:  77%|#######7  | 5719000/7400423 [09:31<02:42, 10317.93\nexamples/s]', '\\rMap:  77%|#######7  | 5721000/7400423 [09:31<02:42, 10352.11\nexamples/s]', '\\rMap:  77%|#######7  | 5723000/7400423 [09:31<02:41, 10378.31\nexamples/s]', '\\rMap:  77%|#######7  | 5725000/7400423 [09:31<02:41, 10371.15\nexamples/s]', '\\rMap:  77%|#######7  | 5727000/7400423 [09:32<02:41, 10336.01\nexamples/s]', '\\rMap:  77%|#######7  | 5729000/7400423 [09:32<02:41, 10351.00\nexamples/s]', '\\rMap:  77%|#######7  | 5731000/7400423 [09:32<02:44, 10139.98\nexamples/s]', '\\rMap:  77%|#######7  | 5733000/7400423 [09:32<02:45, 10048.30\nexamples/s]', '\\rMap:  77%|#######7  | 5735000/7400423 [09:32<02:43, 10157.17\nexamples/s]', '\\rMap:  78%|#######7  | 5737000/7400423 [09:33<02:45, 10041.34\nexamples/s]', '\\rMap:  78%|#######7  | 5739000/7400423 [09:33<02:45, 10011.19\nexamples/s]', '\\rMap:  78%|#######7  | 5741000/7400423 [09:33<02:41, 10263.55\nexamples/s]', '\\rMap:  78%|#######7  | 5743000/7400423 [09:33<02:40, 10346.23\nexamples/s]', '\\rMap:  78%|#######7  | 5745000/7400423 [09:33<02:37, 10493.42\nexamples/s]', '\\rMap:  78%|#######7  | 5747000/7400423 [09:34<02:36, 10551.31\nexamples/s]', '\\rMap:  78%|#######7  | 5749000/7400423 [09:34<02:38, 10435.00\nexamples/s]', '\\rMap:  78%|#######7  | 5751000/7400423 [09:34<02:38, 10387.21\nexamples/s]', '\\rMap:  78%|#######7  | 5753000/7400423 [09:34<02:39, 10342.63\nexamples/s]', '\\rMap:  78%|#######7  | 5755000/7400423 [09:34<02:40, 10281.19\nexamples/s]', '\\rMap:  78%|#######7  | 5757000/7400423 [09:34<02:38, 10336.91\nexamples/s]', '\\rMap:  78%|#######7  | 5759000/7400423 [09:35<02:37, 10417.23\nexamples/s]', '\\rMap:  78%|#######7  | 5761000/7400423 [09:35<02:39, 10300.79\nexamples/s]', '\\rMap:  78%|#######7  | 5763000/7400423 [09:35<02:36, 10439.13\nexamples/s]', '\\rMap:  78%|#######7  | 5765000/7400423 [09:35<02:36, 10471.32\nexamples/s]', '\\rMap:  78%|#######7  | 5767000/7400423 [09:35<02:32, 10715.84\nexamples/s]', '\\rMap:  78%|#######7  | 5769000/7400423 [09:36<02:32, 10698.17\nexamples/s]', '\\rMap:  78%|#######7  | 5771000/7400423 [09:36<02:27, 11038.16\nexamples/s]', '\\rMap:  78%|#######8  | 5773000/7400423 [09:36<02:25, 11147.61\nexamples/s]', '\\rMap:  78%|#######8  | 5775000/7400423 [09:36<02:23, 11301.10\nexamples/s]', '\\rMap:  78%|#######8  | 5777000/7400423 [09:37<03:16, 8280.41\nexamples/s] ', '\\rMap:  78%|#######8  | 5779000/7400423 [09:37<03:00, 8982.28\nexamples/s]', '\\rMap:  78%|#######8  | 5781000/7400423 [09:37<02:49, 9545.56\nexamples/s]', '\\rMap:  78%|#######8  | 5783000/7400423 [09:37<02:44, 9859.17\nexamples/s]', '\\rMap:  78%|#######8  | 5785000/7400423 [09:37<02:32, 10566.18\nexamples/s]', '\\rMap:  78%|#######8  | 5787000/7400423 [09:37<02:25, 11075.38\nexamples/s]', '\\rMap:  78%|#######8  | 5789000/7400423 [09:38<02:21, 11359.51\nexamples/s]', '\\rMap:  78%|#######8  | 5791000/7400423 [09:38<02:18, 11651.15\nexamples/s]', '\\rMap:  78%|#######8  | 5793000/7400423 [09:38<02:16, 11763.62\nexamples/s]', '\\rMap:  78%|#######8  | 5795000/7400423 [09:38<02:15, 11824.68\nexamples/s]', '\\rMap:  78%|#######8  | 5797000/7400423 [09:38<02:17, 11690.77\nexamples/s]', '\\rMap:  78%|#######8  | 5799000/7400423 [09:38<02:18, 11524.92\nexamples/s]', '\\rMap:  78%|#######8  | 5801000/7400423 [09:39<02:18, 11560.44\nexamples/s]', '\\rMap:  78%|#######8  | 5803000/7400423 [09:39<02:18, 11574.20\nexamples/s]', '\\rMap:  78%|#######8  | 5805000/7400423 [09:39<02:20, 11394.15\nexamples/s]', '\\rMap:  78%|#######8  | 5807000/7400423 [09:39<02:23, 11094.49\nexamples/s]', '\\rMap:  78%|#######8  | 5809000/7400423 [09:39<02:25, 10952.75\nexamples/s]', '\\rMap:  79%|#######8  | 5811000/7400423 [09:40<02:28, 10687.18\nexamples/s]', '\\rMap:  79%|#######8  | 5813000/7400423 [09:40<02:28, 10679.24\nexamples/s]', '\\rMap:  79%|#######8  | 5815000/7400423 [09:40<02:34, 10260.97\nexamples/s]', '\\rMap:  79%|#######8  | 5817000/7400423 [09:40<02:40, 9878.56\nexamples/s] ', '\\rMap:  79%|#######8  | 5818000/7400423 [09:40<02:42, 9761.20\nexamples/s]', '\\rMap:  79%|#######8  | 5819000/7400423 [09:40<02:42, 9750.81\nexamples/s]', '\\rMap:  79%|#######8  | 5820000/7400423 [09:40<02:41, 9768.90\nexamples/s]', '\\rMap:  79%|#######8  | 5821000/7400423 [09:41<02:42, 9740.15\nexamples/s]', '\\rMap:  79%|#######8  | 5822000/7400423 [09:41<02:43, 9678.62\nexamples/s]', '\\rMap:  79%|#######8  | 5823000/7400423 [09:41<02:43, 9654.99\nexamples/s]', '\\rMap:  79%|#######8  | 5824000/7400423 [09:41<02:43, 9626.51\nexamples/s]', '\\rMap:  79%|#######8  | 5825000/7400423 [09:41<02:44, 9571.00\nexamples/s]', '\\rMap:  79%|#######8  | 5826000/7400423 [09:41<02:44, 9572.25\nexamples/s]', '\\rMap:  79%|#######8  | 5827000/7400423 [09:41<02:42, 9674.63\nexamples/s]', '\\rMap:  79%|#######8  | 5828000/7400423 [09:41<02:41, 9709.68\nexamples/s]', '\\rMap:  79%|#######8  | 5829000/7400423 [09:41<02:41, 9707.39\nexamples/s]', '\\rMap:  79%|#######8  | 5830000/7400423 [09:41<02:42, 9671.26\nexamples/s]', '\\rMap:  79%|#######8  | 5831000/7400423 [09:42<02:45, 9454.43\nexamples/s]', '\\rMap:  79%|#######8  | 5832000/7400423 [09:42<02:47, 9391.51\nexamples/s]', '\\rMap:  79%|#######8  | 5833000/7400423 [09:42<02:49, 9222.92\nexamples/s]', '\\rMap:  79%|#######8  | 5834000/7400423 [09:42<02:50, 9184.23\nexamples/s]', '\\rMap:  79%|#######8  | 5835000/7400423 [09:42<02:51, 9124.51\nexamples/s]', '\\rMap:  79%|#######8  | 5836000/7400423 [09:42<02:50, 9170.61\nexamples/s]', '\\rMap:  79%|#######8  | 5837000/7400423 [09:42<02:49, 9234.37\nexamples/s]', '\\rMap:  79%|#######8  | 5838000/7400423 [09:42<02:50, 9142.52\nexamples/s]', '\\rMap:  79%|#######8  | 5839000/7400423 [09:42<02:50, 9175.43\nexamples/s]', '\\rMap:  79%|#######8  | 5840000/7400423 [09:43<02:51, 9081.01\nexamples/s]', '\\rMap:  79%|#######8  | 5841000/7400423 [09:43<02:59, 8696.80\nexamples/s]', '\\rMap:  79%|#######8  | 5842000/7400423 [09:43<03:02, 8521.25\nexamples/s]', '\\rMap:  79%|#######8  | 5843000/7400423 [09:43<03:03, 8489.68\nexamples/s]', '\\rMap:  79%|#######8  | 5844000/7400423 [09:43<03:00, 8623.11\nexamples/s]', '\\rMap:  79%|#######8  | 5845000/7400423 [09:43<02:58, 8692.96\nexamples/s]', '\\rMap:  79%|#######8  | 5846000/7400423 [09:43<02:53, 8959.79\nexamples/s]', '\\rMap:  79%|#######9  | 5847000/7400423 [09:43<02:51, 9072.54\nexamples/s]', '\\rMap:  79%|#######9  | 5848000/7400423 [09:43<02:47, 9293.97\nexamples/s]', '\\rMap:  79%|#######9  | 5849000/7400423 [09:44<02:45, 9401.75\nexamples/s]', '\\rMap:  79%|#######9  | 5850000/7400423 [09:44<02:46, 9324.26\nexamples/s]', '\\rMap:  79%|#######9  | 5852000/7400423 [09:44<02:29, 10323.03\nexamples/s]', '\\rMap:  79%|#######9  | 5854000/7400423 [09:44<02:20, 11016.54\nexamples/s]', '\\rMap:  79%|#######9  | 5856000/7400423 [09:44<02:14, 11505.75\nexamples/s]', '\\rMap:  79%|#######9  | 5858000/7400423 [09:44<02:14, 11441.54\nexamples/s]', '\\rMap:  79%|#######9  | 5860000/7400423 [09:45<02:16, 11313.59\nexamples/s]', '\\rMap:  79%|#######9  | 5862000/7400423 [09:45<02:18, 11068.75\nexamples/s]', '\\rMap:  79%|#######9  | 5864000/7400423 [09:45<02:21, 10878.00\nexamples/s]', '\\rMap:  79%|#######9  | 5866000/7400423 [09:45<02:23, 10690.78\nexamples/s]', '\\rMap:  79%|#######9  | 5868000/7400423 [09:45<02:26, 10437.80\nexamples/s]', '\\rMap:  79%|#######9  | 5870000/7400423 [09:46<02:31, 10124.81\nexamples/s]', '\\rMap:  79%|#######9  | 5872000/7400423 [09:46<02:31, 10091.27\nexamples/s]', '\\rMap:  79%|#######9  | 5874000/7400423 [09:46<02:30, 10115.24\nexamples/s]', '\\rMap:  79%|#######9  | 5876000/7400423 [09:46<03:16, 7744.49\nexamples/s] ', '\\rMap:  79%|#######9  | 5878000/7400423 [09:47<03:00, 8436.50\nexamples/s]', '\\rMap:  79%|#######9  | 5880000/7400423 [09:47<02:48, 8999.81\nexamples/s]', '\\rMap:  79%|#######9  | 5882000/7400423 [09:47<02:43, 9298.25\nexamples/s]', '\\rMap:  80%|#######9  | 5884000/7400423 [09:47<02:39, 9524.34\nexamples/s]', '\\rMap:  80%|#######9  | 5886000/7400423 [09:47<02:36, 9647.45\nexamples/s]', '\\rMap:  80%|#######9  | 5887000/7400423 [09:47<02:36, 9671.64\nexamples/s]', '\\rMap:  80%|#######9  | 5888000/7400423 [09:48<02:36, 9669.17\nexamples/s]', '\\rMap:  80%|#######9  | 5889000/7400423 [09:48<02:35, 9708.96\nexamples/s]', '\\rMap:  80%|#######9  | 5891000/7400423 [09:48<02:33, 9835.13\nexamples/s]', '\\rMap:  80%|#######9  | 5892000/7400423 [09:48<02:33, 9816.55\nexamples/s]', '\\rMap:  80%|#######9  | 5894000/7400423 [09:48<02:28, 10141.49\nexamples/s]', '\\rMap:  80%|#######9  | 5896000/7400423 [09:48<02:25, 10334.07\nexamples/s]', '\\rMap:  80%|#######9  | 5898000/7400423 [09:48<02:25, 10317.91\nexamples/s]', '\\rMap:  80%|#######9  | 5900000/7400423 [09:49<02:24, 10406.12\nexamples/s]', '\\rMap:  80%|#######9  | 5902000/7400423 [09:49<02:24, 10372.02\nexamples/s]', '\\rMap:  80%|#######9  | 5904000/7400423 [09:49<02:28, 10082.60\nexamples/s]', '\\rMap:  80%|#######9  | 5906000/7400423 [09:49<02:30, 9946.54\nexamples/s] ', '\\rMap:  80%|#######9  | 5908000/7400423 [09:49<02:28, 10036.36\nexamples/s]', '\\rMap:  80%|#######9  | 5910000/7400423 [09:50<02:31, 9854.34\nexamples/s] ', '\\rMap:  80%|#######9  | 5911000/7400423 [09:50<02:32, 9736.65\nexamples/s]', '\\rMap:  80%|#######9  | 5912000/7400423 [09:50<02:32, 9755.66\nexamples/s]', '\\rMap:  80%|#######9  | 5913000/7400423 [09:50<02:32, 9753.64\nexamples/s]', '\\rMap:  80%|#######9  | 5915000/7400423 [09:50<02:29, 9939.65\nexamples/s]', '\\rMap:  80%|#######9  | 5917000/7400423 [09:50<02:27, 10026.13\nexamples/s]', '\\rMap:  80%|#######9  | 5919000/7400423 [09:51<02:27, 10059.75\nexamples/s]', '\\rMap:  80%|########  | 5921000/7400423 [09:51<02:26, 10118.54\nexamples/s]', '\\rMap:  80%|########  | 5923000/7400423 [09:51<02:26, 10051.05\nexamples/s]', '\\rMap:  80%|########  | 5925000/7400423 [09:51<02:27, 9974.21\nexamples/s] ', '\\rMap:  80%|########  | 5926000/7400423 [09:51<02:28, 9953.28\nexamples/s]', '\\rMap:  80%|########  | 5928000/7400423 [09:51<02:26, 10065.73\nexamples/s]', '\\rMap:  80%|########  | 5930000/7400423 [09:52<02:23, 10213.51\nexamples/s]', '\\rMap:  80%|########  | 5932000/7400423 [09:52<02:22, 10297.33\nexamples/s]', '\\rMap:  80%|########  | 5934000/7400423 [09:52<02:21, 10329.00\nexamples/s]', '\\rMap:  80%|########  | 5936000/7400423 [09:52<02:24, 10165.83\nexamples/s]', '\\rMap:  80%|########  | 5938000/7400423 [09:52<02:22, 10297.28\nexamples/s]', '\\rMap:  80%|########  | 5940000/7400423 [09:53<02:21, 10311.26\nexamples/s]', '\\rMap:  80%|########  | 5942000/7400423 [09:53<02:22, 10264.75\nexamples/s]', '\\rMap:  80%|########  | 5944000/7400423 [09:53<02:20, 10343.92\nexamples/s]', '\\rMap:  80%|########  | 5946000/7400423 [09:53<02:19, 10453.70\nexamples/s]', '\\rMap:  80%|########  | 5948000/7400423 [09:53<02:17, 10573.81\nexamples/s]', '\\rMap:  80%|########  | 5950000/7400423 [09:54<02:16, 10597.30\nexamples/s]', '\\rMap:  80%|########  | 5952000/7400423 [09:54<02:16, 10646.69\nexamples/s]', '\\rMap:  80%|########  | 5954000/7400423 [09:54<02:12, 10895.11\nexamples/s]', '\\rMap:  80%|########  | 5956000/7400423 [09:54<02:10, 11072.16\nexamples/s]', '\\rMap:  81%|########  | 5958000/7400423 [09:54<02:26, 9867.66\nexamples/s] ', '\\rMap:  81%|########  | 5960000/7400423 [09:55<02:34, 9298.31\nexamples/s]', '\\rMap:  81%|########  | 5961000/7400423 [09:55<02:35, 9228.69\nexamples/s]', '\\rMap:  81%|########  | 5962000/7400423 [09:55<02:38, 9101.67\nexamples/s]', '\\rMap:  81%|########  | 5963000/7400423 [09:55<03:47, 6320.54\nexamples/s]', '\\rMap:  81%|########  | 5964000/7400423 [09:55<03:38, 6572.91\nexamples/s]', '\\rMap:  81%|########  | 5965000/7400423 [09:55<03:26, 6955.76\nexamples/s]', '\\rMap:  81%|########  | 5966000/7400423 [09:56<03:20, 7148.65\nexamples/s]', '\\rMap:  81%|########  | 5967000/7400423 [09:56<03:14, 7366.75\nexamples/s]', '\\rMap:  81%|########  | 5968000/7400423 [09:56<03:10, 7511.53\nexamples/s]', '\\rMap:  81%|########  | 5969000/7400423 [09:56<03:06, 7661.33\nexamples/s]', '\\rMap:  81%|########  | 5970000/7400423 [09:56<03:05, 7704.30\nexamples/s]', '\\rMap:  81%|########  | 5971000/7400423 [09:56<03:04, 7766.62\nexamples/s]', '\\rMap:  81%|########  | 5972000/7400423 [09:56<03:03, 7774.00\nexamples/s]', '\\rMap:  81%|########  | 5973000/7400423 [09:56<02:59, 7942.32\nexamples/s]', '\\rMap:  81%|########  | 5974000/7400423 [09:57<02:58, 7987.23\nexamples/s]', '\\rMap:  81%|########  | 5975000/7400423 [09:57<02:54, 8153.12\nexamples/s]', '\\rMap:  81%|########  | 5976000/7400423 [09:57<02:52, 8236.52\nexamples/s]', '\\rMap:  81%|########  | 5977000/7400423 [09:57<02:48, 8447.37\nexamples/s]', '\\rMap:  81%|########  | 5978000/7400423 [09:57<02:45, 8580.24\nexamples/s]', '\\rMap:  81%|########  | 5979000/7400423 [09:57<02:47, 8476.62\nexamples/s]', '\\rMap:  81%|########  | 5980000/7400423 [09:57<02:45, 8590.69\nexamples/s]', '\\rMap:  81%|########  | 5981000/7400423 [09:57<02:48, 8443.16\nexamples/s]', '\\rMap:  81%|########  | 5982000/7400423 [09:57<02:50, 8320.86\nexamples/s]', '\\rMap:  81%|########  | 5983000/7400423 [09:58<02:53, 8184.16\nexamples/s]', '\\rMap:  81%|########  | 5984000/7400423 [09:58<02:50, 8286.59\nexamples/s]', '\\rMap:  81%|########  | 5985000/7400423 [09:58<02:44, 8613.00\nexamples/s]', '\\rMap:  81%|########  | 5986000/7400423 [09:58<02:38, 8898.44\nexamples/s]', '\\rMap:  81%|########  | 5987000/7400423 [09:58<02:37, 8956.75\nexamples/s]', '\\rMap:  81%|########  | 5989000/7400423 [09:58<02:29, 9429.42\nexamples/s]', '\\rMap:  81%|########  | 5991000/7400423 [09:58<02:26, 9622.83\nexamples/s]', '\\rMap:  81%|########  | 5993000/7400423 [09:59<02:25, 9688.56\nexamples/s]', '\\rMap:  81%|########1 | 5995000/7400423 [09:59<02:22, 9857.47\nexamples/s]', '\\rMap:  81%|########1 | 5997000/7400423 [09:59<02:20, 9989.73\nexamples/s]', '\\rMap:  81%|########1 | 5999000/7400423 [09:59<02:19, 10053.09\nexamples/s]', '\\rMap:  81%|########1 | 6001000/7400423 [09:59<02:17, 10199.58\nexamples/s]', '\\rMap:  81%|########1 | 6003000/7400423 [10:00<02:16, 10207.22\nexamples/s]', '\\rMap:  81%|########1 | 6005000/7400423 [10:00<02:20, 9966.27\nexamples/s] ', '\\rMap:  81%|########1 | 6007000/7400423 [10:00<02:17, 10141.76\nexamples/s]', '\\rMap:  81%|########1 | 6009000/7400423 [10:00<02:15, 10286.03\nexamples/s]', '\\rMap:  81%|########1 | 6011000/7400423 [10:00<02:14, 10320.32\nexamples/s]', '\\rMap:  81%|########1 | 6013000/7400423 [10:01<02:13, 10371.95\nexamples/s]', '\\rMap:  81%|########1 | 6015000/7400423 [10:01<02:17, 10065.27\nexamples/s]', '\\rMap:  81%|########1 | 6017000/7400423 [10:01<02:20, 9877.20\nexamples/s] ', '\\rMap:  81%|########1 | 6018000/7400423 [10:01<02:23, 9638.07\nexamples/s]', '\\rMap:  81%|########1 | 6019000/7400423 [10:01<02:25, 9514.12\nexamples/s]', '\\rMap:  81%|########1 | 6020000/7400423 [10:01<02:28, 9284.58\nexamples/s]', '\\rMap:  81%|########1 | 6021000/7400423 [10:01<02:30, 9170.26\nexamples/s]', '\\rMap:  81%|########1 | 6022000/7400423 [10:02<02:32, 9048.03\nexamples/s]', '\\rMap:  81%|########1 | 6023000/7400423 [10:02<02:31, 9069.82\nexamples/s]', '\\rMap:  81%|########1 | 6024000/7400423 [10:02<02:31, 9070.99\nexamples/s]', '\\rMap:  81%|########1 | 6025000/7400423 [10:02<02:31, 9066.82\nexamples/s]', '\\rMap:  81%|########1 | 6026000/7400423 [10:02<02:29, 9187.38\nexamples/s]', '\\rMap:  81%|########1 | 6027000/7400423 [10:02<02:27, 9295.72\nexamples/s]', '\\rMap:  81%|########1 | 6028000/7400423 [10:02<02:25, 9413.69\nexamples/s]', '\\rMap:  81%|########1 | 6029000/7400423 [10:02<02:25, 9457.07\nexamples/s]', '\\rMap:  81%|########1 | 6030000/7400423 [10:02<02:23, 9571.61\nexamples/s]', '\\rMap:  81%|########1 | 6031000/7400423 [10:03<02:21, 9656.67\nexamples/s]', '\\rMap:  82%|########1 | 6032000/7400423 [10:03<02:21, 9657.98\nexamples/s]', '\\rMap:  82%|########1 | 6034000/7400423 [10:03<02:20, 9723.32\nexamples/s]', '\\rMap:  82%|########1 | 6036000/7400423 [10:03<02:17, 9893.13\nexamples/s]', '\\rMap:  82%|########1 | 6038000/7400423 [10:03<02:12, 10265.98\nexamples/s]', '\\rMap:  82%|########1 | 6040000/7400423 [10:04<02:59, 7561.59\nexamples/s] ', '\\rMap:  82%|########1 | 6042000/7400423 [10:04<02:44, 8264.68\nexamples/s]', '\\rMap:  82%|########1 | 6043000/7400423 [10:04<02:40, 8459.38\nexamples/s]', '\\rMap:  82%|########1 | 6044000/7400423 [10:04<02:39, 8529.23\nexamples/s]', '\\rMap:  82%|########1 | 6045000/7400423 [10:04<02:38, 8565.36\nexamples/s]', '\\rMap:  82%|########1 | 6046000/7400423 [10:04<02:38, 8531.08\nexamples/s]', '\\rMap:  82%|########1 | 6047000/7400423 [10:04<02:36, 8639.34\nexamples/s]', '\\rMap:  82%|########1 | 6048000/7400423 [10:05<02:36, 8664.80\nexamples/s]', '\\rMap:  82%|########1 | 6049000/7400423 [10:05<02:33, 8808.18\nexamples/s]', '\\rMap:  82%|########1 | 6050000/7400423 [10:05<02:28, 9074.16\nexamples/s]', '\\rMap:  82%|########1 | 6051000/7400423 [10:05<02:28, 9057.65\nexamples/s]', '\\rMap:  82%|########1 | 6052000/7400423 [10:05<02:26, 9217.42\nexamples/s]', '\\rMap:  82%|########1 | 6053000/7400423 [10:05<02:27, 9137.32\nexamples/s]', '\\rMap:  82%|########1 | 6054000/7400423 [10:05<02:26, 9217.89\nexamples/s]', '\\rMap:  82%|########1 | 6055000/7400423 [10:05<02:22, 9412.74\nexamples/s]', '\\rMap:  82%|########1 | 6056000/7400423 [10:05<02:20, 9541.10\nexamples/s]', '\\rMap:  82%|########1 | 6057000/7400423 [10:05<02:22, 9442.54\nexamples/s]', '\\rMap:  82%|########1 | 6058000/7400423 [10:06<02:28, 9053.41\nexamples/s]', '\\rMap:  82%|########1 | 6059000/7400423 [10:06<02:30, 8927.41\nexamples/s]', '\\rMap:  82%|########1 | 6060000/7400423 [10:06<02:30, 8880.51\nexamples/s]', '\\rMap:  82%|########1 | 6061000/7400423 [10:06<02:31, 8840.15\nexamples/s]', '\\rMap:  82%|########1 | 6062000/7400423 [10:06<02:33, 8728.17\nexamples/s]', '\\rMap:  82%|########1 | 6063000/7400423 [10:06<02:31, 8834.15\nexamples/s]', '\\rMap:  82%|########1 | 6064000/7400423 [10:06<02:27, 9053.52\nexamples/s]', '\\rMap:  82%|########1 | 6065000/7400423 [10:06<02:25, 9184.11\nexamples/s]', '\\rMap:  82%|########1 | 6066000/7400423 [10:06<02:25, 9145.38\nexamples/s]', '\\rMap:  82%|########1 | 6067000/7400423 [10:07<02:26, 9105.42\nexamples/s]', '\\rMap:  82%|########1 | 6068000/7400423 [10:07<02:28, 9000.36\nexamples/s]', '\\rMap:  82%|########2 | 6069000/7400423 [10:07<02:24, 9188.57\nexamples/s]', '\\rMap:  82%|########2 | 6070000/7400423 [10:07<02:27, 9023.93\nexamples/s]', '\\rMap:  82%|########2 | 6071000/7400423 [10:07<02:27, 9039.65\nexamples/s]', '\\rMap:  82%|########2 | 6072000/7400423 [10:07<02:25, 9148.42\nexamples/s]', '\\rMap:  82%|########2 | 6073000/7400423 [10:07<02:21, 9380.83\nexamples/s]', '\\rMap:  82%|########2 | 6075000/7400423 [10:07<02:19, 9527.49\nexamples/s]', '\\rMap:  82%|########2 | 6076000/7400423 [10:08<02:26, 9053.55\nexamples/s]', '\\rMap:  82%|########2 | 6077000/7400423 [10:08<02:26, 9012.81\nexamples/s]', '\\rMap:  82%|########2 | 6078000/7400423 [10:08<02:28, 8886.34\nexamples/s]', '\\rMap:  82%|########2 | 6079000/7400423 [10:08<02:28, 8883.01\nexamples/s]', '\\rMap:  82%|########2 | 6080000/7400423 [10:08<02:26, 9035.59\nexamples/s]', '\\rMap:  82%|########2 | 6081000/7400423 [10:08<02:26, 8998.00\nexamples/s]', '\\rMap:  82%|########2 | 6082000/7400423 [10:08<02:32, 8671.26\nexamples/s]', '\\rMap:  82%|########2 | 6083000/7400423 [10:08<02:30, 8741.96\nexamples/s]', '\\rMap:  82%|########2 | 6085000/7400423 [10:09<02:20, 9339.29\nexamples/s]', '\\rMap:  82%|########2 | 6087000/7400423 [10:09<02:13, 9826.65\nexamples/s]', '\\rMap:  82%|########2 | 6088000/7400423 [10:09<02:14, 9777.51\nexamples/s]', '\\rMap:  82%|########2 | 6089000/7400423 [10:09<02:15, 9693.42\nexamples/s]', '\\rMap:  82%|########2 | 6091000/7400423 [10:09<02:07, 10231.10\nexamples/s]', '\\rMap:  82%|########2 | 6093000/7400423 [10:09<02:02, 10674.28\nexamples/s]', '\\rMap:  82%|########2 | 6095000/7400423 [10:09<02:01, 10777.04\nexamples/s]', '\\rMap:  82%|########2 | 6097000/7400423 [10:10<02:01, 10764.47\nexamples/s]', '\\rMap:  82%|########2 | 6099000/7400423 [10:10<01:58, 10997.68\nexamples/s]', '\\rMap:  82%|########2 | 6101000/7400423 [10:10<01:56, 11181.37\nexamples/s]', '\\rMap:  82%|########2 | 6103000/7400423 [10:10<01:55, 11264.96\nexamples/s]', '\\rMap:  82%|########2 | 6105000/7400423 [10:10<01:54, 11266.90\nexamples/s]', '\\rMap:  83%|########2 | 6107000/7400423 [10:11<01:57, 11044.63\nexamples/s]', '\\rMap:  83%|########2 | 6109000/7400423 [10:11<02:00, 10735.11\nexamples/s]', '\\rMap:  83%|########2 | 6111000/7400423 [10:11<02:00, 10691.58\nexamples/s]', '\\rMap:  83%|########2 | 6113000/7400423 [10:11<02:01, 10601.78\nexamples/s]', '\\rMap:  83%|########2 | 6115000/7400423 [10:11<02:01, 10578.91\nexamples/s]', '\\rMap:  83%|########2 | 6117000/7400423 [10:12<02:02, 10467.35\nexamples/s]', '\\rMap:  83%|########2 | 6119000/7400423 [10:12<02:00, 10659.92\nexamples/s]', '\\rMap:  83%|########2 | 6121000/7400423 [10:12<02:01, 10497.90\nexamples/s]', '\\rMap:  83%|########2 | 6123000/7400423 [10:12<02:00, 10604.27\nexamples/s]', '\\rMap:  83%|########2 | 6125000/7400423 [10:12<02:01, 10495.56\nexamples/s]', '\\rMap:  83%|########2 | 6127000/7400423 [10:12<02:00, 10541.27\nexamples/s]', '\\rMap:  83%|########2 | 6129000/7400423 [10:13<02:00, 10569.02\nexamples/s]', '\\rMap:  83%|########2 | 6131000/7400423 [10:13<01:57, 10828.81\nexamples/s]', '\\rMap:  83%|########2 | 6133000/7400423 [10:13<01:58, 10730.47\nexamples/s]', '\\rMap:  83%|########2 | 6135000/7400423 [10:13<01:56, 10850.25\nexamples/s]', '\\rMap:  83%|########2 | 6137000/7400423 [10:13<01:56, 10801.47\nexamples/s]', '\\rMap:  83%|########2 | 6139000/7400423 [10:14<01:56, 10847.87\nexamples/s]', '\\rMap:  83%|########2 | 6141000/7400423 [10:14<01:56, 10830.83\nexamples/s]', '\\rMap:  83%|########3 | 6143000/7400423 [10:14<01:55, 10894.93\nexamples/s]', '\\rMap:  83%|########3 | 6145000/7400423 [10:14<01:54, 10988.50\nexamples/s]', '\\rMap:  83%|########3 | 6147000/7400423 [10:14<01:53, 11078.81\nexamples/s]', '\\rMap:  83%|########3 | 6149000/7400423 [10:15<01:56, 10716.28\nexamples/s]', '\\rMap:  83%|########3 | 6151000/7400423 [10:15<01:57, 10625.02\nexamples/s]', '\\rMap:  83%|########3 | 6153000/7400423 [10:15<01:58, 10526.47\nexamples/s]', '\\rMap:  83%|########3 | 6155000/7400423 [10:15<01:58, 10472.11\nexamples/s]', '\\rMap:  83%|########3 | 6157000/7400423 [10:15<02:02, 10153.27\nexamples/s]', '\\rMap:  83%|########3 | 6159000/7400423 [10:15<02:02, 10139.13\nexamples/s]', '\\rMap:  83%|########3 | 6161000/7400423 [10:16<01:59, 10406.62\nexamples/s]', '\\rMap:  83%|########3 | 6163000/7400423 [10:16<01:56, 10598.21\nexamples/s]', '\\rMap:  83%|########3 | 6165000/7400423 [10:16<02:30, 8209.55\nexamples/s] ', '\\rMap:  83%|########3 | 6167000/7400423 [10:16<02:21, 8704.97\nexamples/s]', '\\rMap:  83%|########3 | 6169000/7400423 [10:17<02:14, 9167.61\nexamples/s]', '\\rMap:  83%|########3 | 6170000/7400423 [10:17<02:13, 9203.27\nexamples/s]', '\\rMap:  83%|########3 | 6171000/7400423 [10:17<02:12, 9252.63\nexamples/s]', '\\rMap:  83%|########3 | 6173000/7400423 [10:17<02:06, 9696.68\nexamples/s]', '\\rMap:  83%|########3 | 6175000/7400423 [10:17<02:03, 9884.35\nexamples/s]', '\\rMap:  83%|########3 | 6177000/7400423 [10:17<02:00, 10123.54\nexamples/s]', '\\rMap:  83%|########3 | 6179000/7400423 [10:18<01:58, 10267.31\nexamples/s]', '\\rMap:  84%|########3 | 6181000/7400423 [10:18<01:55, 10554.09\nexamples/s]', '\\rMap:  84%|########3 | 6183000/7400423 [10:18<01:53, 10756.06\nexamples/s]', '\\rMap:  84%|########3 | 6185000/7400423 [10:18<01:53, 10739.01\nexamples/s]', '\\rMap:  84%|########3 | 6187000/7400423 [10:18<01:52, 10775.90\nexamples/s]', '\\rMap:  84%|########3 | 6189000/7400423 [10:18<01:49, 11109.30\nexamples/s]', '\\rMap:  84%|########3 | 6191000/7400423 [10:19<01:49, 11011.67\nexamples/s]', '\\rMap:  84%|########3 | 6193000/7400423 [10:19<01:52, 10707.79\nexamples/s]', '\\rMap:  84%|########3 | 6195000/7400423 [10:19<01:52, 10741.22\nexamples/s]', '\\rMap:  84%|########3 | 6197000/7400423 [10:19<01:54, 10511.70\nexamples/s]', '\\rMap:  84%|########3 | 6199000/7400423 [10:19<01:55, 10426.00\nexamples/s]', '\\rMap:  84%|########3 | 6201000/7400423 [10:20<01:55, 10426.18\nexamples/s]', '\\rMap:  84%|########3 | 6203000/7400423 [10:20<01:53, 10541.50\nexamples/s]', '\\rMap:  84%|########3 | 6205000/7400423 [10:20<01:53, 10549.56\nexamples/s]', '\\rMap:  84%|########3 | 6207000/7400423 [10:20<01:53, 10560.53\nexamples/s]', '\\rMap:  84%|########3 | 6209000/7400423 [10:20<01:53, 10481.35\nexamples/s]', '\\rMap:  84%|########3 | 6211000/7400423 [10:21<01:53, 10455.35\nexamples/s]', '\\rMap:  84%|########3 | 6213000/7400423 [10:21<01:53, 10426.13\nexamples/s]', '\\rMap:  84%|########3 | 6215000/7400423 [10:21<01:56, 10205.17\nexamples/s]', '\\rMap:  84%|########4 | 6217000/7400423 [10:21<01:55, 10243.35\nexamples/s]', '\\rMap:  84%|########4 | 6219000/7400423 [10:21<01:55, 10266.06\nexamples/s]', '\\rMap:  84%|########4 | 6221000/7400423 [10:22<01:54, 10263.21\nexamples/s]', '\\rMap:  84%|########4 | 6223000/7400423 [10:22<01:53, 10392.33\nexamples/s]', '\\rMap:  84%|########4 | 6225000/7400423 [10:22<01:52, 10471.41\nexamples/s]', '\\rMap:  84%|########4 | 6227000/7400423 [10:22<01:49, 10729.17\nexamples/s]', '\\rMap:  84%|########4 | 6229000/7400423 [10:22<01:50, 10602.50\nexamples/s]', '\\rMap:  84%|########4 | 6231000/7400423 [10:23<01:50, 10625.58\nexamples/s]', '\\rMap:  84%|########4 | 6233000/7400423 [10:23<01:47, 10821.45\nexamples/s]', '\\rMap:  84%|########4 | 6235000/7400423 [10:23<01:48, 10770.57\nexamples/s]', '\\rMap:  84%|########4 | 6237000/7400423 [10:23<01:48, 10745.78\nexamples/s]', '\\rMap:  84%|########4 | 6239000/7400423 [10:23<01:48, 10710.89\nexamples/s]', '\\rMap:  84%|########4 | 6241000/7400423 [10:23<01:47, 10752.80\nexamples/s]', '\\rMap:  84%|########4 | 6243000/7400423 [10:24<01:47, 10810.69\nexamples/s]', '\\rMap:  84%|########4 | 6245000/7400423 [10:24<01:47, 10737.50\nexamples/s]', '\\rMap:  84%|########4 | 6247000/7400423 [10:24<01:47, 10757.86\nexamples/s]', '\\rMap:  84%|########4 | 6249000/7400423 [10:24<01:44, 10977.89\nexamples/s]', '\\rMap:  84%|########4 | 6251000/7400423 [10:24<01:48, 10630.23\nexamples/s]', '\\rMap:  84%|########4 | 6253000/7400423 [10:25<01:48, 10543.14\nexamples/s]', '\\rMap:  85%|########4 | 6255000/7400423 [10:25<01:50, 10365.07\nexamples/s]', '\\rMap:  85%|########4 | 6257000/7400423 [10:25<01:57, 9698.39\nexamples/s] ', '\\rMap:  85%|########4 | 6258000/7400423 [10:25<01:59, 9577.77\nexamples/s]', '\\rMap:  85%|########4 | 6259000/7400423 [10:25<01:59, 9515.38\nexamples/s]', '\\rMap:  85%|########4 | 6260000/7400423 [10:25<02:01, 9403.03\nexamples/s]', '\\rMap:  85%|########4 | 6261000/7400423 [10:25<02:00, 9426.35\nexamples/s]', '\\rMap:  85%|########4 | 6262000/7400423 [10:26<02:05, 9084.53\nexamples/s]', '\\rMap:  85%|########4 | 6263000/7400423 [10:26<02:08, 8847.16\nexamples/s]', '\\rMap:  85%|########4 | 6264000/7400423 [10:26<03:17, 5750.38\nexamples/s]', '\\rMap:  85%|########4 | 6265000/7400423 [10:26<02:59, 6324.53\nexamples/s]', '\\rMap:  85%|########4 | 6266000/7400423 [10:26<02:43, 6934.81\nexamples/s]', '\\rMap:  85%|########4 | 6267000/7400423 [10:26<02:33, 7392.12\nexamples/s]', '\\rMap:  85%|########4 | 6268000/7400423 [10:26<02:26, 7744.59\nexamples/s]', '\\rMap:  85%|########4 | 6269000/7400423 [10:27<02:22, 7953.60\nexamples/s]', '\\rMap:  85%|########4 | 6270000/7400423 [10:27<02:21, 7967.82\nexamples/s]', '\\rMap:  85%|########4 | 6271000/7400423 [10:27<02:18, 8182.93\nexamples/s]', '\\rMap:  85%|########4 | 6272000/7400423 [10:27<02:13, 8458.32\nexamples/s]', '\\rMap:  85%|########4 | 6273000/7400423 [10:27<02:14, 8408.05\nexamples/s]', '\\rMap:  85%|########4 | 6274000/7400423 [10:27<02:08, 8783.43\nexamples/s]', '\\rMap:  85%|########4 | 6275000/7400423 [10:27<02:09, 8720.30\nexamples/s]', '\\rMap:  85%|########4 | 6276000/7400423 [10:27<02:09, 8679.06\nexamples/s]', '\\rMap:  85%|########4 | 6277000/7400423 [10:27<02:07, 8831.94\nexamples/s]', '\\rMap:  85%|########4 | 6278000/7400423 [10:28<02:08, 8767.79\nexamples/s]', '\\rMap:  85%|########4 | 6279000/7400423 [10:28<02:08, 8704.62\nexamples/s]', '\\rMap:  85%|########4 | 6280000/7400423 [10:28<02:05, 8925.89\nexamples/s]', '\\rMap:  85%|########4 | 6281000/7400423 [10:28<02:05, 8918.69\nexamples/s]', '\\rMap:  85%|########4 | 6282000/7400423 [10:28<02:05, 8884.11\nexamples/s]', '\\rMap:  85%|########4 | 6283000/7400423 [10:28<02:06, 8830.67\nexamples/s]', '\\rMap:  85%|########4 | 6284000/7400423 [10:28<02:04, 8987.55\nexamples/s]', '\\rMap:  85%|########4 | 6285000/7400423 [10:28<02:02, 9076.04\nexamples/s]', '\\rMap:  85%|########4 | 6286000/7400423 [10:28<02:02, 9090.18\nexamples/s]', '\\rMap:  85%|########4 | 6287000/7400423 [10:29<02:01, 9143.26\nexamples/s]', '\\rMap:  85%|########4 | 6288000/7400423 [10:29<02:00, 9212.63\nexamples/s]', '\\rMap:  85%|########4 | 6289000/7400423 [10:29<02:03, 8993.69\nexamples/s]', '\\rMap:  85%|########4 | 6290000/7400423 [10:29<02:04, 8890.08\nexamples/s]', '\\rMap:  85%|########5 | 6291000/7400423 [10:29<02:02, 9051.20\nexamples/s]', '\\rMap:  85%|########5 | 6292000/7400423 [10:29<02:04, 8887.23\nexamples/s]', '\\rMap:  85%|########5 | 6293000/7400423 [10:29<02:04, 8870.00\nexamples/s]', '\\rMap:  85%|########5 | 6294000/7400423 [10:29<02:02, 9002.29\nexamples/s]', '\\rMap:  85%|########5 | 6295000/7400423 [10:29<02:01, 9117.44\nexamples/s]', '\\rMap:  85%|########5 | 6297000/7400423 [10:30<01:52, 9825.27\nexamples/s]', '\\rMap:  85%|########5 | 6299000/7400423 [10:30<01:50, 9959.82\nexamples/s]', '\\rMap:  85%|########5 | 6301000/7400423 [10:30<01:44, 10485.49\nexamples/s]', '\\rMap:  85%|########5 | 6303000/7400423 [10:30<01:43, 10577.31\nexamples/s]', '\\rMap:  85%|########5 | 6305000/7400423 [10:30<01:43, 10594.64\nexamples/s]', '\\rMap:  85%|########5 | 6307000/7400423 [10:31<01:43, 10612.27\nexamples/s]', '\\rMap:  85%|########5 | 6309000/7400423 [10:31<01:42, 10656.49\nexamples/s]', '\\rMap:  85%|########5 | 6311000/7400423 [10:31<01:43, 10531.51\nexamples/s]', '\\rMap:  85%|########5 | 6313000/7400423 [10:31<01:43, 10496.78\nexamples/s]', '\\rMap:  85%|########5 | 6315000/7400423 [10:31<01:39, 10860.32\nexamples/s]', '\\rMap:  85%|########5 | 6317000/7400423 [10:32<01:36, 11202.59\nexamples/s]', '\\rMap:  85%|########5 | 6319000/7400423 [10:32<01:34, 11481.19\nexamples/s]', '\\rMap:  85%|########5 | 6321000/7400423 [10:32<01:34, 11412.20\nexamples/s]', '\\rMap:  85%|########5 | 6323000/7400423 [10:32<01:32, 11701.87\nexamples/s]', '\\rMap:  85%|########5 | 6325000/7400423 [10:32<01:37, 11056.73\nexamples/s]', '\\rMap:  85%|########5 | 6327000/7400423 [10:32<01:38, 10878.04\nexamples/s]', '\\rMap:  86%|########5 | 6329000/7400423 [10:33<01:41, 10536.92\nexamples/s]', '\\rMap:  86%|########5 | 6331000/7400423 [10:33<01:41, 10556.86\nexamples/s]', '\\rMap:  86%|########5 | 6333000/7400423 [10:33<01:42, 10369.08\nexamples/s]', '\\rMap:  86%|########5 | 6335000/7400423 [10:33<01:40, 10628.54\nexamples/s]', '\\rMap:  86%|########5 | 6337000/7400423 [10:33<01:39, 10689.96\nexamples/s]', '\\rMap:  86%|########5 | 6339000/7400423 [10:34<01:38, 10812.18\nexamples/s]', '\\rMap:  86%|########5 | 6341000/7400423 [10:34<01:39, 10635.32\nexamples/s]', '\\rMap:  86%|########5 | 6343000/7400423 [10:34<01:42, 10312.10\nexamples/s]', '\\rMap:  86%|########5 | 6345000/7400423 [10:34<01:45, 10040.30\nexamples/s]', '\\rMap:  86%|########5 | 6347000/7400423 [10:34<01:44, 10065.76\nexamples/s]', '\\rMap:  86%|########5 | 6349000/7400423 [10:35<01:44, 10089.10\nexamples/s]', '\\rMap:  86%|########5 | 6351000/7400423 [10:35<02:14, 7789.01\nexamples/s] ', '\\rMap:  86%|########5 | 6353000/7400423 [10:35<02:04, 8432.34\nexamples/s]', '\\rMap:  86%|########5 | 6355000/7400423 [10:35<01:55, 9017.96\nexamples/s]', '\\rMap:  86%|########5 | 6356000/7400423 [10:35<01:53, 9167.68\nexamples/s]', '\\rMap:  86%|########5 | 6357000/7400423 [10:36<01:52, 9289.97\nexamples/s]', '\\rMap:  86%|########5 | 6359000/7400423 [10:36<01:46, 9795.64\nexamples/s]', '\\rMap:  86%|########5 | 6361000/7400423 [10:36<01:43, 10079.69\nexamples/s]', '\\rMap:  86%|########5 | 6363000/7400423 [10:36<01:40, 10346.04\nexamples/s]', '\\rMap:  86%|########6 | 6365000/7400423 [10:36<01:38, 10563.66\nexamples/s]', '\\rMap:  86%|########6 | 6367000/7400423 [10:36<01:39, 10420.23\nexamples/s]', '\\rMap:  86%|########6 | 6369000/7400423 [10:37<01:37, 10574.75\nexamples/s]', '\\rMap:  86%|########6 | 6371000/7400423 [10:37<01:36, 10683.35\nexamples/s]', '\\rMap:  86%|########6 | 6373000/7400423 [10:37<01:35, 10779.37\nexamples/s]', '\\rMap:  86%|########6 | 6375000/7400423 [10:37<01:34, 10882.67\nexamples/s]', '\\rMap:  86%|########6 | 6377000/7400423 [10:37<01:34, 10792.49\nexamples/s]', '\\rMap:  86%|########6 | 6379000/7400423 [10:38<01:34, 10800.78\nexamples/s]', '\\rMap:  86%|########6 | 6381000/7400423 [10:38<01:33, 10954.83\nexamples/s]', '\\rMap:  86%|########6 | 6383000/7400423 [10:38<01:32, 11040.63\nexamples/s]', '\\rMap:  86%|########6 | 6385000/7400423 [10:38<01:32, 11029.95\nexamples/s]', '\\rMap:  86%|########6 | 6387000/7400423 [10:38<01:32, 10987.37\nexamples/s]', '\\rMap:  86%|########6 | 6389000/7400423 [10:38<01:30, 11141.74\nexamples/s]', '\\rMap:  86%|########6 | 6391000/7400423 [10:39<01:30, 11182.79\nexamples/s]', '\\rMap:  86%|########6 | 6393000/7400423 [10:39<01:29, 11305.08\nexamples/s]', '\\rMap:  86%|########6 | 6395000/7400423 [10:39<01:28, 11415.81\nexamples/s]', '\\rMap:  86%|########6 | 6397000/7400423 [10:39<01:28, 11335.78\nexamples/s]', '\\rMap:  86%|########6 | 6399000/7400423 [10:39<01:30, 11095.51\nexamples/s]', '\\rMap:  86%|########6 | 6401000/7400423 [10:40<01:31, 10977.70\nexamples/s]', '\\rMap:  87%|########6 | 6403000/7400423 [10:40<01:31, 10869.66\nexamples/s]', '\\rMap:  87%|########6 | 6405000/7400423 [10:40<01:31, 10915.57\nexamples/s]', '\\rMap:  87%|########6 | 6407000/7400423 [10:40<01:31, 10810.06\nexamples/s]', '\\rMap:  87%|########6 | 6409000/7400423 [10:40<01:31, 10873.86\nexamples/s]', '\\rMap:  87%|########6 | 6411000/7400423 [10:40<01:30, 10953.65\nexamples/s]', '\\rMap:  87%|########6 | 6413000/7400423 [10:41<01:29, 11065.53\nexamples/s]', '\\rMap:  87%|########6 | 6415000/7400423 [10:41<01:29, 11055.56\nexamples/s]', '\\rMap:  87%|########6 | 6417000/7400423 [10:41<01:30, 10926.15\nexamples/s]', '\\rMap:  87%|########6 | 6419000/7400423 [10:41<01:29, 10981.40\nexamples/s]', '\\rMap:  87%|########6 | 6421000/7400423 [10:41<01:29, 10970.36\nexamples/s]', '\\rMap:  87%|########6 | 6423000/7400423 [10:42<01:30, 10838.67\nexamples/s]', '\\rMap:  87%|########6 | 6425000/7400423 [10:42<01:29, 10933.26\nexamples/s]', '\\rMap:  87%|########6 | 6427000/7400423 [10:42<01:56, 8356.01\nexamples/s] ', '\\rMap:  87%|########6 | 6428000/7400423 [10:42<01:53, 8570.31\nexamples/s]', '\\rMap:  87%|########6 | 6429000/7400423 [10:42<01:51, 8682.85\nexamples/s]', '\\rMap:  87%|########6 | 6431000/7400423 [10:42<01:42, 9480.55\nexamples/s]', '\\rMap:  87%|########6 | 6433000/7400423 [10:43<01:37, 9888.33\nexamples/s]', '\\rMap:  87%|########6 | 6435000/7400423 [10:43<01:33, 10282.02\nexamples/s]', '\\rMap:  87%|########6 | 6437000/7400423 [10:43<01:31, 10567.58\nexamples/s]', '\\rMap:  87%|########7 | 6439000/7400423 [10:43<01:29, 10690.20\nexamples/s]', '\\rMap:  87%|########7 | 6441000/7400423 [10:43<01:28, 10832.96\nexamples/s]', '\\rMap:  87%|########7 | 6443000/7400423 [10:44<01:27, 10891.95\nexamples/s]', '\\rMap:  87%|########7 | 6445000/7400423 [10:44<01:27, 10980.70\nexamples/s]', '\\rMap:  87%|########7 | 6447000/7400423 [10:44<01:26, 11019.37\nexamples/s]', '\\rMap:  87%|########7 | 6449000/7400423 [10:44<01:24, 11207.14\nexamples/s]', '\\rMap:  87%|########7 | 6451000/7400423 [10:44<01:24, 11180.45\nexamples/s]', '\\rMap:  87%|########7 | 6453000/7400423 [10:44<01:25, 11098.61\nexamples/s]', '\\rMap:  87%|########7 | 6455000/7400423 [10:45<01:25, 11085.96\nexamples/s]', '\\rMap:  87%|########7 | 6457000/7400423 [10:45<01:24, 11100.19\nexamples/s]', '\\rMap:  87%|########7 | 6459000/7400423 [10:45<01:24, 11126.69\nexamples/s]', '\\rMap:  87%|########7 | 6461000/7400423 [10:45<01:24, 11092.84\nexamples/s]', '\\rMap:  87%|########7 | 6463000/7400423 [10:45<01:22, 11313.45\nexamples/s]', '\\rMap:  87%|########7 | 6465000/7400423 [10:46<01:22, 11398.51\nexamples/s]', '\\rMap:  87%|########7 | 6467000/7400423 [10:46<01:20, 11541.46\nexamples/s]', '\\rMap:  87%|########7 | 6469000/7400423 [10:46<01:20, 11592.58\nexamples/s]', '\\rMap:  87%|########7 | 6471000/7400423 [10:46<01:19, 11753.44\nexamples/s]', '\\rMap:  87%|########7 | 6473000/7400423 [10:46<01:23, 11063.90\nexamples/s]', '\\rMap:  87%|########7 | 6475000/7400423 [10:46<01:24, 10967.55\nexamples/s]', '\\rMap:  88%|########7 | 6477000/7400423 [10:47<01:24, 10936.78\nexamples/s]', '\\rMap:  88%|########7 | 6479000/7400423 [10:47<01:24, 10906.50\nexamples/s]', '\\rMap:  88%|########7 | 6481000/7400423 [10:47<01:26, 10610.40\nexamples/s]', '\\rMap:  88%|########7 | 6483000/7400423 [10:47<01:27, 10533.01\nexamples/s]', '\\rMap:  88%|########7 | 6485000/7400423 [10:47<01:27, 10492.54\nexamples/s]', '\\rMap:  88%|########7 | 6487000/7400423 [10:48<01:27, 10487.88\nexamples/s]', '\\rMap:  88%|########7 | 6489000/7400423 [10:48<01:27, 10359.26\nexamples/s]', '\\rMap:  88%|########7 | 6491000/7400423 [10:48<01:27, 10351.63\nexamples/s]', '\\rMap:  88%|########7 | 6493000/7400423 [10:48<01:27, 10333.06\nexamples/s]', '\\rMap:  88%|########7 | 6495000/7400423 [10:48<01:26, 10447.16\nexamples/s]', '\\rMap:  88%|########7 | 6497000/7400423 [10:49<01:25, 10570.46\nexamples/s]', '\\rMap:  88%|########7 | 6499000/7400423 [10:49<01:24, 10628.92\nexamples/s]', '\\rMap:  88%|########7 | 6501000/7400423 [10:49<01:23, 10741.08\nexamples/s]', '\\rMap:  88%|########7 | 6503000/7400423 [10:49<01:21, 10974.87\nexamples/s]', '\\rMap:  88%|########7 | 6505000/7400423 [10:49<01:22, 10882.50\nexamples/s]', '\\rMap:  88%|########7 | 6507000/7400423 [10:49<01:22, 10885.61\nexamples/s]', '\\rMap:  88%|########7 | 6509000/7400423 [10:50<01:22, 10837.36\nexamples/s]', '\\rMap:  88%|########7 | 6511000/7400423 [10:50<01:21, 10961.87\nexamples/s]', '\\rMap:  88%|########8 | 6513000/7400423 [10:50<01:21, 10948.98\nexamples/s]', '\\rMap:  88%|########8 | 6515000/7400423 [10:50<01:25, 10388.77\nexamples/s]', '\\rMap:  88%|########8 | 6517000/7400423 [10:50<01:26, 10247.42\nexamples/s]', '\\rMap:  88%|########8 | 6519000/7400423 [10:51<01:27, 10109.55\nexamples/s]', '\\rMap:  88%|########8 | 6521000/7400423 [10:51<01:27, 10064.32\nexamples/s]', '\\rMap:  88%|########8 | 6523000/7400423 [10:51<01:26, 10106.60\nexamples/s]', '\\rMap:  88%|########8 | 6525000/7400423 [10:51<01:26, 10136.07\nexamples/s]', '\\rMap:  88%|########8 | 6527000/7400423 [10:51<01:26, 10124.30\nexamples/s]', '\\rMap:  88%|########8 | 6529000/7400423 [10:52<01:24, 10289.58\nexamples/s]', '\\rMap:  88%|########8 | 6531000/7400423 [10:52<01:25, 10117.72\nexamples/s]', '\\rMap:  88%|########8 | 6533000/7400423 [10:52<01:25, 10088.45\nexamples/s]', '\\rMap:  88%|########8 | 6535000/7400423 [10:52<01:25, 10151.07\nexamples/s]', '\\rMap:  88%|########8 | 6537000/7400423 [10:52<01:23, 10351.24\nexamples/s]', '\\rMap:  88%|########8 | 6539000/7400423 [10:53<01:21, 10542.84\nexamples/s]', '\\rMap:  88%|########8 | 6541000/7400423 [10:53<01:21, 10530.56\nexamples/s]', '\\rMap:  88%|########8 | 6543000/7400423 [10:53<01:19, 10736.29\nexamples/s]', '\\rMap:  88%|########8 | 6545000/7400423 [10:53<01:18, 10855.12\nexamples/s]', '\\rMap:  88%|########8 | 6547000/7400423 [10:53<01:17, 10955.37\nexamples/s]', '\\rMap:  88%|########8 | 6549000/7400423 [10:53<01:17, 10971.77\nexamples/s]', '\\rMap:  89%|########8 | 6551000/7400423 [10:54<01:16, 11110.84\nexamples/s]', '\\rMap:  89%|########8 | 6553000/7400423 [10:54<01:40, 8422.36\nexamples/s] ', '\\rMap:  89%|########8 | 6555000/7400423 [10:54<01:31, 9255.15\nexamples/s]', '\\rMap:  89%|########8 | 6557000/7400423 [10:54<01:27, 9664.14\nexamples/s]', '\\rMap:  89%|########8 | 6559000/7400423 [10:55<01:24, 9988.10\nexamples/s]', '\\rMap:  89%|########8 | 6561000/7400423 [10:55<01:23, 10052.40\nexamples/s]', '\\rMap:  89%|########8 | 6563000/7400423 [10:55<01:22, 10181.78\nexamples/s]', '\\rMap:  89%|########8 | 6565000/7400423 [10:55<01:18, 10681.15\nexamples/s]', '\\rMap:  89%|########8 | 6567000/7400423 [10:55<01:16, 10870.71\nexamples/s]', '\\rMap:  89%|########8 | 6569000/7400423 [10:55<01:15, 10943.23\nexamples/s]', '\\rMap:  89%|########8 | 6571000/7400423 [10:56<01:14, 11125.27\nexamples/s]', '\\rMap:  89%|########8 | 6573000/7400423 [10:56<01:12, 11396.67\nexamples/s]', '\\rMap:  89%|########8 | 6575000/7400423 [10:56<01:12, 11319.81\nexamples/s]', '\\rMap:  89%|########8 | 6577000/7400423 [10:56<01:12, 11338.21\nexamples/s]', '\\rMap:  89%|########8 | 6579000/7400423 [10:56<01:12, 11346.80\nexamples/s]', '\\rMap:  89%|########8 | 6581000/7400423 [10:56<01:12, 11275.16\nexamples/s]', '\\rMap:  89%|########8 | 6583000/7400423 [10:57<01:13, 11077.69\nexamples/s]', '\\rMap:  89%|########8 | 6585000/7400423 [10:57<01:11, 11343.37\nexamples/s]', '\\rMap:  89%|########9 | 6587000/7400423 [10:57<01:10, 11516.30\nexamples/s]', '\\rMap:  89%|########9 | 6589000/7400423 [10:57<01:09, 11705.12\nexamples/s]', '\\rMap:  89%|########9 | 6591000/7400423 [10:57<01:08, 11871.87\nexamples/s]', '\\rMap:  89%|########9 | 6593000/7400423 [10:58<01:11, 11276.73\nexamples/s]', '\\rMap:  89%|########9 | 6595000/7400423 [10:58<01:13, 10951.97\nexamples/s]', '\\rMap:  89%|########9 | 6597000/7400423 [10:58<01:14, 10840.59\nexamples/s]', '\\rMap:  89%|########9 | 6599000/7400423 [10:58<01:13, 10867.68\nexamples/s]', '\\rMap:  89%|########9 | 6601000/7400423 [10:58<01:13, 10893.63\nexamples/s]', '\\rMap:  89%|########9 | 6603000/7400423 [10:58<01:11, 11145.62\nexamples/s]', '\\rMap:  89%|########9 | 6605000/7400423 [10:59<01:11, 11064.70\nexamples/s]', '\\rMap:  89%|########9 | 6607000/7400423 [10:59<01:11, 11103.62\nexamples/s]', '\\rMap:  89%|########9 | 6609000/7400423 [10:59<01:10, 11193.68\nexamples/s]', '\\rMap:  89%|########9 | 6611000/7400423 [10:59<01:10, 11179.60\nexamples/s]', '\\rMap:  89%|########9 | 6613000/7400423 [10:59<01:09, 11307.05\nexamples/s]', '\\rMap:  89%|########9 | 6615000/7400423 [11:00<01:11, 11019.39\nexamples/s]', '\\rMap:  89%|########9 | 6617000/7400423 [11:00<01:13, 10636.54\nexamples/s]', '\\rMap:  89%|########9 | 6619000/7400423 [11:00<01:15, 10409.20\nexamples/s]', '\\rMap:  89%|########9 | 6621000/7400423 [11:00<01:14, 10401.79\nexamples/s]', '\\rMap:  89%|########9 | 6623000/7400423 [11:00<01:14, 10396.61\nexamples/s]', '\\rMap:  90%|########9 | 6625000/7400423 [11:01<01:13, 10593.62\nexamples/s]', '\\rMap:  90%|########9 | 6627000/7400423 [11:01<01:10, 10930.99\nexamples/s]', '\\rMap:  90%|########9 | 6629000/7400423 [11:01<01:09, 11035.62\nexamples/s]', '\\rMap:  90%|########9 | 6631000/7400423 [11:01<01:08, 11236.14\nexamples/s]', '\\rMap:  90%|########9 | 6633000/7400423 [11:01<01:07, 11357.62\nexamples/s]', '\\rMap:  90%|########9 | 6635000/7400423 [11:01<01:06, 11540.33\nexamples/s]', '\\rMap:  90%|########9 | 6637000/7400423 [11:02<01:09, 11049.99\nexamples/s]', '\\rMap:  90%|########9 | 6639000/7400423 [11:02<01:10, 10842.92\nexamples/s]', '\\rMap:  90%|########9 | 6641000/7400423 [11:02<01:11, 10649.52\nexamples/s]', '\\rMap:  90%|########9 | 6643000/7400423 [11:02<01:11, 10537.10\nexamples/s]', '\\rMap:  90%|########9 | 6645000/7400423 [11:02<01:12, 10383.93\nexamples/s]', '\\rMap:  90%|########9 | 6647000/7400423 [11:03<01:14, 10145.90\nexamples/s]', '\\rMap:  90%|########9 | 6649000/7400423 [11:03<01:14, 10034.76\nexamples/s]', '\\rMap:  90%|########9 | 6651000/7400423 [11:03<01:16, 9833.09\nexamples/s] ', '\\rMap:  90%|########9 | 6652000/7400423 [11:03<01:41, 7359.57\nexamples/s]', '\\rMap:  90%|########9 | 6653000/7400423 [11:03<01:36, 7757.34\nexamples/s]', '\\rMap:  90%|########9 | 6655000/7400423 [11:04<01:28, 8430.08\nexamples/s]', '\\rMap:  90%|########9 | 6656000/7400423 [11:04<01:25, 8676.92\nexamples/s]', '\\rMap:  90%|########9 | 6657000/7400423 [11:04<01:23, 8924.83\nexamples/s]', '\\rMap:  90%|########9 | 6658000/7400423 [11:04<01:21, 9081.99\nexamples/s]', '\\rMap:  90%|########9 | 6660000/7400423 [11:04<01:14, 9969.30\nexamples/s]', '\\rMap:  90%|######### | 6662000/7400423 [11:04<01:10, 10541.13\nexamples/s]', '\\rMap:  90%|######### | 6664000/7400423 [11:04<01:07, 10934.43\nexamples/s]', '\\rMap:  90%|######### | 6666000/7400423 [11:05<01:04, 11330.93\nexamples/s]', '\\rMap:  90%|######### | 6668000/7400423 [11:05<01:03, 11613.66\nexamples/s]', '\\rMap:  90%|######### | 6670000/7400423 [11:05<01:02, 11649.55\nexamples/s]', '\\rMap:  90%|######### | 6672000/7400423 [11:05<01:03, 11561.79\nexamples/s]', '\\rMap:  90%|######### | 6674000/7400423 [11:05<01:03, 11362.44\nexamples/s]', '\\rMap:  90%|######### | 6676000/7400423 [11:05<01:04, 11165.48\nexamples/s]', '\\rMap:  90%|######### | 6678000/7400423 [11:06<01:05, 11108.98\nexamples/s]', '\\rMap:  90%|######### | 6680000/7400423 [11:06<01:05, 11029.71\nexamples/s]', '\\rMap:  90%|######### | 6682000/7400423 [11:06<01:06, 10729.17\nexamples/s]', '\\rMap:  90%|######### | 6684000/7400423 [11:06<01:07, 10597.81\nexamples/s]', '\\rMap:  90%|######### | 6686000/7400423 [11:06<01:07, 10559.49\nexamples/s]', '\\rMap:  90%|######### | 6688000/7400423 [11:07<01:08, 10450.28\nexamples/s]', '\\rMap:  90%|######### | 6690000/7400423 [11:07<01:07, 10494.30\nexamples/s]', '\\rMap:  90%|######### | 6692000/7400423 [11:07<01:05, 10772.70\nexamples/s]', '\\rMap:  90%|######### | 6694000/7400423 [11:07<01:04, 11023.03\nexamples/s]', '\\rMap:  90%|######### | 6696000/7400423 [11:07<01:03, 11165.40\nexamples/s]', '\\rMap:  91%|######### | 6698000/7400423 [11:07<01:03, 11084.51\nexamples/s]', '\\rMap:  91%|######### | 6700000/7400423 [11:08<01:03, 11077.76\nexamples/s]', '\\rMap:  91%|######### | 6702000/7400423 [11:08<01:04, 10762.56\nexamples/s]', '\\rMap:  91%|######### | 6704000/7400423 [11:08<01:04, 10768.05\nexamples/s]', '\\rMap:  91%|######### | 6706000/7400423 [11:08<01:04, 10836.52\nexamples/s]', '\\rMap:  91%|######### | 6708000/7400423 [11:08<01:03, 10873.91\nexamples/s]', '\\rMap:  91%|######### | 6710000/7400423 [11:09<01:03, 10818.70\nexamples/s]', '\\rMap:  91%|######### | 6712000/7400423 [11:09<01:05, 10523.55\nexamples/s]', '\\rMap:  91%|######### | 6714000/7400423 [11:09<01:06, 10372.34\nexamples/s]', '\\rMap:  91%|######### | 6716000/7400423 [11:09<01:06, 10267.53\nexamples/s]', '\\rMap:  91%|######### | 6718000/7400423 [11:09<01:06, 10293.50\nexamples/s]', '\\rMap:  91%|######### | 6720000/7400423 [11:10<01:06, 10231.51\nexamples/s]', '\\rMap:  91%|######### | 6722000/7400423 [11:10<01:04, 10477.33\nexamples/s]', '\\rMap:  91%|######### | 6724000/7400423 [11:10<01:03, 10735.28\nexamples/s]', '\\rMap:  91%|######### | 6726000/7400423 [11:10<01:03, 10647.31\nexamples/s]', '\\rMap:  91%|######### | 6728000/7400423 [11:10<01:04, 10477.69\nexamples/s]', '\\rMap:  91%|######### | 6730000/7400423 [11:11<01:04, 10401.78\nexamples/s]', '\\rMap:  91%|######### | 6732000/7400423 [11:11<01:04, 10366.54\nexamples/s]', '\\rMap:  91%|######### | 6734000/7400423 [11:11<01:03, 10499.61\nexamples/s]', '\\rMap:  91%|#########1| 6736000/7400423 [11:11<01:04, 10359.35\nexamples/s]', '\\rMap:  91%|#########1| 6738000/7400423 [11:11<01:03, 10372.61\nexamples/s]', '\\rMap:  91%|#########1| 6740000/7400423 [11:12<01:24, 7828.79\nexamples/s] ', '\\rMap:  91%|#########1| 6742000/7400423 [11:12<01:17, 8475.97\nexamples/s]', '\\rMap:  91%|#########1| 6744000/7400423 [11:12<01:13, 8983.88\nexamples/s]', '\\rMap:  91%|#########1| 6745000/7400423 [11:12<01:11, 9146.85\nexamples/s]', '\\rMap:  91%|#########1| 6746000/7400423 [11:12<01:10, 9296.83\nexamples/s]', '\\rMap:  91%|#########1| 6748000/7400423 [11:12<01:08, 9586.72\nexamples/s]', '\\rMap:  91%|#########1| 6750000/7400423 [11:13<01:06, 9791.35\nexamples/s]', '\\rMap:  91%|#########1| 6752000/7400423 [11:13<01:05, 9830.30\nexamples/s]', '\\rMap:  91%|#########1| 6753000/7400423 [11:13<01:06, 9802.89\nexamples/s]', '\\rMap:  91%|#########1| 6754000/7400423 [11:13<01:06, 9716.27\nexamples/s]', '\\rMap:  91%|#########1| 6756000/7400423 [11:13<01:05, 9847.47\nexamples/s]', '\\rMap:  91%|#########1| 6757000/7400423 [11:13<01:06, 9715.17\nexamples/s]', '\\rMap:  91%|#########1| 6759000/7400423 [11:14<01:05, 9864.68\nexamples/s]', '\\rMap:  91%|#########1| 6760000/7400423 [11:14<01:05, 9808.76\nexamples/s]', '\\rMap:  91%|#########1| 6761000/7400423 [11:14<01:05, 9744.09\nexamples/s]', '\\rMap:  91%|#########1| 6763000/7400423 [11:14<01:05, 9718.58\nexamples/s]', '\\rMap:  91%|#########1| 6765000/7400423 [11:14<01:04, 9777.93\nexamples/s]', '\\rMap:  91%|#########1| 6767000/7400423 [11:14<01:04, 9886.58\nexamples/s]', '\\rMap:  91%|#########1| 6769000/7400423 [11:15<01:03, 9932.95\nexamples/s]', '\\rMap:  91%|#########1| 6770000/7400423 [11:15<01:04, 9792.45\nexamples/s]', '\\rMap:  91%|#########1| 6771000/7400423 [11:15<01:05, 9645.56\nexamples/s]', '\\rMap:  92%|#########1| 6772000/7400423 [11:15<01:05, 9554.04\nexamples/s]', '\\rMap:  92%|#########1| 6773000/7400423 [11:15<01:05, 9622.60\nexamples/s]', '\\rMap:  92%|#########1| 6774000/7400423 [11:15<01:05, 9608.32\nexamples/s]', '\\rMap:  92%|#########1| 6776000/7400423 [11:15<01:03, 9905.34\nexamples/s]', '\\rMap:  92%|#########1| 6778000/7400423 [11:16<01:00, 10347.83\nexamples/s]', '\\rMap:  92%|#########1| 6780000/7400423 [11:16<00:58, 10658.32\nexamples/s]', '\\rMap:  92%|#########1| 6782000/7400423 [11:16<00:56, 10900.15\nexamples/s]', '\\rMap:  92%|#########1| 6784000/7400423 [11:16<00:55, 11094.59\nexamples/s]', '\\rMap:  92%|#########1| 6786000/7400423 [11:16<00:54, 11304.70\nexamples/s]', '\\rMap:  92%|#########1| 6788000/7400423 [11:16<00:53, 11473.02\nexamples/s]', '\\rMap:  92%|#########1| 6790000/7400423 [11:17<00:53, 11501.02\nexamples/s]', '\\rMap:  92%|#########1| 6792000/7400423 [11:17<00:52, 11503.97\nexamples/s]', '\\rMap:  92%|#########1| 6794000/7400423 [11:17<00:55, 10842.59\nexamples/s]', '\\rMap:  92%|#########1| 6796000/7400423 [11:17<00:58, 10360.18\nexamples/s]', '\\rMap:  92%|#########1| 6798000/7400423 [11:17<00:57, 10444.39\nexamples/s]', '\\rMap:  92%|#########1| 6800000/7400423 [11:18<00:58, 10264.66\nexamples/s]', '\\rMap:  92%|#########1| 6802000/7400423 [11:18<00:58, 10307.67\nexamples/s]', '\\rMap:  92%|#########1| 6804000/7400423 [11:18<00:56, 10527.80\nexamples/s]', '\\rMap:  92%|#########1| 6806000/7400423 [11:18<00:54, 10942.46\nexamples/s]', '\\rMap:  92%|#########1| 6808000/7400423 [11:18<00:56, 10488.39\nexamples/s]', '\\rMap:  92%|#########2| 6810000/7400423 [11:18<00:57, 10184.50\nexamples/s]', '\\rMap:  92%|#########2| 6812000/7400423 [11:19<00:59, 9960.73\nexamples/s] ', '\\rMap:  92%|#########2| 6814000/7400423 [11:19<00:58, 10001.40\nexamples/s]', '\\rMap:  92%|#########2| 6816000/7400423 [11:19<01:15, 7790.22\nexamples/s] ', '\\rMap:  92%|#########2| 6818000/7400423 [11:19<01:07, 8594.18\nexamples/s]', '\\rMap:  92%|#########2| 6819000/7400423 [11:20<01:11, 8114.00\nexamples/s]', '\\rMap:  92%|#########2| 6820000/7400423 [11:20<01:12, 8022.50\nexamples/s]', '\\rMap:  92%|#########2| 6821000/7400423 [11:20<01:12, 8003.65\nexamples/s]', '\\rMap:  92%|#########2| 6822000/7400423 [11:20<01:14, 7722.93\nexamples/s]', '\\rMap:  92%|#########2| 6823000/7400423 [11:20<01:13, 7853.99\nexamples/s]', '\\rMap:  92%|#########2| 6824000/7400423 [11:20<01:12, 7968.48\nexamples/s]', '\\rMap:  92%|#########2| 6825000/7400423 [11:20<01:10, 8134.89\nexamples/s]', '\\rMap:  92%|#########2| 6826000/7400423 [11:20<01:09, 8304.44\nexamples/s]', '\\rMap:  92%|#########2| 6827000/7400423 [11:21<01:09, 8236.52\nexamples/s]', '\\rMap:  92%|#########2| 6828000/7400423 [11:21<01:09, 8255.14\nexamples/s]', '\\rMap:  92%|#########2| 6829000/7400423 [11:21<01:10, 8061.63\nexamples/s]', '\\rMap:  92%|#########2| 6830000/7400423 [11:21<01:08, 8335.41\nexamples/s]', '\\rMap:  92%|#########2| 6831000/7400423 [11:21<01:09, 8193.35\nexamples/s]', '\\rMap:  92%|#########2| 6832000/7400423 [11:21<01:09, 8178.18\nexamples/s]', '\\rMap:  92%|#########2| 6833000/7400423 [11:21<01:10, 8061.08\nexamples/s]', '\\rMap:  92%|#########2| 6834000/7400423 [11:21<01:10, 8021.82\nexamples/s]', '\\rMap:  92%|#########2| 6836000/7400423 [11:22<00:59, 9537.81\nexamples/s]', '\\rMap:  92%|#########2| 6838000/7400423 [11:22<00:53, 10523.57\nexamples/s]', '\\rMap:  92%|#########2| 6840000/7400423 [11:22<00:49, 11258.16\nexamples/s]', '\\rMap:  92%|#########2| 6842000/7400423 [11:22<00:47, 11772.42\nexamples/s]', '\\rMap:  92%|#########2| 6844000/7400423 [11:22<00:47, 11794.82\nexamples/s]', '\\rMap:  93%|#########2| 6846000/7400423 [11:22<00:47, 11677.49\nexamples/s]', '\\rMap:  93%|#########2| 6848000/7400423 [11:23<00:46, 11825.68\nexamples/s]', '\\rMap:  93%|#########2| 6850000/7400423 [11:23<00:46, 11827.07\nexamples/s]', '\\rMap:  93%|#########2| 6852000/7400423 [11:23<00:45, 11950.29\nexamples/s]', '\\rMap:  93%|#########2| 6854000/7400423 [11:23<00:45, 12055.58\nexamples/s]', '\\rMap:  93%|#########2| 6856000/7400423 [11:23<00:45, 11995.44\nexamples/s]', '\\rMap:  93%|#########2| 6858000/7400423 [11:23<00:44, 12136.12\nexamples/s]', '\\rMap:  93%|#########2| 6860000/7400423 [11:24<00:46, 11647.36\nexamples/s]', '\\rMap:  93%|#########2| 6862000/7400423 [11:24<00:46, 11470.40\nexamples/s]', '\\rMap:  93%|#########2| 6864000/7400423 [11:24<00:49, 10866.46\nexamples/s]', '\\rMap:  93%|#########2| 6866000/7400423 [11:24<00:51, 10297.06\nexamples/s]', '\\rMap:  93%|#########2| 6868000/7400423 [11:24<00:53, 10013.46\nexamples/s]', '\\rMap:  93%|#########2| 6870000/7400423 [11:25<00:54, 9675.51\nexamples/s] ', '\\rMap:  93%|#########2| 6871000/7400423 [11:25<00:54, 9691.33\nexamples/s]', '\\rMap:  93%|#########2| 6872000/7400423 [11:25<00:54, 9625.84\nexamples/s]', '\\rMap:  93%|#########2| 6873000/7400423 [11:25<00:55, 9562.61\nexamples/s]', '\\rMap:  93%|#########2| 6874000/7400423 [11:25<00:56, 9293.75\nexamples/s]', '\\rMap:  93%|#########2| 6875000/7400423 [11:25<00:57, 9121.39\nexamples/s]', '\\rMap:  93%|#########2| 6876000/7400423 [11:25<00:58, 9021.36\nexamples/s]', '\\rMap:  93%|#########2| 6877000/7400423 [11:25<00:58, 8980.68\nexamples/s]', '\\rMap:  93%|#########2| 6878000/7400423 [11:26<00:58, 8902.64\nexamples/s]', '\\rMap:  93%|#########2| 6879000/7400423 [11:26<00:59, 8750.14\nexamples/s]', '\\rMap:  93%|#########2| 6880000/7400423 [11:26<00:59, 8681.14\nexamples/s]', '\\rMap:  93%|#########2| 6881000/7400423 [11:26<00:59, 8753.86\nexamples/s]', '\\rMap:  93%|#########2| 6882000/7400423 [11:26<00:59, 8732.04\nexamples/s]', '\\rMap:  93%|#########3| 6883000/7400423 [11:26<00:57, 8994.08\nexamples/s]', '\\rMap:  93%|#########3| 6884000/7400423 [11:26<00:55, 9235.52\nexamples/s]', '\\rMap:  93%|#########3| 6885000/7400423 [11:26<00:55, 9287.82\nexamples/s]', '\\rMap:  93%|#########3| 6886000/7400423 [11:26<00:55, 9308.02\nexamples/s]', '\\rMap:  93%|#########3| 6887000/7400423 [11:27<00:55, 9233.55\nexamples/s]', '\\rMap:  93%|#########3| 6888000/7400423 [11:27<00:54, 9327.88\nexamples/s]', '\\rMap:  93%|#########3| 6889000/7400423 [11:27<00:54, 9340.29\nexamples/s]', '\\rMap:  93%|#########3| 6890000/7400423 [11:27<00:54, 9374.82\nexamples/s]', '\\rMap:  93%|#########3| 6891000/7400423 [11:27<00:54, 9378.39\nexamples/s]', '\\rMap:  93%|#########3| 6893000/7400423 [11:27<00:51, 9816.65\nexamples/s]', '\\rMap:  93%|#########3| 6895000/7400423 [11:27<00:49, 10177.38\nexamples/s]', '\\rMap:  93%|#########3| 6897000/7400423 [11:28<00:50, 9982.66\nexamples/s] ', '\\rMap:  93%|#########3| 6898000/7400423 [11:28<00:50, 9958.21\nexamples/s]', '\\rMap:  93%|#########3| 6899000/7400423 [11:28<00:50, 9933.24\nexamples/s]', '\\rMap:  93%|#########3| 6901000/7400423 [11:28<00:50, 9964.47\nexamples/s]', '\\rMap:  93%|#########3| 6903000/7400423 [11:28<00:49, 10079.35\nexamples/s]', '\\rMap:  93%|#########3| 6905000/7400423 [11:28<00:48, 10168.46\nexamples/s]', '\\rMap:  93%|#########3| 6907000/7400423 [11:29<00:48, 10179.89\nexamples/s]', '\\rMap:  93%|#########3| 6909000/7400423 [11:29<00:49, 9893.30\nexamples/s] ', '\\rMap:  93%|#########3| 6911000/7400423 [11:29<00:48, 10004.18\nexamples/s]', '\\rMap:  93%|#########3| 6913000/7400423 [11:29<00:49, 9879.67\nexamples/s] ', '\\rMap:  93%|#########3| 6914000/7400423 [11:29<00:49, 9794.89\nexamples/s]', '\\rMap:  93%|#########3| 6915000/7400423 [11:29<00:49, 9709.36\nexamples/s]', '\\rMap:  93%|#########3| 6917000/7400423 [11:30<00:48, 9878.43\nexamples/s]', '\\rMap:  93%|#########3| 6919000/7400423 [11:30<00:47, 10059.34\nexamples/s]', '\\rMap:  94%|#########3| 6921000/7400423 [11:30<00:48, 9976.93\nexamples/s] ', '\\rMap:  94%|#########3| 6923000/7400423 [11:30<00:47, 9984.53\nexamples/s]', '\\rMap:  94%|#########3| 6925000/7400423 [11:30<00:47, 10075.86\nexamples/s]', '\\rMap:  94%|#########3| 6927000/7400423 [11:31<00:46, 10213.33\nexamples/s]', '\\rMap:  94%|#########3| 6929000/7400423 [11:31<00:45, 10280.64\nexamples/s]', '\\rMap:  94%|#########3| 6931000/7400423 [11:31<00:46, 10067.68\nexamples/s]', '\\rMap:  94%|#########3| 6933000/7400423 [11:31<00:46, 9959.12\nexamples/s] ', '\\rMap:  94%|#########3| 6935000/7400423 [11:31<00:46, 10082.87\nexamples/s]', '\\rMap:  94%|#########3| 6937000/7400423 [11:32<00:45, 10085.11\nexamples/s]', '\\rMap:  94%|#########3| 6939000/7400423 [11:32<00:45, 10193.33\nexamples/s]', '\\rMap:  94%|#########3| 6941000/7400423 [11:32<00:57, 7983.14\nexamples/s] ', '\\rMap:  94%|#########3| 6943000/7400423 [11:32<00:51, 8814.75\nexamples/s]', '\\rMap:  94%|#########3| 6945000/7400423 [11:32<00:49, 9207.71\nexamples/s]', '\\rMap:  94%|#########3| 6946000/7400423 [11:33<00:48, 9290.52\nexamples/s]', '\\rMap:  94%|#########3| 6947000/7400423 [11:33<00:48, 9398.07\nexamples/s]', '\\rMap:  94%|#########3| 6948000/7400423 [11:33<00:47, 9521.74\nexamples/s]', '\\rMap:  94%|#########3| 6950000/7400423 [11:33<00:46, 9747.04\nexamples/s]', '\\rMap:  94%|#########3| 6952000/7400423 [11:33<00:45, 9882.97\nexamples/s]', '\\rMap:  94%|#########3| 6953000/7400423 [11:33<00:45, 9754.12\nexamples/s]', '\\rMap:  94%|#########3| 6954000/7400423 [11:33<00:46, 9607.13\nexamples/s]', '\\rMap:  94%|#########3| 6955000/7400423 [11:34<00:46, 9657.69\nexamples/s]', '\\rMap:  94%|#########3| 6956000/7400423 [11:34<00:46, 9590.51\nexamples/s]', '\\rMap:  94%|#########4| 6957000/7400423 [11:34<00:45, 9646.56\nexamples/s]', '\\rMap:  94%|#########4| 6958000/7400423 [11:34<00:45, 9729.96\nexamples/s]', '\\rMap:  94%|#########4| 6959000/7400423 [11:34<00:45, 9796.17\nexamples/s]', '\\rMap:  94%|#########4| 6961000/7400423 [11:34<00:43, 10173.51\nexamples/s]', '\\rMap:  94%|#########4| 6963000/7400423 [11:34<00:41, 10544.58\nexamples/s]', '\\rMap:  94%|#########4| 6965000/7400423 [11:34<00:40, 10769.88\nexamples/s]', '\\rMap:  94%|#########4| 6967000/7400423 [11:35<00:40, 10764.79\nexamples/s]', '\\rMap:  94%|#########4| 6969000/7400423 [11:35<00:41, 10500.68\nexamples/s]', '\\rMap:  94%|#########4| 6971000/7400423 [11:35<00:41, 10300.02\nexamples/s]', '\\rMap:  94%|#########4| 6973000/7400423 [11:35<00:41, 10338.97\nexamples/s]', '\\rMap:  94%|#########4| 6975000/7400423 [11:35<00:40, 10380.42\nexamples/s]', '\\rMap:  94%|#########4| 6977000/7400423 [11:36<00:40, 10381.01\nexamples/s]', '\\rMap:  94%|#########4| 6979000/7400423 [11:36<00:40, 10405.43\nexamples/s]', '\\rMap:  94%|#########4| 6981000/7400423 [11:36<00:40, 10421.52\nexamples/s]', '\\rMap:  94%|#########4| 6983000/7400423 [11:36<00:39, 10628.75\nexamples/s]', '\\rMap:  94%|#########4| 6985000/7400423 [11:36<00:38, 10875.23\nexamples/s]', '\\rMap:  94%|#########4| 6987000/7400423 [11:37<00:38, 10829.54\nexamples/s]', '\\rMap:  94%|#########4| 6989000/7400423 [11:37<00:37, 10838.72\nexamples/s]', '\\rMap:  94%|#########4| 6991000/7400423 [11:37<00:37, 10995.43\nexamples/s]', '\\rMap:  94%|#########4| 6993000/7400423 [11:37<00:36, 11087.06\nexamples/s]', '\\rMap:  95%|#########4| 6995000/7400423 [11:37<00:36, 11188.24\nexamples/s]', '\\rMap:  95%|#########4| 6997000/7400423 [11:37<00:36, 10949.79\nexamples/s]', '\\rMap:  95%|#########4| 6999000/7400423 [11:38<00:37, 10677.33\nexamples/s]', '\\rMap:  95%|#########4| 7001000/7400423 [11:38<00:37, 10577.80\nexamples/s]', '\\rMap:  95%|#########4| 7003000/7400423 [11:38<00:37, 10664.66\nexamples/s]', '\\rMap:  95%|#########4| 7005000/7400423 [11:38<00:38, 10344.46\nexamples/s]', '\\rMap:  95%|#########4| 7007000/7400423 [11:38<00:38, 10128.27\nexamples/s]', '\\rMap:  95%|#########4| 7009000/7400423 [11:39<00:38, 10214.83\nexamples/s]', '\\rMap:  95%|#########4| 7011000/7400423 [11:39<00:38, 10243.78\nexamples/s]', '\\rMap:  95%|#########4| 7013000/7400423 [11:39<00:37, 10331.87\nexamples/s]', '\\rMap:  95%|#########4| 7015000/7400423 [11:39<00:36, 10564.00\nexamples/s]', '\\rMap:  95%|#########4| 7017000/7400423 [11:39<00:36, 10582.32\nexamples/s]', '\\rMap:  95%|#########4| 7019000/7400423 [11:40<00:36, 10500.41\nexamples/s]', '\\rMap:  95%|#########4| 7021000/7400423 [11:40<00:37, 10248.80\nexamples/s]', '\\rMap:  95%|#########4| 7023000/7400423 [11:40<00:36, 10228.45\nexamples/s]', '\\rMap:  95%|#########4| 7025000/7400423 [11:40<00:36, 10221.28\nexamples/s]', '\\rMap:  95%|#########4| 7027000/7400423 [11:40<00:36, 10341.20\nexamples/s]', '\\rMap:  95%|#########4| 7029000/7400423 [11:41<00:36, 10182.68\nexamples/s]', '\\rMap:  95%|#########5| 7031000/7400423 [11:41<00:36, 10029.12\nexamples/s]', '\\rMap:  95%|#########5| 7033000/7400423 [11:41<00:36, 10055.13\nexamples/s]', '\\rMap:  95%|#########5| 7035000/7400423 [11:41<00:36, 10127.45\nexamples/s]', '\\rMap:  95%|#########5| 7037000/7400423 [11:41<00:36, 10019.94\nexamples/s]', '\\rMap:  95%|#########5| 7039000/7400423 [11:42<00:35, 10152.18\nexamples/s]', '\\rMap:  95%|#########5| 7041000/7400423 [11:42<00:45, 7824.12\nexamples/s] ', '\\rMap:  95%|#########5| 7043000/7400423 [11:42<00:42, 8416.88\nexamples/s]', '\\rMap:  95%|#########5| 7045000/7400423 [11:42<00:39, 8903.59\nexamples/s]', '\\rMap:  95%|#########5| 7047000/7400423 [11:43<00:37, 9431.62\nexamples/s]', '\\rMap:  95%|#########5| 7049000/7400423 [11:43<00:35, 9907.01\nexamples/s]', '\\rMap:  95%|#########5| 7051000/7400423 [11:43<00:34, 10178.63\nexamples/s]', '\\rMap:  95%|#########5| 7053000/7400423 [11:43<00:33, 10409.30\nexamples/s]', '\\rMap:  95%|#########5| 7055000/7400423 [11:43<00:32, 10655.59\nexamples/s]', '\\rMap:  95%|#########5| 7057000/7400423 [11:43<00:30, 11271.02\nexamples/s]', '\\rMap:  95%|#########5| 7059000/7400423 [11:44<00:29, 11654.96\nexamples/s]', '\\rMap:  95%|#########5| 7061000/7400423 [11:44<00:28, 11729.04\nexamples/s]', '\\rMap:  95%|#########5| 7063000/7400423 [11:44<00:29, 11373.37\nexamples/s]', '\\rMap:  95%|#########5| 7065000/7400423 [11:44<00:29, 11285.87\nexamples/s]', '\\rMap:  95%|#########5| 7067000/7400423 [11:44<00:30, 10970.85\nexamples/s]', '\\rMap:  96%|#########5| 7069000/7400423 [11:44<00:30, 10769.32\nexamples/s]', '\\rMap:  96%|#########5| 7071000/7400423 [11:45<00:31, 10492.02\nexamples/s]', '\\rMap:  96%|#########5| 7073000/7400423 [11:45<00:30, 10911.87\nexamples/s]', '\\rMap:  96%|#########5| 7075000/7400423 [11:45<00:28, 11452.14\nexamples/s]', '\\rMap:  96%|#########5| 7077000/7400423 [11:45<00:27, 11843.19\nexamples/s]', '\\rMap:  96%|#########5| 7079000/7400423 [11:45<00:26, 12119.80\nexamples/s]', '\\rMap:  96%|#########5| 7081000/7400423 [11:45<00:25, 12326.22\nexamples/s]', '\\rMap:  96%|#########5| 7083000/7400423 [11:46<00:25, 12423.51\nexamples/s]', '\\rMap:  96%|#########5| 7085000/7400423 [11:46<00:25, 12552.52\nexamples/s]', '\\rMap:  96%|#########5| 7087000/7400423 [11:46<00:24, 12560.58\nexamples/s]', '\\rMap:  96%|#########5| 7089000/7400423 [11:46<00:24, 12660.78\nexamples/s]', '\\rMap:  96%|#########5| 7091000/7400423 [11:46<00:23, 12957.87\nexamples/s]', '\\rMap:  96%|#########5| 7093000/7400423 [11:46<00:23, 13107.96\nexamples/s]', '\\rMap:  96%|#########5| 7095000/7400423 [11:47<00:23, 12961.30\nexamples/s]', '\\rMap:  96%|#########5| 7097000/7400423 [11:47<00:23, 13141.60\nexamples/s]', '\\rMap:  96%|#########5| 7099000/7400423 [11:47<00:22, 13289.60\nexamples/s]', '\\rMap:  96%|#########5| 7101000/7400423 [11:47<00:22, 13350.83\nexamples/s]', '\\rMap:  96%|#########5| 7103000/7400423 [11:47<00:22, 13163.04\nexamples/s]', '\\rMap:  96%|#########6| 7105000/7400423 [11:47<00:22, 13003.96\nexamples/s]', '\\rMap:  96%|#########6| 7107000/7400423 [11:47<00:22, 13010.52\nexamples/s]', '\\rMap:  96%|#########6| 7109000/7400423 [11:48<00:22, 12987.32\nexamples/s]', '\\rMap:  96%|#########6| 7111000/7400423 [11:48<00:22, 12808.11\nexamples/s]', '\\rMap:  96%|#########6| 7113000/7400423 [11:48<00:22, 12930.52\nexamples/s]', '\\rMap:  96%|#########6| 7115000/7400423 [11:48<00:21, 13099.05\nexamples/s]', '\\rMap:  96%|#########6| 7117000/7400423 [11:48<00:22, 12750.69\nexamples/s]', '\\rMap:  96%|#########6| 7119000/7400423 [11:48<00:22, 12349.08\nexamples/s]', '\\rMap:  96%|#########6| 7121000/7400423 [11:49<00:22, 12255.12\nexamples/s]', '\\rMap:  96%|#########6| 7123000/7400423 [11:49<00:22, 12088.36\nexamples/s]', '\\rMap:  96%|#########6| 7125000/7400423 [11:49<00:22, 12205.16\nexamples/s]', '\\rMap:  96%|#########6| 7127000/7400423 [11:49<00:29, 9118.24\nexamples/s] ', '\\rMap:  96%|#########6| 7129000/7400423 [11:49<00:27, 9873.22\nexamples/s]', '\\rMap:  96%|#########6| 7131000/7400423 [11:50<00:25, 10548.17\nexamples/s]', '\\rMap:  96%|#########6| 7133000/7400423 [11:50<00:23, 11222.05\nexamples/s]', '\\rMap:  96%|#########6| 7135000/7400423 [11:50<00:22, 11716.54\nexamples/s]', '\\rMap:  96%|#########6| 7137000/7400423 [11:50<00:22, 11931.90\nexamples/s]', '\\rMap:  96%|#########6| 7139000/7400423 [11:50<00:23, 11237.12\nexamples/s]', '\\rMap:  96%|#########6| 7141000/7400423 [11:50<00:23, 10827.58\nexamples/s]', '\\rMap:  97%|#########6| 7143000/7400423 [11:51<00:24, 10589.44\nexamples/s]', '\\rMap:  97%|#########6| 7145000/7400423 [11:51<00:24, 10405.50\nexamples/s]', '\\rMap:  97%|#########6| 7147000/7400423 [11:51<00:25, 10023.77\nexamples/s]', '\\rMap:  97%|#########6| 7149000/7400423 [11:51<00:25, 9918.26\nexamples/s] ', '\\rMap:  97%|#########6| 7151000/7400423 [11:51<00:24, 10002.44\nexamples/s]', '\\rMap:  97%|#########6| 7153000/7400423 [11:52<00:24, 9900.59\nexamples/s] ', '\\rMap:  97%|#########6| 7154000/7400423 [11:52<00:25, 9822.02\nexamples/s]', '\\rMap:  97%|#########6| 7155000/7400423 [11:52<00:25, 9774.23\nexamples/s]', '\\rMap:  97%|#########6| 7156000/7400423 [11:52<00:25, 9743.73\nexamples/s]', '\\rMap:  97%|#########6| 7157000/7400423 [11:52<00:25, 9666.41\nexamples/s]', '\\rMap:  97%|#########6| 7159000/7400423 [11:52<00:24, 9810.28\nexamples/s]', '\\rMap:  97%|#########6| 7160000/7400423 [11:52<00:24, 9783.81\nexamples/s]', '\\rMap:  97%|#########6| 7161000/7400423 [11:53<00:24, 9663.09\nexamples/s]', '\\rMap:  97%|#########6| 7162000/7400423 [11:53<00:25, 9520.95\nexamples/s]', '\\rMap:  97%|#########6| 7163000/7400423 [11:53<00:25, 9464.06\nexamples/s]', '\\rMap:  97%|#########6| 7164000/7400423 [11:53<00:25, 9435.66\nexamples/s]', '\\rMap:  97%|#########6| 7165000/7400423 [11:53<00:25, 9343.14\nexamples/s]', '\\rMap:  97%|#########6| 7166000/7400423 [11:53<00:25, 9352.03\nexamples/s]', '\\rMap:  97%|#########6| 7168000/7400423 [11:53<00:24, 9564.61\nexamples/s]', '\\rMap:  97%|#########6| 7169000/7400423 [11:53<00:24, 9641.31\nexamples/s]', '\\rMap:  97%|#########6| 7170000/7400423 [11:53<00:24, 9582.52\nexamples/s]', '\\rMap:  97%|#########6| 7172000/7400423 [11:54<00:23, 9658.71\nexamples/s]', '\\rMap:  97%|#########6| 7173000/7400423 [11:54<00:23, 9650.24\nexamples/s]', '\\rMap:  97%|#########6| 7174000/7400423 [11:54<00:23, 9649.96\nexamples/s]', '\\rMap:  97%|#########6| 7176000/7400423 [11:54<00:22, 9772.83\nexamples/s]', '\\rMap:  97%|#########6| 7177000/7400423 [11:54<00:23, 9677.57\nexamples/s]', '\\rMap:  97%|#########6| 7178000/7400423 [11:54<00:23, 9574.37\nexamples/s]', '\\rMap:  97%|#########7| 7179000/7400423 [11:54<00:23, 9334.50\nexamples/s]', '\\rMap:  97%|#########7| 7180000/7400423 [11:55<00:23, 9246.24\nexamples/s]', '\\rMap:  97%|#########7| 7181000/7400423 [11:55<00:23, 9282.23\nexamples/s]', '\\rMap:  97%|#########7| 7182000/7400423 [11:55<00:23, 9403.89\nexamples/s]', '\\rMap:  97%|#########7| 7183000/7400423 [11:55<00:23, 9122.32\nexamples/s]', '\\rMap:  97%|#########7| 7184000/7400423 [11:55<00:24, 8920.48\nexamples/s]', '\\rMap:  97%|#########7| 7185000/7400423 [11:55<00:23, 9037.19\nexamples/s]', '\\rMap:  97%|#########7| 7186000/7400423 [11:55<00:23, 8950.51\nexamples/s]', '\\rMap:  97%|#########7| 7187000/7400423 [11:55<00:23, 9061.11\nexamples/s]', '\\rMap:  97%|#########7| 7188000/7400423 [11:55<00:23, 9198.11\nexamples/s]', '\\rMap:  97%|#########7| 7189000/7400423 [11:55<00:22, 9291.56\nexamples/s]', '\\rMap:  97%|#########7| 7190000/7400423 [11:56<00:22, 9276.03\nexamples/s]', '\\rMap:  97%|#########7| 7191000/7400423 [11:56<00:22, 9336.67\nexamples/s]', '\\rMap:  97%|#########7| 7192000/7400423 [11:56<00:22, 9392.93\nexamples/s]', '\\rMap:  97%|#########7| 7193000/7400423 [11:56<00:21, 9502.61\nexamples/s]', '\\rMap:  97%|#########7| 7194000/7400423 [11:56<00:21, 9396.22\nexamples/s]', '\\rMap:  97%|#########7| 7196000/7400423 [11:56<00:21, 9691.87\nexamples/s]', '\\rMap:  97%|#########7| 7197000/7400423 [11:56<00:21, 9639.53\nexamples/s]', '\\rMap:  97%|#########7| 7199000/7400423 [11:57<00:20, 9736.01\nexamples/s]', '\\rMap:  97%|#########7| 7200000/7400423 [11:57<00:20, 9747.05\nexamples/s]', '\\rMap:  97%|#########7| 7201000/7400423 [11:57<00:20, 9672.07\nexamples/s]', '\\rMap:  97%|#########7| 7202000/7400423 [11:57<00:20, 9636.82\nexamples/s]', '\\rMap:  97%|#########7| 7203000/7400423 [11:57<00:31, 6217.15\nexamples/s]', '\\rMap:  97%|#########7| 7204000/7400423 [11:57<00:28, 6914.24\nexamples/s]', '\\rMap:  97%|#########7| 7205000/7400423 [11:57<00:25, 7571.01\nexamples/s]', '\\rMap:  97%|#########7| 7207000/7400423 [11:58<00:22, 8488.68\nexamples/s]', '\\rMap:  97%|#########7| 7209000/7400423 [11:58<00:21, 9078.78\nexamples/s]', '\\rMap:  97%|#########7| 7211000/7400423 [11:58<00:19, 9587.56\nexamples/s]', '\\rMap:  97%|#########7| 7212000/7400423 [11:58<00:19, 9657.56\nexamples/s]', '\\rMap:  97%|#########7| 7214000/7400423 [11:58<00:19, 9799.25\nexamples/s]', '\\rMap:  97%|#########7| 7215000/7400423 [11:58<00:19, 9645.10\nexamples/s]', '\\rMap:  98%|#########7| 7216000/7400423 [11:58<00:19, 9566.20\nexamples/s]', '\\rMap:  98%|#########7| 7218000/7400423 [11:59<00:18, 10066.31\nexamples/s]', '\\rMap:  98%|#########7| 7220000/7400423 [11:59<00:18, 9892.95\nexamples/s] ', '\\rMap:  98%|#########7| 7221000/7400423 [11:59<00:18, 9848.18\nexamples/s]', '\\rMap:  98%|#########7| 7222000/7400423 [11:59<00:18, 9863.69\nexamples/s]', '\\rMap:  98%|#########7| 7224000/7400423 [11:59<00:17, 10065.98\nexamples/s]', '\\rMap:  98%|#########7| 7226000/7400423 [11:59<00:17, 9962.21\nexamples/s] ', '\\rMap:  98%|#########7| 7227000/7400423 [12:00<00:17, 9947.12\nexamples/s]', '\\rMap:  98%|#########7| 7229000/7400423 [12:00<00:16, 10087.29\nexamples/s]', '\\rMap:  98%|#########7| 7231000/7400423 [12:00<00:16, 10050.80\nexamples/s]', '\\rMap:  98%|#########7| 7233000/7400423 [12:00<00:16, 9939.63\nexamples/s] ', '\\rMap:  98%|#########7| 7234000/7400423 [12:00<00:16, 9839.33\nexamples/s]', '\\rMap:  98%|#########7| 7236000/7400423 [12:00<00:16, 9906.34\nexamples/s]', '\\rMap:  98%|#########7| 7237000/7400423 [12:01<00:16, 9923.74\nexamples/s]', '\\rMap:  98%|#########7| 7238000/7400423 [12:01<00:16, 9655.72\nexamples/s]', '\\rMap:  98%|#########7| 7240000/7400423 [12:01<00:16, 9952.16\nexamples/s]', '\\rMap:  98%|#########7| 7241000/7400423 [12:01<00:16, 9945.16\nexamples/s]', '\\rMap:  98%|#########7| 7242000/7400423 [12:01<00:15, 9930.20\nexamples/s]', '\\rMap:  98%|#########7| 7244000/7400423 [12:01<00:15, 10027.35\nexamples/s]', '\\rMap:  98%|#########7| 7245000/7400423 [12:01<00:15, 9966.26\nexamples/s] ', '\\rMap:  98%|#########7| 7247000/7400423 [12:02<00:15, 10178.18\nexamples/s]', '\\rMap:  98%|#########7| 7249000/7400423 [12:02<00:15, 10070.93\nexamples/s]', '\\rMap:  98%|#########7| 7251000/7400423 [12:02<00:15, 9789.86\nexamples/s] ', '\\rMap:  98%|#########7| 7252000/7400423 [12:02<00:15, 9829.29\nexamples/s]', '\\rMap:  98%|#########8| 7253000/7400423 [12:02<00:15, 9558.30\nexamples/s]', '\\rMap:  98%|#########8| 7254000/7400423 [12:02<00:15, 9539.56\nexamples/s]', '\\rMap:  98%|#########8| 7255000/7400423 [12:02<00:15, 9576.34\nexamples/s]', '\\rMap:  98%|#########8| 7256000/7400423 [12:03<00:15, 9509.10\nexamples/s]', '\\rMap:  98%|#########8| 7257000/7400423 [12:03<00:15, 9224.32\nexamples/s]', '\\rMap:  98%|#########8| 7259000/7400423 [12:03<00:13, 10286.20\nexamples/s]', '\\rMap:  98%|#########8| 7261000/7400423 [12:03<00:12, 10925.88\nexamples/s]', '\\rMap:  98%|#########8| 7263000/7400423 [12:03<00:12, 11350.69\nexamples/s]', '\\rMap:  98%|#########8| 7265000/7400423 [12:03<00:11, 11506.47\nexamples/s]', '\\rMap:  98%|#########8| 7267000/7400423 [12:03<00:12, 11024.38\nexamples/s]', '\\rMap:  98%|#########8| 7269000/7400423 [12:04<00:12, 10919.32\nexamples/s]', '\\rMap:  98%|#########8| 7271000/7400423 [12:04<00:11, 10899.57\nexamples/s]', '\\rMap:  98%|#########8| 7273000/7400423 [12:04<00:11, 10947.80\nexamples/s]', '\\rMap:  98%|#########8| 7275000/7400423 [12:04<00:11, 11167.96\nexamples/s]', '\\rMap:  98%|#########8| 7277000/7400423 [12:04<00:11, 11098.17\nexamples/s]', '\\rMap:  98%|#########8| 7279000/7400423 [12:05<00:11, 10909.30\nexamples/s]', '\\rMap:  98%|#########8| 7281000/7400423 [12:05<00:11, 10547.50\nexamples/s]', '\\rMap:  98%|#########8| 7283000/7400423 [12:05<00:11, 10323.97\nexamples/s]', '\\rMap:  98%|#########8| 7285000/7400423 [12:05<00:11, 10155.27\nexamples/s]', '\\rMap:  98%|#########8| 7287000/7400423 [12:05<00:11, 10121.23\nexamples/s]', '\\rMap:  98%|#########8| 7289000/7400423 [12:06<00:11, 9937.34\nexamples/s] ', '\\rMap:  99%|#########8| 7290000/7400423 [12:06<00:11, 9858.51\nexamples/s]', '\\rMap:  99%|#########8| 7291000/7400423 [12:06<00:11, 9832.48\nexamples/s]', '\\rMap:  99%|#########8| 7292000/7400423 [12:06<00:11, 9734.96\nexamples/s]', '\\rMap:  99%|#########8| 7293000/7400423 [12:06<00:10, 9773.35\nexamples/s]', '\\rMap:  99%|#########8| 7294000/7400423 [12:06<00:10, 9745.10\nexamples/s]', '\\rMap:  99%|#########8| 7295000/7400423 [12:06<00:10, 9611.77\nexamples/s]', '\\rMap:  99%|#########8| 7296000/7400423 [12:06<00:10, 9555.43\nexamples/s]', '\\rMap:  99%|#########8| 7297000/7400423 [12:06<00:11, 9343.06\nexamples/s]', '\\rMap:  99%|#########8| 7298000/7400423 [12:07<00:10, 9484.43\nexamples/s]', '\\rMap:  99%|#########8| 7300000/7400423 [12:07<00:10, 9619.97\nexamples/s]', '\\rMap:  99%|#########8| 7302000/7400423 [12:07<00:10, 9817.06\nexamples/s]', '\\rMap:  99%|#########8| 7303000/7400423 [12:07<00:10, 9651.95\nexamples/s]', '\\rMap:  99%|#########8| 7304000/7400423 [12:07<00:10, 9617.15\nexamples/s]', '\\rMap:  99%|#########8| 7305000/7400423 [12:07<00:09, 9656.38\nexamples/s]', '\\rMap:  99%|#########8| 7307000/7400423 [12:07<00:09, 9779.78\nexamples/s]', '\\rMap:  99%|#########8| 7308000/7400423 [12:08<00:09, 9689.75\nexamples/s]', '\\rMap:  99%|#########8| 7310000/7400423 [12:08<00:09, 9724.72\nexamples/s]', '\\rMap:  99%|#########8| 7312000/7400423 [12:08<00:09, 9787.91\nexamples/s]', '\\rMap:  99%|#########8| 7313000/7400423 [12:08<00:09, 9703.98\nexamples/s]', '\\rMap:  99%|#########8| 7314000/7400423 [12:08<00:08, 9756.58\nexamples/s]', '\\rMap:  99%|#########8| 7315000/7400423 [12:08<00:08, 9804.22\nexamples/s]', '\\rMap:  99%|#########8| 7316000/7400423 [12:08<00:08, 9722.89\nexamples/s]', '\\rMap:  99%|#########8| 7317000/7400423 [12:08<00:08, 9721.80\nexamples/s]', '\\rMap:  99%|#########8| 7318000/7400423 [12:09<00:08, 9796.23\nexamples/s]', '\\rMap:  99%|#########8| 7320000/7400423 [12:09<00:08, 9681.34\nexamples/s]', '\\rMap:  99%|#########8| 7322000/7400423 [12:09<00:08, 9797.44\nexamples/s]', '\\rMap:  99%|#########8| 7323000/7400423 [12:09<00:07, 9746.11\nexamples/s]', '\\rMap:  99%|#########8| 7324000/7400423 [12:09<00:07, 9781.61\nexamples/s]', '\\rMap:  99%|#########8| 7325000/7400423 [12:09<00:07, 9792.61\nexamples/s]', '\\rMap:  99%|#########8| 7326000/7400423 [12:09<00:07, 9685.38\nexamples/s]', '\\rMap:  99%|#########9| 7327000/7400423 [12:10<00:07, 9650.42\nexamples/s]', '\\rMap:  99%|#########9| 7329000/7400423 [12:10<00:10, 6720.57\nexamples/s]', '\\rMap:  99%|#########9| 7331000/7400423 [12:10<00:08, 7815.37\nexamples/s]', '\\rMap:  99%|#########9| 7332000/7400423 [12:10<00:08, 8188.17\nexamples/s]', '\\rMap:  99%|#########9| 7333000/7400423 [12:10<00:07, 8513.89\nexamples/s]', '\\rMap:  99%|#########9| 7334000/7400423 [12:10<00:07, 8831.63\nexamples/s]', '\\rMap:  99%|#########9| 7336000/7400423 [12:11<00:07, 9196.11\nexamples/s]', '\\rMap:  99%|#########9| 7337000/7400423 [12:11<00:06, 9364.93\nexamples/s]', '\\rMap:  99%|#########9| 7338000/7400423 [12:11<00:06, 9328.26\nexamples/s]', '\\rMap:  99%|#########9| 7339000/7400423 [12:11<00:06, 9463.79\nexamples/s]', '\\rMap:  99%|#########9| 7340000/7400423 [12:11<00:06, 9534.27\nexamples/s]', '\\rMap:  99%|#########9| 7341000/7400423 [12:11<00:06, 9656.78\nexamples/s]', '\\rMap:  99%|#########9| 7342000/7400423 [12:11<00:06, 9663.56\nexamples/s]', '\\rMap:  99%|#########9| 7344000/7400423 [12:11<00:05, 9748.51\nexamples/s]', '\\rMap:  99%|#########9| 7345000/7400423 [12:12<00:05, 9631.04\nexamples/s]', '\\rMap:  99%|#########9| 7346000/7400423 [12:12<00:05, 9506.04\nexamples/s]', '\\rMap:  99%|#########9| 7347000/7400423 [12:12<00:05, 9582.34\nexamples/s]', '\\rMap:  99%|#########9| 7348000/7400423 [12:12<00:05, 9619.98\nexamples/s]', '\\rMap:  99%|#########9| 7349000/7400423 [12:12<00:05, 9669.33\nexamples/s]', '\\rMap:  99%|#########9| 7350000/7400423 [12:12<00:05, 9538.79\nexamples/s]', '\\rMap:  99%|#########9| 7351000/7400423 [12:12<00:05, 9536.94\nexamples/s]', '\\rMap:  99%|#########9| 7352000/7400423 [12:12<00:05, 9533.29\nexamples/s]', '\\rMap:  99%|#########9| 7353000/7400423 [12:12<00:05, 9479.86\nexamples/s]', '\\rMap:  99%|#########9| 7355000/7400423 [12:13<00:04, 9809.59\nexamples/s]', '\\rMap:  99%|#########9| 7356000/7400423 [12:13<00:04, 9459.64\nexamples/s]', '\\rMap:  99%|#########9| 7357000/7400423 [12:13<00:04, 9245.15\nexamples/s]', '\\rMap:  99%|#########9| 7358000/7400423 [12:13<00:04, 9073.35\nexamples/s]', '\\rMap:  99%|#########9| 7359000/7400423 [12:13<00:04, 9245.27\nexamples/s]', '\\rMap:  99%|#########9| 7360000/7400423 [12:13<00:04, 9262.93\nexamples/s]', '\\rMap:  99%|#########9| 7362000/7400423 [12:13<00:03, 9863.20\nexamples/s]', '\\rMap: 100%|#########9| 7364000/7400423 [12:14<00:03, 10020.43\nexamples/s]', '\\rMap: 100%|#########9| 7366000/7400423 [12:14<00:03, 10234.44\nexamples/s]', '\\rMap: 100%|#########9| 7368000/7400423 [12:14<00:03, 9861.82\nexamples/s] ', '\\rMap: 100%|#########9| 7369000/7400423 [12:14<00:03, 9772.78\nexamples/s]', '\\rMap: 100%|#########9| 7370000/7400423 [12:14<00:03, 9684.66\nexamples/s]', '\\rMap: 100%|#########9| 7371000/7400423 [12:14<00:03, 9554.18\nexamples/s]', '\\rMap: 100%|#########9| 7372000/7400423 [12:14<00:02, 9566.72\nexamples/s]', '\\rMap: 100%|#########9| 7373000/7400423 [12:14<00:02, 9669.80\nexamples/s]', '\\rMap: 100%|#########9| 7374000/7400423 [12:15<00:02, 9639.26\nexamples/s]', '\\rMap: 100%|#########9| 7375000/7400423 [12:15<00:02, 9667.48\nexamples/s]', '\\rMap: 100%|#########9| 7377000/7400423 [12:15<00:02, 9799.92\nexamples/s]', '\\rMap: 100%|#########9| 7379000/7400423 [12:15<00:02, 10076.32\nexamples/s]', '\\rMap: 100%|#########9| 7381000/7400423 [12:15<00:01, 10182.29\nexamples/s]', '\\rMap: 100%|#########9| 7383000/7400423 [12:15<00:01, 10144.17\nexamples/s]', '\\rMap: 100%|#########9| 7385000/7400423 [12:16<00:01, 10080.80\nexamples/s]', '\\rMap: 100%|#########9| 7387000/7400423 [12:16<00:01, 10175.83\nexamples/s]', '\\rMap: 100%|#########9| 7389000/7400423 [12:16<00:01, 10280.12\nexamples/s]', '\\rMap: 100%|#########9| 7391000/7400423 [12:16<00:00, 10110.26\nexamples/s]', '\\rMap: 100%|#########9| 7393000/7400423 [12:16<00:00, 9973.36\nexamples/s] ', '\\rMap: 100%|#########9| 7394000/7400423 [12:17<00:00, 9947.86\nexamples/s]', '\\rMap: 100%|#########9| 7396000/7400423 [12:17<00:00, 10051.61\nexamples/s]', '\\rMap: 100%|#########9| 7398000/7400423 [12:17<00:00, 9902.51\nexamples/s] ', '\\rMap: 100%|#########9| 7399000/7400423 [12:17<00:00, 9736.47\nexamples/s]', '\\rMap: 100%|#########9| 7400000/7400423 [12:17<00:00, 9597.16\nexamples/s]', '', '\\rMap: 100%|##########| 7400423/7400423 [12:17<00:00,\n10030.46 examples/s]', '\\n', '\\rMap:   0%|          | 0/7400423 [00:00<?, ?\nexamples/s]', '\\rMap:   0%|          | 5000/7400423 [00:00<03:04, 40106.79\nexamples/s]', '\\rMap:   0%|          | 10000/7400423 [00:00<03:13, 38171.87\nexamples/s]', '\\rMap:   0%|          | 14000/7400423 [00:00<03:16, 37500.74\nexamples/s]', '\\rMap:   0%|          | 18000/7400423 [00:00<03:22, 36451.03\nexamples/s]', '\\rMap:   0%|          | 22000/7400423 [00:00<03:27, 35584.51\nexamples/s]', '\\rMap:   0%|          | 26000/7400423 [00:00<03:33, 34537.81\nexamples/s]', '\\rMap:   0%|          | 30000/7400423 [00:00<03:39, 33647.45\nexamples/s]', '\\rMap:   0%|          | 34000/7400423 [00:00<03:37, 33878.03\nexamples/s]', '\\rMap:   1%|          | 38000/7400423 [00:01<03:40, 33437.60\nexamples/s]', '\\rMap:   1%|          | 42000/7400423 [00:01<03:39, 33592.73\nexamples/s]', '\\rMap:   1%|          | 46000/7400423 [00:01<03:37, 33765.86\nexamples/s]', '\\rMap:   1%|          | 50000/7400423 [00:01<03:36, 33882.13\nexamples/s]', '\\rMap:   1%|          | 54000/7400423 [00:01<03:37, 33790.56\nexamples/s]', '\\rMap:   1%|          | 58000/7400423 [00:01<03:38, 33674.06\nexamples/s]', '\\rMap:   1%|          | 62000/7400423 [00:01<03:35, 34039.04\nexamples/s]', '\\rMap:   1%|          | 66000/7400423 [00:01<03:29, 34968.25\nexamples/s]', '\\rMap:   1%|          | 70000/7400423 [00:02<03:26, 35477.33\nexamples/s]', '\\rMap:   1%|1         | 75000/7400423 [00:02<03:17, 37029.73\nexamples/s]', '\\rMap:   1%|1         | 79000/7400423 [00:02<03:18, 36817.01\nexamples/s]', '\\rMap:   1%|1         | 83000/7400423 [00:02<03:21, 36392.25\nexamples/s]', '\\rMap:   1%|1         | 87000/7400423 [00:02<03:18, 36826.82\nexamples/s]', '\\rMap:   1%|1         | 92000/7400423 [00:02<03:11, 38257.06\nexamples/s]', '\\rMap:   1%|1         | 97000/7400423 [00:02<03:06, 39095.36\nexamples/s]', '\\rMap:   1%|1         | 102000/7400423 [00:02<03:03, 39715.51\nexamples/s]', '\\rMap:   1%|1         | 108000/7400423 [00:03<04:23, 27628.57\nexamples/s]', '\\rMap:   2%|1         | 113000/7400423 [00:03<03:58, 30555.09\nexamples/s]', '\\rMap:   2%|1         | 117000/7400423 [00:03<03:52, 31307.51\nexamples/s]', '\\rMap:   2%|1         | 121000/7400423 [00:03<03:46, 32178.77\nexamples/s]', '\\rMap:   2%|1         | 125000/7400423 [00:03<03:43, 32620.49\nexamples/s]', '\\rMap:   2%|1         | 129000/7400423 [00:03<03:40, 33029.60\nexamples/s]', '\\rMap:   2%|1         | 133000/7400423 [00:03<03:38, 33329.29\nexamples/s]', '\\rMap:   2%|1         | 137000/7400423 [00:03<03:36, 33542.34\nexamples/s]', '\\rMap:   2%|1         | 141000/7400423 [00:04<03:30, 34461.13\nexamples/s]', '\\rMap:   2%|1         | 145000/7400423 [00:04<03:24, 35402.42\nexamples/s]', '\\rMap:   2%|2         | 149000/7400423 [00:04<03:21, 36014.42\nexamples/s]', '\\rMap:   2%|2         | 153000/7400423 [00:04<03:19, 36334.75\nexamples/s]', '\\rMap:   2%|2         | 157000/7400423 [00:04<03:19, 36286.04\nexamples/s]', '\\rMap:   2%|2         | 161000/7400423 [00:04<03:18, 36436.42\nexamples/s]', '\\rMap:   2%|2         | 165000/7400423 [00:04<03:15, 37020.65\nexamples/s]', '\\rMap:   2%|2         | 169000/7400423 [00:04<03:15, 36927.79\nexamples/s]', '\\rMap:   2%|2         | 173000/7400423 [00:04<03:12, 37488.01\nexamples/s]', '\\rMap:   2%|2         | 177000/7400423 [00:05<03:11, 37668.74\nexamples/s]', '\\rMap:   2%|2         | 181000/7400423 [00:05<03:12, 37459.60\nexamples/s]', '\\rMap:   2%|2         | 185000/7400423 [00:05<03:09, 38018.98\nexamples/s]', '\\rMap:   3%|2         | 189000/7400423 [00:05<03:08, 38251.68\nexamples/s]', '\\rMap:   3%|2         | 193000/7400423 [00:05<03:10, 37917.24\nexamples/s]', '\\rMap:   3%|2         | 198000/7400423 [00:05<03:08, 38307.35\nexamples/s]', '\\rMap:   3%|2         | 202000/7400423 [00:05<03:08, 38130.91\nexamples/s]', '\\rMap:   3%|2         | 206000/7400423 [00:05<03:10, 37812.94\nexamples/s]', '\\rMap:   3%|2         | 210000/7400423 [00:05<03:13, 37190.99\nexamples/s]', '\\rMap:   3%|2         | 214000/7400423 [00:06<03:11, 37564.84\nexamples/s]', '\\rMap:   3%|2         | 218000/7400423 [00:06<03:08, 38046.86\nexamples/s]', '\\rMap:   3%|2         | 222000/7400423 [00:06<03:08, 38170.67\nexamples/s]', '\\rMap:   3%|3         | 226000/7400423 [00:06<03:08, 38081.72\nexamples/s]', '\\rMap:   3%|3         | 230000/7400423 [00:06<03:13, 37125.67\nexamples/s]', '\\rMap:   3%|3         | 235000/7400423 [00:06<03:11, 37361.64\nexamples/s]', '\\rMap:   3%|3         | 239000/7400423 [00:06<03:20, 35718.08\nexamples/s]', '\\rMap:   3%|3         | 243000/7400423 [00:06<03:16, 36391.50\nexamples/s]', '\\rMap:   3%|3         | 247000/7400423 [00:06<03:13, 36901.55\nexamples/s]', '\\rMap:   3%|3         | 251000/7400423 [00:07<03:13, 37006.44\nexamples/s]', '\\rMap:   3%|3         | 257000/7400423 [00:07<03:07, 38130.18\nexamples/s]', '\\rMap:   4%|3         | 261000/7400423 [00:07<03:05, 38511.36\nexamples/s]', '\\rMap:   4%|3         | 266000/7400423 [00:07<03:03, 38798.80\nexamples/s]', '\\rMap:   4%|3         | 270000/7400423 [00:07<03:05, 38524.81\nexamples/s]', '\\rMap:   4%|3         | 275000/7400423 [00:07<03:04, 38689.13\nexamples/s]', '\\rMap:   4%|3         | 279000/7400423 [00:07<03:07, 38055.60\nexamples/s]', '\\rMap:   4%|3         | 283000/7400423 [00:07<03:11, 37254.14\nexamples/s]', '\\rMap:   4%|3         | 287000/7400423 [00:08<03:23, 34968.78\nexamples/s]', '\\rMap:   4%|3         | 291000/7400423 [00:08<03:33, 33271.23\nexamples/s]', '\\rMap:   4%|3         | 295000/7400423 [00:08<03:36, 32757.06\nexamples/s]', '\\rMap:   4%|4         | 299000/7400423 [00:08<03:38, 32499.04\nexamples/s]', '\\rMap:   4%|4         | 303000/7400423 [00:08<03:32, 33449.12\nexamples/s]', '\\rMap:   4%|4         | 308000/7400423 [00:08<03:19, 35542.44\nexamples/s]', '\\rMap:   4%|4         | 314000/7400423 [00:08<03:10, 37284.45\nexamples/s]', '\\rMap:   4%|4         | 318000/7400423 [00:08<03:10, 37191.71\nexamples/s]', '\\rMap:   4%|4         | 324000/7400423 [00:09<03:03, 38580.56\nexamples/s]', '\\rMap:   4%|4         | 330000/7400423 [00:09<02:56, 39980.01\nexamples/s]', '\\rMap:   5%|4         | 336000/7400423 [00:09<02:56, 39913.72\nexamples/s]', '\\rMap:   5%|4         | 340000/7400423 [00:09<03:07, 37559.25\nexamples/s]', '\\rMap:   5%|4         | 344000/7400423 [00:09<03:13, 36396.52\nexamples/s]', '\\rMap:   5%|4         | 348000/7400423 [00:09<03:24, 34436.71\nexamples/s]', '\\rMap:   5%|4         | 352000/7400423 [00:09<03:29, 33631.61\nexamples/s]', '\\rMap:   5%|4         | 356000/7400423 [00:09<03:34, 32835.65\nexamples/s]', '\\rMap:   5%|4         | 360000/7400423 [00:10<03:33, 33043.67\nexamples/s]', '\\rMap:   5%|4         | 364000/7400423 [00:10<03:31, 33224.75\nexamples/s]', '\\rMap:   5%|4         | 368000/7400423 [00:10<03:32, 33065.25\nexamples/s]', '\\rMap:   5%|5         | 372000/7400423 [00:10<03:39, 31970.97\nexamples/s]', '\\rMap:   5%|5         | 376000/7400423 [00:10<03:38, 32188.69\nexamples/s]', '\\rMap:   5%|5         | 380000/7400423 [00:10<03:39, 31961.23\nexamples/s]', '\\rMap:   5%|5         | 384000/7400423 [00:10<03:41, 31704.35\nexamples/s]', '\\rMap:   5%|5         | 388000/7400423 [00:10<03:43, 31388.92\nexamples/s]', '\\rMap:   5%|5         | 392000/7400423 [00:11<03:43, 31417.45\nexamples/s]', '\\rMap:   5%|5         | 396000/7400423 [00:11<03:39, 31981.13\nexamples/s]', '\\rMap:   5%|5         | 400000/7400423 [00:11<03:34, 32674.53\nexamples/s]', '\\rMap:   5%|5         | 404000/7400423 [00:11<03:26, 33882.89\nexamples/s]', '\\rMap:   6%|5         | 408000/7400423 [00:11<03:26, 33936.77\nexamples/s]', '\\rMap:   6%|5         | 412000/7400423 [00:11<03:32, 32934.96\nexamples/s]', '\\rMap:   6%|5         | 416000/7400423 [00:11<03:35, 32391.67\nexamples/s]', '\\rMap:   6%|5         | 420000/7400423 [00:11<03:27, 33599.47\nexamples/s]', '\\rMap:   6%|5         | 424000/7400423 [00:12<03:24, 34094.24\nexamples/s]', '\\rMap:   6%|5         | 428000/7400423 [00:12<03:20, 34787.96\nexamples/s]', '\\rMap:   6%|5         | 432000/7400423 [00:12<03:17, 35266.85\nexamples/s]', '\\rMap:   6%|5         | 436000/7400423 [00:12<03:16, 35412.65\nexamples/s]', '\\rMap:   6%|5         | 440000/7400423 [00:12<03:13, 35929.88\nexamples/s]', '\\rMap:   6%|5         | 444000/7400423 [00:12<03:11, 36281.77\nexamples/s]', '\\rMap:   6%|6         | 448000/7400423 [00:12<03:15, 35604.30\nexamples/s]', '\\rMap:   6%|6         | 452000/7400423 [00:12<03:11, 36302.65\nexamples/s]', '\\rMap:   6%|6         | 456000/7400423 [00:12<03:15, 35435.83\nexamples/s]', '\\rMap:   6%|6         | 460000/7400423 [00:13<03:19, 34741.45\nexamples/s]', '\\rMap:   6%|6         | 464000/7400423 [00:13<03:23, 34019.26\nexamples/s]', '\\rMap:   6%|6         | 468000/7400423 [00:13<03:23, 34123.45\nexamples/s]', '\\rMap:   6%|6         | 472000/7400423 [00:13<03:18, 34839.10\nexamples/s]', '\\rMap:   6%|6         | 476000/7400423 [00:13<03:11, 36196.76\nexamples/s]', '\\rMap:   7%|6         | 482000/7400423 [00:13<03:00, 38289.41\nexamples/s]', '\\rMap:   7%|6         | 488000/7400423 [00:13<02:55, 39407.69\nexamples/s]', '\\rMap:   7%|6         | 494000/7400423 [00:13<02:53, 39740.81\nexamples/s]', '\\rMap:   7%|6         | 500000/7400423 [00:14<02:51, 40188.01\nexamples/s]', '\\rMap:   7%|6         | 505000/7400423 [00:14<02:52, 39954.97\nexamples/s]', '\\rMap:   7%|6         | 510000/7400423 [00:14<02:52, 39839.57\nexamples/s]', '\\rMap:   7%|6         | 515000/7400423 [00:14<02:52, 39847.32\nexamples/s]', '\\rMap:   7%|7         | 519000/7400423 [00:14<02:55, 39139.88\nexamples/s]', '\\rMap:   7%|7         | 523000/7400423 [00:14<02:58, 38532.38\nexamples/s]', '\\rMap:   7%|7         | 527000/7400423 [00:14<03:02, 37599.97\nexamples/s]', '\\rMap:   7%|7         | 531000/7400423 [00:14<03:03, 37372.13\nexamples/s]', '\\rMap:   7%|7         | 535000/7400423 [00:14<03:05, 37109.00\nexamples/s]', '\\rMap:   7%|7         | 539000/7400423 [00:15<03:02, 37581.81\nexamples/s]', '\\rMap:   7%|7         | 545000/7400423 [00:15<02:58, 38357.71\nexamples/s]', '\\rMap:   7%|7         | 550000/7400423 [00:15<02:56, 38765.42\nexamples/s]', '\\rMap:   7%|7         | 555000/7400423 [00:15<02:57, 38653.95\nexamples/s]', '\\rMap:   8%|7         | 559000/7400423 [00:15<03:03, 37363.31\nexamples/s]', '\\rMap:   8%|7         | 563000/7400423 [00:15<03:05, 36866.73\nexamples/s]', '\\rMap:   8%|7         | 569000/7400423 [00:15<02:50, 40020.97\nexamples/s]', '\\rMap:   8%|7         | 575000/7400423 [00:16<02:49, 40378.08\nexamples/s]', '\\rMap:   8%|7         | 581000/7400423 [00:16<02:44, 41511.75\nexamples/s]', '\\rMap:   8%|7         | 587000/7400423 [00:16<02:42, 41862.67\nexamples/s]', '\\rMap:   8%|8         | 593000/7400423 [00:16<02:41, 42111.43\nexamples/s]', '\\rMap:   8%|8         | 599000/7400423 [00:16<02:42, 41807.79\nexamples/s]', '\\rMap:   8%|8         | 605000/7400423 [00:16<02:42, 41716.49\nexamples/s]', '\\rMap:   8%|8         | 610000/7400423 [00:16<02:48, 40218.68\nexamples/s]', '\\rMap:   8%|8         | 616000/7400423 [00:17<02:59, 37758.34\nexamples/s]', '\\rMap:   8%|8         | 620000/7400423 [00:17<02:59, 37804.04\nexamples/s]', '\\rMap:   8%|8         | 625000/7400423 [00:17<03:00, 37447.54\nexamples/s]', '\\rMap:   8%|8         | 629000/7400423 [00:17<03:10, 35525.08\nexamples/s]', '\\rMap:   9%|8         | 633000/7400423 [00:17<03:12, 35190.38\nexamples/s]', '\\rMap:   9%|8         | 637000/7400423 [00:17<03:17, 34310.33\nexamples/s]', '\\rMap:   9%|8         | 641000/7400423 [00:17<03:17, 34277.34\nexamples/s]', '\\rMap:   9%|8         | 645000/7400423 [00:17<03:12, 35071.36\nexamples/s]', '\\rMap:   9%|8         | 649000/7400423 [00:17<03:16, 34410.95\nexamples/s]', '\\rMap:   9%|8         | 653000/7400423 [00:18<03:16, 34379.47\nexamples/s]', '\\rMap:   9%|8         | 657000/7400423 [00:18<03:17, 34207.84\nexamples/s]', '\\rMap:   9%|8         | 661000/7400423 [00:18<03:20, 33561.03\nexamples/s]', '\\rMap:   9%|8         | 665000/7400423 [00:18<03:22, 33202.62\nexamples/s]', '\\rMap:   9%|9         | 669000/7400423 [00:18<03:22, 33251.69\nexamples/s]', '\\rMap:   9%|9         | 673000/7400423 [00:18<03:23, 33125.91\nexamples/s]', '\\rMap:   9%|9         | 677000/7400423 [00:18<03:16, 34176.95\nexamples/s]', '\\rMap:   9%|9         | 683000/7400423 [00:18<02:57, 37876.99\nexamples/s]', '\\rMap:   9%|9         | 689000/7400423 [00:19<02:46, 40328.07\nexamples/s]', '\\rMap:   9%|9         | 695000/7400423 [00:19<02:42, 41351.97\nexamples/s]', '\\rMap:   9%|9         | 701000/7400423 [00:19<02:38, 42163.69\nexamples/s]', '\\rMap:  10%|9         | 706000/7400423 [00:19<02:41, 41517.86\nexamples/s]', '\\rMap:  10%|9         | 712000/7400423 [00:19<02:46, 40176.43\nexamples/s]', '\\rMap:  10%|9         | 718000/7400423 [00:19<02:51, 38939.65\nexamples/s]', '\\rMap:  10%|9         | 722000/7400423 [00:19<02:50, 39061.21\nexamples/s]', '\\rMap:  10%|9         | 726000/7400423 [00:20<02:53, 38500.58\nexamples/s]', '\\rMap:  10%|9         | 730000/7400423 [00:20<02:57, 37675.28\nexamples/s]', '\\rMap:  10%|9         | 734000/7400423 [00:20<02:58, 37385.96\nexamples/s]', '\\rMap:  10%|9         | 738000/7400423 [00:20<02:57, 37430.17\nexamples/s]', '\\rMap:  10%|#         | 742000/7400423 [00:20<02:58, 37329.82\nexamples/s]', '\\rMap:  10%|#         | 746000/7400423 [00:20<02:56, 37779.33\nexamples/s]', '\\rMap:  10%|#         | 750000/7400423 [00:20<02:54, 38167.74\nexamples/s]', '\\rMap:  10%|#         | 754000/7400423 [00:20<02:54, 37986.23\nexamples/s]', '\\rMap:  10%|#         | 758000/7400423 [00:20<02:54, 38072.04\nexamples/s]', '\\rMap:  10%|#         | 762000/7400423 [00:20<02:53, 38173.82\nexamples/s]', '\\rMap:  10%|#         | 766000/7400423 [00:21<02:54, 38038.07\nexamples/s]', '\\rMap:  10%|#         | 770000/7400423 [00:21<02:55, 37801.79\nexamples/s]', '\\rMap:  10%|#         | 774000/7400423 [00:21<03:30, 31497.93\nexamples/s]', '\\rMap:  11%|#         | 778000/7400423 [00:21<03:55, 28129.04\nexamples/s]', '\\rMap:  11%|#         | 782000/7400423 [00:21<03:58, 27793.14\nexamples/s]', '\\rMap:  11%|#         | 786000/7400423 [00:21<03:57, 27824.16\nexamples/s]', '\\rMap:  11%|#         | 790000/7400423 [00:21<04:03, 27183.55\nexamples/s]', '\\rMap:  11%|#         | 794000/7400423 [00:22<05:59, 18366.20\nexamples/s]', '\\rMap:  11%|#         | 798000/7400423 [00:22<05:29, 20024.25\nexamples/s]', '\\rMap:  11%|#         | 802000/7400423 [00:22<05:07, 21441.23\nexamples/s]', '\\rMap:  11%|#         | 806000/7400423 [00:22<04:52, 22549.61\nexamples/s]', '\\rMap:  11%|#         | 810000/7400423 [00:22<04:39, 23580.54\nexamples/s]', '\\rMap:  11%|#         | 814000/7400423 [00:23<04:38, 23668.16\nexamples/s]', '\\rMap:  11%|#1        | 818000/7400423 [00:23<04:39, 23537.50\nexamples/s]', '\\rMap:  11%|#1        | 822000/7400423 [00:23<04:30, 24312.68\nexamples/s]', '\\rMap:  11%|#1        | 826000/7400423 [00:23<04:21, 25129.53\nexamples/s]', '\\rMap:  11%|#1        | 830000/7400423 [00:23<04:20, 25234.72\nexamples/s]', '\\rMap:  11%|#1        | 834000/7400423 [00:23<04:11, 26103.85\nexamples/s]', '\\rMap:  11%|#1        | 838000/7400423 [00:24<04:05, 26699.52\nexamples/s]', '\\rMap:  11%|#1        | 842000/7400423 [00:24<04:05, 26734.24\nexamples/s]', '\\rMap:  11%|#1        | 846000/7400423 [00:24<04:10, 26171.98\nexamples/s]', '\\rMap:  11%|#1        | 850000/7400423 [00:24<04:12, 25902.19\nexamples/s]', '\\rMap:  12%|#1        | 854000/7400423 [00:24<04:12, 25899.17\nexamples/s]', '\\rMap:  12%|#1        | 858000/7400423 [00:24<04:12, 25879.74\nexamples/s]', '\\rMap:  12%|#1        | 862000/7400423 [00:24<04:06, 26518.27\nexamples/s]', '\\rMap:  12%|#1        | 866000/7400423 [00:25<03:42, 29356.35\nexamples/s]', '\\rMap:  12%|#1        | 872000/7400423 [00:25<03:17, 33032.46\nexamples/s]', '\\rMap:  12%|#1        | 876000/7400423 [00:25<03:10, 34160.25\nexamples/s]', '\\rMap:  12%|#1        | 880000/7400423 [00:25<03:07, 34860.86\nexamples/s]', '\\rMap:  12%|#1        | 884000/7400423 [00:25<03:02, 35626.99\nexamples/s]', '\\rMap:  12%|#1        | 888000/7400423 [00:25<02:58, 36518.19\nexamples/s]', '\\rMap:  12%|#2        | 892000/7400423 [00:25<02:58, 36397.72\nexamples/s]', '\\rMap:  12%|#2        | 896000/7400423 [00:25<02:57, 36743.55\nexamples/s]', '\\rMap:  12%|#2        | 901000/7400423 [00:26<02:50, 38040.06\nexamples/s]', '\\rMap:  12%|#2        | 906000/7400423 [00:26<02:48, 38561.19\nexamples/s]', '\\rMap:  12%|#2        | 912000/7400423 [00:26<02:44, 39534.81\nexamples/s]', '\\rMap:  12%|#2        | 918000/7400423 [00:26<02:41, 40066.57\nexamples/s]', '\\rMap:  12%|#2        | 923000/7400423 [00:26<02:42, 39858.44\nexamples/s]', '\\rMap:  13%|#2        | 927000/7400423 [00:26<02:44, 39437.45\nexamples/s]', '\\rMap:  13%|#2        | 932000/7400423 [00:26<02:43, 39650.22\nexamples/s]', '\\rMap:  13%|#2        | 937000/7400423 [00:26<02:43, 39530.78\nexamples/s]', '\\rMap:  13%|#2        | 941000/7400423 [00:27<02:47, 38670.82\nexamples/s]', '\\rMap:  13%|#2        | 945000/7400423 [00:27<02:52, 37466.81\nexamples/s]', '\\rMap:  13%|#2        | 949000/7400423 [00:27<02:55, 36713.87\nexamples/s]', '\\rMap:  13%|#2        | 953000/7400423 [00:27<02:58, 36218.45\nexamples/s]', '\\rMap:  13%|#2        | 957000/7400423 [00:27<03:01, 35494.74\nexamples/s]', '\\rMap:  13%|#2        | 961000/7400423 [00:27<03:02, 35279.88\nexamples/s]', '\\rMap:  13%|#3        | 965000/7400423 [00:27<03:04, 34867.14\nexamples/s]', '\\rMap:  13%|#3        | 969000/7400423 [00:27<02:59, 35798.21\nexamples/s]', '\\rMap:  13%|#3        | 975000/7400423 [00:27<02:46, 38520.35\nexamples/s]', '\\rMap:  13%|#3        | 981000/7400423 [00:28<02:41, 39628.21\nexamples/s]', '\\rMap:  13%|#3        | 987000/7400423 [00:28<02:39, 40192.08\nexamples/s]', '\\rMap:  13%|#3        | 992000/7400423 [00:28<02:40, 40022.99\nexamples/s]', '\\rMap:  13%|#3        | 996000/7400423 [00:28<02:44, 38869.08\nexamples/s]', '\\rMap:  14%|#3        | 1000000/7400423 [00:28<02:45, 38615.03\nexamples/s]', '\\rMap:  14%|#3        | 1004000/7400423 [00:28<02:45, 38640.26\nexamples/s]', '\\rMap:  14%|#3        | 1010000/7400423 [00:28<02:38, 40214.50\nexamples/s]', '\\rMap:  14%|#3        | 1016000/7400423 [00:28<02:33, 41500.37\nexamples/s]', '\\rMap:  14%|#3        | 1022000/7400423 [00:29<02:31, 42191.82\nexamples/s]', '\\rMap:  14%|#3        | 1028000/7400423 [00:29<02:30, 42411.08\nexamples/s]', '\\rMap:  14%|#3        | 1034000/7400423 [00:29<02:30, 42381.55\nexamples/s]', '\\rMap:  14%|#4        | 1039000/7400423 [00:29<02:32, 41663.33\nexamples/s]', '\\rMap:  14%|#4        | 1045000/7400423 [00:29<02:32, 41701.36\nexamples/s]', '\\rMap:  14%|#4        | 1051000/7400423 [00:29<02:29, 42535.98\nexamples/s]', '\\rMap:  14%|#4        | 1057000/7400423 [00:29<02:26, 43355.20\nexamples/s]', '\\rMap:  14%|#4        | 1063000/7400423 [00:30<02:26, 43349.03\nexamples/s]', '\\rMap:  14%|#4        | 1069000/7400423 [00:30<02:26, 43293.09\nexamples/s]', '\\rMap:  15%|#4        | 1075000/7400423 [00:30<02:24, 43797.42\nexamples/s]', '\\rMap:  15%|#4        | 1081000/7400423 [00:30<02:36, 40348.86\nexamples/s]', '\\rMap:  15%|#4        | 1087000/7400423 [00:30<02:38, 39855.53\nexamples/s]', '\\rMap:  15%|#4        | 1092000/7400423 [00:30<02:37, 40135.12\nexamples/s]', '\\rMap:  15%|#4        | 1099000/7400423 [00:30<02:39, 39553.81\nexamples/s]', '\\rMap:  15%|#4        | 1103000/7400423 [00:31<02:41, 38951.29\nexamples/s]', '\\rMap:  15%|#4        | 1108000/7400423 [00:31<02:42, 38807.25\nexamples/s]', '\\rMap:  15%|#5        | 1112000/7400423 [00:31<02:44, 38174.13\nexamples/s]', '\\rMap:  15%|#5        | 1116000/7400423 [00:31<02:47, 37521.06\nexamples/s]', '\\rMap:  15%|#5        | 1120000/7400423 [00:31<02:47, 37477.71\nexamples/s]', '\\rMap:  15%|#5        | 1125000/7400423 [00:31<02:46, 37647.82\nexamples/s]', '\\rMap:  15%|#5        | 1130000/7400423 [00:31<02:40, 38958.12\nexamples/s]', '\\rMap:  15%|#5        | 1136000/7400423 [00:31<02:33, 40777.33\nexamples/s]', '\\rMap:  15%|#5        | 1142000/7400423 [00:32<02:39, 39315.35\nexamples/s]', '\\rMap:  15%|#5        | 1146000/7400423 [00:32<02:45, 37851.42\nexamples/s]', '\\rMap:  16%|#5        | 1151000/7400423 [00:32<02:40, 38824.37\nexamples/s]', '\\rMap:  16%|#5        | 1157000/7400423 [00:32<02:34, 40291.25\nexamples/s]', '\\rMap:  16%|#5        | 1163000/7400423 [00:32<02:30, 41568.15\nexamples/s]', '\\rMap:  16%|#5        | 1169000/7400423 [00:32<02:41, 38570.97\nexamples/s]', '\\rMap:  16%|#5        | 1173000/7400423 [00:32<02:44, 37792.23\nexamples/s]', '\\rMap:  16%|#5        | 1177000/7400423 [00:32<02:45, 37514.49\nexamples/s]', '\\rMap:  16%|#5        | 1181000/7400423 [00:33<02:48, 36913.42\nexamples/s]', '\\rMap:  16%|#6        | 1185000/7400423 [00:33<02:49, 36654.90\nexamples/s]', '\\rMap:  16%|#6        | 1189000/7400423 [00:33<02:52, 36026.83\nexamples/s]', '\\rMap:  16%|#6        | 1193000/7400423 [00:33<02:52, 35924.54\nexamples/s]', '\\rMap:  16%|#6        | 1197000/7400423 [00:33<02:52, 35889.68\nexamples/s]', '\\rMap:  16%|#6        | 1201000/7400423 [00:33<02:55, 35421.44\nexamples/s]', '\\rMap:  16%|#6        | 1205000/7400423 [00:33<02:59, 34570.69\nexamples/s]', '\\rMap:  16%|#6        | 1209000/7400423 [00:33<02:55, 35298.20\nexamples/s]', '\\rMap:  16%|#6        | 1213000/7400423 [00:34<02:57, 34784.73\nexamples/s]', '\\rMap:  16%|#6        | 1217000/7400423 [00:34<02:56, 35072.63\nexamples/s]', '\\rMap:  16%|#6        | 1221000/7400423 [00:34<02:54, 35433.19\nexamples/s]', '\\rMap:  17%|#6        | 1225000/7400423 [00:34<02:56, 35027.25\nexamples/s]', '\\rMap:  17%|#6        | 1229000/7400423 [00:34<02:56, 35032.93\nexamples/s]', '\\rMap:  17%|#6        | 1233000/7400423 [00:34<03:01, 33952.71\nexamples/s]', '\\rMap:  17%|#6        | 1237000/7400423 [00:34<03:04, 33374.32\nexamples/s]', '\\rMap:  17%|#6        | 1241000/7400423 [00:34<03:03, 33530.71\nexamples/s]', '\\rMap:  17%|#6        | 1245000/7400423 [00:34<03:01, 33861.29\nexamples/s]', '\\rMap:  17%|#6        | 1249000/7400423 [00:35<02:55, 34976.14\nexamples/s]', '\\rMap:  17%|#6        | 1253000/7400423 [00:35<02:57, 34700.02\nexamples/s]', '\\rMap:  17%|#6        | 1257000/7400423 [00:35<02:58, 34346.03\nexamples/s]', '\\rMap:  17%|#7        | 1261000/7400423 [00:35<03:02, 33672.05\nexamples/s]', '\\rMap:  17%|#7        | 1265000/7400423 [00:35<02:54, 35186.15\nexamples/s]', '\\rMap:  17%|#7        | 1269000/7400423 [00:35<02:51, 35725.77\nexamples/s]', '\\rMap:  17%|#7        | 1274000/7400423 [00:35<02:45, 36984.71\nexamples/s]', '\\rMap:  17%|#7        | 1279000/7400423 [00:35<02:39, 38263.59\nexamples/s]', '\\rMap:  17%|#7        | 1284000/7400423 [00:36<02:37, 38817.65\nexamples/s]', '\\rMap:  17%|#7        | 1288000/7400423 [00:36<02:39, 38238.88\nexamples/s]', '\\rMap:  17%|#7        | 1292000/7400423 [00:36<02:42, 37566.03\nexamples/s]', '\\rMap:  18%|#7        | 1297000/7400423 [00:36<02:34, 39626.82\nexamples/s]', '\\rMap:  18%|#7        | 1302000/7400423 [00:36<02:32, 40002.89\nexamples/s]', '\\rMap:  18%|#7        | 1308000/7400423 [00:36<02:30, 40483.26\nexamples/s]', '\\rMap:  18%|#7        | 1314000/7400423 [00:36<02:26, 41557.68\nexamples/s]', '\\rMap:  18%|#7        | 1319000/7400423 [00:36<02:26, 41393.49\nexamples/s]', '\\rMap:  18%|#7        | 1325000/7400423 [00:37<02:34, 39426.86\nexamples/s]', '\\rMap:  18%|#7        | 1329000/7400423 [00:37<02:46, 36450.98\nexamples/s]', '\\rMap:  18%|#8        | 1333000/7400423 [00:37<02:50, 35589.69\nexamples/s]', '\\rMap:  18%|#8        | 1337000/7400423 [00:37<02:49, 35709.25\nexamples/s]', '\\rMap:  18%|#8        | 1341000/7400423 [00:37<02:50, 35544.85\nexamples/s]', '\\rMap:  18%|#8        | 1345000/7400423 [00:37<02:49, 35709.11\nexamples/s]', '\\rMap:  18%|#8        | 1349000/7400423 [00:37<02:48, 35822.95\nexamples/s]', '\\rMap:  18%|#8        | 1353000/7400423 [00:37<02:56, 34303.68\nexamples/s]', '\\rMap:  18%|#8        | 1357000/7400423 [00:37<02:58, 33922.91\nexamples/s]', '\\rMap:  18%|#8        | 1361000/7400423 [00:38<02:56, 34125.72\nexamples/s]', '\\rMap:  18%|#8        | 1365000/7400423 [00:38<03:07, 32149.32\nexamples/s]', '\\rMap:  19%|#8        | 1370000/7400423 [00:38<02:54, 34593.56\nexamples/s]', '\\rMap:  19%|#8        | 1376000/7400423 [00:38<02:38, 38101.06\nexamples/s]', '\\rMap:  19%|#8        | 1382000/7400423 [00:38<02:27, 40673.90\nexamples/s]', '\\rMap:  19%|#8        | 1388000/7400423 [00:38<02:20, 42929.01\nexamples/s]', '\\rMap:  19%|#8        | 1394000/7400423 [00:38<02:15, 44174.77\nexamples/s]', '\\rMap:  19%|#8        | 1400000/7400423 [00:39<02:26, 41045.87\nexamples/s]', '\\rMap:  19%|#8        | 1406000/7400423 [00:39<02:31, 39518.58\nexamples/s]', '\\rMap:  19%|#9        | 1413000/7400423 [00:39<02:33, 39005.81\nexamples/s]', '\\rMap:  19%|#9        | 1417000/7400423 [00:39<02:36, 38266.71\nexamples/s]', '\\rMap:  19%|#9        | 1423000/7400423 [00:39<02:32, 39096.48\nexamples/s]', '\\rMap:  19%|#9        | 1429000/7400423 [00:39<02:29, 39878.00\nexamples/s]', '\\rMap:  19%|#9        | 1435000/7400423 [00:39<02:26, 40766.48\nexamples/s]', '\\rMap:  19%|#9        | 1441000/7400423 [00:40<02:21, 42055.82\nexamples/s]', '\\rMap:  20%|#9        | 1446000/7400423 [00:40<02:22, 41687.26\nexamples/s]', '\\rMap:  20%|#9        | 1451000/7400423 [00:40<02:24, 41265.42\nexamples/s]', '\\rMap:  20%|#9        | 1457000/7400423 [00:40<02:22, 41815.84\nexamples/s]', '\\rMap:  20%|#9        | 1462000/7400423 [00:40<02:24, 41237.08\nexamples/s]', '\\rMap:  20%|#9        | 1468000/7400423 [00:40<02:24, 40966.91\nexamples/s]', '\\rMap:  20%|#9        | 1474000/7400423 [00:40<02:33, 38668.01\nexamples/s]', '\\rMap:  20%|#9        | 1479000/7400423 [00:41<02:34, 38445.33\nexamples/s]', '\\rMap:  20%|##        | 1485000/7400423 [00:41<02:27, 40166.85\nexamples/s]', '\\rMap:  20%|##        | 1491000/7400423 [00:41<02:22, 41379.00\nexamples/s]', '\\rMap:  20%|##        | 1497000/7400423 [00:41<02:21, 41681.16\nexamples/s]', '\\rMap:  20%|##        | 1504000/7400423 [00:41<02:24, 40850.95\nexamples/s]', '\\rMap:  20%|##        | 1510000/7400423 [00:41<02:27, 40007.40\nexamples/s]', '\\rMap:  20%|##        | 1516000/7400423 [00:41<02:27, 40002.48\nexamples/s]', '\\rMap:  21%|##        | 1522000/7400423 [00:42<02:31, 38868.55\nexamples/s]', '\\rMap:  21%|##        | 1526000/7400423 [00:42<02:33, 38376.29\nexamples/s]', '\\rMap:  21%|##        | 1530000/7400423 [00:42<02:34, 37964.49\nexamples/s]', '\\rMap:  21%|##        | 1535000/7400423 [00:42<02:34, 38061.24\nexamples/s]', '\\rMap:  21%|##        | 1541000/7400423 [00:42<02:29, 39109.28\nexamples/s]', '\\rMap:  21%|##        | 1546000/7400423 [00:42<02:29, 39137.25\nexamples/s]', '\\rMap:  21%|##        | 1550000/7400423 [00:42<02:30, 38920.52\nexamples/s]', '\\rMap:  21%|##        | 1554000/7400423 [00:42<02:30, 38786.58\nexamples/s]', '\\rMap:  21%|##1       | 1558000/7400423 [00:43<02:31, 38610.23\nexamples/s]', '\\rMap:  21%|##1       | 1562000/7400423 [00:43<02:31, 38428.95\nexamples/s]', '\\rMap:  21%|##1       | 1566000/7400423 [00:43<02:32, 38364.82\nexamples/s]', '\\rMap:  21%|##1       | 1570000/7400423 [00:43<02:32, 38223.78\nexamples/s]', '\\rMap:  21%|##1       | 1575000/7400423 [00:43<02:30, 38751.01\nexamples/s]', '\\rMap:  21%|##1       | 1579000/7400423 [00:43<02:31, 38468.73\nexamples/s]', '\\rMap:  21%|##1       | 1583000/7400423 [00:43<02:30, 38645.82\nexamples/s]', '\\rMap:  21%|##1       | 1587000/7400423 [00:43<02:31, 38332.87\nexamples/s]', '\\rMap:  21%|##1       | 1591000/7400423 [00:43<02:37, 36817.25\nexamples/s]', '\\rMap:  22%|##1       | 1595000/7400423 [00:44<04:16, 22662.79\nexamples/s]', '\\rMap:  22%|##1       | 1601000/7400423 [00:44<03:28, 27851.36\nexamples/s]', '\\rMap:  22%|##1       | 1607000/7400423 [00:44<03:01, 31976.82\nexamples/s]', '\\rMap:  22%|##1       | 1611000/7400423 [00:44<02:52, 33616.85\nexamples/s]', '\\rMap:  22%|##1       | 1616000/7400423 [00:44<02:41, 35754.98\nexamples/s]', '\\rMap:  22%|##1       | 1622000/7400423 [00:44<02:34, 37367.10\nexamples/s]', '\\rMap:  22%|##1       | 1628000/7400423 [00:45<02:28, 38860.72\nexamples/s]', '\\rMap:  22%|##2       | 1634000/7400423 [00:45<02:26, 39469.58\nexamples/s]', '\\rMap:  22%|##2       | 1640000/7400423 [00:45<02:33, 37564.42\nexamples/s]', '\\rMap:  22%|##2       | 1644000/7400423 [00:45<02:40, 35921.70\nexamples/s]', '\\rMap:  22%|##2       | 1648000/7400423 [00:45<02:44, 34927.38\nexamples/s]', '\\rMap:  22%|##2       | 1653000/7400423 [00:45<02:37, 36422.02\nexamples/s]', '\\rMap:  22%|##2       | 1658000/7400423 [00:45<02:37, 36484.27\nexamples/s]', '\\rMap:  22%|##2       | 1662000/7400423 [00:45<02:35, 36801.13\nexamples/s]', '\\rMap:  23%|##2       | 1667000/7400423 [00:46<02:32, 37625.81\nexamples/s]', '\\rMap:  23%|##2       | 1671000/7400423 [00:46<02:31, 37710.86\nexamples/s]', '\\rMap:  23%|##2       | 1676000/7400423 [00:46<02:31, 37843.05\nexamples/s]', '\\rMap:  23%|##2       | 1680000/7400423 [00:46<02:35, 36834.78\nexamples/s]', '\\rMap:  23%|##2       | 1684000/7400423 [00:46<02:37, 36277.14\nexamples/s]', '\\rMap:  23%|##2       | 1688000/7400423 [00:46<02:36, 36611.20\nexamples/s]', '\\rMap:  23%|##2       | 1692000/7400423 [00:46<02:34, 37043.86\nexamples/s]', '\\rMap:  23%|##2       | 1696000/7400423 [00:46<02:31, 37553.21\nexamples/s]', '\\rMap:  23%|##2       | 1701000/7400423 [00:47<02:28, 38267.19\nexamples/s]', '\\rMap:  23%|##3       | 1706000/7400423 [00:47<02:26, 38755.98\nexamples/s]', '\\rMap:  23%|##3       | 1711000/7400423 [00:47<02:25, 39033.61\nexamples/s]', '\\rMap:  23%|##3       | 1715000/7400423 [00:47<02:28, 38305.63\nexamples/s]', '\\rMap:  23%|##3       | 1719000/7400423 [00:47<02:29, 38064.05\nexamples/s]', '\\rMap:  23%|##3       | 1725000/7400423 [00:47<02:22, 39715.50\nexamples/s]', '\\rMap:  23%|##3       | 1730000/7400423 [00:47<02:23, 39378.49\nexamples/s]', '\\rMap:  23%|##3       | 1734000/7400423 [00:47<02:25, 38863.00\nexamples/s]', '\\rMap:  23%|##3       | 1739000/7400423 [00:47<02:25, 38958.51\nexamples/s]', '\\rMap:  24%|##3       | 1743000/7400423 [00:48<02:27, 38350.71\nexamples/s]', '\\rMap:  24%|##3       | 1747000/7400423 [00:48<02:27, 38285.61\nexamples/s]', '\\rMap:  24%|##3       | 1751000/7400423 [00:48<02:27, 38386.75\nexamples/s]', '\\rMap:  24%|##3       | 1755000/7400423 [00:48<02:28, 38014.64\nexamples/s]', '\\rMap:  24%|##3       | 1759000/7400423 [00:48<02:28, 38094.03\nexamples/s]', '\\rMap:  24%|##3       | 1764000/7400423 [00:48<02:26, 38376.97\nexamples/s]', '\\rMap:  24%|##3       | 1769000/7400423 [00:48<02:24, 38867.00\nexamples/s]', '\\rMap:  24%|##3       | 1774000/7400423 [00:48<02:27, 38130.06\nexamples/s]', '\\rMap:  24%|##4       | 1778000/7400423 [00:49<02:31, 37121.61\nexamples/s]', '\\rMap:  24%|##4       | 1784000/7400423 [00:49<02:25, 38726.05\nexamples/s]', '\\rMap:  24%|##4       | 1790000/7400423 [00:49<02:20, 40060.35\nexamples/s]', '\\rMap:  24%|##4       | 1795000/7400423 [00:49<02:22, 39290.03\nexamples/s]', '\\rMap:  24%|##4       | 1800000/7400423 [00:49<02:21, 39635.11\nexamples/s]', '\\rMap:  24%|##4       | 1805000/7400423 [00:49<02:20, 39795.18\nexamples/s]', '\\rMap:  24%|##4       | 1809000/7400423 [00:49<02:28, 37772.25\nexamples/s]', '\\rMap:  24%|##4       | 1813000/7400423 [00:49<02:34, 36097.07\nexamples/s]', '\\rMap:  25%|##4       | 1817000/7400423 [00:50<02:40, 34820.87\nexamples/s]', '\\rMap:  25%|##4       | 1821000/7400423 [00:50<02:38, 35210.11\nexamples/s]', '\\rMap:  25%|##4       | 1825000/7400423 [00:50<02:40, 34708.15\nexamples/s]', '\\rMap:  25%|##4       | 1829000/7400423 [00:50<02:38, 35118.64\nexamples/s]', '\\rMap:  25%|##4       | 1833000/7400423 [00:50<02:36, 35552.47\nexamples/s]', '\\rMap:  25%|##4       | 1837000/7400423 [00:50<02:39, 34788.61\nexamples/s]', '\\rMap:  25%|##4       | 1841000/7400423 [00:50<02:40, 34589.50\nexamples/s]', '\\rMap:  25%|##4       | 1845000/7400423 [00:50<02:39, 34809.20\nexamples/s]', '\\rMap:  25%|##4       | 1849000/7400423 [00:50<02:37, 35166.26\nexamples/s]', '\\rMap:  25%|##5       | 1853000/7400423 [00:51<02:35, 35634.13\nexamples/s]', '\\rMap:  25%|##5       | 1857000/7400423 [00:51<02:39, 34703.90\nexamples/s]', '\\rMap:  25%|##5       | 1861000/7400423 [00:51<02:37, 35083.48\nexamples/s]', '\\rMap:  25%|##5       | 1865000/7400423 [00:51<02:35, 35598.02\nexamples/s]', '\\rMap:  25%|##5       | 1869000/7400423 [00:51<02:37, 35199.92\nexamples/s]', '\\rMap:  25%|##5       | 1874000/7400423 [00:51<02:33, 36010.14\nexamples/s]', '\\rMap:  25%|##5       | 1878000/7400423 [00:51<02:31, 36367.34\nexamples/s]', '\\rMap:  25%|##5       | 1882000/7400423 [00:51<02:31, 36331.31\nexamples/s]', '\\rMap:  25%|##5       | 1886000/7400423 [00:51<02:35, 35372.64\nexamples/s]', '\\rMap:  26%|##5       | 1890000/7400423 [00:52<02:35, 35517.65\nexamples/s]', '\\rMap:  26%|##5       | 1894000/7400423 [00:52<02:33, 35894.82\nexamples/s]', '\\rMap:  26%|##5       | 1898000/7400423 [00:52<02:31, 36376.56\nexamples/s]', '\\rMap:  26%|##5       | 1902000/7400423 [00:52<02:32, 36043.96\nexamples/s]', '\\rMap:  26%|##5       | 1906000/7400423 [00:52<02:33, 35733.19\nexamples/s]', '\\rMap:  26%|##5       | 1910000/7400423 [00:52<02:34, 35509.89\nexamples/s]', '\\rMap:  26%|##5       | 1914000/7400423 [00:52<02:34, 35580.82\nexamples/s]', '\\rMap:  26%|##5       | 1918000/7400423 [00:52<02:34, 35591.43\nexamples/s]', '\\rMap:  26%|##5       | 1922000/7400423 [00:53<02:36, 34933.10\nexamples/s]', '\\rMap:  26%|##6       | 1926000/7400423 [00:53<02:34, 35363.65\nexamples/s]', '\\rMap:  26%|##6       | 1930000/7400423 [00:53<02:35, 35206.55\nexamples/s]', '\\rMap:  26%|##6       | 1934000/7400423 [00:53<02:43, 33433.48\nexamples/s]', '\\rMap:  26%|##6       | 1938000/7400423 [00:53<02:45, 32985.69\nexamples/s]', '\\rMap:  26%|##6       | 1942000/7400423 [00:53<02:47, 32671.16\nexamples/s]', '\\rMap:  26%|##6       | 1946000/7400423 [00:53<02:48, 32428.82\nexamples/s]', '\\rMap:  26%|##6       | 1950000/7400423 [00:53<02:45, 32872.66\nexamples/s]', '\\rMap:  26%|##6       | 1954000/7400423 [00:53<02:47, 32506.84\nexamples/s]', '\\rMap:  26%|##6       | 1958000/7400423 [00:54<02:48, 32286.14\nexamples/s]', '\\rMap:  27%|##6       | 1963000/7400423 [00:54<02:38, 34327.77\nexamples/s]', '\\rMap:  27%|##6       | 1967000/7400423 [00:54<02:36, 34772.64\nexamples/s]', '\\rMap:  27%|##6       | 1973000/7400423 [00:54<02:19, 39045.87\nexamples/s]', '\\rMap:  27%|##6       | 1978000/7400423 [00:54<02:16, 39771.59\nexamples/s]', '\\rMap:  27%|##6       | 1982000/7400423 [00:54<02:18, 39102.78\nexamples/s]', '\\rMap:  27%|##6       | 1986000/7400423 [00:54<02:29, 36230.23\nexamples/s]', '\\rMap:  27%|##6       | 1991000/7400423 [00:54<02:28, 36532.43\nexamples/s]', '\\rMap:  27%|##6       | 1996000/7400423 [00:55<02:24, 37358.06\nexamples/s]', '\\rMap:  27%|##7       | 2001000/7400423 [00:55<02:21, 38123.59\nexamples/s]', '\\rMap:  27%|##7       | 2005000/7400423 [00:55<02:21, 38108.57\nexamples/s]', '\\rMap:  27%|##7       | 2009000/7400423 [00:55<02:21, 38013.42\nexamples/s]', '\\rMap:  27%|##7       | 2013000/7400423 [00:55<02:23, 37414.05\nexamples/s]', '\\rMap:  27%|##7       | 2019000/7400423 [00:55<02:20, 38189.89\nexamples/s]', '\\rMap:  27%|##7       | 2023000/7400423 [00:55<02:23, 37529.27\nexamples/s]', '\\rMap:  27%|##7       | 2027000/7400423 [00:55<02:23, 37326.70\nexamples/s]', '\\rMap:  27%|##7       | 2031000/7400423 [00:56<02:23, 37528.78\nexamples/s]', '\\rMap:  27%|##7       | 2035000/7400423 [00:56<02:22, 37713.01\nexamples/s]', '\\rMap:  28%|##7       | 2039000/7400423 [00:56<02:21, 37816.27\nexamples/s]', '\\rMap:  28%|##7       | 2044000/7400423 [00:56<02:19, 38455.16\nexamples/s]', '\\rMap:  28%|##7       | 2048000/7400423 [00:56<02:26, 36648.12\nexamples/s]', '\\rMap:  28%|##7       | 2052000/7400423 [00:56<02:30, 35588.38\nexamples/s]', '\\rMap:  28%|##7       | 2056000/7400423 [00:56<02:34, 34552.28\nexamples/s]', '\\rMap:  28%|##7       | 2061000/7400423 [00:56<02:26, 36528.90\nexamples/s]', '\\rMap:  28%|##7       | 2067000/7400423 [00:56<02:18, 38524.66\nexamples/s]', '\\rMap:  28%|##8       | 2073000/7400423 [00:57<02:14, 39535.70\nexamples/s]', '\\rMap:  28%|##8       | 2077000/7400423 [00:57<02:15, 39167.57\nexamples/s]', '\\rMap:  28%|##8       | 2082000/7400423 [00:57<02:15, 39381.32\nexamples/s]', '\\rMap:  28%|##8       | 2086000/7400423 [00:57<02:17, 38588.93\nexamples/s]', '\\rMap:  28%|##8       | 2090000/7400423 [00:57<02:16, 38778.90\nexamples/s]', '\\rMap:  28%|##8       | 2095000/7400423 [00:57<02:16, 38782.95\nexamples/s]', '\\rMap:  28%|##8       | 2099000/7400423 [00:57<02:19, 37989.61\nexamples/s]', '\\rMap:  28%|##8       | 2103000/7400423 [00:57<02:21, 37359.37\nexamples/s]', '\\rMap:  28%|##8       | 2107000/7400423 [00:58<02:20, 37665.00\nexamples/s]', '\\rMap:  29%|##8       | 2111000/7400423 [00:58<02:20, 37619.56\nexamples/s]', '\\rMap:  29%|##8       | 2115000/7400423 [00:58<02:20, 37551.42\nexamples/s]', '\\rMap:  29%|##8       | 2121000/7400423 [00:58<02:14, 39330.36\nexamples/s]', '\\rMap:  29%|##8       | 2127000/7400423 [00:58<02:10, 40376.70\nexamples/s]', '\\rMap:  29%|##8       | 2133000/7400423 [00:58<02:07, 41296.94\nexamples/s]', '\\rMap:  29%|##8       | 2139000/7400423 [00:58<02:06, 41633.61\nexamples/s]', '\\rMap:  29%|##8       | 2144000/7400423 [00:58<02:07, 41384.43\nexamples/s]', '\\rMap:  29%|##9       | 2150000/7400423 [00:59<02:09, 40584.48\nexamples/s]', '\\rMap:  29%|##9       | 2156000/7400423 [00:59<02:12, 39448.50\nexamples/s]', '\\rMap:  29%|##9       | 2160000/7400423 [00:59<02:13, 39343.50\nexamples/s]', '\\rMap:  29%|##9       | 2166000/7400423 [00:59<02:10, 39989.47\nexamples/s]', '\\rMap:  29%|##9       | 2172000/7400423 [00:59<02:08, 40808.80\nexamples/s]', '\\rMap:  29%|##9       | 2178000/7400423 [00:59<02:07, 41078.36\nexamples/s]', '\\rMap:  30%|##9       | 2184000/7400423 [00:59<02:04, 41926.62\nexamples/s]', '\\rMap:  30%|##9       | 2190000/7400423 [01:00<02:04, 41909.00\nexamples/s]', '\\rMap:  30%|##9       | 2196000/7400423 [01:00<02:00, 43032.77\nexamples/s]', '\\rMap:  30%|##9       | 2202000/7400423 [01:00<01:58, 43927.42\nexamples/s]', '\\rMap:  30%|##9       | 2208000/7400423 [01:00<01:57, 44080.47\nexamples/s]', '\\rMap:  30%|##9       | 2214000/7400423 [01:00<01:58, 43918.41\nexamples/s]', '\\rMap:  30%|##9       | 2220000/7400423 [01:00<01:59, 43500.11\nexamples/s]', '\\rMap:  30%|###       | 2226000/7400423 [01:00<02:00, 42886.05\nexamples/s]', '\\rMap:  30%|###       | 2231000/7400423 [01:00<02:00, 42811.60\nexamples/s]', '\\rMap:  30%|###       | 2237000/7400423 [01:01<01:57, 43784.55\nexamples/s]', '\\rMap:  30%|###       | 2243000/7400423 [01:01<01:56, 44154.85\nexamples/s]', '\\rMap:  30%|###       | 2249000/7400423 [01:01<01:55, 44681.32\nexamples/s]', '\\rMap:  30%|###       | 2255000/7400423 [01:01<01:53, 45183.60\nexamples/s]', '\\rMap:  31%|###       | 2261000/7400423 [01:01<01:53, 45384.76\nexamples/s]', '\\rMap:  31%|###       | 2267000/7400423 [01:01<01:53, 45037.28\nexamples/s]', '\\rMap:  31%|###       | 2273000/7400423 [01:01<01:55, 44270.49\nexamples/s]', '\\rMap:  31%|###       | 2279000/7400423 [01:02<01:58, 43373.55\nexamples/s]', '\\rMap:  31%|###       | 2285000/7400423 [01:02<01:58, 43259.83\nexamples/s]', '\\rMap:  31%|###       | 2291000/7400423 [01:02<01:57, 43416.24\nexamples/s]', '\\rMap:  31%|###1      | 2297000/7400423 [01:02<01:56, 43949.90\nexamples/s]', '\\rMap:  31%|###1      | 2303000/7400423 [01:02<01:55, 44116.29\nexamples/s]', '\\rMap:  31%|###1      | 2309000/7400423 [01:02<01:54, 44511.34\nexamples/s]', '\\rMap:  31%|###1      | 2315000/7400423 [01:02<01:54, 44358.70\nexamples/s]', '\\rMap:  31%|###1      | 2321000/7400423 [01:03<01:54, 44465.72\nexamples/s]', '\\rMap:  31%|###1      | 2327000/7400423 [01:03<01:53, 44742.21\nexamples/s]', '\\rMap:  32%|###1      | 2333000/7400423 [01:03<01:52, 45086.80\nexamples/s]', '\\rMap:  32%|###1      | 2339000/7400423 [01:03<01:53, 44578.21\nexamples/s]', '\\rMap:  32%|###1      | 2345000/7400423 [01:03<01:53, 44514.69\nexamples/s]', '\\rMap:  32%|###1      | 2351000/7400423 [01:03<01:54, 44039.50\nexamples/s]', '\\rMap:  32%|###1      | 2357000/7400423 [01:03<01:55, 43667.70\nexamples/s]', '\\rMap:  32%|###1      | 2362000/7400423 [01:03<01:56, 43084.90\nexamples/s]', '\\rMap:  32%|###1      | 2368000/7400423 [01:04<01:59, 42226.57\nexamples/s]', '\\rMap:  32%|###2      | 2374000/7400423 [01:04<02:10, 38430.90\nexamples/s]', '\\rMap:  32%|###2      | 2378000/7400423 [01:04<02:15, 37009.52\nexamples/s]', '\\rMap:  32%|###2      | 2382000/7400423 [01:04<02:22, 35296.68\nexamples/s]', '\\rMap:  32%|###2      | 2386000/7400423 [01:04<02:25, 34514.93\nexamples/s]', '\\rMap:  32%|###2      | 2390000/7400423 [01:04<02:32, 32923.62\nexamples/s]', '\\rMap:  32%|###2      | 2394000/7400423 [01:04<02:43, 30608.91\nexamples/s]', '\\rMap:  32%|###2      | 2398000/7400423 [01:05<02:44, 30344.48\nexamples/s]', '\\rMap:  32%|###2      | 2402000/7400423 [01:05<02:41, 31008.44\nexamples/s]', '\\rMap:  33%|###2      | 2406000/7400423 [01:05<02:39, 31247.29\nexamples/s]', '\\rMap:  33%|###2      | 2410000/7400423 [01:05<02:35, 32040.48\nexamples/s]', '\\rMap:  33%|###2      | 2416000/7400423 [01:05<02:21, 35228.35\nexamples/s]', '\\rMap:  33%|###2      | 2422000/7400423 [01:05<02:13, 37190.66\nexamples/s]', '\\rMap:  33%|###2      | 2428000/7400423 [01:05<02:08, 38738.34\nexamples/s]', '\\rMap:  33%|###2      | 2433000/7400423 [01:06<02:06, 39311.06\nexamples/s]', '\\rMap:  33%|###2      | 2439000/7400423 [01:06<02:03, 40171.51\nexamples/s]', '\\rMap:  33%|###3      | 2445000/7400423 [01:06<02:00, 41252.14\nexamples/s]', '\\rMap:  33%|###3      | 2451000/7400423 [01:06<02:00, 41121.89\nexamples/s]', '\\rMap:  33%|###3      | 2457000/7400423 [01:06<01:58, 41685.66\nexamples/s]', '\\rMap:  33%|###3      | 2463000/7400423 [01:06<01:58, 41837.74\nexamples/s]', '\\rMap:  33%|###3      | 2469000/7400423 [01:06<01:58, 41639.25\nexamples/s]', '\\rMap:  33%|###3      | 2475000/7400423 [01:07<01:57, 41996.95\nexamples/s]', '\\rMap:  34%|###3      | 2481000/7400423 [01:07<01:56, 42325.49\nexamples/s]', '\\rMap:  34%|###3      | 2486000/7400423 [01:07<01:57, 41843.19\nexamples/s]', '\\rMap:  34%|###3      | 2491000/7400423 [01:07<01:58, 41309.70\nexamples/s]', '\\rMap:  34%|###3      | 2497000/7400423 [01:07<01:56, 42045.72\nexamples/s]', '\\rMap:  34%|###3      | 2504000/7400423 [01:07<02:50, 28794.23\nexamples/s]', '\\rMap:  34%|###3      | 2510000/7400423 [01:08<02:33, 31872.02\nexamples/s]', '\\rMap:  34%|###3      | 2516000/7400423 [01:08<02:22, 34347.55\nexamples/s]', '\\rMap:  34%|###4      | 2522000/7400423 [01:08<02:13, 36616.27\nexamples/s]', '\\rMap:  34%|###4      | 2528000/7400423 [01:08<02:06, 38547.41\nexamples/s]', '\\rMap:  34%|###4      | 2534000/7400423 [01:08<02:01, 39900.36\nexamples/s]', '\\rMap:  34%|###4      | 2540000/7400423 [01:08<01:59, 40597.14\nexamples/s]', '\\rMap:  34%|###4      | 2546000/7400423 [01:08<01:58, 40977.10\nexamples/s]', '\\rMap:  34%|###4      | 2552000/7400423 [01:09<01:56, 41584.85\nexamples/s]', '\\rMap:  35%|###4      | 2558000/7400423 [01:09<01:54, 42158.89\nexamples/s]', '\\rMap:  35%|###4      | 2564000/7400423 [01:09<01:54, 42249.65\nexamples/s]', '\\rMap:  35%|###4      | 2570000/7400423 [01:09<01:53, 42382.59\nexamples/s]', '\\rMap:  35%|###4      | 2576000/7400423 [01:09<01:53, 42547.77\nexamples/s]', '\\rMap:  35%|###4      | 2582000/7400423 [01:09<01:51, 43187.94\nexamples/s]', '\\rMap:  35%|###4      | 2588000/7400423 [01:09<01:51, 43337.95\nexamples/s]', '\\rMap:  35%|###5      | 2593000/7400423 [01:09<01:52, 42566.79\nexamples/s]', '\\rMap:  35%|###5      | 2599000/7400423 [01:10<01:53, 42330.03\nexamples/s]', '\\rMap:  35%|###5      | 2604000/7400423 [01:10<01:55, 41437.50\nexamples/s]', '\\rMap:  35%|###5      | 2610000/7400423 [01:10<01:58, 40364.39\nexamples/s]', '\\rMap:  35%|###5      | 2616000/7400423 [01:10<01:55, 41313.49\nexamples/s]', '\\rMap:  35%|###5      | 2622000/7400423 [01:10<01:53, 41978.54\nexamples/s]', '\\rMap:  36%|###5      | 2628000/7400423 [01:10<01:52, 42284.76\nexamples/s]', '\\rMap:  36%|###5      | 2634000/7400423 [01:10<01:51, 42713.51\nexamples/s]', '\\rMap:  36%|###5      | 2639000/7400423 [01:11<01:52, 42205.63\nexamples/s]', '\\rMap:  36%|###5      | 2645000/7400423 [01:11<01:52, 42306.95\nexamples/s]', '\\rMap:  36%|###5      | 2651000/7400423 [01:11<01:52, 42315.39\nexamples/s]', '\\rMap:  36%|###5      | 2657000/7400423 [01:11<01:52, 42067.57\nexamples/s]', '\\rMap:  36%|###5      | 2663000/7400423 [01:11<01:53, 41834.56\nexamples/s]', '\\rMap:  36%|###6      | 2669000/7400423 [01:11<01:53, 41692.47\nexamples/s]', '\\rMap:  36%|###6      | 2675000/7400423 [01:11<01:53, 41552.64\nexamples/s]', '\\rMap:  36%|###6      | 2680000/7400423 [01:12<01:54, 41334.06\nexamples/s]', '\\rMap:  36%|###6      | 2685000/7400423 [01:12<01:54, 41201.52\nexamples/s]', '\\rMap:  36%|###6      | 2691000/7400423 [01:12<01:58, 39775.44\nexamples/s]', '\\rMap:  36%|###6      | 2695000/7400423 [01:12<02:00, 39106.61\nexamples/s]', '\\rMap:  36%|###6      | 2699000/7400423 [01:12<02:03, 38026.00\nexamples/s]', '\\rMap:  37%|###6      | 2703000/7400423 [01:12<02:04, 37796.86\nexamples/s]', '\\rMap:  37%|###6      | 2707000/7400423 [01:12<02:05, 37436.69\nexamples/s]', '\\rMap:  37%|###6      | 2711000/7400423 [01:12<02:05, 37376.46\nexamples/s]', '\\rMap:  37%|###6      | 2715000/7400423 [01:13<02:04, 37660.08\nexamples/s]', '\\rMap:  37%|###6      | 2721000/7400423 [01:13<01:59, 39064.62\nexamples/s]', '\\rMap:  37%|###6      | 2727000/7400423 [01:13<01:56, 40115.45\nexamples/s]', '\\rMap:  37%|###6      | 2731000/7400423 [01:13<01:58, 39453.53\nexamples/s]', '\\rMap:  37%|###6      | 2736000/7400423 [01:13<01:57, 39865.02\nexamples/s]', '\\rMap:  37%|###7      | 2741000/7400423 [01:13<01:57, 39623.02\nexamples/s]', '\\rMap:  37%|###7      | 2745000/7400423 [01:13<01:59, 38964.76\nexamples/s]', '\\rMap:  37%|###7      | 2749000/7400423 [01:13<02:05, 37155.72\nexamples/s]', '\\rMap:  37%|###7      | 2753000/7400423 [01:14<02:10, 35615.55\nexamples/s]', '\\rMap:  37%|###7      | 2757000/7400423 [01:14<02:10, 35664.60\nexamples/s]', '\\rMap:  37%|###7      | 2761000/7400423 [01:14<02:07, 36459.29\nexamples/s]', '\\rMap:  37%|###7      | 2766000/7400423 [01:14<02:04, 37136.74\nexamples/s]', '\\rMap:  37%|###7      | 2770000/7400423 [01:14<02:04, 37075.42\nexamples/s]', '\\rMap:  37%|###7      | 2774000/7400423 [01:14<02:05, 36803.25\nexamples/s]', '\\rMap:  38%|###7      | 2778000/7400423 [01:14<02:06, 36526.67\nexamples/s]', '\\rMap:  38%|###7      | 2782000/7400423 [01:14<02:07, 36207.10\nexamples/s]', '\\rMap:  38%|###7      | 2786000/7400423 [01:14<02:05, 36744.90\nexamples/s]', '\\rMap:  38%|###7      | 2790000/7400423 [01:15<02:05, 36692.04\nexamples/s]', '\\rMap:  38%|###7      | 2794000/7400423 [01:15<02:06, 36405.62\nexamples/s]', '\\rMap:  38%|###7      | 2798000/7400423 [01:15<02:04, 36883.53\nexamples/s]', '\\rMap:  38%|###7      | 2802000/7400423 [01:15<02:05, 36722.02\nexamples/s]', '\\rMap:  38%|###7      | 2806000/7400423 [01:15<02:04, 36987.64\nexamples/s]', '\\rMap:  38%|###7      | 2810000/7400423 [01:15<02:05, 36457.21\nexamples/s]', '\\rMap:  38%|###8      | 2814000/7400423 [01:15<02:05, 36455.15\nexamples/s]', '\\rMap:  38%|###8      | 2818000/7400423 [01:15<02:06, 36185.41\nexamples/s]', '\\rMap:  38%|###8      | 2822000/7400423 [01:15<02:09, 35248.51\nexamples/s]', '\\rMap:  38%|###8      | 2827000/7400423 [01:16<02:06, 36094.45\nexamples/s]', '\\rMap:  38%|###8      | 2833000/7400423 [01:16<01:59, 38097.14\nexamples/s]', '\\rMap:  38%|###8      | 2838000/7400423 [01:16<01:57, 38703.23\nexamples/s]', '\\rMap:  38%|###8      | 2843000/7400423 [01:16<01:58, 38605.51\nexamples/s]', '\\rMap:  38%|###8      | 2848000/7400423 [01:16<01:54, 39919.35\nexamples/s]', '\\rMap:  39%|###8      | 2854000/7400423 [01:16<01:50, 41121.65\nexamples/s]', '\\rMap:  39%|###8      | 2859000/7400423 [01:16<01:51, 40861.11\nexamples/s]', '\\rMap:  39%|###8      | 2865000/7400423 [01:16<01:50, 40900.20\nexamples/s]', '\\rMap:  39%|###8      | 2871000/7400423 [01:17<01:49, 41486.85\nexamples/s]', '\\rMap:  39%|###8      | 2877000/7400423 [01:17<01:45, 42908.61\nexamples/s]', '\\rMap:  39%|###8      | 2882000/7400423 [01:17<01:47, 41946.18\nexamples/s]', '\\rMap:  39%|###9      | 2887000/7400423 [01:17<01:48, 41448.00\nexamples/s]', '\\rMap:  39%|###9      | 2893000/7400423 [01:17<01:48, 41594.16\nexamples/s]', '\\rMap:  39%|###9      | 2899000/7400423 [01:17<01:51, 40344.88\nexamples/s]', '\\rMap:  39%|###9      | 2904000/7400423 [01:17<01:52, 39884.50\nexamples/s]', '\\rMap:  39%|###9      | 2908000/7400423 [01:18<01:53, 39430.77\nexamples/s]', '\\rMap:  39%|###9      | 2914000/7400423 [01:18<01:51, 40328.51\nexamples/s]', '\\rMap:  39%|###9      | 2920000/7400423 [01:18<01:46, 42172.53\nexamples/s]', '\\rMap:  40%|###9      | 2926000/7400423 [01:18<01:44, 42772.19\nexamples/s]', '\\rMap:  40%|###9      | 2932000/7400423 [01:18<01:44, 42693.05\nexamples/s]', '\\rMap:  40%|###9      | 2938000/7400423 [01:18<01:42, 43578.41\nexamples/s]', '\\rMap:  40%|###9      | 2944000/7400423 [01:18<01:42, 43296.32\nexamples/s]', '\\rMap:  40%|###9      | 2950000/7400423 [01:18<01:44, 42412.10\nexamples/s]', '\\rMap:  40%|###9      | 2956000/7400423 [01:19<01:44, 42677.73\nexamples/s]', '\\rMap:  40%|####      | 2961000/7400423 [01:19<01:45, 41994.77\nexamples/s]', '\\rMap:  40%|####      | 2966000/7400423 [01:19<01:46, 41592.74\nexamples/s]', '\\rMap:  40%|####      | 2972000/7400423 [01:19<01:47, 41319.29\nexamples/s]', '\\rMap:  40%|####      | 2978000/7400423 [01:19<01:46, 41676.61\nexamples/s]', '\\rMap:  40%|####      | 2983000/7400423 [01:19<01:47, 41280.82\nexamples/s]', '\\rMap:  40%|####      | 2989000/7400423 [01:19<01:51, 39713.75\nexamples/s]', '\\rMap:  40%|####      | 2993000/7400423 [01:20<01:52, 39016.34\nexamples/s]', '\\rMap:  40%|####      | 2997000/7400423 [01:20<01:54, 38598.36\nexamples/s]', '\\rMap:  41%|####      | 3001000/7400423 [01:20<01:53, 38847.99\nexamples/s]', '\\rMap:  41%|####      | 3006000/7400423 [01:20<01:51, 39331.80\nexamples/s]', '\\rMap:  41%|####      | 3011000/7400423 [01:20<01:50, 39716.26\nexamples/s]', '\\rMap:  41%|####      | 3017000/7400423 [01:20<01:47, 40634.26\nexamples/s]', '\\rMap:  41%|####      | 3023000/7400423 [01:20<01:46, 40943.28\nexamples/s]', '\\rMap:  41%|####      | 3029000/7400423 [01:20<01:44, 41692.98\nexamples/s]', '\\rMap:  41%|####1     | 3036000/7400423 [01:21<01:46, 40915.79\nexamples/s]', '\\rMap:  41%|####1     | 3042000/7400423 [01:21<01:49, 39704.38\nexamples/s]', '\\rMap:  41%|####1     | 3046000/7400423 [01:21<01:51, 39155.21\nexamples/s]', '\\rMap:  41%|####1     | 3050000/7400423 [01:21<01:53, 38398.39\nexamples/s]', '\\rMap:  41%|####1     | 3054000/7400423 [01:21<01:54, 38083.08\nexamples/s]', '\\rMap:  41%|####1     | 3058000/7400423 [01:21<01:54, 37905.77\nexamples/s]', '\\rMap:  41%|####1     | 3062000/7400423 [01:21<02:01, 35590.02\nexamples/s]', '\\rMap:  41%|####1     | 3066000/7400423 [01:21<02:06, 34218.00\nexamples/s]', '\\rMap:  41%|####1     | 3070000/7400423 [01:22<02:14, 32154.21\nexamples/s]', '\\rMap:  42%|####1     | 3074000/7400423 [01:22<02:19, 31018.38\nexamples/s]', '\\rMap:  42%|####1     | 3078000/7400423 [01:22<02:18, 31188.63\nexamples/s]', '\\rMap:  42%|####1     | 3082000/7400423 [01:22<02:15, 31954.07\nexamples/s]', '\\rMap:  42%|####1     | 3086000/7400423 [01:22<02:11, 32810.61\nexamples/s]', '\\rMap:  42%|####1     | 3090000/7400423 [01:22<02:07, 33797.53\nexamples/s]', '\\rMap:  42%|####1     | 3094000/7400423 [01:22<02:05, 34302.20\nexamples/s]', '\\rMap:  42%|####1     | 3098000/7400423 [01:22<02:03, 34927.01\nexamples/s]', '\\rMap:  42%|####1     | 3102000/7400423 [01:23<02:00, 35591.25\nexamples/s]', '\\rMap:  42%|####1     | 3106000/7400423 [01:23<02:01, 35301.46\nexamples/s]', '\\rMap:  42%|####2     | 3110000/7400423 [01:23<02:00, 35635.92\nexamples/s]', '\\rMap:  42%|####2     | 3114000/7400423 [01:23<02:04, 34400.03\nexamples/s]', '\\rMap:  42%|####2     | 3118000/7400423 [01:23<02:08, 33393.41\nexamples/s]', '\\rMap:  42%|####2     | 3123000/7400423 [01:23<02:00, 35356.20\nexamples/s]', '\\rMap:  42%|####2     | 3129000/7400423 [01:23<01:55, 36875.58\nexamples/s]', '\\rMap:  42%|####2     | 3133000/7400423 [01:23<01:56, 36542.41\nexamples/s]', '\\rMap:  42%|####2     | 3137000/7400423 [01:24<01:55, 37046.93\nexamples/s]', '\\rMap:  42%|####2     | 3142000/7400423 [01:24<01:52, 37835.54\nexamples/s]', '\\rMap:  43%|####2     | 3147000/7400423 [01:24<01:50, 38516.63\nexamples/s]', '\\rMap:  43%|####2     | 3153000/7400423 [01:24<01:47, 39335.89\nexamples/s]', '\\rMap:  43%|####2     | 3158000/7400423 [01:24<01:46, 39755.51\nexamples/s]', '\\rMap:  43%|####2     | 3163000/7400423 [01:24<01:47, 39478.77\nexamples/s]', '\\rMap:  43%|####2     | 3169000/7400423 [01:24<01:46, 39648.53\nexamples/s]', '\\rMap:  43%|####2     | 3175000/7400423 [01:24<01:45, 40217.11\nexamples/s]', '\\rMap:  43%|####2     | 3180000/7400423 [01:25<01:44, 40386.95\nexamples/s]', '\\rMap:  43%|####3     | 3186000/7400423 [01:25<01:42, 41246.96\nexamples/s]', '\\rMap:  43%|####3     | 3192000/7400423 [01:25<01:47, 39039.41\nexamples/s]', '\\rMap:  43%|####3     | 3196000/7400423 [01:25<01:51, 37634.30\nexamples/s]', '\\rMap:  43%|####3     | 3200000/7400423 [01:25<01:54, 36628.36\nexamples/s]', '\\rMap:  43%|####3     | 3204000/7400423 [01:25<01:54, 36655.61\nexamples/s]', '\\rMap:  43%|####3     | 3208000/7400423 [01:25<01:52, 37299.60\nexamples/s]', '\\rMap:  43%|####3     | 3212000/7400423 [01:25<01:53, 36870.24\nexamples/s]', '\\rMap:  43%|####3     | 3216000/7400423 [01:26<01:55, 36348.26\nexamples/s]', '\\rMap:  44%|####3     | 3220000/7400423 [01:26<01:55, 36281.26\nexamples/s]', '\\rMap:  44%|####3     | 3224000/7400423 [01:26<01:54, 36443.74\nexamples/s]', '\\rMap:  44%|####3     | 3228000/7400423 [01:26<01:56, 35883.04\nexamples/s]', '\\rMap:  44%|####3     | 3232000/7400423 [01:26<01:55, 36002.68\nexamples/s]', '\\rMap:  44%|####3     | 3236000/7400423 [01:26<01:57, 35521.90\nexamples/s]', '\\rMap:  44%|####3     | 3240000/7400423 [01:26<01:58, 35255.39\nexamples/s]', '\\rMap:  44%|####3     | 3244000/7400423 [01:26<01:57, 35422.92\nexamples/s]', '\\rMap:  44%|####3     | 3248000/7400423 [01:26<01:56, 35627.51\nexamples/s]', '\\rMap:  44%|####3     | 3252000/7400423 [01:27<01:55, 35938.03\nexamples/s]', '\\rMap:  44%|####3     | 3256000/7400423 [01:27<01:54, 36159.64\nexamples/s]', '\\rMap:  44%|####4     | 3260000/7400423 [01:27<01:55, 35788.93\nexamples/s]', '\\rMap:  44%|####4     | 3264000/7400423 [01:27<01:54, 36107.13\nexamples/s]', '\\rMap:  44%|####4     | 3268000/7400423 [01:27<01:54, 36169.66\nexamples/s]', '\\rMap:  44%|####4     | 3272000/7400423 [01:27<01:54, 35958.44\nexamples/s]', '\\rMap:  44%|####4     | 3276000/7400423 [01:27<01:53, 36247.08\nexamples/s]', '\\rMap:  44%|####4     | 3280000/7400423 [01:27<01:52, 36509.39\nexamples/s]', '\\rMap:  44%|####4     | 3284000/7400423 [01:27<01:52, 36558.06\nexamples/s]', '\\rMap:  44%|####4     | 3290000/7400423 [01:28<01:45, 39028.12\nexamples/s]', '\\rMap:  45%|####4     | 3296000/7400423 [01:28<01:40, 40843.31\nexamples/s]', '\\rMap:  45%|####4     | 3302000/7400423 [01:28<01:36, 42491.59\nexamples/s]', '\\rMap:  45%|####4     | 3308000/7400423 [01:28<01:33, 43643.12\nexamples/s]', '\\rMap:  45%|####4     | 3314000/7400423 [01:28<01:33, 43567.40\nexamples/s]', '\\rMap:  45%|####4     | 3320000/7400423 [01:28<01:33, 43743.10\nexamples/s]', '\\rMap:  45%|####4     | 3326000/7400423 [01:28<01:33, 43805.66\nexamples/s]', '\\rMap:  45%|####5     | 3332000/7400423 [01:29<01:31, 44225.32\nexamples/s]', '\\rMap:  45%|####5     | 3338000/7400423 [01:29<01:30, 44774.34\nexamples/s]', '\\rMap:  45%|####5     | 3344000/7400423 [01:29<01:30, 44594.89\nexamples/s]', '\\rMap:  45%|####5     | 3350000/7400423 [01:29<01:30, 44557.81\nexamples/s]', '\\rMap:  45%|####5     | 3356000/7400423 [01:29<01:32, 43907.53\nexamples/s]', '\\rMap:  45%|####5     | 3362000/7400423 [01:29<01:32, 43869.84\nexamples/s]', '\\rMap:  46%|####5     | 3368000/7400423 [01:29<01:33, 42977.52\nexamples/s]', '\\rMap:  46%|####5     | 3374000/7400423 [01:29<01:34, 42735.31\nexamples/s]', '\\rMap:  46%|####5     | 3380000/7400423 [01:30<01:31, 43853.10\nexamples/s]', '\\rMap:  46%|####5     | 3386000/7400423 [01:30<01:29, 44798.58\nexamples/s]', '\\rMap:  46%|####5     | 3392000/7400423 [01:30<01:29, 44640.94\nexamples/s]', '\\rMap:  46%|####5     | 3398000/7400423 [01:30<01:29, 44780.25\nexamples/s]', '\\rMap:  46%|####5     | 3404000/7400423 [01:30<01:29, 44721.17\nexamples/s]', '\\rMap:  46%|####6     | 3410000/7400423 [01:30<01:28, 44997.27\nexamples/s]', '\\rMap:  46%|####6     | 3416000/7400423 [01:31<02:09, 30731.80\nexamples/s]', '\\rMap:  46%|####6     | 3422000/7400423 [01:31<01:56, 34017.09\nexamples/s]', '\\rMap:  46%|####6     | 3428000/7400423 [01:31<01:47, 36812.81\nexamples/s]', '\\rMap:  46%|####6     | 3434000/7400423 [01:31<01:40, 39545.74\nexamples/s]', '\\rMap:  46%|####6     | 3440000/7400423 [01:31<01:38, 40324.67\nexamples/s]', '\\rMap:  47%|####6     | 3446000/7400423 [01:31<01:35, 41209.07\nexamples/s]', '\\rMap:  47%|####6     | 3452000/7400423 [01:31<01:34, 41844.65\nexamples/s]', '\\rMap:  47%|####6     | 3458000/7400423 [01:32<01:31, 42864.62\nexamples/s]', '\\rMap:  47%|####6     | 3464000/7400423 [01:32<01:30, 43663.15\nexamples/s]', '\\rMap:  47%|####6     | 3470000/7400423 [01:32<01:29, 44115.63\nexamples/s]', '\\rMap:  47%|####6     | 3476000/7400423 [01:32<01:28, 44105.20\nexamples/s]', '\\rMap:  47%|####7     | 3482000/7400423 [01:32<01:30, 43289.69\nexamples/s]', '\\rMap:  47%|####7     | 3488000/7400423 [01:32<01:30, 43222.58\nexamples/s]', '\\rMap:  47%|####7     | 3494000/7400423 [01:32<01:30, 43175.44\nexamples/s]', '\\rMap:  47%|####7     | 3500000/7400423 [01:33<01:29, 43478.11\nexamples/s]', '\\rMap:  47%|####7     | 3505000/7400423 [01:33<01:31, 42536.64\nexamples/s]', '\\rMap:  47%|####7     | 3510000/7400423 [01:33<01:32, 42013.94\nexamples/s]', '\\rMap:  48%|####7     | 3516000/7400423 [01:33<01:31, 42389.03\nexamples/s]', '\\rMap:  48%|####7     | 3522000/7400423 [01:33<01:30, 43010.29\nexamples/s]', '\\rMap:  48%|####7     | 3528000/7400423 [01:33<01:28, 43670.72\nexamples/s]', '\\rMap:  48%|####7     | 3533000/7400423 [01:33<01:30, 42761.89\nexamples/s]', '\\rMap:  48%|####7     | 3539000/7400423 [01:33<01:30, 42861.39\nexamples/s]', '\\rMap:  48%|####7     | 3545000/7400423 [01:34<01:28, 43585.03\nexamples/s]', '\\rMap:  48%|####7     | 3551000/7400423 [01:34<01:26, 44299.97\nexamples/s]', '\\rMap:  48%|####8     | 3557000/7400423 [01:34<01:25, 45093.49\nexamples/s]', '\\rMap:  48%|####8     | 3563000/7400423 [01:34<01:24, 45678.05\nexamples/s]', '\\rMap:  48%|####8     | 3569000/7400423 [01:34<01:23, 45808.12\nexamples/s]', '\\rMap:  48%|####8     | 3575000/7400423 [01:34<01:29, 42723.79\nexamples/s]', '\\rMap:  48%|####8     | 3580000/7400423 [01:34<01:29, 42570.36\nexamples/s]', '\\rMap:  48%|####8     | 3586000/7400423 [01:34<01:27, 43388.57\nexamples/s]', '\\rMap:  49%|####8     | 3591000/7400423 [01:35<01:28, 43068.72\nexamples/s]', '\\rMap:  49%|####8     | 3597000/7400423 [01:35<01:29, 42524.15\nexamples/s]', '\\rMap:  49%|####8     | 3602000/7400423 [01:35<01:31, 41722.73\nexamples/s]', '\\rMap:  49%|####8     | 3609000/7400423 [01:35<01:32, 40923.00\nexamples/s]', '\\rMap:  49%|####8     | 3615000/7400423 [01:35<01:37, 38916.91\nexamples/s]', '\\rMap:  49%|####8     | 3619000/7400423 [01:35<01:39, 37987.11\nexamples/s]', '\\rMap:  49%|####8     | 3623000/7400423 [01:35<01:42, 36840.74\nexamples/s]', '\\rMap:  49%|####9     | 3627000/7400423 [01:36<01:44, 36281.38\nexamples/s]', '\\rMap:  49%|####9     | 3631000/7400423 [01:36<01:42, 36695.01\nexamples/s]', '\\rMap:  49%|####9     | 3636000/7400423 [01:36<01:41, 36990.99\nexamples/s]', '\\rMap:  49%|####9     | 3640000/7400423 [01:36<01:41, 37061.97\nexamples/s]', '\\rMap:  49%|####9     | 3644000/7400423 [01:36<01:39, 37565.61\nexamples/s]', '\\rMap:  49%|####9     | 3648000/7400423 [01:36<01:39, 37773.25\nexamples/s]', '\\rMap:  49%|####9     | 3652000/7400423 [01:36<01:41, 37062.96\nexamples/s]', '\\rMap:  49%|####9     | 3658000/7400423 [01:36<01:34, 39536.23\nexamples/s]', '\\rMap:  50%|####9     | 3664000/7400423 [01:37<01:30, 41098.75\nexamples/s]', '\\rMap:  50%|####9     | 3670000/7400423 [01:37<01:28, 42110.73\nexamples/s]', '\\rMap:  50%|####9     | 3676000/7400423 [01:37<01:26, 42849.20\nexamples/s]', '\\rMap:  50%|####9     | 3682000/7400423 [01:37<01:25, 43613.04\nexamples/s]', '\\rMap:  50%|####9     | 3688000/7400423 [01:37<01:23, 44711.54\nexamples/s]', '\\rMap:  50%|####9     | 3694000/7400423 [01:37<01:22, 44884.46\nexamples/s]', '\\rMap:  50%|####9     | 3700000/7400423 [01:37<01:22, 44789.33\nexamples/s]', '\\rMap:  50%|#####     | 3706000/7400423 [01:37<01:21, 45393.09\nexamples/s]', '\\rMap:  50%|#####     | 3712000/7400423 [01:38<01:20, 45669.30\nexamples/s]', '\\rMap:  50%|#####     | 3718000/7400423 [01:38<01:20, 45685.01\nexamples/s]', '\\rMap:  50%|#####     | 3724000/7400423 [01:38<01:20, 45541.86\nexamples/s]', '\\rMap:  50%|#####     | 3730000/7400423 [01:38<01:21, 44991.19\nexamples/s]', '\\rMap:  50%|#####     | 3735000/7400423 [01:38<01:23, 43952.79\nexamples/s]', '\\rMap:  51%|#####     | 3740000/7400423 [01:38<01:24, 43074.96\nexamples/s]', '\\rMap:  51%|#####     | 3746000/7400423 [01:38<01:24, 43149.09\nexamples/s]', '\\rMap:  51%|#####     | 3751000/7400423 [01:38<01:25, 42630.02\nexamples/s]', '\\rMap:  51%|#####     | 3756000/7400423 [01:39<01:26, 42062.03\nexamples/s]', '\\rMap:  51%|#####     | 3761000/7400423 [01:39<01:27, 41742.67\nexamples/s]', '\\rMap:  51%|#####     | 3766000/7400423 [01:39<01:28, 41041.75\nexamples/s]', '\\rMap:  51%|#####     | 3772000/7400423 [01:39<01:31, 39786.51\nexamples/s]', '\\rMap:  51%|#####1    | 3777000/7400423 [01:39<01:30, 39969.58\nexamples/s]', '\\rMap:  51%|#####1    | 3783000/7400423 [01:39<01:27, 41200.49\nexamples/s]', '\\rMap:  51%|#####1    | 3789000/7400423 [01:39<01:24, 42556.32\nexamples/s]', '\\rMap:  51%|#####1    | 3795000/7400423 [01:40<01:23, 43008.51\nexamples/s]', '\\rMap:  51%|#####1    | 3800000/7400423 [01:40<01:25, 42117.06\nexamples/s]', '\\rMap:  51%|#####1    | 3805000/7400423 [01:40<01:27, 41168.74\nexamples/s]', '\\rMap:  51%|#####1    | 3810000/7400423 [01:40<01:28, 40366.79\nexamples/s]', '\\rMap:  52%|#####1    | 3816000/7400423 [01:40<01:34, 38082.01\nexamples/s]', '\\rMap:  52%|#####1    | 3820000/7400423 [01:40<01:36, 37148.75\nexamples/s]', '\\rMap:  52%|#####1    | 3824000/7400423 [01:40<01:38, 36279.89\nexamples/s]', '\\rMap:  52%|#####1    | 3828000/7400423 [01:40<01:38, 36365.14\nexamples/s]', '\\rMap:  52%|#####1    | 3832000/7400423 [01:41<01:38, 36407.17\nexamples/s]', '\\rMap:  52%|#####1    | 3836000/7400423 [01:41<01:37, 36632.39\nexamples/s]', '\\rMap:  52%|#####1    | 3840000/7400423 [01:41<01:36, 36912.25\nexamples/s]', '\\rMap:  52%|#####1    | 3844000/7400423 [01:41<01:35, 37397.99\nexamples/s]', '\\rMap:  52%|#####2    | 3849000/7400423 [01:41<01:34, 37617.69\nexamples/s]', '\\rMap:  52%|#####2    | 3854000/7400423 [01:41<01:36, 36747.09\nexamples/s]', '\\rMap:  52%|#####2    | 3858000/7400423 [01:41<01:42, 34532.14\nexamples/s]', '\\rMap:  52%|#####2    | 3862000/7400423 [01:41<01:39, 35446.87\nexamples/s]', '\\rMap:  52%|#####2    | 3867000/7400423 [01:42<01:34, 37451.07\nexamples/s]', '\\rMap:  52%|#####2    | 3873000/7400423 [01:42<01:30, 38989.16\nexamples/s]', '\\rMap:  52%|#####2    | 3879000/7400423 [01:42<01:26, 40564.44\nexamples/s]', '\\rMap:  52%|#####2    | 3885000/7400423 [01:42<01:26, 40869.61\nexamples/s]', '\\rMap:  53%|#####2    | 3890000/7400423 [01:42<01:26, 40397.22\nexamples/s]', '\\rMap:  53%|#####2    | 3896000/7400423 [01:42<01:26, 40695.91\nexamples/s]', '\\rMap:  53%|#####2    | 3901000/7400423 [01:42<01:26, 40667.58\nexamples/s]', '\\rMap:  53%|#####2    | 3907000/7400423 [01:42<01:24, 41395.98\nexamples/s]', '\\rMap:  53%|#####2    | 3913000/7400423 [01:43<01:25, 40910.56\nexamples/s]', '\\rMap:  53%|#####2    | 3918000/7400423 [01:43<01:24, 41118.47\nexamples/s]', '\\rMap:  53%|#####3    | 3924000/7400423 [01:43<01:24, 40990.06\nexamples/s]', '\\rMap:  53%|#####3    | 3930000/7400423 [01:43<01:22, 41884.24\nexamples/s]', '\\rMap:  53%|#####3    | 3935000/7400423 [01:43<01:24, 41156.04\nexamples/s]', '\\rMap:  53%|#####3    | 3940000/7400423 [01:43<01:24, 40798.49\nexamples/s]', '\\rMap:  53%|#####3    | 3946000/7400423 [01:43<01:22, 41746.17\nexamples/s]', '\\rMap:  53%|#####3    | 3951000/7400423 [01:44<01:23, 41347.54\nexamples/s]', '\\rMap:  53%|#####3    | 3958000/7400423 [01:44<01:23, 41288.73\nexamples/s]', '\\rMap:  54%|#####3    | 3964000/7400423 [01:44<01:22, 41489.50\nexamples/s]', '\\rMap:  54%|#####3    | 3969000/7400423 [01:44<01:23, 41203.93\nexamples/s]', '\\rMap:  54%|#####3    | 3975000/7400423 [01:44<01:24, 40341.81\nexamples/s]', '\\rMap:  54%|#####3    | 3981000/7400423 [01:44<01:28, 38729.30\nexamples/s]', '\\rMap:  54%|#####3    | 3985000/7400423 [01:44<01:29, 38173.95\nexamples/s]', '\\rMap:  54%|#####3    | 3989000/7400423 [01:45<01:31, 37467.62\nexamples/s]', '\\rMap:  54%|#####3    | 3993000/7400423 [01:45<01:32, 36872.65\nexamples/s]', '\\rMap:  54%|#####4    | 3997000/7400423 [01:45<01:34, 36054.94\nexamples/s]', '\\rMap:  54%|#####4    | 4001000/7400423 [01:45<01:38, 34616.16\nexamples/s]', '\\rMap:  54%|#####4    | 4005000/7400423 [01:45<01:39, 34186.78\nexamples/s]', '\\rMap:  54%|#####4    | 4009000/7400423 [01:45<01:39, 33936.45\nexamples/s]', '\\rMap:  54%|#####4    | 4013000/7400423 [01:45<01:39, 34091.82\nexamples/s]', '\\rMap:  54%|#####4    | 4017000/7400423 [01:45<01:37, 34811.82\nexamples/s]', '\\rMap:  54%|#####4    | 4021000/7400423 [01:45<01:36, 34867.80\nexamples/s]', '\\rMap:  54%|#####4    | 4025000/7400423 [01:46<01:38, 34343.22\nexamples/s]', '\\rMap:  54%|#####4    | 4029000/7400423 [01:46<01:40, 33490.16\nexamples/s]', '\\rMap:  54%|#####4    | 4033000/7400423 [01:46<01:42, 32738.80\nexamples/s]', '\\rMap:  55%|#####4    | 4037000/7400423 [01:46<01:41, 33053.68\nexamples/s]', '\\rMap:  55%|#####4    | 4041000/7400423 [01:46<01:38, 34102.51\nexamples/s]', '\\rMap:  55%|#####4    | 4045000/7400423 [01:46<01:38, 34140.70\nexamples/s]', '\\rMap:  55%|#####4    | 4049000/7400423 [01:46<01:36, 34694.96\nexamples/s]', '\\rMap:  55%|#####4    | 4053000/7400423 [01:46<01:34, 35499.92\nexamples/s]', '\\rMap:  55%|#####4    | 4057000/7400423 [01:46<01:31, 36404.11\nexamples/s]', '\\rMap:  55%|#####4    | 4061000/7400423 [01:47<01:29, 37168.85\nexamples/s]', '\\rMap:  55%|#####4    | 4066000/7400423 [01:47<01:26, 38667.12\nexamples/s]', '\\rMap:  55%|#####5    | 4071000/7400423 [01:47<01:25, 39089.36\nexamples/s]', '\\rMap:  55%|#####5    | 4076000/7400423 [01:47<01:24, 39230.85\nexamples/s]', '\\rMap:  55%|#####5    | 4081000/7400423 [01:47<01:24, 39209.33\nexamples/s]', '\\rMap:  55%|#####5    | 4085000/7400423 [01:47<01:25, 38984.61\nexamples/s]', '\\rMap:  55%|#####5    | 4089000/7400423 [01:47<01:25, 38628.23\nexamples/s]', '\\rMap:  55%|#####5    | 4093000/7400423 [01:47<01:25, 38704.96\nexamples/s]', '\\rMap:  55%|#####5    | 4097000/7400423 [01:48<01:26, 38311.03\nexamples/s]', '\\rMap:  55%|#####5    | 4101000/7400423 [01:48<01:26, 37936.73\nexamples/s]', '\\rMap:  55%|#####5    | 4105000/7400423 [01:48<01:27, 37826.09\nexamples/s]', '\\rMap:  56%|#####5    | 4109000/7400423 [01:48<01:26, 37857.17\nexamples/s]', '\\rMap:  56%|#####5    | 4113000/7400423 [01:48<01:27, 37554.71\nexamples/s]', '\\rMap:  56%|#####5    | 4117000/7400423 [01:48<01:28, 37000.45\nexamples/s]', '\\rMap:  56%|#####5    | 4121000/7400423 [01:48<01:27, 37504.93\nexamples/s]', '\\rMap:  56%|#####5    | 4125000/7400423 [01:48<01:27, 37446.80\nexamples/s]', '\\rMap:  56%|#####5    | 4129000/7400423 [01:48<01:27, 37599.05\nexamples/s]', '\\rMap:  56%|#####5    | 4133000/7400423 [01:48<01:27, 37283.11\nexamples/s]', '\\rMap:  56%|#####5    | 4138000/7400423 [01:49<01:26, 37874.78\nexamples/s]', '\\rMap:  56%|#####5    | 4142000/7400423 [01:49<01:26, 37497.02\nexamples/s]', '\\rMap:  56%|#####6    | 4146000/7400423 [01:49<01:26, 37617.74\nexamples/s]', '\\rMap:  56%|#####6    | 4151000/7400423 [01:49<01:25, 37924.07\nexamples/s]', '\\rMap:  56%|#####6    | 4155000/7400423 [01:49<01:25, 38104.37\nexamples/s]', '\\rMap:  56%|#####6    | 4159000/7400423 [01:49<01:24, 38263.40\nexamples/s]', '\\rMap:  56%|#####6    | 4163000/7400423 [01:49<01:25, 37705.19\nexamples/s]', '\\rMap:  56%|#####6    | 4167000/7400423 [01:49<01:26, 37584.38\nexamples/s]', '\\rMap:  56%|#####6    | 4172000/7400423 [01:50<01:24, 38175.08\nexamples/s]', '\\rMap:  56%|#####6    | 4176000/7400423 [01:50<01:25, 37928.91\nexamples/s]', '\\rMap:  56%|#####6    | 4180000/7400423 [01:50<01:25, 37782.07\nexamples/s]', '\\rMap:  57%|#####6    | 4184000/7400423 [01:50<01:26, 37002.19\nexamples/s]', '\\rMap:  57%|#####6    | 4188000/7400423 [01:50<01:27, 36610.18\nexamples/s]', '\\rMap:  57%|#####6    | 4192000/7400423 [01:50<01:28, 36227.91\nexamples/s]', '\\rMap:  57%|#####6    | 4196000/7400423 [01:50<01:27, 36590.14\nexamples/s]', '\\rMap:  57%|#####6    | 4200000/7400423 [01:50<01:26, 36795.25\nexamples/s]', '\\rMap:  57%|#####6    | 4204000/7400423 [01:50<01:26, 37115.20\nexamples/s]', '\\rMap:  57%|#####6    | 4208000/7400423 [01:50<01:24, 37721.44\nexamples/s]', '\\rMap:  57%|#####6    | 4212000/7400423 [01:51<01:24, 37685.03\nexamples/s]', '\\rMap:  57%|#####6    | 4217000/7400423 [01:51<01:22, 38437.99\nexamples/s]', '\\rMap:  57%|#####7    | 4222000/7400423 [01:51<01:20, 39524.46\nexamples/s]', '\\rMap:  57%|#####7    | 4227000/7400423 [01:51<01:20, 39296.86\nexamples/s]', '\\rMap:  57%|#####7    | 4231000/7400423 [01:51<01:21, 38994.93\nexamples/s]', '\\rMap:  57%|#####7    | 4235000/7400423 [01:51<01:22, 38477.56\nexamples/s]', '\\rMap:  57%|#####7    | 4239000/7400423 [01:51<01:23, 37875.18\nexamples/s]', '\\rMap:  57%|#####7    | 4243000/7400423 [01:51<01:22, 38302.82\nexamples/s]', '\\rMap:  57%|#####7    | 4248000/7400423 [01:52<01:21, 38597.24\nexamples/s]', '\\rMap:  57%|#####7    | 4252000/7400423 [01:52<01:22, 38236.45\nexamples/s]', '\\rMap:  58%|#####7    | 4256000/7400423 [01:52<01:24, 37128.83\nexamples/s]', '\\rMap:  58%|#####7    | 4260000/7400423 [01:52<01:26, 36491.81\nexamples/s]', '\\rMap:  58%|#####7    | 4264000/7400423 [01:52<01:24, 36922.44\nexamples/s]', '\\rMap:  58%|#####7    | 4268000/7400423 [01:52<01:26, 36044.59\nexamples/s]', '\\rMap:  58%|#####7    | 4272000/7400423 [01:52<01:24, 36940.25\nexamples/s]', '\\rMap:  58%|#####7    | 4277000/7400423 [01:52<01:22, 37643.09\nexamples/s]', '\\rMap:  58%|#####7    | 4282000/7400423 [01:52<01:20, 38894.18\nexamples/s]', '\\rMap:  58%|#####7    | 4288000/7400423 [01:53<01:18, 39841.08\nexamples/s]', '\\rMap:  58%|#####8    | 4294000/7400423 [01:53<01:16, 40753.87\nexamples/s]', '\\rMap:  58%|#####8    | 4300000/7400423 [01:53<01:14, 41622.09\nexamples/s]', '\\rMap:  58%|#####8    | 4305000/7400423 [01:53<01:15, 41133.33\nexamples/s]', '\\rMap:  58%|#####8    | 4310000/7400423 [01:53<01:15, 40934.12\nexamples/s]', '\\rMap:  58%|#####8    | 4316000/7400423 [01:53<01:14, 41197.99\nexamples/s]', '\\rMap:  58%|#####8    | 4321000/7400423 [01:53<01:14, 41197.09\nexamples/s]', '\\rMap:  58%|#####8    | 4327000/7400423 [01:53<01:13, 41778.71\nexamples/s]', '\\rMap:  59%|#####8    | 4333000/7400423 [01:54<01:16, 40247.54\nexamples/s]', '\\rMap:  59%|#####8    | 4338000/7400423 [01:54<01:15, 40591.25\nexamples/s]', '\\rMap:  59%|#####8    | 4343000/7400423 [01:54<01:15, 40696.62\nexamples/s]', '\\rMap:  59%|#####8    | 4349000/7400423 [01:54<01:14, 41044.05\nexamples/s]', '\\rMap:  59%|#####8    | 4356000/7400423 [01:54<01:16, 39948.13\nexamples/s]', '\\rMap:  59%|#####8    | 4362000/7400423 [01:55<01:54, 26546.59\nexamples/s]', '\\rMap:  59%|#####8    | 4366000/7400423 [01:55<01:46, 28436.94\nexamples/s]', '\\rMap:  59%|#####9    | 4370000/7400423 [01:55<01:43, 29158.44\nexamples/s]', '\\rMap:  59%|#####9    | 4374000/7400423 [01:55<01:41, 29687.47\nexamples/s]', '\\rMap:  59%|#####9    | 4378000/7400423 [01:55<01:39, 30377.26\nexamples/s]', '\\rMap:  59%|#####9    | 4382000/7400423 [01:55<01:37, 30860.16\nexamples/s]', '\\rMap:  59%|#####9    | 4386000/7400423 [01:55<01:38, 30733.03\nexamples/s]', '\\rMap:  59%|#####9    | 4390000/7400423 [01:55<01:37, 31030.43\nexamples/s]', '\\rMap:  59%|#####9    | 4395000/7400423 [01:56<01:30, 33294.87\nexamples/s]', '\\rMap:  59%|#####9    | 4399000/7400423 [01:56<01:27, 34410.96\nexamples/s]', '\\rMap:  60%|#####9    | 4404000/7400423 [01:56<01:24, 35664.26\nexamples/s]', '\\rMap:  60%|#####9    | 4408000/7400423 [01:56<01:23, 35639.38\nexamples/s]', '\\rMap:  60%|#####9    | 4412000/7400423 [01:56<01:24, 35481.83\nexamples/s]', '\\rMap:  60%|#####9    | 4416000/7400423 [01:56<01:25, 34769.39\nexamples/s]', '\\rMap:  60%|#####9    | 4420000/7400423 [01:56<01:24, 35436.39\nexamples/s]', '\\rMap:  60%|#####9    | 4424000/7400423 [01:56<01:23, 35734.75\nexamples/s]', '\\rMap:  60%|#####9    | 4428000/7400423 [01:57<01:23, 35612.99\nexamples/s]', '\\rMap:  60%|#####9    | 4432000/7400423 [01:57<01:23, 35703.77\nexamples/s]', '\\rMap:  60%|#####9    | 4436000/7400423 [01:57<01:23, 35595.35\nexamples/s]', '\\rMap:  60%|#####9    | 4440000/7400423 [01:57<01:23, 35376.91\nexamples/s]', '\\rMap:  60%|######    | 4444000/7400423 [01:57<01:26, 34137.67\nexamples/s]', '\\rMap:  60%|######    | 4448000/7400423 [01:57<01:29, 32934.23\nexamples/s]', '\\rMap:  60%|######    | 4452000/7400423 [01:57<01:27, 33547.51\nexamples/s]', '\\rMap:  60%|######    | 4456000/7400423 [01:57<01:26, 34073.57\nexamples/s]', '\\rMap:  60%|######    | 4460000/7400423 [01:57<01:28, 33147.44\nexamples/s]', '\\rMap:  60%|######    | 4464000/7400423 [01:58<01:28, 33127.64\nexamples/s]', '\\rMap:  60%|######    | 4468000/7400423 [01:58<01:28, 33190.36\nexamples/s]', '\\rMap:  60%|######    | 4472000/7400423 [01:58<01:27, 33519.48\nexamples/s]', '\\rMap:  60%|######    | 4476000/7400423 [01:58<01:26, 33728.54\nexamples/s]', '\\rMap:  61%|######    | 4480000/7400423 [01:58<01:27, 33438.37\nexamples/s]', '\\rMap:  61%|######    | 4484000/7400423 [01:58<01:27, 33428.91\nexamples/s]', '\\rMap:  61%|######    | 4488000/7400423 [01:58<01:29, 32573.97\nexamples/s]', '\\rMap:  61%|######    | 4492000/7400423 [01:58<01:27, 33338.56\nexamples/s]', '\\rMap:  61%|######    | 4496000/7400423 [01:59<01:26, 33482.88\nexamples/s]', '\\rMap:  61%|######    | 4500000/7400423 [01:59<01:25, 34108.49\nexamples/s]', '\\rMap:  61%|######    | 4504000/7400423 [01:59<01:23, 34608.33\nexamples/s]', '\\rMap:  61%|######    | 4508000/7400423 [01:59<01:26, 33275.84\nexamples/s]', '\\rMap:  61%|######    | 4512000/7400423 [01:59<01:32, 31211.34\nexamples/s]', '\\rMap:  61%|######1   | 4516000/7400423 [01:59<01:38, 29422.12\nexamples/s]', '\\rMap:  61%|######1   | 4520000/7400423 [01:59<01:35, 30128.71\nexamples/s]', '\\rMap:  61%|######1   | 4524000/7400423 [01:59<01:34, 30408.22\nexamples/s]', '\\rMap:  61%|######1   | 4528000/7400423 [02:00<01:32, 30901.24\nexamples/s]', '\\rMap:  61%|######1   | 4533000/7400423 [02:00<01:26, 33196.09\nexamples/s]', '\\rMap:  61%|######1   | 4537000/7400423 [02:00<01:24, 33900.30\nexamples/s]', '\\rMap:  61%|######1   | 4541000/7400423 [02:00<01:25, 33630.94\nexamples/s]', '\\rMap:  61%|######1   | 4545000/7400423 [02:00<01:23, 34091.35\nexamples/s]', '\\rMap:  61%|######1   | 4549000/7400423 [02:00<01:22, 34576.54\nexamples/s]', '\\rMap:  62%|######1   | 4553000/7400423 [02:00<01:21, 34943.34\nexamples/s]', '\\rMap:  62%|######1   | 4557000/7400423 [02:00<01:20, 35468.65\nexamples/s]', '\\rMap:  62%|######1   | 4561000/7400423 [02:01<01:23, 33837.83\nexamples/s]', '\\rMap:  62%|######1   | 4565000/7400423 [02:01<01:24, 33535.97\nexamples/s]', '\\rMap:  62%|######1   | 4569000/7400423 [02:01<01:28, 32020.68\nexamples/s]', '\\rMap:  62%|######1   | 4573000/7400423 [02:01<01:30, 31251.94\nexamples/s]', '\\rMap:  62%|######1   | 4577000/7400423 [02:01<01:33, 30183.65\nexamples/s]', '\\rMap:  62%|######1   | 4581000/7400423 [02:01<01:35, 29397.06\nexamples/s]', '\\rMap:  62%|######1   | 4585000/7400423 [02:01<01:35, 29354.99\nexamples/s]', '\\rMap:  62%|######2   | 4589000/7400423 [02:01<01:34, 29643.75\nexamples/s]', '\\rMap:  62%|######2   | 4593000/7400423 [02:02<01:31, 30831.69\nexamples/s]', '\\rMap:  62%|######2   | 4597000/7400423 [02:02<01:26, 32419.99\nexamples/s]', '\\rMap:  62%|######2   | 4601000/7400423 [02:02<01:26, 32441.43\nexamples/s]', '\\rMap:  62%|######2   | 4605000/7400423 [02:02<01:25, 32515.40\nexamples/s]', '\\rMap:  62%|######2   | 4609000/7400423 [02:02<01:25, 32816.97\nexamples/s]', '\\rMap:  62%|######2   | 4613000/7400423 [02:02<01:24, 33161.26\nexamples/s]', '\\rMap:  62%|######2   | 4617000/7400423 [02:02<01:20, 34405.09\nexamples/s]', '\\rMap:  62%|######2   | 4621000/7400423 [02:02<01:19, 35178.93\nexamples/s]', '\\rMap:  62%|######2   | 4625000/7400423 [02:03<01:17, 35799.12\nexamples/s]', '\\rMap:  63%|######2   | 4630000/7400423 [02:03<01:15, 36861.18\nexamples/s]', '\\rMap:  63%|######2   | 4634000/7400423 [02:03<01:14, 37168.36\nexamples/s]', '\\rMap:  63%|######2   | 4638000/7400423 [02:03<01:14, 36914.66\nexamples/s]', '\\rMap:  63%|######2   | 4643000/7400423 [02:03<01:12, 37831.09\nexamples/s]', '\\rMap:  63%|######2   | 4649000/7400423 [02:03<01:10, 39095.83\nexamples/s]', '\\rMap:  63%|######2   | 4654000/7400423 [02:03<01:10, 39066.02\nexamples/s]', '\\rMap:  63%|######2   | 4658000/7400423 [02:03<01:09, 39233.03\nexamples/s]', '\\rMap:  63%|######3   | 4664000/7400423 [02:04<01:09, 39357.76\nexamples/s]', '\\rMap:  63%|######3   | 4668000/7400423 [02:04<01:10, 38810.04\nexamples/s]', '\\rMap:  63%|######3   | 4672000/7400423 [02:04<01:10, 38689.89\nexamples/s]', '\\rMap:  63%|######3   | 4676000/7400423 [02:04<01:12, 37828.18\nexamples/s]', '\\rMap:  63%|######3   | 4680000/7400423 [02:04<01:11, 37840.48\nexamples/s]', '\\rMap:  63%|######3   | 4684000/7400423 [02:04<01:12, 37477.57\nexamples/s]', '\\rMap:  63%|######3   | 4688000/7400423 [02:04<01:12, 37310.58\nexamples/s]', '\\rMap:  63%|######3   | 4692000/7400423 [02:04<01:11, 37857.45\nexamples/s]', '\\rMap:  63%|######3   | 4696000/7400423 [02:04<01:14, 36419.16\nexamples/s]', '\\rMap:  64%|######3   | 4700000/7400423 [02:04<01:15, 35814.02\nexamples/s]', '\\rMap:  64%|######3   | 4704000/7400423 [02:05<01:16, 35426.98\nexamples/s]', '\\rMap:  64%|######3   | 4708000/7400423 [02:05<01:16, 35387.25\nexamples/s]', '\\rMap:  64%|######3   | 4712000/7400423 [02:05<01:15, 35789.61\nexamples/s]', '\\rMap:  64%|######3   | 4717000/7400423 [02:05<01:12, 36844.31\nexamples/s]', '\\rMap:  64%|######3   | 4721000/7400423 [02:05<01:11, 37634.46\nexamples/s]', '\\rMap:  64%|######3   | 4727000/7400423 [02:05<01:08, 39304.27\nexamples/s]', '\\rMap:  64%|######3   | 4733000/7400423 [02:05<01:05, 40904.08\nexamples/s]', '\\rMap:  64%|######4   | 4739000/7400423 [02:05<01:06, 40138.54\nexamples/s]', '\\rMap:  64%|######4   | 4744000/7400423 [02:06<01:05, 40270.35\nexamples/s]', '\\rMap:  64%|######4   | 4750000/7400423 [02:06<01:04, 41270.65\nexamples/s]', '\\rMap:  64%|######4   | 4756000/7400423 [02:06<01:01, 43035.57\nexamples/s]', '\\rMap:  64%|######4   | 4762000/7400423 [02:06<00:59, 44604.43\nexamples/s]', '\\rMap:  64%|######4   | 4768000/7400423 [02:06<00:58, 45382.80\nexamples/s]', '\\rMap:  65%|######4   | 4774000/7400423 [02:06<00:58, 44939.72\nexamples/s]', '\\rMap:  65%|######4   | 4780000/7400423 [02:06<00:58, 44414.91\nexamples/s]', '\\rMap:  65%|######4   | 4786000/7400423 [02:07<00:58, 44682.74\nexamples/s]', '\\rMap:  65%|######4   | 4792000/7400423 [02:07<00:58, 44832.01\nexamples/s]', '\\rMap:  65%|######4   | 4797000/7400423 [02:07<00:58, 44155.65\nexamples/s]', '\\rMap:  65%|######4   | 4803000/7400423 [02:07<00:59, 43538.94\nexamples/s]', '\\rMap:  65%|######4   | 4809000/7400423 [02:07<01:02, 41261.58\nexamples/s]', '\\rMap:  65%|######5   | 4816000/7400423 [02:07<01:03, 40999.28\nexamples/s]', '\\rMap:  65%|######5   | 4822000/7400423 [02:07<01:01, 42038.32\nexamples/s]', '\\rMap:  65%|######5   | 4828000/7400423 [02:08<00:59, 42934.50\nexamples/s]', '\\rMap:  65%|######5   | 4833000/7400423 [02:08<01:00, 42687.31\nexamples/s]', '\\rMap:  65%|######5   | 4839000/7400423 [02:08<01:03, 40656.82\nexamples/s]', '\\rMap:  65%|######5   | 4845000/7400423 [02:08<01:04, 39427.29\nexamples/s]', '\\rMap:  66%|######5   | 4849000/7400423 [02:08<01:05, 38688.36\nexamples/s]', '\\rMap:  66%|######5   | 4853000/7400423 [02:08<01:07, 38002.15\nexamples/s]', '\\rMap:  66%|######5   | 4858000/7400423 [02:08<01:05, 39062.16\nexamples/s]', '\\rMap:  66%|######5   | 4864000/7400423 [02:08<01:01, 41226.02\nexamples/s]', '\\rMap:  66%|######5   | 4870000/7400423 [02:09<00:59, 42546.63\nexamples/s]', '\\rMap:  66%|######5   | 4876000/7400423 [02:09<00:57, 43605.33\nexamples/s]', '\\rMap:  66%|######5   | 4882000/7400423 [02:09<00:58, 43241.90\nexamples/s]', '\\rMap:  66%|######6   | 4888000/7400423 [02:09<00:57, 43405.79\nexamples/s]', '\\rMap:  66%|######6   | 4894000/7400423 [02:09<00:57, 43222.60\nexamples/s]', '\\rMap:  66%|######6   | 4900000/7400423 [02:09<00:57, 43613.81\nexamples/s]', '\\rMap:  66%|######6   | 4905000/7400423 [02:09<00:57, 43179.91\nexamples/s]', '\\rMap:  66%|######6   | 4911000/7400423 [02:10<01:02, 39575.49\nexamples/s]', '\\rMap:  66%|######6   | 4917000/7400423 [02:10<01:07, 36637.37\nexamples/s]', '\\rMap:  66%|######6   | 4921000/7400423 [02:10<01:07, 36863.71\nexamples/s]', '\\rMap:  67%|######6   | 4925000/7400423 [02:10<01:06, 37010.20\nexamples/s]', '\\rMap:  67%|######6   | 4929000/7400423 [02:10<01:05, 37598.97\nexamples/s]', '\\rMap:  67%|######6   | 4934000/7400423 [02:10<01:04, 38394.77\nexamples/s]', '\\rMap:  67%|######6   | 4940000/7400423 [02:10<01:03, 38914.08\nexamples/s]', '\\rMap:  67%|######6   | 4945000/7400423 [02:10<01:02, 39177.44\nexamples/s]', '\\rMap:  67%|######6   | 4949000/7400423 [02:11<01:03, 38385.85\nexamples/s]', '\\rMap:  67%|######6   | 4953000/7400423 [02:11<01:03, 38320.23\nexamples/s]', '\\rMap:  67%|######6   | 4957000/7400423 [02:11<01:04, 37911.06\nexamples/s]', '\\rMap:  67%|######7   | 4962000/7400423 [02:11<01:02, 38767.59\nexamples/s]', '\\rMap:  67%|######7   | 4968000/7400423 [02:11<00:59, 40556.97\nexamples/s]', '\\rMap:  67%|######7   | 4974000/7400423 [02:11<00:59, 40853.00\nexamples/s]', '\\rMap:  67%|######7   | 4980000/7400423 [02:11<00:58, 41038.01\nexamples/s]', '\\rMap:  67%|######7   | 4986000/7400423 [02:11<00:59, 40684.58\nexamples/s]', '\\rMap:  67%|######7   | 4991000/7400423 [02:12<00:59, 40246.88\nexamples/s]', '\\rMap:  68%|######7   | 4997000/7400423 [02:12<01:01, 39091.09\nexamples/s]', '\\rMap:  68%|######7   | 5001000/7400423 [02:12<01:02, 38488.93\nexamples/s]', '\\rMap:  68%|######7   | 5006000/7400423 [02:12<01:01, 38880.69\nexamples/s]', '\\rMap:  68%|######7   | 5010000/7400423 [02:12<01:02, 38521.94\nexamples/s]', '\\rMap:  68%|######7   | 5014000/7400423 [02:12<01:01, 38688.12\nexamples/s]', '\\rMap:  68%|######7   | 5019000/7400423 [02:12<01:01, 38879.07\nexamples/s]', '\\rMap:  68%|######7   | 5023000/7400423 [02:12<01:01, 38529.23\nexamples/s]', '\\rMap:  68%|######7   | 5027000/7400423 [02:13<01:01, 38434.64\nexamples/s]', '\\rMap:  68%|######7   | 5031000/7400423 [02:13<01:02, 38162.69\nexamples/s]', '\\rMap:  68%|######8   | 5035000/7400423 [02:13<01:01, 38428.23\nexamples/s]', '\\rMap:  68%|######8   | 5040000/7400423 [02:13<01:01, 38693.72\nexamples/s]', '\\rMap:  68%|######8   | 5045000/7400423 [02:13<01:00, 39163.74\nexamples/s]', '\\rMap:  68%|######8   | 5050000/7400423 [02:13<00:59, 39214.09\nexamples/s]', '\\rMap:  68%|######8   | 5056000/7400423 [02:13<00:59, 39575.79\nexamples/s]', '\\rMap:  68%|######8   | 5060000/7400423 [02:13<00:59, 39121.22\nexamples/s]', '\\rMap:  68%|######8   | 5064000/7400423 [02:14<01:00, 38790.01\nexamples/s]', '\\rMap:  68%|######8   | 5068000/7400423 [02:14<01:00, 38722.60\nexamples/s]', '\\rMap:  69%|######8   | 5072000/7400423 [02:14<01:01, 37944.81\nexamples/s]', '\\rMap:  69%|######8   | 5076000/7400423 [02:14<01:01, 37700.37\nexamples/s]', '\\rMap:  69%|######8   | 5080000/7400423 [02:14<01:03, 36579.33\nexamples/s]', '\\rMap:  69%|######8   | 5084000/7400423 [02:14<01:03, 36712.49\nexamples/s]', '\\rMap:  69%|######8   | 5088000/7400423 [02:14<01:03, 36453.04\nexamples/s]', '\\rMap:  69%|######8   | 5092000/7400423 [02:14<01:03, 36244.38\nexamples/s]', '\\rMap:  69%|######8   | 5096000/7400423 [02:14<01:04, 36002.94\nexamples/s]', '\\rMap:  69%|######8   | 5100000/7400423 [02:15<01:04, 35699.37\nexamples/s]', '\\rMap:  69%|######8   | 5104000/7400423 [02:15<01:04, 35836.60\nexamples/s]', '\\rMap:  69%|######9   | 5109000/7400423 [02:15<01:02, 36639.56\nexamples/s]', '\\rMap:  69%|######9   | 5114000/7400423 [02:15<01:01, 37409.03\nexamples/s]', '\\rMap:  69%|######9   | 5118000/7400423 [02:15<01:01, 37261.55\nexamples/s]', '\\rMap:  69%|######9   | 5123000/7400423 [02:15<00:59, 37990.10\nexamples/s]', '\\rMap:  69%|######9   | 5129000/7400423 [02:15<00:57, 39291.74\nexamples/s]', '\\rMap:  69%|######9   | 5135000/7400423 [02:15<00:54, 41303.69\nexamples/s]', '\\rMap:  69%|######9   | 5140000/7400423 [02:16<00:54, 41608.68\nexamples/s]', '\\rMap:  70%|######9   | 5146000/7400423 [02:16<01:30, 24951.33\nexamples/s]', '\\rMap:  70%|######9   | 5150000/7400423 [02:16<01:29, 25080.37\nexamples/s]', '\\rMap:  70%|######9   | 5154000/7400423 [02:16<01:28, 25409.04\nexamples/s]', '\\rMap:  70%|######9   | 5158000/7400423 [02:16<01:26, 25827.57\nexamples/s]', '\\rMap:  70%|######9   | 5162000/7400423 [02:17<01:25, 26052.25\nexamples/s]', '\\rMap:  70%|######9   | 5166000/7400423 [02:17<01:25, 26252.05\nexamples/s]', '\\rMap:  70%|######9   | 5170000/7400423 [02:17<01:25, 26004.72\nexamples/s]', '\\rMap:  70%|######9   | 5174000/7400423 [02:17<01:22, 26839.47\nexamples/s]', '\\rMap:  70%|######9   | 5178000/7400423 [02:17<01:20, 27516.30\nexamples/s]', '\\rMap:  70%|#######   | 5182000/7400423 [02:17<01:18, 28219.89\nexamples/s]', '\\rMap:  70%|#######   | 5186000/7400423 [02:17<01:16, 29094.17\nexamples/s]', '\\rMap:  70%|#######   | 5190000/7400423 [02:18<01:14, 29483.20\nexamples/s]', '\\rMap:  70%|#######   | 5194000/7400423 [02:18<01:14, 29587.76\nexamples/s]', '\\rMap:  70%|#######   | 5198000/7400423 [02:18<01:12, 30569.84\nexamples/s]', '\\rMap:  70%|#######   | 5202000/7400423 [02:18<01:09, 31782.97\nexamples/s]', '\\rMap:  70%|#######   | 5207000/7400423 [02:18<01:04, 33953.34\nexamples/s]', '\\rMap:  70%|#######   | 5211000/7400423 [02:18<01:04, 34025.64\nexamples/s]', '\\rMap:  70%|#######   | 5215000/7400423 [02:18<01:04, 33641.65\nexamples/s]', '\\rMap:  71%|#######   | 5219000/7400423 [02:18<01:05, 33305.36\nexamples/s]', '\\rMap:  71%|#######   | 5223000/7400423 [02:18<01:04, 33598.09\nexamples/s]', '\\rMap:  71%|#######   | 5227000/7400423 [02:19<01:03, 34416.44\nexamples/s]', '\\rMap:  71%|#######   | 5231000/7400423 [02:19<01:02, 34839.03\nexamples/s]', '\\rMap:  71%|#######   | 5235000/7400423 [02:19<01:02, 34871.94\nexamples/s]', '\\rMap:  71%|#######   | 5239000/7400423 [02:19<01:01, 34899.88\nexamples/s]', '\\rMap:  71%|#######   | 5243000/7400423 [02:19<01:03, 33985.61\nexamples/s]', '\\rMap:  71%|#######   | 5247000/7400423 [02:19<01:04, 33442.42\nexamples/s]', '\\rMap:  71%|#######   | 5251000/7400423 [02:19<01:03, 33728.69\nexamples/s]', '\\rMap:  71%|#######1  | 5255000/7400423 [02:19<01:03, 33856.89\nexamples/s]', '\\rMap:  71%|#######1  | 5259000/7400423 [02:20<01:02, 34097.70\nexamples/s]', '\\rMap:  71%|#######1  | 5263000/7400423 [02:20<01:02, 34342.58\nexamples/s]', '\\rMap:  71%|#######1  | 5267000/7400423 [02:20<01:01, 34564.05\nexamples/s]', '\\rMap:  71%|#######1  | 5272000/7400423 [02:20<00:58, 36340.41\nexamples/s]', '\\rMap:  71%|#######1  | 5278000/7400423 [02:20<00:54, 38699.31\nexamples/s]', '\\rMap:  71%|#######1  | 5284000/7400423 [02:20<00:53, 39832.14\nexamples/s]', '\\rMap:  71%|#######1  | 5290000/7400423 [02:20<00:50, 41515.18\nexamples/s]', '\\rMap:  72%|#######1  | 5296000/7400423 [02:20<00:49, 42219.56\nexamples/s]', '\\rMap:  72%|#######1  | 5302000/7400423 [02:21<00:49, 42477.26\nexamples/s]', '\\rMap:  72%|#######1  | 5308000/7400423 [02:21<00:49, 42169.25\nexamples/s]', '\\rMap:  72%|#######1  | 5314000/7400423 [02:21<00:49, 42378.61\nexamples/s]', '\\rMap:  72%|#######1  | 5320000/7400423 [02:21<00:49, 42177.83\nexamples/s]', '\\rMap:  72%|#######1  | 5326000/7400423 [02:21<00:47, 43583.67\nexamples/s]', '\\rMap:  72%|#######2  | 5332000/7400423 [02:21<00:47, 43604.07\nexamples/s]', '\\rMap:  72%|#######2  | 5338000/7400423 [02:21<00:49, 41263.28\nexamples/s]', '\\rMap:  72%|#######2  | 5344000/7400423 [02:22<00:48, 42365.26\nexamples/s]', '\\rMap:  72%|#######2  | 5350000/7400423 [02:22<00:47, 42819.67\nexamples/s]', '\\rMap:  72%|#######2  | 5356000/7400423 [02:22<00:47, 43276.59\nexamples/s]', '\\rMap:  72%|#######2  | 5362000/7400423 [02:22<00:46, 43959.41\nexamples/s]', '\\rMap:  73%|#######2  | 5368000/7400423 [02:22<00:46, 44154.67\nexamples/s]', '\\rMap:  73%|#######2  | 5374000/7400423 [02:22<00:45, 44249.44\nexamples/s]', '\\rMap:  73%|#######2  | 5380000/7400423 [02:22<00:44, 45072.60\nexamples/s]', '\\rMap:  73%|#######2  | 5386000/7400423 [02:23<00:43, 46198.30\nexamples/s]', '\\rMap:  73%|#######2  | 5392000/7400423 [02:23<00:42, 46724.67\nexamples/s]', '\\rMap:  73%|#######2  | 5398000/7400423 [02:23<00:42, 46581.77\nexamples/s]', '\\rMap:  73%|#######3  | 5404000/7400423 [02:23<00:43, 45563.62\nexamples/s]', '\\rMap:  73%|#######3  | 5410000/7400423 [02:23<00:44, 45124.70\nexamples/s]', '\\rMap:  73%|#######3  | 5416000/7400423 [02:23<00:43, 45754.65\nexamples/s]', '\\rMap:  73%|#######3  | 5422000/7400423 [02:23<00:43, 45859.51\nexamples/s]', '\\rMap:  73%|#######3  | 5427000/7400423 [02:23<00:43, 45194.20\nexamples/s]', '\\rMap:  73%|#######3  | 5433000/7400423 [02:24<00:46, 41964.55\nexamples/s]', '\\rMap:  73%|#######3  | 5439000/7400423 [02:24<00:49, 39910.53\nexamples/s]', '\\rMap:  74%|#######3  | 5445000/7400423 [02:24<00:51, 38030.43\nexamples/s]', '\\rMap:  74%|#######3  | 5449000/7400423 [02:24<00:52, 37334.39\nexamples/s]', '\\rMap:  74%|#######3  | 5453000/7400423 [02:24<00:51, 37529.21\nexamples/s]', '\\rMap:  74%|#######3  | 5458000/7400423 [02:24<00:50, 38790.23\nexamples/s]', '\\rMap:  74%|#######3  | 5463000/7400423 [02:24<00:48, 39884.19\nexamples/s]', '\\rMap:  74%|#######3  | 5469000/7400423 [02:25<00:48, 40146.69\nexamples/s]', '\\rMap:  74%|#######3  | 5474000/7400423 [02:25<00:48, 39837.90\nexamples/s]', '\\rMap:  74%|#######4  | 5478000/7400423 [02:25<00:48, 39480.58\nexamples/s]', '\\rMap:  74%|#######4  | 5482000/7400423 [02:25<00:48, 39283.34\nexamples/s]', '\\rMap:  74%|#######4  | 5486000/7400423 [02:25<00:48, 39163.28\nexamples/s]', '\\rMap:  74%|#######4  | 5491000/7400423 [02:25<00:48, 39221.40\nexamples/s]', '\\rMap:  74%|#######4  | 5495000/7400423 [02:25<00:48, 39287.27\nexamples/s]', '\\rMap:  74%|#######4  | 5500000/7400423 [02:25<00:47, 39607.76\nexamples/s]', '\\rMap:  74%|#######4  | 5505000/7400423 [02:25<00:47, 40073.93\nexamples/s]', '\\rMap:  74%|#######4  | 5510000/7400423 [02:26<00:46, 40363.61\nexamples/s]', '\\rMap:  75%|#######4  | 5517000/7400423 [02:26<00:47, 39875.89\nexamples/s]', '\\rMap:  75%|#######4  | 5522000/7400423 [02:26<00:46, 39988.33\nexamples/s]', '\\rMap:  75%|#######4  | 5528000/7400423 [02:26<00:48, 38739.60\nexamples/s]', '\\rMap:  75%|#######4  | 5533000/7400423 [02:26<00:47, 39006.60\nexamples/s]', '\\rMap:  75%|#######4  | 5538000/7400423 [02:26<00:47, 39585.55\nexamples/s]', '\\rMap:  75%|#######4  | 5542000/7400423 [02:26<00:47, 39432.94\nexamples/s]', '\\rMap:  75%|#######4  | 5547000/7400423 [02:26<00:46, 39600.29\nexamples/s]', '\\rMap:  75%|#######5  | 5553000/7400423 [02:27<00:46, 40120.87\nexamples/s]', '\\rMap:  75%|#######5  | 5559000/7400423 [02:27<00:45, 40412.58\nexamples/s]', '\\rMap:  75%|#######5  | 5564000/7400423 [02:27<00:45, 40353.07\nexamples/s]', '\\rMap:  75%|#######5  | 5570000/7400423 [02:27<00:44, 40761.02\nexamples/s]', '\\rMap:  75%|#######5  | 5575000/7400423 [02:27<00:44, 40945.42\nexamples/s]', '\\rMap:  75%|#######5  | 5581000/7400423 [02:27<00:44, 40881.91\nexamples/s]', '\\rMap:  75%|#######5  | 5586000/7400423 [02:27<00:44, 40525.01\nexamples/s]', '\\rMap:  76%|#######5  | 5592000/7400423 [02:28<00:44, 40596.27\nexamples/s]', '\\rMap:  76%|#######5  | 5598000/7400423 [02:28<00:43, 41053.51\nexamples/s]', '\\rMap:  76%|#######5  | 5605000/7400423 [02:28<00:44, 40713.96\nexamples/s]', '\\rMap:  76%|#######5  | 5611000/7400423 [02:28<00:43, 40669.64\nexamples/s]', '\\rMap:  76%|#######5  | 5617000/7400423 [02:28<00:43, 41216.65\nexamples/s]', '\\rMap:  76%|#######5  | 5622000/7400423 [02:28<00:43, 40956.20\nexamples/s]', '\\rMap:  76%|#######6  | 5627000/7400423 [02:28<00:43, 40965.62\nexamples/s]', '\\rMap:  76%|#######6  | 5632000/7400423 [02:29<00:43, 40844.36\nexamples/s]', '\\rMap:  76%|#######6  | 5638000/7400423 [02:29<00:42, 41246.53\nexamples/s]', '\\rMap:  76%|#######6  | 5643000/7400423 [02:29<00:42, 41103.45\nexamples/s]', '\\rMap:  76%|#######6  | 5649000/7400423 [02:29<00:42, 41473.09\nexamples/s]', '\\rMap:  76%|#######6  | 5655000/7400423 [02:29<00:42, 41499.61\nexamples/s]', '\\rMap:  76%|#######6  | 5661000/7400423 [02:29<00:42, 41125.69\nexamples/s]', '\\rMap:  77%|#######6  | 5667000/7400423 [02:29<00:42, 41088.31\nexamples/s]', '\\rMap:  77%|#######6  | 5673000/7400423 [02:30<00:43, 39982.63\nexamples/s]', '\\rMap:  77%|#######6  | 5678000/7400423 [02:30<00:43, 39948.87\nexamples/s]', '\\rMap:  77%|#######6  | 5683000/7400423 [02:30<00:43, 39653.29\nexamples/s]', '\\rMap:  77%|#######6  | 5689000/7400423 [02:30<00:42, 39906.27\nexamples/s]', '\\rMap:  77%|#######6  | 5694000/7400423 [02:30<00:42, 40056.51\nexamples/s]', '\\rMap:  77%|#######7  | 5699000/7400423 [02:30<00:42, 40007.38\nexamples/s]', '\\rMap:  77%|#######7  | 5706000/7400423 [02:30<00:42, 39474.81\nexamples/s]', '\\rMap:  77%|#######7  | 5710000/7400423 [02:31<00:43, 38799.85\nexamples/s]', '\\rMap:  77%|#######7  | 5714000/7400423 [02:31<00:43, 38529.33\nexamples/s]', '\\rMap:  77%|#######7  | 5718000/7400423 [02:31<00:43, 38405.06\nexamples/s]', '\\rMap:  77%|#######7  | 5722000/7400423 [02:31<00:43, 38726.75\nexamples/s]', '\\rMap:  77%|#######7  | 5726000/7400423 [02:31<00:43, 38589.47\nexamples/s]', '\\rMap:  77%|#######7  | 5730000/7400423 [02:31<00:43, 38296.57\nexamples/s]', '\\rMap:  77%|#######7  | 5734000/7400423 [02:31<00:44, 37622.33\nexamples/s]', '\\rMap:  78%|#######7  | 5738000/7400423 [02:31<00:44, 37489.15\nexamples/s]', '\\rMap:  78%|#######7  | 5743000/7400423 [02:31<00:43, 38175.12\nexamples/s]', '\\rMap:  78%|#######7  | 5748000/7400423 [02:32<00:42, 38804.18\nexamples/s]', '\\rMap:  78%|#######7  | 5752000/7400423 [02:32<00:42, 38689.82\nexamples/s]', '\\rMap:  78%|#######7  | 5756000/7400423 [02:32<00:42, 38493.14\nexamples/s]', '\\rMap:  78%|#######7  | 5761000/7400423 [02:32<00:42, 38712.74\nexamples/s]', '\\rMap:  78%|#######7  | 5767000/7400423 [02:32<00:41, 39511.22\nexamples/s]', '\\rMap:  78%|#######8  | 5773000/7400423 [02:32<00:39, 40728.47\nexamples/s]', '\\rMap:  78%|#######8  | 5778000/7400423 [02:32<00:39, 40864.92\nexamples/s]', '\\rMap:  78%|#######8  | 5783000/7400423 [02:32<00:39, 41224.69\nexamples/s]', '\\rMap:  78%|#######8  | 5789000/7400423 [02:32<00:37, 43515.75\nexamples/s]', '\\rMap:  78%|#######8  | 5795000/7400423 [02:33<00:35, 44650.01\nexamples/s]', '\\rMap:  78%|#######8  | 5801000/7400423 [02:33<00:36, 44078.09\nexamples/s]', '\\rMap:  78%|#######8  | 5806000/7400423 [02:33<00:36, 43484.68\nexamples/s]', '\\rMap:  79%|#######8  | 5812000/7400423 [02:33<00:37, 41870.70\nexamples/s]', '\\rMap:  79%|#######8  | 5818000/7400423 [02:33<00:40, 39052.10\nexamples/s]', '\\rMap:  79%|#######8  | 5822000/7400423 [02:33<00:41, 38162.93\nexamples/s]', '\\rMap:  79%|#######8  | 5826000/7400423 [02:33<00:42, 37344.53\nexamples/s]', '\\rMap:  79%|#######8  | 5830000/7400423 [02:34<00:42, 36874.30\nexamples/s]', '\\rMap:  79%|#######8  | 5834000/7400423 [02:34<00:43, 35669.86\nexamples/s]', '\\rMap:  79%|#######8  | 5838000/7400423 [02:34<00:44, 35022.28\nexamples/s]', '\\rMap:  79%|#######8  | 5842000/7400423 [02:34<00:46, 33359.90\nexamples/s]', '\\rMap:  79%|#######8  | 5846000/7400423 [02:34<00:46, 33208.08\nexamples/s]', '\\rMap:  79%|#######9  | 5850000/7400423 [02:34<00:46, 33478.93\nexamples/s]', '\\rMap:  79%|#######9  | 5856000/7400423 [02:34<00:41, 37662.44\nexamples/s]', '\\rMap:  79%|#######9  | 5861000/7400423 [02:34<00:39, 38895.18\nexamples/s]', '\\rMap:  79%|#######9  | 5866000/7400423 [02:35<00:39, 39140.42\nexamples/s]', '\\rMap:  79%|#######9  | 5870000/7400423 [02:35<00:40, 38159.83\nexamples/s]', '\\rMap:  79%|#######9  | 5874000/7400423 [02:35<00:40, 37956.44\nexamples/s]', '\\rMap:  79%|#######9  | 5878000/7400423 [02:35<00:39, 38489.42\nexamples/s]', '\\rMap:  79%|#######9  | 5883000/7400423 [02:35<00:39, 38486.39\nexamples/s]', '\\rMap:  80%|#######9  | 5887000/7400423 [02:35<00:39, 37906.82\nexamples/s]', '\\rMap:  80%|#######9  | 5891000/7400423 [02:35<00:40, 37417.66\nexamples/s]', '\\rMap:  80%|#######9  | 5896000/7400423 [02:35<00:39, 38036.33\nexamples/s]', '\\rMap:  80%|#######9  | 5901000/7400423 [02:35<00:38, 38554.58\nexamples/s]', '\\rMap:  80%|#######9  | 5905000/7400423 [02:36<00:39, 37911.05\nexamples/s]', '\\rMap:  80%|#######9  | 5909000/7400423 [02:36<00:39, 37643.17\nexamples/s]', '\\rMap:  80%|#######9  | 5913000/7400423 [02:36<00:40, 36947.04\nexamples/s]', '\\rMap:  80%|#######9  | 5917000/7400423 [02:36<00:39, 37292.22\nexamples/s]', '\\rMap:  80%|########  | 5921000/7400423 [02:36<00:39, 37570.56\nexamples/s]', '\\rMap:  80%|########  | 5925000/7400423 [02:36<00:39, 37389.45\nexamples/s]', '\\rMap:  80%|########  | 5929000/7400423 [02:36<00:38, 37766.72\nexamples/s]', '\\rMap:  80%|########  | 5933000/7400423 [02:36<00:38, 38264.85\nexamples/s]', '\\rMap:  80%|########  | 5937000/7400423 [02:36<00:38, 38202.84\nexamples/s]', '\\rMap:  80%|########  | 5942000/7400423 [02:37<00:37, 38540.12\nexamples/s]', '\\rMap:  80%|########  | 5947000/7400423 [02:37<00:37, 39168.33\nexamples/s]', '\\rMap:  80%|########  | 5953000/7400423 [02:37<00:36, 39878.04\nexamples/s]', '\\rMap:  81%|########  | 5958000/7400423 [02:37<00:37, 38648.43\nexamples/s]', '\\rMap:  81%|########  | 5962000/7400423 [02:37<00:39, 36157.59\nexamples/s]', '\\rMap:  81%|########  | 5966000/7400423 [02:37<00:42, 33493.16\nexamples/s]', '\\rMap:  81%|########  | 5970000/7400423 [02:37<00:44, 31824.86\nexamples/s]', '\\rMap:  81%|########  | 5974000/7400423 [02:38<00:46, 30623.13\nexamples/s]', '\\rMap:  81%|########  | 5978000/7400423 [02:38<00:46, 30777.13\nexamples/s]', '\\rMap:  81%|########  | 5982000/7400423 [02:38<00:46, 30337.60\nexamples/s]', '\\rMap:  81%|########  | 5986000/7400423 [02:38<00:46, 30685.84\nexamples/s]', '\\rMap:  81%|########  | 5990000/7400423 [02:38<00:43, 32205.55\nexamples/s]', '\\rMap:  81%|########  | 5994000/7400423 [02:38<00:42, 33427.49\nexamples/s]', '\\rMap:  81%|########1 | 5998000/7400423 [02:38<01:01, 22720.83\nexamples/s]', '\\rMap:  81%|########1 | 6003000/7400423 [02:39<00:52, 26552.79\nexamples/s]', '\\rMap:  81%|########1 | 6007000/7400423 [02:39<00:48, 28867.49\nexamples/s]', '\\rMap:  81%|########1 | 6012000/7400423 [02:39<00:44, 31517.24\nexamples/s]', '\\rMap:  81%|########1 | 6016000/7400423 [02:39<00:42, 32458.52\nexamples/s]', '\\rMap:  81%|########1 | 6020000/7400423 [02:39<00:42, 32396.30\nexamples/s]', '\\rMap:  81%|########1 | 6024000/7400423 [02:39<00:42, 32197.38\nexamples/s]', '\\rMap:  81%|########1 | 6028000/7400423 [02:39<00:41, 32777.60\nexamples/s]', '\\rMap:  82%|########1 | 6032000/7400423 [02:39<00:40, 33505.74\nexamples/s]', '\\rMap:  82%|########1 | 6036000/7400423 [02:39<00:39, 34413.44\nexamples/s]', '\\rMap:  82%|########1 | 6041000/7400423 [02:40<00:37, 35977.41\nexamples/s]', '\\rMap:  82%|########1 | 6045000/7400423 [02:40<00:38, 35288.98\nexamples/s]', '\\rMap:  82%|########1 | 6049000/7400423 [02:40<00:39, 34033.86\nexamples/s]', '\\rMap:  82%|########1 | 6053000/7400423 [02:40<00:39, 33861.93\nexamples/s]', '\\rMap:  82%|########1 | 6057000/7400423 [02:40<00:39, 34223.99\nexamples/s]', '\\rMap:  82%|########1 | 6061000/7400423 [02:40<00:40, 33119.90\nexamples/s]', '\\rMap:  82%|########1 | 6065000/7400423 [02:40<00:40, 33076.64\nexamples/s]', '\\rMap:  82%|########2 | 6069000/7400423 [02:40<00:40, 33238.05\nexamples/s]', '\\rMap:  82%|########2 | 6073000/7400423 [02:41<00:39, 33343.82\nexamples/s]', '\\rMap:  82%|########2 | 6077000/7400423 [02:41<00:40, 33005.44\nexamples/s]', '\\rMap:  82%|########2 | 6081000/7400423 [02:41<00:40, 32704.98\nexamples/s]', '\\rMap:  82%|########2 | 6085000/7400423 [02:41<00:40, 32797.98\nexamples/s]', '\\rMap:  82%|########2 | 6090000/7400423 [02:41<00:37, 34485.43\nexamples/s]', '\\rMap:  82%|########2 | 6096000/7400423 [02:41<00:35, 37219.32\nexamples/s]', '\\rMap:  82%|########2 | 6102000/7400423 [02:41<00:33, 39167.43\nexamples/s]', '\\rMap:  83%|########2 | 6107000/7400423 [02:41<00:32, 39993.46\nexamples/s]', '\\rMap:  83%|########2 | 6112000/7400423 [02:42<00:32, 39703.65\nexamples/s]', '\\rMap:  83%|########2 | 6117000/7400423 [02:42<00:32, 39557.25\nexamples/s]', '\\rMap:  83%|########2 | 6122000/7400423 [02:42<00:32, 39857.76\nexamples/s]', '\\rMap:  83%|########2 | 6126000/7400423 [02:42<00:32, 39703.61\nexamples/s]', '\\rMap:  83%|########2 | 6131000/7400423 [02:42<00:31, 40164.69\nexamples/s]', '\\rMap:  83%|########2 | 6136000/7400423 [02:42<00:31, 40280.75\nexamples/s]', '\\rMap:  83%|########2 | 6142000/7400423 [02:42<00:30, 40668.29\nexamples/s]', '\\rMap:  83%|########3 | 6148000/7400423 [02:43<00:30, 40996.62\nexamples/s]', '\\rMap:  83%|########3 | 6154000/7400423 [02:43<00:31, 40180.23\nexamples/s]', '\\rMap:  83%|########3 | 6161000/7400423 [02:43<00:31, 39352.98\nexamples/s]', '\\rMap:  83%|########3 | 6167000/7400423 [02:43<00:31, 39733.26\nexamples/s]', '\\rMap:  83%|########3 | 6171000/7400423 [02:43<00:31, 39102.59\nexamples/s]', '\\rMap:  83%|########3 | 6177000/7400423 [02:43<00:31, 39369.54\nexamples/s]', '\\rMap:  84%|########3 | 6183000/7400423 [02:43<00:30, 40216.68\nexamples/s]', '\\rMap:  84%|########3 | 6189000/7400423 [02:44<00:29, 40973.57\nexamples/s]', '\\rMap:  84%|########3 | 6195000/7400423 [02:44<00:29, 40735.02\nexamples/s]', '\\rMap:  84%|########3 | 6201000/7400423 [02:44<00:30, 39805.95\nexamples/s]', '\\rMap:  84%|########3 | 6207000/7400423 [02:44<00:29, 39901.52\nexamples/s]', '\\rMap:  84%|########3 | 6211000/7400423 [02:44<00:30, 39499.79\nexamples/s]', '\\rMap:  84%|########3 | 6215000/7400423 [02:44<00:30, 38896.91\nexamples/s]', '\\rMap:  84%|########4 | 6219000/7400423 [02:44<00:30, 38894.45\nexamples/s]', '\\rMap:  84%|########4 | 6224000/7400423 [02:44<00:30, 39082.97\nexamples/s]', '\\rMap:  84%|########4 | 6229000/7400423 [02:45<00:29, 39681.10\nexamples/s]', '\\rMap:  84%|########4 | 6234000/7400423 [02:45<00:29, 39964.79\nexamples/s]', '\\rMap:  84%|########4 | 6239000/7400423 [02:45<00:29, 39824.10\nexamples/s]', '\\rMap:  84%|########4 | 6244000/7400423 [02:45<00:28, 39952.49\nexamples/s]', '\\rMap:  84%|########4 | 6249000/7400423 [02:45<00:28, 40367.83\nexamples/s]', '\\rMap:  85%|########4 | 6255000/7400423 [02:45<00:28, 39535.02\nexamples/s]', '\\rMap:  85%|########4 | 6259000/7400423 [02:45<00:30, 37076.55\nexamples/s]', '\\rMap:  85%|########4 | 6263000/7400423 [02:45<00:32, 35517.68\nexamples/s]', '\\rMap:  85%|########4 | 6267000/7400423 [02:46<00:33, 34309.39\nexamples/s]', '\\rMap:  85%|########4 | 6271000/7400423 [02:46<00:34, 33191.85\nexamples/s]', '\\rMap:  85%|########4 | 6275000/7400423 [02:46<00:34, 32848.80\nexamples/s]', '\\rMap:  85%|########4 | 6279000/7400423 [02:46<00:34, 32568.83\nexamples/s]', '\\rMap:  85%|########4 | 6283000/7400423 [02:46<00:34, 32470.56\nexamples/s]', '\\rMap:  85%|########4 | 6287000/7400423 [02:46<00:33, 32810.57\nexamples/s]', '\\rMap:  85%|########5 | 6291000/7400423 [02:46<00:33, 32756.18\nexamples/s]', '\\rMap:  85%|########5 | 6295000/7400423 [02:46<00:33, 32674.50\nexamples/s]', '\\rMap:  85%|########5 | 6300000/7400423 [02:47<00:31, 35010.64\nexamples/s]', '\\rMap:  85%|########5 | 6305000/7400423 [02:47<00:29, 36862.65\nexamples/s]', '\\rMap:  85%|########5 | 6310000/7400423 [02:47<00:28, 37788.28\nexamples/s]', '\\rMap:  85%|########5 | 6315000/7400423 [02:47<00:27, 38932.94\nexamples/s]', '\\rMap:  85%|########5 | 6321000/7400423 [02:47<00:26, 41458.17\nexamples/s]', '\\rMap:  85%|########5 | 6326000/7400423 [02:47<00:25, 41618.81\nexamples/s]', '\\rMap:  86%|########5 | 6332000/7400423 [02:47<00:26, 39984.59\nexamples/s]', '\\rMap:  86%|########5 | 6338000/7400423 [02:48<00:26, 40596.15\nexamples/s]', '\\rMap:  86%|########5 | 6343000/7400423 [02:48<00:26, 39712.61\nexamples/s]', '\\rMap:  86%|########5 | 6347000/7400423 [02:48<00:27, 38575.54\nexamples/s]', '\\rMap:  86%|########5 | 6351000/7400423 [02:48<00:27, 38609.76\nexamples/s]', '\\rMap:  86%|########5 | 6356000/7400423 [02:48<00:27, 38631.67\nexamples/s]', '\\rMap:  86%|########5 | 6361000/7400423 [02:48<00:26, 39169.46\nexamples/s]', '\\rMap:  86%|########6 | 6366000/7400423 [02:48<00:26, 39578.15\nexamples/s]', '\\rMap:  86%|########6 | 6371000/7400423 [02:48<00:25, 40119.09\nexamples/s]', '\\rMap:  86%|########6 | 6377000/7400423 [02:48<00:25, 40688.11\nexamples/s]', '\\rMap:  86%|########6 | 6383000/7400423 [02:49<00:24, 41284.44\nexamples/s]', '\\rMap:  86%|########6 | 6389000/7400423 [02:49<00:24, 41650.03\nexamples/s]', '\\rMap:  86%|########6 | 6395000/7400423 [02:49<00:23, 42399.12\nexamples/s]', '\\rMap:  86%|########6 | 6401000/7400423 [02:49<00:23, 42177.60\nexamples/s]', '\\rMap:  87%|########6 | 6406000/7400423 [02:49<00:23, 41860.66\nexamples/s]', '\\rMap:  87%|########6 | 6412000/7400423 [02:49<00:23, 41841.32\nexamples/s]', '\\rMap:  87%|########6 | 6418000/7400423 [02:49<00:23, 41986.95\nexamples/s]', '\\rMap:  87%|########6 | 6423000/7400423 [02:50<00:23, 41519.02\nexamples/s]', '\\rMap:  87%|########6 | 6429000/7400423 [02:50<00:23, 40917.18\nexamples/s]', '\\rMap:  87%|########6 | 6435000/7400423 [02:50<00:23, 41549.55\nexamples/s]', '\\rMap:  87%|########7 | 6441000/7400423 [02:50<00:22, 41878.36\nexamples/s]', '\\rMap:  87%|########7 | 6447000/7400423 [02:50<00:22, 42197.93\nexamples/s]', '\\rMap:  87%|########7 | 6452000/7400423 [02:50<00:22, 42374.02\nexamples/s]', '\\rMap:  87%|########7 | 6458000/7400423 [02:50<00:22, 42385.85\nexamples/s]', '\\rMap:  87%|########7 | 6464000/7400423 [02:51<00:21, 42822.33\nexamples/s]', '\\rMap:  87%|########7 | 6470000/7400423 [02:51<00:21, 43598.79\nexamples/s]', '\\rMap:  88%|########7 | 6476000/7400423 [02:51<00:21, 42448.78\nexamples/s]', '\\rMap:  88%|########7 | 6481000/7400423 [02:51<00:22, 41661.97\nexamples/s]', '\\rMap:  88%|########7 | 6487000/7400423 [02:51<00:22, 40814.50\nexamples/s]', '\\rMap:  88%|########7 | 6493000/7400423 [02:51<00:22, 39902.07\nexamples/s]', '\\rMap:  88%|########7 | 6498000/7400423 [02:51<00:22, 39977.87\nexamples/s]', '\\rMap:  88%|########7 | 6504000/7400423 [02:52<00:22, 40597.60\nexamples/s]', '\\rMap:  88%|########7 | 6510000/7400423 [02:52<00:21, 40729.97\nexamples/s]', '\\rMap:  88%|########8 | 6515000/7400423 [02:52<00:22, 40155.45\nexamples/s]', '\\rMap:  88%|########8 | 6521000/7400423 [02:52<00:22, 39182.09\nexamples/s]', '\\rMap:  88%|########8 | 6525000/7400423 [02:52<00:22, 38921.56\nexamples/s]', '\\rMap:  88%|########8 | 6530000/7400423 [02:52<00:22, 38761.46\nexamples/s]', '\\rMap:  88%|########8 | 6534000/7400423 [02:52<00:22, 38469.61\nexamples/s]', '\\rMap:  88%|########8 | 6540000/7400423 [02:52<00:21, 39372.05\nexamples/s]', '\\rMap:  88%|########8 | 6546000/7400423 [02:53<00:21, 40249.47\nexamples/s]', '\\rMap:  89%|########8 | 6552000/7400423 [02:53<00:20, 41321.69\nexamples/s]', '\\rMap:  89%|########8 | 6558000/7400423 [02:53<00:19, 42210.98\nexamples/s]', '\\rMap:  89%|########8 | 6565000/7400423 [02:53<00:19, 41772.55\nexamples/s]', '\\rMap:  89%|########8 | 6571000/7400423 [02:53<00:19, 42282.49\nexamples/s]', '\\rMap:  89%|########8 | 6577000/7400423 [02:53<00:19, 42943.58\nexamples/s]', '\\rMap:  89%|########8 | 6583000/7400423 [02:53<00:19, 42571.02\nexamples/s]', '\\rMap:  89%|########9 | 6589000/7400423 [02:54<00:18, 43873.11\nexamples/s]', '\\rMap:  89%|########9 | 6594000/7400423 [02:54<00:18, 43085.12\nexamples/s]', '\\rMap:  89%|########9 | 6599000/7400423 [02:54<00:19, 42116.86\nexamples/s]', '\\rMap:  89%|########9 | 6605000/7400423 [02:54<00:18, 42312.28\nexamples/s]', '\\rMap:  89%|########9 | 6611000/7400423 [02:54<00:18, 42537.29\nexamples/s]', '\\rMap:  89%|########9 | 6616000/7400423 [02:54<00:18, 42087.41\nexamples/s]', '\\rMap:  89%|########9 | 6622000/7400423 [02:54<00:19, 40309.99\nexamples/s]', '\\rMap:  90%|########9 | 6628000/7400423 [02:55<00:18, 41215.27\nexamples/s]', '\\rMap:  90%|########9 | 6634000/7400423 [02:55<00:18, 42255.22\nexamples/s]', '\\rMap:  90%|########9 | 6639000/7400423 [02:55<00:18, 41629.25\nexamples/s]', '\\rMap:  90%|########9 | 6645000/7400423 [02:55<00:18, 40333.13\nexamples/s]', '\\rMap:  90%|########9 | 6651000/7400423 [02:55<00:19, 38436.98\nexamples/s]', '\\rMap:  90%|########9 | 6655000/7400423 [02:55<00:19, 38086.61\nexamples/s]', '\\rMap:  90%|########9 | 6659000/7400423 [02:55<00:19, 37905.84\nexamples/s]', '\\rMap:  90%|######### | 6665000/7400423 [02:55<00:18, 40457.37\nexamples/s]', '\\rMap:  90%|######### | 6671000/7400423 [02:56<00:17, 42440.43\nexamples/s]', '\\rMap:  90%|######### | 6676000/7400423 [02:56<00:17, 41831.24\nexamples/s]', '\\rMap:  90%|######### | 6681000/7400423 [02:56<00:17, 41220.02\nexamples/s]', '\\rMap:  90%|######### | 6687000/7400423 [02:56<00:17, 40241.73\nexamples/s]', '\\rMap:  90%|######### | 6692000/7400423 [02:56<00:17, 40467.74\nexamples/s]', '\\rMap:  91%|######### | 6698000/7400423 [02:56<00:16, 41417.23\nexamples/s]', '\\rMap:  91%|######### | 6703000/7400423 [02:56<00:16, 41115.88\nexamples/s]', '\\rMap:  91%|######### | 6709000/7400423 [02:57<00:16, 41056.91\nexamples/s]', '\\rMap:  91%|######### | 6715000/7400423 [02:57<00:17, 39829.91\nexamples/s]', '\\rMap:  91%|######### | 6719000/7400423 [02:57<00:17, 39451.23\nexamples/s]', '\\rMap:  91%|######### | 6725000/7400423 [02:57<00:16, 40156.61\nexamples/s]', '\\rMap:  91%|######### | 6731000/7400423 [02:57<00:17, 39329.08\nexamples/s]', '\\rMap:  91%|#########1| 6735000/7400423 [02:57<00:16, 39161.62\nexamples/s]', '\\rMap:  91%|#########1| 6739000/7400423 [02:57<00:16, 39181.62\nexamples/s]', '\\rMap:  91%|#########1| 6744000/7400423 [02:57<00:16, 39288.34\nexamples/s]', '\\rMap:  91%|#########1| 6748000/7400423 [02:58<00:16, 38835.14\nexamples/s]', '\\rMap:  91%|#########1| 6752000/7400423 [02:58<00:16, 38295.88\nexamples/s]', '\\rMap:  91%|#########1| 6756000/7400423 [02:58<00:17, 37542.63\nexamples/s]', '\\rMap:  91%|#########1| 6760000/7400423 [02:58<00:17, 36920.98\nexamples/s]', '\\rMap:  91%|#########1| 6764000/7400423 [02:58<00:17, 36322.53\nexamples/s]', '\\rMap:  91%|#########1| 6768000/7400423 [02:58<00:17, 36666.56\nexamples/s]', '\\rMap:  92%|#########1| 6772000/7400423 [02:58<00:17, 35700.41\nexamples/s]', '\\rMap:  92%|#########1| 6776000/7400423 [02:58<00:17, 35921.76\nexamples/s]', '\\rMap:  92%|#########1| 6782000/7400423 [02:58<00:16, 38450.84\nexamples/s]', '\\rMap:  92%|#########1| 6788000/7400423 [02:59<00:15, 40634.14\nexamples/s]', '\\rMap:  92%|#########1| 6793000/7400423 [02:59<00:14, 41090.49\nexamples/s]', '\\rMap:  92%|#########1| 6799000/7400423 [02:59<00:15, 39388.79\nexamples/s]', '\\rMap:  92%|#########1| 6803000/7400423 [02:59<00:15, 38985.12\nexamples/s]', '\\rMap:  92%|#########1| 6808000/7400423 [02:59<00:15, 39398.79\nexamples/s]', '\\rMap:  92%|#########2| 6812000/7400423 [02:59<00:15, 38072.98\nexamples/s]', '\\rMap:  92%|#########2| 6817000/7400423 [02:59<00:15, 38607.57\nexamples/s]', '\\rMap:  92%|#########2| 6821000/7400423 [03:00<00:16, 34925.14\nexamples/s]', '\\rMap:  92%|#########2| 6825000/7400423 [03:00<00:17, 32759.97\nexamples/s]', '\\rMap:  92%|#########2| 6829000/7400423 [03:00<00:18, 31544.56\nexamples/s]', '\\rMap:  92%|#########2| 6833000/7400423 [03:00<00:18, 30861.58\nexamples/s]', '\\rMap:  92%|#########2| 6838000/7400423 [03:00<00:16, 33872.36\nexamples/s]', '\\rMap:  92%|#########2| 6844000/7400423 [03:00<00:14, 38169.50\nexamples/s]', '\\rMap:  93%|#########2| 6850000/7400423 [03:00<00:13, 40374.62\nexamples/s]', '\\rMap:  93%|#########2| 6856000/7400423 [03:00<00:12, 42441.32\nexamples/s]', '\\rMap:  93%|#########2| 6862000/7400423 [03:01<00:12, 42980.06\nexamples/s]', '\\rMap:  93%|#########2| 6868000/7400423 [03:01<00:13, 39965.71\nexamples/s]', '\\rMap:  93%|#########2| 6874000/7400423 [03:01<00:13, 37644.49\nexamples/s]', '\\rMap:  93%|#########2| 6878000/7400423 [03:01<00:14, 36100.95\nexamples/s]', '\\rMap:  93%|#########2| 6882000/7400423 [03:01<00:14, 34819.20\nexamples/s]', '\\rMap:  93%|#########3| 6886000/7400423 [03:01<00:14, 34987.84\nexamples/s]', '\\rMap:  93%|#########3| 6890000/7400423 [03:01<00:14, 34830.16\nexamples/s]', '\\rMap:  93%|#########3| 6894000/7400423 [03:01<00:14, 35634.25\nexamples/s]', '\\rMap:  93%|#########3| 6898000/7400423 [03:02<00:13, 35944.37\nexamples/s]', '\\rMap:  93%|#########3| 6902000/7400423 [03:02<00:13, 36303.72\nexamples/s]', '\\rMap:  93%|#########3| 6906000/7400423 [03:02<00:13, 36999.85\nexamples/s]', '\\rMap:  93%|#########3| 6910000/7400423 [03:02<00:13, 36634.87\nexamples/s]', '\\rMap:  93%|#########3| 6914000/7400423 [03:02<00:13, 36438.42\nexamples/s]', '\\rMap:  93%|#########3| 6918000/7400423 [03:02<00:13, 36766.04\nexamples/s]', '\\rMap:  94%|#########3| 6922000/7400423 [03:02<00:12, 36884.13\nexamples/s]', '\\rMap:  94%|#########3| 6926000/7400423 [03:02<00:12, 37328.08\nexamples/s]', '\\rMap:  94%|#########3| 6930000/7400423 [03:02<00:12, 37646.86\nexamples/s]', '\\rMap:  94%|#########3| 6934000/7400423 [03:03<00:12, 36981.99\nexamples/s]', '\\rMap:  94%|#########3| 6938000/7400423 [03:03<00:20, 22375.37\nexamples/s]', '\\rMap:  94%|#########3| 6944000/7400423 [03:03<00:16, 27587.49\nexamples/s]', '\\rMap:  94%|#########3| 6948000/7400423 [03:03<00:15, 29849.64\nexamples/s]', '\\rMap:  94%|#########3| 6952000/7400423 [03:03<00:14, 31814.14\nexamples/s]', '\\rMap:  94%|#########3| 6956000/7400423 [03:03<00:13, 32517.56\nexamples/s]', '\\rMap:  94%|#########4| 6960000/7400423 [03:03<00:13, 33788.58\nexamples/s]', '\\rMap:  94%|#########4| 6965000/7400423 [03:04<00:11, 36301.25\nexamples/s]', '\\rMap:  94%|#########4| 6970000/7400423 [03:04<00:11, 36949.13\nexamples/s]', '\\rMap:  94%|#########4| 6974000/7400423 [03:04<00:11, 37187.17\nexamples/s]', '\\rMap:  94%|#########4| 6979000/7400423 [03:04<00:11, 37892.89\nexamples/s]', '\\rMap:  94%|#########4| 6984000/7400423 [03:04<00:10, 38779.81\nexamples/s]', '\\rMap:  94%|#########4| 6990000/7400423 [03:04<00:10, 39819.66\nexamples/s]', '\\rMap:  95%|#########4| 6996000/7400423 [03:04<00:09, 41130.29\nexamples/s]', '\\rMap:  95%|#########4| 7003000/7400423 [03:05<00:09, 39910.83\nexamples/s]', '\\rMap:  95%|#########4| 7009000/7400423 [03:05<00:10, 38818.41\nexamples/s]', '\\rMap:  95%|#########4| 7013000/7400423 [03:05<00:09, 38903.14\nexamples/s]', '\\rMap:  95%|#########4| 7018000/7400423 [03:05<00:09, 39193.90\nexamples/s]', '\\rMap:  95%|#########4| 7022000/7400423 [03:05<00:09, 38584.31\nexamples/s]', '\\rMap:  95%|#########4| 7026000/7400423 [03:05<00:09, 38490.47\nexamples/s]', '\\rMap:  95%|#########4| 7030000/7400423 [03:05<00:09, 38056.43\nexamples/s]', '\\rMap:  95%|#########5| 7034000/7400423 [03:05<00:09, 37945.10\nexamples/s]', '\\rMap:  95%|#########5| 7038000/7400423 [03:05<00:09, 37536.03\nexamples/s]', '\\rMap:  95%|#########5| 7042000/7400423 [03:06<00:09, 38087.88\nexamples/s]', '\\rMap:  95%|#########5| 7046000/7400423 [03:06<00:09, 38122.72\nexamples/s]', '\\rMap:  95%|#########5| 7052000/7400423 [03:06<00:08, 39491.03\nexamples/s]', '\\rMap:  95%|#########5| 7058000/7400423 [03:06<00:08, 41767.49\nexamples/s]', '\\rMap:  95%|#########5| 7064000/7400423 [03:06<00:07, 42445.00\nexamples/s]', '\\rMap:  96%|#########5| 7069000/7400423 [03:06<00:07, 41660.47\nexamples/s]', '\\rMap:  96%|#########5| 7074000/7400423 [03:06<00:07, 41868.72\nexamples/s]', '\\rMap:  96%|#########5| 7080000/7400423 [03:06<00:07, 44519.00\nexamples/s]', '\\rMap:  96%|#########5| 7086000/7400423 [03:07<00:06, 46155.56\nexamples/s]', '\\rMap:  96%|#########5| 7092000/7400423 [03:07<00:06, 47992.35\nexamples/s]', '\\rMap:  96%|#########5| 7098000/7400423 [03:07<00:06, 49167.53\nexamples/s]', '\\rMap:  96%|#########5| 7104000/7400423 [03:07<00:05, 49996.86\nexamples/s]', '\\rMap:  96%|#########6| 7110000/7400423 [03:07<00:05, 50317.48\nexamples/s]', '\\rMap:  96%|#########6| 7116000/7400423 [03:07<00:05, 50681.15\nexamples/s]', '\\rMap:  96%|#########6| 7122000/7400423 [03:07<00:05, 48793.38\nexamples/s]', '\\rMap:  96%|#########6| 7128000/7400423 [03:07<00:05, 48710.98\nexamples/s]', '\\rMap:  96%|#########6| 7134000/7400423 [03:08<00:05, 49245.28\nexamples/s]', '\\rMap:  96%|#########6| 7139000/7400423 [03:08<00:05, 47656.73\nexamples/s]', '\\rMap:  97%|#########6| 7145000/7400423 [03:08<00:05, 43890.54\nexamples/s]', '\\rMap:  97%|#########6| 7151000/7400423 [03:08<00:06, 40983.01\nexamples/s]', '\\rMap:  97%|#########6| 7157000/7400423 [03:08<00:06, 39202.80\nexamples/s]', '\\rMap:  97%|#########6| 7163000/7400423 [03:08<00:06, 37905.03\nexamples/s]', '\\rMap:  97%|#########6| 7167000/7400423 [03:08<00:06, 37211.60\nexamples/s]', '\\rMap:  97%|#########6| 7171000/7400423 [03:09<00:06, 36809.17\nexamples/s]', '\\rMap:  97%|#########6| 7175000/7400423 [03:09<00:06, 36373.69\nexamples/s]', '\\rMap:  97%|#########7| 7179000/7400423 [03:09<00:06, 35684.45\nexamples/s]', '\\rMap:  97%|#########7| 7183000/7400423 [03:09<00:06, 34898.21\nexamples/s]', '\\rMap:  97%|#########7| 7187000/7400423 [03:09<00:06, 34162.65\nexamples/s]', '\\rMap:  97%|#########7| 7191000/7400423 [03:09<00:06, 34395.16\nexamples/s]', '\\rMap:  97%|#########7| 7195000/7400423 [03:09<00:05, 34831.91\nexamples/s]', '\\rMap:  97%|#########7| 7199000/7400423 [03:09<00:05, 35094.75\nexamples/s]', '\\rMap:  97%|#########7| 7203000/7400423 [03:09<00:05, 35266.85\nexamples/s]', '\\rMap:  97%|#########7| 7207000/7400423 [03:10<00:05, 35819.23\nexamples/s]', '\\rMap:  97%|#########7| 7212000/7400423 [03:10<00:05, 36720.59\nexamples/s]', '\\rMap:  98%|#########7| 7216000/7400423 [03:10<00:05, 36541.50\nexamples/s]', '\\rMap:  98%|#########7| 7221000/7400423 [03:10<00:04, 36900.83\nexamples/s]', '\\rMap:  98%|#########7| 7225000/7400423 [03:10<00:04, 37070.91\nexamples/s]', '\\rMap:  98%|#########7| 7229000/7400423 [03:10<00:04, 37090.13\nexamples/s]', '\\rMap:  98%|#########7| 7233000/7400423 [03:10<00:04, 36756.67\nexamples/s]', '\\rMap:  98%|#########7| 7237000/7400423 [03:10<00:04, 36672.56\nexamples/s]', '\\rMap:  98%|#########7| 7241000/7400423 [03:10<00:04, 36718.40\nexamples/s]', '\\rMap:  98%|#########7| 7245000/7400423 [03:11<00:04, 36813.31\nexamples/s]', '\\rMap:  98%|#########7| 7249000/7400423 [03:11<00:04, 37138.77\nexamples/s]', '\\rMap:  98%|#########8| 7253000/7400423 [03:11<00:04, 36077.53\nexamples/s]', '\\rMap:  98%|#########8| 7257000/7400423 [03:11<00:04, 35354.71\nexamples/s]', '\\rMap:  98%|#########8| 7263000/7400423 [03:11<00:03, 39165.57\nexamples/s]', '\\rMap:  98%|#########8| 7268000/7400423 [03:11<00:03, 39867.97\nexamples/s]', '\\rMap:  98%|#########8| 7274000/7400423 [03:11<00:03, 40457.43\nexamples/s]', '\\rMap:  98%|#########8| 7279000/7400423 [03:11<00:02, 40742.91\nexamples/s]', '\\rMap:  98%|#########8| 7285000/7400423 [03:12<00:02, 38985.99\nexamples/s]', '\\rMap:  98%|#########8| 7289000/7400423 [03:12<00:02, 38240.49\nexamples/s]', '\\rMap:  99%|#########8| 7293000/7400423 [03:12<00:02, 37378.73\nexamples/s]', '\\rMap:  99%|#########8| 7297000/7400423 [03:12<00:02, 36313.99\nexamples/s]', '\\rMap:  99%|#########8| 7301000/7400423 [03:12<00:02, 36403.61\nexamples/s]', '\\rMap:  99%|#########8| 7305000/7400423 [03:12<00:02, 36059.41\nexamples/s]', '\\rMap:  99%|#########8| 7309000/7400423 [03:12<00:02, 36101.56\nexamples/s]', '\\rMap:  99%|#########8| 7313000/7400423 [03:12<00:02, 35972.55\nexamples/s]', '\\rMap:  99%|#########8| 7317000/7400423 [03:13<00:02, 36230.55\nexamples/s]', '\\rMap:  99%|#########8| 7321000/7400423 [03:13<00:02, 36208.96\nexamples/s]', '\\rMap:  99%|#########8| 7325000/7400423 [03:13<00:02, 36136.15\nexamples/s]', '\\rMap:  99%|#########9| 7329000/7400423 [03:13<00:01, 35923.35\nexamples/s]', '\\rMap:  99%|#########9| 7334000/7400423 [03:13<00:01, 36530.71\nexamples/s]', '\\rMap:  99%|#########9| 7338000/7400423 [03:13<00:01, 36375.27\nexamples/s]', '\\rMap:  99%|#########9| 7342000/7400423 [03:13<00:01, 36592.17\nexamples/s]', '\\rMap:  99%|#########9| 7346000/7400423 [03:13<00:01, 36097.54\nexamples/s]', '\\rMap:  99%|#########9| 7350000/7400423 [03:13<00:01, 35920.51\nexamples/s]', '\\rMap:  99%|#########9| 7354000/7400423 [03:14<00:01, 36053.32\nexamples/s]', '\\rMap:  99%|#########9| 7358000/7400423 [03:14<00:01, 35115.98\nexamples/s]', '\\rMap:  99%|#########9| 7363000/7400423 [03:14<00:01, 35857.37\nexamples/s]', '\\rMap: 100%|#########9| 7367000/7400423 [03:14<00:00, 36322.44\nexamples/s]', '\\rMap: 100%|#########9| 7371000/7400423 [03:14<00:00, 35747.55\nexamples/s]', '\\rMap: 100%|#########9| 7375000/7400423 [03:14<00:00, 35801.31\nexamples/s]', '\\rMap: 100%|#########9| 7379000/7400423 [03:14<00:00, 36491.46\nexamples/s]', '\\rMap: 100%|#########9| 7383000/7400423 [03:14<00:00, 36855.55\nexamples/s]', '\\rMap: 100%|#########9| 7387000/7400423 [03:14<00:00, 37056.16\nexamples/s]', '\\rMap: 100%|#########9| 7391000/7400423 [03:15<00:00, 37212.80\nexamples/s]', '\\rMap: 100%|#########9| 7395000/7400423 [03:15<00:00, 37049.99\nexamples/s]', '\\rMap: 100%|#########9| 7399000/7400423 [03:15<00:00, 36448.02\nexamples/s]', '', '\\rMap: 100%|##########| 7400423/7400423 [03:15<00:00,\n37858.59 examples/s]', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 129, in <module>\\n    out = model(\\n          ^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/models/gpt2/modeling_gpt2.py\", line 1210, in forward\\n\ntransformer_outputs = self.transformer(\\n\n^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/models/gpt2/modeling_gpt2.py\", line 939, in forward\\n\noutputs = block(\\n              ^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/utils/deprecation.py\", line 172, in wrapped_func\\n\nreturn func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/models/gpt2/modeling_gpt2.py\", line 403, in forward\\n\nattn_output, self_attn_weights = self.attn(\\n\n^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/utils/deprecation.py\", line 172, in wrapped_func\\n\nreturn func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/models/gpt2/modeling_gpt2.py\", line 348, in forward\\n\nattn_output = self.c_proj(attn_output)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\\n\nreturn self._call_impl(*args, **kwargs)\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/torch/nn/modules/module.py\", line 1747, in _call_impl\\n    return\nforward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/transformers/pytorch_utils.py\", line 119, in forward\\n    x =\nx.view(size_out)\\n        ^^^^^^^^^^^^^^^^\\nKeyboardInterrupt\\n', 'TimeoutError:\nExecution exceeded the time limit of an hour']", "['Using device: cuda', '\\n', '[ag_news] Step 0: Spearman Corr=-0.6977,\nN_meta=10', '\\n', '[ag_news] Step 10: Spearman Corr=-0.4376, N_meta=20', '\\n',\n'[ag_news] Epoch 0: val_loss=1.3688, val_acc=0.4100', '\\n', '[ag_news] Step 0:\nSpearman Corr=-0.5053, N_meta=10', '\\n', '[ag_news] Step 10: Spearman\nCorr=-0.1835, N_meta=20', '\\n', '[ag_news] Epoch 1: val_loss=1.3292,\nval_acc=0.5000', '\\n', '[ag_news] Step 0: Spearman Corr=-0.0030, N_meta=40',\n'\\n', '[ag_news] Epoch 2: val_loss=1.2536, val_acc=0.6350', '\\n', '[yelp] Step\n0: Spearman Corr=0.2571, N_meta=10', '\\n', '[yelp] Step 10: Spearman\nCorr=0.2647, N_meta=20', '\\n', '[yelp] Epoch 0: val_loss=0.6728,\nval_acc=0.6900', '\\n', '[yelp] Step 0: Spearman Corr=-0.0211, N_meta=10', '\\n',\n'[yelp] Step 10: Spearman Corr=-0.4060, N_meta=5', '\\n', '[yelp] Step 15:\nSpearman Corr=-0.3023, N_meta=10', '\\n', '[yelp] Epoch 1: val_loss=0.6239,\nval_acc=0.8100', '\\n', '[yelp] Step 0: Spearman Corr=-0.1444, N_meta=20', '\\n',\n'[yelp] Epoch 2: val_loss=0.5459, val_acc=0.8400', '\\n', '[dbpedia] Step 0:\nSpearman Corr=0.1053, N_meta=10', '\\n', '[dbpedia] Step 10: Spearman\nCorr=0.3609, N_meta=20', '\\n', '[dbpedia] Epoch 0: val_loss=2.5969,\nval_acc=0.4350', '\\n', '[dbpedia] Step 0: Spearman Corr=0.1338, N_meta=10',\n'\\n', '[dbpedia] Step 10: Spearman Corr=0.1414, N_meta=20', '\\n', '[dbpedia]\nEpoch 1: val_loss=2.5175, val_acc=0.5700', '\\n', '[dbpedia] Step 0: Spearman\nCorr=0.2195, N_meta=40', '\\n', '[dbpedia] Epoch 2: val_loss=2.3887,\nval_acc=0.6150', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 33\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '[ag_news] Step 0: Spearman Corr=-0.1459,\nN_meta=10', '\\n', '[ag_news] Step 10: Spearman Corr=-0.1444, N_meta=20', '\\n',\n'[ag_news] Epoch 0: val_loss=1.3664, val_acc=0.5450', '\\n', '[ag_news] Step 0:\nSpearman Corr=-0.2180, N_meta=10', '\\n', '[ag_news] Step 10: Spearman\nCorr=-0.1669, N_meta=20', '\\n', '[ag_news] Epoch 1: val_loss=1.3271,\nval_acc=0.5800', '\\n', '[ag_news] Step 0: Spearman Corr=-0.1293, N_meta=40',\n'\\n', '[ag_news] Epoch 2: val_loss=1.2524, val_acc=0.6650', '\\n', '[yelp] Step\n0: Spearman Corr=0.1504, N_meta=10', '\\n', '[yelp] Step 10: Spearman\nCorr=0.2571, N_meta=20', '\\n', '[yelp] Epoch 0: val_loss=0.6695,\nval_acc=0.7200', '\\n', '[yelp] Step 0: Spearman Corr=0.0301, N_meta=10', '\\n',\n'[yelp] Step 10: Spearman Corr=0.3865, N_meta=20', '\\n', '[yelp] Epoch 1:\nval_loss=0.6205, val_acc=0.8050', '\\n', '[yelp] Step 0: Spearman Corr=0.3158,\nN_meta=10', '\\n', '[yelp] Step 10: Spearman Corr=0.8165, N_meta=20', '\\n',\n'[yelp] Epoch 2: val_loss=0.5449, val_acc=0.8250', '\\n', '[dbpedia] Step 0:\nSpearman Corr=-0.0662, N_meta=10', '\\n', '[dbpedia] Step 10: Spearman\nCorr=0.0301, N_meta=20', '\\n', '[dbpedia] Epoch 0: val_loss=2.6012,\nval_acc=0.2500', '\\n', '[dbpedia] Step 0: Spearman Corr=-0.0526, N_meta=10',\n'\\n', '[dbpedia] Step 10: Spearman Corr=0.1398, N_meta=20', '\\n', '[dbpedia]\nEpoch 1: val_loss=2.5308, val_acc=0.5100', '\\n', '[dbpedia] Step 0: Spearman\nCorr=-0.3474, N_meta=10', '\\n', '[dbpedia] Step 10: Spearman Corr=0.1985,\nN_meta=20', '\\n', '[dbpedia] Epoch 2: val_loss=2.4069, val_acc=0.6250', '\\n',\n'Saved experiment_data.npy', '\\n', 'Execution time: 30 seconds seconds (time\nlimit is an hour).']", "['Using device: cuda', '\\n', '[ag_news] Step 0: Spearman Corr=-0.2000,\nN_meta=10', '\\n', '[ag_news] Step 10: Spearman Corr=-0.8647, N_meta=5', '\\n',\n'[ag_news] Step 15: Spearman Corr=-0.3429, N_meta=10', '\\n', '[ag_news] Epoch 0:\nval_loss=1.3702, val_acc=0.2350', '\\n', '[ag_news] Step 0: Spearman\nCorr=-0.0842, N_meta=20', '\\n', '[ag_news] Epoch 1: val_loss=1.3254,\nval_acc=0.5850', '\\n', '[ag_news] Step 0: Spearman Corr=-0.1128, N_meta=10',\n'\\n', '[ag_news] Step 10: Spearman Corr=0.5474, N_meta=20', '\\n', '[ag_news]\nEpoch 2: val_loss=1.2434, val_acc=0.6800', '\\n', '[yelp] Step 0: Spearman\nCorr=0.0060, N_meta=10', '\\n', '[yelp] Step 10: Spearman Corr=-0.0135,\nN_meta=5', '\\n', '[yelp] Step 15: Spearman Corr=-0.1368, N_meta=2', '\\n',\n'[yelp] Epoch 0: val_loss=0.6707, val_acc=0.7950', '\\n', '[yelp] Step 0:\nSpearman Corr=-0.5414, N_meta=1', '\\n', '[yelp] Step 1: Spearman Corr=0.1594,\nN_meta=2', '\\n', '[yelp] Step 2: Spearman Corr=-0.1293, N_meta=1', '\\n', '[yelp]\nStep 3: Spearman Corr=-0.1293, N_meta=1', '\\n', '[yelp] Step 4: Spearman\nCorr=0.0316, N_meta=2', '\\n', '[yelp] Step 6: Spearman Corr=0.1714, N_meta=4',\n'\\n', '[yelp] Step 8: Spearman Corr=-0.2195, N_meta=2', '\\n', '[yelp] Step 10:\nSpearman Corr=0.1474, N_meta=4', '\\n', '[yelp] Step 12: Spearman Corr=-0.1338,\nN_meta=2', '\\n', '[yelp] Step 14: Spearman Corr=-0.0511, N_meta=4', '\\n',\n'[yelp] Epoch 1: val_loss=0.6180, val_acc=0.8200', '\\n', '[yelp] Step 0:\nSpearman Corr=-0.2466, N_meta=2', '\\n', '[yelp] Step 2: Spearman Corr=-0.0707,\nN_meta=4', '\\n', '[yelp] Step 4: Spearman Corr=-0.0195, N_meta=8', '\\n', '[yelp]\nStep 8: Spearman Corr=0.6481, N_meta=16', '\\n', '[yelp] Epoch 2:\nval_loss=0.5353, val_acc=0.8350', '\\n', '[dbpedia] Step 0: Spearman Corr=0.2827,\nN_meta=10', '\\n', '[dbpedia] Step 10: Spearman Corr=0.1196, N_meta=5', '\\n',\n'[dbpedia] Step 15: Spearman Corr=0.0932, N_meta=2', '\\n', '[dbpedia] Epoch 0:\nval_loss=2.6001, val_acc=0.2150', '\\n', '[dbpedia] Step 0: Spearman Corr=0.3278,\nN_meta=4', '\\n', '[dbpedia] Step 4: Spearman Corr=0.2271, N_meta=2', '\\n',\n'[dbpedia] Step 6: Spearman Corr=-0.2481, N_meta=1', '\\n', '[dbpedia] Step 7:\nSpearman Corr=-0.1624, N_meta=2', '\\n', '[dbpedia] Step 8: Spearman\nCorr=-0.0135, N_meta=4', '\\n', '[dbpedia] Step 12: Spearman Corr=-0.1925,\nN_meta=2', '\\n', '[dbpedia] Step 14: Spearman Corr=0.3098, N_meta=4', '\\n',\n'[dbpedia] Epoch 1: val_loss=2.5223, val_acc=0.4950', '\\n', '[dbpedia] Step 0:\nSpearman Corr=0.3008, N_meta=2', '\\n', '[dbpedia] Step 2: Spearman Corr=0.0677,\nN_meta=1', '\\n', '[dbpedia] Step 3: Spearman Corr=0.1955, N_meta=2', '\\n',\n'[dbpedia] Step 4: Spearman Corr=-0.1519, N_meta=1', '\\n', '[dbpedia] Step 5:\nSpearman Corr=0.0677, N_meta=2', '\\n', '[dbpedia] Step 6: Spearman Corr=-0.2376,\nN_meta=1', '\\n', '[dbpedia] Step 7: Spearman Corr=-0.3353, N_meta=1', '\\n',\n'[dbpedia] Step 8: Spearman Corr=0.1820, N_meta=2', '\\n', '[dbpedia] Step 10:\nSpearman Corr=-0.0075, N_meta=1', '\\n', '[dbpedia] Step 11: Spearman\nCorr=0.1579, N_meta=2', '\\n', '[dbpedia] Step 12: Spearman Corr=0.1278,\nN_meta=1', '\\n', '[dbpedia] Step 13: Spearman Corr=-0.3805, N_meta=1', '\\n',\n'[dbpedia] Step 14: Spearman Corr=0.2301, N_meta=2', '\\n', '[dbpedia] Epoch 2:\nval_loss=2.3888, val_acc=0.6200', '\\n', 'Saved experiment_data.npy', '\\n',\n'Execution time: 31 seconds seconds (time limit is an hour).']", ""], "analysis": ["", "The script fails with ImportError: cannot import name 'AdamW' from\n'transformers'. The fix is to import AdamW from torch.optim (i.e., use 'from\ntorch.optim import AdamW') or from 'transformers.optimization' instead of from\nthe top-level transformers module.", "The script crashes during the DVN meta-update when trying to index\n`train_dataset` with a 0-d torch tensor `idx`. HuggingFace datasets expect\nPython ints or lists of ints for indexing, not torch Tensors. Proposed fix:\nconvert `idx` to a Python integer (e.g., use `idx.item()`), or iterate over a\nPython list of ints via `.tolist()` (e.g., `for idx in\ntorch.randperm(len(train_dataset)).tolist()[:M]:`).", "", "The script crashes at loss.backward() with RuntimeError: \u2018Trying to backward\nthrough the graph a second time\u2026\u2019. This occurs because\ntorch.autograd.grad(loss_i.sum(), Xb) is called with the default\nretain_graph=False, which frees the computation graph before the subsequent\nbackward pass. To fix this, you can call torch.autograd.grad(loss_i.sum(), Xb,\nretain_graph=True) so that the graph is preserved for the main backward, or\nalternatively detach and compute the gradient-norm in a no_grad context or\nrecompute features to avoid reusing the same graph.", "The DVN input features in the meta\u2010update loop are built with only two values\n(loss and entropy), but the DVN network\u2019s first Linear layer expects three\nfeatures. This leads to a shape mismatch (20\u00d72 vs. 3\u00d732). To fix, include the\nrepresentation\u2010norm feature when constructing feats_list (e.g., compute rep_norm\nfor each xi and append [li, ent_i, rep_norm] to feats_list) or alternatively\nchange the DVN\u2019s input dimension to 2.", "The training script executed successfully on GPU and completed all epochs for\nthree HuggingFace datasets (ag_news, yelp_polarity, dbpedia_14) without any\nruntime errors. The main MLP classifier steadily improved (validation loss\ndecreased from 1.36\u21921.25 on ag_news, 0.67\u21920.54 on yelp, and 2.60\u21922.39 on\ndbpedia). The Meta-Learned Data Valuation Network (DVN) showed moderate positive\nSpearman correlations on ag_news (0.63\u21920.58\u21920.01) and yelp (0.22\u21920.66) but\nconsistently negative correlations on the more complex 14-class dbpedia task\n(down to \u20130.64). Adaptive meta-update frequency behaved as designed. To address\nthe poor DVN performance on dbpedia, consider enriching the per-sample feature\nset (e.g., add gradient norms or representation embeddings), increasing K_meta\nor DVN capacity, and tuning hyperparameters (meta_steps, learning rates, softmax\ntemperature) to stabilize the contribution predictions.", "The script ran to completion without runtime errors, but the Meta\u2010Learned Data\nValuation Network (DVN) failed to learn meaningful sample contributions: the\nSpearman correlation with true contributions remained near zero or negative\nacross all datasets. Likely causes include using only 10 meta\u2010samples (K_meta)\nand just 5 gradient steps per meta update, insufficient for the DVN to\ngeneralize; missing gradient\u2010norm or richer embedding\u2010diversity features (the\nDVN input only included loss and entropy measures); and the dynamic N_meta\nadjustment prematurely reducing meta\u2010update frequency when early correlation is\nlow. To fix this, increase K_meta (number of meta\u2010samples) and number of DVN\nupdate steps, incorporate gradient\u2010norm and richer representation features into\nthe DVN input, use a larger held\u2010out set for ground\u2010truth contribution\nestimates, and revise the N_meta scheduling logic to ensure consistent\nmeta\u2010training early on.", "The code crashes when computing torch.autograd.grad on xi because\nxi.requires_grad is False by default. Before computing gradients w.r.t. xi in\nthe meta-update loop, you must call xi.requires_grad_(True). This will allow\nautograd to track xi and avoid the \u2018does not require grad\u2019 runtime error.", "", "The script times out during the bookcorpus run due to the extremely slow ground-\ntruth contribution measurement loop. For each of ~20 sampled training examples\nit deep-copies the full GPT-2 model and re-evaluates the entire validation set\ntwice (before and after a gradient step), resulting in enormous computation\noverhead. This stalls the training and triggers a TimeoutError. A fix is to\nreduce or vectorize the contribution computations\u2014for example, sample far fewer\nexamples or validation batches, use a much smaller hold-out set, or approximate\ninfluence scores without deep-copying and full-validation loops.", "The script executed to completion for all three datasets (ag_news, dbpedia_14,\nyelp_polarity) without any runtime errors. Training and meta-learning loops ran\nas expected, Spearman correlations were computed and used to adapt the meta_freq\nhyperparameter, and validation losses steadily decreased over the three epochs.\nThe required three HF datasets were properly loaded and processed. No execution\nfailures or critical bugs were detected.", "The experiment crashed with a RuntimeError when calling .view() on a non-\ncontiguous tensor while computing held-out losses in the DVN meta-update (at\nrunfile.py line 142). PyTorch requires .view() only on contiguous tensors, so\nthe fix is to either call .contiguous() before .view() or use .reshape() instead\n(e.g., replace .view(-1, vocab_size) with .reshape(-1, vocab_size)).\nAdditionally, you can suppress the harmless \u2018generation flags not valid\u2019\nwarnings by removing the unsupported output_hidden_states flag from any\ngenerate() calls.", "The script timed out (KeyboardInterrupt/TimeoutError) while processing and\ntraining on the bookcorpus dataset. Loading 10% of BookCorpus still yields\nmillions of examples, and running 10 epochs of GPT2-medium with heavy\nmapping/training loops exceeded the one-hour limit. To fix this, restrict the\nBookCorpus subset to a much smaller fraction (e.g. train[:1%] or a fixed cap of\nN examples), reduce the number of epochs for large datasets, or leverage\nstreaming/caching to speed up dataset tokenization and training steps.", "", "", "", ""], "exc_type": [null, "ImportError", "TypeError", null, "RuntimeError", "RuntimeError", null, null, "RuntimeError", null, "TimeoutError", null, "RuntimeError", "TimeoutError", null, null, null, null], "exc_info": [null, {"args": ["cannot import name 'AdamW' from 'transformers' (/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/__init__.py)"], "name": "transformers", "msg": "cannot import name 'AdamW' from 'transformers' (/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/__init__.py)"}, {"args": ["len() of a 0-d tensor"]}, null, {"args": ["Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."]}, {"args": ["mat1 and mat2 shapes cannot be multiplied (20x2 and 3x32)"]}, null, null, {"args": ["One of the differentiated Tensors does not require grad"]}, null, {"args": []}, null, {"args": ["view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."]}, {"args": []}, null, null, null, null], "exc_stack": [null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 9, "<module>", "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 112, "<module>", "sample = train_dataset[idx]"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/arrow_dataset.py", 2777, "__getitem__", "return self._getitem(key)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/arrow_dataset.py", 2761, "_getitem", "pa_subtable = query_table(self._data, key, indices=self._indices)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/formatting/formatting.py", 607, "query_table", "_check_valid_index_key(key, size)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/formatting/formatting.py", 556, "_check_valid_index_key", "if len(key) > 0:"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/_tensor.py", 1087, "__len__", "raise TypeError(\"len() of a 0-d tensor\")"]], null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 165, "<module>", "loss.backward()"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/_tensor.py", 581, "backward", "torch.autograd.backward("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/autograd/__init__.py", 347, "backward", "_engine_run_backward("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/autograd/graph.py", 825, "_engine_run_backward", "return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 131, "<module>", "p = dvn_model(feats_meta)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["runfile.py", 62, "forward", "return self.net(x)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/container.py", 250, "forward", "input = module(input)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/linear.py", 125, "forward", "return F.linear(input, self.weight, self.bias)"]], null, null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 112, "<module>", "torch.autograd.grad(crit_main(main_model(xi), yi).sum(), xi)[0]"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/autograd/__init__.py", 496, "grad", "result = _engine_run_backward("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/autograd/graph.py", 825, "_engine_run_backward", "return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass"]], null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 114, "<module>", "out = model("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py", 1210, "forward", "transformer_outputs = self.transformer("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py", 939, "forward", "outputs = block("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/utils/deprecation.py", 172, "wrapped_func", "return func(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py", 439, "forward", "feed_forward_hidden_states = self.mlp(hidden_states)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py", 364, "forward", "hidden_states = self.c_fc(hidden_states)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/pytorch_utils.py", 117, "forward", "size_out = x.size()[:-1] + (self.nf,)"]], null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 142, "<module>", "o0.logits[..., :-1, :].view(-1, vocab_size),"]], [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 129, "<module>", "out = model("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py", 1210, "forward", "transformer_outputs = self.transformer("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py", 939, "forward", "outputs = block("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/utils/deprecation.py", 172, "wrapped_func", "return func(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py", 403, "forward", "attn_output, self_attn_weights = self.attn("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/utils/deprecation.py", 172, "wrapped_func", "return func(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py", 348, "forward", "attn_output = self.c_proj(attn_output)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1736, "_wrapped_call_impl", "return self._call_impl(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/torch/nn/modules/module.py", 1747, "_call_impl", "return forward_call(*args, **kwargs)"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/transformers/pytorch_utils.py", 119, "forward", "x = x.view(size_out)"]], null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Final training loss on the synthetic dataset", "data": [{"dataset_name": "Synthetic Dataset", "final_value": 0.0113, "best_value": 0.0113}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Final validation loss on the synthetic dataset", "data": [{"dataset_name": "Synthetic Dataset", "final_value": 0.01, "best_value": 0.01}]}, {"metric_name": "Spearman correlation", "lower_is_better": false, "description": "Final Spearman correlation on the synthetic dataset", "data": [{"dataset_name": "Synthetic Dataset", "final_value": 0.3038, "best_value": 0.3038}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "ag_news", "final_value": 1.2654, "best_value": 1.2654}, {"dataset_name": "yelp", "final_value": 0.5525, "best_value": 0.5525}, {"dataset_name": "dbpedia", "final_value": 2.4078, "best_value": 2.4078}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "ag_news", "final_value": 0.58, "best_value": 0.58}, {"dataset_name": "yelp", "final_value": 0.815, "best_value": 0.815}, {"dataset_name": "dbpedia", "final_value": 0.725, "best_value": 0.725}]}, {"metric_name": "spearman correlation", "lower_is_better": false, "description": "Spearman correlation on the validation set", "data": [{"dataset_name": "ag_news", "final_value": 0.0752, "best_value": 0.0752}, {"dataset_name": "yelp", "final_value": 0.2602, "best_value": 0.2602}, {"dataset_name": "dbpedia", "final_value": 0.0932, "best_value": 0.0932}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset", "data": [{"dataset_name": "ag_news", "final_value": 0.797, "best_value": 0.797}, {"dataset_name": "yelp", "final_value": 0.863, "best_value": 0.863}, {"dataset_name": "dbpedia", "final_value": 0.712, "best_value": 0.712}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset", "data": [{"dataset_name": "ag_news", "final_value": 0.68, "best_value": 0.68}, {"dataset_name": "yelp", "final_value": 0.84, "best_value": 0.84}, {"dataset_name": "dbpedia", "final_value": 0.575, "best_value": 0.575}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "ag_news", "final_value": 1.2041, "best_value": 1.2041}, {"dataset_name": "yelp", "final_value": 0.5234, "best_value": 0.5234}, {"dataset_name": "dbpedia", "final_value": 2.3043, "best_value": 2.3043}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "ag_news", "final_value": 1.247, "best_value": 1.247}, {"dataset_name": "yelp", "final_value": 0.5444, "best_value": 0.5444}, {"dataset_name": "dbpedia", "final_value": 2.3937, "best_value": 2.3937}]}]}, {"metric_names": [{"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "ag_news", "final_value": 1.0158, "best_value": 1.0158}, {"dataset_name": "yelp", "final_value": 0.4314, "best_value": 0.4314}, {"dataset_name": "dbpedia", "final_value": 1.9922, "best_value": 1.9922}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "ag_news", "final_value": 0.725, "best_value": 0.725}, {"dataset_name": "yelp", "final_value": 0.85, "best_value": 0.85}, {"dataset_name": "dbpedia", "final_value": 0.63, "best_value": 0.63}]}, {"metric_name": "spearman correlation", "lower_is_better": false, "description": "Spearman correlation", "data": [{"dataset_name": "ag_news", "final_value": 0.0818, "best_value": 0.0818}, {"dataset_name": "yelp", "final_value": -0.1667, "best_value": -0.1667}, {"dataset_name": "dbpedia", "final_value": -0.397, "best_value": -0.397}]}, {"metric_name": "fairness disparity", "lower_is_better": true, "description": "Fairness disparity", "data": [{"dataset_name": "yelp", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "ag_news", "final_value": 1.2479195594787598, "best_value": 1.2479195594787598}, {"dataset_name": "yelp", "final_value": 0.5452219247817993, "best_value": 0.5452219247817993}, {"dataset_name": "dbpedia", "final_value": 2.405613899230957, "best_value": 2.405613899230957}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "ag_news", "final_value": 0.699999988079071, "best_value": 0.699999988079071}, {"dataset_name": "yelp", "final_value": 0.8349999785423279, "best_value": 0.8349999785423279}, {"dataset_name": "dbpedia", "final_value": 0.625, "best_value": 0.625}]}, {"metric_name": "Spearman correlation", "lower_is_better": false, "description": "Spearman correlation on validation set", "data": [{"dataset_name": "ag_news", "final_value": 0.4300751879699248, "best_value": 0.4300751879699248}, {"dataset_name": "yelp", "final_value": 0.4285714285714286, "best_value": 0.4285714285714286}, {"dataset_name": "dbpedia", "final_value": 0.10375939849624059, "best_value": 0.10375939849624059}]}, {"metric_name": "N_meta", "lower_is_better": false, "description": "Number of meta features", "data": [{"dataset_name": "ag_news", "final_value": 20.0, "best_value": 20.0}, {"dataset_name": "yelp", "final_value": 8.0, "best_value": 8.0}, {"dataset_name": "dbpedia", "final_value": 20.0, "best_value": 20.0}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset", "data": [{"dataset_name": "ag_news", "final_value": 0.804, "best_value": 0.804}, {"dataset_name": "dbpedia", "final_value": 0.806, "best_value": 0.806}, {"dataset_name": "yelp", "final_value": 0.862, "best_value": 0.862}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset", "data": [{"dataset_name": "ag_news", "final_value": 0.69, "best_value": 0.69}, {"dataset_name": "dbpedia", "final_value": 0.71, "best_value": 0.71}, {"dataset_name": "yelp", "final_value": 0.835, "best_value": 0.835}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Cross-entropy loss on the training dataset", "data": [{"dataset_name": "ag_news", "final_value": 1.2036, "best_value": 1.2036}, {"dataset_name": "dbpedia", "final_value": 2.3126, "best_value": 2.3126}, {"dataset_name": "yelp", "final_value": 0.5138, "best_value": 0.5138}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Cross-entropy loss on the validation dataset", "data": [{"dataset_name": "ag_news", "final_value": 1.2461, "best_value": 1.2461}, {"dataset_name": "dbpedia", "final_value": 2.3858, "best_value": 2.3858}, {"dataset_name": "yelp", "final_value": 0.5335, "best_value": 0.5335}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "ag_news", "final_value": 1.253648042678833, "best_value": 1.253648042678833}, {"dataset_name": "yelp", "final_value": 0.5458672046661377, "best_value": 0.5458672046661377}, {"dataset_name": "dbpedia", "final_value": 2.3887181282043457, "best_value": 2.3887181282043457}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "ag_news", "final_value": 0.6349999904632568, "best_value": 0.6349999904632568}, {"dataset_name": "yelp", "final_value": 0.8399999737739563, "best_value": 0.8399999737739563}, {"dataset_name": "dbpedia", "final_value": 0.6150000095367432, "best_value": 0.6150000095367432}]}, {"metric_name": "Spearman correlation", "lower_is_better": false, "description": "Spearman rank correlation coefficient", "data": [{"dataset_name": "ag_news", "final_value": -0.003007518796992481, "best_value": -0.003007518796992481}, {"dataset_name": "yelp", "final_value": -0.14436090225563908, "best_value": -0.14436090225563908}, {"dataset_name": "dbpedia", "final_value": 0.21954887218045113, "best_value": 0.21954887218045113}]}]}, {"metric_names": [{"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "ag_news", "final_value": 1.2523906230926514, "best_value": 1.2523906230926514}, {"dataset_name": "yelp", "final_value": 0.5448862314224243, "best_value": 0.5448862314224243}, {"dataset_name": "dbpedia", "final_value": 2.406888008117676, "best_value": 2.406888008117676}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "ag_news", "final_value": 0.6649999618530273, "best_value": 0.6649999618530273}, {"dataset_name": "yelp", "final_value": 0.824999988079071, "best_value": 0.824999988079071}, {"dataset_name": "dbpedia", "final_value": 0.625, "best_value": 0.625}]}, {"metric_name": "Spearman correlation", "lower_is_better": false, "description": "Spearman correlation", "data": [{"dataset_name": "ag_news", "final_value": -0.12932330827067667, "best_value": -0.12932330827067667}, {"dataset_name": "yelp", "final_value": 0.8165413533834586, "best_value": 0.8165413533834586}, {"dataset_name": "dbpedia", "final_value": 0.19849624060150375, "best_value": 0.19849624060150375}]}, {"metric_name": "N_meta", "lower_is_better": false, "description": "Number of meta examples", "data": [{"dataset_name": "ag_news", "final_value": 40.0, "best_value": 40.0}, {"dataset_name": "yelp", "final_value": 20.0, "best_value": 20.0}, {"dataset_name": "dbpedia", "final_value": 20.0, "best_value": 20.0}]}]}, {"metric_names": [{"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss on the dataset", "data": [{"dataset_name": "ag_news", "final_value": 1.2433955669403076, "best_value": 1.2433955669403076}, {"dataset_name": "yelp", "final_value": 0.5353274941444397, "best_value": 0.5353274941444397}, {"dataset_name": "dbpedia", "final_value": 2.3887720108032227, "best_value": 2.3887720108032227}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy on the dataset", "data": [{"dataset_name": "ag_news", "final_value": 0.6800000071525574, "best_value": 0.6800000071525574}, {"dataset_name": "yelp", "final_value": 0.8349999785423279, "best_value": 0.8349999785423279}, {"dataset_name": "dbpedia", "final_value": 0.6200000047683716, "best_value": 0.6200000047683716}]}, {"metric_name": "Spearman correlation", "lower_is_better": false, "description": "Spearman correlation coefficient on the dataset", "data": [{"dataset_name": "ag_news", "final_value": 0.5473684210526315, "best_value": 0.5473684210526315}, {"dataset_name": "yelp", "final_value": 0.6481203007518795, "best_value": 0.6481203007518795}, {"dataset_name": "dbpedia", "final_value": 0.2300751879699248, "best_value": 0.2300751879699248}]}, {"metric_name": "N_meta", "lower_is_better": false, "description": "Number of meta examples used in training", "data": [{"dataset_name": "ag_news", "final_value": 20.0, "best_value": 20.0}, {"dataset_name": "yelp", "final_value": 16.0, "best_value": 16.0}, {"dataset_name": "dbpedia", "final_value": 2.0, "best_value": 2.0}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_57ddd97f0ac5481ab77c3607a38a5106_proc_234628/synthetic_spearman_corr.png", "../../logs/0-run/experiment_results/experiment_57ddd97f0ac5481ab77c3607a38a5106_proc_234628/synthetic_loss_curves.png"], [], [], ["../../logs/0-run/experiment_results/experiment_ae9043840b324ee199f8e8bac2fcf34e_proc_247310/spearman_correlation_comparison.png", "../../logs/0-run/experiment_results/experiment_ae9043840b324ee199f8e8bac2fcf34e_proc_247310/validation_loss_comparison.png", "../../logs/0-run/experiment_results/experiment_ae9043840b324ee199f8e8bac2fcf34e_proc_247310/final_val_accuracy_per_dataset.png", "../../logs/0-run/experiment_results/experiment_ae9043840b324ee199f8e8bac2fcf34e_proc_247310/val_loss_vs_spearman_corr.png", "../../logs/0-run/experiment_results/experiment_ae9043840b324ee199f8e8bac2fcf34e_proc_247310/validation_accuracy_comparison.png"], [], [], ["../../logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/ag_news_spearman_corr.png", "../../logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/dbpedia_loss_curves.png", "../../logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/yelp_pred_vs_true.png", "../../logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/dbpedia_spearman_corr.png", "../../logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/yelp_spearman_corr.png", "../../logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/ag_news_pred_vs_true.png", "../../logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/dbpedia_pred_vs_true.png", "../../logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/ag_news_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/yelp_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/comparison_val_accuracy.png", "../../logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/ag_news_loss_curves.png", "../../logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/yelp_loss_curves.png", "../../logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/dbpedia_accuracy_curves.png"], [], [], ["../../logs/0-run/experiment_results/experiment_00e6b1fa6e634ab2a71a23d999f23588_proc_247309/spearman_correlation_comparison.png", "../../logs/0-run/experiment_results/experiment_00e6b1fa6e634ab2a71a23d999f23588_proc_247309/validation_loss_comparison.png", "../../logs/0-run/experiment_results/experiment_00e6b1fa6e634ab2a71a23d999f23588_proc_247309/validation_accuracy_comparison.png", "../../logs/0-run/experiment_results/experiment_00e6b1fa6e634ab2a71a23d999f23588_proc_247309/n_meta_history_comparison.png"], [], ["../../logs/0-run/experiment_results/experiment_6c1b96f8ff5144f387a228ebcd65727b_proc_247308/all_datasets_loss_curves.png", "../../logs/0-run/experiment_results/experiment_6c1b96f8ff5144f387a228ebcd65727b_proc_247308/final_accuracies_comparison.png", "../../logs/0-run/experiment_results/experiment_6c1b96f8ff5144f387a228ebcd65727b_proc_247308/all_datasets_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_6c1b96f8ff5144f387a228ebcd65727b_proc_247308/all_datasets_spearman_corr.png"], [], [], ["../../logs/0-run/experiment_results/experiment_4a73eaea65a04a7095f224a36e30f169_proc_247309/spearman_correlation_comparison.png", "../../logs/0-run/experiment_results/experiment_4a73eaea65a04a7095f224a36e30f169_proc_247309/validation_loss_comparison.png", "../../logs/0-run/experiment_results/experiment_4a73eaea65a04a7095f224a36e30f169_proc_247309/validation_accuracy_comparison.png", "../../logs/0-run/experiment_results/experiment_4a73eaea65a04a7095f224a36e30f169_proc_247309/n_meta_history_comparison.png"], ["../../logs/0-run/experiment_results/experiment_81a8a3a45d6b49a3ac2d29fa4e5e3199_proc_247308/spearman_correlation_comparison.png", "../../logs/0-run/experiment_results/experiment_81a8a3a45d6b49a3ac2d29fa4e5e3199_proc_247308/validation_loss_comparison.png", "../../logs/0-run/experiment_results/experiment_81a8a3a45d6b49a3ac2d29fa4e5e3199_proc_247308/validation_accuracy_comparison.png", "../../logs/0-run/experiment_results/experiment_81a8a3a45d6b49a3ac2d29fa4e5e3199_proc_247308/n_meta_history_comparison.png"], ["../../logs/0-run/experiment_results/experiment_e759358712174d5e983304d5fc8a3b03_proc_247310/spearman_correlation_comparison.png", "../../logs/0-run/experiment_results/experiment_e759358712174d5e983304d5fc8a3b03_proc_247310/validation_loss_comparison.png", "../../logs/0-run/experiment_results/experiment_e759358712174d5e983304d5fc8a3b03_proc_247310/validation_accuracy_comparison.png", "../../logs/0-run/experiment_results/experiment_e759358712174d5e983304d5fc8a3b03_proc_247310/n_meta_history_comparison.png"], ["../../logs/0-run/experiment_results/seed_aggregation_e8d2aebcdb754533b6422e37b8a78818/val_acc_mean_sem.png", "../../logs/0-run/experiment_results/seed_aggregation_e8d2aebcdb754533b6422e37b8a78818/N_meta_history_mean_sem.png", "../../logs/0-run/experiment_results/seed_aggregation_e8d2aebcdb754533b6422e37b8a78818/train_loss_mean_sem.png", "../../logs/0-run/experiment_results/seed_aggregation_e8d2aebcdb754533b6422e37b8a78818/corrs_mean_sem.png", "../../logs/0-run/experiment_results/seed_aggregation_e8d2aebcdb754533b6422e37b8a78818/train_acc_mean_sem.png", "../../logs/0-run/experiment_results/seed_aggregation_e8d2aebcdb754533b6422e37b8a78818/val_loss_mean_sem.png"]], "plot_paths": [["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_57ddd97f0ac5481ab77c3607a38a5106_proc_234628/synthetic_spearman_corr.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_57ddd97f0ac5481ab77c3607a38a5106_proc_234628/synthetic_loss_curves.png"], [], [], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_ae9043840b324ee199f8e8bac2fcf34e_proc_247310/spearman_correlation_comparison.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_ae9043840b324ee199f8e8bac2fcf34e_proc_247310/validation_loss_comparison.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_ae9043840b324ee199f8e8bac2fcf34e_proc_247310/final_val_accuracy_per_dataset.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_ae9043840b324ee199f8e8bac2fcf34e_proc_247310/val_loss_vs_spearman_corr.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_ae9043840b324ee199f8e8bac2fcf34e_proc_247310/validation_accuracy_comparison.png"], [], [], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/ag_news_spearman_corr.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/dbpedia_loss_curves.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/yelp_pred_vs_true.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/dbpedia_spearman_corr.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/yelp_spearman_corr.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/ag_news_pred_vs_true.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/dbpedia_pred_vs_true.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/ag_news_accuracy_curves.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/yelp_accuracy_curves.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/comparison_val_accuracy.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/ag_news_loss_curves.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/yelp_loss_curves.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/dbpedia_accuracy_curves.png"], [], [], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_00e6b1fa6e634ab2a71a23d999f23588_proc_247309/spearman_correlation_comparison.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_00e6b1fa6e634ab2a71a23d999f23588_proc_247309/validation_loss_comparison.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_00e6b1fa6e634ab2a71a23d999f23588_proc_247309/validation_accuracy_comparison.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_00e6b1fa6e634ab2a71a23d999f23588_proc_247309/n_meta_history_comparison.png"], [], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_6c1b96f8ff5144f387a228ebcd65727b_proc_247308/all_datasets_loss_curves.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_6c1b96f8ff5144f387a228ebcd65727b_proc_247308/final_accuracies_comparison.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_6c1b96f8ff5144f387a228ebcd65727b_proc_247308/all_datasets_accuracy_curves.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_6c1b96f8ff5144f387a228ebcd65727b_proc_247308/all_datasets_spearman_corr.png"], [], [], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_4a73eaea65a04a7095f224a36e30f169_proc_247309/spearman_correlation_comparison.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_4a73eaea65a04a7095f224a36e30f169_proc_247309/validation_loss_comparison.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_4a73eaea65a04a7095f224a36e30f169_proc_247309/validation_accuracy_comparison.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_4a73eaea65a04a7095f224a36e30f169_proc_247309/n_meta_history_comparison.png"], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_81a8a3a45d6b49a3ac2d29fa4e5e3199_proc_247308/spearman_correlation_comparison.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_81a8a3a45d6b49a3ac2d29fa4e5e3199_proc_247308/validation_loss_comparison.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_81a8a3a45d6b49a3ac2d29fa4e5e3199_proc_247308/validation_accuracy_comparison.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_81a8a3a45d6b49a3ac2d29fa4e5e3199_proc_247308/n_meta_history_comparison.png"], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e759358712174d5e983304d5fc8a3b03_proc_247310/spearman_correlation_comparison.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e759358712174d5e983304d5fc8a3b03_proc_247310/validation_loss_comparison.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e759358712174d5e983304d5fc8a3b03_proc_247310/validation_accuracy_comparison.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e759358712174d5e983304d5fc8a3b03_proc_247310/n_meta_history_comparison.png"], ["experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/seed_aggregation_e8d2aebcdb754533b6422e37b8a78818/val_acc_mean_sem.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/seed_aggregation_e8d2aebcdb754533b6422e37b8a78818/N_meta_history_mean_sem.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/seed_aggregation_e8d2aebcdb754533b6422e37b8a78818/train_loss_mean_sem.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/seed_aggregation_e8d2aebcdb754533b6422e37b8a78818/corrs_mean_sem.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/seed_aggregation_e8d2aebcdb754533b6422e37b8a78818/train_acc_mean_sem.png", "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/seed_aggregation_e8d2aebcdb754533b6422e37b8a78818/val_loss_mean_sem.png"]], "plot_analyses": [[{"analysis": "", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_57ddd97f0ac5481ab77c3607a38a5106_proc_234628/synthetic_spearman_corr.png"}], [], [], [{"analysis": "Iteration 1: Yelp shows a modest positive Spearman correlation (~+0.26), indicating the DVN\u2019s initial predictions align somewhat with true contributions on that dataset. AG News and DBpedia start with negative correlations (\u2248\u20130.36 and \u20130.12), suggesting early DVN predictions may be misleading for those corpora.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_ae9043840b324ee199f8e8bac2fcf34e_proc_247310/spearman_correlation_comparison.png"}, {"analysis": "Iteration 2\u20133: DBpedia correlation climbs to \u2248+0.09 by iteration 2 and \u2248\u20130.11 at iteration 3, while AG News moves toward zero (+0.07 at iter 3). Yelp dips at iter 2 (\u2248+0.03) but rebounds strongly to \u2248+0.23 at iter 3. This indicates fluctuating DVN alignment across updates, with Yelp recovering fastest.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_ae9043840b324ee199f8e8bac2fcf34e_proc_247310/validation_loss_comparison.png"}, {"analysis": "Iteration 4\u20136: Yelp plummets to \u20130.50 at iter 4 then recovers modestly to \u20130.32 at iter 5 and \u20130.41 at iter 6, showing instability. AG News remains negative throughout (bottoming at \u20130.45), while DBpedia improves steadily to +0.05, dips to \u20130.32, then peaks at +0.09 by iter 6. DBpedia appears most stable long term, while AG News and Yelp exhibit oscillations.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_ae9043840b324ee199f8e8bac2fcf34e_proc_247310/final_val_accuracy_per_dataset.png"}], [], [], [{"analysis": "Validation accuracy per epoch across AG News, Yelp, and DBpedia shows that Yelp converges fastest and achieves the highest performance (\u22480.67 \u2192 0.83 \u2192 0.84), AG News improves moderately (\u22480.64 \u2192 0.69 \u2192 0.68) with a slight dip at epoch 3, and DBpedia starts very low but makes the steepest relative gains (\u22480.19 \u2192 0.42 \u2192 0.57). This suggests that the DVN\u2010driven sampler accelerates learning most effectively on sentiment data (Yelp), yields steady improvement on topic classification (AG News), but struggles initially on the more diverse DBpedia corpus, despite ultimately making solid progress by epoch 3.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_68dad85675ec499386cc4c514b9b7ff0_proc_247309/ag_news_spearman_corr.png"}], [], [], [{"analysis": "Spearman correlation between DVN predictions and true contributions shows an initially high variance across meta-update steps, especially for ag_news and yelp. For ag_news, early steps are negative but correlation climbs steadily after step 4 and peaks above 0.4 by step 6. Yelp follows a U-shaped trajectory with negative dips at steps 2\u20133 before recovering from step 4 onward, ultimately stabilizing around 0.3\u20130.4. Dbpedia exhibits a brief spike above 0.5 at step 3 but with only four updates its correlation then drops, suggesting an underexplored trend due to fewer meta-updates.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_00e6b1fa6e634ab2a71a23d999f23588_proc_247309/spearman_correlation_comparison.png"}, {"analysis": "Validation loss consistently decreases across epochs for all three datasets. Yelp exhibits the fastest drop (0.67 \u2192 0.54), indicating the easiest learning curve under the current DVN sampler. Ag_news shows a modest decline (1.36 \u2192 1.25), while dbpedia remains the most challenging with loss easing from 2.62 to 2.40. This confirms that adaptive sampling does not hinder convergence against uniform baselines.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_00e6b1fa6e634ab2a71a23d999f23588_proc_247309/validation_loss_comparison.png"}, {"analysis": "Validation accuracy improves across epochs, mirroring the loss curves. Yelp achieves the highest accuracy, climbing from 0.65 to 0.83. Ag_news jumps from 0.54 to 0.70 by epoch 2 then plateaus, hinting at saturation under current capacity. Dbpedia accuracy leaps from 0.33 to 0.63 by epoch 2 before a slight dip, reflecting domain complexity and potential overfitting or high-variance sampling at later stages.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_00e6b1fa6e634ab2a71a23d999f23588_proc_247309/validation_accuracy_comparison.png"}, {"analysis": "The adaptive meta-batch size N_meta varies markedly per dataset, reflecting DVN\u2019s uncertainty: ag_news toggles between 5 and 20, suggesting reactive adjustments to maintain stable meta-gradient estimates. Yelp\u2019s N_meta oscillates between 2 and 10 in a damped pattern, aligning with its smoother correlation recovery and fast learning. Dbpedia spikes to N_meta=40 at step 3\u2014coinciding with its correlation surge\u2014then halves, indicating the DVN increases sample aggregation when signals are noisy.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_00e6b1fa6e634ab2a71a23d999f23588_proc_247309/n_meta_history_comparison.png"}], [], [{"analysis": "Training and validation losses consistently decrease across epochs for all three datasets, indicating the model is effectively learning from each corpus. ag_news shows a moderate downward trend, with validation loss remaining close to training loss (from ~1.35 to ~1.25), suggesting good generalization without severe overfitting. dbpedia starts at a higher loss (~2.6) and also steadily decreases (~2.4 at epoch 3), but the gap between training and validation curves is slightly wider, hinting at a bit more noise or complexity in the dbpedia data. yelp exhibits the lowest losses from the outset (~0.65 to ~0.50), reflecting that sentiment classification on yelp reviews may be an easier task or that the model architecture is particularly well-suited to this data.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_6c1b96f8ff5144f387a228ebcd65727b_proc_247308/all_datasets_loss_curves.png"}, {"analysis": "Train and validation accuracies at the final epoch reveal that yelp yields the highest performance (train ~0.87, val ~0.84), with a small generalization gap (~0.03). dbpedia achieves moderate accuracy (train ~0.81, val ~0.71), indicating some difficulty in capturing fine-grained entity classification from Wikipedia abstracts. ag_news lands in between (train ~0.78, val ~0.69), with a similar gap (~0.09) as dbpedia. The relative ordering (yelp > dbpedia > ag_news) reflects both inherent task difficulty and dataset-specific model fit.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_6c1b96f8ff5144f387a228ebcd65727b_proc_247308/final_accuracies_comparison.png"}, {"analysis": "Across epochs, yelp starts with high accuracy (~0.85 train, ~0.80 val) and improves only marginally, indicating rapid early convergence and saturation by epoch 3 (~0.86/0.83). ag_news shows steady improvement from ~0.74/0.60 to ~0.80/0.68, but the validation curve lags training by ~0.12\u20130.13, suggesting potential underfitting or the need for stronger regularization. dbpedia exhibits the most dramatic gain: initial accuracy is low (~0.37 train, ~0.32 val) but jumps to ~0.73/0.65 by epoch 2 and ~0.81/0.70 by epoch 3, demonstrating that the model quickly learns key patterns after a slow start.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_6c1b96f8ff5144f387a228ebcd65727b_proc_247308/all_datasets_accuracy_curves.png"}, {"analysis": "Spearman correlations for DVN predictions versus ground-truth contributions vary significantly across datasets and evaluation steps. For ag_news, high initial correlation (~0.72) drops precipitously to ~0.20 by the second evaluation and plateaus near ~0.18, implying the DVN\u2019s predictive signal weakened after early updates. dbpedia shows poor initial correlation (~\u20130.02) but improves steadily after additional meta-training: modest (~0.24/0.19) at steps 2/3 and strong (~0.52/0.53) by steps 4/5, indicating the DVN benefits from more frequent or richer contribution measurements on this dataset. yelp\u2019s correlations remain negative or near zero for most steps (from ~\u20130.45 up to ~0.06 and back down), suggesting the current feature set or update schedule fails to capture sample usefulness in sentiment data.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_6c1b96f8ff5144f387a228ebcd65727b_proc_247308/all_datasets_spearman_corr.png"}], [], [], [{"analysis": "Spearman correlation curves reveal that DVN\u2019s ability to rank high-impact samples varies substantially across datasets and meta-update steps. On dbpedia, correlations jump from around 0.10 to 0.35 by the second update and then stabilize in the 0.13\u20130.22 range before rising again, indicating a robust early signal that levels off as the DVN fine-tunes its predictions. Yelp begins with a healthy ~0.25 correlation for two updates but crashes to \u20130.40 at step 4, then partially recovers to about \u20130.15, suggesting that certain meta-updates or hyperparameter shifts disrupt its sample-value estimations. ag_news shows the greatest instability, plunging from \u20130.70 to \u20130.45, then recovering to near zero only by step 6. This pattern hints that the news dataset\u2019s higher topical diversity makes per-sample contribution harder to predict, and that timing and size of meta-updates critically affect learning quality. Adjusting the rhythm or size of ground-truth valuation phases may smooth these fluctuations, especially for datasets with wide semantic variance.", "valid_plots_received": true, "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_4a73eaea65a04a7095f224a36e30f169_proc_247309/spearman_correlation_comparison.png"}, {"analysis": "Validation losses decrease steadily across all three corpora, confirming that DVN-guided sampling supports consistent downstream learning. ag_news loss drops from 1.36 to 1.25 over three epochs, yelp from 0.67 to 0.53, and dbpedia from 2.60 to 2.38. The largest absolute gain occurs on dbpedia (\u0394\u22480.22), likely because its higher initial perplexity offers more room for rapid reduction. Yelp\u2019s swift decline also suggests that sentiment data benefits early from selecting high-impact examples. These monotonic improvements show that even when ranking quality temporarily suffers (as in step 4 of the correlation plot), the overall adaptive sampling scheme still yields net gains in model fit over modest training budgets.", "vlm_feedback_summary": "Validation accuracy curves mirror the loss trends, rising across epochs for every dataset. Yelp climbs from 69% to 84%, dbpedia from 43% to 62%, and ag_news from 41% to 63%, with the steepest accuracy slope on ag_news between epochs 2 and 3. This confirms that DVN-informed batches accelerate generalization across text genres, though starting points and absolute ceilings differ by dataset.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_4a73eaea65a04a7095f224a36e30f169_proc_247309/validation_loss_comparison.png"}], [{"analysis": "Spearman correlation vs. true contributions across six meta\u2010update steps for AG News, Yelp, and DBpedia reveals a strong dataset dependence in DVN ranking quality. Yelp shows a clear upward trend and culminates in very high correlation (~0.82) by step 6, indicating the DVN has effectively learned to predict which mini\u2010batches most benefit held\u2010out performance. AG News correlations remain slightly negative throughout (around \u20130.2 to \u20130.1), suggesting that for this task the DVN is struggling to distinguish high\u2010impact samples from low\u2010impact ones. DBpedia exhibits large swings (from \u20130.35 up to ~0.20), pointing to instability and insufficient meta\u2010training signal. Overall, the DVN sampler performs best on the Yelp polarity dataset, but fails to generalize its valuation to AG News and shows high variance on DBpedia.", "valid_plots_received": true, "vlm_feedback_summary": "DVN ranking works well for Yelp, poorly for AG News, unstable for DBpedia; dataset characteristics strongly influence meta\u2010learner performance.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_81a8a3a45d6b49a3ac2d29fa4e5e3199_proc_247308/spearman_correlation_comparison.png"}], [{"analysis": "Spearman correlation traces for ag_news, yelp, and dbpedia reveal distinct dynamics in DVN prediction quality across meta-update steps. dbpedia maintains a consistently positive correlation (\u223c0.1\u20130.3) with only occasional dips around steps 6, 16, and 22, indicating reliable value estimates. ag_news shows early instability (\u20130.25 down to \u20130.85) before a sharp rise to ~0.55 at step 7 and smaller oscillations thereafter, suggesting the DVN struggles initially on this domain but then adapts. yelp exhibits moderate fluctuations around zero, with a notable negative spike (~\u20130.55) at step 3 followed by a strong peak (~0.65) at step 17, indicating delayed alignment of DVN signals with true contributions. These patterns imply the need for dataset- specific warm-up strategies or adaptive learning rates for DVN updates.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e759358712174d5e983304d5fc8a3b03_proc_247310/spearman_correlation_comparison.png"}, {"analysis": "Validation loss declines steadily on all three datasets over three epochs. ag_news falls from 1.35 to 1.25 (\u22487% reduction), yelp from 0.68 to 0.53 (\u224822% reduction), and dbpedia from 2.60 to 2.38 (\u22488% reduction), with the most pronounced drop in yelp indicating its relative ease. The consistently downward trends confirm that adaptive sampling guided by DVN does not hinder convergence and may accelerate loss reduction on datasets with initially lower correlation stability.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e759358712174d5e983304d5fc8a3b03_proc_247310/validation_loss_comparison.png"}, {"analysis": "Accuracy gains mirror loss reductions: ag_news jumps from 24% to 58% in epoch 2 and to 68% by epoch 3, dbpedia climbs from 22% to 49% then 62%, while yelp moves from a strong baseline of 79% up to 84%. The sharper improvements for ag_news and dbpedia between epochs 1 and 2\u2014where DVN rapidly improves sampling\u2014highlight the utility of meta-learned sampling for underperforming domains. Saturation effects appear by epoch 3 on yelp, suggesting a ceiling for easy tasks.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e759358712174d5e983304d5fc8a3b03_proc_247310/validation_accuracy_comparison.png"}, {"analysis": "Adaptive meta-batch size (N_meta) histories reflect dataset-specific calibration. For ag_news, N_meta oscillates 5\u219210\u219220 before settling around 10\u201320, indicating increased meta-batch sampling to stabilize DVN when early correlations were poor. yelp\u2019s N_meta remains low (1\u20134) until correlation peaks late, then ramps to 8 and 16, implying on-demand scaling of ground-truth measurements. dbpedia\u2019s N_meta monotonically declines from 10 down to 1\u20132, matching its stable correlation and suggesting fewer meta-updates are needed. This adaptive scheme conserves compute for stable domains while allocating more resources where the DVN is less certain.", "plot_path": "experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e759358712174d5e983304d5fc8a3b03_proc_247310/n_meta_history_comparison.png"}], []], "vlm_feedback_summary": ["Correlations improve markedly with more meta\u2010training epochs but suffer\nintermittent declines; tuning update frequency and meta\u2010learning rate schedules\nshould stabilize DVN performance.", "[]", "[]", "DVN\u2019s ability to rank samples varies significantly by dataset and iteration.\nAlthough Yelp started strong, it suffers volatility mid-run, while DBpedia shows\ngradual improvement and AG News remains challenging. This suggests dataset-\nspecific tuning (e.g., update frequency, feature subsets) is needed. Future work\nshould analyze which per-sample features drive positive correlation and whether\nincreasing DVN update frequency stabilizes predictions, especially on volatile\ncorpora.", "[]", "[]", "User requires a straightforward analysis of experimental plots; focus on three\nHuggingFace datasets: AG News, DBpedia, Yelp.", "[]", "[]", "DVN learning curves across three HuggingFace datasets confirm that meta-updates\nimprove sample valuation over time, with stronger correlation and stable\nvalidation metrics. Adaptive N_meta helps control variance but may require\ntuning for domain complexity.", "[]", "Overall, the adaptive sampler shows promising dataset-dependent behavior.\nTraining dynamics and final metrics confirm that yelp is easiest for both\nuniform and DVN-driven sampling, whereas dbpedia requires more epochs but\nultimately benefits most from a calibrated DVN. ag_news sees diminishing DVN\nutility after initial steps, and yelp offers too noisy a signal under current\nfeatures. Future work should focus on improving DVN feature representations,\nadjusting update frequency, and exploring additional datasets (e.g., SST-2,\nIMDB, Amazon Reviews) to enhance generalization and fairness.", "[]", "[]", "Adaptive meta-batch sizing appears correlated with ranking quality dips and\nrecoveries. Where meta-batch size shrinks\u2014especially yelp at step 4 (N_meta=5)\nand ag_news/dbpedia at step 3 (N_meta=10)\u2014Spearman correlation plunges.\nConversely, expansions at later steps (e.g., N_meta = 40 at step 5 for dbpedia\nand ag_news) coincide with rebounds in correlation. This sensitivity suggests a\nfeedback-driven schedule for N_meta: when ranking stability drops below a\nthreshold, increase meta-batch size to gather more ground-truth signals; when\naccuracy is high, scale it back to save compute. Implementing an automated\ncontroller based on moving averages of Spearman correlation could optimize\nefficiency and consistency. Additionally, introducing a small curriculum on\ndataset complexity\u2014gradually ramping up N_meta for more heterogeneous\ncorpora\u2014may further stabilize DVN performance.", "[]", "The DVN exhibits dataset-specific learning curves: it quickly stabilizes on\ndbpedia with modest meta-batch requirements, whereas ag_news and yelp require\nlarger or dynamically adjusted meta-batches to overcome early prediction noise.\nValidation loss and accuracy consistently improve, confirming that meta-learned\nsampling accelerates convergence and boosts generalization. These insights\nsuggest refining DVN warm-up schedules, adaptive update frequencies, and\nfeature-set tailoring per domain to further enhance efficiency and fairness.", "[]"], "exec_time": [7.528475761413574, 2.5899221897125244, 36.03254055976868, 28.89803409576416, 11.169734477996826, 11.18373441696167, 30.467372179031372, 28.9271399974823, 10.71740436553955, 32.85571789741516, 3600.9484202861786, 28.86271357536316, 37.23044967651367, 3600.929924249649, 33.05171060562134, 30.94667410850525, 31.190378189086914, null], "exec_time_feedback": ["", "", "", "", "", "", "", "Implementation works but runs too quickly (0.51 minutes).We have up to 60\nminutes available for each experiment.Make sure to scale up the experiment by\nincreasing the number of epochs, using a larger model, or working with bigger\ndatasets.Given that the current execution time is {exec_time_minutes:.2f}\nminutes, think about how changing the number of epochs to run, or using a larger\nmodel, or working with bigger datasets to runwill affect the execution time, and\nmake sure to scale up the experiment accordingly.", "", "", "Implementation works but runs too quickly (0.55 minutes).We have up to 60\nminutes available for each experiment.Make sure to scale up the experiment by\nincreasing the number of epochs, using a larger model, or working with bigger\ndatasets.Given that the current execution time is {exec_time_minutes:.2f}\nminutes, think about how changing the number of epochs to run, or using a larger\nmodel, or working with bigger datasets to runwill affect the execution time, and\nmake sure to scale up the experiment accordingly.", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[synthetic]"], [], [], ["[\"DBpedia\"]"], [], [], ["[AG News", "DBpedia", "Yelp]"], [], [], ["['ag_news'", "'yelp']"], [], ["[\"dbpedia\"]"], [], [], ["['dbpedia']"], [], ["[ag_news", "yelp", "dbpedia]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    syn = experiment_data[\"hyperparam_tuning_type_1\"][\"synthetic\"]\n    param_values = syn[\"param_values\"]\n    loss_train = syn[\"losses\"][\"train\"]\n    loss_val = syn[\"losses\"][\"val\"]\n    corrs = syn[\"correlations\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot training/validation loss curves\ntry:\n    plt.figure()\n    for idx, p in enumerate(param_values):\n        epochs = np.arange(1, len(loss_train[idx]) + 1)\n        plt.plot(epochs, loss_train[idx], label=f\"{p} epochs train\")\n        plt.plot(epochs, loss_val[idx], linestyle=\"--\", label=f\"{p} epochs val\")\n    plt.suptitle(\"Synthetic Dataset Training/Validation Loss\")\n    plt.title(\"Solid: Training Loss, Dashed: Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# Plot Spearman correlation curves\ntry:\n    plt.figure()\n    for idx, p in enumerate(param_values):\n        epochs = np.arange(1, len(corrs[idx]) + 1)\n        plt.plot(epochs, corrs[idx], marker=\"o\", label=f\"{p} epochs\")\n    plt.suptitle(\"Synthetic Dataset Spearman Correlation\")\n    plt.title(\"Correlation of DVN Predictions vs True Contributions\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Spearman Corr\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_spearman_corr.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Spearman correlation plot: {e}\")\n    plt.close()\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# 1. Validation Loss Comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        loss = data.get(\"val_loss\", [])\n        epochs = np.arange(1, len(loss) + 1)\n        plt.plot(epochs, loss, marker=\"o\", label=name)\n    plt.suptitle(\"Validation Loss Comparison\")\n    plt.title(\"Loss vs Epoch for all datasets\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"validation_loss_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation loss comparison plot: {e}\")\n    plt.close()\n\n# 2. Validation Accuracy Comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        acc = data.get(\"val_acc\", [])\n        epochs = np.arange(1, len(acc) + 1)\n        plt.plot(epochs, acc, marker=\"s\", label=name)\n    plt.suptitle(\"Validation Accuracy Comparison\")\n    plt.title(\"Accuracy vs Epoch for all datasets\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"validation_accuracy_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation accuracy comparison plot: {e}\")\n    plt.close()\n\n# 3. Spearman Correlation Curves\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        corrs = data.get(\"corrs\", [])\n        iters = np.arange(1, len(corrs) + 1)\n        plt.plot(iters, corrs, marker=\"x\", label=name)\n    plt.suptitle(\"Spearman Correlation Comparison\")\n    plt.title(\"DVN Predictions vs True Contributions\")\n    plt.xlabel(\"Iteration\")\n    plt.ylabel(\"Spearman Correlation\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"spearman_correlation_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Spearman correlation comparison plot: {e}\")\n    plt.close()\n\n# 4. Scatter: Val Loss vs Spearman Corr\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        loss = data.get(\"val_loss\", [])\n        corrs = data.get(\"corrs\", [])\n        n = min(len(loss), len(corrs))\n        if n > 0:\n            plt.scatter(loss[:n], corrs[:n], label=name)\n    plt.suptitle(\"Validation Loss vs Spearman Correlation\")\n    plt.xlabel(\"Validation Loss\")\n    plt.ylabel(\"Spearman Correlation\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"val_loss_vs_spearman_corr.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating val-loss vs corr scatter plot: {e}\")\n    plt.close()\n\n# 5. Bar Chart: Final Validation Accuracy\ntry:\n    plt.figure()\n    names = list(experiment_data.keys())\n    final_acc = [experiment_data[n].get(\"val_acc\", [0])[-1] for n in names]\n    x = np.arange(len(names))\n    plt.bar(x, final_acc, tick_label=names)\n    plt.suptitle(\"Final Validation Accuracy per Dataset\")\n    plt.ylabel(\"Accuracy\")\n    plt.xticks(rotation=45)\n    plt.savefig(os.path.join(working_dir, \"final_val_accuracy_per_dataset.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final accuracy bar chart: {e}\")\n    plt.close()\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Print summary metrics\nfor name, data in experiment_data.items():\n    try:\n        tr_loss = data[\"losses\"][\"train\"][-1]\n        val_loss = data[\"losses\"][\"val\"][-1]\n        tr_acc = data[\"metrics\"][\"train\"][-1]\n        val_acc = data[\"metrics\"][\"val\"][-1]\n        print(\n            f\"{name}: Final Train Loss={tr_loss:.4f}, Val Loss={val_loss:.4f}, Train Acc={tr_acc:.4f}, Val Acc={val_acc:.4f}\"\n        )\n    except Exception:\n        print(f\"Skipping summary for {name} due to missing data\")\n\n# Plot per dataset\nfor name, data in experiment_data.items():\n    losses = data.get(\"losses\", {})\n    metrics = data.get(\"metrics\", {})\n    corrs = data.get(\"corrs\", [])\n    preds = data.get(\"predictions\", [])\n    tr_losses = losses.get(\"train\", [])\n    val_losses = losses.get(\"val\", [])\n    tr_accs = metrics.get(\"train\", [])\n    val_accs = metrics.get(\"val\", [])\n    # Loss curves\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(tr_losses) + 1)\n        plt.plot(epochs, tr_losses, label=\"Train Loss\")\n        plt.plot(epochs, val_losses, linestyle=\"--\", label=\"Val Loss\")\n        plt.suptitle(f\"Dataset: {name} Loss Curves\")\n        plt.title(\"Solid: Train Loss, Dashed: Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{name}_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating {name} loss curves: {e}\")\n        plt.close()\n    # Accuracy curves\n    try:\n        plt.figure()\n        epochs = np.arange(1, len(tr_accs) + 1)\n        plt.plot(epochs, tr_accs, label=\"Train Acc\")\n        plt.plot(epochs, val_accs, linestyle=\"--\", label=\"Val Acc\")\n        plt.suptitle(f\"Dataset: {name} Accuracy Curves\")\n        plt.title(\"Solid: Train Accuracy, Dashed: Validation Accuracy\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{name}_accuracy_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating {name} accuracy curves: {e}\")\n        plt.close()\n    # Spearman correlation\n    try:\n        plt.figure()\n        steps = np.arange(1, len(corrs) + 1)\n        plt.plot(steps, corrs, marker=\"o\")\n        plt.suptitle(f\"Dataset: {name} Spearman Correlation\")\n        plt.title(\"Correlation of DVN Predictions vs True Contributions\")\n        plt.xlabel(\"Meta Update Step\")\n        plt.ylabel(\"Spearman Corr\")\n        plt.savefig(os.path.join(working_dir, f\"{name}_spearman_corr.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating {name} spearman corr plot: {e}\")\n        plt.close()\n    # Scatter of predictions vs ground truth\n    try:\n        truths = np.array(data.get(\"ground_truth\", []))\n        predictions = np.array(preds)\n        if truths.size and predictions.size:\n            plt.figure()\n            plt.scatter(truths, predictions, alpha=0.6)\n            mn = min(truths.min(), predictions.min())\n            mx = max(truths.max(), predictions.max())\n            plt.plot([mn, mx], [mn, mx], color=\"red\", linestyle=\"--\")\n            plt.suptitle(f\"Dataset: {name} Predictions vs True Contributions\")\n            plt.title(\"DVN Predictions vs Ground Truth\")\n            plt.xlabel(\"True Contributions\")\n            plt.ylabel(\"DVN Predictions\")\n            plt.savefig(os.path.join(working_dir, f\"{name}_pred_vs_true.png\"))\n            plt.close()\n        else:\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating {name} pred vs true scatter: {e}\")\n        plt.close()\n\n# Comparison plot of validation accuracy\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        val_accs = data.get(\"metrics\", {}).get(\"val\", [])\n        if val_accs:\n            epochs = np.arange(1, len(val_accs) + 1)\n            plt.plot(epochs, val_accs, marker=\"o\", label=name)\n    plt.suptitle(\"Comparison of Validation Accuracy Across Datasets\")\n    plt.title(\"Validation Accuracy per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"comparison_val_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating comparison accuracy plot: {e}\")\n    plt.close()\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot validation loss comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        vals = data.get(\"val_loss\", [])\n        plt.plot(np.arange(1, len(vals) + 1), vals, marker=\"o\", label=name)\n    plt.suptitle(\"Validation Loss Across Datasets\")\n    plt.title(\"Validation Loss per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"validation_loss_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation loss plot: {e}\")\n    plt.close()\n\n# Plot validation accuracy comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        accs = data.get(\"val_acc\", [])\n        plt.plot(np.arange(1, len(accs) + 1), accs, marker=\"s\", label=name)\n    plt.suptitle(\"Validation Accuracy Across Datasets\")\n    plt.title(\"Validation Accuracy per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"validation_accuracy_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation accuracy plot: {e}\")\n    plt.close()\n\n# Plot Spearman correlation comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        corrs = data.get(\"corrs\", [])\n        plt.plot(np.arange(1, len(corrs) + 1), corrs, marker=\"^\", label=name)\n    plt.suptitle(\"Spearman Correlation of DVN Predictions\")\n    plt.title(\"Correlation vs. True Contributions\")\n    plt.xlabel(\"Meta\u2010update Step\")\n    plt.ylabel(\"Spearman Correlation\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"spearman_correlation_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Spearman correlation plot: {e}\")\n    plt.close()\n\n# Plot N_meta update history comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        nmeta = data.get(\"N_meta_history\", [])\n        plt.plot(np.arange(1, len(nmeta) + 1), nmeta, marker=\"d\", label=name)\n    plt.suptitle(\"Meta\u2010batch Size (N_meta) History\")\n    plt.title(\"Adaptive N_meta over Training\")\n    plt.xlabel(\"Meta\u2010update Step\")\n    plt.ylabel(\"N_meta Value\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"n_meta_history_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating N_meta history plot: {e}\")\n    plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Extract per-dataset metrics\ndatasets = list(experiment_data.keys())\nlosses_train = {d: experiment_data[d][\"losses\"][\"train\"] for d in datasets}\nlosses_val = {d: experiment_data[d][\"losses\"][\"val\"] for d in datasets}\nmetrics_train = {d: experiment_data[d][\"metrics\"][\"train\"] for d in datasets}\nmetrics_val = {d: experiment_data[d][\"metrics\"][\"val\"] for d in datasets}\ncorrs = {d: experiment_data[d][\"corrs\"] for d in datasets}\n\n# Print final accuracies\nfor d in datasets:\n    if metrics_train[d] and metrics_val[d]:\n        print(\n            f\"[{d}] Final Train Acc = {metrics_train[d][-1]:.4f}, Final Val Acc = {metrics_val[d][-1]:.4f}\"\n        )\n\n# Combined loss curves\ntry:\n    plt.figure()\n    for d in datasets:\n        epochs = np.arange(1, len(losses_train[d]) + 1)\n        plt.plot(epochs, losses_train[d], label=f\"{d} train\")\n        plt.plot(epochs, losses_val[d], linestyle=\"--\", label=f\"{d} val\")\n    plt.suptitle(\"Training/Validation Loss Curves Across Datasets\")\n    plt.title(\"Solid: Training, Dashed: Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"all_datasets_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# Combined accuracy curves\ntry:\n    plt.figure()\n    for d in datasets:\n        epochs = np.arange(1, len(metrics_train[d]) + 1)\n        plt.plot(epochs, metrics_train[d], label=f\"{d} train\")\n        plt.plot(epochs, metrics_val[d], linestyle=\"--\", label=f\"{d} val\")\n    plt.suptitle(\"Training/Validation Accuracy Across Datasets\")\n    plt.title(\"Solid: Training, Dashed: Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"all_datasets_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves: {e}\")\n    plt.close()\n\n# Combined Spearman correlation curves\ntry:\n    plt.figure()\n    for d in datasets:\n        steps = np.arange(1, len(corrs[d]) + 1)\n        plt.plot(steps, corrs[d], marker=\"o\", label=d)\n    plt.suptitle(\"DVN Spearman Correlations Across Datasets\")\n    plt.title(\"Correlation of Predicted vs True Contributions\")\n    plt.xlabel(\"Evaluation Step\")\n    plt.ylabel(\"Spearman Corr\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"all_datasets_spearman_corr.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Spearman correlation plot: {e}\")\n    plt.close()\n\n# Final accuracy comparison bar chart\ntry:\n    plt.figure()\n    x = np.arange(len(datasets))\n    final_train = [metrics_train[d][-1] for d in datasets]\n    final_val = [metrics_val[d][-1] for d in datasets]\n    width = 0.35\n    plt.bar(x - width / 2, final_train, width, label=\"Train\")\n    plt.bar(x + width / 2, final_val, width, label=\"Val\")\n    plt.xticks(x, datasets)\n    plt.suptitle(\"Final Train/Validation Accuracies by Dataset\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"final_accuracies_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final accuracy comparison: {e}\")\n    plt.close()\n", null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot validation loss comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        vals = data.get(\"val_loss\", [])\n        plt.plot(np.arange(1, len(vals) + 1), vals, marker=\"o\", label=name)\n    plt.suptitle(\"Validation Loss Across Datasets\")\n    plt.title(\"Validation Loss per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"validation_loss_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation loss plot: {e}\")\n    plt.close()\n\n# Plot validation accuracy comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        accs = data.get(\"val_acc\", [])\n        plt.plot(np.arange(1, len(accs) + 1), accs, marker=\"s\", label=name)\n    plt.suptitle(\"Validation Accuracy Across Datasets\")\n    plt.title(\"Validation Accuracy per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"validation_accuracy_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation accuracy plot: {e}\")\n    plt.close()\n\n# Plot Spearman correlation comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        corrs = data.get(\"corrs\", [])\n        plt.plot(np.arange(1, len(corrs) + 1), corrs, marker=\"^\", label=name)\n    plt.suptitle(\"Spearman Correlation of DVN Predictions\")\n    plt.title(\"Correlation vs. True Contributions\")\n    plt.xlabel(\"Meta\u2010update Step\")\n    plt.ylabel(\"Spearman Correlation\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"spearman_correlation_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Spearman correlation plot: {e}\")\n    plt.close()\n\n# Plot N_meta update history comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        nmeta = data.get(\"N_meta_history\", [])\n        plt.plot(np.arange(1, len(nmeta) + 1), nmeta, marker=\"d\", label=name)\n    plt.suptitle(\"Meta\u2010batch Size (N_meta) History\")\n    plt.title(\"Adaptive N_meta over Training\")\n    plt.xlabel(\"Meta\u2010update Step\")\n    plt.ylabel(\"N_meta Value\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"n_meta_history_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating N_meta history plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot validation loss comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        vals = data.get(\"val_loss\", [])\n        plt.plot(np.arange(1, len(vals) + 1), vals, marker=\"o\", label=name)\n    plt.suptitle(\"Validation Loss Across Datasets\")\n    plt.title(\"Validation Loss per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"validation_loss_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation loss plot: {e}\")\n    plt.close()\n\n# Plot validation accuracy comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        accs = data.get(\"val_acc\", [])\n        plt.plot(np.arange(1, len(accs) + 1), accs, marker=\"s\", label=name)\n    plt.suptitle(\"Validation Accuracy Across Datasets\")\n    plt.title(\"Validation Accuracy per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"validation_accuracy_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation accuracy plot: {e}\")\n    plt.close()\n\n# Plot Spearman correlation comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        corrs = data.get(\"corrs\", [])\n        plt.plot(np.arange(1, len(corrs) + 1), corrs, marker=\"^\", label=name)\n    plt.suptitle(\"Spearman Correlation of DVN Predictions\")\n    plt.title(\"Correlation vs. True Contributions\")\n    plt.xlabel(\"Meta\u2010update Step\")\n    plt.ylabel(\"Spearman Correlation\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"spearman_correlation_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Spearman correlation plot: {e}\")\n    plt.close()\n\n# Plot N_meta update history comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        nmeta = data.get(\"N_meta_history\", [])\n        plt.plot(np.arange(1, len(nmeta) + 1), nmeta, marker=\"d\", label=name)\n    plt.suptitle(\"Meta\u2010batch Size (N_meta) History\")\n    plt.title(\"Adaptive N_meta over Training\")\n    plt.xlabel(\"Meta\u2010update Step\")\n    plt.ylabel(\"N_meta Value\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"n_meta_history_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating N_meta history plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot validation loss comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        vals = data.get(\"val_loss\", [])\n        plt.plot(np.arange(1, len(vals) + 1), vals, marker=\"o\", label=name)\n    plt.suptitle(\"Validation Loss Across Datasets\")\n    plt.title(\"Validation Loss per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"validation_loss_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation loss plot: {e}\")\n    plt.close()\n\n# Plot validation accuracy comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        accs = data.get(\"val_acc\", [])\n        plt.plot(np.arange(1, len(accs) + 1), accs, marker=\"s\", label=name)\n    plt.suptitle(\"Validation Accuracy Across Datasets\")\n    plt.title(\"Validation Accuracy per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"validation_accuracy_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating validation accuracy plot: {e}\")\n    plt.close()\n\n# Plot Spearman correlation comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        corrs = data.get(\"corrs\", [])\n        plt.plot(np.arange(1, len(corrs) + 1), corrs, marker=\"^\", label=name)\n    plt.suptitle(\"Spearman Correlation of DVN Predictions\")\n    plt.title(\"Correlation vs. True Contributions\")\n    plt.xlabel(\"Meta\u2010update Step\")\n    plt.ylabel(\"Spearman Correlation\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"spearman_correlation_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating Spearman correlation plot: {e}\")\n    plt.close()\n\n# Plot N_meta update history comparison\ntry:\n    plt.figure()\n    for name, data in experiment_data.items():\n        nmeta = data.get(\"N_meta_history\", [])\n        plt.plot(np.arange(1, len(nmeta) + 1), nmeta, marker=\"d\", label=name)\n    plt.suptitle(\"Meta\u2010batch Size (N_meta) History\")\n    plt.title(\"Adaptive N_meta over Training\")\n    plt.xlabel(\"Meta\u2010update Step\")\n    plt.ylabel(\"N_meta Value\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"n_meta_history_comparison.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating N_meta history plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Paths to the three experiment_data.npy files\nexperiment_data_path_list = [\n    \"experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_4a73eaea65a04a7095f224a36e30f169_proc_247309/experiment_data.npy\",\n    \"experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_81a8a3a45d6b49a3ac2d29fa4e5e3199_proc_247308/experiment_data.npy\",\n    \"experiments/2025-06-08_16-25-53_meta_data_sampler_attempt_0/logs/0-run/experiment_results/experiment_e759358712174d5e983304d5fc8a3b03_proc_247310/experiment_data.npy\",\n]\n\n# Load all experiment data\nall_experiment_data = []\ntry:\n    for rel_path in experiment_data_path_list:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\"), rel_path)\n        exp_data = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(exp_data)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Determine dataset names from the first run\ndataset_names = list(all_experiment_data[0].keys()) if all_experiment_data else []\n\n# Metrics to aggregate and plot (will skip any missing ones)\nmetrics_to_plot = {\n    \"train_loss\": \"Training Loss\",\n    \"val_loss\": \"Validation Loss\",\n    \"train_acc\": \"Training Accuracy\",\n    \"val_acc\": \"Validation Accuracy\",\n    \"corrs\": \"Spearman Correlation\",\n    \"N_meta_history\": \"Meta-batch Size (N_meta)\",\n}\n\nfor metric_key, metric_name in metrics_to_plot.items():\n    try:\n        plt.figure()\n        for ds in dataset_names:\n            # Gather arrays for this metric across experiments\n            arrs = []\n            for exp_data in all_experiment_data:\n                data = exp_data.get(ds, {})\n                vals = data.get(metric_key, None)\n                if vals is not None and len(vals) > 0:\n                    arrs.append(np.array(vals))\n            if not arrs:\n                continue  # no data for this metric/dataset\n            # Align lengths by truncation\n            min_len = min(arr.shape[0] for arr in arrs)\n            stacked = np.vstack([arr[:min_len] for arr in arrs])\n            mean = np.mean(stacked, axis=0)\n            sem = np.std(stacked, axis=0, ddof=1) / np.sqrt(stacked.shape[0])\n            x = np.arange(1, min_len + 1)\n            plt.plot(x, mean, marker=\"o\", label=ds)\n            plt.fill_between(x, mean - sem, mean + sem, alpha=0.2)\n        plt.suptitle(f\"{metric_name} Across Datasets\")\n        plt.title(\"Mean \u00b1 SEM across experiments\")\n        xlabel = \"Epoch\" if metric_key not in [\"N_meta_history\"] else \"Meta-update Step\"\n        plt.xlabel(xlabel)\n        plt.ylabel(metric_name)\n        plt.legend()\n        fname = f\"{metric_key}_mean_sem.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating {metric_key} mean sem plot: {e}\")\n        plt.close()\n"], "plot_plan": ["We will load the saved experiment_data.npy from working_dir and select the synthetic dataset results. Then for each hyperparameter value (EPOCHS), we extract the per-epoch training and validation losses and Spearman correlations. We create two plots: (1) Training and validation loss curves for each EPOCHS run on a single figure, using solid lines for training and dashed lines for validation, (2) Spearman correlation versus epoch for each EPOCHS value with distinct markers. Each figure is generated inside a try-except block to catch errors, includes a main title specifying \"Synthetic Dataset\" and a clear subtitle, and is saved to working_dir with descriptive filenames. We add legends, axis labels, and include both suptitle and title for clarity. After saving, we ensure the figure is closed whether the plot succeeded or an exception occurred. This approach provides a concise visual summary of the training dynamics and DVN performance across hyperparameter sweeps.", null, null, "We load experiment_data from working_dir and extract per-dataset validation loss, validation accuracy, and Spearman correlations. First, we plot validation loss curves for all datasets in one figure to compare loss across epochs. Second, we plot validation accuracy curves similarly to compare accuracy trends. Third, we plot Spearman correlation curves per dataset to assess DVN predictions over iterations. Fourth, we create a scatter plot of validation loss versus Spearman correlation to explore their relationship. Fifth, we produce a bar chart of final validation accuracy for each dataset to summarize performance. Each plot is wrapped in a try-except block, saved with descriptive filenames to working_dir, and closed after saving. Error messages are printed if loading or plotting fails to ensure robustness.", null, null, "First, we load the saved experiment data from experiment_data.npy. Then, for each dataset (ag_news, yelp, dbpedia), we plot the training/validation loss curves, the training/validation accuracy curves, and the Spearman correlation across DVN meta-updates, each in its own try-except block and saved with descriptive filenames. We include a comparison plot of validation accuracy across all datasets. We also generate scatter plots of DVN predictions versus true contributions for each dataset. All figures include clear titles and subtitles, and we always close figures in finally-like blocks. Filenames include dataset names and plot types to maintain clarity. Finally, we print summary metrics (final loss and accuracy) for each dataset. No values are simulated; we only plot from experiment_data.npy.", null, null, "We first load the saved experiment data from the working directory. Then we aggregate the per\u2010epoch validation loss and accuracy, the Spearman correlation history, and the dynamically adjusted N_meta history across all datasets. We generate four comparison figures: one each for validation loss, validation accuracy, Spearman correlations, and N_meta updates. Each plot is wrapped in its own try/except block to handle possible errors and always closes the figure after saving. We add clear suptitles and subtitles, label axes, and include legends identifying each dataset. Filenames include descriptive names indicating the metric being compared. All plots are saved in `working_dir` using basic Matplotlib.", null, "We\u2019ll load the saved experiment data and extract per-dataset losses, accuracies, and Spearman correlations. We\u2019ll first print final train and validation accuracies for each dataset. Then in separate try-except blocks we\u2019ll produce a combined loss curve (solid for training, dashed for validation), a combined accuracy curve, a combined Spearman correlation curve, and finally a grouped bar chart comparing final train/validation accuracies across datasets. Each plot will be titled and labeled clearly and saved into the working directory with descriptive filenames. We\u2019ll ensure figures are closed after saving. All plotting uses basic matplotlib and numpy with no extra styling or simulated data.", null, null, null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": ["EPOCHS", null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script begins by loading the saved `experiment_data.npy` from the \u201cworking\u201d\ndirectory and extracting the `synthetic` dataset under\n`hyperparam_tuning_type_1`. It then computes the final training loss, validation\nloss, and Spearman correlation for each hyperparameter setting, selects the run\nwith the lowest final validation loss, and prints out the dataset name, the\nchosen number of epochs, and the corresponding final metric values with clear\nlabels. The code runs immediately at global scope and does not require any\nspecial entry point.", "", "", "The script sets the working directory, loads the saved experiment data, and then\nfor each dataset prints its name followed by its final validation loss, final\nvalidation accuracy, and the highest Spearman correlation observed during\ntraining. Each metric is clearly labeled (e.g., \u201cvalidation loss,\u201d \u201cvalidation\naccuracy,\u201d \u201cbest spearman correlation\u201d). The code runs at the global scope and\nrequires no special entry point.", "", "", "The script below loads the saved `experiment_data.npy` from the working\ndirectory and iterates over each dataset entry to extract the final (last)\ntraining and validation accuracies and losses. It prints the dataset name\nfollowed by clearly labeled metrics: \u201cTrain Accuracy,\u201d \u201cValidation Accuracy,\u201d\n\u201cTrain Loss,\u201d and \u201cValidation Loss.\u201d No plotting or special entry\u2010point guards\nare used; the code runs immediately at the global scope.", "The script sets the working directory to \u201cworking\u201d under the current path, then\nloads the saved NumPy dictionary with allow_pickle. It iterates over each\ndataset key, retrieves the epoch\u2010wise lists of validation loss, validation\naccuracy, Spearman correlation, and fairness disparity, and prints the final\nentry of each list. Fairness disparity is only printed if it is not None for a\ngiven dataset.", "The following script constructs the working directory path, loads the saved\n`experiment_data.npy` file, and then iterates over each dataset to print its\nname followed by the final train accuracy, validation accuracy, train loss, and\nvalidation loss with clear metric labels. All code is at the global scope and\nruns immediately without any entry\u2010point guard.", "Below is a script that immediately loads the saved `experiment_data.npy` from\nthe `working` directory, iterates over each dataset\u2019s recorded metrics, extracts\ntheir final values, and prints them with clear, descriptive labels.", "", "The following script locates and loads the saved `experiment_data.npy` from the\n`working` directory, which contains per\u2010dataset dictionaries of recorded metrics\nand losses. It then iterates through each dataset entry, extracts the final\n(last\u2010epoch) train/validation accuracy and loss values, and prints them with\nclear, descriptive labels. All code is at the global level and will execute\nimmediately when the script is run.", "", "", "Below is a script that immediately loads the saved `experiment_data.npy` from\nthe `working` directory, iterates over each dataset\u2019s recorded metrics, extracts\ntheir final values, and prints them with clear, descriptive labels.", "Below is a script that immediately loads the saved `experiment_data.npy` from\nthe `working` directory, iterates over each dataset\u2019s recorded metrics, extracts\ntheir final values, and prints them with clear, descriptive labels.", "Below is a script that immediately loads the saved `experiment_data.npy` from\nthe `working` directory, iterates over each dataset\u2019s recorded metrics, extracts\ntheir final values, and prints them with clear, descriptive labels.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Extract the synthetic dataset entry\nsynthetic = experiment_data[\"hyperparam_tuning_type_1\"][\"synthetic\"]\nparam_name = synthetic[\"param_name\"]\nparam_values = synthetic[\"param_values\"]\ntrain_losses = synthetic[\"metrics\"][\"train\"]\nval_losses = synthetic[\"metrics\"][\"val\"]\ncorrelations = synthetic[\"correlations\"]\n\n# Compute the final values for each run\nfinal_train = [losses[-1] for losses in train_losses]\nfinal_val = [losses[-1] for losses in val_losses]\nfinal_corr = [corrs[-1] for corrs in correlations]\n\n# Select the run with the lowest final validation loss\nbest_idx = int(np.argmin(final_val))\nbest_epochs = param_values[best_idx]\nbest_train_loss = final_train[best_idx]\nbest_val_loss = final_val[best_idx]\nbest_spearman = final_corr[best_idx]\n\n# Print out the results\nprint(\"Synthetic Dataset:\")\nprint(f\"Hyperparameter {param_name}: {best_epochs}\")\nprint(f\"Final Training Loss: {best_train_loss:.4f}\")\nprint(f\"Final Validation Loss: {best_val_loss:.4f}\")\nprint(f\"Final Spearman Correlation: {best_spearman:.4f}\")\n", "", "", "import os\nimport numpy as np\n\n# Define working directory and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each dataset and print metrics\nfor dataset_name, metrics in experiment_data.items():\n    # Extract final validation loss and accuracy\n    val_loss = metrics.get(\"val_loss\", [])\n    val_acc = metrics.get(\"val_acc\", [])\n    corrs = metrics.get(\"corrs\", [])\n\n    if val_loss:\n        final_val_loss = val_loss[-1]\n    else:\n        final_val_loss = float(\"nan\")\n\n    if val_acc:\n        final_val_acc = val_acc[-1]\n    else:\n        final_val_acc = float(\"nan\")\n\n    best_corr = max(corrs) if corrs else float(\"nan\")\n\n    # Print the dataset metrics\n    print(f\"{dataset_name}\")\n    print(f\"validation loss: {final_val_loss:.4f}\")\n    print(f\"validation accuracy: {final_val_acc:.4f}\")\n    print(f\"best spearman correlation: {best_corr:.4f}\")\n", "", "", "import os\nimport numpy as np\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over datasets and print final metrics and losses\nfor dataset_name, results in experiment_data.items():\n    train_acc_list = results[\"metrics\"][\"train\"]\n    val_acc_list = results[\"metrics\"][\"val\"]\n    train_loss_list = results[\"losses\"][\"train\"]\n    val_loss_list = results[\"losses\"][\"val\"]\n\n    final_train_acc = train_acc_list[-1]\n    final_val_acc = val_acc_list[-1]\n    final_train_loss = train_loss_list[-1]\n    final_val_loss = val_loss_list[-1]\n\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"Train Accuracy: {final_train_acc:.4f}\")\n    print(f\"Validation Accuracy: {final_val_acc:.4f}\")\n    print(f\"Train Loss: {final_train_loss:.4f}\")\n    print(f\"Validation Loss: {final_val_loss:.4f}\\n\")\n", "import os\nimport numpy as np\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# Load the experiment results (a dict of dicts)\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\nfor dataset_name, metrics in experiment_data.items():\n    print(f\"{dataset_name}\")\n    # Extract final values from each metric list\n    final_val_loss = metrics[\"val_loss\"][-1]\n    final_val_acc = metrics[\"val_acc\"][-1]\n    final_corr = metrics[\"corr\"][-1]\n    final_fairness = metrics[\"fairness\"][-1]\n\n    print(f\"validation loss: {final_val_loss:.4f}\")\n    print(f\"validation accuracy: {final_val_acc:.4f}\")\n    print(f\"spearman correlation: {final_corr:.4f}\")\n    if final_fairness is not None:\n        print(f\"fairness disparity: {final_fairness:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Print final metrics for each dataset\nfor dataset_name, results in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n    final_train_acc = results[\"metrics\"][\"train\"][-1]\n    final_val_acc = results[\"metrics\"][\"val\"][-1]\n    final_train_loss = results[\"losses\"][\"train\"][-1]\n    final_val_loss = results[\"losses\"][\"val\"][-1]\n    print(f\"Train accuracy: {final_train_acc:.4f}\")\n    print(f\"Validation accuracy: {final_val_acc:.4f}\")\n    print(f\"Train loss: {final_train_loss:.4f}\")\n    print(f\"Validation loss: {final_val_loss:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Determine working directory and load saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print the final recorded metrics\nfor dataset_name, metrics in experiment_data.items():\n    # Extract final values for each metric\n    final_val_loss = metrics.get(\"val_loss\", [None])[-1]\n    final_val_acc = metrics.get(\"val_acc\", [None])[-1]\n    final_corr = metrics.get(\"corrs\", [None])[-1]\n    final_n_meta = metrics.get(\"N_meta_history\", [None])[-1]\n\n    # Print results with clear metric names\n    print(dataset_name)\n    print(f\"validation loss: {final_val_loss}\")\n    print(f\"validation accuracy: {final_val_acc}\")\n    if final_corr is not None:\n        print(f\"Spearman correlation (final): {final_corr}\")\n    if final_n_meta is not None:\n        print(f\"N_meta (final): {final_n_meta}\")\n", "", "import os\nimport numpy as np\n\n# Locate the working directory and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print the final metrics\nfor dataset_name, results in experiment_data.items():\n    # Extract lists of metrics and losses\n    train_acc_list = results[\"metrics\"][\"train\"]\n    val_acc_list = results[\"metrics\"][\"val\"]\n    train_loss_list = results[\"losses\"][\"train\"]\n    val_loss_list = results[\"losses\"][\"val\"]\n\n    # Get the final values for each metric\n    final_train_acc = train_acc_list[-1]\n    final_val_acc = val_acc_list[-1]\n    final_train_loss = train_loss_list[-1]\n    final_val_loss = val_loss_list[-1]\n\n    # Print dataset name and final metrics\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"Train accuracy: {final_train_acc:.4f}\")\n    print(f\"Validation accuracy: {final_val_acc:.4f}\")\n    print(f\"Train loss: {final_train_loss:.4f}\")\n    print(f\"Validation loss: {final_val_loss:.4f}\\n\")\n", "", "", "import os\nimport numpy as np\n\n# Determine working directory and load saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print the final recorded metrics\nfor dataset_name, metrics in experiment_data.items():\n    # Extract final values for each metric\n    final_val_loss = metrics.get(\"val_loss\", [None])[-1]\n    final_val_acc = metrics.get(\"val_acc\", [None])[-1]\n    final_corr = metrics.get(\"corrs\", [None])[-1]\n    final_n_meta = metrics.get(\"N_meta_history\", [None])[-1]\n\n    # Print results with clear metric names\n    print(dataset_name)\n    print(f\"validation loss: {final_val_loss}\")\n    print(f\"validation accuracy: {final_val_acc}\")\n    if final_corr is not None:\n        print(f\"Spearman correlation (final): {final_corr}\")\n    if final_n_meta is not None:\n        print(f\"N_meta (final): {final_n_meta}\")\n", "import os\nimport numpy as np\n\n# Determine working directory and load saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print the final recorded metrics\nfor dataset_name, metrics in experiment_data.items():\n    # Extract final values for each metric\n    final_val_loss = metrics.get(\"val_loss\", [None])[-1]\n    final_val_acc = metrics.get(\"val_acc\", [None])[-1]\n    final_corr = metrics.get(\"corrs\", [None])[-1]\n    final_n_meta = metrics.get(\"N_meta_history\", [None])[-1]\n\n    # Print results with clear metric names\n    print(dataset_name)\n    print(f\"validation loss: {final_val_loss}\")\n    print(f\"validation accuracy: {final_val_acc}\")\n    if final_corr is not None:\n        print(f\"Spearman correlation (final): {final_corr}\")\n    if final_n_meta is not None:\n        print(f\"N_meta (final): {final_n_meta}\")\n", "import os\nimport numpy as np\n\n# Determine working directory and load saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print the final recorded metrics\nfor dataset_name, metrics in experiment_data.items():\n    # Extract final values for each metric\n    final_val_loss = metrics.get(\"val_loss\", [None])[-1]\n    final_val_acc = metrics.get(\"val_acc\", [None])[-1]\n    final_corr = metrics.get(\"corrs\", [None])[-1]\n    final_n_meta = metrics.get(\"N_meta_history\", [None])[-1]\n\n    # Print results with clear metric names\n    print(dataset_name)\n    print(f\"validation loss: {final_val_loss}\")\n    print(f\"validation accuracy: {final_val_acc}\")\n    if final_corr is not None:\n        print(f\"Spearman correlation (final): {final_corr}\")\n    if final_n_meta is not None:\n        print(f\"N_meta (final): {final_n_meta}\")\n", ""], "parse_term_out": ["['Synthetic Dataset:', '\\n', 'Hyperparameter EPOCHS: 50', '\\n', 'Final Training\nLoss: 0.0113', '\\n', 'Final Validation Loss: 0.0100', '\\n', 'Final Spearman\nCorrelation: 0.3038', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "", "", "['ag_news', '\\n', 'validation loss: 1.2654', '\\n', 'validation accuracy:\n0.5800', '\\n', 'best spearman correlation: 0.0752', '\\n', 'yelp', '\\n',\n'validation loss: 0.5525', '\\n', 'validation accuracy: 0.8150', '\\n', 'best\nspearman correlation: 0.2602', '\\n', 'dbpedia', '\\n', 'validation loss: 2.4078',\n'\\n', 'validation accuracy: 0.7250', '\\n', 'best spearman correlation: 0.0932',\n'\\n', 'Execution time: a moment seconds (time limit is an hour).']", "", "", "['Dataset: ag_news', '\\n', 'Train Accuracy: 0.7970', '\\n', 'Validation Accuracy:\n0.6800', '\\n', 'Train Loss: 1.2041', '\\n', 'Validation Loss: 1.2470\\n', '\\n',\n'Dataset: yelp', '\\n', 'Train Accuracy: 0.8630', '\\n', 'Validation Accuracy:\n0.8400', '\\n', 'Train Loss: 0.5234', '\\n', 'Validation Loss: 0.5444\\n', '\\n',\n'Dataset: dbpedia', '\\n', 'Train Accuracy: 0.7120', '\\n', 'Validation Accuracy:\n0.5750', '\\n', 'Train Loss: 2.3043', '\\n', 'Validation Loss: 2.3937\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['ag_news', '\\n', 'validation loss: 1.0158', '\\n', 'validation accuracy:\n0.7250', '\\n', 'spearman correlation: 0.0818', '\\n', '\\n', 'yelp', '\\n',\n'validation loss: 0.4314', '\\n', 'validation accuracy: 0.8500', '\\n', 'spearman\ncorrelation: -0.1667', '\\n', 'fairness disparity: 0.0000', '\\n', '\\n',\n'dbpedia', '\\n', 'validation loss: 1.9922', '\\n', 'validation accuracy: 0.6300',\n'\\n', 'spearman correlation: -0.3970', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: ag_news', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 12, in <module>\\n    final_train_acc =\nresults[\"metrics\"][\"train\"][-1]\\n\n~~~~~~~^^^^^^^^^^^\\nKeyError: \\'metrics\\'\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['ag_news', '\\n', 'validation loss: 1.2479195594787598', '\\n', 'validation\naccuracy: 0.699999988079071', '\\n', 'Spearman correlation (final):\n0.4300751879699248', '\\n', 'N_meta (final): 20', '\\n', 'yelp', '\\n', 'validation\nloss: 0.5452219247817993', '\\n', 'validation accuracy: 0.8349999785423279',\n'\\n', 'Spearman correlation (final): 0.4285714285714286', '\\n', 'N_meta (final):\n8', '\\n', 'dbpedia', '\\n', 'validation loss: 2.405613899230957', '\\n',\n'validation accuracy: 0.625', '\\n', 'Spearman correlation (final):\n0.10375939849624059', '\\n', 'N_meta (final): 20', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "", "['Dataset: ag_news', '\\n', 'Train accuracy: 0.8040', '\\n', 'Validation accuracy:\n0.6900', '\\n', 'Train loss: 1.2036', '\\n', 'Validation loss: 1.2461\\n', '\\n',\n'Dataset: dbpedia', '\\n', 'Train accuracy: 0.8060', '\\n', 'Validation accuracy:\n0.7100', '\\n', 'Train loss: 2.3126', '\\n', 'Validation loss: 2.3858\\n', '\\n',\n'Dataset: yelp', '\\n', 'Train accuracy: 0.8620', '\\n', 'Validation accuracy:\n0.8350', '\\n', 'Train loss: 0.5138', '\\n', 'Validation loss: 0.5335\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "", "", "['ag_news', '\\n', 'validation loss: 1.253648042678833', '\\n', 'validation\naccuracy: 0.6349999904632568', '\\n', 'Spearman correlation (final):\n-0.003007518796992481', '\\n', 'N_meta (final): 40', '\\n', 'yelp', '\\n',\n'validation loss: 0.5458672046661377', '\\n', 'validation accuracy:\n0.8399999737739563', '\\n', 'Spearman correlation (final): -0.14436090225563908',\n'\\n', 'N_meta (final): 20', '\\n', 'dbpedia', '\\n', 'validation loss:\n2.3887181282043457', '\\n', 'validation accuracy: 0.6150000095367432', '\\n',\n'Spearman correlation (final): 0.21954887218045113', '\\n', 'N_meta (final): 40',\n'\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['ag_news', '\\n', 'validation loss: 1.2523906230926514', '\\n', 'validation\naccuracy: 0.6649999618530273', '\\n', 'Spearman correlation (final):\n-0.12932330827067667', '\\n', 'N_meta (final): 40', '\\n', 'yelp', '\\n',\n'validation loss: 0.5448862314224243', '\\n', 'validation accuracy:\n0.824999988079071', '\\n', 'Spearman correlation (final): 0.8165413533834586',\n'\\n', 'N_meta (final): 20', '\\n', 'dbpedia', '\\n', 'validation loss:\n2.406888008117676', '\\n', 'validation accuracy: 0.625', '\\n', 'Spearman\ncorrelation (final): 0.19849624060150375', '\\n', 'N_meta (final): 20', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['ag_news', '\\n', 'validation loss: 1.2433955669403076', '\\n', 'validation\naccuracy: 0.6800000071525574', '\\n', 'Spearman correlation (final):\n0.5473684210526315', '\\n', 'N_meta (final): 20', '\\n', 'yelp', '\\n', 'validation\nloss: 0.5353274941444397', '\\n', 'validation accuracy: 0.8349999785423279',\n'\\n', 'Spearman correlation (final): 0.6481203007518795', '\\n', 'N_meta (final):\n16', '\\n', 'dbpedia', '\\n', 'validation loss: 2.3887720108032227', '\\n',\n'validation accuracy: 0.6200000047683716', '\\n', 'Spearman correlation (final):\n0.2300751879699248', '\\n', 'N_meta (final): 2', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, "KeyError", null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, {"args": ["metrics"]}, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 12, "<module>", "final_train_acc = results[\"metrics\"][\"train\"][-1]"]], null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>

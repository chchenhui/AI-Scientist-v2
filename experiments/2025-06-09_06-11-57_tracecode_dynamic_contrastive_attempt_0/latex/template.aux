\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{jain2020contrastivecr,liu2023contrabertec,ding2023concordcc}
\citation{huang2024coderp}
\citation{maciver2019hypothesisan}
\citation{oord2018representationlw}
\citation{jain2020contrastivecr}
\citation{liu2023contrabertec}
\citation{ding2023concordcc}
\citation{guo2020graphcodebertpc}
\citation{wang2022codemvplt}
\citation{zou2021inlineformulatexmathnx}
\citation{huang2024coderp}
\citation{vaswani2017attentionia}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{1}{section.2}\protected@file@percent }
\citation{kingma2014adamam}
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Setup}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{2}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Synthetic dataset ablation for budgets $E\!=\!10,30,50$. Solid lines = train, dashed = val. (a) All runs reach zero train loss by $\approx 15$ epochs; val loss plateaus with spikes. (b) Retrieval accuracy jumps to 1.0 at epochs $\{8,10,12\}$ for $E=\{10,30,50\}$.}}{2}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Loss vs.\ Epoch}}}{2}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Accuracy vs.\ Epoch}}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:epoch}{{1}{2}{Synthetic dataset ablation for budgets $E\!=\!10,30,50$. Solid lines = train, dashed = val. (a) All runs reach zero train loss by $\approx 15$ epochs; val loss plateaus with spikes. (b) Retrieval accuracy jumps to 1.0 at epochs $\{8,10,12\}$ for $E=\{10,30,50\}$}{figure.1}{}}
\newlabel{fig:epoch@cref}{{[figure][1][]1}{[1][2][]2}}
\@writefile{toc}{\contentsline {paragraph}{Epoch Budget Ablation}{2}{figure.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Negative‐Sampling Hardness}{2}{figure.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Distance‐Metric Ablation}{2}{figure.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Embedding‐Dimension Ablation}{2}{figure.4}\protected@file@percent }
\bibdata{references}
\bibcite{ding2023concordcc}{{1}{2023}{{Ding et~al.}}{{Ding, Chakraborty, Buratti, Pujar, Morari, Kaiser, and Ray}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Random vs.\ hard negatives (left: loss, right: accuracy). Rows: random (top), hard (bottom). Hard negatives slow convergence and yield val accuracy $\approx 0.8$, while random negatives quickly reach $\approx 1.0$ but show loss spikes.}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:neg}{{2}{3}{Random vs.\ hard negatives (left: loss, right: accuracy). Rows: random (top), hard (bottom). Hard negatives slow convergence and yield val accuracy $\approx 0.8$, while random negatives quickly reach $\approx 1.0$ but show loss spikes}{figure.2}{}}
\newlabel{fig:neg@cref}{{[figure][2][]2}{[1][2][]3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Euclidean vs.\ cosine (left: loss, right: accuracy). Euclidean converges faster and more stably; cosine shows higher validation‐loss variance and delayed accuracy gains.}}{3}{figure.3}\protected@file@percent }
\newlabel{fig:dist}{{3}{3}{Euclidean vs.\ cosine (left: loss, right: accuracy). Euclidean converges faster and more stably; cosine shows higher validation‐loss variance and delayed accuracy gains}{figure.3}{}}
\newlabel{fig:dist@cref}{{[figure][3][]3}{[1][2][]3}}
\@writefile{toc}{\contentsline {paragraph}{Key Lessons}{3}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{3}{section.6}\protected@file@percent }
\bibcite{guo2020graphcodebertpc}{{2}{2020}{{Guo et~al.}}{{Guo, Ren, Lu, Feng, Tang, Liu, Zhou, Duan, Yin, Jiang, and Zhou}}}
\bibcite{huang2024coderp}{{3}{2024}{{Huang et~al.}}{{Huang, Zhao, Rong, Guo, He, and Chen}}}
\bibcite{jain2020contrastivecr}{{4}{2020}{{Jain et~al.}}{{Jain, Jain, Zhang, Abbeel, Gonzalez, and Stoica}}}
\bibcite{kingma2014adamam}{{5}{2014}{{Kingma \& Ba}}{{Kingma and Ba}}}
\bibcite{liu2023contrabertec}{{6}{2023}{{Liu et~al.}}{{Liu, Wu, Xie, Meng, and Liu}}}
\bibcite{maciver2019hypothesisan}{{7}{2019}{{MacIver \& Hatfield-Dodds}}{{MacIver and Hatfield-Dodds}}}
\bibcite{oord2018representationlw}{{8}{2018}{{van~den Oord et~al.}}{{van~den Oord, Li, and Vinyals}}}
\bibcite{vaswani2017attentionia}{{9}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{wang2022codemvplt}{{10}{2022}{{Wang et~al.}}{{Wang, Wang, Wan, Wang, Zhou, Li, Wu, and Liu}}}
\bibcite{zou2021inlineformulatexmathnx}{{11}{2021}{{Zou et~al.}}{{Zou, Wang, Xu, Li, and Jin}}}
\bibstyle{iclr2025}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Embedding size sweep (left: loss, right: accuracy). Dimensions $\ge 64$ reach zero loss and 100\% accuracy within $\approx 5$ epochs; smaller dims plateau at higher loss and lower accuracy.}}{4}{figure.4}\protected@file@percent }
\newlabel{fig:dim}{{4}{4}{Embedding size sweep (left: loss, right: accuracy). Dimensions $\ge 64$ reach zero loss and 100\% accuracy within $\approx 5$ epochs; smaller dims plateau at higher loss and lower accuracy}{figure.4}{}}
\newlabel{fig:dim@cref}{{[figure][4][]4}{[1][2][]4}}
\@writefile{toc}{\contentsline {paragraph}{Implementation Details}{4}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Additional Ablations}{4}{figure.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Architecture ablation: comparing Transformer‐only, LSTM‐only, and joint encoders. Joint model yields highest retrieval accuracy on synthetic task.}}{5}{figure.5}\protected@file@percent }
\newlabel{app:arch}{{5}{5}{Architecture ablation: comparing Transformer‐only, LSTM‐only, and joint encoders. Joint model yields highest retrieval accuracy on synthetic task}{figure.5}{}}
\newlabel{app:arch@cref}{{[figure][5][2147483647]5}{[1][4][]5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Triplet‐margin sweep: larger margins slow convergence and reduce final accuracy on the synthetic retrieval task.}}{5}{figure.6}\protected@file@percent }
\newlabel{app:margin}{{6}{5}{Triplet‐margin sweep: larger margins slow convergence and reduce final accuracy on the synthetic retrieval task}{figure.6}{}}
\newlabel{app:margin@cref}{{[figure][6][2147483647]6}{[1][4][]5}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Variable renaming invariance: performance remains near‐perfect, confirming trace grouping is unaffected by superficial code edits.}}{5}{figure.7}\protected@file@percent }
\newlabel{app:rename}{{7}{5}{Variable renaming invariance: performance remains near‐perfect, confirming trace grouping is unaffected by superficial code edits}{figure.7}{}}
\newlabel{app:rename@cref}{{[figure][7][2147483647]7}{[1][4][]5}}
\gdef \@abspage@last{5}

{"edges": [[2, 5], [2, 3], [2, 6], [2, 4]], "layout": [[0.0, 0.0], [0.2857142857142857, 0.0], [0.5714285714285714, 0.0], [0.14285714285714285, 1.0], [0.42857142857142855, 1.0], [0.7142857142857143, 1.0], [1.0, 1.0]], "plan": ["We use a synthetic corpus of small arithmetic functions (add, mul, square) and\nmanually write three semantically equivalent variants per function. For each\ngroup we generate a shared set of random inputs and record their outputs as\ndynamic traces, which ensures grouping of trace\u2010equivalent snippets. We then\nbuild a triplet dataset (anchor, positive, negative) where positives share the\nsame trace and negatives come from a different function group. A simple\ncharacter\u2010level encoder embeds each code string via a PyTorch Embedding plus\naverage pooling and an MLP projection, followed by L2\u2010normalization. We train\nthis encoder with a margin\u2010ranking loss to pull together trace\u2010equivalent\nvariants and push apart dissimilar ones. At each epoch we compute validation\nloss and Top\u20101 trace\u2010equivalence retrieval accuracy (the fraction of times the\npositive is scored closer than the negative). We log and print validation loss\nand retrieval accuracy every epoch and save all metrics and losses into\n`./working/experiment_data.npy`. This gives an end\u2010to\u2010end, GPU\u2010enabled baseline\nfrom dynamic trace generation to contrastive pre\u2010training and evaluation.", "We create a toy corpus of simple Python functions along with semantically\nidentical refactored variants, and automatically generate random integer inputs\nto record their output traces for pairing.  We build character\u2010level\nvocabularies over the code strings and pad all sequences to uniform length.  A\nlightweight Transformer encoder maps each token sequence into a dense embedding\nvia average\u2010pooling.  We train with a triplet margin loss by sampling anchors,\ntheir trace\u2010equivalent positives, and random negatives.  After each epoch, we\ncompute validation loss and the Top\u20101 trace\u2010equivalence retrieval accuracy by\nchecking if the nearest neighbor in embedding space matches the known variant.\nAll per\u2010epoch losses and metrics are stored in a nested dictionary and finally\nsaved to an .npy file under the working directory.  This simple pipeline serves\nas a solid baseline for dynamic\u2010trace\u2010augmented contrastive pre\u2010training.", "In this preliminary implementation, we synthetically generate pairs of simple\nPython functions with equivalent dynamic traces (e.g. addition with swapped\noperands) by executing each snippet on random inputs and grouping them by output\ntraces. We then char\u2010tokenize each code snippet, pad to uniform length, and\nbuild a small retrieval dataset of positive and negative pairs for contrastive\ntraining. A lightweight LSTM\u2010based encoder maps padded sequences to fixed\u2010size\nembeddings. We train with a Triplet Margin Loss, sampling, for each anchor, a\npositive variant from the same trace group and a negative from a different\ngroup. During training, we log the average training and validation loss per\nepoch, and compute Top\u20101 Trace\u2010Equivalence retrieval accuracy on held\u2010out groups\nby ranking each snippet\u2019s nearest neighbor in embedding space. All metrics,\nlosses, predictions, and ground truth labels are stored in experiment_data and\nsaved as a numpy file in the working directory. Finally, we print per\u2010epoch\nvalidation losses and the final retrieval accuracy. This simple baseline sets up\nthe pipeline from code preparation to contrastive training and evaluation.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# reproducibility\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# 1) Synthetic function variants\ngroup_data = [\n    {\n        \"variants\": [\n            \"def add(a,b): return a+b\",\n            \"def add(x,y): return x+y\",\n            \"def sum_vals(a,b): return b+a\",\n        ],\n        \"arity\": 2,\n    },\n    {\n        \"variants\": [\n            \"def mul(a,b): return a*b\",\n            \"def mul(x,y): return x*y\",\n            \"def multiply(a,b): return b*a\",\n        ],\n        \"arity\": 2,\n    },\n    {\n        \"variants\": [\n            \"def pow2(a): return a**2\",\n            \"def square(x): return x*x\",\n            \"def sq(a): return a*a\",\n        ],\n        \"arity\": 1,\n    },\n]\n\n# 2) Generate dynamic traces (not used beyond checking equivalence here)\nfor grp in group_data:\n    # shared random inputs\n    if grp[\"arity\"] == 2:\n        inputs = [(random.randint(-10, 10), random.randint(-10, 10)) for _ in range(16)]\n    else:\n        inputs = [(random.randint(-10, 10),) for _ in range(16)]\n    grp[\"traces\"] = {}\n    for code in grp[\"variants\"]:\n        local = {}\n        exec(code, {}, local)\n        fn = next(v for v in local.values() if callable(v))\n        outputs = []\n        for inp in inputs:\n            try:\n                outputs.append(fn(*inp))\n            except:\n                outputs.append(None)\n        grp[\"traces\"][code] = tuple(outputs)\n\n# 3) Build triplet dataset\ntriplets = []\nfor i, grp in enumerate(group_data):\n    other_idxs = [j for j in range(len(group_data)) if j != i]\n    for a, b in [\n        (x, y)\n        for idx, x in enumerate(grp[\"variants\"])\n        for y in grp[\"variants\"][idx + 1 :]\n    ]:\n        neg_grp = group_data[random.choice(other_idxs)]\n        neg = random.choice(neg_grp[\"variants\"])\n        triplets.append((a, b, neg))\n        triplets.append((b, a, neg))\n\n\nclass CodeTripletDataset(Dataset):\n    def __init__(self, triplets):\n        self.triplets = triplets\n        # build char vocab\n        chars = set(\"\".join([c for t in triplets for s in t for c in s]))\n        self.char2idx = {\"<pad>\": 0, \"<unk>\": 1}\n        for c in sorted(chars):\n            self.char2idx[c] = len(self.char2idx)\n        self.pad_idx = 0\n        self.unk_idx = 1\n        # max seq length\n        self.max_len = max(len(s) for t in triplets for s in t)\n\n    def __len__(self):\n        return len(self.triplets)\n\n    def __getitem__(self, idx):\n        a, p, n = self.triplets[idx]\n        return self.encode(a), self.encode(p), self.encode(n)\n\n    def encode(self, s):\n        arr = [self.char2idx.get(c, self.unk_idx) for c in s]\n        if len(arr) < self.max_len:\n            arr = arr + [self.pad_idx] * (self.max_len - len(arr))\n        return torch.LongTensor(arr)\n\n\ndataset = CodeTripletDataset(triplets)\nloader = DataLoader(dataset, batch_size=8, shuffle=True)\n\n\n# 4) Define encoder model\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=32, proj_dim=16):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.fc = nn.Linear(embed_dim, proj_dim)\n\n    def forward(self, x):\n        emb = self.embed(x)  # (B, L, E)\n        mask = (x != 0).unsqueeze(-1).float()  # (B, L, 1)\n        emb = emb * mask\n        summed = emb.sum(1)  # (B, E)\n        lengths = mask.sum(1).clamp(min=1)  # (B,1)\n        avg = summed / lengths\n        out = self.fc(avg)\n        return F.normalize(out, dim=1)\n\n\nmodel = CodeEncoder(len(dataset.char2idx)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MarginRankingLoss(margin=1.0)\n\n# 5) Experiment tracking\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# 6) Training & evaluation\nepochs = 10\nfor epoch in range(epochs):\n    model.train()\n    t_loss = 0.0\n    t_corr = 0\n    t_tot = 0\n    for a, p, n in loader:\n        a, p, n = a.to(device), p.to(device), n.to(device)\n        ea, ep, en = model(a), model(p), model(n)\n        sim_pos = (ea * ep).sum(1)\n        sim_neg = (ea * en).sum(1)\n        tgt = torch.ones_like(sim_pos, device=device)\n        loss = criterion(sim_pos, sim_neg, tgt)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        bs = a.size(0)\n        t_loss += loss.item() * bs\n        t_corr += (sim_pos > sim_neg).sum().item()\n        t_tot += bs\n    train_loss = t_loss / t_tot\n    train_acc = t_corr / t_tot\n\n    model.eval()\n    v_loss = 0.0\n    v_corr = 0\n    v_tot = 0\n    with torch.no_grad():\n        for a, p, n in loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            ea, ep, en = model(a), model(p), model(n)\n            sim_pos = (ea * ep).sum(1)\n            sim_neg = (ea * en).sum(1)\n            tgt = torch.ones_like(sim_pos, device=device)\n            loss = criterion(sim_pos, sim_neg, tgt)\n            bs = a.size(0)\n            v_loss += loss.item() * bs\n            v_corr += (sim_pos > sim_neg).sum().item()\n            v_tot += bs\n    val_loss = v_loss / v_tot\n    val_acc = v_corr / v_tot\n\n    print(\n        f\"Epoch {epoch+1}: validation_loss = {val_loss:.4f}, Top-1 retrieval accuracy = {val_acc:.4f}\"\n    )\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(\n        {\"epoch\": epoch + 1, \"retrieval_accuracy\": train_acc}\n    )\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(\n        {\"epoch\": epoch + 1, \"retrieval_accuracy\": val_acc}\n    )\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(val_loss)\n\n# 7) Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# GPU setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# 1) Build synthetic code corpus with simple variants\nbase_codes = [\n    \"def f(x): return x+1\",\n    \"def g(x): return x*2\",\n    \"def h(x): return x if x>0 else -x\",\n    \"def p(x): return x*x\",\n]\ncode_list = []\nfor code in base_codes:\n    name = code.split()[1].split(\"(\")[0]\n    expr = code.split(\"return \")[1]\n    variant = f\"def {name}(x):\\n    tmp = {expr}\\n    return tmp\"\n    code_list += [code, variant]\n\n# 2) Execute each snippet on random inputs to get dynamic traces\ninputs = [random.randint(-10, 10) for _ in range(20)]\nfunc_objs = []\nfor code in code_list:\n    env = {}\n    exec(code, {}, env)\n    for v in env.values():\n        if callable(v):\n            func_objs.append(v)\n            break\ntraces = [[f(x) for x in inputs] for f in func_objs]\n\n# 3) Char-level tokenization\nchars = sorted({c for s in code_list for c in s})\nchar2idx = {c: i + 1 for i, c in enumerate(chars)}\nmax_len = max(len(s) for s in code_list)\ntokens_list = []\nfor s in code_list:\n    seq = [char2idx[c] for c in s] + [0] * (max_len - len(s))\n    tokens_list.append(seq)\n\n# 4) Dataset of triplets (anchor, positive, negative)\npairs = [(i, i + 1) for i in range(0, len(code_list), 2)]\n\n\nclass TraceDataset(Dataset):\n    def __init__(self, tokens, pairs):\n        self.tokens = tokens\n        self.pairs = pairs\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        i, j = self.pairs[idx]\n        # random negative index\n        neg = random.choice([k for k in range(len(self.tokens)) if k not in (i, j)])\n        return (\n            torch.LongTensor(self.tokens[i]),\n            torch.LongTensor(self.tokens[j]),\n            torch.LongTensor(self.tokens[neg]),\n        )\n\n\ndef collate(batch):\n    A = torch.stack([a for a, _, _ in batch])\n    P = torch.stack([p for _, p, _ in batch])\n    N = torch.stack([n for _, _, n in batch])\n    return A, P, N\n\n\ndataset = TraceDataset(tokens_list, pairs)\nloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate)\n\n\n# 5) Model: Transformer encoder + pooling\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim=64):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=4)\n        self.transformer = nn.TransformerEncoder(layer, num_layers=1)\n        self.pool = nn.AdaptiveAvgPool1d(1)\n\n    def forward(self, x):\n        # x: (B, L)\n        emb = self.embedding(x).transpose(0, 1)  # L, B, D\n        out = self.transformer(emb).transpose(0, 1)  # B, L, D\n        return self.pool(out.transpose(1, 2)).squeeze(-1)  # B, D\n\n\nmodel = CodeEncoder(len(char2idx) + 1).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# 6) Experiment logging structure\nexperiment_data = {\n    \"trace_dataset\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# 7) Training loop with triplet loss and retrieval eval\nfor epoch in range(1, 11):\n    model.train()\n    total_loss = 0\n    for A, P, N in loader:\n        A, P, N = A.to(device), P.to(device), N.to(device)\n        eA, eP, eN = model(A), model(P), model(N)\n        loss = F.triplet_margin_loss(eA, eP, eN, margin=1.0)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * A.size(0)\n    train_loss = total_loss / len(dataset)\n    # validation pass (same data here for simplicity)\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for A, P, N in loader:\n            A, P, N = A.to(device), P.to(device), N.to(device)\n            eA, eP, eN = model(A), model(P), model(N)\n            val_loss += F.triplet_margin_loss(eA, eP, eN, margin=1.0).item() * A.size(0)\n    val_loss /= len(dataset)\n    # retrieval accuracy\n    with torch.no_grad():\n        all_toks = torch.LongTensor(tokens_list).to(device)\n        emb = F.normalize(model(all_toks), dim=1)\n        correct = 0\n        for i, j in pairs:\n            sims = (emb[i : i + 1] @ emb.t()).squeeze(0)\n            sims[i] = -1e9\n            pred = sims.argmax().item()\n            if pred == j:\n                correct += 1\n    acc = correct / len(pairs)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, retrieval_acc = {acc:.4f}\")\n    experiment_data[\"trace_dataset\"][\"losses\"][\"train\"].append(train_loss)\n    experiment_data[\"trace_dataset\"][\"losses\"][\"val\"].append(val_loss)\n    experiment_data[\"trace_dataset\"][\"metrics\"][\"train\"].append(acc)\n    experiment_data[\"trace_dataset\"][\"metrics\"][\"val\"].append(acc)\n\n# 8) Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic code snippets\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\n\n# generate dynamic traces\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: ids for gid, ids in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# encode as char tokens\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\n# split groups into train/val\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# define encoder\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\nmodel = CodeEncoder(len(stoi), 64, 64).to(device)\noptimizer = Adam(model.parameters(), lr=1e-3)\nloss_fn = nn.TripletMarginLoss(margin=1.0)\n\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n    # training\n    model.train()\n    train_loss = 0.0\n    for a, p, n in train_loader:\n        a, p, n = a.to(device), p.to(device), n.to(device)\n        emb_a = model(a)\n        emb_p = model(p)\n        emb_n = model(n)\n        loss = loss_fn(emb_a, emb_p, emb_n)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    train_loss /= len(train_loader)\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(train_loss)\n    # validation loss\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for a, p, n in val_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a = model(a)\n            emb_p = model(p)\n            emb_n = model(n)\n            val_loss += loss_fn(emb_a, emb_p, emb_n).item()\n    val_loss /= len(val_loader)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(val_loss)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n    # retrieval accuracy on train and val\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n\n        def compute_acc(indices):\n            correct = 0\n            for i in indices:\n                sim = sims[i].clone()\n                sim[i] = -float(\"inf\")\n                top1 = torch.argmax(sim).item()\n                if index_to_gid[top1] == index_to_gid[i]:\n                    correct += 1\n            return correct / len(indices) if indices else 0\n\n        train_acc = compute_acc(train_indices)\n        val_acc = compute_acc(val_indices)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(train_acc)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(val_acc)\n\n# store final predictions and ground truth for val\nwith torch.no_grad():\n    emb_all = model(encoded.to(device))\n    emb_norm = F.normalize(emb_all, dim=1)\n    sims = emb_norm @ emb_norm.T\n    for i in val_indices:\n        sim = sims[i].clone()\n        sim[i] = -float(\"inf\")\n        top1 = torch.argmax(sim).item()\n        experiment_data[\"synthetic\"][\"predictions\"].append(index_to_gid[top1])\n        experiment_data[\"synthetic\"][\"ground_truth\"].append(index_to_gid[i])\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Top-1 Trace-Equivalence Retrieval Accuracy: {val_acc:.4f}\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic code snippets\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\n\n# generate dynamic traces\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: ids for gid, ids in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# encode as char tokens\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\n# split groups into train/val\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# define encoder\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\nmodel = CodeEncoder(len(stoi), 64, 64).to(device)\noptimizer = Adam(model.parameters(), lr=1e-3)\nloss_fn = nn.TripletMarginLoss(margin=1.0)\n\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n    # training\n    model.train()\n    train_loss = 0.0\n    for a, p, n in train_loader:\n        a, p, n = a.to(device), p.to(device), n.to(device)\n        emb_a = model(a)\n        emb_p = model(p)\n        emb_n = model(n)\n        loss = loss_fn(emb_a, emb_p, emb_n)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    train_loss /= len(train_loader)\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(train_loss)\n    # validation loss\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for a, p, n in val_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a = model(a)\n            emb_p = model(p)\n            emb_n = model(n)\n            val_loss += loss_fn(emb_a, emb_p, emb_n).item()\n    val_loss /= len(val_loader)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(val_loss)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n    # retrieval accuracy on train and val\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n\n        def compute_acc(indices):\n            correct = 0\n            for i in indices:\n                sim = sims[i].clone()\n                sim[i] = -float(\"inf\")\n                top1 = torch.argmax(sim).item()\n                if index_to_gid[top1] == index_to_gid[i]:\n                    correct += 1\n            return correct / len(indices) if indices else 0\n\n        train_acc = compute_acc(train_indices)\n        val_acc = compute_acc(val_indices)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(train_acc)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(val_acc)\n\n# store final predictions and ground truth for val\nwith torch.no_grad():\n    emb_all = model(encoded.to(device))\n    emb_norm = F.normalize(emb_all, dim=1)\n    sims = emb_norm @ emb_norm.T\n    for i in val_indices:\n        sim = sims[i].clone()\n        sim[i] = -float(\"inf\")\n        top1 = torch.argmax(sim).item()\n        experiment_data[\"synthetic\"][\"predictions\"].append(index_to_gid[top1])\n        experiment_data[\"synthetic\"][\"ground_truth\"].append(index_to_gid[i])\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Top-1 Trace-Equivalence Retrieval Accuracy: {val_acc:.4f}\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic code snippets\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\n\n# generate dynamic traces\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: ids for gid, ids in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# encode as char tokens\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\n# split groups into train/val\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# define encoder\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\nmodel = CodeEncoder(len(stoi), 64, 64).to(device)\noptimizer = Adam(model.parameters(), lr=1e-3)\nloss_fn = nn.TripletMarginLoss(margin=1.0)\n\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n    # training\n    model.train()\n    train_loss = 0.0\n    for a, p, n in train_loader:\n        a, p, n = a.to(device), p.to(device), n.to(device)\n        emb_a = model(a)\n        emb_p = model(p)\n        emb_n = model(n)\n        loss = loss_fn(emb_a, emb_p, emb_n)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    train_loss /= len(train_loader)\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(train_loss)\n    # validation loss\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for a, p, n in val_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a = model(a)\n            emb_p = model(p)\n            emb_n = model(n)\n            val_loss += loss_fn(emb_a, emb_p, emb_n).item()\n    val_loss /= len(val_loader)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(val_loss)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n    # retrieval accuracy on train and val\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n\n        def compute_acc(indices):\n            correct = 0\n            for i in indices:\n                sim = sims[i].clone()\n                sim[i] = -float(\"inf\")\n                top1 = torch.argmax(sim).item()\n                if index_to_gid[top1] == index_to_gid[i]:\n                    correct += 1\n            return correct / len(indices) if indices else 0\n\n        train_acc = compute_acc(train_indices)\n        val_acc = compute_acc(val_indices)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(train_acc)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(val_acc)\n\n# store final predictions and ground truth for val\nwith torch.no_grad():\n    emb_all = model(encoded.to(device))\n    emb_norm = F.normalize(emb_all, dim=1)\n    sims = emb_norm @ emb_norm.T\n    for i in val_indices:\n        sim = sims[i].clone()\n        sim[i] = -float(\"inf\")\n        top1 = torch.argmax(sim).item()\n        experiment_data[\"synthetic\"][\"predictions\"].append(index_to_gid[top1])\n        experiment_data[\"synthetic\"][\"ground_truth\"].append(index_to_gid[i])\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Top-1 Trace-Equivalence Retrieval Accuracy: {val_acc:.4f}\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic code snippets\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\n\n# generate dynamic traces\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: ids for gid, ids in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# encode as char tokens\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\n# split groups into train/val\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# define encoder\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\nmodel = CodeEncoder(len(stoi), 64, 64).to(device)\noptimizer = Adam(model.parameters(), lr=1e-3)\nloss_fn = nn.TripletMarginLoss(margin=1.0)\n\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n    # training\n    model.train()\n    train_loss = 0.0\n    for a, p, n in train_loader:\n        a, p, n = a.to(device), p.to(device), n.to(device)\n        emb_a = model(a)\n        emb_p = model(p)\n        emb_n = model(n)\n        loss = loss_fn(emb_a, emb_p, emb_n)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    train_loss /= len(train_loader)\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(train_loss)\n    # validation loss\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for a, p, n in val_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a = model(a)\n            emb_p = model(p)\n            emb_n = model(n)\n            val_loss += loss_fn(emb_a, emb_p, emb_n).item()\n    val_loss /= len(val_loader)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(val_loss)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n    # retrieval accuracy on train and val\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n\n        def compute_acc(indices):\n            correct = 0\n            for i in indices:\n                sim = sims[i].clone()\n                sim[i] = -float(\"inf\")\n                top1 = torch.argmax(sim).item()\n                if index_to_gid[top1] == index_to_gid[i]:\n                    correct += 1\n            return correct / len(indices) if indices else 0\n\n        train_acc = compute_acc(train_indices)\n        val_acc = compute_acc(val_indices)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(train_acc)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(val_acc)\n\n# store final predictions and ground truth for val\nwith torch.no_grad():\n    emb_all = model(encoded.to(device))\n    emb_norm = F.normalize(emb_all, dim=1)\n    sims = emb_norm @ emb_norm.T\n    for i in val_indices:\n        sim = sims[i].clone()\n        sim[i] = -float(\"inf\")\n        top1 = torch.argmax(sim).item()\n        experiment_data[\"synthetic\"][\"predictions\"].append(index_to_gid[top1])\n        experiment_data[\"synthetic\"][\"ground_truth\"].append(index_to_gid[i])\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Top-1 Trace-Equivalence Retrieval Accuracy: {val_acc:.4f}\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 0.9377, Top-1 retrieval\naccuracy = 0.7778', '\\n', 'Epoch 2: validation_loss = 0.9177, Top-1 retrieval\naccuracy = 0.7778', '\\n', 'Epoch 3: validation_loss = 0.8911, Top-1 retrieval\naccuracy = 0.7778', '\\n', 'Epoch 4: validation_loss = 0.8583, Top-1 retrieval\naccuracy = 0.7778', '\\n', 'Epoch 5: validation_loss = 0.8153, Top-1 retrieval\naccuracy = 0.7778', '\\n', 'Epoch 6: validation_loss = 0.7618, Top-1 retrieval\naccuracy = 0.7778', '\\n', 'Epoch 7: validation_loss = 0.6960, Top-1 retrieval\naccuracy = 0.8333', '\\n', 'Epoch 8: validation_loss = 0.6150, Top-1 retrieval\naccuracy = 0.8333', '\\n', 'Epoch 9: validation_loss = 0.5166, Top-1 retrieval\naccuracy = 0.8333', '\\n', 'Epoch 10: validation_loss = 0.4203, Top-1 retrieval\naccuracy = 0.8333', '\\n', 'Execution time: 2 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 0.1448, retrieval_acc =\n1.0000', '\\n', 'Epoch 2: validation_loss = 0.0000, retrieval_acc = 1.0000',\n'\\n', 'Epoch 3: validation_loss = 0.0879, retrieval_acc = 1.0000', '\\n', 'Epoch\n4: validation_loss = 0.0155, retrieval_acc = 1.0000', '\\n', 'Epoch 5:\nvalidation_loss = 0.0000, retrieval_acc = 1.0000', '\\n', 'Epoch 6:\nvalidation_loss = 0.0000, retrieval_acc = 1.0000', '\\n', 'Epoch 7:\nvalidation_loss = 0.0000, retrieval_acc = 1.0000', '\\n', 'Epoch 8:\nvalidation_loss = 0.0454, retrieval_acc = 1.0000', '\\n', 'Epoch 9:\nvalidation_loss = 0.0614, retrieval_acc = 1.0000', '\\n', 'Epoch 10:\nvalidation_loss = 0.0000, retrieval_acc = 1.0000', '\\n', 'Execution time: 2\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 0.9259', '\\n', 'Epoch\n1: validation_loss = 0.8038', '\\n', 'Epoch 2: validation_loss = 0.7574', '\\n',\n'Epoch 3: validation_loss = 0.6785', '\\n', 'Epoch 4: validation_loss = 0.6791',\n'\\n', 'Epoch 5: validation_loss = 0.6019', '\\n', 'Epoch 6: validation_loss =\n0.6162', '\\n', 'Epoch 7: validation_loss = 0.3668', '\\n', 'Epoch 8:\nvalidation_loss = 0.1523', '\\n', 'Epoch 9: validation_loss = 0.2121', '\\n',\n'Top-1 Trace-Equivalence Retrieval Accuracy: 1.0000', '\\n', 'Execution time: 2\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 1.0275', '\\n', 'Epoch\n1: validation_loss = 0.9309', '\\n', 'Epoch 2: validation_loss = 0.9957', '\\n',\n'Epoch 3: validation_loss = 0.9464', '\\n', 'Epoch 4: validation_loss = 0.8584',\n'\\n', 'Epoch 5: validation_loss = 0.8299', '\\n', 'Epoch 6: validation_loss =\n0.6945', '\\n', 'Epoch 7: validation_loss = 0.4956', '\\n', 'Epoch 8:\nvalidation_loss = 0.5143', '\\n', 'Epoch 9: validation_loss = 0.5151', '\\n',\n'Top-1 Trace-Equivalence Retrieval Accuracy: 0.7500', '\\n', 'Execution time: 2\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 0.6533', '\\n', 'Epoch\n1: validation_loss = 0.8356', '\\n', 'Epoch 2: validation_loss = 0.6893', '\\n',\n'Epoch 3: validation_loss = 0.5608', '\\n', 'Epoch 4: validation_loss = 0.7791',\n'\\n', 'Epoch 5: validation_loss = 0.5170', '\\n', 'Epoch 6: validation_loss =\n0.6539', '\\n', 'Epoch 7: validation_loss = 0.4873', '\\n', 'Epoch 8:\nvalidation_loss = 0.3796', '\\n', 'Epoch 9: validation_loss = 0.2513', '\\n',\n'Top-1 Trace-Equivalence Retrieval Accuracy: 1.0000', '\\n', 'Execution time: 2\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 0.8554', '\\n', 'Epoch\n1: validation_loss = 0.8122', '\\n', 'Epoch 2: validation_loss = 0.7088', '\\n',\n'Epoch 3: validation_loss = 0.8413', '\\n', 'Epoch 4: validation_loss = 0.7214',\n'\\n', 'Epoch 5: validation_loss = 0.5332', '\\n', 'Epoch 6: validation_loss =\n0.3991', '\\n', 'Epoch 7: validation_loss = 0.3836', '\\n', 'Epoch 8:\nvalidation_loss = 0.3590', '\\n', 'Epoch 9: validation_loss = 0.2506', '\\n',\n'Top-1 Trace-Equivalence Retrieval Accuracy: 1.0000', '\\n', 'Execution time: 2\nseconds seconds (time limit is an hour).']", ""], "analysis": ["", "", "", "", "", "", ""], "exc_type": [null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the synthetic training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.8333, "best_value": 0.8333}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the synthetic validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.8333, "best_value": 0.8333}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the synthetic training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.4919, "best_value": 0.4919}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the synthetic validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.4203, "best_value": 0.4203}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset", "data": [{"dataset_name": "trace_dataset", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset", "data": [{"dataset_name": "trace_dataset", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "trace_dataset", "final_value": 0.1193, "best_value": 0.1193}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "trace_dataset", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test accuracy on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.1601, "best_value": 0.1601}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.2121, "best_value": 0.2121}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.75, "best_value": 0.75}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.75, "best_value": 0.75}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.2674, "best_value": 0.2674}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.5151, "best_value": 0.5151}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset.", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset.", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Accuracy on the test dataset.", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset.", "data": [{"dataset_name": "synthetic", "final_value": 0.1998, "best_value": 0.1998}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset.", "data": [{"dataset_name": "synthetic", "final_value": 0.2513, "best_value": 0.2513}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training set accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation set accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test set accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training set loss", "data": [{"dataset_name": "synthetic", "final_value": 0.1324, "best_value": 0.1324}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation set loss", "data": [{"dataset_name": "synthetic", "final_value": 0.2506, "best_value": 0.2506}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, true, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_0337a5d77c6d4eccbe6147dfd6c195f6_proc_382491/synthetic_loss.png", "../../logs/0-run/experiment_results/experiment_0337a5d77c6d4eccbe6147dfd6c195f6_proc_382491/synthetic_retrieval_accuracy.png"], ["../../logs/0-run/experiment_results/experiment_63f223fd2abe4199b548b5b24232e550_proc_382492/trace_dataset_loss_curves.png", "../../logs/0-run/experiment_results/experiment_63f223fd2abe4199b548b5b24232e550_proc_382492/trace_dataset_retrieval_accuracy.png"], ["../../logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_loss_curve.png"], ["../../logs/0-run/experiment_results/experiment_70241602ee6f4d26a04db7350577e588_proc_382491/synthetic_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_70241602ee6f4d26a04db7350577e588_proc_382491/synthetic_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_70241602ee6f4d26a04db7350577e588_proc_382491/synthetic_loss_curve.png"], ["../../logs/0-run/experiment_results/experiment_aa4c7b01b0f64f9d869be6c4211da64b_proc_382492/synthetic_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_aa4c7b01b0f64f9d869be6c4211da64b_proc_382492/synthetic_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_aa4c7b01b0f64f9d869be6c4211da64b_proc_382492/synthetic_loss_curve.png"], ["../../logs/0-run/experiment_results/experiment_225a468a0a3c4355adc54655885e19e0_proc_382493/synthetic_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_225a468a0a3c4355adc54655885e19e0_proc_382493/synthetic_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_225a468a0a3c4355adc54655885e19e0_proc_382493/synthetic_loss_curve.png"], ["../../logs/0-run/experiment_results/seed_aggregation_8505715e1e99473181b676fb98a20580/synthetic_loss_curve_mean_sem.png", "../../logs/0-run/experiment_results/seed_aggregation_8505715e1e99473181b676fb98a20580/synthetic_accuracy_curve_mean_sem.png"]], "plot_paths": [["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_0337a5d77c6d4eccbe6147dfd6c195f6_proc_382491/synthetic_loss.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_0337a5d77c6d4eccbe6147dfd6c195f6_proc_382491/synthetic_retrieval_accuracy.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_63f223fd2abe4199b548b5b24232e550_proc_382492/trace_dataset_loss_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_63f223fd2abe4199b548b5b24232e550_proc_382492/trace_dataset_retrieval_accuracy.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_accuracy_curve.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_confusion_matrix.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_loss_curve.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_70241602ee6f4d26a04db7350577e588_proc_382491/synthetic_accuracy_curve.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_70241602ee6f4d26a04db7350577e588_proc_382491/synthetic_confusion_matrix.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_70241602ee6f4d26a04db7350577e588_proc_382491/synthetic_loss_curve.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_aa4c7b01b0f64f9d869be6c4211da64b_proc_382492/synthetic_accuracy_curve.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_aa4c7b01b0f64f9d869be6c4211da64b_proc_382492/synthetic_confusion_matrix.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_aa4c7b01b0f64f9d869be6c4211da64b_proc_382492/synthetic_loss_curve.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_225a468a0a3c4355adc54655885e19e0_proc_382493/synthetic_accuracy_curve.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_225a468a0a3c4355adc54655885e19e0_proc_382493/synthetic_confusion_matrix.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_225a468a0a3c4355adc54655885e19e0_proc_382493/synthetic_loss_curve.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/seed_aggregation_8505715e1e99473181b676fb98a20580/synthetic_loss_curve_mean_sem.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/seed_aggregation_8505715e1e99473181b676fb98a20580/synthetic_accuracy_curve_mean_sem.png"]], "plot_analyses": [[{"analysis": "Loss decreases steadily for both training and validation splits, starting around 0.95 and 0.93 respectively at epoch 1 and falling to roughly 0.49 (training) and 0.42 (validation) by epoch 10. The close tracking of curves with validation loss even dipping slightly below training loss suggests a well\u2010regularized model and stable optimization. There is no obvious overfitting or divergence; instead, the model continues to make incremental gains throughout 10 epochs. The downward curvature remains roughly uniform, indicating that further epochs would likely yield additional, though diminishing, improvements in loss.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_0337a5d77c6d4eccbe6147dfd6c195f6_proc_382491/synthetic_loss.png"}, {"analysis": "Retrieval accuracy on both training and validation starts at ~66.7% and ~77.8% respectively. By epoch 2, training accuracy jumps to the validation level (~77.8%) and remains flat until epoch 6. At epoch 7 validation accuracy climbs to ~83.3%, followed by training reaching the same plateau at epoch 8. Both curves then saturate at ~83.3% through epoch 10. The early plateau of accuracy\u2014despite continued loss reduction\u2014indicates that embedding space distances have stabilized enough for most retrieval tasks, but finer loss improvements do not translate into retrieval gains. This suggests the synthetic data may be too simplistic or that the contrastive setup has already captured its signal.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_0337a5d77c6d4eccbe6147dfd6c195f6_proc_382491/synthetic_retrieval_accuracy.png"}], [{"analysis": "Trace Dataset Loss Curves (Training and Validation Loss): Training loss begins around 0.84 at epoch 1 and drops sharply to about 0.18 by epoch 2, then nearly zeros out by epoch 3. Afterward, it fluctuates closely around zero (minor spikes at epochs 5 and 10). Validation loss mirrors this trend: starting at 0.15, dipping to near zero by epoch 2, a small bump at epoch 3 (~0.08), and otherwise hovering around zero with slight increases around epochs 8\u201310. Such rapid convergence to near-zero loss and very small fluctuations afterward suggest the model quickly fits the data\u2014possibly too easily\u2014raising concerns about triviality of the task or overfitting, especially given the minor late-epoch noise.", "} ,{": ": ", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_63f223fd2abe4199b548b5b24232e550_proc_382492/trace_dataset_loss_curves.png"}, {"analysis": "Trace Dataset Retrieval Accuracy (Training and Validation Accuracy): Both training and validation accuracy sit exactly at 100% across all epochs. This perfect, flat line indicates the model is trivially separating positive and negative trace pairs with zero error. While ideal on paper, it likely reflects an overly simple setup (e.g., extremely easy negatives, data leakage, or very small dataset) rather than robust generalization. It signals a need for harder negative mining, more diverse code variants, or larger-scale data to properly challenge the contrastive objective.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_63f223fd2abe4199b548b5b24232e550_proc_382492/trace_dataset_retrieval_accuracy.png"}], [{"analysis": "Train accuracy starts below chance and steadily climbs to perfect (1.0) by epoch 7. Validation accuracy, however, remains at chance (0.5) until epoch 7, then abruptly jumps to perfect by epoch 8. Such a sudden leap suggests an issue in the evaluation pipeline (e.g., data leakage or mislabeled validation batches) rather than genuine learning of runtime semantics. The plateau at chance for seven epochs indicates the model wasn\u2019t capturing useful patterns until an unexpected event\u2014likely code or data handling bug\u2014triggered perfect scores.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_accuracy_curve.png"}, {"analysis": "The confusion matrix on the synthetic dataset shows nonzero counts only for True Group IDs 6 and 7 (two correct predictions for class 6, one for class 7) with all other classes absent. This implies the model only ever outputs two of the eight possible classes, effectively ignoring six classes completely. The mismatch between the observed perfect validation accuracy and this highly skewed confusion matrix highlights an evaluation or indexing error\u2014labels or predictions are likely being remapped or filtered incorrectly.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_confusion_matrix.png"}, {"analysis": "Loss curves decrease smoothly from around 0.95 (train) and 0.93 (val) to below 0.2. A minor bump around epoch 5 suggests transient optimization instability, but overall the loss drop is consistent with overfitting on a tiny synthetic set. Combined with the accuracy and confusion results, the loss dynamics further point to correct gradient flow but flawed validation logic causing misleading metrics.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_loss_curve.png"}], [{"analysis": "Accuracy curves show that the model\u2019s training accuracy remains at 50% for the first three epochs, then climbs steeply to 80% at epoch 5 and reaches 100% by epoch 6, staying there through epoch 10. Validation accuracy, by contrast, stays flat at 50% until epoch 8 before jumping to 75% at epochs 9 and 10. This suggests the model quickly memorizes the synthetic data but generalizes poorly until late in training.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_70241602ee6f4d26a04db7350577e588_proc_382491/synthetic_accuracy_curve.png"}, {"analysis": "Confusion matrix reveals only two true classes with nonzero counts. All samples of class 6 are correctly predicted (perfect precision and recall for this class). True class 9 has one sample misclassified as class 0 and one correctly classified as class 9, indicating some confusion and a trivial dataset structure that may be too simplistic.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_70241602ee6f4d26a04db7350577e588_proc_382491/synthetic_confusion_matrix.png"}, {"analysis": "Loss curves mirror the accuracy trends. Training loss starts around 0.8, slightly rises early on, then steadily falls to 0.26 by epoch 10. Validation loss starts above 1.0, fluctuates modestly through epoch 3, then declines to about 0.5 by epoch 8, with a minor bump around epoch 9. The widening gap between train and val loss until epoch 8 confirms overfitting, though val loss does improve later.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_70241602ee6f4d26a04db7350577e588_proc_382491/synthetic_loss_curve.png"}], [{"analysis": "Accuracy curves show steady improvement in training accuracy from around 45% at epoch 1 to 100% by epoch 9\u201310. Validation accuracy remains at roughly 50% through epoch 5, then jumps to 60% at epoch 6 and reaches 100% by epoch 7, thereafter matching training accuracy. This pattern suggests initial underfitting on the synthetic data, followed by rapid generalization once the model captures key dynamic\u2010trace features. The sudden validation jump may also hint at dataset simplicity or memorization rather than gradual learning.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_aa4c7b01b0f64f9d869be6c4211da64b_proc_382492/synthetic_accuracy_curve.png"}, {"analysis": "Confusion matrix reveals perfect classification for the two present classes (group IDs 1 and 2), with two true samples each and zero off\u2010diagonal predictions. Every instance is correctly placed in its true group, indicating clear separability of semantic behaviors in this synthetic scenario and confirming that the learned embeddings effectively differentiate trace\u2010equivalent and trace\u2010dissimilar code variants here.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_aa4c7b01b0f64f9d869be6c4211da64b_proc_382492/synthetic_confusion_matrix.png"}, {"analysis": "Loss curves display an overall downward trend in both training and validation loss\u2014from ~0.92 to ~0.22 on the training side, and from ~0.65 to ~0.25 on validation by epoch 10\u2014but with noticeable spikes at epochs 2, 4, 5, and 7. These fluctuations could stem from learning\u2010rate settings or stochastic batch effects. The general decline confirms learning progress, but smoothing out these oscillations through hyperparameter tuning could yield more stable convergence.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_aa4c7b01b0f64f9d869be6c4211da64b_proc_382492/synthetic_loss_curve.png"}], [{"analysis": "Accuracy curves image: training accuracy stays at around 50% for the first three epochs before climbing steadily from epoch 4, reaching 100% by epoch 7; validation accuracy remains at chance until epoch 5, then jumps to 75% and also hits 100% by epoch 7. The one-epoch lag of validation accuracy behind training suggests minimal overfitting and smooth convergence on this toy dataset.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_225a468a0a3c4355adc54655885e19e0_proc_382493/synthetic_accuracy_curve.png"}, {"analysis": "Confusion matrix image: both classes are perfectly separated\u2014each true class has two correct predictions on the diagonal and zero off-diagonal entries. This indicates flawless classification performance on the synthetic examples provided.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_225a468a0a3c4355adc54655885e19e0_proc_382493/synthetic_confusion_matrix.png"}, {"analysis": "Loss curves image: training loss decreases from about 1.05 down to 0.15 over ten epochs, with a minor uptick at epoch 3 but otherwise a steady decline; validation loss starts around 0.85, fluctuates slightly at epochs 2\u20134, then consistently drops to roughly 0.25 by epoch 10. The small gap between training and validation loss points to stable generalization without pronounced overfitting.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_225a468a0a3c4355adc54655885e19e0_proc_382493/synthetic_loss_curve.png"}], []], "vlm_feedback_summary": ["The loss curves indicate robust and stable convergence without overfitting.\nRetrieval accuracy saturates early at ~83% by epoch 8, even as loss continues to\ndecrease. This decoupling between loss and task metric suggests the current\nsynthetic dataset may be too easy or lacks diversity to push the model further;\nconsider using a more challenging benchmark or adjusting the contrastive\nsampling strategy to drive accuracy improvements in later epochs.", "Model converges too quickly on simple preliminary dataset; perfect retrieval\naccuracy suggests trivial task. Recommend introducing harder negatives, scaling\nup dataset complexity, and adding regularization or variance to dynamic traces\nto properly evaluate the added value of runtime behavior.", "Results show steady training loss reduction and perfect final train/val scores,\nbut validation accuracy remains at chance for many epochs and then spikes,\ncoupled with a confusion matrix that only covers two of eight classes. This\nstrongly indicates bugs in the evaluation, label mapping, or data splitting\nrather than true model performance. Recommend auditing the data loader and\nevaluation code for misindexed labels, ensuring no train\u2013val leakage, and\nrerunning with a properly shuffled synthetic dataset.", "The model overfits quickly on the synthetic dataset, achieving perfect training\naccuracy early while validation performance lags. The confusion matrix\nhighlights class imbalance or dataset triviality. To improve robustness, expand\nthe dataset diversity, incorporate stronger regularization or dropout, and\nvalidate on more realistic code benchmarks. Experiment with varying the number\nand complexity of dynamic traces, augment negative pairs, and tune the\ncontrastive loss weight to boost generalization.", "Model shows strong performance on this toy dataset, achieving perfect accuracy\nand loss convergence. Early stagnation in validation and intermittent loss\nspikes suggest hyperparameter tuning and more challenging data will be crucial\nnext steps to verify robustness beyond synthetic settings.", "Model establishes basic capability on synthetic data, achieving perfect accuracy\nand low loss; loss trajectories show stable convergence with minor early-epoch\nnoise; confusion matrix confirms zero misclassifications, indicating that more\nchallenging datasets should be introduced to further test dynamic-trace\ncontrastive learning.", "[]"], "exec_time": [2.1047627925872803, 2.343501091003418, 2.0860533714294434, 2.2379610538482666, 2.203232765197754, 2.1893136501312256, null], "exec_time_feedback": ["", "", "", "", "", "", ""], "datasets_successfully_tested": [["['synthetic']"], ["[]"], [""], [""], ["['synthetic']"], ["[\"synthetic\"]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot retrieval accuracy\ntry:\n    train_metrics = experiment_data[\"synthetic\"][\"metrics\"][\"train\"]\n    val_metrics = experiment_data[\"synthetic\"][\"metrics\"][\"val\"]\n    epochs = [m[\"epoch\"] for m in train_metrics]\n    acc_train = [m[\"retrieval_accuracy\"] for m in train_metrics]\n    acc_val = [m[\"retrieval_accuracy\"] for m in val_metrics]\n\n    plt.figure()\n    plt.plot(epochs, acc_train, marker=\"o\", label=\"Training\")\n    plt.plot(epochs, acc_val, marker=\"o\", label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Retrieval Accuracy\")\n    plt.title(\"Retrieval Accuracy over Epochs on synthetic dataset\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_retrieval_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating retrieval accuracy plot: {e}\")\n    plt.close()\n\n# Plot loss curves\ntry:\n    loss_train = experiment_data[\"synthetic\"][\"losses\"][\"train\"]\n    loss_val = experiment_data[\"synthetic\"][\"losses\"][\"val\"]\n    epochs = [m[\"epoch\"] for m in experiment_data[\"synthetic\"][\"metrics\"][\"train\"]]\n\n    plt.figure()\n    plt.plot(epochs, loss_train, marker=\"o\", label=\"Training\")\n    plt.plot(epochs, loss_val, marker=\"o\", label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss over Epochs on synthetic dataset\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    data_path = os.path.join(working_dir, \"experiment_data.npy\")\n    experiment_data = np.load(data_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot loss curves\ntry:\n    losses = experiment_data[\"trace_dataset\"][\"losses\"]\n    epochs = list(range(1, len(losses[\"train\"]) + 1))\n    plt.figure()\n    plt.plot(epochs, losses[\"train\"], label=\"Train Loss\", color=\"blue\")\n    plt.plot(epochs, losses[\"val\"], label=\"Val Loss\", color=\"orange\")\n    plt.suptitle(\"Trace Dataset Loss Curves\")\n    plt.title(\"Left: Training Loss, Right: Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"trace_dataset_loss_curves.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Plot retrieval accuracy\ntry:\n    metrics = experiment_data[\"trace_dataset\"][\"metrics\"]\n    epochs = list(range(1, len(metrics[\"train\"]) + 1))\n    plt.figure()\n    plt.plot(epochs, metrics[\"train\"], label=\"Train Acc\", color=\"blue\")\n    plt.plot(epochs, metrics[\"val\"], label=\"Val Acc\", color=\"orange\")\n    plt.suptitle(\"Trace Dataset Retrieval Accuracy\")\n    plt.title(\"Left: Training Accuracy, Right: Validation Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    fname = os.path.join(working_dir, \"trace_dataset_retrieval_accuracy.png\")\n    plt.savefig(fname)\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot loss curves\ntry:\n    losses = experiment_data[\"synthetic\"][\"losses\"]\n    epochs = np.arange(1, len(losses[\"train\"]) + 1)\n    plt.figure()\n    plt.plot(epochs, losses[\"train\"], label=\"Train Loss\")\n    plt.plot(epochs, losses[\"val\"], label=\"Val Loss\")\n    plt.title(\"Loss Curves for synthetic dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# Plot accuracy curves\ntry:\n    acc = experiment_data[\"synthetic\"][\"metrics\"]\n    epochs = np.arange(1, len(acc[\"train\"]) + 1)\n    plt.figure()\n    plt.plot(epochs, acc[\"train\"], label=\"Train Acc\")\n    plt.plot(epochs, acc[\"val\"], label=\"Val Acc\")\n    plt.title(\"Accuracy Curves for synthetic dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Retrieval Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve plot: {e}\")\n    plt.close()\n\n# Plot confusion matrix\ntry:\n    preds = np.array(experiment_data[\"synthetic\"][\"predictions\"])\n    truths = np.array(experiment_data[\"synthetic\"][\"ground_truth\"])\n    num_classes = int(max(preds.max(), truths.max()) + 1)\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for t, p in zip(truths, preds):\n        cm[t, p] += 1\n    plt.figure()\n    plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n    plt.title(\"Confusion Matrix for synthetic dataset\")\n    plt.xlabel(\"Predicted Group ID\")\n    plt.ylabel(\"True Group ID\")\n    plt.colorbar()\n    plt.savefig(os.path.join(working_dir, \"synthetic_confusion_matrix.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot loss curves\ntry:\n    losses = experiment_data[\"synthetic\"][\"losses\"]\n    epochs = np.arange(1, len(losses[\"train\"]) + 1)\n    plt.figure()\n    plt.plot(epochs, losses[\"train\"], label=\"Train Loss\")\n    plt.plot(epochs, losses[\"val\"], label=\"Val Loss\")\n    plt.title(\"Loss Curves for synthetic dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# Plot accuracy curves\ntry:\n    acc = experiment_data[\"synthetic\"][\"metrics\"]\n    epochs = np.arange(1, len(acc[\"train\"]) + 1)\n    plt.figure()\n    plt.plot(epochs, acc[\"train\"], label=\"Train Acc\")\n    plt.plot(epochs, acc[\"val\"], label=\"Val Acc\")\n    plt.title(\"Accuracy Curves for synthetic dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Retrieval Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve plot: {e}\")\n    plt.close()\n\n# Plot confusion matrix\ntry:\n    preds = np.array(experiment_data[\"synthetic\"][\"predictions\"])\n    truths = np.array(experiment_data[\"synthetic\"][\"ground_truth\"])\n    num_classes = int(max(preds.max(), truths.max()) + 1)\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for t, p in zip(truths, preds):\n        cm[t, p] += 1\n    plt.figure()\n    plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n    plt.title(\"Confusion Matrix for synthetic dataset\")\n    plt.xlabel(\"Predicted Group ID\")\n    plt.ylabel(\"True Group ID\")\n    plt.colorbar()\n    plt.savefig(os.path.join(working_dir, \"synthetic_confusion_matrix.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot loss curves\ntry:\n    losses = experiment_data[\"synthetic\"][\"losses\"]\n    epochs = np.arange(1, len(losses[\"train\"]) + 1)\n    plt.figure()\n    plt.plot(epochs, losses[\"train\"], label=\"Train Loss\")\n    plt.plot(epochs, losses[\"val\"], label=\"Val Loss\")\n    plt.title(\"Loss Curves for synthetic dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# Plot accuracy curves\ntry:\n    acc = experiment_data[\"synthetic\"][\"metrics\"]\n    epochs = np.arange(1, len(acc[\"train\"]) + 1)\n    plt.figure()\n    plt.plot(epochs, acc[\"train\"], label=\"Train Acc\")\n    plt.plot(epochs, acc[\"val\"], label=\"Val Acc\")\n    plt.title(\"Accuracy Curves for synthetic dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Retrieval Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve plot: {e}\")\n    plt.close()\n\n# Plot confusion matrix\ntry:\n    preds = np.array(experiment_data[\"synthetic\"][\"predictions\"])\n    truths = np.array(experiment_data[\"synthetic\"][\"ground_truth\"])\n    num_classes = int(max(preds.max(), truths.max()) + 1)\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for t, p in zip(truths, preds):\n        cm[t, p] += 1\n    plt.figure()\n    plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n    plt.title(\"Confusion Matrix for synthetic dataset\")\n    plt.xlabel(\"Predicted Group ID\")\n    plt.ylabel(\"True Group ID\")\n    plt.colorbar()\n    plt.savefig(os.path.join(working_dir, \"synthetic_confusion_matrix.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot loss curves\ntry:\n    losses = experiment_data[\"synthetic\"][\"losses\"]\n    epochs = np.arange(1, len(losses[\"train\"]) + 1)\n    plt.figure()\n    plt.plot(epochs, losses[\"train\"], label=\"Train Loss\")\n    plt.plot(epochs, losses[\"val\"], label=\"Val Loss\")\n    plt.title(\"Loss Curves for synthetic dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# Plot accuracy curves\ntry:\n    acc = experiment_data[\"synthetic\"][\"metrics\"]\n    epochs = np.arange(1, len(acc[\"train\"]) + 1)\n    plt.figure()\n    plt.plot(epochs, acc[\"train\"], label=\"Train Acc\")\n    plt.plot(epochs, acc[\"val\"], label=\"Val Acc\")\n    plt.title(\"Accuracy Curves for synthetic dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Retrieval Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve plot: {e}\")\n    plt.close()\n\n# Plot confusion matrix\ntry:\n    preds = np.array(experiment_data[\"synthetic\"][\"predictions\"])\n    truths = np.array(experiment_data[\"synthetic\"][\"ground_truth\"])\n    num_classes = int(max(preds.max(), truths.max()) + 1)\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for t, p in zip(truths, preds):\n        cm[t, p] += 1\n    plt.figure()\n    plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n    plt.title(\"Confusion Matrix for synthetic dataset\")\n    plt.xlabel(\"Predicted Group ID\")\n    plt.ylabel(\"True Group ID\")\n    plt.colorbar()\n    plt.savefig(os.path.join(working_dir, \"synthetic_confusion_matrix.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load all experiment data\ntry:\n    experiment_data_path_list = [\n        \"experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_70241602ee6f4d26a04db7350577e588_proc_382491/experiment_data.npy\",\n        \"experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_aa4c7b01b0f64f9d869be6c4211da64b_proc_382492/experiment_data.npy\",\n        \"experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_225a468a0a3c4355adc54655885e19e0_proc_382493/experiment_data.npy\",\n    ]\n    all_experiment_data = []\n    for rel_path in experiment_data_path_list:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\"), rel_path)\n        data = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(data)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Determine datasets\ndataset_names = all_experiment_data[0].keys() if all_experiment_data else []\n\n# Aggregate and plot loss curves\nfor ds in dataset_names:\n    try:\n        # gather losses\n        runs_train = [\n            np.array(exp[ds][\"losses\"][\"train\"]) for exp in all_experiment_data\n        ]\n        runs_val = [np.array(exp[ds][\"losses\"][\"val\"]) for exp in all_experiment_data]\n        min_epochs = min(arr.shape[0] for arr in runs_train + runs_val)\n        runs_train = np.vstack([arr[:min_epochs] for arr in runs_train])\n        runs_val = np.vstack([arr[:min_epochs] for arr in runs_val])\n        epochs = np.arange(1, min_epochs + 1)\n        mean_train = runs_train.mean(axis=0)\n        sem_train = runs_train.std(axis=0, ddof=1) / np.sqrt(runs_train.shape[0])\n        mean_val = runs_val.mean(axis=0)\n        sem_val = runs_val.std(axis=0, ddof=1) / np.sqrt(runs_val.shape[0])\n        plt.figure()\n        plt.errorbar(\n            epochs, mean_train, yerr=sem_train, label=\"Train Loss Mean \u00b1 SEM\", capsize=3\n        )\n        plt.errorbar(\n            epochs, mean_val, yerr=sem_val, label=\"Val Loss Mean \u00b1 SEM\", capsize=3\n        )\n        plt.title(f\"Loss Curves for {ds} dataset\\nMean \u00b1 SEM over experiments\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ds}_loss_curve_mean_sem.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ds}: {e}\")\n        plt.close()\n\n# Aggregate and plot accuracy curves\nfor ds in dataset_names:\n    try:\n        runs_train_acc = [\n            np.array(exp[ds][\"metrics\"][\"train\"]) for exp in all_experiment_data\n        ]\n        runs_val_acc = [\n            np.array(exp[ds][\"metrics\"][\"val\"]) for exp in all_experiment_data\n        ]\n        min_epochs_acc = min(arr.shape[0] for arr in runs_train_acc + runs_val_acc)\n        runs_train_acc = np.vstack([arr[:min_epochs_acc] for arr in runs_train_acc])\n        runs_val_acc = np.vstack([arr[:min_epochs_acc] for arr in runs_val_acc])\n        epochs_acc = np.arange(1, min_epochs_acc + 1)\n        mean_train_acc = runs_train_acc.mean(axis=0)\n        sem_train_acc = runs_train_acc.std(axis=0, ddof=1) / np.sqrt(\n            runs_train_acc.shape[0]\n        )\n        mean_val_acc = runs_val_acc.mean(axis=0)\n        sem_val_acc = runs_val_acc.std(axis=0, ddof=1) / np.sqrt(runs_val_acc.shape[0])\n        plt.figure()\n        plt.errorbar(\n            epochs_acc,\n            mean_train_acc,\n            yerr=sem_train_acc,\n            label=\"Train Acc Mean \u00b1 SEM\",\n            capsize=3,\n        )\n        plt.errorbar(\n            epochs_acc,\n            mean_val_acc,\n            yerr=sem_val_acc,\n            label=\"Val Acc Mean \u00b1 SEM\",\n            capsize=3,\n        )\n        plt.title(f\"Accuracy Curves for {ds} dataset\\nMean \u00b1 SEM over experiments\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{ds}_accuracy_curve_mean_sem.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot for {ds}: {e}\")\n        plt.close()\n\n# Print final evaluation metrics\nfor ds in dataset_names:\n    try:\n        final_vals = [exp[ds][\"metrics\"][\"val\"][-1] for exp in all_experiment_data]\n        mean_final = np.mean(final_vals)\n        sem_final = np.std(final_vals, ddof=1) / np.sqrt(len(final_vals))\n        print(f\"Dataset {ds}: Final Val Acc = {mean_final:.3f} \u00b1 {sem_final:.3f}\")\n    except Exception as e:\n        print(f\"Error computing final metric for {ds}: {e}\")\n"], "plot_plan": ["I will load the saved experiment data and extract the per\u2010epoch retrieval accuracy and loss for both training and validation on the synthetic dataset. I will then construct two figures: one showing retrieval accuracy curves, and another showing loss curves, each comparing training versus validation across epochs. Each plotting step will be enclosed in its own try\u2010except block to capture and report any errors, and every figure will be closed after saving to release resources. The resulting images will be stored in the working directory with descriptive filenames that include the dataset name and plot type.", "I will load the saved experiment data and then plot both the loss curves and the retrieval accuracy curves for the trace dataset.  First, I define the working directory and attempt to load the NumPy experiment file.  Next, I extract the training and validation losses and plot them against epochs, labeling training in blue and validation in orange.  I add a clear overall title \"Trace Dataset Loss Curves\" and a subtitle \"Left: Training Loss, Right: Validation Loss\".  This figure is saved under a descriptive filename in the working directory inside a try-except block.  Then I repeat the same steps for the retrieval accuracy, titling it \"Trace Dataset Retrieval Accuracy\" with the subtitle \"Left: Training Accuracy, Right: Validation Accuracy\".  Each plot is generated and saved separately, always closing the figure in finally or except clauses to avoid resource leaks.  Any errors during loading or plotting are caught and printed.", "We start by loading the saved experiment data from the working directory using numpy. In the first try\u2010except block we extract training and validation loss lists and plot them together in one figure, labeling axes and saving as \u201csynthetic_loss_curve.png\u201d. The second block similarly plots retrieval accuracy curves for both train and validation sets, saving \u201csynthetic_accuracy_curve.png\u201d. In a third block we compute a confusion matrix between the final ground truth and predicted group IDs, visualize it as a heatmap with true vs predicted axes, and save it as \u201csynthetic_confusion_matrix.png\u201d. Each figure includes a clear title specifying the \u201csynthetic\u201d dataset and appropriate axis labels. We handle any plotting errors gracefully by printing an error message and ensuring figures are always closed. All plots are saved into the same working directory.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null], "hyperparam_name": [null, null, null, null, null, null, null], "is_seed_node": [false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, true], "parse_metrics_plan": ["I will load the saved experiment data from the working directory, then iterate\nover each dataset to pull out the final recorded retrieval accuracies and\nlosses. For each dataset, I will print its name, followed by clearly labeled\n\u201ctrain accuracy,\u201d \u201cvalidation accuracy,\u201d \u201ctrain loss,\u201d and \u201cvalidation loss\u201d\nwith their final values. The script runs at the global scope and does not\nrequire any special entry point.", "I will construct the working directory path and load the saved experiment data\nusing NumPy with pickle support. Then I will iterate over each dataset entry,\nextract the final values of train/validation accuracy and train/validation loss,\nand print them with explicit labels. Each dataset\u2019s name is printed first,\nfollowed by the final metric values. This script runs immediately without\nrequiring any special entry point.", "The script first locates and loads the saved experiment data from the working\ndirectory. It then iterates over each dataset entry, extracting the final epoch\nvalues for training and validation accuracy and loss. If prediction and\nground\u2010truth arrays are present, it computes and prints the overall test\naccuracy. All outputs are clearly labeled and printed directly at the global\nscope.", "The script first locates and loads the saved experiment data from the working\ndirectory. It then iterates over each dataset entry, extracting the final epoch\nvalues for training and validation accuracy and loss. If prediction and\nground\u2010truth arrays are present, it computes and prints the overall test\naccuracy. All outputs are clearly labeled and printed directly at the global\nscope.", "The script first locates and loads the saved experiment data from the working\ndirectory. It then iterates over each dataset entry, extracting the final epoch\nvalues for training and validation accuracy and loss. If prediction and\nground\u2010truth arrays are present, it computes and prints the overall test\naccuracy. All outputs are clearly labeled and printed directly at the global\nscope.", "The script first locates and loads the saved experiment data from the working\ndirectory. It then iterates over each dataset entry, extracting the final epoch\nvalues for training and validation accuracy and loss. If prediction and\nground\u2010truth arrays are present, it computes and prints the overall test\naccuracy. All outputs are clearly labeled and printed directly at the global\nscope.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Locate the working directory and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset in the saved experiment data\nfor dataset_name, results in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n    # Extract metric lists\n    train_metrics = results.get(\"metrics\", {}).get(\"train\", [])\n    val_metrics = results.get(\"metrics\", {}).get(\"val\", [])\n    train_losses = results.get(\"losses\", {}).get(\"train\", [])\n    val_losses = results.get(\"losses\", {}).get(\"val\", [])\n\n    # Safely get the final values if they exist\n    if train_metrics:\n        final_train_acc = train_metrics[-1].get(\"retrieval_accuracy\")\n        print(f\"Train accuracy: {final_train_acc:.4f}\")\n    if val_metrics:\n        final_val_acc = val_metrics[-1].get(\"retrieval_accuracy\")\n        print(f\"Validation accuracy: {final_val_acc:.4f}\")\n    if train_losses:\n        final_train_loss = train_losses[-1]\n        print(f\"Train loss: {final_train_loss:.4f}\")\n    if val_losses:\n        final_val_loss = val_losses[-1]\n        print(f\"Validation loss: {final_val_loss:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Define the working directory and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over each dataset in the experiment data\nfor dataset_name, dataset_info in experiment_data.items():\n    # Extract final metric and loss values\n    final_train_acc = dataset_info[\"metrics\"][\"train\"][-1]\n    final_val_acc = dataset_info[\"metrics\"][\"val\"][-1]\n    final_train_loss = dataset_info[\"losses\"][\"train\"][-1]\n    final_val_loss = dataset_info[\"losses\"][\"val\"][-1]\n\n    # Print dataset name and final metrics with clear labels\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"Final train accuracy: {final_train_acc:.4f}\")\n    print(f\"Final validation accuracy: {final_val_acc:.4f}\")\n    print(f\"Final train loss: {final_train_loss:.4f}\")\n    print(f\"Final validation loss: {final_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, dataset in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n    # Extract and print final accuracies\n    metrics = dataset.get(\"metrics\", {})\n    train_accs = metrics.get(\"train\", [])\n    val_accs = metrics.get(\"val\", [])\n    if train_accs:\n        print(f\"Train accuracy: {train_accs[-1]:.4f}\")\n    if val_accs:\n        print(f\"Validation accuracy: {val_accs[-1]:.4f}\")\n    # Extract and print final losses\n    losses = dataset.get(\"losses\", {})\n    train_losses = losses.get(\"train\", [])\n    val_losses = losses.get(\"val\", [])\n    if train_losses:\n        print(f\"Train loss: {train_losses[-1]:.4f}\")\n    if val_losses:\n        print(f\"Validation loss: {val_losses[-1]:.4f}\")\n    # Compute and print test accuracy if predictions are available\n    preds = dataset.get(\"predictions\", [])\n    gts = dataset.get(\"ground_truth\", [])\n    if preds and gts and len(preds) == len(gts):\n        test_acc = sum(p == g for p, g in zip(preds, gts)) / len(gts)\n        print(f\"Test accuracy: {test_acc:.4f}\")\n    print()  # Blank line between datasets\n", "import os\nimport numpy as np\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, dataset in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n    # Extract and print final accuracies\n    metrics = dataset.get(\"metrics\", {})\n    train_accs = metrics.get(\"train\", [])\n    val_accs = metrics.get(\"val\", [])\n    if train_accs:\n        print(f\"Train accuracy: {train_accs[-1]:.4f}\")\n    if val_accs:\n        print(f\"Validation accuracy: {val_accs[-1]:.4f}\")\n    # Extract and print final losses\n    losses = dataset.get(\"losses\", {})\n    train_losses = losses.get(\"train\", [])\n    val_losses = losses.get(\"val\", [])\n    if train_losses:\n        print(f\"Train loss: {train_losses[-1]:.4f}\")\n    if val_losses:\n        print(f\"Validation loss: {val_losses[-1]:.4f}\")\n    # Compute and print test accuracy if predictions are available\n    preds = dataset.get(\"predictions\", [])\n    gts = dataset.get(\"ground_truth\", [])\n    if preds and gts and len(preds) == len(gts):\n        test_acc = sum(p == g for p, g in zip(preds, gts)) / len(gts)\n        print(f\"Test accuracy: {test_acc:.4f}\")\n    print()  # Blank line between datasets\n", "import os\nimport numpy as np\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, dataset in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n    # Extract and print final accuracies\n    metrics = dataset.get(\"metrics\", {})\n    train_accs = metrics.get(\"train\", [])\n    val_accs = metrics.get(\"val\", [])\n    if train_accs:\n        print(f\"Train accuracy: {train_accs[-1]:.4f}\")\n    if val_accs:\n        print(f\"Validation accuracy: {val_accs[-1]:.4f}\")\n    # Extract and print final losses\n    losses = dataset.get(\"losses\", {})\n    train_losses = losses.get(\"train\", [])\n    val_losses = losses.get(\"val\", [])\n    if train_losses:\n        print(f\"Train loss: {train_losses[-1]:.4f}\")\n    if val_losses:\n        print(f\"Validation loss: {val_losses[-1]:.4f}\")\n    # Compute and print test accuracy if predictions are available\n    preds = dataset.get(\"predictions\", [])\n    gts = dataset.get(\"ground_truth\", [])\n    if preds and gts and len(preds) == len(gts):\n        test_acc = sum(p == g for p, g in zip(preds, gts)) / len(gts)\n        print(f\"Test accuracy: {test_acc:.4f}\")\n    print()  # Blank line between datasets\n", "import os\nimport numpy as np\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, dataset in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n    # Extract and print final accuracies\n    metrics = dataset.get(\"metrics\", {})\n    train_accs = metrics.get(\"train\", [])\n    val_accs = metrics.get(\"val\", [])\n    if train_accs:\n        print(f\"Train accuracy: {train_accs[-1]:.4f}\")\n    if val_accs:\n        print(f\"Validation accuracy: {val_accs[-1]:.4f}\")\n    # Extract and print final losses\n    losses = dataset.get(\"losses\", {})\n    train_losses = losses.get(\"train\", [])\n    val_losses = losses.get(\"val\", [])\n    if train_losses:\n        print(f\"Train loss: {train_losses[-1]:.4f}\")\n    if val_losses:\n        print(f\"Validation loss: {val_losses[-1]:.4f}\")\n    # Compute and print test accuracy if predictions are available\n    preds = dataset.get(\"predictions\", [])\n    gts = dataset.get(\"ground_truth\", [])\n    if preds and gts and len(preds) == len(gts):\n        test_acc = sum(p == g for p, g in zip(preds, gts)) / len(gts)\n        print(f\"Test accuracy: {test_acc:.4f}\")\n    print()  # Blank line between datasets\n", ""], "parse_term_out": ["['Dataset: synthetic', '\\n', 'Train accuracy: 0.8333', '\\n', 'Validation\naccuracy: 0.8333', '\\n', 'Train loss: 0.4919', '\\n', 'Validation loss: 0.4203',\n'\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: trace_dataset', '\\n', 'Final train accuracy: 1.0000', '\\n', 'Final\nvalidation accuracy: 1.0000', '\\n', 'Final train loss: 0.1193', '\\n', 'Final\nvalidation loss: 0.0000', '\\n', 'Execution time: a moment seconds (time limit is\nan hour).']", "['Dataset: synthetic', '\\n', 'Train accuracy: 1.0000', '\\n', 'Validation\naccuracy: 1.0000', '\\n', 'Train loss: 0.1601', '\\n', 'Validation loss: 0.2121',\n'\\n', 'Test accuracy: 1.0000', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Train accuracy: 1.0000', '\\n', 'Validation\naccuracy: 0.7500', '\\n', 'Train loss: 0.2674', '\\n', 'Validation loss: 0.5151',\n'\\n', 'Test accuracy: 0.7500', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Train accuracy: 1.0000', '\\n', 'Validation\naccuracy: 1.0000', '\\n', 'Train loss: 0.1998', '\\n', 'Validation loss: 0.2513',\n'\\n', 'Test accuracy: 1.0000', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Train accuracy: 1.0000', '\\n', 'Validation\naccuracy: 1.0000', '\\n', 'Train loss: 0.1324', '\\n', 'Validation loss: 0.2506',\n'\\n', 'Test accuracy: 1.0000', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null], "completed_stages": ["Stage_1"]}
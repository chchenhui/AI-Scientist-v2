<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 4], [0, 1], [0, 5], [0, 14], [0, 10], [0, 13], [0, 17], [0, 3], [0, 11], [0, 16], [0, 12], [0, 9], [0, 2], [3, 6], [3, 7], [3, 8], [9, 15], [16, 19], [16, 20], [16, 21], [16, 18]], "layout": [[0.48, 0.0], [0.0, 0.5], [0.08, 0.5], [0.16, 0.5], [0.24, 0.5], [0.32, 0.5], [0.08, 1.0], [0.16, 1.0], [0.24, 1.0], [0.4, 0.5], [0.48, 0.5], [0.56, 0.5], [0.64, 0.5], [0.72, 0.5], [0.8, 0.5], [0.4, 1.0], [0.88, 0.5], [0.96, 0.5], [0.76, 1.0], [0.84, 1.0], [0.92, 1.0], [1.0, 1.0]], "plan": ["Hyperparam tuning name: EPOCHS. I will loop over different epoch settings (e.g.,\n10, 30, 50), reinitialize and train the LSTM encoder from scratch for each\nsetting, and record per\u2010epoch losses and retrieval accuracies as well as final\nvalidation predictions. All results are organized under the hyperparameter\ntuning key \u201cEPOCHS\u201d and saved to a single NumPy file for later plotting or\nanalysis.", "Ablation name: Multi\u2010Dataset Synthetic Ablation. We generate three synthetic\ncode distributions\u2014arithmetic, branching, and loops\u2014each with two semantically\nequivalent variants per constant to ensure retrieval groups.  For each dataset,\nwe compute execution traces over a shared input set, group snippets by identical\ntraces, encode them as character sequences, and split into triplet\u2010based\ntrain/validation loaders.  We then train the same LSTM\u2010based encoder for epochs\nin [10,30,50], recording triplet losses and retrieval accuracies at each epoch\nas well as final top\u20101 predictions.  All metrics, losses, predictions, and\nground truths are stored in a unified `experiment_data.npy` under\n`multi_dataset_synthetic_ablation`.", "Ablation name: Negative Sampling Hardness Ablation. We extend the baseline by\ncreating two training regimes: random-negative sampling and semi-hard negative\nmining, where each negative is the closest non-matching snippet in embedding\nspace. For both, we train over multiple epoch counts, recording train/val\nlosses, retrieval accuracies, and final predictions. The hard-negative variant\ndynamically computes all code embeddings each batch to select challenging\nnegatives. Results for each ablation and epoch count on the synthetic dataset\nare stored in a nested experiment_data dict and saved as 'experiment_data.npy'.", "Ablation name: Sequence Aggregation Ablation (Mean\u2010Pool vs LSTM). We define two\nencoder variants\u2014one with an LSTM and one with simple mean\u2010pooling over the\ncharacter embeddings\u2014then train both under the same triplet\u2010margin setup on our\nsynthetic code traces. For each ablation (LSTM vs. mean\u2010pool), we sweep epoch\ncounts in [10, 30, 50], logging losses, retrieval accuracies, and final\nnearest\u2010neighbor predictions. All per\u2010ablation results are stored in a nested\n`experiment_data` dictionary and saved as \u201cexperiment_data.npy.\u201d This isolates\nthe impact of sequence modeling by comparing order\u2010sensitive LSTM\nrepresentations against a bag\u2010of\u2010characters baseline.", "Ablation name: Triplet Margin Hyperparameter Ablation. We will run the synthetic\ncode experiment for margins=[0.1,0.5,1.0,2.0] over a fixed number of epochs\n(e.g. 30), reinitializing the model and optimizer for each margin. During\ntraining and validation we record average losses and retrieval accuracies per\nepoch, then at the end collect top-1 predictions on the validation set. All\ncollected data (metrics, losses, predictions, ground truth) is stored in a\nnested dict keyed by margin values and saved as `experiment_data.npy` for later\nanalysis.", "Ablation name: Contrastive Loss vs. Triplet Loss. I will extend the original\nscript to run two ablations\u2014TripletMarginLoss and a temperature\u2010scaled InfoNCE\n(NT\u2010Xent) loss\u2014over the same synthetic dataset and hyperparameters. The\ncontrastive loss is implemented by normalizing 2N embeddings per batch (anchor\nand positive) and computing a log\u2010softmax over all in\u2010batch negatives. We\ncollect train/val losses, retrieval accuracies, and final predictions for each\nepoch count, then save everything in a single `experiment_data.npy` file for\nlater analysis.", "We fix the MeanPoolEncoder instantiation by parameterizing each encoder in the\nablations dict and unpacking only the expected kwargs, avoiding passing a\n\u2018hidden\u2019 argument to mean pooling. We also add GPU/CPU initialization and device\nprinting, move models and tensors to the correct device, and track and print\nvalidation loss and retrieval accuracy at each epoch. Additionally, we compute\nand record the Contrastive Alignment Gap (difference between mean positive and\nnegative cosine similarities) for both train and validation sets per epoch.\nFinally, we save all metrics, losses, predictions, and ground truth into\n`experiment_data.npy` in the working directory.", "We add an optional `hidden` argument to the `MeanPoolEncoder` so that the\nablation\u2010loop can uniformly pass `hidden` to both encoders without error.  We\nalso augment the training loop to compute and log the contrastive alignment gap\n(difference between mean cosine similarity of positive vs. negative pairs) at\neach epoch, print validation loss and alignment gap, and finally save all\nmetrics, losses, and predictions into a single `experiment_data.npy`.", "We encountered a TypeError because `MeanPoolEncoder` does not expect a `hidden`\nargument. To fix this, we selectively pass the `hidden` parameter only when\ninstantiating the `LSTMEncoder` and omit it for the `MeanPoolEncoder`. The rest\nof the experimental pipeline remains unchanged, ensuring reproducibility and\nproper device placement.", "Ablation name: Tokenization Granularity Ablation. Below is a self-contained\nscript that runs the tokenization granularity ablation, comparing char-level,\nsimple regex subword, and AST-based tokenizations on the same synthetic dataset,\nusing the same encoder and triplet loss. It records losses, retrieval metrics,\nfinal predictions, and ground truths for epoch settings [10,\u200930,\u200950] and saves\neverything in `experiment_data.npy`.", "Ablation name: Distance Metric Ablation. Below is a single\u2010file script that runs\nboth the original Euclidean triplet loss and the cosine\u2010distance variant (with\nL2\u2010normalization) on the synthetic code dataset, logs train/val losses and\nretrieval accuracies for each epoch setting, collects final predictions vs.\nground truth, and saves everything in one `experiment_data.npy` file under\n`working/`.", "Ablation name: Embedding Dimension Ablation. Below is a sketch of the solution.\nWe extend the baseline by looping over embedding/hidden sizes in\n[16,32,64,128,256], training a fresh CodeEncoder for a fixed number of epochs on\nthe synthetic dataset with triplet loss, and logging per\u2010epoch train/val losses\nand retrieval accuracies. After training each model we also collect final top\u20101\npredictions on the validation split. All metrics, losses, predictions, and\nground truths are aggregated in a nested `experiment_data` dict under the key\n`\"embed_hidden\"\u2192\"synthetic\"[dim]` and saved via `np.save(\"experiment_data.npy\",\nexperiment_data)` for downstream plotting and analysis.", "Ablation name: Variable Renaming Invariance Ablation. Below is a single\u2010file\nPython script that implements the variable\u2010renaming invariance ablation.  We\ngenerate a random new name for each function and its argument, build a joint\nvocabulary over both original and renamed snippets, train the triplet model on\nthe original scheme, and at each epoch compute standard train/val retrieval as\nwell as val retrieval on the unseen renamed versions.  Final predictions on both\noriginal and renamed val queries are also saved.", "Ablation name: Bidirectional LSTM Ablation. I will extend the encoder to take a\n`bidirectional` flag, halving the per-direction hidden size when enabled, and\nrun experiments across both unidirectional and bidirectional variants for epoch\ncounts [10,30,50].  For each setting we\u2019ll train using triplet loss on our\nsynthetic code dataset, recording per-epoch train/val losses and retrieval\naccuracies, then collect final validation predictions vs. ground truth.  All\nmetrics, losses, predictions, and ground truths get stored in a nested\n`experiment_data['bidirectional_lstm_ablation']['synthetic'][variant][E]`\ndictionary as numpy arrays and saved to `experiment_data.npy`.", "Ablation name: Dead Code Injection Ablation. Here\u2019s a self-contained script that\ngenerates both the clean and dead-code-injected synthetic snippets, trains and\nevaluates the same encoder on each over multiple epoch budgets, records losses,\nretrieval accuracies, predictions and ground truth for each setting, and saves\neverything in a single `experiment_data.npy` file under the ablation key\n`dead_code_injection`.", "We enrich the AST tokenizer to emit constant values (and variable names) so that\ndifferent \u201cc\u201d parameters yield distinct token sequences, fixing the collapse\nthat led to zero ast accuracy. We also compute the Contrastive Alignment Gap\neach epoch by taking the difference of mean cosine similarities between positive\nand negative pairs, record it alongside train/val loss and accuracy, and print\nthe validation loss at every epoch. All models and data are moved to the correct\ndevice, and we save the full experiment data at the end.", "Ablation name: Projection Head Ablation. Below is a single\u2010file script that\nextends the baseline by defining a configurable projection head on top of the\nLSTM encoder and runs an ablation over 0/1/2\u2010layer MLP heads and different epoch\nbudgets.  For each combination it trains with triplet loss, tracks train/val\nlosses and retrieval accuracies, saves predictions and ground truth, and stores\neverything in a nested dict which is finally saved via np.save.", "Ablation name: CNN_ENCODER_ABLATION. Below is a complete single\u2010file Python\nscript that replaces the LSTM encoder with a 1D CNN encoder (multiple filter\nwidths + global max\u2010pooling + linear projection) and runs the same triplet\u2010loss\nretrieval experiments over epoch counts [10,30,50]. It records train/val losses,\naccuracies, final predictions, ground truth labels, and parameter counts under\n`experiment_data['CNN_ENCODER_ABLATION']['synthetic'][E]` and saves everything\nas `experiment_data.npy`.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# generate synthetic code snippets and traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: ids for gid, ids in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# encode snippets as character sequences\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\n# dataset & loaders\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# model definition\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# hyperparameter tuning over different epoch counts\nEPOCH_LIST = [10, 30, 50]\nexperiment_data = {\"EPOCHS\": {\"synthetic\": {}}}\n\nfor E in EPOCH_LIST:\n    # reinitialize model & optimizer\n    model = CodeEncoder(len(stoi), 64, 64).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.TripletMarginLoss(margin=1.0)\n\n    # containers for this setting\n    data = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    # training + validation\n    for epoch in range(E):\n        model.train()\n        total_train_loss = 0.0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a, emb_p, emb_n = model(a), model(p), model(n)\n            loss = loss_fn(emb_a, emb_p, emb_n)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_train_loss += loss.item()\n        avg_train_loss = total_train_loss / len(train_loader)\n        data[\"losses\"][\"train\"].append(avg_train_loss)\n\n        model.eval()\n        total_val_loss = 0.0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                total_val_loss += loss_fn(model(a), model(p), model(n)).item()\n        avg_val_loss = total_val_loss / len(val_loader)\n        data[\"losses\"][\"val\"].append(avg_val_loss)\n\n        # compute retrieval accuracies\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n\n            def compute_acc(indices):\n                correct = 0\n                for i in indices:\n                    sim = sims[i].clone()\n                    sim[i] = -float(\"inf\")\n                    top1 = torch.argmax(sim).item()\n                    if index_to_gid[top1] == index_to_gid[i]:\n                        correct += 1\n                return correct / len(indices)\n\n            train_acc = compute_acc(train_indices)\n            val_acc = compute_acc(val_indices)\n        data[\"metrics\"][\"train\"].append(train_acc)\n        data[\"metrics\"][\"val\"].append(val_acc)\n\n    # final predictions on validation\n    model.eval()\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n        for i in val_indices:\n            sim = sims[i].clone()\n            sim[i] = -float(\"inf\")\n            top1 = torch.argmax(sim).item()\n            data[\"predictions\"].append(index_to_gid[top1])\n            data[\"ground_truth\"].append(index_to_gid[i])\n\n    experiment_data[\"EPOCHS\"][\"synthetic\"][E] = data\n    print(f\"Finished EPOCHS={E}: final val_acc={val_acc:.4f}\")\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os, random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# reproducibility\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# hyperparameters\nEPOCH_LIST = [10, 30, 50]\nexperiment_data = {\"multi_dataset_synthetic_ablation\": {}}\n\n# shared input set for tracing\ninput_set = np.random.randint(0, 20, size=100)\n\n# define dataset names\ndataset_names = [\"arith\", \"branch\", \"loop\"]\n\n\n# triplet dataset\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\n# LSTM encoder\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# run ablation across datasets\nfor name in dataset_names:\n    # generate synthetic code variants\n    codes = []\n    if name == \"arith\":\n        for c in range(1, 11):\n            codes.append(f\"def f(x): return x+{c}\")\n            codes.append(f\"def f(x): return {c}+x\")\n    elif name == \"branch\":\n        for c in range(1, 11):\n            codes.append(\n                f\"def f(x):\\n\"\n                f\"    if x%{c}==0:\\n\"\n                f\"        return x//{c}\\n\"\n                f\"    else:\\n\"\n                f\"        return {c}*x\"\n            )\n            codes.append(f\"def f(x): return (x//{c} if x%{c}==0 else {c}*x)\")\n    elif name == \"loop\":\n        for c in range(1, 11):\n            codes.append(\n                f\"def f(x):\\n\"\n                f\"    s=0\\n\"\n                f\"    for i in range(x):\\n\"\n                f\"        s+=i*{c}\\n\"\n                f\"    return s\"\n            )\n            codes.append(\n                f\"def f(x):\\n\"\n                f\"    s=0\\n\"\n                f\"    i=0\\n\"\n                f\"    while i<x:\\n\"\n                f\"        s+=i*{c}\\n\"\n                f\"        i+=1\\n\"\n                f\"    return s\"\n            )\n    # execute and record traces\n    traces = []\n    for code in codes:\n        env = {}\n        exec(code, env)\n        f = env[\"f\"]\n        traces.append(tuple(f(int(x)) for x in input_set))\n    # group by identical trace\n    trace_to_indices = {}\n    for idx, t in enumerate(traces):\n        trace_to_indices.setdefault(t, []).append(idx)\n    group_to_indices = {\n        gid: idxs for gid, (_, idxs) in enumerate(trace_to_indices.items())\n    }\n    index_to_gid = [None] * len(codes)\n    for gid, idxs in group_to_indices.items():\n        for i in idxs:\n            index_to_gid[i] = gid\n    # encode as character sequences\n    vocab = sorted(set(\"\".join(codes)))\n    stoi = {c: i + 1 for i, c in enumerate(vocab)}\n    stoi[\"PAD\"] = 0\n    max_len = max(len(s) for s in codes)\n    encoded = torch.LongTensor(\n        [[stoi[ch] for ch in s] + [0] * (max_len - len(s)) for s in codes]\n    )\n    # train/val split on groups\n    all_gids = list(group_to_indices.keys())\n    random.shuffle(all_gids)\n    split = int(0.8 * len(all_gids))\n    train_gids, val_gids = all_gids[:split], all_gids[split:]\n    train_indices = [i for g in train_gids for i in group_to_indices[g]]\n    val_indices = [i for g in val_gids for i in group_to_indices[g]]\n    train_loader = DataLoader(\n        Subset(CodeDataset(encoded, group_to_indices, index_to_gid), train_indices),\n        batch_size=8,\n        shuffle=True,\n    )\n    val_loader = DataLoader(\n        Subset(CodeDataset(encoded, group_to_indices, index_to_gid), val_indices),\n        batch_size=8,\n        shuffle=False,\n    )\n    # run experiments for each epoch count\n    per_epoch = {}\n    for E in EPOCH_LIST:\n        model = CodeEncoder(len(stoi), 64, 64).to(device)\n        optimizer = Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.TripletMarginLoss(margin=1.0)\n        data = {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        # train & validate\n        for epoch in range(E):\n            model.train()\n            tot = 0.0\n            for a, p, n in train_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                la, lp, ln = model(a), model(p), model(n)\n                loss = loss_fn(la, lp, ln)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                tot += loss.item()\n            data[\"losses\"][\"train\"].append(tot / len(train_loader))\n            # val loss\n            model.eval()\n            totv = 0.0\n            with torch.no_grad():\n                for a, p, n in val_loader:\n                    totv += loss_fn(\n                        model(a.to(device)), model(p.to(device)), model(n.to(device))\n                    ).item()\n            data[\"losses\"][\"val\"].append(totv / len(val_loader))\n            # retrieval acc\n            with torch.no_grad():\n                emb_all = model(encoded.to(device))\n                emb_n = F.normalize(emb_all, dim=1)\n                sims = emb_n @ emb_n.T\n\n                def get_acc(idxs):\n                    c = 0\n                    for i in idxs:\n                        v = sims[i].clone()\n                        v[i] = -1e9\n                        if index_to_gid[torch.argmax(v).item()] == index_to_gid[i]:\n                            c += 1\n                    return c / len(idxs)\n\n                data[\"metrics\"][\"train\"].append(get_acc(train_indices))\n                data[\"metrics\"][\"val\"].append(get_acc(val_indices))\n        # final top-1 predictions\n        model.eval()\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_n = F.normalize(emb_all, dim=1)\n            sims = emb_n @ emb_n.T\n            for i in val_indices:\n                v = sims[i].clone()\n                v[i] = -1e9\n                data[\"predictions\"].append(index_to_gid[torch.argmax(v).item()])\n                data[\"ground_truth\"].append(index_to_gid[i])\n        per_epoch[E] = data\n        print(\n            f\"Dataset={name}, EPOCHS={E}, final val_acc={data['metrics']['val'][-1]:.4f}\"\n        )\n    experiment_data[\"multi_dataset_synthetic_ablation\"][name] = {\"EPOCHS\": per_epoch}\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# generate synthetic code snippets and traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: ids for gid, ids in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# encode snippets as character sequences\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n# train/val split at group level\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\n\n\n# baseline random-negative dataset\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\n# hard-negative dataset returns only anchor+pos+idx\nclass HardCodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        return torch.tensor(idx), anchor, self.encoded[pos]\n\n\n# prepare dataloaders\nbatch_size = 8\nrandom_dataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nrandom_train_loader = DataLoader(\n    Subset(random_dataset, train_indices), batch_size=batch_size, shuffle=True\n)\nrandom_val_loader = DataLoader(\n    Subset(random_dataset, val_indices), batch_size=batch_size, shuffle=False\n)\n\nhard_dataset = HardCodeDataset(encoded, group_to_indices, index_to_gid)\nhard_train_loader = DataLoader(\n    Subset(hard_dataset, train_indices), batch_size=batch_size, shuffle=True\n)\nhard_val_loader = DataLoader(\n    Subset(hard_dataset, val_indices), batch_size=batch_size, shuffle=False\n)\n\n\n# model definition\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# experiment setup\nEPOCH_LIST = [10, 30, 50]\nexperiment_data = {\n    \"random_negative\": {\"synthetic\": {}},\n    \"hard_negative\": {\"synthetic\": {}},\n}\n\n# run experiments\nfor ablation in experiment_data:\n    print(f\"Starting ablation: {ablation}\")\n    for E in EPOCH_LIST:\n        model = CodeEncoder(len(stoi), 64, 64).to(device)\n        optimizer = Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.TripletMarginLoss(margin=1.0)\n        data = {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        # training + validation\n        for epoch in range(E):\n            model.train()\n            total_train_loss = 0.0\n            if ablation == \"random_negative\":\n                for a, p, n in random_train_loader:\n                    a, p, n = a.to(device), p.to(device), n.to(device)\n                    emb_a, emb_p, emb_n = model(a), model(p), model(n)\n                    loss = loss_fn(emb_a, emb_p, emb_n)\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n                    total_train_loss += loss.item()\n                train_batches = len(random_train_loader)\n            else:\n                for idxs, a, p in hard_train_loader:\n                    idxs = idxs.tolist()\n                    a, p = a.to(device), p.to(device)\n                    emb_a, emb_p = model(a), model(p)\n                    enc_dev = encoded.to(device)\n                    emb_all = model(enc_dev)\n                    emb_n = torch.zeros_like(emb_a)\n                    for i, idx in enumerate(idxs):\n                        dist = torch.norm(emb_all - emb_a[i].unsqueeze(0), p=2, dim=1)\n                        gid = index_to_gid[idx]\n                        for j, jgid in enumerate(index_to_gid):\n                            if jgid == gid:\n                                dist[j] = float(\"inf\")\n                        neg_idx = torch.argmin(dist).item()\n                        emb_n[i] = emb_all[neg_idx]\n                    loss = loss_fn(emb_a, emb_p, emb_n)\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n                    total_train_loss += loss.item()\n                train_batches = len(hard_train_loader)\n            avg_train_loss = total_train_loss / train_batches\n            data[\"losses\"][\"train\"].append(avg_train_loss)\n\n            model.eval()\n            total_val_loss = 0.0\n            with torch.no_grad():\n                if ablation == \"random_negative\":\n                    for a, p, n in random_val_loader:\n                        a, p, n = a.to(device), p.to(device), n.to(device)\n                        total_val_loss += loss_fn(model(a), model(p), model(n)).item()\n                    val_batches = len(random_val_loader)\n                else:\n                    for idxs, a, p in hard_val_loader:\n                        idxs = idxs.tolist()\n                        a, p = a.to(device), p.to(device)\n                        emb_a, emb_p = model(a), model(p)\n                        enc_dev = encoded.to(device)\n                        emb_all = model(enc_dev)\n                        emb_n = torch.zeros_like(emb_a)\n                        for i, idx in enumerate(idxs):\n                            dist = torch.norm(\n                                emb_all - emb_a[i].unsqueeze(0), p=2, dim=1\n                            )\n                            gid = index_to_gid[idx]\n                            for j, jgid in enumerate(index_to_gid):\n                                if jgid == gid:\n                                    dist[j] = float(\"inf\")\n                            neg_idx = torch.argmin(dist).item()\n                            emb_n[i] = emb_all[neg_idx]\n                        total_val_loss += loss_fn(emb_a, emb_p, emb_n).item()\n                    val_batches = len(hard_val_loader)\n            avg_val_loss = total_val_loss / max(val_batches, 1)\n            data[\"losses\"][\"val\"].append(avg_val_loss)\n\n            # retrieval accuracies\n            with torch.no_grad():\n                emb_all = model(encoded.to(device))\n                emb_norm = F.normalize(emb_all, dim=1)\n                sims = emb_norm @ emb_norm.T\n\n                def compute_acc(indices):\n                    correct = 0\n                    for i in indices:\n                        sim = sims[i].clone()\n                        sim[i] = -float(\"inf\")\n                        top1 = torch.argmax(sim).item()\n                        if index_to_gid[top1] == index_to_gid[i]:\n                            correct += 1\n                    return correct / len(indices)\n\n                train_acc = compute_acc(train_indices)\n                val_acc = compute_acc(val_indices)\n            data[\"metrics\"][\"train\"].append(train_acc)\n            data[\"metrics\"][\"val\"].append(val_acc)\n\n        # final predictions on validation\n        model.eval()\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n            for i in val_indices:\n                sim = sims[i].clone()\n                sim[i] = -float(\"inf\")\n                top1 = torch.argmax(sim).item()\n                data[\"predictions\"].append(index_to_gid[top1])\n                data[\"ground_truth\"].append(index_to_gid[i])\n\n        experiment_data[ablation][\"synthetic\"][E] = data\n        print(f\"Finished {ablation}, EPOCHS={E}: final val_acc={val_acc:.4f}\")\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# generate synthetic code snippets and execution traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group code snippets by identical trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: idxs for gid, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for i in idxs:\n        index_to_gid[i] = gid\n\n# character\u2010level encoding\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\n# dataset & loaders\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# encoder variants\nclass LSTMEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\nclass MeanPoolEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n\n    def forward(self, x):\n        emb = self.embed(x)  # [B, L, D]\n        mask = (x != 0).unsqueeze(-1).float()  # [B, L, 1]\n        summed = (emb * mask).sum(dim=1)  # [B, D]\n        lengths = mask.sum(dim=1).clamp(min=1.0)  # [B, 1]\n        return summed / lengths\n\n\n# experimental sweep\nEPOCH_LIST = [10, 30, 50]\nablations = {\"lstm\": LSTMEncoder, \"mean_pool\": MeanPoolEncoder}\n\nexperiment_data = {\"lstm\": {\"synthetic\": {}}, \"mean_pool\": {\"synthetic\": {}}}\n\nfor name, Encoder in ablations.items():\n    for E in EPOCH_LIST:\n        # model, optimizer, loss\n        model = Encoder(\n            len(stoi), embed_dim=64, hidden=64 if name == \"lstm\" else None\n        ).to(device)\n        optimizer = Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.TripletMarginLoss(margin=1.0)\n\n        data = {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # training & validation\n        for epoch in range(E):\n            model.train()\n            tot_tr = 0.0\n            for a, p, n in train_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                emb_a, emb_p, emb_n = model(a), model(p), model(n)\n                loss = loss_fn(emb_a, emb_p, emb_n)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                tot_tr += loss.item()\n            data[\"losses\"][\"train\"].append(tot_tr / len(train_loader))\n\n            model.eval()\n            tot_val = 0.0\n            with torch.no_grad():\n                for a, p, n in val_loader:\n                    a, p, n = a.to(device), p.to(device), n.to(device)\n                    tot_val += loss_fn(model(a), model(p), model(n)).item()\n            data[\"losses\"][\"val\"].append(tot_val / len(val_loader))\n\n            # retrieval acc\n            with torch.no_grad():\n                emb_all = model(encoded.to(device))\n                emb_norm = F.normalize(emb_all, dim=1)\n                sims = emb_norm @ emb_norm.T\n\n                def compute_acc(idxs):\n                    c = 0\n                    for i in idxs:\n                        sim = sims[i].clone()\n                        sim[i] = -1e9\n                        top1 = torch.argmax(sim).item()\n                        if index_to_gid[top1] == index_to_gid[i]:\n                            c += 1\n                    return c / len(idxs)\n\n                data[\"metrics\"][\"train\"].append(compute_acc(train_indices))\n                data[\"metrics\"][\"val\"].append(compute_acc(val_indices))\n\n        # final predictions on val set\n        model.eval()\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n            for i in val_indices:\n                sim = sims[i].clone()\n                sim[i] = -1e9\n                top1 = torch.argmax(sim).item()\n                data[\"predictions\"].append(index_to_gid[top1])\n                data[\"ground_truth\"].append(index_to_gid[i])\n\n        experiment_data[name][\"synthetic\"][E] = data\n        print(f\"Finished {name}, E={E}: final val_acc={data['metrics']['val'][-1]:.4f}\")\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# generate synthetic code snippets and traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: ids for gid, ids in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# encode snippets as character sequences\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\n# dataset & loaders\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# model definition\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# ablation over triplet margin hyperparameter\nMARGIN_LIST = [0.1, 0.5, 1.0, 2.0]\nEPOCHS = 30\nexperiment_data = {\"triplet_margin_ablation\": {\"synthetic\": {}}}\n\nfor margin in MARGIN_LIST:\n    # init model, optimizer, loss\n    model = CodeEncoder(len(stoi), 64, 64).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.TripletMarginLoss(margin=margin)\n\n    data = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    # training + validation\n    for epoch in range(EPOCHS):\n        model.train()\n        total_train_loss = 0.0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a, emb_p, emb_n = model(a), model(p), model(n)\n            loss = loss_fn(emb_a, emb_p, emb_n)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_train_loss += loss.item()\n        avg_train_loss = total_train_loss / len(train_loader)\n        data[\"losses\"][\"train\"].append(avg_train_loss)\n\n        model.eval()\n        total_val_loss = 0.0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                total_val_loss += loss_fn(model(a), model(p), model(n)).item()\n        avg_val_loss = total_val_loss / len(val_loader)\n        data[\"losses\"][\"val\"].append(avg_val_loss)\n\n        # retrieval accuracy\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n\n            def compute_acc(indices):\n                correct = 0\n                for i in indices:\n                    sim = sims[i].clone()\n                    sim[i] = -float(\"inf\")\n                    top1 = torch.argmax(sim).item()\n                    if index_to_gid[top1] == index_to_gid[i]:\n                        correct += 1\n                return correct / len(indices)\n\n            train_acc = compute_acc(train_indices)\n            val_acc = compute_acc(val_indices)\n        data[\"metrics\"][\"train\"].append(train_acc)\n        data[\"metrics\"][\"val\"].append(val_acc)\n\n    # final predictions\n    model.eval()\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n        for i in val_indices:\n            sim = sims[i].clone()\n            sim[i] = -float(\"inf\")\n            top1 = torch.argmax(sim).item()\n            data[\"predictions\"].append(index_to_gid[top1])\n            data[\"ground_truth\"].append(index_to_gid[i])\n\n    experiment_data[\"triplet_margin_ablation\"][\"synthetic\"][margin] = data\n    final_val_acc = data[\"metrics\"][\"val\"][-1]\n    print(f\"Finished margin={margin}: final val_acc={final_val_acc:.4f}\")\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os, random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# synthetic data\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninputs = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in inputs))\ntrace_to_indices = {}\nfor idx, t in enumerate(traces):\n    trace_to_indices.setdefault(t, []).append(idx)\ngroup_to_indices = {g: idxs for g, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for i in idxs:\n        index_to_gid[i] = gid\n\n# encode\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = torch.LongTensor(\n    [[stoi[c] for c in s] + [0] * (max_len - len(s)) for s in codes]\n)\n\n\n# dataset\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.enc = encoded\n        self.g2i = group_to_indices\n        self.i2g = index_to_gid\n\n    def __len__(self):\n        return len(self.i2g)\n\n    def __getitem__(self, idx):\n        a = self.enc[idx]\n        gid = self.i2g[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.g2i[gid])\n        neg_gid = random.choice([g for g in self.g2i if g != gid])\n        neg = random.choice(self.g2i[neg_gid])\n        return a, self.enc[pos], self.enc[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_g, val_g = all_gids[:split], all_gids[split:]\ntrain_idx = [i for g in train_g for i in group_to_indices[g]]\nval_idx = [i for g in val_g for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_idx), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_idx), batch_size=8, shuffle=False)\n\n\n# model\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x, _ = self.embed(x), None\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# NT-Xent\ndef nt_xent_loss(z1, z2, tau=0.5):\n    N = z1.size(0)\n    z = torch.cat([z1, z2], dim=0)\n    z = F.normalize(z, p=2, dim=1)\n    sim = torch.matmul(z, z.T) / tau\n    mask = ~torch.eye(2 * N, device=sim.device, dtype=torch.bool)\n    sim_masked = sim.masked_fill(~mask, -1e9)\n    log_prob = F.log_softmax(sim_masked, dim=1)\n    pos_idx = (\n        torch.arange(N, 2 * N, device=sim.device).tolist()\n        + torch.arange(0, N, device=sim.device).tolist()\n    )\n    pos_idx = torch.tensor(pos_idx, device=sim.device)\n    loss = -log_prob[torch.arange(2 * N, device=sim.device), pos_idx].mean()\n    return loss\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_kwargs = dict(vocab_size=len(stoi), embed_dim=64, hidden=64)\nEPOCH_LIST = [10, 30, 50]\nexperiment_data = {\"triplet\": {\"synthetic\": {}}, \"contrastive\": {\"synthetic\": {}}}\n\nfor ablation in [\"triplet\", \"contrastive\"]:\n    for E in EPOCH_LIST:\n        model = CodeEncoder(**model_kwargs).to(device)\n        optimizer = Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.TripletMarginLoss(margin=1.0) if ablation == \"triplet\" else None\n        data = {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        for epoch in range(E):\n            model.train()\n            tot_tr = 0\n            for a, p, n in train_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                za, zp = model(a), model(p)\n                if ablation == \"triplet\":\n                    zn = model(n)\n                    loss = loss_fn(za, zp, zn)\n                else:\n                    loss = nt_xent_loss(za, zp, tau=0.5)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                tot_tr += loss.item()\n            avg_tr = tot_tr / len(train_loader)\n            data[\"losses\"][\"train\"].append(avg_tr)\n\n            model.eval()\n            tot_val = 0\n            with torch.no_grad():\n                for a, p, n in val_loader:\n                    a, p = a.to(device), p.to(device)\n                    if ablation == \"triplet\":\n                        loss = loss_fn(model(a), model(p), model(n.to(device)))\n                    else:\n                        loss = nt_xent_loss(model(a), model(p), tau=0.5)\n                    tot_val += loss.item()\n            avg_val = tot_val / len(val_loader)\n            data[\"losses\"][\"val\"].append(avg_val)\n\n            # retrieval\n            with torch.no_grad():\n                emb = model(encoded.to(device))\n                emb = F.normalize(emb, dim=1)\n                sims = emb @ emb.T\n\n                def acc(idxs):\n                    c = 0\n                    for i in idxs:\n                        row = sims[i].clone()\n                        row[i] = -1e9\n                        if index_to_gid[int(row.argmax())] == index_to_gid[i]:\n                            c += 1\n                    return c / len(idxs)\n\n                data[\"metrics\"][\"train\"].append(acc(train_idx))\n                data[\"metrics\"][\"val\"].append(acc(val_idx))\n\n        # final preds\n        model.eval()\n        with torch.no_grad():\n            emb = F.normalize(model(encoded.to(device)), dim=1)\n            sims = emb @ emb.T\n            for i in val_idx:\n                row = sims[i].clone()\n                row[i] = -1e9\n                pred = int(row.argmax())\n                data[\"predictions\"].append(index_to_gid[pred])\n                data[\"ground_truth\"].append(index_to_gid[i])\n\n        experiment_data[ablation][\"synthetic\"][E] = data\n        print(f\"{ablation} E={E} val_acc={data['metrics']['val'][-1]:.4f}\")\n\nnp.save(\"experiment_data.npy\", experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\n# generate synthetic code snippets and execution traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group code snippets by identical trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: idxs for gid, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for i in idxs:\n        index_to_gid[i] = gid\n\n# character-level encoding\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded).to(device)\n\n\n# dataset & loaders\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return {\n            \"anchor\": anchor,\n            \"pos\": self.encoded[pos],\n            \"neg\": self.encoded[neg],\n            \"idx\": idx,\n        }\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# encoder variants\nclass LSTMEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\nclass MeanPoolEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n\n    def forward(self, x):\n        emb = self.embed(x)\n        mask = (x != 0).unsqueeze(-1).float()\n        summed = (emb * mask).sum(dim=1)\n        lengths = mask.sum(dim=1).clamp(min=1.0)\n        return summed / lengths\n\n\n# experimental sweep\nEPOCH_LIST = [10, 30, 50]\nablations = {\n    \"lstm\": (LSTMEncoder, {\"embed_dim\": 64, \"hidden\": 64}),\n    \"mean_pool\": (MeanPoolEncoder, {\"embed_dim\": 64}),\n}\nexperiment_data = {name: {\"synthetic\": {}} for name in ablations}\n\nfor name, (Encoder, params) in ablations.items():\n    for E in EPOCH_LIST:\n        model = Encoder(len(stoi), **params).to(device)\n        optimizer = Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.TripletMarginLoss(margin=1.0)\n        data = {\n            \"metrics\": {\"train_acc\": [], \"val_acc\": [], \"gap_train\": [], \"gap_val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        for epoch in range(1, E + 1):\n            # training\n            model.train()\n            tot_tr, sum_pos, sum_neg, cnt = 0.0, 0.0, 0.0, 0\n            for batch in train_loader:\n                a = batch[\"anchor\"].to(device)\n                p = batch[\"pos\"].to(device)\n                n = batch[\"neg\"].to(device)\n                emb_a, emb_p, emb_n = model(a), model(p), model(n)\n                loss = loss_fn(emb_a, emb_p, emb_n)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                tot_tr += loss.item()\n                pos_cos = F.cosine_similarity(emb_a, emb_p, dim=1)\n                neg_cos = F.cosine_similarity(emb_a, emb_n, dim=1)\n                sum_pos += pos_cos.sum().item()\n                sum_neg += neg_cos.sum().item()\n                cnt += a.size(0)\n            train_loss = tot_tr / len(train_loader)\n            gap_train = (sum_pos / cnt) - (sum_neg / cnt)\n            data[\"losses\"][\"train\"].append(train_loss)\n\n            # validation\n            model.eval()\n            tot_val, sum_pos_v, sum_neg_v, cnt_v = 0.0, 0.0, 0.0, 0\n            with torch.no_grad():\n                for batch in val_loader:\n                    a = batch[\"anchor\"].to(device)\n                    p = batch[\"pos\"].to(device)\n                    n = batch[\"neg\"].to(device)\n                    emb_a, emb_p, emb_n = model(a), model(p), model(n)\n                    tot_val += loss_fn(emb_a, emb_p, emb_n).item()\n                    sum_pos_v += F.cosine_similarity(emb_a, emb_p, dim=1).sum().item()\n                    sum_neg_v += F.cosine_similarity(emb_a, emb_n, dim=1).sum().item()\n                    cnt_v += a.size(0)\n            val_loss = tot_val / len(val_loader)\n            gap_val = (sum_pos_v / cnt_v) - (sum_neg_v / cnt_v)\n            data[\"losses\"][\"val\"].append(val_loss)\n            data[\"metrics\"][\"gap_train\"].append(gap_train)\n            data[\"metrics\"][\"gap_val\"].append(gap_val)\n\n            # retrieval accuracy\n            with torch.no_grad():\n                emb_all = model(encoded)\n                emb_norm = F.normalize(emb_all, dim=1)\n                sims = emb_norm @ emb_norm.T\n\n                def compute_acc(idxs):\n                    c = 0\n                    for i in idxs:\n                        sim = sims[i].clone()\n                        sim[i] = -1e9\n                        top1 = torch.argmax(sim).item()\n                        if index_to_gid[top1] == index_to_gid[i]:\n                            c += 1\n                    return c / len(idxs)\n\n                train_acc = compute_acc(train_indices)\n                val_acc = compute_acc(val_indices)\n            data[\"metrics\"][\"train_acc\"].append(train_acc)\n            data[\"metrics\"][\"val_acc\"].append(val_acc)\n            print(\n                f\"{name} Epoch {epoch}: validation_loss = {val_loss:.4f}, val_acc = {val_acc:.4f}, gap_val = {gap_val:.4f}\"\n            )\n\n        # final predictions\n        model.eval()\n        with torch.no_grad():\n            emb_all = model(encoded)\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n            for i in val_indices:\n                sim = sims[i].clone()\n                sim[i] = -1e9\n                top1 = torch.argmax(sim).item()\n                data[\"predictions\"].append(index_to_gid[top1])\n                data[\"ground_truth\"].append(index_to_gid[i])\n\n        experiment_data[name][\"synthetic\"][E] = data\n        print(\n            f\"Finished {name}, E={E}: final val_acc={data['metrics']['val_acc'][-1]:.4f}\"\n        )\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# generate synthetic code snippets and execution traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group code snippets by identical trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: idxs for gid, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for i in idxs:\n        index_to_gid[i] = gid\n\n# character\u2010level encoding\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = torch.LongTensor(\n    [[stoi[c] for c in s] + [0] * (max_len - len(s)) for s in codes]\n).to(device)\n\n\n# dataset & loaders\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# encoder variants\nclass LSTMEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\nclass MeanPoolEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=None):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n\n    def forward(self, x):\n        emb = self.embed(x)  # [B, L, D]\n        mask = (x != 0).unsqueeze(-1).float()  # [B, L, 1]\n        summed = (emb * mask).sum(dim=1)  # [B, D]\n        lengths = mask.sum(dim=1).clamp(min=1.0)  # [B, 1]\n        return summed / lengths\n\n\n# experimental sweep\nEPOCH_LIST = [10, 30, 50]\nablations = {\"lstm\": LSTMEncoder, \"mean_pool\": MeanPoolEncoder}\n\nexperiment_data = {name: {\"synthetic\": {}} for name in ablations}\n\nfor name, Encoder in ablations.items():\n    for E in EPOCH_LIST:\n        # instantiate model with or without hidden\n        if name == \"lstm\":\n            model = Encoder(len(stoi), embed_dim=64, hidden=64).to(device)\n        else:\n            model = Encoder(len(stoi), embed_dim=64).to(device)\n        optimizer = Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.TripletMarginLoss(margin=1.0)\n\n        data = {\n            \"metrics\": {\n                \"train\": [],\n                \"val\": [],\n                \"align_gap_train\": [],\n                \"align_gap_val\": [],\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        for epoch in range(1, E + 1):\n            # training\n            model.train()\n            tot_train_loss = 0.0\n            for a, p, n in train_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                emb_a, emb_p, emb_n = model(a), model(p), model(n)\n                loss = loss_fn(emb_a, emb_p, emb_n)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                tot_train_loss += loss.item()\n            train_loss = tot_train_loss / len(train_loader)\n            data[\"losses\"][\"train\"].append(train_loss)\n\n            # validation & alignment gap\n            model.eval()\n            tot_val_loss = 0.0\n            train_pos, train_neg = [], []\n            val_pos, val_neg = [], []\n            with torch.no_grad():\n                for a, p, n in train_loader:\n                    a, p, n = a.to(device), p.to(device), n.to(device)\n                    e_a, e_p, e_n = model(a), model(p), model(n)\n                    train_pos.append(F.cosine_similarity(e_a, e_p, dim=1))\n                    train_neg.append(F.cosine_similarity(e_a, e_n, dim=1))\n                for a, p, n in val_loader:\n                    a, p, n = a.to(device), p.to(device), n.to(device)\n                    e_a, e_p, e_n = model(a), model(p), model(n)\n                    tot_val_loss += loss_fn(e_a, e_p, e_n).item()\n                    val_pos.append(F.cosine_similarity(e_a, e_p, dim=1))\n                    val_neg.append(F.cosine_similarity(e_a, e_n, dim=1))\n            val_loss = tot_val_loss / len(val_loader)\n            data[\"losses\"][\"val\"].append(val_loss)\n\n            train_pos = torch.cat(train_pos)\n            train_neg = torch.cat(train_neg)\n            val_pos = torch.cat(val_pos)\n            val_neg = torch.cat(val_neg)\n            align_gap_train = train_pos.mean().item() - train_neg.mean().item()\n            align_gap_val = val_pos.mean().item() - val_neg.mean().item()\n            data[\"metrics\"][\"align_gap_train\"].append(align_gap_train)\n            data[\"metrics\"][\"align_gap_val\"].append(align_gap_val)\n\n            # retrieval accuracy\n            with torch.no_grad():\n                emb_all = model(encoded)\n                emb_norm = F.normalize(emb_all, dim=1)\n                sims = emb_norm @ emb_norm.T\n\n                def compute_acc(idxs):\n                    c = 0\n                    for i in idxs:\n                        s = sims[i].clone()\n                        s[i] = -1e9\n                        top1 = torch.argmax(s).item()\n                        if index_to_gid[top1] == index_to_gid[i]:\n                            c += 1\n                    return c / len(idxs)\n\n                train_acc = compute_acc(train_indices)\n                val_acc = compute_acc(val_indices)\n            data[\"metrics\"][\"train\"].append(train_acc)\n            data[\"metrics\"][\"val\"].append(val_acc)\n\n            print(\n                f\"Finished {name}, E={E}, Epoch {epoch}: validation_loss = {val_loss:.4f}, val_align_gap = {align_gap_val:.4f}\"\n            )\n\n        # final predictions on val set\n        model.eval()\n        with torch.no_grad():\n            emb_all = model(encoded)\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n            for i in val_indices:\n                s = sims[i].clone()\n                s[i] = -1e9\n                pick = torch.argmax(s).item()\n                data[\"predictions\"].append(index_to_gid[pick])\n                data[\"ground_truth\"].append(index_to_gid[i])\n\n        experiment_data[name][\"synthetic\"][E] = data\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\n# generate synthetic code snippets and execution traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group code snippets by identical trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: idxs for gid, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for i in idxs:\n        index_to_gid[i] = gid\n\n# character-level encoding\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded).to(device)\n\n\n# dataset & loaders\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# encoder variants\nclass LSTMEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\nclass MeanPoolEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n\n    def forward(self, x):\n        emb = self.embed(x)\n        mask = (x != 0).unsqueeze(-1).float()\n        summed = (emb * mask).sum(dim=1)\n        lengths = mask.sum(dim=1).clamp(min=1.0)\n        return summed / lengths\n\n\n# experimental sweep\nEPOCH_LIST = [10, 30, 50]\nablations = {\"lstm\": LSTMEncoder, \"mean_pool\": MeanPoolEncoder}\nexperiment_data = {\"lstm\": {\"synthetic\": {}}, \"mean_pool\": {\"synthetic\": {}}}\n\nfor name, Encoder in ablations.items():\n    for E in EPOCH_LIST:\n        # initialize model correctly without passing hidden to MeanPoolEncoder\n        if name == \"lstm\":\n            model = Encoder(len(stoi), embed_dim=64, hidden=64).to(device)\n        else:\n            model = Encoder(len(stoi), embed_dim=64).to(device)\n        optimizer = Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.TripletMarginLoss(margin=1.0)\n\n        data = {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # training & validation\n        for epoch in range(E):\n            model.train()\n            tot_tr = 0.0\n            for a, p, n in train_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                emb_a, emb_p, emb_n = model(a), model(p), model(n)\n                loss = loss_fn(emb_a, emb_p, emb_n)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                tot_tr += loss.item()\n            train_loss = tot_tr / len(train_loader)\n            data[\"losses\"][\"train\"].append(train_loss)\n\n            model.eval()\n            tot_val = 0.0\n            with torch.no_grad():\n                for a, p, n in val_loader:\n                    a, p, n = a.to(device), p.to(device), n.to(device)\n                    tot_val += loss_fn(model(a), model(p), model(n)).item()\n            val_loss = tot_val / len(val_loader)\n            data[\"losses\"][\"val\"].append(val_loss)\n            print(f\"Epoch {epoch+1}: validation_loss = {val_loss:.4f}\")\n\n            # retrieval accuracy\n            with torch.no_grad():\n                emb_all = model(encoded)\n                emb_norm = F.normalize(emb_all, dim=1)\n                sims = emb_norm @ emb_norm.T\n\n                def compute_acc(idxs):\n                    c = 0\n                    for i in idxs:\n                        sim = sims[i].clone()\n                        sim[i] = -1e9\n                        top1 = torch.argmax(sim).item()\n                        if index_to_gid[top1] == index_to_gid[i]:\n                            c += 1\n                    return c / len(idxs)\n\n                data[\"metrics\"][\"train\"].append(compute_acc(train_indices))\n                data[\"metrics\"][\"val\"].append(compute_acc(val_indices))\n\n        # final predictions on val set\n        model.eval()\n        with torch.no_grad():\n            emb_all = model(encoded)\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n            for i in val_indices:\n                sim = sims[i].clone()\n                sim[i] = -1e9\n                top1 = torch.argmax(sim).item()\n                data[\"predictions\"].append(index_to_gid[top1])\n                data[\"ground_truth\"].append(index_to_gid[i])\n\n        experiment_data[name][\"synthetic\"][E] = data\n        print(f\"Finished {name}, E={E}: final val_acc={data['metrics']['val'][-1]:.4f}\")\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os, random, re, ast, numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# working dir & device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# generate synthetic functions and traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group by trace\ntrace_to_indices = {}\nfor idx, tr in enumerate(traces):\n    trace_to_indices.setdefault(tr, []).append(idx)\ngroup_to_indices = {gid: idxs for gid, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# train/val split on groups\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\n\n\n# Dataset\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\n# Encoder\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# tokenizers\ndef char_tokenize(s):\n    return list(s)\n\n\ndef subword_tokenize(s):\n    return re.findall(r\"\\w+|[^\\s\\w]\", s)\n\n\ndef ast_tokenize(s):\n    tokens = []\n\n    def visit(node):\n        tokens.append(type(node).__name__)\n        for _, v in ast.iter_fields(node):\n            if isinstance(v, list):\n                for elt in v:\n                    if isinstance(elt, ast.AST):\n                        visit(elt)\n            elif isinstance(v, ast.AST):\n                visit(v)\n\n    visit(ast.parse(s))\n    return tokens\n\n\nschemes = {\"char\": char_tokenize, \"subword\": subword_tokenize, \"ast\": ast_tokenize}\nEPOCH_LIST = [10, 30, 50]\n\n# run ablation\nexperiment_data = {\"tokenization_granularity\": {\"synthetic\": {}}}\nfor scheme, tokfn in schemes.items():\n    synth_data = {}\n    # tokenize & build vocab\n    toks_list = [tokfn(s) for s in codes]\n    vocab = sorted({t for toks in toks_list for t in toks})\n    stoi = {t: i + 1 for i, t in enumerate(vocab)}\n    stoi[\"PAD\"] = 0\n    max_len = max(len(toks) for toks in toks_list)\n    encoded = torch.LongTensor(\n        [[stoi[t] for t in toks] + [0] * (max_len - len(toks)) for toks in toks_list]\n    )\n    # loaders\n    dataset = CodeDataset(encoded, group_to_indices, index_to_gid)\n    train_loader = DataLoader(\n        Subset(dataset, train_indices), batch_size=8, shuffle=True\n    )\n    val_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n    # experiments over epoch counts\n    for E in EPOCH_LIST:\n        model = CodeEncoder(len(stoi), 64, 64).to(device)\n        opt = Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.TripletMarginLoss(margin=1.0)\n        data = {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        for epoch in range(E):\n            model.train()\n            tot = 0.0\n            for a, p, n in train_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                ea, ep, en = model(a), model(p), model(n)\n                l = loss_fn(ea, ep, en)\n                opt.zero_grad()\n                l.backward()\n                opt.step()\n                tot += l.item()\n            data[\"losses\"][\"train\"].append(tot / len(train_loader))\n            # val loss\n            model.eval()\n            tv = 0.0\n            with torch.no_grad():\n                for a, p, n in val_loader:\n                    a, p, n = a.to(device), p.to(device), n.to(device)\n                    tv += loss_fn(model(a), model(p), model(n)).item()\n            data[\"losses\"][\"val\"].append(tv / len(val_loader))\n            # retrieval acc\n            with torch.no_grad():\n                emb = model(encoded.to(device))\n                nm = F.normalize(emb, dim=1)\n                sims = nm @ nm.T\n\n                def acc(idxs):\n                    cnt = 0\n                    for i in idxs:\n                        s = sims[i].clone()\n                        s[i] = -1e9\n                        top = torch.argmax(s).item()\n                        if index_to_gid[top] == index_to_gid[i]:\n                            cnt += 1\n                    return cnt / len(idxs)\n\n                data[\"metrics\"][\"train\"].append(acc(train_indices))\n                data[\"metrics\"][\"val\"].append(acc(val_indices))\n        # final preds\n        model.eval()\n        with torch.no_grad():\n            emb = model(encoded.to(device))\n            nm = F.normalize(emb, dim=1)\n            sims = nm @ nm.T\n            for i in val_indices:\n                s = sims[i].clone()\n                s[i] = -1e9\n                top = torch.argmax(s).item()\n                data[\"predictions\"].append(index_to_gid[top])\n                data[\"ground_truth\"].append(index_to_gid[i])\n        synth_data[E] = data\n        print(f\"Scheme={scheme}, E={E}, final val acc={data['metrics']['val'][-1]:.4f}\")\n    experiment_data[\"tokenization_granularity\"][\"synthetic\"][scheme] = synth_data\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# generate synthetic code snippets and traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: ids for gid, ids in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# encode snippets as character sequences\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\n# dataset & loaders\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# model definition\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# distance function for cosine triplet\ndef cosine_dist(x, y):\n    return 1 - F.cosine_similarity(x, y)\n\n\n# experiment configurations\nEPOCH_LIST = [10, 30, 50]\nablation_types = [\"euclidean\", \"cosine\"]\nexperiment_data = {}\n\nfor ablation in ablation_types:\n    experiment_data[ablation] = {\"synthetic\": {}}\n    for E in EPOCH_LIST:\n        # init model, optimizer, loss\n        model = CodeEncoder(len(stoi), 64, 64).to(device)\n        optimizer = Adam(model.parameters(), lr=1e-3)\n        if ablation == \"euclidean\":\n            loss_fn = nn.TripletMarginLoss(margin=1.0)\n        else:\n            loss_fn = nn.TripletMarginWithDistanceLoss(\n                distance_function=cosine_dist, margin=1.0\n            )\n\n        data = {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # training + validation loop\n        for epoch in range(E):\n            model.train()\n            total_train_loss = 0.0\n            for a, p, n in train_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                emb_a, emb_p, emb_n = model(a), model(p), model(n)\n                if ablation == \"cosine\":\n                    emb_a = F.normalize(emb_a, dim=1)\n                    emb_p = F.normalize(emb_p, dim=1)\n                    emb_n = F.normalize(emb_n, dim=1)\n                loss = loss_fn(emb_a, emb_p, emb_n)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                total_train_loss += loss.item()\n            avg_train_loss = total_train_loss / len(train_loader)\n            data[\"losses\"][\"train\"].append(avg_train_loss)\n\n            model.eval()\n            total_val_loss = 0.0\n            with torch.no_grad():\n                for a, p, n in val_loader:\n                    a, p, n = a.to(device), p.to(device), n.to(device)\n                    emb_a, emb_p, emb_n = model(a), model(p), model(n)\n                    if ablation == \"cosine\":\n                        emb_a = F.normalize(emb_a, dim=1)\n                        emb_p = F.normalize(emb_p, dim=1)\n                        emb_n = F.normalize(emb_n, dim=1)\n                    total_val_loss += loss_fn(emb_a, emb_p, emb_n).item()\n            avg_val_loss = total_val_loss / len(val_loader)\n            data[\"losses\"][\"val\"].append(avg_val_loss)\n\n            # retrieval metrics\n            with torch.no_grad():\n                emb_all = model(encoded.to(device))\n                emb_norm = F.normalize(emb_all, dim=1)\n                sims = emb_norm @ emb_norm.T\n\n                def compute_acc(indices):\n                    correct = 0\n                    for i in indices:\n                        sim = sims[i].clone()\n                        sim[i] = -float(\"inf\")\n                        top1 = torch.argmax(sim).item()\n                        if index_to_gid[top1] == index_to_gid[i]:\n                            correct += 1\n                    return correct / len(indices)\n\n                train_acc = compute_acc(train_indices)\n                val_acc = compute_acc(val_indices)\n            data[\"metrics\"][\"train\"].append(train_acc)\n            data[\"metrics\"][\"val\"].append(val_acc)\n\n        # final predictions\n        model.eval()\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n            for i in val_indices:\n                sim = sims[i].clone()\n                sim[i] = -float(\"inf\")\n                top1 = torch.argmax(sim).item()\n                data[\"predictions\"].append(index_to_gid[top1])\n                data[\"ground_truth\"].append(index_to_gid[i])\n\n        experiment_data[ablation][\"synthetic\"][E] = data\n        print(f\"Ablation={ablation}, E={E}, final val_acc={val_acc:.4f}\")\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# generate synthetic code snippets and traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: idxs for gid, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# encode snippets as character sequences\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded).to(device)\n\n\n# dataset & loaders\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# model definition\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# ablation over embedding & hidden dims\nDIM_LIST = [16, 32, 64, 128, 256]\nEPOCHS = 30\nexperiment_data = {\"embed_hidden\": {\"synthetic\": {}}}\n\nfor dim in DIM_LIST:\n    print(f\"Starting ablation for dim={dim}\")\n    model = CodeEncoder(len(stoi), embed_dim=dim, hidden=dim).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.TripletMarginLoss(margin=1.0)\n\n    data = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    for epoch in range(EPOCHS):\n        model.train()\n        total_train_loss = 0.0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a, emb_p, emb_n = model(a), model(p), model(n)\n            loss = loss_fn(emb_a, emb_p, emb_n)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_train_loss += loss.item()\n        data[\"losses\"][\"train\"].append(total_train_loss / len(train_loader))\n\n        model.eval()\n        total_val_loss = 0.0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                total_val_loss += loss_fn(model(a), model(p), model(n)).item()\n        data[\"losses\"][\"val\"].append(total_val_loss / len(val_loader))\n\n        # compute retrieval accuracy\n        with torch.no_grad():\n            emb_all = model(encoded)\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n\n            def compute_acc(indices):\n                correct = 0\n                for i in indices:\n                    sim = sims[i].clone()\n                    sim[i] = -float(\"inf\")\n                    top1 = torch.argmax(sim).item()\n                    if index_to_gid[top1] == index_to_gid[i]:\n                        correct += 1\n                return correct / len(indices)\n\n            train_acc = compute_acc(train_indices)\n            val_acc = compute_acc(val_indices)\n            data[\"metrics\"][\"train\"].append(train_acc)\n            data[\"metrics\"][\"val\"].append(val_acc)\n\n    # final predictions on validation\n    model.eval()\n    with torch.no_grad():\n        emb_all = model(encoded)\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n        for i in val_indices:\n            sim = sims[i].clone()\n            sim[i] = -float(\"inf\")\n            top1 = torch.argmax(sim).item()\n            data[\"predictions\"].append(index_to_gid[top1])\n            data[\"ground_truth\"].append(index_to_gid[i])\n\n    experiment_data[\"embed_hidden\"][\"synthetic\"][dim] = data\n    print(f\"Finished dim={dim}, final val_acc={data['metrics']['val'][-1]:.4f}\")\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport random\nimport re\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# reproducibility & setup\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# generate synthetic snippets and semantic traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group by semantic equivalence\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: idxs for gid, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# generate one unseen random renaming per snippet\nrenamed_codes = []\nfor s in codes:\n    func = f\"g{random.randint(100,999)}\"\n    var = f\"y{random.randint(100,999)}\"\n    s2 = re.sub(r\"\\bdef\\s+f\\(\", f\"def {func}(\", s)\n    s3 = re.sub(r\"\\bx\\b\", var, s2)\n    renamed_codes.append(s3)\n\n# build joint vocab over original + renamed\nall_text = \"\".join(codes + renamed_codes)\nvocab = sorted(set(all_text))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in (codes + renamed_codes))\n\n\n# helper to encode and pad\ndef encode_list(strs):\n    arr = []\n    for s in strs:\n        seq = [stoi.get(c, 0) for c in s] + [0] * (max_len - len(s))\n        arr.append(seq)\n    return torch.LongTensor(arr)\n\n\nencoded = encode_list(codes)\nencoded_renamed = encode_list(renamed_codes)\n\n# train/val split on groups\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\n\n\n# dataset & loaders\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# model\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# ablation experiment\nEPOCH_LIST = [10, 30, 50]\nexperiment_data = {\"variable_renaming_invariance\": {\"synthetic\": {}}}\n\nfor E in EPOCH_LIST:\n    model = CodeEncoder(len(stoi), 64, 64).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.TripletMarginLoss(margin=1.0)\n\n    data = {\n        \"metrics\": {\"train\": [], \"val\": [], \"rename\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"rename_predictions\": [],\n        \"rename_ground_truth\": [],\n    }\n\n    # train + epochwise eval\n    for epoch in range(E):\n        model.train()\n        tot_tr = 0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            ea, ep, en = model(a), model(p), model(n)\n            loss = loss_fn(ea, ep, en)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            tot_tr += loss.item()\n        data[\"losses\"][\"train\"].append(tot_tr / len(train_loader))\n\n        model.eval()\n        tot_val = 0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                tot_val += loss_fn(model(a), model(p), model(n)).item()\n        data[\"losses\"][\"val\"].append(tot_val / len(val_loader))\n\n        # retrieval acc on original\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n\n            def compute_acc(idxs):\n                corr = 0\n                for i in idxs:\n                    s = sims[i].clone()\n                    s[i] = -1e9\n                    j = torch.argmax(s).item()\n                    if index_to_gid[j] == index_to_gid[i]:\n                        corr += 1\n                return corr / len(idxs)\n\n            tr_acc = compute_acc(train_indices)\n            v_acc = compute_acc(val_indices)\n\n        # retrieval acc on unseen renamings (val only)\n        with torch.no_grad():\n            emb_ren = model(encoded_renamed.to(device))\n            ren_norm = F.normalize(emb_ren, dim=1)\n            sims2 = ren_norm @ emb_norm.T\n            corr2 = 0\n            for i in val_indices:\n                j = torch.argmax(sims2[i]).item()\n                if index_to_gid[j] == index_to_gid[i]:\n                    corr2 += 1\n            ren_acc = corr2 / len(val_indices)\n\n        data[\"metrics\"][\"train\"].append(tr_acc)\n        data[\"metrics\"][\"val\"].append(v_acc)\n        data[\"metrics\"][\"rename\"].append(ren_acc)\n\n    # final predictions\n    model.eval()\n    with torch.no_grad():\n        emb_o = model(encoded.to(device))\n        norm_o = F.normalize(emb_o, dim=1)\n        sims_o = norm_o @ norm_o.T\n        emb_r = model(encoded_renamed.to(device))\n        norm_r = F.normalize(emb_r, dim=1)\n        sims_r = norm_r @ norm_o.T\n\n        for i in val_indices:\n            # original\n            s = sims_o[i].clone()\n            s[i] = -1e9\n            j = torch.argmax(s).item()\n            data[\"predictions\"].append(index_to_gid[j])\n            data[\"ground_truth\"].append(index_to_gid[i])\n            # rename\n            j2 = torch.argmax(sims_r[i]).item()\n            data[\"rename_predictions\"].append(index_to_gid[j2])\n            data[\"rename_ground_truth\"].append(index_to_gid[i])\n\n    experiment_data[\"variable_renaming_invariance\"][\"synthetic\"][E] = data\n    print(\n        f\"Done E={E}: val_acc={data['metrics']['val'][-1]:.4f}, rename_acc={data['metrics']['rename'][-1]:.4f}\"\n    )\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os, random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# synthetic codes & traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, 100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: idxs for gid, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for i in idxs:\n        index_to_gid[i] = gid\n\n# char\u2010level encoding\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = torch.LongTensor(\n    [[stoi[c] for c in s] + [0] * (max_len - len(s)) for s in codes]\n)\n\n\n# dataset & splits\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded, self.group_to_indices, self.index_to_gid = (\n            encoded,\n            group_to_indices,\n            index_to_gid,\n        )\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        a = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        p = idx\n        while p == idx:\n            p = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        n = random.choice(self.group_to_indices[neg_gid])\n        return a, self.encoded[p], self.encoded[n]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# encoder with optional bidirectionality\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64, bidirectional=False):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.bidirectional = bidirectional\n        nh = hidden // (2 if bidirectional else 1)\n        self.lstm = nn.LSTM(\n            embed_dim, nh, batch_first=True, bidirectional=bidirectional\n        )\n\n    def forward(self, x):\n        x = self.embed(x)\n        h, _ = self.lstm(x)  # h: (B, T, D*num_dirs)\n        # get final hidden states\n        _, (hn, _) = self.lstm(x)\n        if self.bidirectional:\n            h1, h2 = hn[0], hn[1]\n            return torch.cat([h1, h2], dim=1)\n        else:\n            return hn.squeeze(0)\n\n\n# ablation study\nvariants = [\"unidirectional\", \"bidirectional\"]\nEPOCH_LIST = [10, 30, 50]\nexperiment_data = {\n    \"bidirectional_lstm_ablation\": {\"synthetic\": {v: {} for v in variants}}\n}\n\nfor variant in variants:\n    is_bi = variant == \"bidirectional\"\n    for E in EPOCH_LIST:\n        model = CodeEncoder(len(stoi), embed_dim=64, hidden=64, bidirectional=is_bi).to(\n            device\n        )\n        optimizer = Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.TripletMarginLoss(margin=1.0)\n\n        data = {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        for epoch in range(E):\n            model.train()\n            tr_loss = 0.0\n            for a, p, n in train_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                ea, ep, en = model(a), model(p), model(n)\n                l = loss_fn(ea, ep, en)\n                optimizer.zero_grad()\n                l.backward()\n                optimizer.step()\n                tr_loss += l.item()\n            data[\"losses\"][\"train\"].append(tr_loss / len(train_loader))\n\n            model.eval()\n            vl = 0.0\n            with torch.no_grad():\n                for a, p, n in val_loader:\n                    a, p, n = a.to(device), p.to(device), n.to(device)\n                    vl += loss_fn(model(a), model(p), model(n)).item()\n            data[\"losses\"][\"val\"].append(vl / len(val_loader))\n\n            # retrieval acc\n            with torch.no_grad():\n                emb_all = model(encoded.to(device))\n                emb_n = F.normalize(emb_all, dim=1)\n                sims = emb_n @ emb_n.T\n\n                def comp_acc(indices):\n                    corr = 0\n                    for i in indices:\n                        s = sims[i].clone()\n                        s[i] = -1e9\n                        top1 = torch.argmax(s).item()\n                        if index_to_gid[top1] == index_to_gid[i]:\n                            corr += 1\n                    return corr / len(indices)\n\n                data[\"metrics\"][\"train\"].append(comp_acc(train_indices))\n                data[\"metrics\"][\"val\"].append(comp_acc(val_indices))\n\n        # final predictions\n        model.eval()\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_n = F.normalize(emb_all, dim=1)\n            sims = emb_n @ emb_n.T\n            for i in val_indices:\n                s = sims[i].clone()\n                s[i] = -1e9\n                top1 = torch.argmax(s).item()\n                data[\"predictions\"].append(index_to_gid[top1])\n                data[\"ground_truth\"].append(index_to_gid[i])\n\n        # convert to numpy arrays\n        data_np = {\n            \"metrics\": {\n                \"train\": np.array(data[\"metrics\"][\"train\"]),\n                \"val\": np.array(data[\"metrics\"][\"val\"]),\n            },\n            \"losses\": {\n                \"train\": np.array(data[\"losses\"][\"train\"]),\n                \"val\": np.array(data[\"losses\"][\"val\"]),\n            },\n            \"predictions\": np.array(data[\"predictions\"]),\n            \"ground_truth\": np.array(data[\"ground_truth\"]),\n        }\n        experiment_data[\"bidirectional_lstm_ablation\"][\"synthetic\"][variant][\n            E\n        ] = data_np\n        print(f\"{variant} E={E} final val acc={data_np['metrics']['val'][-1]:.4f}\")\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# initialize experiment storage\nexperiment_data = {\n    \"dead_code_injection\": {\n        \"synthetic_clean\": {},\n        \"synthetic_injected\": {},\n    }\n}\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# generate base synthetic code snippets and their traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by identical trace (functional equivalence)\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: idxs for gid, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# train/validation split of groups\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\n\n\n# dead code injector utility\ndef inject_dead_code(code):\n    # Turn \"def f(x): return expr\" into a multi-line with no-ops\n    sig, expr = code.split(\": return \")\n    return (\n        f\"{sig}:\\n\"\n        \"    dummy = 0\\n\"\n        \"    for _ in range(1):\\n\"\n        \"        pass\\n\"\n        f\"    return {expr}\"\n    )\n\n\n# dataset class\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\n# model definition\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# hyperparameters\nEPOCH_LIST = [10, 30, 50]\n\n# run both clean and injected versions\nfor dataset_name in [\"synthetic_clean\", \"synthetic_injected\"]:\n    # pick the code variants\n    if dataset_name == \"synthetic_clean\":\n        codes_use = codes\n    else:\n        codes_use = [inject_dead_code(c) for c in codes]\n\n    # build vocabulary and encode\n    vocab = sorted(set(\"\".join(codes_use)))\n    stoi = {c: i + 1 for i, c in enumerate(vocab)}\n    stoi[\"PAD\"] = 0\n    max_len = max(len(s) for s in codes_use)\n    encoded_list = []\n    for s in codes_use:\n        seq = [stoi[ch] for ch in s] + [0] * (max_len - len(s))\n        encoded_list.append(seq)\n    encoded_use = torch.LongTensor(encoded_list)\n\n    # prepare data loaders\n    dataset = CodeDataset(encoded_use, group_to_indices, index_to_gid)\n    train_loader = DataLoader(\n        Subset(dataset, train_indices), batch_size=8, shuffle=True\n    )\n    val_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n    # per-epoch-budget experiments\n    for E in EPOCH_LIST:\n        # init model & optimizer\n        model = CodeEncoder(len(stoi), 64, 64).to(device)\n        optimizer = Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.TripletMarginLoss(margin=1.0)\n        data = {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n\n        # train + validate\n        for epoch in range(E):\n            model.train()\n            total_train_loss = 0.0\n            for a, p, n in train_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                emb_a, emb_p, emb_n = model(a), model(p), model(n)\n                loss = loss_fn(emb_a, emb_p, emb_n)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                total_train_loss += loss.item()\n            data[\"losses\"][\"train\"].append(total_train_loss / len(train_loader))\n\n            model.eval()\n            total_val_loss = 0.0\n            with torch.no_grad():\n                for a, p, n in val_loader:\n                    a, p, n = a.to(device), p.to(device), n.to(device)\n                    total_val_loss += loss_fn(model(a), model(p), model(n)).item()\n            data[\"losses\"][\"val\"].append(total_val_loss / len(val_loader))\n\n            # retrieval accuracy\n            with torch.no_grad():\n                embs = model(encoded_use.to(device))\n                embs = F.normalize(embs, dim=1)\n                sims = embs @ embs.t()\n\n                def compute_acc(idxs):\n                    correct = 0\n                    for i in idxs:\n                        s = sims[i].clone()\n                        s[i] = -float(\"inf\")\n                        top1 = torch.argmax(s).item()\n                        if index_to_gid[top1] == index_to_gid[i]:\n                            correct += 1\n                    return correct / len(idxs)\n\n                data[\"metrics\"][\"train\"].append(compute_acc(train_indices))\n                data[\"metrics\"][\"val\"].append(compute_acc(val_indices))\n\n        # final predictions on validation\n        model.eval()\n        with torch.no_grad():\n            embs = model(encoded_use.to(device))\n            embs = F.normalize(embs, dim=1)\n            sims = embs @ embs.t()\n            for i in val_indices:\n                s = sims[i].clone()\n                s[i] = -float(\"inf\")\n                top1 = torch.argmax(s).item()\n                data[\"predictions\"].append(index_to_gid[top1])\n                data[\"ground_truth\"].append(index_to_gid[i])\n\n        experiment_data[\"dead_code_injection\"][dataset_name][E] = data\n        print(\n            f\"Dataset={dataset_name}, EPOCHS={E}, final val_acc={data['metrics']['val'][-1]:.4f}\"\n        )\n\n# save everything\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\nimport random, re, ast\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# generate synthetic functions and traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group by trace\ntrace_to_indices = {}\nfor idx, tr in enumerate(traces):\n    trace_to_indices.setdefault(tr, []).append(idx)\ngroup_to_indices = {gid: idxs for gid, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# train/val split\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\n\n\n# Dataset\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\n# Encoder\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# tokenizers\ndef char_tokenize(s):\n    return list(s)\n\n\ndef subword_tokenize(s):\n    return re.findall(r\"\\w+|[^\\s\\w]\", s)\n\n\ndef ast_tokenize(s):\n    tokens = []\n\n    def visit(node):\n        if isinstance(node, ast.Constant):\n            tokens.append(f\"Const_{node.value}\")\n        elif isinstance(node, ast.Name):\n            tokens.append(f\"Name_{node.id}\")\n        else:\n            tokens.append(type(node).__name__)\n            for _, v in ast.iter_fields(node):\n                if isinstance(v, list):\n                    for elt in v:\n                        if isinstance(elt, ast.AST):\n                            visit(elt)\n                elif isinstance(v, ast.AST):\n                    visit(v)\n\n    visit(ast.parse(s))\n    return tokens\n\n\nschemes = {\"char\": char_tokenize, \"subword\": subword_tokenize, \"ast\": ast_tokenize}\nEPOCH_LIST = [10, 30, 50]\nexperiment_data = {\"tokenization_granularity\": {\"synthetic\": {}}}\n\nfor scheme, tokfn in schemes.items():\n    toks_list = [tokfn(s) for s in codes]\n    vocab = sorted({t for toks in toks_list for t in toks})\n    stoi = {t: i + 1 for i, t in enumerate(vocab)}\n    stoi[\"PAD\"] = 0\n    max_len = max(len(toks) for toks in toks_list)\n    encoded = torch.LongTensor(\n        [[stoi[t] for t in toks] + [0] * (max_len - len(toks)) for toks in toks_list]\n    )\n    dataset = CodeDataset(encoded, group_to_indices, index_to_gid)\n    train_loader = DataLoader(\n        Subset(dataset, train_indices), batch_size=8, shuffle=True\n    )\n    val_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n    synth_data = {}\n    for E in EPOCH_LIST:\n        model = CodeEncoder(len(stoi), 64, 64).to(device)\n        opt = Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.TripletMarginLoss(margin=1.0)\n        data = {\n            \"metrics\": {\n                \"train_acc\": [],\n                \"val_acc\": [],\n                \"train_alignment_gap\": [],\n                \"val_alignment_gap\": [],\n            },\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        for epoch in range(E):\n            # training\n            model.train()\n            tot_loss = 0.0\n            sum_pos, sum_neg, count = 0.0, 0.0, 0\n            for a, p, n in train_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                ea, ep, en = model(a), model(p), model(n)\n                loss = loss_fn(ea, ep, en)\n                opt.zero_grad()\n                loss.backward()\n                opt.step()\n                tot_loss += loss.item()\n                pos = F.cosine_similarity(ea, ep, dim=1)\n                neg = F.cosine_similarity(ea, en, dim=1)\n                sum_pos += pos.sum().item()\n                sum_neg += neg.sum().item()\n                count += pos.size(0)\n            avg_train_loss = tot_loss / len(train_loader)\n            data[\"losses\"][\"train\"].append(avg_train_loss)\n            data[\"metrics\"][\"train_alignment_gap\"].append(\n                sum_pos / count - sum_neg / count\n            )\n            # train acc\n            model.eval()\n            with torch.no_grad():\n                emb = model(encoded.to(device))\n                nm = F.normalize(emb, dim=1)\n                sims = nm @ nm.T\n\n                def acc(idxs):\n                    cnt = 0\n                    for i in idxs:\n                        s = sims[i].clone()\n                        s[i] = -1e9\n                        top = torch.argmax(s).item()\n                        if index_to_gid[top] == index_to_gid[i]:\n                            cnt += 1\n                    return cnt / len(idxs)\n\n                data[\"metrics\"][\"train_acc\"].append(acc(train_indices))\n            # validation\n            tv_loss = 0.0\n            sum_pos_v, sum_neg_v, count_v = 0.0, 0.0, 0\n            with torch.no_grad():\n                for a, p, n in val_loader:\n                    a, p, n = a.to(device), p.to(device), n.to(device)\n                    ea, ep, en = model(a), model(p), model(n)\n                    tv_loss += loss_fn(ea, ep, en).item()\n                    pos_v = F.cosine_similarity(ea, ep, dim=1)\n                    neg_v = F.cosine_similarity(ea, en, dim=1)\n                    sum_pos_v += pos_v.sum().item()\n                    sum_neg_v += neg_v.sum().item()\n                    count_v += pos_v.size(0)\n            avg_val_loss = tv_loss / len(val_loader)\n            data[\"losses\"][\"val\"].append(avg_val_loss)\n            data[\"metrics\"][\"val_alignment_gap\"].append(\n                sum_pos_v / count_v - sum_neg_v / count_v\n            )\n            print(f\"Epoch {epoch+1}/{E}: validation_loss = {avg_val_loss:.4f}\")\n            with torch.no_grad():\n                nm = F.normalize(model(encoded.to(device)), dim=1)\n                sims = nm @ nm.T\n                data[\"metrics\"][\"val_acc\"].append(acc(val_indices))\n        # final predictions\n        model.eval()\n        with torch.no_grad():\n            emb = model(encoded.to(device))\n            nm = F.normalize(emb, dim=1)\n            sims = nm @ nm.T\n            for i in val_indices:\n                s = sims[i].clone()\n                s[i] = -1e9\n                top = torch.argmax(s).item()\n                data[\"predictions\"].append(index_to_gid[top])\n                data[\"ground_truth\"].append(index_to_gid[i])\n        synth_data[E] = data\n        print(\n            f\"Scheme={scheme}, E={E}, final val acc={data['metrics']['val_acc'][-1]:.4f}\"\n        )\n    experiment_data[\"tokenization_granularity\"][\"synthetic\"][scheme] = synth_data\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic code + traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n# grouping\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: idxs for gid, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for i in idxs:\n        index_to_gid[i] = gid\n\n# encode chars\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\n# dataset\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# model with optional projection head\nclass CodeEncoderWithHead(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64, head_layers=0):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n        self.head = None\n        if head_layers > 0:\n            layers = []\n            for i in range(head_layers):\n                layers.append(nn.Linear(hidden, hidden))\n                layers.append(nn.ReLU(inplace=True))\n            self.head = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        h = h.squeeze(0)\n        return self.head(h) if self.head is not None else h\n\n\n# ablation & training\nEPOCH_LIST = [10, 30, 50]\nHEAD_LIST = [0, 1, 2]\nexperiment_data = {\"projection_head_ablation\": {\"synthetic\": {}}}\n\nfor head_layers in HEAD_LIST:\n    experiment_data[\"projection_head_ablation\"][\"synthetic\"][f\"head_{head_layers}\"] = {}\n    for E in EPOCH_LIST:\n        model = CodeEncoderWithHead(len(stoi), 64, 64, head_layers).to(device)\n        optimizer = Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.TripletMarginLoss(margin=1.0)\n        data = {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        # train/val per epoch\n        for epoch in range(E):\n            model.train()\n            tot_tr = 0.0\n            for a, p, n in train_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                ea, ep, en = model(a), model(p), model(n)\n                loss = loss_fn(ea, ep, en)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                tot_tr += loss.item()\n            data[\"losses\"][\"train\"].append(tot_tr / len(train_loader))\n            model.eval()\n            tot_v = 0.0\n            with torch.no_grad():\n                for a, p, n in val_loader:\n                    a, p, n = a.to(device), p.to(device), n.to(device)\n                    tot_v += loss_fn(model(a), model(p), model(n)).item()\n            data[\"losses\"][\"val\"].append(tot_v / len(val_loader))\n            # retrieval acc\n            with torch.no_grad():\n                emb_all = model(encoded.to(device))\n                normed = F.normalize(emb_all, dim=1)\n                sims = normed @ normed.T\n\n                def acc(indices):\n                    c = 0\n                    for i in indices:\n                        row = sims[i].clone()\n                        row[i] = -1e9\n                        pred = torch.argmax(row).item()\n                        c += index_to_gid[pred] == index_to_gid[i]\n                    return c / len(indices)\n\n                data[\"metrics\"][\"train\"].append(acc(train_indices))\n                data[\"metrics\"][\"val\"].append(acc(val_indices))\n        # final preds\n        model.eval()\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            normed = F.normalize(emb_all, dim=1)\n            sims = normed @ normed.T\n            for i in val_indices:\n                row = sims[i].clone()\n                row[i] = -1e9\n                pred = torch.argmax(row).item()\n                data[\"predictions\"].append(index_to_gid[pred])\n                data[\"ground_truth\"].append(index_to_gid[i])\n        experiment_data[\"projection_head_ablation\"][\"synthetic\"][f\"head_{head_layers}\"][\n            E\n        ] = data\n        print(\n            f\"head={head_layers}, epochs={E}, final val_acc={data['metrics']['val'][-1]:.4f}\"\n        )\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os, random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# working dir\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic data\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninp = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in inp))\n\ntrace_to_idxs = {}\nfor i, t in enumerate(traces):\n    trace_to_idxs.setdefault(t, []).append(i)\ngroup_to_idxs = {g: idxs for g, idxs in enumerate(trace_to_idxs.values())}\nindex_to_gid = [None] * len(codes)\nfor g, idxs in group_to_idxs.items():\n    for i in idxs:\n        index_to_gid[i] = g\n\n# encode chars\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = torch.LongTensor(\n    [[stoi[c] for c in s] + [0] * (max_len - len(s)) for s in codes]\n)\n\n\n# dataset\nclass CodeDataset(Dataset):\n    def __init__(self, enc, group_to_idxs, index_to_gid):\n        self.enc = enc\n        self.g2i = group_to_idxs\n        self.i2g = index_to_gid\n\n    def __len__(self):\n        return len(self.i2g)\n\n    def __getitem__(self, idx):\n        a = self.enc[idx]\n        g = self.i2g[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.g2i[g])\n        neg_g = random.choice([x for x in self.g2i if x != g])\n        neg = random.choice(self.g2i[neg_g])\n        return a, self.enc[pos], self.enc[neg]\n\n\ndataset = CodeDataset(encoded, group_to_idxs, index_to_gid)\nall_gs = list(group_to_idxs.keys())\nrandom.shuffle(all_gs)\nsplit = int(0.8 * len(all_gs))\ntrain_gs, val_gs = all_gs[:split], all_gs[split:]\ntrain_idxs = [i for g in train_gs for i in group_to_idxs[g]]\nval_idxs = [i for g in val_gs for i in group_to_idxs[g]]\ntrain_loader = DataLoader(Subset(dataset, train_idxs), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_idxs), batch_size=8, shuffle=False)\n\n\n# CNN encoder\nclass CodeEncoderCNN(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        embed_dim=64,\n        hidden=64,\n        kernel_sizes=[3, 4, 5],\n        num_filters=64,\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.convs = nn.ModuleList(\n            [\n                nn.Conv1d(\n                    in_channels=embed_dim, out_channels=num_filters, kernel_size=k\n                )\n                for k in kernel_sizes\n            ]\n        )\n        self.fc = nn.Linear(num_filters * len(kernel_sizes), hidden)\n\n    def forward(self, x):\n        x = self.embed(x)  # (B, L, E)\n        x = x.transpose(1, 2)  # (B, E, L)\n        outs = [F.relu(conv(x)) for conv in self.convs]\n        pools = [F.max_pool1d(o, o.size(2)).squeeze(2) for o in outs]\n        cat = torch.cat(pools, dim=1)\n        return self.fc(cat)\n\n\n# experiment scaffolding\nEPOCH_LIST = [10, 30, 50]\nexperiment_data = {\"CNN_ENCODER_ABLATION\": {\"synthetic\": {}}}\n\nfor E in EPOCH_LIST:\n    model = CodeEncoderCNN(len(stoi), 64, 64).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.TripletMarginLoss(margin=1.0)\n    data = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"param_count\": sum(p.numel() for p in model.parameters()),\n    }\n\n    for epoch in range(E):\n        model.train()\n        tr_loss = 0.0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            ea, ep, en = model(a), model(p), model(n)\n            loss = loss_fn(ea, ep, en)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            tr_loss += loss.item()\n        tr_loss /= len(train_loader)\n        data[\"losses\"][\"train\"].append(tr_loss)\n\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                val_loss += loss_fn(model(a), model(p), model(n)).item()\n        val_loss /= len(val_loader)\n        data[\"losses\"][\"val\"].append(val_loss)\n\n        with torch.no_grad():\n            emb = model(encoded.to(device))\n            emb_n = F.normalize(emb, dim=1)\n            sims = emb_n @ emb_n.T\n\n            def acc(idx_list):\n                c = 0\n                for i in idx_list:\n                    s = sims[i].clone()\n                    s[i] = -1e9\n                    top1 = torch.argmax(s).item()\n                    if index_to_gid[top1] == index_to_gid[i]:\n                        c += 1\n                return c / len(idx_list)\n\n            data[\"metrics\"][\"train\"].append(acc(train_idxs))\n            data[\"metrics\"][\"val\"].append(acc(val_idxs))\n\n    # final predictions\n    model.eval()\n    with torch.no_grad():\n        emb = model(encoded.to(device))\n        emb_n = F.normalize(emb, dim=1)\n        sims = emb_n @ emb_n.T\n        for i in val_idxs:\n            s = sims[i].clone()\n            s[i] = -1e9\n            top1 = torch.argmax(s).item()\n            data[\"predictions\"].append(index_to_gid[top1])\n            data[\"ground_truth\"].append(index_to_gid[i])\n\n    experiment_data[\"CNN_ENCODER_ABLATION\"][\"synthetic\"][E] = data\n    print(\n        f\"Done CNN ablation EPOCHS={E}, final val acc={data['metrics']['val'][-1]:.4f}\"\n    )\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic code + traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n# grouping\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: idxs for gid, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for i in idxs:\n        index_to_gid[i] = gid\n\n# encode chars\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\n# dataset\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# model with optional projection head\nclass CodeEncoderWithHead(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64, head_layers=0):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n        self.head = None\n        if head_layers > 0:\n            layers = []\n            for i in range(head_layers):\n                layers.append(nn.Linear(hidden, hidden))\n                layers.append(nn.ReLU(inplace=True))\n            self.head = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        h = h.squeeze(0)\n        return self.head(h) if self.head is not None else h\n\n\n# ablation & training\nEPOCH_LIST = [10, 30, 50]\nHEAD_LIST = [0, 1, 2]\nexperiment_data = {\"projection_head_ablation\": {\"synthetic\": {}}}\n\nfor head_layers in HEAD_LIST:\n    experiment_data[\"projection_head_ablation\"][\"synthetic\"][f\"head_{head_layers}\"] = {}\n    for E in EPOCH_LIST:\n        model = CodeEncoderWithHead(len(stoi), 64, 64, head_layers).to(device)\n        optimizer = Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.TripletMarginLoss(margin=1.0)\n        data = {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        # train/val per epoch\n        for epoch in range(E):\n            model.train()\n            tot_tr = 0.0\n            for a, p, n in train_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                ea, ep, en = model(a), model(p), model(n)\n                loss = loss_fn(ea, ep, en)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                tot_tr += loss.item()\n            data[\"losses\"][\"train\"].append(tot_tr / len(train_loader))\n            model.eval()\n            tot_v = 0.0\n            with torch.no_grad():\n                for a, p, n in val_loader:\n                    a, p, n = a.to(device), p.to(device), n.to(device)\n                    tot_v += loss_fn(model(a), model(p), model(n)).item()\n            data[\"losses\"][\"val\"].append(tot_v / len(val_loader))\n            # retrieval acc\n            with torch.no_grad():\n                emb_all = model(encoded.to(device))\n                normed = F.normalize(emb_all, dim=1)\n                sims = normed @ normed.T\n\n                def acc(indices):\n                    c = 0\n                    for i in indices:\n                        row = sims[i].clone()\n                        row[i] = -1e9\n                        pred = torch.argmax(row).item()\n                        c += index_to_gid[pred] == index_to_gid[i]\n                    return c / len(indices)\n\n                data[\"metrics\"][\"train\"].append(acc(train_indices))\n                data[\"metrics\"][\"val\"].append(acc(val_indices))\n        # final preds\n        model.eval()\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            normed = F.normalize(emb_all, dim=1)\n            sims = normed @ normed.T\n            for i in val_indices:\n                row = sims[i].clone()\n                row[i] = -1e9\n                pred = torch.argmax(row).item()\n                data[\"predictions\"].append(index_to_gid[pred])\n                data[\"ground_truth\"].append(index_to_gid[i])\n        experiment_data[\"projection_head_ablation\"][\"synthetic\"][f\"head_{head_layers}\"][\n            E\n        ] = data\n        print(\n            f\"head={head_layers}, epochs={E}, final val_acc={data['metrics']['val'][-1]:.4f}\"\n        )\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic code + traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n# grouping\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: idxs for gid, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for i in idxs:\n        index_to_gid[i] = gid\n\n# encode chars\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\n# dataset\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# model with optional projection head\nclass CodeEncoderWithHead(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64, head_layers=0):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n        self.head = None\n        if head_layers > 0:\n            layers = []\n            for i in range(head_layers):\n                layers.append(nn.Linear(hidden, hidden))\n                layers.append(nn.ReLU(inplace=True))\n            self.head = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        h = h.squeeze(0)\n        return self.head(h) if self.head is not None else h\n\n\n# ablation & training\nEPOCH_LIST = [10, 30, 50]\nHEAD_LIST = [0, 1, 2]\nexperiment_data = {\"projection_head_ablation\": {\"synthetic\": {}}}\n\nfor head_layers in HEAD_LIST:\n    experiment_data[\"projection_head_ablation\"][\"synthetic\"][f\"head_{head_layers}\"] = {}\n    for E in EPOCH_LIST:\n        model = CodeEncoderWithHead(len(stoi), 64, 64, head_layers).to(device)\n        optimizer = Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.TripletMarginLoss(margin=1.0)\n        data = {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        # train/val per epoch\n        for epoch in range(E):\n            model.train()\n            tot_tr = 0.0\n            for a, p, n in train_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                ea, ep, en = model(a), model(p), model(n)\n                loss = loss_fn(ea, ep, en)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                tot_tr += loss.item()\n            data[\"losses\"][\"train\"].append(tot_tr / len(train_loader))\n            model.eval()\n            tot_v = 0.0\n            with torch.no_grad():\n                for a, p, n in val_loader:\n                    a, p, n = a.to(device), p.to(device), n.to(device)\n                    tot_v += loss_fn(model(a), model(p), model(n)).item()\n            data[\"losses\"][\"val\"].append(tot_v / len(val_loader))\n            # retrieval acc\n            with torch.no_grad():\n                emb_all = model(encoded.to(device))\n                normed = F.normalize(emb_all, dim=1)\n                sims = normed @ normed.T\n\n                def acc(indices):\n                    c = 0\n                    for i in indices:\n                        row = sims[i].clone()\n                        row[i] = -1e9\n                        pred = torch.argmax(row).item()\n                        c += index_to_gid[pred] == index_to_gid[i]\n                    return c / len(indices)\n\n                data[\"metrics\"][\"train\"].append(acc(train_indices))\n                data[\"metrics\"][\"val\"].append(acc(val_indices))\n        # final preds\n        model.eval()\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            normed = F.normalize(emb_all, dim=1)\n            sims = normed @ normed.T\n            for i in val_indices:\n                row = sims[i].clone()\n                row[i] = -1e9\n                pred = torch.argmax(row).item()\n                data[\"predictions\"].append(index_to_gid[pred])\n                data[\"ground_truth\"].append(index_to_gid[i])\n        experiment_data[\"projection_head_ablation\"][\"synthetic\"][f\"head_{head_layers}\"][\n            E\n        ] = data\n        print(\n            f\"head={head_layers}, epochs={E}, final val_acc={data['metrics']['val'][-1]:.4f}\"\n        )\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic code + traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n# grouping\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: idxs for gid, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for i in idxs:\n        index_to_gid[i] = gid\n\n# encode chars\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\n# dataset\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# model with optional projection head\nclass CodeEncoderWithHead(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64, head_layers=0):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n        self.head = None\n        if head_layers > 0:\n            layers = []\n            for i in range(head_layers):\n                layers.append(nn.Linear(hidden, hidden))\n                layers.append(nn.ReLU(inplace=True))\n            self.head = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        h = h.squeeze(0)\n        return self.head(h) if self.head is not None else h\n\n\n# ablation & training\nEPOCH_LIST = [10, 30, 50]\nHEAD_LIST = [0, 1, 2]\nexperiment_data = {\"projection_head_ablation\": {\"synthetic\": {}}}\n\nfor head_layers in HEAD_LIST:\n    experiment_data[\"projection_head_ablation\"][\"synthetic\"][f\"head_{head_layers}\"] = {}\n    for E in EPOCH_LIST:\n        model = CodeEncoderWithHead(len(stoi), 64, 64, head_layers).to(device)\n        optimizer = Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.TripletMarginLoss(margin=1.0)\n        data = {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        # train/val per epoch\n        for epoch in range(E):\n            model.train()\n            tot_tr = 0.0\n            for a, p, n in train_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                ea, ep, en = model(a), model(p), model(n)\n                loss = loss_fn(ea, ep, en)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                tot_tr += loss.item()\n            data[\"losses\"][\"train\"].append(tot_tr / len(train_loader))\n            model.eval()\n            tot_v = 0.0\n            with torch.no_grad():\n                for a, p, n in val_loader:\n                    a, p, n = a.to(device), p.to(device), n.to(device)\n                    tot_v += loss_fn(model(a), model(p), model(n)).item()\n            data[\"losses\"][\"val\"].append(tot_v / len(val_loader))\n            # retrieval acc\n            with torch.no_grad():\n                emb_all = model(encoded.to(device))\n                normed = F.normalize(emb_all, dim=1)\n                sims = normed @ normed.T\n\n                def acc(indices):\n                    c = 0\n                    for i in indices:\n                        row = sims[i].clone()\n                        row[i] = -1e9\n                        pred = torch.argmax(row).item()\n                        c += index_to_gid[pred] == index_to_gid[i]\n                    return c / len(indices)\n\n                data[\"metrics\"][\"train\"].append(acc(train_indices))\n                data[\"metrics\"][\"val\"].append(acc(val_indices))\n        # final preds\n        model.eval()\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            normed = F.normalize(emb_all, dim=1)\n            sims = normed @ normed.T\n            for i in val_indices:\n                row = sims[i].clone()\n                row[i] = -1e9\n                pred = torch.argmax(row).item()\n                data[\"predictions\"].append(index_to_gid[pred])\n                data[\"ground_truth\"].append(index_to_gid[i])\n        experiment_data[\"projection_head_ablation\"][\"synthetic\"][f\"head_{head_layers}\"][\n            E\n        ] = data\n        print(\n            f\"head={head_layers}, epochs={E}, final val_acc={data['metrics']['val'][-1]:.4f}\"\n        )\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Finished EPOCHS=10: final val_acc=1.0000', '\\n',\n'Finished EPOCHS=30: final val_acc=1.0000', '\\n', 'Finished EPOCHS=50: final\nval_acc=1.0000', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 3\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Dataset=arith, EPOCHS=10, final val_acc=0.7500',\n'\\n', 'Dataset=arith, EPOCHS=30, final val_acc=1.0000', '\\n', 'Dataset=arith,\nEPOCHS=50, final val_acc=1.0000', '\\n', 'Dataset=branch, EPOCHS=10, final\nval_acc=0.0000', '\\n', 'Dataset=branch, EPOCHS=30, final val_acc=0.0000', '\\n',\n'Dataset=branch, EPOCHS=50, final val_acc=0.5000', '\\n', 'Dataset=loop,\nEPOCHS=10, final val_acc=0.0000', '\\n', 'Dataset=loop, EPOCHS=30, final\nval_acc=0.0000', '\\n', 'Dataset=loop, EPOCHS=50, final val_acc=0.0000', '\\n',\n'Saved experiment_data.npy', '\\n', 'Execution time: 8 seconds seconds (time\nlimit is an hour).']", "['Using device: cuda', '\\n', 'Starting ablation: random_negative', '\\n',\n'Finished random_negative, EPOCHS=10: final val_acc=0.7500', '\\n', 'Finished\nrandom_negative, EPOCHS=30: final val_acc=1.0000', '\\n', 'Finished\nrandom_negative, EPOCHS=50: final val_acc=1.0000', '\\n', 'Starting ablation:\nhard_negative', '\\n', 'Finished hard_negative, EPOCHS=10: final val_acc=0.7500',\n'\\n', 'Finished hard_negative, EPOCHS=30: final val_acc=1.0000', '\\n', 'Finished\nhard_negative, EPOCHS=50: final val_acc=1.0000', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time limit is an\nhour).']", "['Finished lstm, E=10: final val_acc=1.0000', '\\n', 'Finished lstm, E=30: final\nval_acc=1.0000', '\\n', 'Finished lstm, E=50: final val_acc=1.0000', '\\n',\n'Traceback (most recent call last):\\n  File \"runfile.py\", line 126, in\n<module>\\n    model = Encoder(\\n            ^^^^^^^^\\nTypeError:\nMeanPoolEncoder.__init__() got an unexpected keyword argument \\'hidden\\'\\n',\n'Execution time: 3 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished margin=0.1: final val_acc=1.0000', '\\n',\n'Finished margin=0.5: final val_acc=1.0000', '\\n', 'Finished margin=1.0: final\nval_acc=1.0000', '\\n', 'Finished margin=2.0: final val_acc=1.0000', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 4 seconds seconds (time limit is an\nhour).']", "['triplet E=10 val_acc=1.0000', '\\n', 'triplet E=30 val_acc=1.0000', '\\n',\n'triplet E=50 val_acc=1.0000', '\\n', 'contrastive E=10 val_acc=1.0000', '\\n',\n'contrastive E=30 val_acc=1.0000', '\\n', 'contrastive E=50 val_acc=1.0000',\n'\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds\n(time limit is an hour).']", "['Using device: cuda', '\\n', 'lstm Epoch 1: validation_loss = 0.9474, val_acc =\n0.5000, gap_val = 0.0759', '\\n', 'lstm Epoch 2: validation_loss = 0.5846,\nval_acc = 0.5000, gap_val = 0.3619', '\\n', 'lstm Epoch 3: validation_loss =\n0.7915, val_acc = 0.5000, gap_val = 0.1634', '\\n', 'lstm Epoch 4:\nvalidation_loss = 0.6438, val_acc = 0.5000, gap_val = 0.3036', '\\n', 'lstm Epoch\n5: validation_loss = 0.6687, val_acc = 0.5000, gap_val = 0.2944', '\\n', 'lstm\nEpoch 6: validation_loss = 0.6491, val_acc = 0.7500, gap_val = 0.3000', '\\n',\n'lstm Epoch 7: validation_loss = 0.4976, val_acc = 1.0000, gap_val = 0.3701',\n'\\n', 'lstm Epoch 8: validation_loss = 0.6073, val_acc = 1.0000, gap_val =\n0.3513', '\\n', 'lstm Epoch 9: validation_loss = 0.4626, val_acc = 1.0000,\ngap_val = 0.3934', '\\n', 'lstm Epoch 10: validation_loss = 0.4970, val_acc =\n1.0000, gap_val = 0.3250', '\\n', 'Finished lstm, E=10: final val_acc=1.0000',\n'\\n', 'lstm Epoch 1: validation_loss = 1.0329, val_acc = 0.5000, gap_val =\n0.0130', '\\n', 'lstm Epoch 2: validation_loss = 0.7099, val_acc = 0.5000,\ngap_val = 0.2342', '\\n', 'lstm Epoch 3: validation_loss = 0.8992, val_acc =\n0.5000, gap_val = 0.1147', '\\n', 'lstm Epoch 4: validation_loss = 0.6250,\nval_acc = 0.5000, gap_val = 0.4041', '\\n', 'lstm Epoch 5: validation_loss =\n0.5932, val_acc = 0.7500, gap_val = 0.4020', '\\n', 'lstm Epoch 6:\nvalidation_loss = 0.5932, val_acc = 1.0000, gap_val = 0.3718', '\\n', 'lstm Epoch\n7: validation_loss = 0.6672, val_acc = 1.0000, gap_val = 0.3145', '\\n', 'lstm\nEpoch 8: validation_loss = 0.3296, val_acc = 1.0000, gap_val = 0.6182', '\\n',\n'lstm Epoch 9: validation_loss = 0.5493, val_acc = 1.0000, gap_val = 0.3937',\n'\\n', 'lstm Epoch 10: validation_loss = 0.4379, val_acc = 1.0000, gap_val =\n0.4649', '\\n', 'lstm Epoch 11: validation_loss = 0.4149, val_acc = 1.0000,\ngap_val = 0.4105', '\\n', 'lstm Epoch 12: validation_loss = 0.2801, val_acc =\n1.0000, gap_val = 0.4415', '\\n', 'lstm Epoch 13: validation_loss = 0.0701,\nval_acc = 1.0000, gap_val = 0.5516', '\\n', 'lstm Epoch 14: validation_loss =\n0.2190, val_acc = 1.0000, gap_val = 0.2803', '\\n', 'lstm Epoch 15:\nvalidation_loss = 0.1182, val_acc = 1.0000, gap_val = 0.4009', '\\n', 'lstm Epoch\n16: validation_loss = 0.0000, val_acc = 1.0000, gap_val = 0.4908', '\\n', 'lstm\nEpoch 17: validation_loss = 0.0000, val_acc = 1.0000, gap_val = 0.5721', '\\n',\n'lstm Epoch 18: validation_loss = 0.0000, val_acc = 1.0000, gap_val = 0.4172',\n'\\n', 'lstm Epoch 19: validation_loss = 0.0000, val_acc = 1.0000, gap_val =\n0.4973', '\\n', 'lstm Epoch 20: validation_loss = 0.0000, val_acc = 1.0000,\ngap_val = 0.4501', '\\n', 'lstm Epoch 21: validation_loss = 0.0000, val_acc =\n1.0000, gap_val = 0.3766', '\\n', 'lstm Epoch 22: validation_loss = 0.0000,\nval_acc = 1.0000, gap_val = 0.5208', '\\n', 'lstm Epoch 23: validation_loss =\n0.0000, val_acc = 1.0000, gap_val = 0.5758', '\\n', 'lstm Epoch 24:\nvalidation_loss = 0.0000, val_acc = 1.0000, gap_val = 0.4055', '\\n', 'lstm Epoch\n25: validation_loss = 0.0000, val_acc = 1.0000, gap_val = 0.4242', '\\n', 'lstm\nEpoch 26: validation_loss = 0.0000, val_acc = 1.0000, gap_val = 0.3926', '\\n',\n'lstm Epoch 27: validation_loss = 0.0000, val_acc = 1.0000, gap_val = 0.3362',\n'\\n', 'lstm Epoch 28: validation_loss = 0.0000, val_acc = 1.0000, gap_val =\n0.5150', '\\n', 'lstm Epoch 29: validation_loss = 0.0000, val_acc = 1.0000,\ngap_val = 0.5181', '\\n', 'lstm Epoch 30: validation_loss = 0.0000, val_acc =\n1.0000, gap_val = 0.4213', '\\n', 'Finished lstm, E=30: final val_acc=1.0000',\n'\\n', 'lstm Epoch 1: validation_loss = 0.9798, val_acc = 0.5000, gap_val =\n0.0284', '\\n', 'lstm Epoch 2: validation_loss = 0.9555, val_acc = 0.5000,\ngap_val = 0.0729', '\\n', 'lstm Epoch 3: validation_loss = 0.7228, val_acc =\n0.5000, gap_val = 0.1905', '\\n', 'lstm Epoch 4: validation_loss = 0.9266,\nval_acc = 0.5000, gap_val = 0.1076', '\\n', 'lstm Epoch 5: validation_loss =\n0.6719, val_acc = 0.5000, gap_val = 0.3226', '\\n', 'lstm Epoch 6:\nvalidation_loss = 0.7405, val_acc = 0.5000, gap_val = 0.2581', '\\n', 'lstm Epoch\n7: validation_loss = 0.6462, val_acc = 0.7500, gap_val = 0.1877', '\\n', 'lstm\nEpoch 8: validation_loss = 0.4450, val_acc = 1.0000, gap_val = 0.2181', '\\n',\n'lstm Epoch 9: validation_loss = 0.3385, val_acc = 1.0000, gap_val = 0.3397',\n'\\n', 'lstm Epoch 10: validation_loss = 0.2966, val_acc = 1.0000, gap_val =\n0.4608', '\\n', 'lstm Epoch 11: validation_loss = 0.3095, val_acc = 1.0000,\ngap_val = 0.4480', '\\n', 'lstm Epoch 12: validation_loss = 0.2877, val_acc =\n1.0000, gap_val = 0.3383', '\\n', 'lstm Epoch 13: validation_loss = 0.0597,\nval_acc = 1.0000, gap_val = 0.6025', '\\n', 'lstm Epoch 14: validation_loss =\n0.0614, val_acc = 1.0000, gap_val = 0.4122', '\\n', 'lstm Epoch 15:\nvalidation_loss = 0.0599, val_acc = 1.0000, gap_val = 0.5114', '\\n', 'lstm Epoch\n16: validation_loss = 0.0776, val_acc = 1.0000, gap_val = 0.4155', '\\n', 'lstm\nEpoch 17: validation_loss = 0.0437, val_acc = 1.0000, gap_val = 0.3511', '\\n',\n'lstm Epoch 18: validation_loss = 0.0040, val_acc = 1.0000, gap_val = 0.3830',\n'\\n', 'lstm Epoch 19: validation_loss = 0.0153, val_acc = 1.0000, gap_val =\n0.3502', '\\n', 'lstm Epoch 20: validation_loss = 0.0360, val_acc = 1.0000,\ngap_val = 0.3011', '\\n', 'lstm Epoch 21: validation_loss = 0.0000, val_acc =\n1.0000, gap_val = 0.3876', '\\n', 'lstm Epoch 22: validation_loss = 0.0213,\nval_acc = 1.0000, gap_val = 0.2460', '\\n', 'lstm Epoch 23: validation_loss =\n0.0447, val_acc = 1.0000, gap_val = 0.2625', '\\n', 'lstm Epoch 24:\nvalidation_loss = 0.0000, val_acc = 1.0000, gap_val = 0.3886', '\\n', 'lstm Epoch\n25: validation_loss = 0.0000, val_acc = 1.0000, gap_val = 0.3309', '\\n', 'lstm\nEpoch 26: validation_loss = 0.0031, val_acc = 1.0000, gap_val = 0.2862', '\\n',\n'lstm Epoch 27: validation_loss = 0.0129, val_acc = 1.0000, gap_val = 0.3259',\n'\\n', 'lstm Epoch 28: validation_loss = 0.0000, val_acc = 1.0000, gap_val =\n0.4504', '\\n', 'lstm Epoch 29: validation_loss = 0.0000, val_acc = 1.0000,\ngap_val = 0.3447', '\\n', 'lstm Epoch 30: validation_loss = 0.0289, val_acc =\n1.0000, gap_val = 0.3089', '\\n', 'lstm Epoch 31: validation_loss = 0.0000,\nval_acc = 1.0000, gap_val = 0.2837', '\\n', 'lstm Epoch 32: validation_loss =\n0.0000, val_acc = 1.0000, gap_val = 0.3219', '\\n', 'lstm Epoch 33:\nvalidation_loss = 0.0000, val_acc = 1.0000, gap_val = 0.2562', '\\n', 'lstm Epoch\n34: validation_loss = 0.0000, val_acc = 1.0000, gap_val = 0.3128', '\\n', 'lstm\nEpoch 35: validation_loss = 0.0000, val_acc = 1.0000, gap_val = 0.3792', '\\n',\n'lstm Epoch 36: validation_loss = 0.0000, val_acc = 1.0000, gap_val = 0.2944',\n'\\n', 'lstm Epoch 37: validation_loss = 0.0110, val_acc = 1.0000, gap_val =\n0.2856', '\\n', 'lstm Epoch 38: validation_loss = 0.0000, val_acc = 1.0000,\ngap_val = 0.4300', '\\n', 'lstm Epoch 39: validation_loss = 0.0000, val_acc =\n1.0000, gap_val = 0.5275', '\\n', 'lstm Epoch 40: validation_loss = 0.0000,\nval_acc = 1.0000, gap_val = 0.3054', '\\n', 'lstm Epoch 41: validation_loss =\n0.0103, val_acc = 1.0000, gap_val = 0.3475', '\\n', 'lstm Epoch 42:\nvalidation_loss = 0.0000, val_acc = 1.0000, gap_val = 0.5139', '\\n', 'lstm Epoch\n43: validation_loss = 0.0391, val_acc = 1.0000, gap_val = 0.2864', '\\n', 'lstm\nEpoch 44: validation_loss = 0.0000, val_acc = 1.0000, gap_val = 0.3766', '\\n',\n'lstm Epoch 45: validation_loss = 0.0000, val_acc = 1.0000, gap_val = 0.5663',\n'\\n', 'lstm Epoch 46: validation_loss = 0.0000, val_acc = 1.0000, gap_val =\n0.4060', '\\n', 'lstm Epoch 47: validation_loss = 0.0000, val_acc = 1.0000,\ngap_val = 0.4010', '\\n', 'lstm Epoch 48: validation_loss = 0.0265, val_acc =\n1.0000, gap_val = 0.4070', '\\n', 'lstm Epoch 49: validation_loss = 0.0580,\nval_acc = 1.0000, gap_val = 0.4132', '\\n', 'lstm Epoch 50: validation_loss =\n0.0587, val_acc = 1.0000, gap_val = 0.3865', '\\n', 'Finished lstm, E=50: final\nval_acc=1.0000', '\\n', 'mean_pool Epoch 1: validation_loss = 0.4627, val_acc =\n1.0000, gap_val = 0.0216', '\\n', 'mean_pool Epoch 2: validation_loss = 0.3982,\nval_acc = 1.0000, gap_val = 0.0259', '\\n', 'mean_pool Epoch 3: validation_loss =\n0.4185, val_acc = 1.0000, gap_val = 0.0244', '\\n', 'mean_pool Epoch 4:\nvalidation_loss = 0.4575, val_acc = 1.0000, gap_val = 0.0217', '\\n', 'mean_pool\nEpoch 5: validation_loss = 0.4743, val_acc = 1.0000, gap_val = 0.0204', '\\n',\n'mean_pool Epoch 6: validation_loss = 0.4132, val_acc = 1.0000, gap_val =\n0.0247', '\\n', 'mean_pool Epoch 7: validation_loss = 0.4379, val_acc = 1.0000,\ngap_val = 0.0226', '\\n', 'mean_pool Epoch 8: validation_loss = 0.4359, val_acc =\n1.0000, gap_val = 0.0236', '\\n', 'mean_pool Epoch 9: validation_loss = 0.4302,\nval_acc = 1.0000, gap_val = 0.0232', '\\n', 'mean_pool Epoch 10: validation_loss\n= 0.4066, val_acc = 1.0000, gap_val = 0.0253', '\\n', 'Finished mean_pool, E=10:\nfinal val_acc=1.0000', '\\n', 'mean_pool Epoch 1: validation_loss = 0.4420,\nval_acc = 1.0000, gap_val = 0.0346', '\\n', 'mean_pool Epoch 2: validation_loss =\n0.4043, val_acc = 1.0000, gap_val = 0.0397', '\\n', 'mean_pool Epoch 3:\nvalidation_loss = 0.4188, val_acc = 1.0000, gap_val = 0.0368', '\\n', 'mean_pool\nEpoch 4: validation_loss = 0.4135, val_acc = 1.0000, gap_val = 0.0372', '\\n',\n'mean_pool Epoch 5: validation_loss = 0.4452, val_acc = 1.0000, gap_val =\n0.0341', '\\n', 'mean_pool Epoch 6: validation_loss = 0.4416, val_acc = 1.0000,\ngap_val = 0.0342', '\\n', 'mean_pool Epoch 7: validation_loss = 0.4305, val_acc =\n1.0000, gap_val = 0.0349', '\\n', 'mean_pool Epoch 8: validation_loss = 0.3672,\nval_acc = 1.0000, gap_val = 0.0429', '\\n', 'mean_pool Epoch 9: validation_loss =\n0.4391, val_acc = 1.0000, gap_val = 0.0341', '\\n', 'mean_pool Epoch 10:\nvalidation_loss = 0.4620, val_acc = 1.0000, gap_val = 0.0315', '\\n', 'mean_pool\nEpoch 11: validation_loss = 0.4386, val_acc = 1.0000, gap_val = 0.0347', '\\n',\n'mean_pool Epoch 12: validation_loss = 0.4007, val_acc = 1.0000, gap_val =\n0.0381', '\\n', 'mean_pool Epoch 13: validation_loss = 0.4072, val_acc = 1.0000,\ngap_val = 0.0369', '\\n', 'mean_pool Epoch 14: validation_loss = 0.4512, val_acc\n= 1.0000, gap_val = 0.0319', '\\n', 'mean_pool Epoch 15: validation_loss =\n0.3952, val_acc = 1.0000, gap_val = 0.0380', '\\n', 'mean_pool Epoch 16:\nvalidation_loss = 0.4545, val_acc = 1.0000, gap_val = 0.0316', '\\n', 'mean_pool\nEpoch 17: validation_loss = 0.4659, val_acc = 1.0000, gap_val = 0.0310', '\\n',\n'mean_pool Epoch 18: validation_loss = 0.4578, val_acc = 1.0000, gap_val =\n0.0311', '\\n', 'mean_pool Epoch 19: validation_loss = 0.3807, val_acc = 1.0000,\ngap_val = 0.0400', '\\n', 'mean_pool Epoch 20: validation_loss = 0.3941, val_acc\n= 1.0000, gap_val = 0.0389', '\\n', 'mean_pool Epoch 21: validation_loss =\n0.3682, val_acc = 1.0000, gap_val = 0.0411', '\\n', 'mean_pool Epoch 22:\nvalidation_loss = 0.3940, val_acc = 1.0000, gap_val = 0.0381', '\\n', 'mean_pool\nEpoch 23: validation_loss = 0.4450, val_acc = 1.0000, gap_val = 0.0318', '\\n',\n'mean_pool Epoch 24: validation_loss = 0.3600, val_acc = 1.0000, gap_val =\n0.0417', '\\n', 'mean_pool Epoch 25: validation_loss = 0.4129, val_acc = 1.0000,\ngap_val = 0.0368', '\\n', 'mean_pool Epoch 26: validation_loss = 0.4202, val_acc\n= 1.0000, gap_val = 0.0339', '\\n', 'mean_pool Epoch 27: validation_loss =\n0.3844, val_acc = 1.0000, gap_val = 0.0381', '\\n', 'mean_pool Epoch 28:\nvalidation_loss = 0.4221, val_acc = 1.0000, gap_val = 0.0337', '\\n', 'mean_pool\nEpoch 29: validation_loss = 0.4219, val_acc = 1.0000, gap_val = 0.0340', '\\n',\n'mean_pool Epoch 30: validation_loss = 0.4406, val_acc = 1.0000, gap_val =\n0.0318', '\\n', 'Finished mean_pool, E=30: final val_acc=1.0000', '\\n',\n'mean_pool Epoch 1: validation_loss = 0.4759, val_acc = 1.0000, gap_val =\n0.0225', '\\n', 'mean_pool Epoch 2: validation_loss = 0.5192, val_acc = 1.0000,\ngap_val = 0.0188', '\\n', 'mean_pool Epoch 3: validation_loss = 0.4632, val_acc =\n1.0000, gap_val = 0.0233', '\\n', 'mean_pool Epoch 4: validation_loss = 0.4642,\nval_acc = 1.0000, gap_val = 0.0231', '\\n', 'mean_pool Epoch 5: validation_loss =\n0.4227, val_acc = 1.0000, gap_val = 0.0262', '\\n', 'mean_pool Epoch 6:\nvalidation_loss = 0.4694, val_acc = 1.0000, gap_val = 0.0229', '\\n', 'mean_pool\nEpoch 7: validation_loss = 0.4783, val_acc = 1.0000, gap_val = 0.0223', '\\n',\n'mean_pool Epoch 8: validation_loss = 0.5100, val_acc = 1.0000, gap_val =\n0.0192', '\\n', 'mean_pool Epoch 9: validation_loss = 0.4601, val_acc = 1.0000,\ngap_val = 0.0229', '\\n', 'mean_pool Epoch 10: validation_loss = 0.4529, val_acc\n= 1.0000, gap_val = 0.0238', '\\n', 'mean_pool Epoch 11: validation_loss =\n0.4311, val_acc = 1.0000, gap_val = 0.0253', '\\n', 'mean_pool Epoch 12:\nvalidation_loss = 0.4515, val_acc = 1.0000, gap_val = 0.0239', '\\n', 'mean_pool\nEpoch 13: validation_loss = 0.4706, val_acc = 1.0000, gap_val = 0.0228', '\\n',\n'mean_pool Epoch 14: validation_loss = 0.5201, val_acc = 1.0000, gap_val =\n0.0177', '\\n', 'mean_pool Epoch 15: validation_loss = 0.4017, val_acc = 1.0000,\ngap_val = 0.0279', '\\n', 'mean_pool Epoch 16: validation_loss = 0.4671, val_acc\n= 1.0000, gap_val = 0.0232', '\\n', 'mean_pool Epoch 17: validation_loss =\n0.4844, val_acc = 1.0000, gap_val = 0.0204', '\\n', 'mean_pool Epoch 18:\nvalidation_loss = 0.4418, val_acc = 1.0000, gap_val = 0.0247', '\\n', 'mean_pool\nEpoch 19: validation_loss = 0.4241, val_acc = 1.0000, gap_val = 0.0256', '\\n',\n'mean_pool Epoch 20: validation_loss = 0.4230, val_acc = 1.0000, gap_val =\n0.0254', '\\n', 'mean_pool Epoch 21: validation_loss = 0.4340, val_acc = 1.0000,\ngap_val = 0.0251', '\\n', 'mean_pool Epoch 22: validation_loss = 0.4440, val_acc\n= 1.0000, gap_val = 0.0243', '\\n', 'mean_pool Epoch 23: validation_loss =\n0.3946, val_acc = 1.0000, gap_val = 0.0294', '\\n', 'mean_pool Epoch 24:\nvalidation_loss = 0.4615, val_acc = 1.0000, gap_val = 0.0224', '\\n', 'mean_pool\nEpoch 25: validation_loss = 0.4675, val_acc = 1.0000, gap_val = 0.0227', '\\n',\n'mean_pool Epoch 26: validation_loss = 0.4214, val_acc = 1.0000, gap_val =\n0.0268', '\\n', 'mean_pool Epoch 27: validation_loss = 0.4382, val_acc = 1.0000,\ngap_val = 0.0249', '\\n', 'mean_pool Epoch 28: validation_loss = 0.4623, val_acc\n= 1.0000, gap_val = 0.0217', '\\n', 'mean_pool Epoch 29: validation_loss =\n0.4600, val_acc = 1.0000, gap_val = 0.0230', '\\n', 'mean_pool Epoch 30:\nvalidation_loss = 0.4621, val_acc = 1.0000, gap_val = 0.0223', '\\n', 'mean_pool\nEpoch 31: validation_loss = 0.4213, val_acc = 1.0000, gap_val = 0.0261', '\\n',\n'mean_pool Epoch 32: validation_loss = 0.4230, val_acc = 1.0000, gap_val =\n0.0254', '\\n', 'mean_pool Epoch 33: validation_loss = 0.4340, val_acc = 1.0000,\ngap_val = 0.0255', '\\n', 'mean_pool Epoch 34: validation_loss = 0.4645, val_acc\n= 1.0000, gap_val = 0.0210', '\\n', 'mean_pool Epoch 35: validation_loss =\n0.4483, val_acc = 1.0000, gap_val = 0.0234', '\\n', 'mean_pool Epoch 36:\nvalidation_loss = 0.4339, val_acc = 1.0000, gap_val = 0.0250', '\\n', 'mean_pool\nEpoch 37: validation_loss = 0.3915, val_acc = 1.0000, gap_val = 0.0275', '\\n',\n'mean_pool Epoch 38: validation_loss = 0.4364, val_acc = 1.0000, gap_val =\n0.0243', '\\n', 'mean_pool Epoch 39: validation_loss = 0.4508, val_acc = 1.0000,\ngap_val = 0.0235', '\\n', 'mean_pool Epoch 40: validation_loss = 0.4476, val_acc\n= 1.0000, gap_val = 0.0229', '\\n', 'mean_pool Epoch 41: validation_loss =\n0.4375, val_acc = 1.0000, gap_val = 0.0234', '\\n', 'mean_pool Epoch 42:\nvalidation_loss = 0.4462, val_acc = 1.0000, gap_val = 0.0234', '\\n', 'mean_pool\nEpoch 43: validation_loss = 0.4024, val_acc = 1.0000, gap_val = 0.0275', '\\n',\n'mean_pool Epoch 44: validation_loss = 0.4012, val_acc = 1.0000, gap_val =\n0.0264', '\\n', 'mean_pool Epoch 45: validation_loss = 0.4250, val_acc = 1.0000,\ngap_val = 0.0234', '\\n', 'mean_pool Epoch 46: validation_loss = 0.4372, val_acc\n= 1.0000, gap_val = 0.0237', '\\n', 'mean_pool Epoch 47: validation_loss =\n0.4799, val_acc = 1.0000, gap_val = 0.0206', '\\n', 'mean_pool Epoch 48:\nvalidation_loss = 0.3741, val_acc = 1.0000, gap_val = 0.0299', '\\n', 'mean_pool\nEpoch 49: validation_loss = 0.4377, val_acc = 1.0000, gap_val = 0.0234', '\\n',\n'mean_pool Epoch 50: validation_loss = 0.4192, val_acc = 1.0000, gap_val =\n0.0240', '\\n', 'Finished mean_pool, E=50: final val_acc=1.0000', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 4 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Finished lstm, E=10, Epoch 1: validation_loss =\n0.6274, val_align_gap = 0.2861', '\\n', 'Finished lstm, E=10, Epoch 2:\nvalidation_loss = 0.9142, val_align_gap = 0.0442', '\\n', 'Finished lstm, E=10,\nEpoch 3: validation_loss = 0.9623, val_align_gap = 0.0896', '\\n', 'Finished\nlstm, E=10, Epoch 4: validation_loss = 0.6772, val_align_gap = 0.2779', '\\n',\n'Finished lstm, E=10, Epoch 5: validation_loss = 0.7207, val_align_gap =\n0.2952', '\\n', 'Finished lstm, E=10, Epoch 6: validation_loss = 0.6963,\nval_align_gap = 0.2856', '\\n', 'Finished lstm, E=10, Epoch 7: validation_loss =\n0.7893, val_align_gap = 0.2312', '\\n', 'Finished lstm, E=10, Epoch 8:\nvalidation_loss = 0.6257, val_align_gap = 0.3290', '\\n', 'Finished lstm, E=10,\nEpoch 9: validation_loss = 0.6322, val_align_gap = 0.2865', '\\n', 'Finished\nlstm, E=10, Epoch 10: validation_loss = 0.4235, val_align_gap = 0.3914', '\\n',\n'Finished lstm, E=30, Epoch 1: validation_loss = 0.9575, val_align_gap =\n0.0936', '\\n', 'Finished lstm, E=30, Epoch 2: validation_loss = 0.9435,\nval_align_gap = 0.1137', '\\n', 'Finished lstm, E=30, Epoch 3: validation_loss =\n0.6771, val_align_gap = 0.3304', '\\n', 'Finished lstm, E=30, Epoch 4:\nvalidation_loss = 0.6981, val_align_gap = 0.3759', '\\n', 'Finished lstm, E=30,\nEpoch 5: validation_loss = 0.7008, val_align_gap = 0.3962', '\\n', 'Finished\nlstm, E=30, Epoch 6: validation_loss = 0.6690, val_align_gap = 0.3583', '\\n',\n'Finished lstm, E=30, Epoch 7: validation_loss = 0.6604, val_align_gap =\n0.3939', '\\n', 'Finished lstm, E=30, Epoch 8: validation_loss = 0.6239,\nval_align_gap = 0.3823', '\\n', 'Finished lstm, E=30, Epoch 9: validation_loss =\n0.4850, val_align_gap = 0.3493', '\\n', 'Finished lstm, E=30, Epoch 10:\nvalidation_loss = 0.4079, val_align_gap = 0.4973', '\\n', 'Finished lstm, E=30,\nEpoch 11: validation_loss = 0.3441, val_align_gap = 0.4058', '\\n', 'Finished\nlstm, E=30, Epoch 12: validation_loss = 0.2909, val_align_gap = 0.5675', '\\n',\n'Finished lstm, E=30, Epoch 13: validation_loss = 0.1456, val_align_gap =\n0.6128', '\\n', 'Finished lstm, E=30, Epoch 14: validation_loss = 0.0000,\nval_align_gap = 0.7265', '\\n', 'Finished lstm, E=30, Epoch 15: validation_loss =\n0.0311, val_align_gap = 0.6090', '\\n', 'Finished lstm, E=30, Epoch 16:\nvalidation_loss = 0.0935, val_align_gap = 0.5707', '\\n', 'Finished lstm, E=30,\nEpoch 17: validation_loss = 0.0000, val_align_gap = 0.7196', '\\n', 'Finished\nlstm, E=30, Epoch 18: validation_loss = 0.0000, val_align_gap = 0.6342', '\\n',\n'Finished lstm, E=30, Epoch 19: validation_loss = 0.0211, val_align_gap =\n0.5997', '\\n', 'Finished lstm, E=30, Epoch 20: validation_loss = 0.0381,\nval_align_gap = 0.5925', '\\n', 'Finished lstm, E=30, Epoch 21: validation_loss =\n0.0466, val_align_gap = 0.4571', '\\n', 'Finished lstm, E=30, Epoch 22:\nvalidation_loss = 0.0741, val_align_gap = 0.4703', '\\n', 'Finished lstm, E=30,\nEpoch 23: validation_loss = 0.0000, val_align_gap = 0.7674', '\\n', 'Finished\nlstm, E=30, Epoch 24: validation_loss = 0.0000, val_align_gap = 0.8362', '\\n',\n'Finished lstm, E=30, Epoch 25: validation_loss = 0.0000, val_align_gap =\n0.5090', '\\n', 'Finished lstm, E=30, Epoch 26: validation_loss = 0.0281,\nval_align_gap = 0.5426', '\\n', 'Finished lstm, E=30, Epoch 27: validation_loss =\n0.0000, val_align_gap = 0.4461', '\\n', 'Finished lstm, E=30, Epoch 28:\nvalidation_loss = 0.0000, val_align_gap = 0.6625', '\\n', 'Finished lstm, E=30,\nEpoch 29: validation_loss = 0.0000, val_align_gap = 0.8438', '\\n', 'Finished\nlstm, E=30, Epoch 30: validation_loss = 0.0000, val_align_gap = 0.6533', '\\n',\n'Finished lstm, E=50, Epoch 1: validation_loss = 0.8265, val_align_gap =\n0.1952', '\\n', 'Finished lstm, E=50, Epoch 2: validation_loss = 0.8292,\nval_align_gap = 0.1570', '\\n', 'Finished lstm, E=50, Epoch 3: validation_loss =\n0.6050, val_align_gap = 0.3044', '\\n', 'Finished lstm, E=50, Epoch 4:\nvalidation_loss = 0.6099, val_align_gap = 0.2594', '\\n', 'Finished lstm, E=50,\nEpoch 5: validation_loss = 0.7166, val_align_gap = 0.2262', '\\n', 'Finished\nlstm, E=50, Epoch 6: validation_loss = 0.4725, val_align_gap = 0.4097', '\\n',\n'Finished lstm, E=50, Epoch 7: validation_loss = 0.3916, val_align_gap =\n0.3369', '\\n', 'Finished lstm, E=50, Epoch 8: validation_loss = 0.3330,\nval_align_gap = 0.3244', '\\n', 'Finished lstm, E=50, Epoch 9: validation_loss =\n0.2378, val_align_gap = 0.4614', '\\n', 'Finished lstm, E=50, Epoch 10:\nvalidation_loss = 0.2801, val_align_gap = 0.4785', '\\n', 'Finished lstm, E=50,\nEpoch 11: validation_loss = 0.0285, val_align_gap = 0.5535', '\\n', 'Finished\nlstm, E=50, Epoch 12: validation_loss = 0.0000, val_align_gap = 0.7072', '\\n',\n'Finished lstm, E=50, Epoch 13: validation_loss = 0.1748, val_align_gap =\n0.5091', '\\n', 'Finished lstm, E=50, Epoch 14: validation_loss = 0.0152,\nval_align_gap = 0.3905', '\\n', 'Finished lstm, E=50, Epoch 15: validation_loss =\n0.0000, val_align_gap = 0.5177', '\\n', 'Finished lstm, E=50, Epoch 16:\nvalidation_loss = 0.1657, val_align_gap = 0.3390', '\\n', 'Finished lstm, E=50,\nEpoch 17: validation_loss = 0.0000, val_align_gap = 0.5711', '\\n', 'Finished\nlstm, E=50, Epoch 18: validation_loss = 0.0000, val_align_gap = 0.5210', '\\n',\n'Finished lstm, E=50, Epoch 19: validation_loss = 0.0000, val_align_gap =\n0.3751', '\\n', 'Finished lstm, E=50, Epoch 20: validation_loss = 0.0000,\nval_align_gap = 0.3899', '\\n', 'Finished lstm, E=50, Epoch 21: validation_loss =\n0.0000, val_align_gap = 0.5269', '\\n', 'Finished lstm, E=50, Epoch 22:\nvalidation_loss = 0.0000, val_align_gap = 0.3736', '\\n', 'Finished lstm, E=50,\nEpoch 23: validation_loss = 0.0000, val_align_gap = 0.4937', '\\n', 'Finished\nlstm, E=50, Epoch 24: validation_loss = 0.0000, val_align_gap = 0.4882', '\\n',\n'Finished lstm, E=50, Epoch 25: validation_loss = 0.1030, val_align_gap =\n0.4119', '\\n', 'Finished lstm, E=50, Epoch 26: validation_loss = 0.0000,\nval_align_gap = 0.3998', '\\n', 'Finished lstm, E=50, Epoch 27: validation_loss =\n0.0000, val_align_gap = 0.5419', '\\n', 'Finished lstm, E=50, Epoch 28:\nvalidation_loss = 0.0000, val_align_gap = 0.4704', '\\n', 'Finished lstm, E=50,\nEpoch 29: validation_loss = 0.0000, val_align_gap = 0.3371', '\\n', 'Finished\nlstm, E=50, Epoch 30: validation_loss = 0.0000, val_align_gap = 0.4090', '\\n',\n'Finished lstm, E=50, Epoch 31: validation_loss = 0.0000, val_align_gap =\n0.5043', '\\n', 'Finished lstm, E=50, Epoch 32: validation_loss = 0.0308,\nval_align_gap = 0.2653', '\\n', 'Finished lstm, E=50, Epoch 33: validation_loss =\n0.0000, val_align_gap = 0.3759', '\\n', 'Finished lstm, E=50, Epoch 34:\nvalidation_loss = 0.0000, val_align_gap = 0.4249', '\\n', 'Finished lstm, E=50,\nEpoch 35: validation_loss = 0.0000, val_align_gap = 0.4019', '\\n', 'Finished\nlstm, E=50, Epoch 36: validation_loss = 0.0000, val_align_gap = 0.3030', '\\n',\n'Finished lstm, E=50, Epoch 37: validation_loss = 0.0000, val_align_gap =\n0.2818', '\\n', 'Finished lstm, E=50, Epoch 38: validation_loss = 0.0000,\nval_align_gap = 0.4634', '\\n', 'Finished lstm, E=50, Epoch 39: validation_loss =\n0.0000, val_align_gap = 0.4556', '\\n', 'Finished lstm, E=50, Epoch 40:\nvalidation_loss = 0.0000, val_align_gap = 0.3851', '\\n', 'Finished lstm, E=50,\nEpoch 41: validation_loss = 0.0000, val_align_gap = 0.4217', '\\n', 'Finished\nlstm, E=50, Epoch 42: validation_loss = 0.0000, val_align_gap = 0.2746', '\\n',\n'Finished lstm, E=50, Epoch 43: validation_loss = 0.0000, val_align_gap =\n0.4959', '\\n', 'Finished lstm, E=50, Epoch 44: validation_loss = 0.0000,\nval_align_gap = 0.3900', '\\n', 'Finished lstm, E=50, Epoch 45: validation_loss =\n0.0165, val_align_gap = 0.3782', '\\n', 'Finished lstm, E=50, Epoch 46:\nvalidation_loss = 0.0000, val_align_gap = 0.3787', '\\n', 'Finished lstm, E=50,\nEpoch 47: validation_loss = 0.0000, val_align_gap = 0.3871', '\\n', 'Finished\nlstm, E=50, Epoch 48: validation_loss = 0.0000, val_align_gap = 0.3590', '\\n',\n'Finished lstm, E=50, Epoch 49: validation_loss = 0.0000, val_align_gap =\n0.3392', '\\n', 'Finished lstm, E=50, Epoch 50: validation_loss = 0.0000,\nval_align_gap = 0.4839', '\\n', 'Finished mean_pool, E=10, Epoch 1:\nvalidation_loss = 0.3765, val_align_gap = 0.0322', '\\n', 'Finished mean_pool,\nE=10, Epoch 2: validation_loss = 0.4138, val_align_gap = 0.0284', '\\n',\n'Finished mean_pool, E=10, Epoch 3: validation_loss = 0.3997, val_align_gap =\n0.0298', '\\n', 'Finished mean_pool, E=10, Epoch 4: validation_loss = 0.3741,\nval_align_gap = 0.0321', '\\n', 'Finished mean_pool, E=10, Epoch 5:\nvalidation_loss = 0.3314, val_align_gap = 0.0372', '\\n', 'Finished mean_pool,\nE=10, Epoch 6: validation_loss = 0.4226, val_align_gap = 0.0273', '\\n',\n'Finished mean_pool, E=10, Epoch 7: validation_loss = 0.3634, val_align_gap =\n0.0342', '\\n', 'Finished mean_pool, E=10, Epoch 8: validation_loss = 0.4087,\nval_align_gap = 0.0284', '\\n', 'Finished mean_pool, E=10, Epoch 9:\nvalidation_loss = 0.3836, val_align_gap = 0.0305', '\\n', 'Finished mean_pool,\nE=10, Epoch 10: validation_loss = 0.4398, val_align_gap = 0.0252', '\\n',\n'Finished mean_pool, E=30, Epoch 1: validation_loss = 0.4577, val_align_gap =\n0.0197', '\\n', 'Finished mean_pool, E=30, Epoch 2: validation_loss = 0.4528,\nval_align_gap = 0.0192', '\\n', 'Finished mean_pool, E=30, Epoch 3:\nvalidation_loss = 0.4751, val_align_gap = 0.0173', '\\n', 'Finished mean_pool,\nE=30, Epoch 4: validation_loss = 0.5123, val_align_gap = 0.0161', '\\n',\n'Finished mean_pool, E=30, Epoch 5: validation_loss = 0.4480, val_align_gap =\n0.0192', '\\n', 'Finished mean_pool, E=30, Epoch 6: validation_loss = 0.4631,\nval_align_gap = 0.0189', '\\n', 'Finished mean_pool, E=30, Epoch 7:\nvalidation_loss = 0.4943, val_align_gap = 0.0163', '\\n', 'Finished mean_pool,\nE=30, Epoch 8: validation_loss = 0.4911, val_align_gap = 0.0174', '\\n',\n'Finished mean_pool, E=30, Epoch 9: validation_loss = 0.5139, val_align_gap =\n0.0152', '\\n', 'Finished mean_pool, E=30, Epoch 10: validation_loss = 0.5339,\nval_align_gap = 0.0146', '\\n', 'Finished mean_pool, E=30, Epoch 11:\nvalidation_loss = 0.4505, val_align_gap = 0.0204', '\\n', 'Finished mean_pool,\nE=30, Epoch 12: validation_loss = 0.4738, val_align_gap = 0.0176', '\\n',\n'Finished mean_pool, E=30, Epoch 13: validation_loss = 0.4994, val_align_gap =\n0.0161', '\\n', 'Finished mean_pool, E=30, Epoch 14: validation_loss = 0.4444,\nval_align_gap = 0.0185', '\\n', 'Finished mean_pool, E=30, Epoch 15:\nvalidation_loss = 0.4958, val_align_gap = 0.0149', '\\n', 'Finished mean_pool,\nE=30, Epoch 16: validation_loss = 0.5150, val_align_gap = 0.0146', '\\n',\n'Finished mean_pool, E=30, Epoch 17: validation_loss = 0.4174, val_align_gap =\n0.0209', '\\n', 'Finished mean_pool, E=30, Epoch 18: validation_loss = 0.5009,\nval_align_gap = 0.0154', '\\n', 'Finished mean_pool, E=30, Epoch 19:\nvalidation_loss = 0.5171, val_align_gap = 0.0143', '\\n', 'Finished mean_pool,\nE=30, Epoch 20: validation_loss = 0.4666, val_align_gap = 0.0169', '\\n',\n'Finished mean_pool, E=30, Epoch 21: validation_loss = 0.4687, val_align_gap =\n0.0174', '\\n', 'Finished mean_pool, E=30, Epoch 22: validation_loss = 0.4554,\nval_align_gap = 0.0181', '\\n', 'Finished mean_pool, E=30, Epoch 23:\nvalidation_loss = 0.4880, val_align_gap = 0.0156', '\\n', 'Finished mean_pool,\nE=30, Epoch 24: validation_loss = 0.5024, val_align_gap = 0.0160', '\\n',\n'Finished mean_pool, E=30, Epoch 25: validation_loss = 0.4681, val_align_gap =\n0.0173', '\\n', 'Finished mean_pool, E=30, Epoch 26: validation_loss = 0.4614,\nval_align_gap = 0.0178', '\\n', 'Finished mean_pool, E=30, Epoch 27:\nvalidation_loss = 0.5249, val_align_gap = 0.0134', '\\n', 'Finished mean_pool,\nE=30, Epoch 28: validation_loss = 0.5106, val_align_gap = 0.0146', '\\n',\n'Finished mean_pool, E=30, Epoch 29: validation_loss = 0.4508, val_align_gap =\n0.0173', '\\n', 'Finished mean_pool, E=30, Epoch 30: validation_loss = 0.4747,\nval_align_gap = 0.0160', '\\n', 'Finished mean_pool, E=50, Epoch 1:\nvalidation_loss = 0.4468, val_align_gap = 0.0234', '\\n', 'Finished mean_pool,\nE=50, Epoch 2: validation_loss = 0.4498, val_align_gap = 0.0231', '\\n',\n'Finished mean_pool, E=50, Epoch 3: validation_loss = 0.4613, val_align_gap =\n0.0217', '\\n', 'Finished mean_pool, E=50, Epoch 4: validation_loss = 0.4578,\nval_align_gap = 0.0222', '\\n', 'Finished mean_pool, E=50, Epoch 5:\nvalidation_loss = 0.4318, val_align_gap = 0.0244', '\\n', 'Finished mean_pool,\nE=50, Epoch 6: validation_loss = 0.4693, val_align_gap = 0.0212', '\\n',\n'Finished mean_pool, E=50, Epoch 7: validation_loss = 0.3469, val_align_gap =\n0.0330', '\\n', 'Finished mean_pool, E=50, Epoch 8: validation_loss = 0.4388,\nval_align_gap = 0.0239', '\\n', 'Finished mean_pool, E=50, Epoch 9:\nvalidation_loss = 0.4165, val_align_gap = 0.0257', '\\n', 'Finished mean_pool,\nE=50, Epoch 10: validation_loss = 0.4046, val_align_gap = 0.0270', '\\n',\n'Finished mean_pool, E=50, Epoch 11: validation_loss = 0.3894, val_align_gap =\n0.0276', '\\n', 'Finished mean_pool, E=50, Epoch 12: validation_loss = 0.3820,\nval_align_gap = 0.0289', '\\n', 'Finished mean_pool, E=50, Epoch 13:\nvalidation_loss = 0.4432, val_align_gap = 0.0225', '\\n', 'Finished mean_pool,\nE=50, Epoch 14: validation_loss = 0.4521, val_align_gap = 0.0223', '\\n',\n'Finished mean_pool, E=50, Epoch 15: validation_loss = 0.3826, val_align_gap =\n0.0286', '\\n', 'Finished mean_pool, E=50, Epoch 16: validation_loss = 0.4690,\nval_align_gap = 0.0207', '\\n', 'Finished mean_pool, E=50, Epoch 17:\nvalidation_loss = 0.4199, val_align_gap = 0.0253', '\\n', 'Finished mean_pool,\nE=50, Epoch 18: validation_loss = 0.3996, val_align_gap = 0.0271', '\\n',\n'Finished mean_pool, E=50, Epoch 19: validation_loss = 0.4469, val_align_gap =\n0.0222', '\\n', 'Finished mean_pool, E=50, Epoch 20: validation_loss = 0.4346,\nval_align_gap = 0.0231', '\\n', 'Finished mean_pool, E=50, Epoch 21:\nvalidation_loss = 0.4270, val_align_gap = 0.0239', '\\n', 'Finished mean_pool,\nE=50, Epoch 22: validation_loss = 0.4766, val_align_gap = 0.0197', '\\n',\n'Finished mean_pool, E=50, Epoch 23: validation_loss = 0.4100, val_align_gap =\n0.0249', '\\n', 'Finished mean_pool, E=50, Epoch 24: validation_loss = 0.3909,\nval_align_gap = 0.0265', '\\n', 'Finished mean_pool, E=50, Epoch 25:\nvalidation_loss = 0.4006, val_align_gap = 0.0263', '\\n', 'Finished mean_pool,\nE=50, Epoch 26: validation_loss = 0.4426, val_align_gap = 0.0219', '\\n',\n'Finished mean_pool, E=50, Epoch 27: validation_loss = 0.4203, val_align_gap =\n0.0238', '\\n', 'Finished mean_pool, E=50, Epoch 28: validation_loss = 0.4059,\nval_align_gap = 0.0249', '\\n', 'Finished mean_pool, E=50, Epoch 29:\nvalidation_loss = 0.3867, val_align_gap = 0.0264', '\\n', 'Finished mean_pool,\nE=50, Epoch 30: validation_loss = 0.4073, val_align_gap = 0.0249', '\\n',\n'Finished mean_pool, E=50, Epoch 31: validation_loss = 0.4429, val_align_gap =\n0.0214', '\\n', 'Finished mean_pool, E=50, Epoch 32: validation_loss = 0.4386,\nval_align_gap = 0.0219', '\\n', 'Finished mean_pool, E=50, Epoch 33:\nvalidation_loss = 0.4283, val_align_gap = 0.0225', '\\n', 'Finished mean_pool,\nE=50, Epoch 34: validation_loss = 0.3525, val_align_gap = 0.0292', '\\n',\n'Finished mean_pool, E=50, Epoch 35: validation_loss = 0.3563, val_align_gap =\n0.0291', '\\n', 'Finished mean_pool, E=50, Epoch 36: validation_loss = 0.4392,\nval_align_gap = 0.0215', '\\n', 'Finished mean_pool, E=50, Epoch 37:\nvalidation_loss = 0.3793, val_align_gap = 0.0266', '\\n', 'Finished mean_pool,\nE=50, Epoch 38: validation_loss = 0.3533, val_align_gap = 0.0284', '\\n',\n'Finished mean_pool, E=50, Epoch 39: validation_loss = 0.4266, val_align_gap =\n0.0221', '\\n', 'Finished mean_pool, E=50, Epoch 40: validation_loss = 0.4654,\nval_align_gap = 0.0190', '\\n', 'Finished mean_pool, E=50, Epoch 41:\nvalidation_loss = 0.4336, val_align_gap = 0.0216', '\\n', 'Finished mean_pool,\nE=50, Epoch 42: validation_loss = 0.4249, val_align_gap = 0.0225', '\\n',\n'Finished mean_pool, E=50, Epoch 43: validation_loss = 0.4117, val_align_gap =\n0.0236', '\\n', 'Finished mean_pool, E=50, Epoch 44: validation_loss = 0.3806,\nval_align_gap = 0.0258', '\\n', 'Finished mean_pool, E=50, Epoch 45:\nvalidation_loss = 0.3825, val_align_gap = 0.0262', '\\n', 'Finished mean_pool,\nE=50, Epoch 46: validation_loss = 0.3365, val_align_gap = 0.0291', '\\n',\n'Finished mean_pool, E=50, Epoch 47: validation_loss = 0.4081, val_align_gap =\n0.0233', '\\n', 'Finished mean_pool, E=50, Epoch 48: validation_loss = 0.4537,\nval_align_gap = 0.0192', '\\n', 'Finished mean_pool, E=50, Epoch 49:\nvalidation_loss = 0.3838, val_align_gap = 0.0251', '\\n', 'Finished mean_pool,\nE=50, Epoch 50: validation_loss = 0.3764, val_align_gap = 0.0251', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 4 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 0.9474', '\\n', 'Epoch\n2: validation_loss = 0.5846', '\\n', 'Epoch 3: validation_loss = 0.7915', '\\n',\n'Epoch 4: validation_loss = 0.6438', '\\n', 'Epoch 5: validation_loss = 0.6687',\n'\\n', 'Epoch 6: validation_loss = 0.6491', '\\n', 'Epoch 7: validation_loss =\n0.4976', '\\n', 'Epoch 8: validation_loss = 0.6073', '\\n', 'Epoch 9:\nvalidation_loss = 0.4626', '\\n', 'Epoch 10: validation_loss = 0.4970', '\\n',\n'Finished lstm, E=10: final val_acc=1.0000', '\\n', 'Epoch 1: validation_loss =\n1.0329', '\\n', 'Epoch 2: validation_loss = 0.7099', '\\n', 'Epoch 3:\nvalidation_loss = 0.8992', '\\n', 'Epoch 4: validation_loss = 0.6250', '\\n',\n'Epoch 5: validation_loss = 0.5932', '\\n', 'Epoch 6: validation_loss = 0.5932',\n'\\n', 'Epoch 7: validation_loss = 0.6672', '\\n', 'Epoch 8: validation_loss =\n0.3296', '\\n', 'Epoch 9: validation_loss = 0.5493', '\\n', 'Epoch 10:\nvalidation_loss = 0.4379', '\\n', 'Epoch 11: validation_loss = 0.4149', '\\n',\n'Epoch 12: validation_loss = 0.2801', '\\n', 'Epoch 13: validation_loss =\n0.0701', '\\n', 'Epoch 14: validation_loss = 0.2190', '\\n', 'Epoch 15:\nvalidation_loss = 0.1182', '\\n', 'Epoch 16: validation_loss = 0.0000', '\\n',\n'Epoch 17: validation_loss = 0.0000', '\\n', 'Epoch 18: validation_loss =\n0.0000', '\\n', 'Epoch 19: validation_loss = 0.0000', '\\n', 'Epoch 20:\nvalidation_loss = 0.0000', '\\n', 'Epoch 21: validation_loss = 0.0000', '\\n',\n'Epoch 22: validation_loss = 0.0000', '\\n', 'Epoch 23: validation_loss =\n0.0000', '\\n', 'Epoch 24: validation_loss = 0.0000', '\\n', 'Epoch 25:\nvalidation_loss = 0.0000', '\\n', 'Epoch 26: validation_loss = 0.0000', '\\n',\n'Epoch 27: validation_loss = 0.0000', '\\n', 'Epoch 28: validation_loss =\n0.0000', '\\n', 'Epoch 29: validation_loss = 0.0000', '\\n', 'Epoch 30:\nvalidation_loss = 0.0000', '\\n', 'Finished lstm, E=30: final val_acc=1.0000',\n'\\n', 'Epoch 1: validation_loss = 0.9798', '\\n', 'Epoch 2: validation_loss =\n0.9555', '\\n', 'Epoch 3: validation_loss = 0.7228', '\\n', 'Epoch 4:\nvalidation_loss = 0.9266', '\\n', 'Epoch 5: validation_loss = 0.6719', '\\n',\n'Epoch 6: validation_loss = 0.7405', '\\n', 'Epoch 7: validation_loss = 0.6462',\n'\\n', 'Epoch 8: validation_loss = 0.4450', '\\n', 'Epoch 9: validation_loss =\n0.3385', '\\n', 'Epoch 10: validation_loss = 0.2966', '\\n', 'Epoch 11:\nvalidation_loss = 0.3095', '\\n', 'Epoch 12: validation_loss = 0.2877', '\\n',\n'Epoch 13: validation_loss = 0.0597', '\\n', 'Epoch 14: validation_loss =\n0.0614', '\\n', 'Epoch 15: validation_loss = 0.0599', '\\n', 'Epoch 16:\nvalidation_loss = 0.0776', '\\n', 'Epoch 17: validation_loss = 0.0437', '\\n',\n'Epoch 18: validation_loss = 0.0040', '\\n', 'Epoch 19: validation_loss =\n0.0153', '\\n', 'Epoch 20: validation_loss = 0.0360', '\\n', 'Epoch 21:\nvalidation_loss = 0.0000', '\\n', 'Epoch 22: validation_loss = 0.0213', '\\n',\n'Epoch 23: validation_loss = 0.0447', '\\n', 'Epoch 24: validation_loss =\n0.0000', '\\n', 'Epoch 25: validation_loss = 0.0000', '\\n', 'Epoch 26:\nvalidation_loss = 0.0031', '\\n', 'Epoch 27: validation_loss = 0.0129', '\\n',\n'Epoch 28: validation_loss = 0.0000', '\\n', 'Epoch 29: validation_loss =\n0.0000', '\\n', 'Epoch 30: validation_loss = 0.0289', '\\n', 'Epoch 31:\nvalidation_loss = 0.0000', '\\n', 'Epoch 32: validation_loss = 0.0000', '\\n',\n'Epoch 33: validation_loss = 0.0000', '\\n', 'Epoch 34: validation_loss =\n0.0000', '\\n', 'Epoch 35: validation_loss = 0.0000', '\\n', 'Epoch 36:\nvalidation_loss = 0.0000', '\\n', 'Epoch 37: validation_loss = 0.0110', '\\n',\n'Epoch 38: validation_loss = 0.0000', '\\n', 'Epoch 39: validation_loss =\n0.0000', '\\n', 'Epoch 40: validation_loss = 0.0000', '\\n', 'Epoch 41:\nvalidation_loss = 0.0103', '\\n', 'Epoch 42: validation_loss = 0.0000', '\\n',\n'Epoch 43: validation_loss = 0.0391', '\\n', 'Epoch 44: validation_loss =\n0.0000', '\\n', 'Epoch 45: validation_loss = 0.0000', '\\n', 'Epoch 46:\nvalidation_loss = 0.0000', '\\n', 'Epoch 47: validation_loss = 0.0000', '\\n',\n'Epoch 48: validation_loss = 0.0265', '\\n', 'Epoch 49: validation_loss =\n0.0580', '\\n', 'Epoch 50: validation_loss = 0.0587', '\\n', 'Finished lstm, E=50:\nfinal val_acc=1.0000', '\\n', 'Epoch 1: validation_loss = 0.4627', '\\n', 'Epoch\n2: validation_loss = 0.3982', '\\n', 'Epoch 3: validation_loss = 0.4185', '\\n',\n'Epoch 4: validation_loss = 0.4575', '\\n', 'Epoch 5: validation_loss = 0.4743',\n'\\n', 'Epoch 6: validation_loss = 0.4132', '\\n', 'Epoch 7: validation_loss =\n0.4379', '\\n', 'Epoch 8: validation_loss = 0.4359', '\\n', 'Epoch 9:\nvalidation_loss = 0.4302', '\\n', 'Epoch 10: validation_loss = 0.4066', '\\n',\n'Finished mean_pool, E=10: final val_acc=1.0000', '\\n', 'Epoch 1:\nvalidation_loss = 0.4420', '\\n', 'Epoch 2: validation_loss = 0.4043', '\\n',\n'Epoch 3: validation_loss = 0.4188', '\\n', 'Epoch 4: validation_loss = 0.4135',\n'\\n', 'Epoch 5: validation_loss = 0.4452', '\\n', 'Epoch 6: validation_loss =\n0.4416', '\\n', 'Epoch 7: validation_loss = 0.4305', '\\n', 'Epoch 8:\nvalidation_loss = 0.3672', '\\n', 'Epoch 9: validation_loss = 0.4391', '\\n',\n'Epoch 10: validation_loss = 0.4620', '\\n', 'Epoch 11: validation_loss =\n0.4386', '\\n', 'Epoch 12: validation_loss = 0.4007', '\\n', 'Epoch 13:\nvalidation_loss = 0.4072', '\\n', 'Epoch 14: validation_loss = 0.4512', '\\n',\n'Epoch 15: validation_loss = 0.3952', '\\n', 'Epoch 16: validation_loss =\n0.4545', '\\n', 'Epoch 17: validation_loss = 0.4659', '\\n', 'Epoch 18:\nvalidation_loss = 0.4578', '\\n', 'Epoch 19: validation_loss = 0.3807', '\\n',\n'Epoch 20: validation_loss = 0.3941', '\\n', 'Epoch 21: validation_loss =\n0.3682', '\\n', 'Epoch 22: validation_loss = 0.3940', '\\n', 'Epoch 23:\nvalidation_loss = 0.4450', '\\n', 'Epoch 24: validation_loss = 0.3600', '\\n',\n'Epoch 25: validation_loss = 0.4129', '\\n', 'Epoch 26: validation_loss =\n0.4202', '\\n', 'Epoch 27: validation_loss = 0.3844', '\\n', 'Epoch 28:\nvalidation_loss = 0.4221', '\\n', 'Epoch 29: validation_loss = 0.4219', '\\n',\n'Epoch 30: validation_loss = 0.4406', '\\n', 'Finished mean_pool, E=30: final\nval_acc=1.0000', '\\n', 'Epoch 1: validation_loss = 0.4759', '\\n', 'Epoch 2:\nvalidation_loss = 0.5192', '\\n', 'Epoch 3: validation_loss = 0.4632', '\\n',\n'Epoch 4: validation_loss = 0.4642', '\\n', 'Epoch 5: validation_loss = 0.4227',\n'\\n', 'Epoch 6: validation_loss = 0.4694', '\\n', 'Epoch 7: validation_loss =\n0.4783', '\\n', 'Epoch 8: validation_loss = 0.5100', '\\n', 'Epoch 9:\nvalidation_loss = 0.4601', '\\n', 'Epoch 10: validation_loss = 0.4529', '\\n',\n'Epoch 11: validation_loss = 0.4311', '\\n', 'Epoch 12: validation_loss =\n0.4515', '\\n', 'Epoch 13: validation_loss = 0.4706', '\\n', 'Epoch 14:\nvalidation_loss = 0.5201', '\\n', 'Epoch 15: validation_loss = 0.4017', '\\n',\n'Epoch 16: validation_loss = 0.4671', '\\n', 'Epoch 17: validation_loss =\n0.4844', '\\n', 'Epoch 18: validation_loss = 0.4418', '\\n', 'Epoch 19:\nvalidation_loss = 0.4241', '\\n', 'Epoch 20: validation_loss = 0.4230', '\\n',\n'Epoch 21: validation_loss = 0.4340', '\\n', 'Epoch 22: validation_loss =\n0.4440', '\\n', 'Epoch 23: validation_loss = 0.3946', '\\n', 'Epoch 24:\nvalidation_loss = 0.4615', '\\n', 'Epoch 25: validation_loss = 0.4675', '\\n',\n'Epoch 26: validation_loss = 0.4214', '\\n', 'Epoch 27: validation_loss =\n0.4382', '\\n', 'Epoch 28: validation_loss = 0.4623', '\\n', 'Epoch 29:\nvalidation_loss = 0.4600', '\\n', 'Epoch 30: validation_loss = 0.4621', '\\n',\n'Epoch 31: validation_loss = 0.4213', '\\n', 'Epoch 32: validation_loss =\n0.4230', '\\n', 'Epoch 33: validation_loss = 0.4340', '\\n', 'Epoch 34:\nvalidation_loss = 0.4645', '\\n', 'Epoch 35: validation_loss = 0.4483', '\\n',\n'Epoch 36: validation_loss = 0.4339', '\\n', 'Epoch 37: validation_loss =\n0.3915', '\\n', 'Epoch 38: validation_loss = 0.4364', '\\n', 'Epoch 39:\nvalidation_loss = 0.4508', '\\n', 'Epoch 40: validation_loss = 0.4476', '\\n',\n'Epoch 41: validation_loss = 0.4375', '\\n', 'Epoch 42: validation_loss =\n0.4462', '\\n', 'Epoch 43: validation_loss = 0.4024', '\\n', 'Epoch 44:\nvalidation_loss = 0.4012', '\\n', 'Epoch 45: validation_loss = 0.4250', '\\n',\n'Epoch 46: validation_loss = 0.4372', '\\n', 'Epoch 47: validation_loss =\n0.4799', '\\n', 'Epoch 48: validation_loss = 0.3741', '\\n', 'Epoch 49:\nvalidation_loss = 0.4377', '\\n', 'Epoch 50: validation_loss = 0.4192', '\\n',\n'Finished mean_pool, E=50: final val_acc=1.0000', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 4 seconds seconds (time limit is an\nhour).']", "['Scheme=char, E=10, final val acc=0.7500', '\\n', 'Scheme=char, E=30, final val\nacc=1.0000', '\\n', 'Scheme=char, E=50, final val acc=1.0000', '\\n',\n'Scheme=subword, E=10, final val acc=1.0000', '\\n', 'Scheme=subword, E=30, final\nval acc=1.0000', '\\n', 'Scheme=subword, E=50, final val acc=1.0000', '\\n',\n'Scheme=ast, E=10, final val acc=0.0000', '\\n', 'Scheme=ast, E=30, final val\nacc=0.0000', '\\n', 'Scheme=ast, E=50, final val acc=0.0000', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 7 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Ablation=euclidean, E=10, final val_acc=0.7500',\n'\\n', 'Ablation=euclidean, E=30, final val_acc=1.0000', '\\n',\n'Ablation=euclidean, E=50, final val_acc=1.0000', '\\n', 'Ablation=cosine, E=10,\nfinal val_acc=0.7500', '\\n', 'Ablation=cosine, E=30, final val_acc=1.0000',\n'\\n', 'Ablation=cosine, E=50, final val_acc=1.0000', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 6 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Starting ablation for dim=16', '\\n', 'Finished\ndim=16, final val_acc=0.7500', '\\n', 'Starting ablation for dim=32', '\\n',\n'Finished dim=32, final val_acc=1.0000', '\\n', 'Starting ablation for dim=64',\n'\\n', 'Finished dim=64, final val_acc=1.0000', '\\n', 'Starting ablation for\ndim=128', '\\n', 'Finished dim=128, final val_acc=1.0000', '\\n', 'Starting\nablation for dim=256', '\\n', 'Finished dim=256, final val_acc=1.0000', '\\n',\n'Saved experiment_data.npy', '\\n', 'Execution time: 5 seconds seconds (time\nlimit is an hour).']", "['Using device: cuda', '\\n', 'Done E=10: val_acc=1.0000, rename_acc=0.0000',\n'\\n', 'Done E=30: val_acc=1.0000, rename_acc=0.2500', '\\n', 'Done E=50:\nval_acc=1.0000, rename_acc=0.0000', '\\n', 'Saved experiment_data.npy', '\\n',\n'Execution time: 4 seconds seconds (time limit is an hour).']", "['unidirectional E=10 final val acc=1.0000', '\\n', 'unidirectional E=30 final\nval acc=1.0000', '\\n', 'unidirectional E=50 final val acc=1.0000', '\\n',\n'bidirectional E=10 final val acc=0.7500', '\\n', 'bidirectional E=30 final val\nacc=1.0000', '\\n', 'bidirectional E=50 final val acc=1.0000', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 7 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Dataset=synthetic_clean, EPOCHS=10, final\nval_acc=0.7500', '\\n', 'Dataset=synthetic_clean, EPOCHS=30, final\nval_acc=1.0000', '\\n', 'Dataset=synthetic_clean, EPOCHS=50, final\nval_acc=1.0000', '\\n', 'Dataset=synthetic_injected, EPOCHS=10, final\nval_acc=0.7500', '\\n', 'Dataset=synthetic_injected, EPOCHS=30, final\nval_acc=0.5000', '\\n', 'Dataset=synthetic_injected, EPOCHS=50, final\nval_acc=0.5000', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 5\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Epoch 1/10: validation_loss = 1.1227', '\\n',\n'Epoch 2/10: validation_loss = 0.9664', '\\n', 'Epoch 3/10: validation_loss =\n0.8935', '\\n', 'Epoch 4/10: validation_loss = 1.0293', '\\n', 'Epoch 5/10:\nvalidation_loss = 0.7196', '\\n', 'Epoch 6/10: validation_loss = 0.7700', '\\n',\n'Epoch 7/10: validation_loss = 0.6719', '\\n', 'Epoch 8/10: validation_loss =\n0.3774', '\\n', 'Epoch 9/10: validation_loss = 0.5172', '\\n', 'Epoch 10/10:\nvalidation_loss = 0.1815', '\\n', 'Scheme=char, E=10, final val acc=0.7500',\n'\\n', 'Epoch 1/30: validation_loss = 1.2162', '\\n', 'Epoch 2/30: validation_loss\n= 0.9444', '\\n', 'Epoch 3/30: validation_loss = 0.9193', '\\n', 'Epoch 4/30:\nvalidation_loss = 0.8924', '\\n', 'Epoch 5/30: validation_loss = 0.9035', '\\n',\n'Epoch 6/30: validation_loss = 0.5058', '\\n', 'Epoch 7/30: validation_loss =\n0.4182', '\\n', 'Epoch 8/30: validation_loss = 0.5901', '\\n', 'Epoch 9/30:\nvalidation_loss = 0.5890', '\\n', 'Epoch 10/30: validation_loss = 0.4485', '\\n',\n'Epoch 11/30: validation_loss = 0.3081', '\\n', 'Epoch 12/30: validation_loss =\n0.1286', '\\n', 'Epoch 13/30: validation_loss = 0.0545', '\\n', 'Epoch 14/30:\nvalidation_loss = 0.0167', '\\n', 'Epoch 15/30: validation_loss = 0.0129', '\\n',\n'Epoch 16/30: validation_loss = 0.0000', '\\n', 'Epoch 17/30: validation_loss =\n0.0000', '\\n', 'Epoch 18/30: validation_loss = 0.0000', '\\n', 'Epoch 19/30:\nvalidation_loss = 0.1249', '\\n', 'Epoch 20/30: validation_loss = 0.0000', '\\n',\n'Epoch 21/30: validation_loss = 0.0000', '\\n', 'Epoch 22/30: validation_loss =\n0.0000', '\\n', 'Epoch 23/30: validation_loss = 0.0000', '\\n', 'Epoch 24/30:\nvalidation_loss = 0.0000', '\\n', 'Epoch 25/30: validation_loss = 0.0000', '\\n',\n'Epoch 26/30: validation_loss = 0.0000', '\\n', 'Epoch 27/30: validation_loss =\n0.0000', '\\n', 'Epoch 28/30: validation_loss = 0.0000', '\\n', 'Epoch 29/30:\nvalidation_loss = 0.0000', '\\n', 'Epoch 30/30: validation_loss = 0.0000', '\\n',\n'Scheme=char, E=30, final val acc=1.0000', '\\n', 'Epoch 1/50: validation_loss =\n1.0678', '\\n', 'Epoch 2/50: validation_loss = 1.0220', '\\n', 'Epoch 3/50:\nvalidation_loss = 1.0956', '\\n', 'Epoch 4/50: validation_loss = 0.9770', '\\n',\n'Epoch 5/50: validation_loss = 0.8571', '\\n', 'Epoch 6/50: validation_loss =\n0.8003', '\\n', 'Epoch 7/50: validation_loss = 0.5423', '\\n', 'Epoch 8/50:\nvalidation_loss = 0.6901', '\\n', 'Epoch 9/50: validation_loss = 0.6092', '\\n',\n'Epoch 10/50: validation_loss = 0.3108', '\\n', 'Epoch 11/50: validation_loss =\n0.2807', '\\n', 'Epoch 12/50: validation_loss = 0.2990', '\\n', 'Epoch 13/50:\nvalidation_loss = 0.1867', '\\n', 'Epoch 14/50: validation_loss = 0.0913', '\\n',\n'Epoch 15/50: validation_loss = 0.3248', '\\n', 'Epoch 16/50: validation_loss =\n0.1026', '\\n', 'Epoch 17/50: validation_loss = 0.0407', '\\n', 'Epoch 18/50:\nvalidation_loss = 0.1883', '\\n', 'Epoch 19/50: validation_loss = 0.0169', '\\n',\n'Epoch 20/50: validation_loss = 0.0000', '\\n', 'Epoch 21/50: validation_loss =\n0.0088', '\\n', 'Epoch 22/50: validation_loss = 0.0055', '\\n', 'Epoch 23/50:\nvalidation_loss = 0.0000', '\\n', 'Epoch 24/50: validation_loss = 0.2539', '\\n',\n'Epoch 25/50: validation_loss = 0.2580', '\\n', 'Epoch 26/50: validation_loss =\n0.0000', '\\n', 'Epoch 27/50: validation_loss = 0.0000', '\\n', 'Epoch 28/50:\nvalidation_loss = 0.0000', '\\n', 'Epoch 29/50: validation_loss = 0.2522', '\\n',\n'Epoch 30/50: validation_loss = 0.0177', '\\n', 'Epoch 31/50: validation_loss =\n0.2667', '\\n', 'Epoch 32/50: validation_loss = 0.0000', '\\n', 'Epoch 33/50:\nvalidation_loss = 0.0000', '\\n', 'Epoch 34/50: validation_loss = 0.0000', '\\n',\n'Epoch 35/50: validation_loss = 0.0000', '\\n', 'Epoch 36/50: validation_loss =\n0.0120', '\\n', 'Epoch 37/50: validation_loss = 0.0000', '\\n', 'Epoch 38/50:\nvalidation_loss = 0.0000', '\\n', 'Epoch 39/50: validation_loss = 0.0216', '\\n',\n'Epoch 40/50: validation_loss = 0.0000', '\\n', 'Epoch 41/50: validation_loss =\n0.0000', '\\n', 'Epoch 42/50: validation_loss = 0.0000', '\\n', 'Epoch 43/50:\nvalidation_loss = 0.0000', '\\n', 'Epoch 44/50: validation_loss = 0.0000', '\\n',\n'Epoch 45/50: validation_loss = 0.1884', '\\n', 'Epoch 46/50: validation_loss =\n0.0000', '\\n', 'Epoch 47/50: validation_loss = 0.2556', '\\n', 'Epoch 48/50:\nvalidation_loss = 0.2556', '\\n', 'Epoch 49/50: validation_loss = 0.0240', '\\n',\n'Epoch 50/50: validation_loss = 0.0000', '\\n', 'Scheme=char, E=50, final val\nacc=1.0000', '\\n', 'Epoch 1/10: validation_loss = 0.7659', '\\n', 'Epoch 2/10:\nvalidation_loss = 0.9588', '\\n', 'Epoch 3/10: validation_loss = 0.9172', '\\n',\n'Epoch 4/10: validation_loss = 0.6191', '\\n', 'Epoch 5/10: validation_loss =\n0.6036', '\\n', 'Epoch 6/10: validation_loss = 0.7401', '\\n', 'Epoch 7/10:\nvalidation_loss = 0.5169', '\\n', 'Epoch 8/10: validation_loss = 0.5989', '\\n',\n'Epoch 9/10: validation_loss = 0.5249', '\\n', 'Epoch 10/10: validation_loss =\n0.2808', '\\n', 'Scheme=subword, E=10, final val acc=0.7500', '\\n', 'Epoch 1/30:\nvalidation_loss = 0.9209', '\\n', 'Epoch 2/30: validation_loss = 0.9275', '\\n',\n'Epoch 3/30: validation_loss = 0.6370', '\\n', 'Epoch 4/30: validation_loss =\n0.9175', '\\n', 'Epoch 5/30: validation_loss = 0.6959', '\\n', 'Epoch 6/30:\nvalidation_loss = 0.7288', '\\n', 'Epoch 7/30: validation_loss = 0.4251', '\\n',\n'Epoch 8/30: validation_loss = 0.5598', '\\n', 'Epoch 9/30: validation_loss =\n0.4918', '\\n', 'Epoch 10/30: validation_loss = 0.2182', '\\n', 'Epoch 11/30:\nvalidation_loss = 0.2637', '\\n', 'Epoch 12/30: validation_loss = 0.2510', '\\n',\n'Epoch 13/30: validation_loss = 0.1777', '\\n', 'Epoch 14/30: validation_loss =\n0.0541', '\\n', 'Epoch 15/30: validation_loss = 0.0608', '\\n', 'Epoch 16/30:\nvalidation_loss = 0.1397', '\\n', 'Epoch 17/30: validation_loss = 0.0981', '\\n',\n'Epoch 18/30: validation_loss = 0.0000', '\\n', 'Epoch 19/30: validation_loss =\n0.0000', '\\n', 'Epoch 20/30: validation_loss = 0.0660', '\\n', 'Epoch 21/30:\nvalidation_loss = 0.0326', '\\n', 'Epoch 22/30: validation_loss = 0.0298', '\\n',\n'Epoch 23/30: validation_loss = 0.0000', '\\n', 'Epoch 24/30: validation_loss =\n0.0269', '\\n', 'Epoch 25/30: validation_loss = 0.0000', '\\n', 'Epoch 26/30:\nvalidation_loss = 0.0000', '\\n', 'Epoch 27/30: validation_loss = 0.0079', '\\n',\n'Epoch 28/30: validation_loss = 0.0205', '\\n', 'Epoch 29/30: validation_loss =\n0.0010', '\\n', 'Epoch 30/30: validation_loss = 0.0168', '\\n', 'Scheme=subword,\nE=30, final val acc=1.0000', '\\n', 'Epoch 1/50: validation_loss = 1.1242', '\\n',\n'Epoch 2/50: validation_loss = 0.7232', '\\n', 'Epoch 3/50: validation_loss =\n0.9315', '\\n', 'Epoch 4/50: validation_loss = 0.6924', '\\n', 'Epoch 5/50:\nvalidation_loss = 0.8314', '\\n', 'Epoch 6/50: validation_loss = 0.7547', '\\n',\n'Epoch 7/50: validation_loss = 0.4581', '\\n', 'Epoch 8/50: validation_loss =\n0.5567', '\\n', 'Epoch 9/50: validation_loss = 0.4698', '\\n', 'Epoch 10/50:\nvalidation_loss = 0.2809', '\\n', 'Epoch 11/50: validation_loss = 0.3771', '\\n',\n'Epoch 12/50: validation_loss = 0.2012', '\\n', 'Epoch 13/50: validation_loss =\n0.2062', '\\n', 'Epoch 14/50: validation_loss = 0.0756', '\\n', 'Epoch 15/50:\nvalidation_loss = 0.1722', '\\n', 'Epoch 16/50: validation_loss = 0.2292', '\\n',\n'Epoch 17/50: validation_loss = 0.1453', '\\n', 'Epoch 18/50: validation_loss =\n0.0576', '\\n', 'Epoch 19/50: validation_loss = 0.0529', '\\n', 'Epoch 20/50:\nvalidation_loss = 0.0805', '\\n', 'Epoch 21/50: validation_loss = 0.0543', '\\n',\n'Epoch 22/50: validation_loss = 0.0886', '\\n', 'Epoch 23/50: validation_loss =\n0.0567', '\\n', 'Epoch 24/50: validation_loss = 0.0000', '\\n', 'Epoch 25/50:\nvalidation_loss = 0.1107', '\\n', 'Epoch 26/50: validation_loss = 0.0096', '\\n',\n'Epoch 27/50: validation_loss = 0.0433', '\\n', 'Epoch 28/50: validation_loss =\n0.0987', '\\n', 'Epoch 29/50: validation_loss = 0.0221', '\\n', 'Epoch 30/50:\nvalidation_loss = 0.0709', '\\n', 'Epoch 31/50: validation_loss = 0.0332', '\\n',\n'Epoch 32/50: validation_loss = 0.0000', '\\n', 'Epoch 33/50: validation_loss =\n0.0000', '\\n', 'Epoch 34/50: validation_loss = 0.0343', '\\n', 'Epoch 35/50:\nvalidation_loss = 0.0602', '\\n', 'Epoch 36/50: validation_loss = 0.0884', '\\n',\n'Epoch 37/50: validation_loss = 0.0419', '\\n', 'Epoch 38/50: validation_loss =\n0.0189', '\\n', 'Epoch 39/50: validation_loss = 0.0035', '\\n', 'Epoch 40/50:\nvalidation_loss = 0.0905', '\\n', 'Epoch 41/50: validation_loss = 0.1326', '\\n',\n'Epoch 42/50: validation_loss = 0.0000', '\\n', 'Epoch 43/50: validation_loss =\n0.0901', '\\n', 'Epoch 44/50: validation_loss = 0.0919', '\\n', 'Epoch 45/50:\nvalidation_loss = 0.0412', '\\n', 'Epoch 46/50: validation_loss = 0.0899', '\\n',\n'Epoch 47/50: validation_loss = 0.0000', '\\n', 'Epoch 48/50: validation_loss =\n0.0271', '\\n', 'Epoch 49/50: validation_loss = 0.0033', '\\n', 'Epoch 50/50:\nvalidation_loss = 0.0342', '\\n', 'Scheme=subword, E=50, final val acc=1.0000',\n'\\n', 'Epoch 1/10: validation_loss = 1.0862', '\\n', 'Epoch 2/10: validation_loss\n= 1.0535', '\\n', 'Epoch 3/10: validation_loss = 0.6919', '\\n', 'Epoch 4/10:\nvalidation_loss = 0.6644', '\\n', 'Epoch 5/10: validation_loss = 0.7272', '\\n',\n'Epoch 6/10: validation_loss = 0.4031', '\\n', 'Epoch 7/10: validation_loss =\n0.7009', '\\n', 'Epoch 8/10: validation_loss = 0.7087', '\\n', 'Epoch 9/10:\nvalidation_loss = 0.5410', '\\n', 'Epoch 10/10: validation_loss = 0.5382', '\\n',\n'Scheme=ast, E=10, final val acc=1.0000', '\\n', 'Epoch 1/30: validation_loss =\n1.0280', '\\n', 'Epoch 2/30: validation_loss = 1.0540', '\\n', 'Epoch 3/30:\nvalidation_loss = 0.8637', '\\n', 'Epoch 4/30: validation_loss = 0.7074', '\\n',\n'Epoch 5/30: validation_loss = 0.6890', '\\n', 'Epoch 6/30: validation_loss =\n0.8314', '\\n', 'Epoch 7/30: validation_loss = 0.7353', '\\n', 'Epoch 8/30:\nvalidation_loss = 0.6555', '\\n', 'Epoch 9/30: validation_loss = 0.5599', '\\n',\n'Epoch 10/30: validation_loss = 0.4183', '\\n', 'Epoch 11/30: validation_loss =\n0.3952', '\\n', 'Epoch 12/30: validation_loss = 0.3682', '\\n', 'Epoch 13/30:\nvalidation_loss = 0.3461', '\\n', 'Epoch 14/30: validation_loss = 0.3375', '\\n',\n'Epoch 15/30: validation_loss = 0.1916', '\\n', 'Epoch 16/30: validation_loss =\n0.1739', '\\n', 'Epoch 17/30: validation_loss = 0.0824', '\\n', 'Epoch 18/30:\nvalidation_loss = 0.0572', '\\n', 'Epoch 19/30: validation_loss = 0.1021', '\\n',\n'Epoch 20/30: validation_loss = 0.0329', '\\n', 'Epoch 21/30: validation_loss =\n0.0498', '\\n', 'Epoch 22/30: validation_loss = 0.0194', '\\n', 'Epoch 23/30:\nvalidation_loss = 0.0145', '\\n', 'Epoch 24/30: validation_loss = 0.1587', '\\n',\n'Epoch 25/30: validation_loss = 0.0203', '\\n', 'Epoch 26/30: validation_loss =\n0.0000', '\\n', 'Epoch 27/30: validation_loss = 0.0000', '\\n', 'Epoch 28/30:\nvalidation_loss = 0.1006', '\\n', 'Epoch 29/30: validation_loss = 0.1790', '\\n',\n'Epoch 30/30: validation_loss = 0.0751', '\\n', 'Scheme=ast, E=30, final val\nacc=1.0000', '\\n', 'Epoch 1/50: validation_loss = 0.9246', '\\n', 'Epoch 2/50:\nvalidation_loss = 0.6755', '\\n', 'Epoch 3/50: validation_loss = 0.6001', '\\n',\n'Epoch 4/50: validation_loss = 0.6723', '\\n', 'Epoch 5/50: validation_loss =\n0.7020', '\\n', 'Epoch 6/50: validation_loss = 0.5851', '\\n', 'Epoch 7/50:\nvalidation_loss = 0.4976', '\\n', 'Epoch 8/50: validation_loss = 0.3474', '\\n',\n'Epoch 9/50: validation_loss = 0.3968', '\\n', 'Epoch 10/50: validation_loss =\n0.2498', '\\n', 'Epoch 11/50: validation_loss = 0.2096', '\\n', 'Epoch 12/50:\nvalidation_loss = 0.0701', '\\n', 'Epoch 13/50: validation_loss = 0.0334', '\\n',\n'Epoch 14/50: validation_loss = 0.0941', '\\n', 'Epoch 15/50: validation_loss =\n0.0675', '\\n', 'Epoch 16/50: validation_loss = 0.0840', '\\n', 'Epoch 17/50:\nvalidation_loss = 0.0094', '\\n', 'Epoch 18/50: validation_loss = 0.0000', '\\n',\n'Epoch 19/50: validation_loss = 0.0000', '\\n', 'Epoch 20/50: validation_loss =\n0.0000', '\\n', 'Epoch 21/50: validation_loss = 0.0000', '\\n', 'Epoch 22/50:\nvalidation_loss = 0.0290', '\\n', 'Epoch 23/50: validation_loss = 0.0000', '\\n',\n'Epoch 24/50: validation_loss = 0.0000', '\\n', 'Epoch 25/50: validation_loss =\n0.0000', '\\n', 'Epoch 26/50: validation_loss = 0.0022', '\\n', 'Epoch 27/50:\nvalidation_loss = 0.0462', '\\n', 'Epoch 28/50: validation_loss = 0.0157', '\\n',\n'Epoch 29/50: validation_loss = 0.0000', '\\n', 'Epoch 30/50: validation_loss =\n0.0000', '\\n', 'Epoch 31/50: validation_loss = 0.0464', '\\n', 'Epoch 32/50:\nvalidation_loss = 0.0000', '\\n', 'Epoch 33/50: validation_loss = 0.0038', '\\n',\n'Epoch 34/50: validation_loss = 0.0000', '\\n', 'Epoch 35/50: validation_loss =\n0.0017', '\\n', 'Epoch 36/50: validation_loss = 0.0000', '\\n', 'Epoch 37/50:\nvalidation_loss = 0.0000', '\\n', 'Epoch 38/50: validation_loss = 0.0000', '\\n',\n'Epoch 39/50: validation_loss = 0.0000', '\\n', 'Epoch 40/50: validation_loss =\n0.0000', '\\n', 'Epoch 41/50: validation_loss = 0.0074', '\\n', 'Epoch 42/50:\nvalidation_loss = 0.0235', '\\n', 'Epoch 43/50: validation_loss = 0.0000', '\\n',\n'Epoch 44/50: validation_loss = 0.0386', '\\n', 'Epoch 45/50: validation_loss =\n0.0230', '\\n', 'Epoch 46/50: validation_loss = 0.0227', '\\n', 'Epoch 47/50:\nvalidation_loss = 0.0058', '\\n', 'Epoch 48/50: validation_loss = 0.0052', '\\n',\n'Epoch 49/50: validation_loss = 0.0000', '\\n', 'Epoch 50/50: validation_loss =\n0.0000', '\\n', 'Scheme=ast, E=50, final val acc=1.0000', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 8 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'head=0, epochs=10, final val_acc=1.0000', '\\n',\n'head=0, epochs=30, final val_acc=1.0000', '\\n', 'head=0, epochs=50, final\nval_acc=1.0000', '\\n', 'head=1, epochs=10, final val_acc=1.0000', '\\n', 'head=1,\nepochs=30, final val_acc=1.0000', '\\n', 'head=1, epochs=50, final\nval_acc=1.0000', '\\n', 'head=2, epochs=10, final val_acc=1.0000', '\\n', 'head=2,\nepochs=30, final val_acc=1.0000', '\\n', 'head=2, epochs=50, final\nval_acc=1.0000', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 8\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Done CNN ablation EPOCHS=10, final val\nacc=0.7500', '\\n', 'Done CNN ablation EPOCHS=30, final val acc=1.0000', '\\n',\n'Done CNN ablation EPOCHS=50, final val acc=1.0000', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 4 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'head=0, epochs=10, final val_acc=0.7500', '\\n',\n'head=0, epochs=30, final val_acc=1.0000', '\\n', 'head=0, epochs=50, final\nval_acc=1.0000', '\\n', 'head=1, epochs=10, final val_acc=0.7500', '\\n', 'head=1,\nepochs=30, final val_acc=1.0000', '\\n', 'head=1, epochs=50, final\nval_acc=0.7500', '\\n', 'head=2, epochs=10, final val_acc=0.5000', '\\n', 'head=2,\nepochs=30, final val_acc=1.0000', '\\n', 'head=2, epochs=50, final\nval_acc=1.0000', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 9\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'head=0, epochs=10, final val_acc=1.0000', '\\n',\n'head=0, epochs=30, final val_acc=1.0000', '\\n', 'head=0, epochs=50, final\nval_acc=1.0000', '\\n', 'head=1, epochs=10, final val_acc=0.7500', '\\n', 'head=1,\nepochs=30, final val_acc=1.0000', '\\n', 'head=1, epochs=50, final\nval_acc=1.0000', '\\n', 'head=2, epochs=10, final val_acc=1.0000', '\\n', 'head=2,\nepochs=30, final val_acc=0.5000', '\\n', 'head=2, epochs=50, final\nval_acc=1.0000', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 9\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'head=0, epochs=10, final val_acc=1.0000', '\\n',\n'head=0, epochs=30, final val_acc=1.0000', '\\n', 'head=0, epochs=50, final\nval_acc=1.0000', '\\n', 'head=1, epochs=10, final val_acc=0.7500', '\\n', 'head=1,\nepochs=30, final val_acc=1.0000', '\\n', 'head=1, epochs=50, final\nval_acc=1.0000', '\\n', 'head=2, epochs=10, final val_acc=1.0000', '\\n', 'head=2,\nepochs=30, final val_acc=0.5000', '\\n', 'head=2, epochs=50, final\nval_acc=1.0000', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 8\nseconds seconds (time limit is an hour).']", ""], "analysis": ["", "", "", "The code crashes when instantiating MeanPoolEncoder because it's passed an\nunexpected 'hidden' argument. To fix this, only pass 'hidden' when using\nLSTMEncoder, or update MeanPoolEncoder\u2019s __init__ signature to accept (and\nignore) a hidden parameter.", "", "The script executed end-to-end without errors, produced perfect validation\naccuracy for both triplet and contrastive training across all epoch settings,\nand saved the experiment results file successfully. All core components (data\npreparation, model training, loss computations, evaluation, and saving) worked\nas intended.", "", "", "", "The AST tokenization function only records node type names and ignores literal\nvalues (e.g., constant values and identifiers). In this synthetic setup, all\nf(x)=x+c ASTs produce identical token sequences, so the model cannot distinguish\nfunctions with different constants, resulting in 0 retrieval accuracy. To fix\nthis, modify ast_tokenize to include node attributes: for example, append the\nconstant value to the token (e.g., f\"Constant_{node.value}\") and likewise\ninclude identifier names for Name nodes, so that two ASTs with different\nconstants yield different token sequences.", "", "", "", "", "", "", "", "", "", "", "", ""], "exc_type": [null, null, null, "TypeError", null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, {"args": ["MeanPoolEncoder.__init__() got an unexpected keyword argument 'hidden'"]}, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 126, "<module>", "model = Encoder("]], null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training set", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy", "data": [{"dataset_name": "arith", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "branch", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "loop", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "arith", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "branch", "final_value": 0.5, "best_value": 0.5}, {"dataset_name": "loop", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "arith", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "branch", "final_value": 0.6874, "best_value": 0.6874}, {"dataset_name": "loop", "final_value": 1.0042, "best_value": 1.0042}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "arith", "final_value": 0.138, "best_value": 0.138}, {"dataset_name": "branch", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "loop", "final_value": 1.2961, "best_value": 1.2961}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training set.", "data": [{"dataset_name": "random_negative", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "hard_negative", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set.", "data": [{"dataset_name": "random_negative", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "hard_negative", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training set.", "data": [{"dataset_name": "random_negative", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "hard_negative", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set.", "data": [{"dataset_name": "random_negative", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "hard_negative", "final_value": 0.3422, "best_value": 0.2762}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training dataset accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation dataset accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training dataset loss", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation dataset loss", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training set accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation set accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Training set loss", "data": [{"dataset_name": "synthetic", "final_value": 0.3854, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation set loss", "data": [{"dataset_name": "synthetic", "final_value": 0.4192, "best_value": 0.0}]}, {"metric_name": "training gap", "lower_is_better": true, "description": "Gap between training and validation performance on the training set", "data": [{"dataset_name": "synthetic", "final_value": 0.0283, "best_value": 0.0247}]}, {"metric_name": "validation gap", "lower_is_better": true, "description": "Gap between training and validation performance on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.024, "best_value": 0.024}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.3479, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.3764, "best_value": 0.0}]}, {"metric_name": "training alignment gap", "lower_is_better": true, "description": "Training alignment gap on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.0278, "best_value": 0.0201}]}, {"metric_name": "validation alignment gap", "lower_is_better": true, "description": "Validation alignment gap on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.0251, "best_value": 0.016}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training set.", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation set.", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss of the model on the training set.", "data": [{"dataset_name": "synthetic", "final_value": 0.3854, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation set.", "data": [{"dataset_name": "synthetic", "final_value": 0.4192, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Proportion of correctly classified samples on the training set.", "data": [{"dataset_name": "synthetic_char", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic_subword", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic_ast", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Proportion of correctly classified samples on the validation set.", "data": [{"dataset_name": "synthetic_char", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic_subword", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic_ast", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training set.", "data": [{"dataset_name": "synthetic_char", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_subword", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_ast", "final_value": 1.0105, "best_value": 1.0028}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set.", "data": [{"dataset_name": "synthetic_char", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_subword", "final_value": 0.07, "best_value": 0.0192}, {"dataset_name": "synthetic_ast", "final_value": 1.0074, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training set", "data": [{"dataset_name": "synthetic (euclidean, 10 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (euclidean, 30 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (euclidean, 50 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (cosine, 10 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (cosine, 30 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (cosine, 50 epochs)", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "synthetic (euclidean, 10 epochs)", "final_value": 0.75, "best_value": 0.75}, {"dataset_name": "synthetic (euclidean, 30 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (euclidean, 50 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (cosine, 10 epochs)", "final_value": 0.75, "best_value": 0.75}, {"dataset_name": "synthetic (cosine, 30 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (cosine, 50 epochs)", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "synthetic (euclidean, 10 epochs)", "final_value": 0.1305, "best_value": 0.1305}, {"dataset_name": "synthetic (euclidean, 30 epochs)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic (euclidean, 50 epochs)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic (cosine, 10 epochs)", "final_value": 0.1678, "best_value": 0.1678}, {"dataset_name": "synthetic (cosine, 30 epochs)", "final_value": 0.0592, "best_value": 0.0592}, {"dataset_name": "synthetic (cosine, 50 epochs)", "final_value": 0.0004, "best_value": 0.0004}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "synthetic (euclidean, 10 epochs)", "final_value": 0.3857, "best_value": 0.3857}, {"dataset_name": "synthetic (euclidean, 30 epochs)", "final_value": 0.2652, "best_value": 0.2652}, {"dataset_name": "synthetic (euclidean, 50 epochs)", "final_value": 0.1913, "best_value": 0.1913}, {"dataset_name": "synthetic (cosine, 10 epochs)", "final_value": 0.2616, "best_value": 0.2616}, {"dataset_name": "synthetic (cosine, 30 epochs)", "final_value": 0.129, "best_value": 0.129}, {"dataset_name": "synthetic (cosine, 50 epochs)", "final_value": 0.1711, "best_value": 0.1711}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset", "data": [{"dataset_name": "synthetic (Dimension: 16)", "final_value": 0.875, "best_value": 0.875}, {"dataset_name": "synthetic (Dimension: 32)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (Dimension: 64)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (Dimension: 128)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (Dimension: 256)", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset", "data": [{"dataset_name": "synthetic (Dimension: 16)", "final_value": 0.75, "best_value": 0.75}, {"dataset_name": "synthetic (Dimension: 32)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (Dimension: 64)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (Dimension: 128)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (Dimension: 256)", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "synthetic (Dimension: 16)", "final_value": 0.584, "best_value": 0.584}, {"dataset_name": "synthetic (Dimension: 32)", "final_value": 0.0683, "best_value": 0.0683}, {"dataset_name": "synthetic (Dimension: 64)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic (Dimension: 128)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic (Dimension: 256)", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "synthetic (Dimension: 16)", "final_value": 0.7061, "best_value": 0.7061}, {"dataset_name": "synthetic (Dimension: 32)", "final_value": 0.2551, "best_value": 0.2551}, {"dataset_name": "synthetic (Dimension: 64)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic (Dimension: 128)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic (Dimension: 256)", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the synthetic training dataset", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the synthetic validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "renaming accuracy", "lower_is_better": false, "description": "Accuracy for the renaming task on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.25}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the synthetic training dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.029, "best_value": 0.029}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the synthetic validation dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training set", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "synthetic", "final_value": 0.0009, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.0454, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "training accuracy", "lower_is_better": false, "description": "Accuracy on the training set", "data": [{"dataset_name": "synthetic_clean", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic_injected", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "synthetic_clean", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic_injected", "final_value": 0.5, "best_value": 0.5}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "synthetic_clean", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_injected", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "synthetic_clean", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_injected", "final_value": 0.3423, "best_value": 0.3423}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training set accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation set accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train alignment gap", "lower_is_better": true, "description": "Alignment gap on the training set", "data": [{"dataset_name": "synthetic", "final_value": 0.6334, "best_value": 0.2954}]}, {"metric_name": "validation alignment gap", "lower_is_better": true, "description": "Alignment gap on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.598, "best_value": 0.2313}]}, {"metric_name": "final training loss", "lower_is_better": true, "description": "Final training loss", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "final validation loss", "lower_is_better": true, "description": "Final validation loss", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy", "data": [{"dataset_name": "synthetic (head_0, 10 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (head_0, 30 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (head_0, 50 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (head_1, 10 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (head_1, 30 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (head_1, 50 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (head_2, 10 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (head_2, 30 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (head_2, 50 epochs)", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "synthetic (head_0, 10 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (head_0, 30 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (head_0, 50 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (head_1, 10 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (head_1, 30 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (head_1, 50 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (head_2, 10 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (head_2, 30 epochs)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (head_2, 50 epochs)", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "synthetic (head_0, 10 epochs)", "final_value": 0.1722, "best_value": 0.1722}, {"dataset_name": "synthetic (head_0, 30 epochs)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic (head_0, 50 epochs)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic (head_1, 10 epochs)", "final_value": 0.3379, "best_value": 0.3379}, {"dataset_name": "synthetic (head_1, 30 epochs)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic (head_1, 50 epochs)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic (head_2, 10 epochs)", "final_value": 0.7093, "best_value": 0.7093}, {"dataset_name": "synthetic (head_2, 30 epochs)", "final_value": 0.0239, "best_value": 0.0239}, {"dataset_name": "synthetic (head_2, 50 epochs)", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "synthetic (head_0, 10 epochs)", "final_value": 0.4165, "best_value": 0.4165}, {"dataset_name": "synthetic (head_0, 30 epochs)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic (head_0, 50 epochs)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic (head_1, 10 epochs)", "final_value": 0.4535, "best_value": 0.4535}, {"dataset_name": "synthetic (head_1, 30 epochs)", "final_value": 0.0109, "best_value": 0.0109}, {"dataset_name": "synthetic (head_1, 50 epochs)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic (head_2, 10 epochs)", "final_value": 0.8796, "best_value": 0.8796}, {"dataset_name": "synthetic (head_2, 30 epochs)", "final_value": 0.031, "best_value": 0.031}, {"dataset_name": "synthetic (head_2, 50 epochs)", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "synthetic", "final_value": 0.1519, "best_value": 0.1519}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Fraction of training samples correctly predicted", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Fraction of validation samples correctly predicted", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training set", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_52c025f14d09471881bea2a75607be65_proc_385044/synthetic_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_52c025f14d09471881bea2a75607be65_proc_385044/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_5221f873ef9f49acba79f917e212757c_proc_400996/branch_train_val_acc_loss.png", "../../logs/0-run/experiment_results/experiment_5221f873ef9f49acba79f917e212757c_proc_400996/loop_train_val_acc_loss.png", "../../logs/0-run/experiment_results/experiment_5221f873ef9f49acba79f917e212757c_proc_400996/arith_train_val_acc_loss.png"], ["../../logs/0-run/experiment_results/experiment_dcb9f657828c4f6dbf6d7dafa8238d07_proc_400997/hard_negative_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_dcb9f657828c4f6dbf6d7dafa8238d07_proc_400997/random_negative_loss_curves.png", "../../logs/0-run/experiment_results/experiment_dcb9f657828c4f6dbf6d7dafa8238d07_proc_400997/random_negative_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_dcb9f657828c4f6dbf6d7dafa8238d07_proc_400997/hard_negative_loss_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_2120288f76b5437ab67ae259f95ba70a_proc_400997/synthetic_margin_1.0_curves.png", "../../logs/0-run/experiment_results/experiment_2120288f76b5437ab67ae259f95ba70a_proc_400997/synthetic_margin_2.0_curves.png", "../../logs/0-run/experiment_results/experiment_2120288f76b5437ab67ae259f95ba70a_proc_400997/synthetic_margin_0.1_curves.png", "../../logs/0-run/experiment_results/experiment_2120288f76b5437ab67ae259f95ba70a_proc_400997/synthetic_final_val_accuracy.png", "../../logs/0-run/experiment_results/experiment_2120288f76b5437ab67ae259f95ba70a_proc_400997/synthetic_margin_0.5_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_78a2aa84090e43c18630696a2802fe5e_proc_400996/synthetic_retrieval_accuracy.png", "../../logs/0-run/experiment_results/experiment_78a2aa84090e43c18630696a2802fe5e_proc_400996/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_78a2aa84090e43c18630696a2802fe5e_proc_400996/synthetic_gap_curves.png"], ["../../logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/lstm_accuracy_synthetic.png", "../../logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/mean_pool_accuracy_synthetic.png", "../../logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/mean_pool_align_gap_synthetic.png", "../../logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/lstm_loss_curves_synthetic.png", "../../logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/mean_pool_loss_curves_synthetic.png", "../../logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/lstm_align_gap_synthetic.png"], ["../../logs/0-run/experiment_results/experiment_0cd0cc9a5efc4a729d845f401c297e8f_proc_400995/lstm_synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_0cd0cc9a5efc4a729d845f401c297e8f_proc_400995/lstm_synthetic_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_0cd0cc9a5efc4a729d845f401c297e8f_proc_400995/mean_pool_synthetic_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_0cd0cc9a5efc4a729d845f401c297e8f_proc_400995/mean_pool_synthetic_loss_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_aad02a7804b94b168a8de324c50e464c_proc_400995/synthetic_cosine_loss_acc.png", "../../logs/0-run/experiment_results/experiment_aad02a7804b94b168a8de324c50e464c_proc_400995/synthetic_euclidean_loss_acc.png"], ["../../logs/0-run/experiment_results/experiment_8009319c799940e694e8f42df978c1e1_proc_400997/synthetic_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_8009319c799940e694e8f42df978c1e1_proc_400997/synthetic_loss_curve.png"], ["../../logs/0-run/experiment_results/experiment_80f4639c872f417d835aa5c4e1f4eea1_proc_400996/synthetic_dataset_E30_curves.png", "../../logs/0-run/experiment_results/experiment_80f4639c872f417d835aa5c4e1f4eea1_proc_400996/synthetic_dataset_E10_curves.png", "../../logs/0-run/experiment_results/experiment_80f4639c872f417d835aa5c4e1f4eea1_proc_400996/synthetic_dataset_E50_curves.png"], ["../../logs/0-run/experiment_results/experiment_b78b574eaf7343a0833f9f2662cbc161_proc_400997/synthetic_bidirectional_lstm_ablation_loss_curves.png", "../../logs/0-run/experiment_results/experiment_b78b574eaf7343a0833f9f2662cbc161_proc_400997/synthetic_bidirectional_lstm_ablation_accuracy_curves.png"], ["../../logs/0-run/experiment_results/experiment_8d56eb13f9fa42b9b5bd093264b67d80_proc_400995/synthetic_clean_loss_curves.png", "../../logs/0-run/experiment_results/experiment_8d56eb13f9fa42b9b5bd093264b67d80_proc_400995/synthetic_injected_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_8d56eb13f9fa42b9b5bd093264b67d80_proc_400995/synthetic_injected_loss_curves.png", "../../logs/0-run/experiment_results/experiment_8d56eb13f9fa42b9b5bd093264b67d80_proc_400995/synthetic_clean_accuracy_curves.png"], ["../../logs/0-run/experiment_results/experiment_fd7719de6901415ca68942fbd80bb152_proc_400997/synthetic_subword_metrics_curves.png", "../../logs/0-run/experiment_results/experiment_fd7719de6901415ca68942fbd80bb152_proc_400997/synthetic_char_metrics_curves.png", "../../logs/0-run/experiment_results/experiment_fd7719de6901415ca68942fbd80bb152_proc_400997/synthetic_ast_metrics_curves.png"], ["../../logs/0-run/experiment_results/experiment_5abfdfa0b7124a7ca1ae3f9490c7c9dd_proc_400996/synthetic_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_5abfdfa0b7124a7ca1ae3f9490c7c9dd_proc_400996/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_f1380c5e79ff46038a65c1799d529f94_proc_400995/synthetic_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_f1380c5e79ff46038a65c1799d529f94_proc_400995/synthetic_loss_curves.png", "../../logs/0-run/experiment_results/experiment_f1380c5e79ff46038a65c1799d529f94_proc_400995/synthetic_final_val_accuracy.png"], ["../../logs/0-run/experiment_results/experiment_43c5b1e3bd914f33839294535e89daa4_proc_400995/synthetic_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_43c5b1e3bd914f33839294535e89daa4_proc_400995/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_b1aaa62174ec4019bdaead133de21454_proc_400996/synthetic_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_b1aaa62174ec4019bdaead133de21454_proc_400996/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_970f702b3a6a4461a61665d7f19beca7_proc_400997/synthetic_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_970f702b3a6a4461a61665d7f19beca7_proc_400997/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/seed_aggregation_3a73306f59a44eaa98e3b2fcffaa5dc6/synthetic_accuracy_mean_sem.png", "../../logs/0-run/experiment_results/seed_aggregation_3a73306f59a44eaa98e3b2fcffaa5dc6/synthetic_loss_mean_sem.png"]], "plot_paths": [["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_52c025f14d09471881bea2a75607be65_proc_385044/synthetic_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_52c025f14d09471881bea2a75607be65_proc_385044/synthetic_loss_curves.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_5221f873ef9f49acba79f917e212757c_proc_400996/branch_train_val_acc_loss.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_5221f873ef9f49acba79f917e212757c_proc_400996/loop_train_val_acc_loss.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_5221f873ef9f49acba79f917e212757c_proc_400996/arith_train_val_acc_loss.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_dcb9f657828c4f6dbf6d7dafa8238d07_proc_400997/hard_negative_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_dcb9f657828c4f6dbf6d7dafa8238d07_proc_400997/random_negative_loss_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_dcb9f657828c4f6dbf6d7dafa8238d07_proc_400997/random_negative_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_dcb9f657828c4f6dbf6d7dafa8238d07_proc_400997/hard_negative_loss_curves.png"], [], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_2120288f76b5437ab67ae259f95ba70a_proc_400997/synthetic_margin_1.0_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_2120288f76b5437ab67ae259f95ba70a_proc_400997/synthetic_margin_2.0_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_2120288f76b5437ab67ae259f95ba70a_proc_400997/synthetic_margin_0.1_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_2120288f76b5437ab67ae259f95ba70a_proc_400997/synthetic_final_val_accuracy.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_2120288f76b5437ab67ae259f95ba70a_proc_400997/synthetic_margin_0.5_curves.png"], [], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_78a2aa84090e43c18630696a2802fe5e_proc_400996/synthetic_retrieval_accuracy.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_78a2aa84090e43c18630696a2802fe5e_proc_400996/synthetic_loss_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_78a2aa84090e43c18630696a2802fe5e_proc_400996/synthetic_gap_curves.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/lstm_accuracy_synthetic.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/mean_pool_accuracy_synthetic.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/mean_pool_align_gap_synthetic.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/lstm_loss_curves_synthetic.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/mean_pool_loss_curves_synthetic.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/lstm_align_gap_synthetic.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_0cd0cc9a5efc4a729d845f401c297e8f_proc_400995/lstm_synthetic_loss_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_0cd0cc9a5efc4a729d845f401c297e8f_proc_400995/lstm_synthetic_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_0cd0cc9a5efc4a729d845f401c297e8f_proc_400995/mean_pool_synthetic_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_0cd0cc9a5efc4a729d845f401c297e8f_proc_400995/mean_pool_synthetic_loss_curves.png"], [], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_aad02a7804b94b168a8de324c50e464c_proc_400995/synthetic_cosine_loss_acc.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_aad02a7804b94b168a8de324c50e464c_proc_400995/synthetic_euclidean_loss_acc.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_8009319c799940e694e8f42df978c1e1_proc_400997/synthetic_accuracy_curve.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_8009319c799940e694e8f42df978c1e1_proc_400997/synthetic_loss_curve.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_80f4639c872f417d835aa5c4e1f4eea1_proc_400996/synthetic_dataset_E30_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_80f4639c872f417d835aa5c4e1f4eea1_proc_400996/synthetic_dataset_E10_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_80f4639c872f417d835aa5c4e1f4eea1_proc_400996/synthetic_dataset_E50_curves.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_b78b574eaf7343a0833f9f2662cbc161_proc_400997/synthetic_bidirectional_lstm_ablation_loss_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_b78b574eaf7343a0833f9f2662cbc161_proc_400997/synthetic_bidirectional_lstm_ablation_accuracy_curves.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_8d56eb13f9fa42b9b5bd093264b67d80_proc_400995/synthetic_clean_loss_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_8d56eb13f9fa42b9b5bd093264b67d80_proc_400995/synthetic_injected_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_8d56eb13f9fa42b9b5bd093264b67d80_proc_400995/synthetic_injected_loss_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_8d56eb13f9fa42b9b5bd093264b67d80_proc_400995/synthetic_clean_accuracy_curves.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_fd7719de6901415ca68942fbd80bb152_proc_400997/synthetic_subword_metrics_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_fd7719de6901415ca68942fbd80bb152_proc_400997/synthetic_char_metrics_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_fd7719de6901415ca68942fbd80bb152_proc_400997/synthetic_ast_metrics_curves.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_5abfdfa0b7124a7ca1ae3f9490c7c9dd_proc_400996/synthetic_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_5abfdfa0b7124a7ca1ae3f9490c7c9dd_proc_400996/synthetic_loss_curves.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_f1380c5e79ff46038a65c1799d529f94_proc_400995/synthetic_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_f1380c5e79ff46038a65c1799d529f94_proc_400995/synthetic_loss_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_f1380c5e79ff46038a65c1799d529f94_proc_400995/synthetic_final_val_accuracy.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_43c5b1e3bd914f33839294535e89daa4_proc_400995/synthetic_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_43c5b1e3bd914f33839294535e89daa4_proc_400995/synthetic_loss_curves.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_b1aaa62174ec4019bdaead133de21454_proc_400996/synthetic_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_b1aaa62174ec4019bdaead133de21454_proc_400996/synthetic_loss_curves.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_970f702b3a6a4461a61665d7f19beca7_proc_400997/synthetic_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_970f702b3a6a4461a61665d7f19beca7_proc_400997/synthetic_loss_curves.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/seed_aggregation_3a73306f59a44eaa98e3b2fcffaa5dc6/synthetic_accuracy_mean_sem.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/seed_aggregation_3a73306f59a44eaa98e3b2fcffaa5dc6/synthetic_loss_mean_sem.png"]], "plot_analyses": [[{"analysis": "Plot 1 shows that all three epoch budgets (10, 30, 50) eventually reach perfect training accuracy, but their validation curves tell different stories. With only 10 epochs, validation peaks around 65\u201370%, indicating underfitting on the held\u2010out set before training halts. Allowing 30 epochs pushes validation up to about 75%, but it plateaus quickly\u2014suggesting the model learns faster but still fails to fully generalize. At 50 epochs, training accuracy climbs more gradually, and validation remains stuck at 50% until roughly epoch 12, when it suddenly jumps to near 100%. This abrupt transition hints at a late learning regime change (e.g., a schedule step or warm\u2010up ending) rather than smooth generalization improvements.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_52c025f14d09471881bea2a75607be65_proc_385044/synthetic_accuracy_curves.png"}, {"analysis": "Plot 2 confirms that training loss falls swiftly to near zero in all settings, but validation loss behaves differently across epoch choices. For 10 epochs, validation loss drops smoothly to around 0.1 and then stops. At 30 epochs, loss also falls fast but shows small oscillations around zero toward the end, reflecting some overfitting. Under the 50\u2010epoch run, validation loss oscillates more noticeably even after hitting near\u2010zero, indicating sensitivity to noise and potential overtraining. The sharper spikes correspond temporally to the delayed accuracy jump seen in the first plot, reinforcing the idea of a late training schedule event.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_52c025f14d09471881bea2a75607be65_proc_385044/synthetic_loss_curves.png"}], [{"analysis": "Dataset: branch \u2013 Training and validation accuracy remain near zero across all 50 epochs for each E setting. Loss curves stay around 1.0 with no clear downward trend. The model fails to learn any meaningful decision boundary on branch\u2010based functions despite varying the trace\u2010length hyperparameter E, indicating that the dynamic traces collected under random inputs do not provide sufficient discriminative signal for branching semantics in contrastive pre\u2010training.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_5221f873ef9f49acba79f917e212757c_proc_400996/branch_train_val_acc_loss.png"}, {"analysis": "Dataset: loop \u2013 Accuracy curves are flat at zero and losses plateau around 1.0 across all epochs and E values. There is no sign of improvement or overfitting. This suggests that the current pipeline for generating and encoding loop traces does not capture iterative behavior patterns well, again leading to a breakdown of the contrastive objective for loop\u2010centric functions.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_5221f873ef9f49acba79f917e212757c_proc_400996/loop_train_val_acc_loss.png"}, {"analysis": "Dataset: arith \u2013 Training accuracy reaches 100% by epoch ~5 for all E values, and validation accuracy follows rapidly. Loss drops to near zero by epoch ~15. The primary effect of varying E is on convergence speed: smaller E yields slightly faster and more stable training and validation curves, while larger E introduces more noise and slower early convergence but ultimately achieves perfect performance. This demonstrates that for simple arithmetic functions, dynamic traces deliver a strong learning signal, and trace\u2010length mainly affects optimization dynamics rather than final quality.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_5221f873ef9f49acba79f917e212757c_proc_400996/arith_train_val_acc_loss.png"}], [{"analysis": "hard_negative: Retrieval Accuracy vs Epoch (Synthetic) shows very rapid training convergence across all refresh intervals (E=10, 30, 50), with training accuracy reaching 100% by epoch 10 or earlier. Validation curves differ sharply: E=30 achieves perfect retrieval by epoch 4, E=10 by epoch 10, and E=50 only by epoch 18. This indicates that a moderate refresh interval (30) yields the fastest generalization, while too-infrequent updates (E=50) slow down validation performance, and too-frequent updates (E=10) offer less stable negatives early on.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_dcb9f657828c4f6dbf6d7dafa8238d07_proc_400997/hard_negative_accuracy_curves.png"}, {"analysis": "random_negative: Loss Curves (Synthetic) reveal that all training losses descend smoothly to zero by roughly epoch 12 or earlier for each refresh interval. Validation losses similarly fall to near zero by epoch 15, though occasional spikes appear for E=50 beyond epoch 20\u201350, likely due to sampling noise. Overall, random negatives constitute an easy task, saturating quickly across all configurations with minimal discrepancy between refresh intervals.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_dcb9f657828c4f6dbf6d7dafa8238d07_proc_400997/random_negative_loss_curves.png"}, {"analysis": "random_negative: Retrieval Accuracy vs Epoch (Synthetic) confirms the ease of random negative mining, with training accuracies reaching 100% by epoch 5 across the board. Validation accuracies hit 75% by epochs 3\u20134 and perfect retrieval by epochs 8 (E=10), 6 (E=30), and 15 (E=50). Similar trends to hard negatives emerge\u2014moderate refresh (E=30) converges fastest, and stale negatives (E=50) lag behind, but overall convergence is faster and more uniform than with hard negatives.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_dcb9f657828c4f6dbf6d7dafa8238d07_proc_400997/random_negative_accuracy_curves.png"}, {"analysis": "hard_negative: Loss Curves (Synthetic) indicate that all training losses drop to zero around epoch 10\u201312, demonstrating effective fitting of contrastive objectives with hard negatives. Validation losses plateau above zero (around 0.27\u20130.35) after ~20\u201330 epochs, reflecting the intrinsic difficulty of hard-negative discrimination. Among intervals, E=30 yields the lowest final validation loss (~0.27), E=10 slightly higher, and E=50 the highest. This highlights that moderate refresh frequency balances negative freshness and training stability to minimize validation loss.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_dcb9f657828c4f6dbf6d7dafa8238d07_proc_400997/hard_negative_loss_curves.png"}], [], [{"analysis": "Loss curves for margin=1.0 show a smooth decay from ~0.95 down to near zero over 15 epochs, with training and validation losses closely tracking one another, indicating stable optimization without overfitting. Accuracy curves reveal that the model achieves perfect train accuracy by epoch 8 and perfect validation accuracy by epoch 10, demonstrating good generalization for this margin setting.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_2120288f76b5437ab67ae259f95ba70a_proc_400997/synthetic_margin_1.0_curves.png"}, {"analysis": "For margin=2.0, the initial loss is higher (~2.0) and declines more slowly, with noticeable noise in the validation loss, reaching near zero around epoch 19. Accuracy climbs steadily and reaches 100% on both train and validation by epoch 10, but with a slower mid\u2010phase improvement compared to margin 1.0. The larger margin increases optimization difficulty yet still converges to perfect accuracy.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_2120288f76b5437ab67ae259f95ba70a_proc_400997/synthetic_margin_2.0_curves.png"}, {"analysis": "With margin=0.1, the loss starts much lower (~0.15) and quickly goes to zero within 8 epochs with minimal oscillation, and accuracy reaches 100% on training by epoch 8 and on validation by epoch 9. The small margin accelerates convergence significantly without harming final performance.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_2120288f76b5437ab67ae259f95ba70a_proc_400997/synthetic_margin_0.1_curves.png"}, {"analysis": "The final validation accuracy vs margin plot is flat at 1.0 across margins [0.1, 0.5, 1.0, 2.0], confirming that all tested margins achieve perfect performance on the synthetic dataset, making final accuracy insensitive to margin choice in this toy setting.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_2120288f76b5437ab67ae259f95ba70a_proc_400997/synthetic_final_val_accuracy.png"}, {"analysis": "For margin=0.5, loss decreases from ~0.5 to zero by epoch 12 with low noise, and accuracy hits 100% for training by epoch 6 and for validation by epoch 7. This intermediate margin yields a convergence speed between the small and large margin cases, consistent with the trend that smaller margins accelerate learning.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_2120288f76b5437ab67ae259f95ba70a_proc_400997/synthetic_margin_0.5_curves.png"}], [], [{"analysis": "Retrieval accuracy curves for the LSTM\u2010based encoder show a clear learning dynamic: all three training regimes (10, 30, 50 epochs) start around chance (\u22480.5) and rapidly climb to perfect retrieval (1.0) by roughly epoch 8. Validation follows training closely, with no overfitting even at 50 epochs. Contrast this with the mean\u2010pool encoder, which sits at 1.0 accuracy from the first epoch onward under all epoch budgets\u2014suggesting either a trivial synthetic retrieval task or a pathology in how similarity is measured for this encoder.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_78a2aa84090e43c18630696a2802fe5e_proc_400996/synthetic_retrieval_accuracy.png"}, {"analysis": "Loss curves reinforce the LSTM encoder\u2019s effective fitting of synthetic data. Training loss falls from \u22481.0 to near zero by epoch 15, and validation loss mirrors this drop with only minor fluctuation; further training (up to 50 epochs) yields diminishing returns. The mean\u2010pool encoder shows neither systematic loss reduction nor separation between train/validation: its loss hovers in the 0.36\u20130.52 range with noisy oscillations throughout, indicating that its embeddings are not improving but still yield perfect retrieval for this dataset.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_78a2aa84090e43c18630696a2802fe5e_proc_400996/synthetic_loss_curves.png"}, {"analysis": "Cosine similarity gap (positive minus negative pair similarity) for the LSTM encoder widens steadily from near zero to around 0.5 by the end of training, confirming that it learns to pull trace\u2010equivalent snippets together and push dissimilar ones apart. Validation gap trends closely behind training. By contrast, the mean\u2010pool encoder remains stuck at a very small gap (\u22480.02\u20130.04), with noisy fluctuations and no upward trend\u2014again showing that this simple aggregator fails to develop meaningful semantic separation despite perfect retrieval accuracy on this synthetic task.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_78a2aa84090e43c18630696a2802fe5e_proc_400996/synthetic_gap_curves.png"}], [{"analysis": "Train curves across embedding sizes E=10, 30, 50 converge to perfect retrieval accuracy within about 10 epochs. Validation curves exhibit a brief lag at lower embedding sizes\u2014E=30 peaks around epoch 10\u2014while E=50 reaches 100% retrieval by epoch 8. E=10 shows an intermediate behavior. The rapid attainment of 100% on both splits indicates the LSTM head has ample capacity to memorize synthetic retrieval pairs, with larger embeddings accelerating validation convergence slightly.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/lstm_accuracy_synthetic.png"}, {"analysis": "Mean-pooling head achieves 100% training and validation retrieval accuracy from epoch 1 onward, regardless of embedding size. This suggests the synthetic retrieval task is trivial under mean aggregation, and the model effectively separates positive and negative pairs immediately. There is no observable learning curve, implying perfect separability using this aggregation.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/mean_pool_accuracy_synthetic.png"}, {"analysis": "Alignment gap under mean pooling remains low and slightly decreases over training. E=30 yields the smallest gap (~0.015 at later epochs), followed by E=10 (~0.025) and E=50 (~0.028). Validation gaps follow similar order but are marginally higher. The small, stable alignment gaps demonstrate strong concordance between trace-augmented views under mean pooling, suggesting minimal distributional shift between train and validation similarity scores.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/mean_pool_align_gap_synthetic.png"}, {"analysis": "LSTM loss curves show both training and validation loss dropping from ~1.0 to zero by epoch 15. This full collapse aligns with perfect retrieval accuracy, confirming the LSTM head overfits the synthetic dataset quickly. The zero loss plateau from epoch 15 onward indicates no regularization effect and a lack of difficulty in the synthetic classification objective.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/lstm_loss_curves_synthetic.png"}, {"analysis": "Under mean pooling, loss curves start around 0.4 for training and 0.33 for validation (E=10) and hover between 0.35 and 0.47 throughout 50 epochs for all embedding sizes. There is no sharp decline; instead, loss oscillates mildly. Larger embeddings (E=50) trend toward lower steady-state loss (~0.36) and smaller embeddings (E=30) sit highest (~0.45). This stable, non-zero loss contrasts with perfect retrieval accuracy, hinting at a margin-based objective where retrieval labels saturate while pairwise distances still contribute gradient signals.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/mean_pool_loss_curves_synthetic.png"}, {"analysis": "Alignment gaps under the LSTM head grow rapidly in early epochs\u2014from ~0.1 to ~0.6 on training and ~0.1 to ~0.5 on validation by epoch 20\u2014then fluctuate at high levels. E=30 shows the highest peak (~0.8 on validation), E=50 and E=10 produce slightly lower peaks (~0.5\u20130.7). The large alignment gaps indicate a growing discrepancy between alignment scores on the two augmented views and a substantial train/validation divergence, revealing that the LSTM head, while scoring retrieval perfectly, struggles to maintain consistent semantic alignment between representations of the same example under different augmentations.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_4407cc6fee664423ae8be210163884eb_proc_400997/lstm_align_gap_synthetic.png"}], [{"analysis": "Synthetic dataset Loss Curves for LSTM Encoder:\nAll three embedding dimensions drive both training and validation loss from ~1.0 to near zero within ~15 epochs. Validation and training lines overlap closely, indicating minimal overfitting. The E=30 variant converges fastest, followed by E=50 and then E=10. Minor noise in the validation loss for E=30 and E=50 after convergence suggests small fluctuations but overall stable behavior.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_0cd0cc9a5efc4a729d845f401c297e8f_proc_400995/lstm_synthetic_loss_curves.png"}, {"analysis": "Synthetic dataset Retrieval Accuracy for LSTM Encoder:\nEvery embedding size reaches 100% retrieval accuracy on both training and validation sets by around 10 epochs or fewer. The E=30 model achieves perfect retrieval by ~6 epochs, slightly ahead of E=50 (~8 epochs) and E=10 (~9 epochs). Once saturated, the accuracy remains flat at 1.0.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_0cd0cc9a5efc4a729d845f401c297e8f_proc_400995/lstm_synthetic_accuracy_curves.png"}, {"analysis": "Synthetic dataset Retrieval Accuracy for MEAN_POOL Encoder:\nAll dimensions hit perfect retrieval accuracy extremely quickly (by 3\u20134 epochs) and maintain it thereafter on training and validation splits. This suggests that mean pooling easily distinguishes the synthetic contrastive pairs, possibly due to the simplicity of the dataset.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_0cd0cc9a5efc4a729d845f401c297e8f_proc_400995/mean_pool_synthetic_accuracy_curves.png"}, {"analysis": "Synthetic dataset Loss Curves for MEAN_POOL Encoder:\nTraining and validation losses hover between ~0.37 and ~0.52 across epochs, without descending toward zero. Loss curves show more variability and a slight downward drift for larger embeddings, but do not match the full convergence seen with LSTM. The persistent non-zero loss, despite perfect retrieval accuracy, indicates weaker alignment of embeddings under the contrastive objective.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_0cd0cc9a5efc4a729d845f401c297e8f_proc_400995/mean_pool_synthetic_loss_curves.png"}], [], [{"analysis": "synthetic dataset \u2013 cosine triplet: Training loss curves for embedding sizes of 10, 30, and 50 dimensions show a rapid decrease, with all configurations reaching near-zero loss by around epoch 15. Validation losses follow a similar downward trajectory initially but exhibit increased noise and plateauing after epoch 20, particularly for the 50-dimensional setting, suggesting slight overfitting or instability in later training. Accuracy curves rise steeply: training accuracy achieves 100% by epoch 5 for higher-dimensional embeddings (E=30, 50) and by epoch 8 for E=10. Validation accuracy mirrors this trend, though the E=10 model lags until epoch 8 before reaching perfect accuracy. The 50-dimensional validation accuracy remains stuck at lower values until a sudden jump around epoch 7, indicating potential underexposure to certain triplet relationships early on. Overall, cosine triplet contrastive pre-training quickly fits the synthetic data but larger embedding sizes may introduce late-stage instability on held-out samples.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_aad02a7804b94b168a8de324c50e464c_proc_400995/synthetic_cosine_loss_acc.png"}, {"analysis": "synthetic dataset \u2013 euclidean triplet: Loss trajectories are similar to the cosine variant but show slightly faster convergence, with training losses hitting zero by epoch 12 across all embedding dimensions. Validation losses again decrease steadily but display sporadic spikes and a noisy plateau for E=50 after epoch 25, pointing to overfitting on synthetic examples. Accuracy for training tops out at 100% by epoch 6 (E=30, 50) and epoch 9 (E=10). Validation accuracy climbs more gradually for E=10 (reaching around 75% by epoch 7 before plateauing) and then jumps to full accuracy by epoch 10. Higher-dimensional models achieve perfect validation accuracy by epoch 6. The behavior suggests that Euclidean-triplet pre-training achieves equally strong or slightly faster convergence compared to cosine triplets, with similar overfitting tendencies in larger embedding spaces.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_aad02a7804b94b168a8de324c50e464c_proc_400995/synthetic_euclidean_loss_acc.png"}], [{"analysis": "Accuracy curves on the synthetic dataset with varying embedding dimensions reveal a strong capacity\u2013performance relationship. Models using 256-dim embeddings reach perfect training and validation accuracy by around epoch 3, while 128-dim models converge slightly slower (by epoch 5). The 64-dim configuration achieves unity on both splits by roughly epoch 7. With 32-dim embeddings, training accuracy climbs steadily but only hits 100% near epoch 13, and validation accuracy plateaus around 0.50 until showing modest improvement late in training. The 16-dim model fails to learn effectively: training accuracy rises gradually and peaks below 0.90, while validation accuracy remains stuck at 0.50 before inching up to about 0.75 at the very end. This pattern indicates that embedding dimension critically affects how quickly and how well the model can distinguish trace-equivalent from trace-dissimilar code.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_8009319c799940e694e8f42df978c1e1_proc_400997/synthetic_accuracy_curve.png"}, {"analysis": "Loss curves mirror the accuracy findings: high-capacity models (256 and 128 dims) drive training and validation loss to near zero within the first few epochs, with negligible divergence between splits. The 64-dim run takes longer (around 7\u20138 epochs), but also achieves minimal loss throughout. The 32-dim setting displays gradual loss reduction, with occasional validation spikes around epochs 25\u201327, signaling some instability or under-capacity. The 16-dim variant retains a high training loss (>0.8) and shows almost no decrease in validation loss, demonstrating its inability to capture the contrastive objectives based on dynamic traces. Overall, larger embedding sizes not only speed up convergence but also produce stable generalization, whereas smaller dimensions struggle to represent runtime behavior effectively.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_8009319c799940e694e8f42df978c1e1_proc_400997/synthetic_loss_curve.png"}], [{"analysis": "In the 30-epoch run on the synthetic dataset, training and validation loss drop sharply from about 1.0 to near zero by epoch 12, with the two curves tracking closely until that point and then exhibiting low-magnitude fluctuations thereafter. Both training and validation accuracy climb rapidly, reaching perfect or near-perfect scores by around epoch 10. In contrast, rename accuracy remains at zero for the first several epochs, then exhibits a few isolated spikes (around epochs 7, 13, and 18) up to around 0.5, but never sustains high values and returns to near zero by the end. This suggests that standard contrastive objectives drive convergence on clone and semantic prediction quickly, while the rename-invariance signal remains weak and sporadic under this schedule.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_80f4639c872f417d835aa5c4e1f4eea1_proc_400996/synthetic_dataset_E30_curves.png"}, {"analysis": "With only 10 epochs of training, the loss on both splits decreases slowly from roughly 0.99 to 0.80, and the two curves stay tightly coupled. Training accuracy rises from roughly 0.4 to 1.0 by epoch 7, and validation accuracy from about 0.5 to 1.0 by epoch 5, indicating the model quickly learns static contrastive tasks even within limited epochs. Rename accuracy holds at about 0.25 for the majority of the schedule (epochs 1\u20138) before dropping to zero in the final two epochs, again failing to build a stable invariance to renaming under this abbreviated training.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_80f4639c872f417d835aa5c4e1f4eea1_proc_400996/synthetic_dataset_E10_curves.png"}, {"analysis": "Extending to 50 epochs yields a very similar pattern to the 30-epoch case: loss plunges between epochs 5 and 15 into the 0\u20130.2 range and then oscillates at low values, and both train and validation accuracy saturate at or near 1.0 by around epoch 10\u201312. Rename accuracy again shows only occasional brief upticks to 0.25 (early), 0.5 (around epoch 28), and 0.25 (around epoch 9), before returning to 0. These transient spikes fail to translate into lasting performance, suggesting that even prolonged exposure does not meaningfully strengthen the model\u2019s rename-invariance under the current contrastive design.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_80f4639c872f417d835aa5c4e1f4eea1_proc_400996/synthetic_dataset_E50_curves.png"}], [{"analysis": "Image 1 shows the training and validation loss curves over 50 epochs on the synthetic dataset for both unidirectional and bidirectional transformer encoders with embedding dimensions E = 10, 30, and 50. Key observations:\n- All configurations eventually drive training loss nearly to zero by around epoch 15, indicating sufficient model capacity to fit the synthetic data.\n- Validation loss follows a similar downward trend but exhibits more fluctuation, especially for bidirectional models at medium and high dimensions (E = 30, 50).  This suggests the bidirectional encoder is more prone to variance on unseen examples in this synthetic setting.\n- Higher-dimensional embeddings (E = 30, 50) consistently converge faster and to a lower final validation loss than E = 10, indicating that very small embedding sizes underrepresent important features of the trace-contrastive task.\n- The unidirectional model at E = 30 achieves the fastest validation loss decline, reaching near-zero error by epoch ~12, whereas the bidirectional E = 30 shows several validation loss spikes as late as epoch 30.\n- There is no strong overfitting regime: validation loss continues to decrease alongside training loss across all settings, though the bidirectional high-dimensional cases display minor late-stage oscillations.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_b78b574eaf7343a0833f9f2662cbc161_proc_400997/synthetic_bidirectional_lstm_ablation_loss_curves.png"}, {"analysis": "Image 2 depicts retrieval accuracy (contrastive recall) over the same 50 epochs and settings. Key findings:\n- All models reach perfect (1.0) training accuracy by around epoch 20, confirming that both unidirectional and bidirectional variants can perfectly discriminate positive and negative pairs in the training set.\n- Validation accuracy lags training by a few epochs but also reaches 100% for all configurations, indicating strong generalization on the synthetic distribution.\n- Unidirectional E = 30 achieves perfect validation accuracy fastest (by epoch ~6), followed by unidirectional E = 50 (epoch ~8) and E = 10 (epoch ~14). This mirrors the loss curves\u2019 indication that medium and high embedding sizes enable quicker learning.\n- Bidirectional models converge more slowly on validation accuracy, with E = 50 hitting 1.0 by epoch ~12 and E = 30 by epoch ~15, while E = 10 requires ~20 epochs. The consistent lag behind unidirectional counterparts suggests directional masking may simplify the contrastive objective on this synthetic task.\n- Minor drops in validation accuracy around mid-training for several curves coincide with loss spikes, reinforcing the link between occasional over-variance in bidirectional training and temporary generalization dips.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_b78b574eaf7343a0833f9f2662cbc161_proc_400997/synthetic_bidirectional_lstm_ablation_accuracy_curves.png"}], [{"analysis": "Loss curves for synthetic_clean show that training loss declines rapidly across all epoch budgets, reaching near zero within roughly 10\u201315 epochs. Validation loss closely follows the training curve and also converges to zero soon after, with minimal gap between train and val on each schedule. Higher epoch budgets (E=50) introduce slight oscillations in validation loss between epochs 5\u201315, but the model still converges to zero loss by epoch ~20. These patterns indicate fast convergence and strong generalization on clean data, with only minor noise at larger budgets.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_8d56eb13f9fa42b9b5bd093264b67d80_proc_400995/synthetic_clean_loss_curves.png"}, {"analysis": "Accuracy curves for synthetic_injected reveal that training accuracy reaches 100% by epoch ~5 for every epoch budget, but validation accuracy is both lower and less stable. The E=10 run plateaus near 75%, while E=30 and E=50 peaks at ~75% later (around epochs 20 and 12 respectively) before dropping back toward 50%. This behavior suggests that injected variants lead to overfitting on noisy traces: perfect memorization of training examples with compromised and unstable generalization that worsens under longer training.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_8d56eb13f9fa42b9b5bd093264b67d80_proc_400995/synthetic_injected_accuracy_curves.png"}, {"analysis": "Loss curves for synthetic_injected indicate that training loss again drops to zero quickly (within ~10 epochs), but validation loss decreases more slowly and exhibits pronounced oscillations. The E=10 schedule only reaches near-zero validation loss by ~20 epochs; E=30 requires ~25 epochs; and E=50 does not settle until well past ~30 epochs. Persistent fluctuations in validation loss reflect the difficulty of reconciling noisy injected traces, leading to slower and more erratic convergence compared to clean data.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_8d56eb13f9fa42b9b5bd093264b67d80_proc_400995/synthetic_injected_loss_curves.png"}, {"analysis": "Accuracy curves for synthetic_clean demonstrate that training accuracy hits 100% by epoch ~4, and validation accuracy also reaches full accuracy by epoch ~6 for E=10, by ~5 for E=30, and as early as ~4 for E=50. The clean dataset yields smooth, rapid generalization with virtually no overfitting gap, confirming that static-only code variants are learned and transferred effectively by the model.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_8d56eb13f9fa42b9b5bd093264b67d80_proc_400995/synthetic_clean_accuracy_curves.png"}], [{"analysis": "Scheme: subword\nLoss curves for all three embedding sizes collapse almost to zero within a dozen epochs. Training loss falls fastest for E30, closely followed by E10 and E50. Validation loss mirrors training down to near zero, though E50 validation remains slightly noisier and elevated after epoch 15.\nAccuracy ramps to perfect scores very quickly. Training accuracy for E30 saturates at 100% by epoch 6, E10 by epoch 8, E50 by epoch 10. Validation accuracy trails by just one or two epochs, reaching full generalization by epoch 9 (E30), 11 (E10), and 15 (E50).\nAlignment gap opens rapidly: training gap for E30 peaks around 0.85, for E50 around 0.7, for E10 around 0.6. Validation gap saturates lower\u2014about 0.65 (E30), 0.55 (E50), and 0.45 (E10). The pronounced gap for E30 suggests highest capacity and overfitting risk under the subword scheme; E10 exhibits the smallest generalization gap but at the cost of marginally slower convergence.", "valid_plots_received": true, "vlm_feedback_summary": "Subword tokenization converges fastest in both loss and accuracy but shows the highest overfitting gap at medium embedding size. Smaller embeddings generalize best here, while larger ones trade speed for slight overfitting.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_fd7719de6901415ca68942fbd80bb152_proc_400997/synthetic_subword_metrics_curves.png"}, {"analysis": "Scheme: char\nLoss declines more gradually than with subword. E30 again leads training speed but not by much; E10 is close behind. E50 shows a longer tail and higher variance in validation loss, only reaching near-zero around epoch 20.\nAccuracy for char levels out at 100% more slowly: training accuracy peaks at epoch 6 (E30), 7 (E10), and 9 (E50). Validation lags more noticeably\u2014saturating by epoch 11 for E30, 12 for E10, and only by 20 for E50.\nAlignment gap growth is tempered compared to subword. Training gap tops out around 0.45 (E30), 0.38 (E50), and 0.58 (E10). Validation gap peaks lower: roughly 0.44 (E30), 0.30 (E50), and 0.58 (E10), though it fluctuates. Char tokenization with larger embeddings yields the smallest gap (best generalization) but at the expense of slower convergence; medium embeddings balance speed and gap.", "valid_plots_received": true, "vlm_feedback_summary": "Character\u2010level tokenization trains slower but generalizes better, especially at higher embedding sizes. Small embeddings overfit more. A mid\u2010sized embedding provides a good speed\u2010vs\u2010generalization trade\u2010off.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_fd7719de6901415ca68942fbd80bb152_proc_400997/synthetic_char_metrics_curves.png"}, {"analysis": "Scheme: ast\nLoss curves under AST schemes resemble subword in shape but validation loss for E30 remains elevated until mid\u2010training. All three settings hit near-zero training loss by epoch 12. Validation loss for E50 and E10 follows quickly; E30 dips later, around epoch 15.\nAccuracy climbs to perfect by epoch 6 (E50), 8 (E10), and 10 (E30). Validation accuracy lags slightly but all reach 100% by epoch 12 (E30), 10 (E10), and 9 (E50).\nAlignment gap grows to roughly 0.8 for E30, 0.75 for E50, and 0.7 for E10 in the training set. Validation gap saturates around 0.6 (E50), 0.55 (E30), and the lowest, 0.33 (E10). Here, small embeddings under AST tokenize deliver the smallest generalization gap but also exhibit the least capacity (slower loss decline). Medium and large sizes both overfit more but recover to perfect accuracy fast.", "valid_plots_received": true, "vlm_feedback_summary": "AST\u2010based inputs converge quickly and reach perfect accuracy, but medium\u2010 and large\u2010dim embeddings show higher overfitting. The smallest embedding under AST yields the best generalization gap, albeit with slower convergence than the largest.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_fd7719de6901415ca68942fbd80bb152_proc_400997/synthetic_ast_metrics_curves.png"}], [{"analysis": "Accuracy curves for nine head configurations (head_0/1/2 with sizes 10/30/50) show very rapid convergence on both training and validation retrieval tasks. All variants exceed 90% validation accuracy by epoch 5\u201310 and plateau at (or very near) 100% around epoch 10\u201315. head_1_30 in particular attains full validation accuracy slightly earlier (around epoch 5) than other settings, suggesting that this specific configuration learns runtime\u2010augmented features most efficiently. head_1_50 shows a small dip mid\u2010training (around epoch 20), hinting at a transient variance or mild overfitting before recovering to perfect accuracy. Overall performance across head sizes and ablation conditions is remarkably similar, indicating that reducing head dimensionality (from 50 to 10) has minimal impact on convergence speed or final accuracy in this synthetic retrieval task.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_5abfdfa0b7124a7ca1ae3f9490c7c9dd_proc_400996/synthetic_accuracy_curves.png"}, {"analysis": "Loss trajectories confirm stable, monotonic decrease across all nine variants, with training and validation losses falling to near zero by epoch 15\u201320. The gap between training and validation loss remains small, implying low overfitting despite rapid convergence. A slight bump in validation loss for head_2_50 around epoch 25 suggests occasional instability in that configuration, but it quickly corrects without harming final performance. Smaller head sizes (10, 30) match larger ones (50) in both rate of loss reduction and convergence, reinforcing that lower\u2010dimension heads suffice under these synthetic conditions.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_5abfdfa0b7124a7ca1ae3f9490c7c9dd_proc_400996/synthetic_loss_curves.png"}], [{"analysis": "Accuracy curves for different epoch budgets show that all configurations\u2019 training accuracy climbs rapidly to near 100% within 10 epochs. With 10 epochs, validation accuracy plateaus around 75%, indicating underfitting on the synthetic code tasks. Extending training to 30 epochs yields validation accuracy rising smoothly to 100% by roughly epoch 15 and stays there. A 50-epoch run reaches perfect validation accuracy even faster (around epoch 8) and remains at 100%, demonstrating that after sufficient training the CNN encoder fully captures the patterns in this synthetic dataset.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_f1380c5e79ff46038a65c1799d529f94_proc_400995/synthetic_accuracy_curves.png"}, {"analysis": "Loss curves reveal that training loss for all epoch settings falls sharply to near zero by epoch 10, confirming quick memorization of the synthetic patterns. Validation loss at 10 epochs remains above 0.2, reinforcing its underfitting behavior. Increasing to 30 epochs steadily reduces validation loss to near zero by about epoch 20, with only minor oscillations. At 50 epochs, validation loss continues to hover around zero but shows more variability, suggesting that additional training beyond 30 epochs yields diminishing returns and may introduce noise without performance gains.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_f1380c5e79ff46038a65c1799d529f94_proc_400995/synthetic_loss_curves.png"}, {"analysis": "Bar chart of final validation accuracy confirms that a 10-epoch budget yields 75% accuracy, while both 30 and 50 epochs achieve 100%. There is no improvement in accuracy beyond 30 epochs, indicating that further training does not provide additional benefit on this synthetic dataset.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_f1380c5e79ff46038a65c1799d529f94_proc_400995/synthetic_final_val_accuracy.png"}], [{"analysis": "All configurations drive training retrieval accuracy to 100% within the first 10 epochs, indicating that the model can perfectly separate positive and negative pairs on the synthetic dataset. Validation accuracy lags behind training and plateaus between roughly 60% and 80%, revealing a gap that suggests some overfitting or limited generalization under certain hyperparameter settings. Within each head group (head_0, head_1, head_2), smaller j (10) consistently yields the slowest and lowest validation accuracy, while medium (30) and large (50) j show faster improvement and higher peaks. The head_2 configurations achieve the best validation accuracy overall, particularly for j=30 and j=50, suggesting that increasing the number of projection heads combined with a moderate or large number of trace\u2010based contrasts leads to stronger retrieval generalization. A slight slowdown or plateau in validation accuracy around epochs 5\u201315 for j=50 indicates that too many negative pairs may require more training before performance stabilizes.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_43c5b1e3bd914f33839294535e89daa4_proc_400995/synthetic_accuracy_curves.png"}, {"analysis": "Training loss decreases smoothly to near zero by epoch 20 for every setting, confirming full convergence on the synthetic task. Validation loss also trends downward but exhibits noticeable spikes after epoch 20, especially for the j=50 runs, indicating occasional misalignment with held\u2010out data or sensitivity to noise in dynamic traces. Configurations with more heads (head_2) and medium j (30) converge to slightly lower final validation loss than head_0 or head_1, mirroring their superior validation accuracy. Smaller j values show marginally noisier validation loss curves, pointing to reduced stability when fewer negative samples are used in contrastive pre\u2010training. Overall, loss behavior reinforces the accuracy\u2010based insight that higher head counts and a moderate to large number of contrasts yield the most robust representations.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_43c5b1e3bd914f33839294535e89daa4_proc_400995/synthetic_loss_curves.png"}], [{"analysis": "Synthetic dataset: Training vs Validation Retrieval Accuracy shows that all methods\u2014static\u2010only (head_0), trace\u2010only (head_1), and static+trace (head_2)\u2014achieve near\u2010perfect training accuracy by epoch 10. Validation curves also climb rapidly, with trace\u2010only and combined static+trace variants typically reaching >95% accuracy slightly faster than static\u2010only. Increasing trace length from 10 to 30 or 50 speeds up early convergence for all heads, but the longest trace setting (head_2_50) exhibits a sharp drop to ~50% validation accuracy around epoch 20, and head_2_30 shows a milder dip at late epochs. Static\u2010only and trace\u2010only baselines maintain smooth validation trajectories, whereas combining static and dynamic signals with long traces can introduce overfitting and instability.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_b1aaa62174ec4019bdaead133de21454_proc_400996/synthetic_accuracy_curves.png"}, {"analysis": "Synthetic dataset: Training vs Validation Loss indicates that training loss for all configurations falls from ~1.0 to near 0 by about epoch 10, mirroring the rapid rise in accuracy and confirming strong overfitting on this synthetic corpus. Validation loss follows a similar trend: static+trace variants achieve marginally lower final loss than pure static or trace baselines but also display pronounced spikes when using long traces (head_2_50 with large noise after epoch 20, and head_2_30 with occasional jumps). Shorter trace contexts (length 10) yield a steadier loss decline with slightly higher asymptotic loss. Overall, these curves suggest that integrating dynamic execution traces accelerates learning and can lower both training and validation loss, but very long trace sequences risk oscillations and reduced generalization.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_b1aaa62174ec4019bdaead133de21454_proc_400996/synthetic_loss_curves.png"}], [{"analysis": "Synthetic dataset: Training vs Validation Retrieval Accuracy shows that all head-and-temperature configurations achieve near-perfect accuracy extremely rapidly, typically by epoch 10. Training and validation curves closely track each other, indicating minimal generalization gap on this synthetic task. Slight dips and oscillations appear in validation accuracy for head_0 at \u03c4=50 and head_2 at \u03c4=50 around epoch 20, suggesting minor instabilities or momentary overfitting when using higher temperature values at those extraction depths.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_970f702b3a6a4461a61665d7f19beca7_proc_400997/synthetic_accuracy_curves.png"}, {"analysis": "Synthetic dataset: Training vs Validation Loss shows a rapid decline to near-zero loss by epoch 15 across all head-and-temperature combinations. Validation loss curves mirror training loss closely, with occasional spikes at head_0 \u03c4=50 and head_2 \u03c4=50 that correspond to the accuracy oscillations noted earlier. Overall, the minimal train\u2013validation gap and swift convergence indicate that this synthetic task is easily mastered by the model, offering limited discrimination between ablation settings.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_970f702b3a6a4461a61665d7f19beca7_proc_400997/synthetic_loss_curves.png"}], []], "vlm_feedback_summary": ["Validation performance under short runs (10 epochs) suffers from underfitting,\nwhile medium runs (30 epochs) strike a better balance but still plateau early.\nVery long runs (50 epochs) introduce unstable behavior until a late jump in\nmetrics, suggesting a learning rate or warm\u2010up schedule impact. To improve,\nconsider fine\u2010tuning the learning\u2010rate schedule and early stopping, and add\nregularization (dropout or weight decay). Proposed additional test datasets from\nHuggingFace: \"mbpp\" for Python problem generalization and\n\"code_x_glue_cc_code_clone_detection\" for adversarial clone detection.", "Dynamic execution traces as used here fail to benefit branching and loop\ntasks\u2014likely due to poor path coverage or insufficient encoding of control\nstructures\u2014while they succeed on arithmetic functions, achieving perfect\ngeneralization. The trace\u2010length hyperparameter E has negligible influence on\nfinal metrics but does affect convergence speed on simpler tasks. Future work\nshould target richer input generation for branch/loop patterns or hybridize\nstatic and dynamic information to strengthen the contrastive signal where\ndynamic traces alone are insufficient.", "Hard negatives present a challenging learning signal that does not fully\ncollapse on validation, whereas random negatives produce trivial tasks with\nrapid saturation; a medium refresh interval (E=30) consistently achieves the\nfastest convergence and best validation metrics across both loss and retrieval\ncurves, outperforming both very frequent (E=10) and very infrequent (E=50)\nnegative updates.", "[]", "The triplet margin hyperparameter primarily affects the initial loss scale and\nconvergence speed: smaller margins lead to faster convergence, while larger\nmargins slow down training but do not alter the final performance. As all margin\nvalues achieve perfect accuracy on this synthetic dataset, margin selection here\nonly impacts training efficiency. For more nuanced insights, future ablations\nshould include more complex or noisy datasets where final generalization might\nvary across margin settings.", "[]", "LSTM encoder exhibits strong learning curves across accuracy, loss, and\nsimilarity gap, while mean\u2010pool encoder shows trivial perfect retrieval accuracy\nbut fails to improve loss or representation separation metrics, pointing to a\nweakness in its ability to capture runtime\u2010driven distinctions.", "LSTM head on synthetic data converges to perfect retrieval and zero loss but at\nthe expense of high alignment gap, indicating overfitting and inconsistent\nsemantic alignment. Mean-pooling head yields immediate perfect retrieval and\nmaintains low alignment gaps, though with moderate, stable loss. Embedding size\nE=30 minimizes alignment gap under mean pooling, while larger sizes accelerate\nconvergence under LSTM.", "LSTM encoders achieve full loss minimization and robust generalization,\nespecially at embedding size 30, while mean\u2010pool models quickly memorize easy\nsynthetic patterns but fail to close the loss gap, revealing less expressive\nrepresentations under dynamic\u2010trace contrastive pre\u2010training.", "[]", "Both contrastive variants converge extremely quickly on the synthetic dataset,\nachieving perfect accuracy in under 10 epochs for sufficiently large embeddings.\nEuclidean triplet shows marginally faster convergence than cosine triplet, but\nboth experience noisy validation loss and potential overfitting at higher\ndimensions. Embedding size influences early-stage learning stability, with E=50\nexhibiting delayed validation accuracy gains and post-convergence loss spikes.\nFuture ablations should explore regularization or early stopping to mitigate\nlate-stage instability and assess performance on more complex or realistic code\ndatasets.", "Embedding dimension scaling exerts a pronounced impact: larger dimensions\naccelerate convergence, minimize loss quickly, and yield perfect or near-perfect\naccuracy on both training and validation sets, while lower dimensions fail to\ncapture the richness of dynamic trace data, resulting in slow or stalled\nlearning and subpar retrieval performance. This capacity-performance tradeoff\nunderscores the necessity of sufficient embedding size when pre-training on\nexecution traces to learn robust code representations.", "Across all three epoch regimes, the model rapidly masters standard contrastive\npre-training objectives (clone detection) but consistently underperforms on the\nrename-invariance subtask. Adjustments to the rename augmentation frequency,\nloss weighting, or integration of auxiliary objectives may be needed to amplify\nthis weak signal.", "Across embedding dimensions, unidirectional encoders learn faster and more\nstably than bidirectional ones on this synthetic retrieval task. Embedding sizes\nof 30 and above substantially accelerate convergence and reduce variance, while\nvery small embeddings (E = 10) slow the process and yield higher final\nloss/longer to perfect accuracy. No overfitting is evident, though bidirectional\nhigh-dimensional models show slight validation fluctuations, suggesting room for\nregularization or curriculum tweaks in later experiments.", "Clean synthetic data leads to rapid, stable convergence and perfect\ngeneralization, whereas injected variants cause noisy validation loss/accuracy,\nslower convergence, and overfitting. Longer runs on injected data only partially\nmitigate noise but introduce instability, highlighting the need for specialized\nstrategies to handle dynamic trace augmentations.", "Across schemes, medium embedding size (30) converges fastest but also overfits\nmost under subword and AST; large size (50) often provides better generalization\nat the cost of slower convergence, most dramatically under char tokenization;\nsmallest size (10) underfits less but converges slower. Char tokenization shows\nthe best generalization overall, subword the fastest training, and AST sits in\nbetween.", "Both metrics indicate that most ablated variants converge quickly and generalize\nequally well on this synthetic dataset. head_1_30 offers the fastest path to\nfull accuracy, while loss curves reveal minimal overfitting and only marginal\ninstability in specific configurations.", "The CNN encoder underfits when trained only 10 epochs but converges fully by 30\nepochs, achieving perfect validation metrics. Training to 50 epochs offers no\nfurther accuracy improvement and introduces slight loss instability, suggesting\n30 epochs as an optimal budget for this synthetic code task.", "Dynamic trace\u2013augmented contrastive pre\u2010training converges quickly on synthetic\nretrieval tasks, but generalization hinges critically on hyperparameters. Larger\nprojection\u2010head counts and moderate to high negative\u2010sample counts outperform\nsmaller settings, improving validation accuracy by up to ~20 points and lowering\nvalidation loss. Validation instability for extreme negative counts (j=50)\nhighlights a trade-off between sample richness and convergence stability.", "Dynamic execution traces improve convergence speed and final performance\ncompared to static\u2010only pretraining. Pure trace\u2010only baselines also outperform\nstatic alone on early epochs. Combining static and dynamic signals yields the\nbest overall metrics, though very long trace contexts (length 50) can\ndestabilize validation performance mid\u2010training. A moderate trace length (around\n30) trades off speed and stability best. Caution is warranted to prevent\noverfitting when using large dynamic traces on simple datasets.", "On the synthetic dataset, varying extraction head and contrastive temperature\nyields nearly identical, rapid convergence with trivial generalization gaps. The\nfew instabilities at higher \u03c4 highlight potential sensitivity in those regimes,\nbut overall the synthetic task may lack sufficient complexity to reveal\nmeaningful differences in ablation configurations. Consider deploying more\nchallenging benchmarks or enriching the synthetic generation process to better\nstress-test each component.", "[]"], "exec_time": [3.924856662750244, 8.12641978263855, 6.5605175495147705, 3.9715845584869385, 4.606361150741577, 5.894964933395386, 4.692869424819946, 4.945537090301514, 4.469752788543701, 7.791959762573242, 6.597585916519165, 5.758482456207275, 4.015139102935791, 7.127994537353516, 5.960958003997803, 8.496861219406128, 8.931773900985718, 4.794105291366577, 9.019335746765137, 9.067100048065186, 8.806149244308472, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["[\"synthetic\"]"], ["[arith]"], ["['synthetic']"], [], ["['synthetic']"], [], ["['synthetic']"], ["['synthetic']"], ["['synthetic']"], [], ["[\"synthetic\"]"], ["['synthetic']"], [""], ["synthetic"], ["['synthetic_clean']"], ["['subword'", "'char'", "'ast']"], ["['synthetic']"], ["[\"synthetic\"]"], ["['synthetic']"], ["['synthetic']"], ["[]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    synthetic_data = experiment_data[\"EPOCHS\"][\"synthetic\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    synthetic_data = {}\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for E, data in synthetic_data.items():\n        epochs = list(range(1, len(data[\"losses\"][\"train\"]) + 1))\n        plt.plot(epochs, data[\"losses\"][\"train\"], label=f\"Train E={E}\")\n        plt.plot(epochs, data[\"losses\"][\"val\"], linestyle=\"--\", label=f\"Val E={E}\")\n    plt.suptitle(\"Synthetic Dataset\")\n    plt.title(\"Train vs Validation Loss Curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Plot accuracy curves\ntry:\n    plt.figure()\n    for E, data in synthetic_data.items():\n        epochs = list(range(1, len(data[\"metrics\"][\"train\"]) + 1))\n        plt.plot(epochs, data[\"metrics\"][\"train\"], label=f\"Train E={E}\")\n        plt.plot(epochs, data[\"metrics\"][\"val\"], linestyle=\"--\", label=f\"Val E={E}\")\n    plt.suptitle(\"Synthetic Dataset\")\n    plt.title(\"Train vs Validation Retrieval Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Extract ablation results\ndata_root = experiment_data.get(\"multi_dataset_synthetic_ablation\", {})\n\n# Print final validation accuracies\nfor name, ds in data_root.items():\n    for E, d in sorted(ds[\"EPOCHS\"].items()):\n        val_acc = d[\"metrics\"][\"val\"][-1]\n        print(f\"Dataset={name}, EPOCHS={E}, final val_acc={val_acc:.4f}\")\n\n# Plot for each dataset\nfor name, ds in data_root.items():\n    try:\n        per_epoch = ds[\"EPOCHS\"]\n        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n        # Left subplot: Accuracy\n        for E, d in sorted(per_epoch.items()):\n            axes[0].plot(\n                range(1, len(d[\"metrics\"][\"train\"]) + 1),\n                d[\"metrics\"][\"train\"],\n                label=f\"Train E={E}\",\n            )\n            axes[0].plot(\n                range(1, len(d[\"metrics\"][\"val\"]) + 1),\n                d[\"metrics\"][\"val\"],\n                linestyle=\"--\",\n                label=f\"Val E={E}\",\n            )\n        axes[0].set_title(\"Left: Training & Validation Accuracy\")\n        axes[0].set_xlabel(\"Epoch\")\n        axes[0].set_ylabel(\"Accuracy\")\n        axes[0].legend()\n\n        # Right subplot: Loss\n        for E, d in sorted(per_epoch.items()):\n            axes[1].plot(\n                range(1, len(d[\"losses\"][\"train\"]) + 1),\n                d[\"losses\"][\"train\"],\n                label=f\"Train E={E}\",\n            )\n            axes[1].plot(\n                range(1, len(d[\"losses\"][\"val\"]) + 1),\n                d[\"losses\"][\"val\"],\n                linestyle=\"--\",\n                label=f\"Val E={E}\",\n            )\n        axes[1].set_title(\"Right: Training & Validation Loss\")\n        axes[1].set_xlabel(\"Epoch\")\n        axes[1].set_ylabel(\"Loss\")\n        axes[1].legend()\n\n        fig.suptitle(f\"Dataset: {name}\")\n        save_path = os.path.join(working_dir, f\"{name}_train_val_acc_loss.png\")\n        plt.savefig(save_path)\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot for dataset {name}: {e}\")\n        plt.close()  # Ensure figure is closed even on error\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# For each ablation, plot loss and accuracy curves\nfor ablation in [\"random_negative\", \"hard_negative\"]:\n    # Loss curves\n    try:\n        plt.figure()\n        for E, data in experiment_data[ablation][\"synthetic\"].items():\n            epochs = list(range(1, len(data[\"losses\"][\"train\"]) + 1))\n            plt.plot(epochs, data[\"losses\"][\"train\"], label=f\"train E={E}\")\n            plt.plot(epochs, data[\"losses\"][\"val\"], label=f\"val E={E}\")\n        plt.title(f\"{ablation}: Loss Curves (Synthetic)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        fname = f\"{ablation}_loss_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {ablation}: {e}\")\n        plt.close()\n\n    # Accuracy curves\n    try:\n        plt.figure()\n        for E, data in experiment_data[ablation][\"synthetic\"].items():\n            epochs = list(range(1, len(data[\"metrics\"][\"train\"]) + 1))\n            plt.plot(epochs, data[\"metrics\"][\"train\"], label=f\"train E={E}\")\n            plt.plot(epochs, data[\"metrics\"][\"val\"], label=f\"val E={E}\")\n        plt.title(f\"{ablation}: Retrieval Accuracy vs Epoch (Synthetic)\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        fname = f\"{ablation}_accuracy_curves.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot for {ablation}: {e}\")\n        plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = experiment_data[\"triplet_margin_ablation\"][\"synthetic\"]\n    margins = sorted(data.keys())\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    margins = []\n\n# Plot loss & accuracy curves for each margin\nfor m in margins:\n    try:\n        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n        fig.suptitle(f\"Synthetic Dataset: Triplet Margin {m}\")\n        epochs = range(1, len(data[m][\"losses\"][\"train\"]) + 1)\n        # Loss curves\n        axs[0].plot(epochs, data[m][\"losses\"][\"train\"], label=\"Train\")\n        axs[0].plot(epochs, data[m][\"losses\"][\"val\"], label=\"Val\")\n        axs[0].set_title(\"Left: Loss curves\")\n        axs[0].set_xlabel(\"Epoch\")\n        axs[0].set_ylabel(\"Loss\")\n        axs[0].legend()\n        # Accuracy curves\n        axs[1].plot(epochs, data[m][\"metrics\"][\"train\"], label=\"Train\")\n        axs[1].plot(epochs, data[m][\"metrics\"][\"val\"], label=\"Val\")\n        axs[1].set_title(\"Right: Accuracy curves\")\n        axs[1].set_xlabel(\"Epoch\")\n        axs[1].set_ylabel(\"Accuracy\")\n        axs[1].legend()\n        fig.savefig(os.path.join(working_dir, f\"synthetic_margin_{m}_curves.png\"))\n        plt.close(fig)\n    except Exception as e:\n        print(f\"Error creating plot for margin={m}: {e}\")\n        plt.close()\n\n# Summary plot of final validation accuracy vs. margin\ntry:\n    final_vals = [data[m][\"metrics\"][\"val\"][-1] for m in margins]\n    plt.figure()\n    plt.plot(margins, final_vals, marker=\"o\")\n    plt.title(\"Synthetic Dataset: Final Validation Accuracy vs Triplet Margin\")\n    plt.xlabel(\"Margin\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_final_val_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating summary plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# load experiment data\ntry:\n    exp = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp = {}\n\n# iterate over ablations and plot\nfor ablation in [\"triplet\", \"contrastive\"]:\n    data_dict = exp.get(ablation, {}).get(\"synthetic\", {})\n    if not data_dict:\n        continue\n\n    # Loss curves\n    try:\n        plt.figure()\n        for E, data in data_dict.items():\n            epochs = np.arange(1, len(data[\"losses\"][\"train\"]) + 1)\n            plt.plot(epochs, data[\"losses\"][\"train\"], label=f\"train E={E}\")\n            plt.plot(epochs, data[\"losses\"][\"val\"], \"--\", label=f\"val E={E}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(f\"{ablation.capitalize()} Ablation Loss Curves on Synthetic Dataset\")\n        plt.legend()\n        fname = f\"{ablation}_loss_curves_synthetic.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating {ablation} loss plot: {e}\")\n        plt.close()\n\n    # Accuracy curves\n    try:\n        plt.figure()\n        for E, data in data_dict.items():\n            epochs = np.arange(1, len(data[\"metrics\"][\"train\"]) + 1)\n            plt.plot(epochs, data[\"metrics\"][\"train\"], label=f\"train E={E}\")\n            plt.plot(epochs, data[\"metrics\"][\"val\"], \"--\", label=f\"val E={E}\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Retrieval Accuracy\")\n        plt.title(\n            f\"{ablation.capitalize()} Ablation Accuracy Curves on Synthetic Dataset\"\n        )\n        plt.legend()\n        fname = f\"{ablation}_accuracy_curves_synthetic.png\"\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating {ablation} accuracy plot: {e}\")\n        plt.close()\n\n    # print final validation accuracies\n    for E, data in data_dict.items():\n        final_val = data[\"metrics\"][\"val\"][-1]\n        print(f\"{ablation} E={E} final_val_acc={final_val:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot 1: Loss Curves\ntry:\n    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n    for ax, model in zip(axs, experiment_data.keys()):\n        disp = \"LSTMEncoder\" if model == \"lstm\" else \"MeanPoolEncoder\"\n        for E in sorted(experiment_data[model][\"synthetic\"].keys()):\n            losses = experiment_data[model][\"synthetic\"][E][\"losses\"]\n            epochs = np.arange(1, len(losses[\"train\"]) + 1)\n            ax.plot(epochs, losses[\"train\"], label=f\"Train E={E}\")\n            ax.plot(epochs, losses[\"val\"], \"--\", label=f\"Val E={E}\")\n        ax.set_title(disp)\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n        ax.legend()\n    fig.suptitle(\n        \"Training and Validation Loss Curves (Synthetic)\\nLeft: LSTMEncoder, Right: MeanPoolEncoder\"\n    )\n    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n    fig.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# Plot 2: Retrieval Accuracy Curves\ntry:\n    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n    for ax, model in zip(axs, experiment_data.keys()):\n        disp = \"LSTMEncoder\" if model == \"lstm\" else \"MeanPoolEncoder\"\n        for E in sorted(experiment_data[model][\"synthetic\"].keys()):\n            metrics = experiment_data[model][\"synthetic\"][E][\"metrics\"]\n            epochs = np.arange(1, len(metrics[\"train_acc\"]) + 1)\n            ax.plot(epochs, metrics[\"train_acc\"], label=f\"Train E={E}\")\n            ax.plot(epochs, metrics[\"val_acc\"], \"--\", label=f\"Val E={E}\")\n        ax.set_title(disp)\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Accuracy\")\n        ax.legend()\n    fig.suptitle(\n        \"Retrieval Accuracy over Epochs (Synthetic)\\nLeft: LSTMEncoder, Right: MeanPoolEncoder\"\n    )\n    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n    fig.savefig(os.path.join(working_dir, \"synthetic_retrieval_accuracy.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating accuracy curves: {e}\")\n    plt.close()\n\n# Plot 3: Cosine Similarity Gap Curves\ntry:\n    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n    for ax, model in zip(axs, experiment_data.keys()):\n        disp = \"LSTMEncoder\" if model == \"lstm\" else \"MeanPoolEncoder\"\n        for E in sorted(experiment_data[model][\"synthetic\"].keys()):\n            metrics = experiment_data[model][\"synthetic\"][E][\"metrics\"]\n            epochs = np.arange(1, len(metrics[\"gap_train\"]) + 1)\n            ax.plot(epochs, metrics[\"gap_train\"], label=f\"Gap Train E={E}\")\n            ax.plot(epochs, metrics[\"gap_val\"], \"--\", label=f\"Gap Val E={E}\")\n        ax.set_title(disp)\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Cosine Gap\")\n        ax.legend()\n    fig.suptitle(\n        \"Cosine Similarity Gap over Epochs (Synthetic)\\nLeft: LSTMEncoder, Right: MeanPoolEncoder\"\n    )\n    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n    fig.savefig(os.path.join(working_dir, \"synthetic_gap_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating gap curves: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nfor name, data in experiment_data.items():\n    synthetic = data.get(\"synthetic\", {})\n    # Loss curves\n    try:\n        plt.figure()\n        for E, d in synthetic.items():\n            epochs = range(1, len(d[\"losses\"][\"train\"]) + 1)\n            plt.plot(epochs, d[\"losses\"][\"train\"], label=f\"Train E={E}\")\n            plt.plot(epochs, d[\"losses\"][\"val\"], label=f\"Val E={E}\")\n        plt.title(f\"{name} on synthetic: Loss curves\")\n        plt.suptitle(\"Left: Train Loss, Right: Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{name}_loss_curves_synthetic.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss plot for {name}: {e}\")\n        plt.close()\n    # Retrieval accuracy\n    try:\n        plt.figure()\n        for E, d in synthetic.items():\n            epochs = range(1, len(d[\"metrics\"][\"train\"]) + 1)\n            plt.plot(epochs, d[\"metrics\"][\"train\"], label=f\"Train E={E}\")\n            plt.plot(epochs, d[\"metrics\"][\"val\"], label=f\"Val E={E}\")\n        plt.title(f\"{name} on synthetic: Retrieval Accuracy\")\n        plt.suptitle(\"Left: Train Accuracy, Right: Val Accuracy\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{name}_accuracy_synthetic.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy plot for {name}: {e}\")\n        plt.close()\n    # Alignment gap\n    try:\n        plt.figure()\n        for E, d in synthetic.items():\n            epochs = range(1, len(d[\"metrics\"][\"align_gap_train\"]) + 1)\n            plt.plot(epochs, d[\"metrics\"][\"align_gap_train\"], label=f\"Train E={E}\")\n            plt.plot(epochs, d[\"metrics\"][\"align_gap_val\"], label=f\"Val E={E}\")\n        plt.title(f\"{name} on synthetic: Alignment Gap\")\n        plt.suptitle(\"Left: Train Align Gap, Right: Val Align Gap\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Alignment Gap\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{name}_align_gap_synthetic.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating align gap plot for {name}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Plot loss and accuracy curves for each encoder\nfor name, ds in experiment_data.items():\n    # Loss curves\n    try:\n        plt.figure()\n        for E, data in ds[\"synthetic\"].items():\n            epochs = np.arange(1, len(data[\"losses\"][\"train\"]) + 1)\n            plt.plot(epochs, data[\"losses\"][\"train\"], label=f\"Train E={E}\")\n            plt.plot(epochs, data[\"losses\"][\"val\"], \"--\", label=f\"Val E={E}\")\n        plt.title(f\"Synthetic dataset Loss Curves for {name.upper()} Encoder\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{name}_synthetic_loss_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curve for {name}: {e}\")\n        plt.close()\n\n    # Accuracy curves\n    try:\n        plt.figure()\n        for E, data in ds[\"synthetic\"].items():\n            epochs = np.arange(1, len(data[\"metrics\"][\"train\"]) + 1)\n            plt.plot(epochs, data[\"metrics\"][\"train\"], label=f\"Train Acc E={E}\")\n            plt.plot(epochs, data[\"metrics\"][\"val\"], \"--\", label=f\"Val Acc E={E}\")\n        plt.title(f\"Synthetic dataset Retrieval Accuracy for {name.upper()} Encoder\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, f\"{name}_synthetic_accuracy_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating accuracy curve for {name}: {e}\")\n        plt.close()\n\n# Print final validation accuracy for each setting\nfor name, ds in experiment_data.items():\n    for E, data in ds[\"synthetic\"].items():\n        final_acc = data[\"metrics\"][\"val\"][-1]\n        print(f\"{name} encoder, E={E}: final validation accuracy = {final_acc:.4f}\")\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot for Euclidean triplet\ntry:\n    ablation = \"euclidean\"\n    synthetic = experiment_data[ablation][\"synthetic\"]\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    for E, info in synthetic.items():\n        epochs = list(range(1, len(info[\"losses\"][\"train\"]) + 1))\n        axes[0].plot(epochs, info[\"losses\"][\"train\"], label=f\"Train E={E}\")\n        axes[0].plot(epochs, info[\"losses\"][\"val\"], linestyle=\"--\", label=f\"Val E={E}\")\n        axes[1].plot(epochs, info[\"metrics\"][\"train\"], label=f\"Train E={E}\")\n        axes[1].plot(epochs, info[\"metrics\"][\"val\"], linestyle=\"--\", label=f\"Val E={E}\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[0].set_title(\"Left: Loss Curves (synthetic)\")\n    axes[0].legend()\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Accuracy\")\n    axes[1].set_title(\"Right: Accuracy Curves (synthetic)\")\n    axes[1].legend()\n    fig.suptitle(\"synthetic dataset \u2013 euclidean triplet\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_euclidean_loss_acc.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating euclidean plot: {e}\")\n    plt.close()\n\n# Plot for Cosine triplet\ntry:\n    ablation = \"cosine\"\n    synthetic = experiment_data[ablation][\"synthetic\"]\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    for E, info in synthetic.items():\n        epochs = list(range(1, len(info[\"losses\"][\"train\"]) + 1))\n        axes[0].plot(epochs, info[\"losses\"][\"train\"], label=f\"Train E={E}\")\n        axes[0].plot(epochs, info[\"losses\"][\"val\"], linestyle=\"--\", label=f\"Val E={E}\")\n        axes[1].plot(epochs, info[\"metrics\"][\"train\"], label=f\"Train E={E}\")\n        axes[1].plot(epochs, info[\"metrics\"][\"val\"], linestyle=\"--\", label=f\"Val E={E}\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[0].set_title(\"Left: Loss Curves (synthetic)\")\n    axes[0].legend()\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Accuracy\")\n    axes[1].set_title(\"Right: Accuracy Curves (synthetic)\")\n    axes[1].legend()\n    fig.suptitle(\"synthetic dataset \u2013 cosine triplet\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_cosine_loss_acc.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating cosine plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = experiment_data[\"embed_hidden\"][\"synthetic\"]\n    dims = sorted(data.keys())\n    epochs = len(data[dims[0]][\"losses\"][\"train\"])\n    xs = list(range(1, epochs + 1))\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data, dims, xs = {}, [], []\n\n# Loss curves\ntry:\n    plt.figure()\n    for d in dims:\n        l = data[d][\"losses\"]\n        plt.plot(xs, l[\"train\"], label=f\"train dim={d}\")\n        plt.plot(xs, l[\"val\"], \"--\", label=f\"val dim={d}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss Curves on synthetic dataset\\nTrain: solid, Val: dashed\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve: {e}\")\n    plt.close()\n\n# Accuracy curves\ntry:\n    plt.figure()\n    for d in dims:\n        m = data[d][\"metrics\"]\n        plt.plot(xs, m[\"train\"], label=f\"train dim={d}\")\n        plt.plot(xs, m[\"val\"], \"--\", label=f\"val dim={d}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Retrieval Accuracy\")\n    plt.title(\"Accuracy Curves on synthetic dataset\\nTrain: solid, Val: dashed\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve: {e}\")\n    plt.close()\n\n# Print final validation accuracies\nfor d in dims:\n    final_acc = data[d][\"metrics\"][\"val\"][-1]\n    print(f\"Final val acc for dim={d}: {final_acc:.4f}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\nsynthetic_data = experiment_data[\"variable_renaming_invariance\"][\"synthetic\"]\n\n# plot for each epoch configuration\nfor E in sorted(synthetic_data.keys()):\n    data = synthetic_data[E]\n    epochs = list(range(1, len(data[\"losses\"][\"train\"]) + 1))\n    try:\n        fig = plt.figure(figsize=(10, 4))\n        ax1 = fig.add_subplot(1, 2, 1)\n        ax1.plot(epochs, data[\"losses\"][\"train\"], label=\"Train Loss\")\n        ax1.plot(epochs, data[\"losses\"][\"val\"], label=\"Val Loss\")\n        ax1.set_title(\"Left: Training and Validation Loss\")\n        ax1.set_xlabel(\"Epoch\")\n        ax1.set_ylabel(\"Loss\")\n        ax1.legend()\n\n        ax2 = fig.add_subplot(1, 2, 2)\n        ax2.plot(epochs, data[\"metrics\"][\"train\"], label=\"Train Acc\")\n        ax2.plot(epochs, data[\"metrics\"][\"val\"], label=\"Val Acc\")\n        ax2.plot(epochs, data[\"metrics\"][\"rename\"], label=\"Rename Acc\")\n        ax2.set_title(\"Right: Training, Validation and Rename Accuracy\")\n        ax2.set_xlabel(\"Epoch\")\n        ax2.set_ylabel(\"Accuracy\")\n        ax2.legend()\n\n        plt.suptitle(f\"Synthetic Dataset (E={E})\")\n        save_path = os.path.join(working_dir, f\"synthetic_dataset_E{E}_curves.png\")\n        plt.savefig(save_path)\n        plt.close(fig)\n        # print final metrics\n        last_val = data[\"metrics\"][\"val\"][-1]\n        last_ren = data[\"metrics\"][\"rename\"][-1]\n        print(f\"E={E}: final val_acc={last_val:.4f}, rename_acc={last_ren:.4f}\")\n    except Exception as e:\n        print(f\"Error creating plot for E={E}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    exp = experiment_data[\"bidirectional_lstm_ablation\"][\"synthetic\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Loss curves plot\ntry:\n    plt.figure()\n    for variant, results in exp.items():\n        for E, data in results.items():\n            tr = data[\"losses\"][\"train\"]\n            vl = data[\"losses\"][\"val\"]\n            plt.plot(tr, label=f\"{variant} train E={E}\")\n            plt.plot(vl, \"--\", label=f\"{variant} val E={E}\")\n    plt.title(\n        \"Training vs Validation Loss\\nsolid: train, dashed: validation on synthetic dataset\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(\n        os.path.join(\n            working_dir, \"synthetic_bidirectional_lstm_ablation_loss_curves.png\"\n        )\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Accuracy curves plot\ntry:\n    plt.figure()\n    for variant, results in exp.items():\n        for E, data in results.items():\n            ta = data[\"metrics\"][\"train\"]\n            va = data[\"metrics\"][\"val\"]\n            plt.plot(ta, label=f\"{variant} train E={E}\")\n            plt.plot(va, \"--\", label=f\"{variant} val E={E}\")\n    plt.title(\n        \"Retrieval Accuracy vs Epoch\\nsolid: train, dashed: validation on synthetic dataset\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(\n        os.path.join(\n            working_dir, \"synthetic_bidirectional_lstm_ablation_accuracy_curves.png\"\n        )\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# load experiment_data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = None\n\nif experiment_data:\n    # Print final validation accuracies\n    for dataset_name, results in experiment_data[\"dead_code_injection\"].items():\n        for E in sorted(results.keys()):\n            val_acc = results[E][\"metrics\"][\"val\"][-1]\n            print(f\"Dataset={dataset_name}, EPOCHS={E}, final val_acc={val_acc:.4f}\")\n\n    # Plot loss curves for each dataset\n    for dataset_name in experiment_data[\"dead_code_injection\"]:\n        try:\n            plt.figure()\n            data = experiment_data[\"dead_code_injection\"][dataset_name]\n            for E in sorted(data.keys()):\n                losses = data[E][\"losses\"]\n                epochs = np.arange(1, len(losses[\"train\"]) + 1)\n                plt.plot(epochs, losses[\"train\"], label=f\"Train E={E}\")\n                plt.plot(epochs, losses[\"val\"], linestyle=\"--\", label=f\"Val E={E}\")\n            plt.title(f\"Loss Curves for {dataset_name} (Train vs Val)\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            save_path = os.path.join(working_dir, f\"{dataset_name}_loss_curves.png\")\n            plt.savefig(save_path)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating {dataset_name} loss plot: {e}\")\n            plt.close()\n\n    # Plot accuracy curves for each dataset\n    for dataset_name in experiment_data[\"dead_code_injection\"]:\n        try:\n            plt.figure()\n            data = experiment_data[\"dead_code_injection\"][dataset_name]\n            for E in sorted(data.keys()):\n                metrics = data[E][\"metrics\"]\n                epochs = np.arange(1, len(metrics[\"train\"]) + 1)\n                plt.plot(epochs, metrics[\"train\"], label=f\"Train E={E}\")\n                plt.plot(epochs, metrics[\"val\"], linestyle=\"--\", label=f\"Val E={E}\")\n            plt.title(f\"Accuracy Curves for {dataset_name} (Train vs Val)\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Accuracy\")\n            plt.legend()\n            save_path = os.path.join(working_dir, f\"{dataset_name}_accuracy_curves.png\")\n            plt.savefig(save_path)\n            plt.close()\n        except Exception as e:\n            print(f\"Error creating {dataset_name} accuracy plot: {e}\")\n            plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Print final validation accuracies\nfor scheme, synth_data in (\n    experiment_data.get(\"tokenization_granularity\", {}).get(\"synthetic\", {}).items()\n):\n    for E, data in synth_data.items():\n        try:\n            fv = data[\"metrics\"][\"val_acc\"][-1]\n            print(f\"Scheme={scheme}, Epochs={E}, Final Val Acc={fv:.4f}\")\n        except Exception:\n            pass\n\n# Plot metrics curves per scheme\nfor scheme, synth_data in (\n    experiment_data.get(\"tokenization_granularity\", {}).get(\"synthetic\", {}).items()\n):\n    try:\n        fig, axes = plt.subplots(3, 1, figsize=(8, 12))\n        fig.suptitle(f\"Synthetic Dataset Metrics \u2013 Scheme: {scheme}\", fontsize=14)\n        for E, data in sorted(synth_data.items()):\n            epochs = np.arange(1, len(data[\"losses\"][\"train\"]) + 1)\n            axes[0].plot(epochs, data[\"losses\"][\"train\"], label=f\"Train loss E{E}\")\n            axes[0].plot(epochs, data[\"losses\"][\"val\"], label=f\"Val loss   E{E}\")\n            axes[1].plot(epochs, data[\"metrics\"][\"train_acc\"], label=f\"Train acc E{E}\")\n            axes[1].plot(epochs, data[\"metrics\"][\"val_acc\"], label=f\"Val acc   E{E}\")\n            axes[2].plot(\n                epochs, data[\"metrics\"][\"train_alignment_gap\"], label=f\"Train gap E{E}\"\n            )\n            axes[2].plot(\n                epochs, data[\"metrics\"][\"val_alignment_gap\"], label=f\"Val gap   E{E}\"\n            )\n        axes[0].set_title(\"Training vs. Validation Loss\")\n        axes[0].set_xlabel(\"Epoch\")\n        axes[0].set_ylabel(\"Loss\")\n        axes[0].legend()\n        axes[1].set_title(\"Training vs. Validation Accuracy\")\n        axes[1].set_xlabel(\"Epoch\")\n        axes[1].set_ylabel(\"Accuracy\")\n        axes[1].legend()\n        axes[2].set_title(\"Training vs. Validation Alignment Gap\")\n        axes[2].set_xlabel(\"Epoch\")\n        axes[2].set_ylabel(\"Alignment Gap\")\n        axes[2].legend()\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        plt.savefig(os.path.join(working_dir, f\"synthetic_{scheme}_metrics_curves.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating plot_{scheme}: {e}\")\n        plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\ntry:\n    experiment_data = np.load(data_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# extract synthetic ablation\nsyn = experiment_data.get(\"projection_head_ablation\", {}).get(\"synthetic\", {})\n\ntry:\n    plt.figure()\n    for head, runs in syn.items():\n        for epochs, d in runs.items():\n            tr = d[\"losses\"][\"train\"]\n            va = d[\"losses\"][\"val\"]\n            x = np.arange(1, len(tr) + 1)\n            plt.plot(x, tr, label=f\"{head}_{epochs}_train\")\n            plt.plot(x, va, label=f\"{head}_{epochs}_val\")\n    plt.title(\"Synthetic dataset: Training vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(loc=\"best\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    for head, runs in syn.items():\n        for epochs, d in runs.items():\n            tr = d[\"metrics\"][\"train\"]\n            va = d[\"metrics\"][\"val\"]\n            x = np.arange(1, len(tr) + 1)\n            plt.plot(x, tr, label=f\"{head}_{epochs}_train_acc\")\n            plt.plot(x, va, label=f\"{head}_{epochs}_val_acc\")\n    plt.title(\"Synthetic dataset: Training vs Validation Retrieval Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend(loc=\"best\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = exp_data[\"CNN_ENCODER_ABLATION\"][\"synthetic\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = {}\n\n# Determine epoch settings\nepochs = sorted(map(int, data.keys()))\n\n# Plot loss curves\ntry:\n    plt.figure(figsize=(8, 5))\n    for E in epochs:\n        d = data[E]\n        x = range(1, len(d[\"losses\"][\"train\"]) + 1)\n        plt.plot(x, d[\"losses\"][\"train\"], label=f\"Train Loss E={E}\")\n        plt.plot(x, d[\"losses\"][\"val\"], \"--\", label=f\"Val Loss E={E}\")\n    plt.title(\"Synthetic Dataset Loss Curves for CNN Encoder Ablation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Plot accuracy curves\ntry:\n    plt.figure(figsize=(8, 5))\n    for E in epochs:\n        d = data[E]\n        x = range(1, len(d[\"metrics\"][\"train\"]) + 1)\n        plt.plot(x, d[\"metrics\"][\"train\"], label=f\"Train Acc E={E}\")\n        plt.plot(x, d[\"metrics\"][\"val\"], \"--\", label=f\"Val Acc E={E}\")\n    plt.title(\"Synthetic Dataset Accuracy Curves for CNN Encoder Ablation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves plot: {e}\")\n    plt.close()\n\n# Plot final validation accuracy bar chart\ntry:\n    final_acc = [data[E][\"metrics\"][\"val\"][-1] for E in epochs]\n    plt.figure(figsize=(6, 4))\n    plt.bar([str(E) for E in epochs], final_acc, color=\"skyblue\")\n    plt.title(\"Synthetic Dataset Final Validation Accuracy\\nfor CNN Encoder Ablation\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Final Val Accuracy\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_final_val_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final val accuracy plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\ntry:\n    experiment_data = np.load(data_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# extract synthetic ablation\nsyn = experiment_data.get(\"projection_head_ablation\", {}).get(\"synthetic\", {})\n\ntry:\n    plt.figure()\n    for head, runs in syn.items():\n        for epochs, d in runs.items():\n            tr = d[\"losses\"][\"train\"]\n            va = d[\"losses\"][\"val\"]\n            x = np.arange(1, len(tr) + 1)\n            plt.plot(x, tr, label=f\"{head}_{epochs}_train\")\n            plt.plot(x, va, label=f\"{head}_{epochs}_val\")\n    plt.title(\"Synthetic dataset: Training vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(loc=\"best\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    for head, runs in syn.items():\n        for epochs, d in runs.items():\n            tr = d[\"metrics\"][\"train\"]\n            va = d[\"metrics\"][\"val\"]\n            x = np.arange(1, len(tr) + 1)\n            plt.plot(x, tr, label=f\"{head}_{epochs}_train_acc\")\n            plt.plot(x, va, label=f\"{head}_{epochs}_val_acc\")\n    plt.title(\"Synthetic dataset: Training vs Validation Retrieval Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend(loc=\"best\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\ntry:\n    experiment_data = np.load(data_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# extract synthetic ablation\nsyn = experiment_data.get(\"projection_head_ablation\", {}).get(\"synthetic\", {})\n\ntry:\n    plt.figure()\n    for head, runs in syn.items():\n        for epochs, d in runs.items():\n            tr = d[\"losses\"][\"train\"]\n            va = d[\"losses\"][\"val\"]\n            x = np.arange(1, len(tr) + 1)\n            plt.plot(x, tr, label=f\"{head}_{epochs}_train\")\n            plt.plot(x, va, label=f\"{head}_{epochs}_val\")\n    plt.title(\"Synthetic dataset: Training vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(loc=\"best\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    for head, runs in syn.items():\n        for epochs, d in runs.items():\n            tr = d[\"metrics\"][\"train\"]\n            va = d[\"metrics\"][\"val\"]\n            x = np.arange(1, len(tr) + 1)\n            plt.plot(x, tr, label=f\"{head}_{epochs}_train_acc\")\n            plt.plot(x, va, label=f\"{head}_{epochs}_val_acc\")\n    plt.title(\"Synthetic dataset: Training vs Validation Retrieval Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend(loc=\"best\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\ntry:\n    experiment_data = np.load(data_path, allow_pickle=True).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# extract synthetic ablation\nsyn = experiment_data.get(\"projection_head_ablation\", {}).get(\"synthetic\", {})\n\ntry:\n    plt.figure()\n    for head, runs in syn.items():\n        for epochs, d in runs.items():\n            tr = d[\"losses\"][\"train\"]\n            va = d[\"losses\"][\"val\"]\n            x = np.arange(1, len(tr) + 1)\n            plt.plot(x, tr, label=f\"{head}_{epochs}_train\")\n            plt.plot(x, va, label=f\"{head}_{epochs}_val\")\n    plt.title(\"Synthetic dataset: Training vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(loc=\"best\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    for head, runs in syn.items():\n        for epochs, d in runs.items():\n            tr = d[\"metrics\"][\"train\"]\n            va = d[\"metrics\"][\"val\"]\n            x = np.arange(1, len(tr) + 1)\n            plt.plot(x, tr, label=f\"{head}_{epochs}_train_acc\")\n            plt.plot(x, va, label=f\"{head}_{epochs}_val_acc\")\n    plt.title(\"Synthetic dataset: Training vs Validation Retrieval Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend(loc=\"best\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nexperiment_data_path_list = [\n    os.path.join(\n        os.getenv(\"AI_SCIENTIST_ROOT\"),\n        \"experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/\"\n        \"logs/0-run/experiment_results/experiment_43c5b1e3bd914f33839294535e89daa4_proc_400995/experiment_data.npy\",\n    ),\n    os.path.join(\n        os.getenv(\"AI_SCIENTIST_ROOT\"),\n        \"experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/\"\n        \"logs/0-run/experiment_results/experiment_b1aaa62174ec4019bdaead133de21454_proc_400996/experiment_data.npy\",\n    ),\n    os.path.join(\n        os.getenv(\"AI_SCIENTIST_ROOT\"),\n        \"experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/\"\n        \"logs/0-run/experiment_results/experiment_970f702b3a6a4461a61665d7f19beca7_proc_400997/experiment_data.npy\",\n    ),\n]\n\n# Load all experiment data\nall_experiment_data = []\nfor path in experiment_data_path_list:\n    try:\n        exp = np.load(path, allow_pickle=True).item()\n        all_experiment_data.append(exp)\n    except Exception as e:\n        print(f\"Error loading {path}: {e}\")\n\n# Aggregate synthetic projection_head_ablation results\nagg = {\"loss\": {}, \"acc\": {}}\nfor exp in all_experiment_data:\n    syn = exp.get(\"projection_head_ablation\", {}).get(\"synthetic\", {})\n    for head, runs in syn.items():\n        for epochs, d in runs.items():\n            # losses\n            agg[\"loss\"].setdefault(head, {}).setdefault(\n                epochs, {\"train\": [], \"val\": []}\n            )\n            agg[\"loss\"][head][epochs][\"train\"].append(np.array(d[\"losses\"][\"train\"]))\n            agg[\"loss\"][head][epochs][\"val\"].append(np.array(d[\"losses\"][\"val\"]))\n            # accuracy\n            agg[\"acc\"].setdefault(head, {}).setdefault(epochs, {\"train\": [], \"val\": []})\n            agg[\"acc\"][head][epochs][\"train\"].append(np.array(d[\"metrics\"][\"train\"]))\n            agg[\"acc\"][head][epochs][\"val\"].append(np.array(d[\"metrics\"][\"val\"]))\n\n# Plot mean \u00b1 SEM for Loss\ntry:\n    plt.figure()\n    for head, runs in agg[\"loss\"].items():\n        for epochs, data in runs.items():\n            arr_tr = np.stack(data[\"train\"], axis=0)\n            arr_va = np.stack(data[\"val\"], axis=0)\n            x = np.arange(1, arr_tr.shape[1] + 1)\n            mean_tr = arr_tr.mean(axis=0)\n            sem_tr = arr_tr.std(axis=0, ddof=1) / np.sqrt(arr_tr.shape[0])\n            mean_va = arr_va.mean(axis=0)\n            sem_va = arr_va.std(axis=0, ddof=1) / np.sqrt(arr_va.shape[0])\n            plt.plot(x, mean_tr, label=f\"{head}_{epochs}_train_mean\")\n            plt.fill_between(x, mean_tr - sem_tr, mean_tr + sem_tr, alpha=0.2)\n            plt.plot(x, mean_va, label=f\"{head}_{epochs}_val_mean\")\n            plt.fill_between(x, mean_va - sem_va, mean_va + sem_va, alpha=0.2)\n    plt.title(\"Synthetic dataset: Mean Loss Curves (Train vs Validation)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(loc=\"best\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_mean_sem.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss mean-SEM plot: {e}\")\n    plt.close()\n\n# Plot mean \u00b1 SEM for Accuracy\ntry:\n    plt.figure()\n    for head, runs in agg[\"acc\"].items():\n        for epochs, data in runs.items():\n            arr_tr = np.stack(data[\"train\"], axis=0)\n            arr_va = np.stack(data[\"val\"], axis=0)\n            x = np.arange(1, arr_tr.shape[1] + 1)\n            mean_tr = arr_tr.mean(axis=0)\n            sem_tr = arr_tr.std(axis=0, ddof=1) / np.sqrt(arr_tr.shape[0])\n            mean_va = arr_va.mean(axis=0)\n            sem_va = arr_va.std(axis=0, ddof=1) / np.sqrt(arr_va.shape[0])\n            plt.plot(x, mean_tr, label=f\"{head}_{epochs}_train_acc_mean\")\n            plt.fill_between(x, mean_tr - sem_tr, mean_tr + sem_tr, alpha=0.2)\n            plt.plot(x, mean_va, label=f\"{head}_{epochs}_val_acc_mean\")\n            plt.fill_between(x, mean_va - sem_va, mean_va + sem_va, alpha=0.2)\n    plt.title(\"Synthetic dataset: Mean Retrieval Accuracy Curves (Train vs Validation)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend(loc=\"best\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_mean_sem.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy mean-SEM plot: {e}\")\n    plt.close()\n"], "plot_plan": ["The code below loads the saved experiment data for the synthetic dataset, then creates two summary plots: one showing train vs. validation loss curves for each epoch setting, and another showing train vs. validation retrieval accuracy curves. Each plotting block is wrapped in a try-except to handle potential errors, uses clear titles and subtitles specifying the synthetic dataset, labels axes, includes a legend, and saves to the working directory with descriptive filenames while always closing the figure.", "We will load the saved experiment data from the working directory and then extract per\u2010epoch training and validation metrics for each synthetic dataset. First, we loop through each dataset (arith, branch, loop) and print out the final validation accuracy for each training length. Then for each dataset we create one figure containing two subplots: the left subplot shows training and validation accuracy curves for all epoch settings (10, 30, 50), and the right subplot shows the corresponding training and validation loss curves. We add clear subplot titles \u201cLeft: Training & Validation Accuracy\u201d and \u201cRight: Training & Validation Loss\u201d, and a figure title indicating the dataset name. Each figure is wrapped in its own try/except block to catch errors, saved into the working directory with a descriptive filename including the dataset name and plot type, and closed after saving. Finally, all prints report the key evaluation metrics.", "I will load the saved experiment data and then plot training and validation curves for both loss and retrieval accuracy. To avoid too many similar figures, I will aggregate all epoch settings (E=10,30,50) into one figure per metric per ablation, yielding four plots total. Each plotting block is wrapped in try-except, figures are closed after saving, and filenames include ablation name and metric type. The title specifies the ablation and the dataset (synthetic).", null, "We load the saved experiment results and extract the synthetic triplet\u2010margin ablation data.  \nFor each margin, we plot a two\u2010panel figure showing the training vs. validation loss on the left and accuracy on the right, with a suptitle indicating the dataset and margin.  \nWe label axes, legends, and subplot titles \u201cLeft: Loss curves\u201d and \u201cRight: Accuracy curves.\u201d  \nEach figure is saved under a descriptive filename including \u201csynthetic\u201d and the margin, and always closed afterwards.  \nAfter looping through all margins (four in total), we compile the final validation accuracies and plot them against the margin values in a summary figure.  \nAll plotting blocks are wrapped in try\u2010except so errors don\u2019t break the script, and we never simulate or assume any missing data.  \nThis produces at most five figures: one for each margin and one summary.", "First, we load the saved experiment data dictionary from the working directory. Then for each ablation type (\u201ctriplet\u201d and \u201ccontrastive\u201d) we pull out the synthetic results for each epoch setting. We create two separate figures per ablation\u2014one for plotting training and validation losses over epochs, and one for plotting training and validation retrieval accuracies. In each figure we iterate over all saved epoch settings (10, 30, 50) and plot the corresponding curves with distinct line styles and labels. Each figure has a descriptive title mentioning the ablation type and synthetic dataset, and axes are labeled clearly. All plotting operations are wrapped in try-except blocks to catch errors without crashing, and each finished figure is saved into working_dir with descriptive filenames before being closed. This keeps the number of figures manageable (4 total) and ensures reproducibility of standard visualizations. Finally, we print out the final validation accuracies for each setting.", "First, the script loads the saved `experiment_data.npy` from the working directory and handles any loading errors gracefully. It then creates three separate plotting blocks wrapped in try-excepts: one for training/validation loss curves, one for retrieval accuracy curves, and one for cosine similarity gap curves, each plotted over epochs. Within each block, side-by-side subplots compare the two encoder variants (\u201cLSTMEncoder\u201d and \u201cMeanPoolEncoder\u201d) on the synthetic dataset, overlaying curves for each epoch setting. Subplot titles and a two-line figure title clearly specify the dataset type and which model is on the left or right. Legends distinguish training vs validation and different epoch counts. Each figure is saved under a descriptive filename to `working_dir` and closed to free resources. Any errors during plotting are printed but do not halt the rest of the script.", "I will set the working directory and load the saved experiment data. Next, I loop over each encoder variant and pull out the synthetic results. For each variant I plot three separate figures: loss curves, retrieval accuracy, and alignment gap over epochs, each overlaying train vs val for all E values. Each figure is wrapped in its own try-except block to handle errors gracefully and always close the figure. The main title names the encoder and dataset, and a subtitle clarifies \u201cLeft: Train \u2026 Right: Val \u2026\u201d. Finally, each plot is saved into working_dir with a descriptive filename.", "I will load the saved experiment results from `experiment_data.npy` and then iterate over each encoder variant in the synthetic dataset. For each encoder (\u201clstm\u201d and \u201cmean_pool\u201d), I will produce two plots: training/validation loss curves and training/validation retrieval accuracy curves, combining all epoch sweeps in a single figure per metric. Each plotting step is wrapped in a separate try-except block to handle errors gracefully and ensure figures are always closed. Plot titles specify the dataset (\u201csynthetic\u201d) and encoder type, and legends clarify which lines represent training vs validation across different epoch budgets. Saved filenames incorporate the encoder name, dataset name, and plot type for clarity. After plotting, I will print out the final validation accuracy for each encoder and epoch configuration to give a concise metrics summary.", null, "First, we load the experiment results dictionary from experiment_data.npy in the working directory. Next, for each ablation type (euclidean and cosine), we assemble a plot of loss and retrieval accuracy curves across the three training regimes (E=10, 30, 50) on the synthetic dataset. We create a figure with two horizontal subplots: the left shows training and validation loss curves, and the right shows training and validation accuracy curves. Each curve is labeled by its split and epoch count. We add a main title indicating the dataset and ablation type, as well as subplot titles \"Left: Loss Curves (synthetic)\" and \"Right: Accuracy Curves (synthetic)\". Legends and axis labels ensure clarity. Each plotting block is wrapped in a try-except to report errors without halting. After saving the figure to working_dir with a descriptive filename, we always close the figure to free resources.", "We\u2019ll load the saved experiment_data.npy and extract the per\u2010dimension curves for the synthetic dataset. For each embedding size we gather train/val losses and accuracies across epochs. We then create two summary figures: one plotting all train vs. val loss curves and one plotting all train vs. val accuracy curves, with clear titles, subtitles, and legends. Each figure is wrapped in a try\u2010except block, saved under working_dir with descriptive names, and closed afterward. Finally, we print out the final validation accuracy for each embedding dimension. This ensures we visualize trends and also report key metrics to the console.", "We will load the saved experiment data and extract the synthetic\u2010dataset results. For each epoch setting E, we retrieve losses and the three retrieval accuracies across training epochs. We then create a two\u2010panel figure: the left panel plots train and validation loss versus epoch, and the right panel plots train, validation, and rename retrieval accuracy versus epoch. We label each subplot with a clear title (\u201cLeft: Training and Validation Loss\u201d and \u201cRight: Training, Validation and Rename Accuracy\u201d), add axis labels and legends, and set a suptitle indicating the dataset and E. Each figure is wrapped in its own try\u2010except to handle any errors and is saved to working_dir under a descriptive filename. After saving, we print the final validation and renaming accuracies for that E. Finally, all figures are closed to free resources.", "I will load the saved experiment data from the working directory and extract the synthetic ablation results for both unidirectional and bidirectional LSTMs. Then I will generate two main figures: one overlaying training and validation loss curves across all ablation settings, and another for the corresponding retrieval accuracy curves. Each plot is wrapped in its own try\u2010except block to handle any missing or corrupted data gracefully. The loss plot will use solid lines for training and dashed lines for validation, with labels indicating the variant and the number of training epochs. Likewise, the accuracy plot will follow the same line\u2010style convention. Both figures will have a clear title specifying the dataset and a subtitle clarifying the legend mapping to train versus validation. Finally, each figure will be saved to the working directory with descriptive filenames, and figures will be closed after saving to free up resources.", "We\u2019ll load the saved experiment data, print out the final validation accuracy for each dataset and epoch budget, then generate aggregated loss and accuracy curves for both synthetic_clean and synthetic_injected.  Each figure shows train vs val curves for all epoch budgets on the same axes, wrapped in its own try/except for robustness.  We include clear titles and labels, save to working_dir with descriptive filenames, and always close figures.", "We load the saved experiment data from the working directory and report the final validation accuracy for each tokenization scheme and training budget. We then iterate over each scheme in the synthetic experiments, wrapping each figure generation into its own try\u2013except block. For each scheme we construct a single figure with three vertically stacked subplots showing (1) training vs. validation loss, (2) training vs. validation accuracy, and (3) training vs. validation alignment gap across epochs. Within each subplot we plot curves for each training budget (E = 10, 30, 50) to facilitate comparison. We add a clear overall title mentioning the synthetic dataset and scheme, and individual subplot titles to make each metric\u2019s context explicit. Each figure is saved into the working directory under a descriptive filename. All figures are closed after saving, and any errors during plotting are caught and printed without interrupting execution.", "I will load the saved experiment data and extract the synthetic projection\u2010head ablation results. Then I produce two figures: (1) training vs validation loss curves across all head\u2010layer and epoch settings; (2) training vs validation retrieval\u2010accuracy curves similarly. Each plot is wrapped in its own try-except, titled clearly with the dataset name (\u201cSynthetic dataset\u201d) and plot type, axes are labeled, and a legend distinguishes each head/Epoch run. All figures are saved to the working directory using descriptive filenames and closed after saving. Errors during plot creation are caught and printed, ensuring the script does not crash. This yields concise, reproducible visual summaries of training dynamics and final accuracy.", "We load the saved experiment data for the CNN encoder ablation on the synthetic dataset, extract the per\u2010epoch loss and accuracy traces for each epoch setting, and sort the epoch settings. We then generate three plots: (1) combined training and validation loss curves over epochs for all experiments, (2) combined training and validation accuracy curves similarly, and (3) a bar chart of the final validation accuracy achieved at the last epoch of each experiment. Each plot is wrapped in a try\u2010except to catch and log errors, closes the figure after saving, and uses descriptive filenames including the dataset name and type of plot. We use only existing data and basic matplotlib, saving everything under the working directory.", null, null, null, null], "ablation_name": [null, "Multi\u2010Dataset Synthetic Ablation", "Negative Sampling Hardness Ablation", "Sequence Aggregation Ablation (Mean\u2010Pool vs LSTM)", "Triplet Margin Hyperparameter Ablation", "Contrastive Loss vs. Triplet Loss", null, null, null, "Tokenization Granularity Ablation", "Distance Metric Ablation", "Embedding Dimension Ablation", "Variable Renaming Invariance Ablation", "Bidirectional LSTM Ablation", "Dead Code Injection Ablation", null, "Projection Head Ablation", "CNN_ENCODER_ABLATION", null, null, null, null], "hyperparam_name": ["EPOCHS", null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["Here\u2019s a script that loads the saved `experiment_data.npy` from the `working`\ndirectory, iterates over each dataset (here just \"synthetic\") and each epoch\nsetting, and prints out the final training/validation accuracy and\ntraining/validation loss with precise labels. The dataset name is printed before\nits metrics, and each metric is clearly labeled. No plotting is performed and\nthere is no `if __name__ == \"__main__\":` block.", "The following script directly loads the `experiment_data.npy` file from the\nworking directory, extracts the \u201cmulti_dataset_synthetic_ablation\u201d section, and\nfor each dataset picks the highest\u2010epoch run (e.g. 50 epochs). It then retrieves\nthe final train accuracy, validation accuracy, train loss, and validation loss\nfrom the last epoch and prints them under each dataset header with clear metric\nlabels. All code is at the global scope and runs immediately without any `if\n__name__ == \"__main__\"` guard.", "The following script loads the saved `experiment_data.npy` from your `working`\ndirectory and iterates over both the \u201crandom_negative\u201d and \u201chard_negative\u201d\nconfigurations. For each epoch setting it extracts the final training accuracy,\nvalidation accuracy, training loss, and validation loss and prints them with\nclear labels. All code is at global scope and will execute immediately without\nany `if __name__ == \"__main__\":` guard.", "", "The following script locates the `working` directory, loads the saved\n`experiment_data.npy` file, and then iterates through the\n`triplet_margin_ablation` results. For each dataset (e.g., synthetic) and each\nmargin value, it prints the final train accuracy, validation accuracy, train\nloss, and validation loss with clear metric labels. All code runs immediately at\nthe top level without any entry-point guard.", "", "Below is a solution that loads the saved NumPy file from the `working`\ndirectory, iterates through each model variant and its associated results for\nthe synthetic dataset, and prints the dataset name followed by the final value\nof each metric with clear labels.", "We will load the saved NumPy experiment data from the working directory and\niterate through each encoder variant and the synthetic dataset. For each epoch\nconfiguration, we extract the final reported values for training accuracy,\nvalidation accuracy, training loss, validation loss, training alignment gap, and\nvalidation alignment gap. We first print the dataset name, then the model name\nand epoch count, followed by each metric clearly labeled. This runs at global\nscope without an entry\u2010point guard.", "I will load the saved experiment data from the \u201cworking\u201d directory and unpack\nthe nested dictionaries to access each model\u2019s results on each dataset. For\nevery dataset, I print its name, then iterate through each model and number of\nepochs to extract and display the final train accuracy, validation accuracy,\ntrain loss, and validation loss. All metric labels are made explicit, and the\nscript runs immediately at global scope without requiring any special entry\npoint.", "The following script loads the saved experiment data from the working directory\nand then iterates through each tokenization granularity dataset. For each\ndataset and each tokenization scheme, it prints the dataset name followed by the\nfinal train accuracy, final validation accuracy, final train loss, and final\nvalidation loss for every choice of epochs. All code runs at the global scope\nwithout any special entry point.", "I will write a script that identifies the \u201cworking\u201d directory, loads the saved\nNumPy experiment data, and unpacks its nested structure. The code then iterates\nover each ablation method and dataset, retrieves the final train and validation\naccuracy and loss for each epoch configuration, and prints them with precise\nlabels. Each output block is prefixed by the dataset name, ablation type, and\nnumber of epochs. All of this runs at the global scope without any special entry\npoint.", "I will load the saved NumPy dictionary from the working directory, then iterate\nover each dataset under the \u201cembed_hidden\u201d key. For each embedding dimension I\nwill extract the last (final) entry from the training and validation accuracy\nand loss lists. I will print the dataset name (and dimension) followed by\nclearly labeled metric names and their final values. All code runs immediately\nat the global scope without any `if __name__ == \"__main__\":` guard.", "The following script loads the saved `experiment_data.npy` file from the\n`working` directory and iterates over the `variable_renaming_invariance`\nexperiments. For each dataset and epoch setting, it extracts the final\n(last\u2010epoch) values of training accuracy, validation accuracy, renaming\naccuracy, training loss, and validation loss, then prints them with clear,\ndescriptive labels. The working directory is constructed via\n`os.path.join(os.getcwd(), 'working')` and the NumPy file is loaded with\n`allow_pickle=True`. All code runs immediately at the global scope without\nneeding an entry\u2010point guard.", "I will load the saved `experiment_data.npy` dictionary, navigate into the\n`\"bidirectional_lstm_ablation\"` \u2192 `\"synthetic\"` structure, and iterate over each\nvariant and epoch setting to extract the final train/validation accuracies and\nlosses. For each dataset, I\u2019ll print a clear dataset header followed by the\nvariant and epoch, then precise metric labels such as \u201ctrain accuracy\u201d and\n\u201cvalidation loss.\u201d No plotting or `if __name__ == \"__main__\"` guard is\nneeded\u2014this script runs immediately.", "Below is a simple script that loads the saved `experiment_data.npy` from the\n`working` folder, picks the results at the largest epoch budget for each of the\ntwo datasets, and prints out the final training/validation accuracy and loss\nwith clear labels.", "The following script loads the saved `experiment_data.npy` from the `working`\ndirectory, then iterates through the \u201csynthetic\u201d dataset under\n`tokenization_granularity` for each tokenization scheme and epoch setting. It\nprints the dataset name first, followed by the final values of train accuracy,\nvalidation accuracy, train alignment gap, validation alignment gap, final\ntraining loss, and final validation loss with clear labels. All code runs\nimmediately without any `if __name__ == \"__main__\":` block.", "Below is a script that immediately loads the `experiment_data.npy` file from the\nworking directory, iterates through each experiment and dataset, and prints out\nthe final train/validation accuracy and loss for every projection-head\nconfiguration and epoch setting.  The metric names are fully spelled out (e.g.,\n\u201cTrain accuracy,\u201d \u201cValidation loss\u201d) and the dataset name is printed once before\nits metrics.", "Here\u2019s a script that locates the `working` directory, loads the experiment data,\nand prints out the final train/validation accuracy and loss for each dataset and\nepoch setting:", "Below is a script that immediately loads the `experiment_data.npy` file from the\nworking directory, iterates through each experiment and dataset, and prints out\nthe final train/validation accuracy and loss for every projection-head\nconfiguration and epoch setting.  The metric names are fully spelled out (e.g.,\n\u201cTrain accuracy,\u201d \u201cValidation loss\u201d) and the dataset name is printed once before\nits metrics.", "Below is a script that immediately loads the `experiment_data.npy` file from the\nworking directory, iterates through each experiment and dataset, and prints out\nthe final train/validation accuracy and loss for every projection-head\nconfiguration and epoch setting.  The metric names are fully spelled out (e.g.,\n\u201cTrain accuracy,\u201d \u201cValidation loss\u201d) and the dataset name is printed once before\nits metrics.", "Below is a script that immediately loads the `experiment_data.npy` file from the\nworking directory, iterates through each experiment and dataset, and prints out\nthe final train/validation accuracy and loss for every projection-head\nconfiguration and epoch setting.  The metric names are fully spelled out (e.g.,\n\u201cTrain accuracy,\u201d \u201cValidation loss\u201d) and the dataset name is printed once before\nits metrics.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over each dataset under the \"EPOCHS\" key\nfor dataset_name, epoch_dict in experiment_data.get(\"EPOCHS\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    # Sort by epoch count for consistent ordering\n    for E in sorted(epoch_dict):\n        data = epoch_dict[E]\n        train_acc = data[\"metrics\"][\"train\"][-1]\n        val_acc = data[\"metrics\"][\"val\"][-1]\n        train_loss = data[\"losses\"][\"train\"][-1]\n        val_loss = data[\"losses\"][\"val\"][-1]\n        print(f\"EPOCHS = {E}\")\n        print(f\"train accuracy: {train_acc:.4f}\")\n        print(f\"validation accuracy: {val_acc:.4f}\")\n        print(f\"training loss: {train_loss:.4f}\")\n        print(f\"validation loss: {val_loss:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Define working directory and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Extract the ablation results\nablation = experiment_data.get(\"multi_dataset_synthetic_ablation\", {})\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, dataset_data in ablation.items():\n    epochs_data = dataset_data.get(\"EPOCHS\", {})\n    if not epochs_data:\n        continue\n\n    # Select the run with the maximum epoch count (e.g., 50)\n    max_epoch = max(epochs_data)\n    data = epochs_data[max_epoch]\n\n    # Extract final metrics from the last epoch\n    train_acc = data[\"metrics\"][\"train\"][-1]\n    val_acc = data[\"metrics\"][\"val\"][-1]\n    train_loss = data[\"losses\"][\"train\"][-1]\n    val_loss = data[\"losses\"][\"val\"][-1]\n\n    # Print results with clear labels\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"Train accuracy: {train_acc:.4f}\")\n    print(f\"Validation accuracy: {val_acc:.4f}\")\n    print(f\"Train loss: {train_loss:.4f}\")\n    print(f\"Validation loss: {val_loss:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Define the working directory and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each ablation and its results\nfor ablation, dataset_types in experiment_data.items():\n    for set_name, exps in dataset_types.items():\n        print(f\"Dataset: {ablation} ({set_name})\")\n        # For each number of epochs, print final metrics\n        for epochs, results in exps.items():\n            train_acc_final = results[\"metrics\"][\"train\"][-1]\n            val_acc_final = results[\"metrics\"][\"val\"][-1]\n            train_loss_final = results[\"losses\"][\"train\"][-1]\n            val_loss_final = results[\"losses\"][\"val\"][-1]\n            print(f\"EPOCHS = {epochs}\")\n            print(f\"Train accuracy: {train_acc_final:.4f}\")\n            print(f\"Validation accuracy: {val_acc_final:.4f}\")\n            print(f\"Train loss: {train_loss_final:.4f}\")\n            print(f\"Validation loss: {val_loss_final:.4f}\")\n        print()\n", "", "import os\nimport numpy as np\n\n# Construct working directory path and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract and print metrics for each dataset and margin\nfor dataset_name, margins in experiment_data.get(\"triplet_margin_ablation\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    for margin, results in margins.items():\n        train_acc = results[\"metrics\"][\"train\"][-1]\n        val_acc = results[\"metrics\"][\"val\"][-1]\n        train_loss = results[\"losses\"][\"train\"][-1]\n        val_loss = results[\"losses\"][\"val\"][-1]\n\n        print(f\"Margin: {margin}\")\n        print(f\"Train accuracy: {train_acc:.4f}\")\n        print(f\"Validation accuracy: {val_acc:.4f}\")\n        print(f\"Train loss: {train_loss:.4f}\")\n        print(f\"Validation loss: {val_loss:.4f}\")\n    print()\n", "", "import os\nimport numpy as np\n\n# 0. Define working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# 1. Load the experiment_data.npy file\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# 2. Extract and print final metrics\nfor model_name, datasets in experiment_data.items():\n    for dataset_name, epochs_dict in datasets.items():\n        # 3. Print dataset name once\n        print(f\"Dataset: {dataset_name}\")\n        # Iterate through each epoch configuration\n        for num_epochs, run_data in epochs_dict.items():\n            # 5. Get final (last epoch) values\n            final_train_acc = run_data[\"metrics\"][\"train_acc\"][-1]\n            final_val_acc = run_data[\"metrics\"][\"val_acc\"][-1]\n            final_train_loss = run_data[\"losses\"][\"train\"][-1]\n            final_val_loss = run_data[\"losses\"][\"val\"][-1]\n            final_gap_train = run_data[\"metrics\"][\"gap_train\"][-1]\n            final_gap_val = run_data[\"metrics\"][\"gap_val\"][-1]\n\n            # 4. Print with clear metric names\n            print(f\"Model Variant: {model_name}, Epochs: {num_epochs}\")\n            print(f\"train accuracy: {final_train_acc:.4f}\")\n            print(f\"validation accuracy: {final_val_acc:.4f}\")\n            print(f\"training loss: {final_train_loss:.4f}\")\n            print(f\"validation loss: {final_val_loss:.4f}\")\n            print(f\"training gap: {final_gap_train:.4f}\")\n            print(f\"validation gap: {final_gap_val:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over each encoder and dataset\nfor model_name, datasets in experiment_data.items():\n    for dataset_name, epochs_dict in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        # For each epoch setting, extract and print the final metrics\n        for num_epochs, data in epochs_dict.items():\n            metrics = data[\"metrics\"]\n            losses = data[\"losses\"]\n            train_acc = metrics[\"train\"][-1]\n            val_acc = metrics[\"val\"][-1]\n            train_loss = losses[\"train\"][-1]\n            val_loss = losses[\"val\"][-1]\n            train_gap = metrics[\"align_gap_train\"][-1]\n            val_gap = metrics[\"align_gap_val\"][-1]\n\n            print(f\"Model: {model_name}, Epochs: {num_epochs}\")\n            print(f\"  train accuracy: {train_acc:.4f}\")\n            print(f\"  validation accuracy: {val_acc:.4f}\")\n            print(f\"  train loss: {train_loss:.4f}\")\n            print(f\"  validation loss: {val_loss:.4f}\")\n            print(f\"  training alignment gap: {train_gap:.4f}\")\n            print(f\"  validation alignment gap: {val_gap:.4f}\")\n", "import os\nimport numpy as np\n\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through the loaded data and print final metrics\nfor model_name, datasets in experiment_data.items():\n    for dataset_name, epochs_dict in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        for epoch_count in sorted(epochs_dict.keys()):\n            result = epochs_dict[epoch_count]\n            final_train_acc = result[\"metrics\"][\"train\"][-1]\n            final_val_acc = result[\"metrics\"][\"val\"][-1]\n            final_train_loss = result[\"losses\"][\"train\"][-1]\n            final_val_loss = result[\"losses\"][\"val\"][-1]\n            print(f\"Model: {model_name}, Epochs: {epoch_count}\")\n            print(f\"train accuracy:      {final_train_acc:.4f}\")\n            print(f\"validation accuracy: {final_val_acc:.4f}\")\n            print(f\"train loss:          {final_train_loss:.4f}\")\n            print(f\"validation loss:     {final_val_loss:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each dataset under tokenization granularity\nfor dataset_name, dataset_dict in experiment_data.get(\n    \"tokenization_granularity\", {}\n).items():\n    print(f\"Dataset: {dataset_name}\")\n    # Each tokenization scheme (e.g., char, subword, ast)\n    for scheme, scheme_dict in dataset_dict.items():\n        # Each epoch count experiment within the scheme\n        for epochs, data in sorted(scheme_dict.items()):\n            # Extract final values for metrics and losses\n            train_acc = data[\"metrics\"][\"train\"][-1]\n            val_acc = data[\"metrics\"][\"val\"][-1]\n            train_loss = data[\"losses\"][\"train\"][-1]\n            val_loss = data[\"losses\"][\"val\"][-1]\n            # Print out the results\n            print(f\"Tokenization scheme: {scheme}, Epochs: {epochs}\")\n            print(f\"  train accuracy: {train_acc:.4f}\")\n            print(f\"  validation accuracy: {val_acc:.4f}\")\n            print(f\"  train loss: {train_loss:.4f}\")\n            print(f\"  validation loss: {val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each ablation method and dataset, extracting final metrics\nfor ablation, ablation_dict in experiment_data.items():\n    for dataset_name, epoch_dict in ablation_dict.items():\n        for epochs, metrics in epoch_dict.items():\n            final_train_acc = metrics[\"metrics\"][\"train\"][-1]\n            final_validation_acc = metrics[\"metrics\"][\"val\"][-1]\n            final_train_loss = metrics[\"losses\"][\"train\"][-1]\n            final_validation_loss = metrics[\"losses\"][\"val\"][-1]\n\n            print(f\"Dataset: {dataset_name}, Ablation: {ablation}, Epochs: {epochs}\")\n            print(f\"Final train accuracy: {final_train_acc:.4f}\")\n            print(f\"Final validation accuracy: {final_validation_acc:.4f}\")\n            print(f\"Final train loss: {final_train_loss:.4f}\")\n            print(f\"Final validation loss: {final_validation_loss:.4f}\")\n            print()\n", "import os\nimport numpy as np\n\n# Define working directory and load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over datasets under 'embed_hidden'\nfor dataset_name, dims_data in experiment_data.get(\"embed_hidden\", {}).items():\n    for dim, data in dims_data.items():\n        # Extract final metrics\n        final_train_acc = data[\"metrics\"][\"train\"][-1]\n        final_val_acc = data[\"metrics\"][\"val\"][-1]\n        final_train_loss = data[\"losses\"][\"train\"][-1]\n        final_val_loss = data[\"losses\"][\"val\"][-1]\n\n        # Print results\n        print(f\"Dataset: {dataset_name} (Dimension: {dim})\")\n        print(f\"train accuracy: {final_train_acc:.4f}\")\n        print(f\"validation accuracy: {final_val_acc:.4f}\")\n        print(f\"training loss: {final_train_loss:.4f}\")\n        print(f\"validation loss: {final_val_loss:.4f}\")\n        print()  # blank line for readability\n", "import os\nimport numpy as np\n\n# Load experiment data from working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through experiments and datasets\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, epoch_results in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        for epochs, data in sorted(epoch_results.items()):\n            # Extract final metrics and losses\n            train_accuracy = data[\"metrics\"][\"train\"][-1]\n            validation_accuracy = data[\"metrics\"][\"val\"][-1]\n            renaming_accuracy = data[\"metrics\"][\"rename\"][-1]\n            training_loss = data[\"losses\"][\"train\"][-1]\n            validation_loss = data[\"losses\"][\"val\"][-1]\n\n            # Print results for this epoch count\n            print(f\"Epochs: {epochs}\")\n            print(f\"Train accuracy: {train_accuracy:.4f}\")\n            print(f\"Validation accuracy: {validation_accuracy:.4f}\")\n            print(f\"Renaming accuracy: {renaming_accuracy:.4f}\")\n            print(f\"Train loss: {training_loss:.4f}\")\n            print(f\"Validation loss: {validation_loss:.4f}\")\n            print()  # blank line for readability\n", "import os\nimport numpy as np\n\n# Define working directory and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Navigate through the ablation study and print final metrics\nfor ablation_name, datasets in experiment_data.items():\n    for dataset_name, variants in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        for variant_name, epochs_dict in variants.items():\n            for epoch_count, results in epochs_dict.items():\n                # Extract final metrics and losses\n                train_acc = results[\"metrics\"][\"train\"][-1]\n                val_acc = results[\"metrics\"][\"val\"][-1]\n                train_loss = results[\"losses\"][\"train\"][-1]\n                val_loss = results[\"losses\"][\"val\"][-1]\n\n                # Print variant and epoch settings\n                print(f\"Variant: {variant_name}, Epochs: {epoch_count}\")\n                # Print precise metric labels\n                print(f\"  train accuracy: {train_acc:.4f}\")\n                print(f\"  validation accuracy: {val_acc:.4f}\")\n                print(f\"  train loss: {train_loss:.4f}\")\n                print(f\"  validation loss: {val_loss:.4f}\")\n        print()  # blank line between datasets\n", "import os\nimport numpy as np\n\n# Locate and load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each dataset in the dead_code_injection experiments\nfor dataset_name, budget_results in experiment_data[\"dead_code_injection\"].items():\n    # Select the results from the largest epoch budget\n    max_epochs = max(budget_results.keys())\n    data = budget_results[max_epochs]\n\n    # Extract the final recorded metrics\n    train_accuracy = data[\"metrics\"][\"train\"][-1]\n    validation_accuracy = data[\"metrics\"][\"val\"][-1]\n    train_loss = data[\"losses\"][\"train\"][-1]\n    validation_loss = data[\"losses\"][\"val\"][-1]\n\n    # Print dataset name and clearly labeled metrics\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Accuracy: {validation_accuracy:.4f}\")\n    print(f\"Training Loss: {train_loss:.4f}\")\n    print(f\"Validation Loss: {validation_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract and print final metrics\nfor dataset_name, dataset in experiment_data.get(\n    \"tokenization_granularity\", {}\n).items():\n    print(f\"Dataset: {dataset_name}\")\n    for scheme, scheme_data in dataset.items():\n        for epochs, results in scheme_data.items():\n            metrics = results[\"metrics\"]\n            losses = results[\"losses\"]\n            # Final metric values\n            train_acc = metrics[\"train_acc\"][-1]\n            val_acc = metrics[\"val_acc\"][-1]\n            train_gap = metrics[\"train_alignment_gap\"][-1]\n            val_gap = metrics[\"val_alignment_gap\"][-1]\n            train_loss_final = losses[\"train\"][-1]\n            val_loss_final = losses[\"val\"][-1]\n            print(f\"\\nTokenization scheme: {scheme}, Epochs: {epochs}\")\n            print(f\"train accuracy: {train_acc:.4f}\")\n            print(f\"validation accuracy: {val_acc:.4f}\")\n            print(f\"train alignment gap: {train_gap:.4f}\")\n            print(f\"validation alignment gap: {val_gap:.4f}\")\n            print(f\"final training loss: {train_loss_final:.4f}\")\n            print(f\"final validation loss: {val_loss_final:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through experiments and datasets\nfor exp_name, exp_content in experiment_data.items():\n    for dataset_name, dataset_content in exp_content.items():\n        print(f\"Dataset: {dataset_name}\")\n        # For each projection-head ablation setting\n        for head_config in sorted(dataset_content.keys()):\n            # For each number of training epochs\n            epochs_dict = dataset_content[head_config]\n            for epoch_count in sorted(epochs_dict.keys()):\n                data = epochs_dict[epoch_count]\n                train_acc = data[\"metrics\"][\"train\"][-1]\n                val_acc = data[\"metrics\"][\"val\"][-1]\n                train_loss = data[\"losses\"][\"train\"][-1]\n                val_loss = data[\"losses\"][\"val\"][-1]\n\n                print(f\"Projection head setting: {head_config}, Epochs: {epoch_count}\")\n                print(f\"  Train accuracy: {train_acc:.4f}\")\n                print(f\"  Validation accuracy: {val_acc:.4f}\")\n                print(f\"  Train loss: {train_loss:.4f}\")\n                print(f\"  Validation loss: {val_loss:.4f}\")\n                print()\n", "import os\nimport numpy as np\n\n# Load saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through experiments and datasets\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, results_by_epoch in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        for epochs, data in results_by_epoch.items():\n            train_acc = data[\"metrics\"][\"train\"][-1]\n            val_acc = data[\"metrics\"][\"val\"][-1]\n            train_loss = data[\"losses\"][\"train\"][-1]\n            val_loss = data[\"losses\"][\"val\"][-1]\n            print(f\"  Epochs: {epochs}\")\n            print(f\"    train accuracy: {train_acc:.4f}\")\n            print(f\"    validation accuracy: {val_acc:.4f}\")\n            print(f\"    train loss: {train_loss:.4f}\")\n            print(f\"    validation loss: {val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through experiments and datasets\nfor exp_name, exp_content in experiment_data.items():\n    for dataset_name, dataset_content in exp_content.items():\n        print(f\"Dataset: {dataset_name}\")\n        # For each projection-head ablation setting\n        for head_config in sorted(dataset_content.keys()):\n            # For each number of training epochs\n            epochs_dict = dataset_content[head_config]\n            for epoch_count in sorted(epochs_dict.keys()):\n                data = epochs_dict[epoch_count]\n                train_acc = data[\"metrics\"][\"train\"][-1]\n                val_acc = data[\"metrics\"][\"val\"][-1]\n                train_loss = data[\"losses\"][\"train\"][-1]\n                val_loss = data[\"losses\"][\"val\"][-1]\n\n                print(f\"Projection head setting: {head_config}, Epochs: {epoch_count}\")\n                print(f\"  Train accuracy: {train_acc:.4f}\")\n                print(f\"  Validation accuracy: {val_acc:.4f}\")\n                print(f\"  Train loss: {train_loss:.4f}\")\n                print(f\"  Validation loss: {val_loss:.4f}\")\n                print()\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through experiments and datasets\nfor exp_name, exp_content in experiment_data.items():\n    for dataset_name, dataset_content in exp_content.items():\n        print(f\"Dataset: {dataset_name}\")\n        # For each projection-head ablation setting\n        for head_config in sorted(dataset_content.keys()):\n            # For each number of training epochs\n            epochs_dict = dataset_content[head_config]\n            for epoch_count in sorted(epochs_dict.keys()):\n                data = epochs_dict[epoch_count]\n                train_acc = data[\"metrics\"][\"train\"][-1]\n                val_acc = data[\"metrics\"][\"val\"][-1]\n                train_loss = data[\"losses\"][\"train\"][-1]\n                val_loss = data[\"losses\"][\"val\"][-1]\n\n                print(f\"Projection head setting: {head_config}, Epochs: {epoch_count}\")\n                print(f\"  Train accuracy: {train_acc:.4f}\")\n                print(f\"  Validation accuracy: {val_acc:.4f}\")\n                print(f\"  Train loss: {train_loss:.4f}\")\n                print(f\"  Validation loss: {val_loss:.4f}\")\n                print()\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through experiments and datasets\nfor exp_name, exp_content in experiment_data.items():\n    for dataset_name, dataset_content in exp_content.items():\n        print(f\"Dataset: {dataset_name}\")\n        # For each projection-head ablation setting\n        for head_config in sorted(dataset_content.keys()):\n            # For each number of training epochs\n            epochs_dict = dataset_content[head_config]\n            for epoch_count in sorted(epochs_dict.keys()):\n                data = epochs_dict[epoch_count]\n                train_acc = data[\"metrics\"][\"train\"][-1]\n                val_acc = data[\"metrics\"][\"val\"][-1]\n                train_loss = data[\"losses\"][\"train\"][-1]\n                val_loss = data[\"losses\"][\"val\"][-1]\n\n                print(f\"Projection head setting: {head_config}, Epochs: {epoch_count}\")\n                print(f\"  Train accuracy: {train_acc:.4f}\")\n                print(f\"  Validation accuracy: {val_acc:.4f}\")\n                print(f\"  Train loss: {train_loss:.4f}\")\n                print(f\"  Validation loss: {val_loss:.4f}\")\n                print()\n", ""], "parse_term_out": ["['Dataset: synthetic', '\\n', 'EPOCHS = 10', '\\n', 'train accuracy: 1.0000',\n'\\n', 'validation accuracy: 1.0000', '\\n', 'training loss: 0.2750', '\\n',\n'validation loss: 0.4489', '\\n', '\\n', 'EPOCHS = 30', '\\n', 'train accuracy:\n1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'training loss: 0.0000',\n'\\n', 'validation loss: 0.0000', '\\n', '\\n', 'EPOCHS = 50', '\\n', 'train\naccuracy: 1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'training loss:\n0.0000', '\\n', 'validation loss: 0.0000', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: arith', '\\n', 'Train accuracy: 1.0000', '\\n', 'Validation accuracy:\n1.0000', '\\n', 'Train loss: 0.0000', '\\n', 'Validation loss: 0.1380\\n', '\\n',\n'Dataset: branch', '\\n', 'Train accuracy: 0.0000', '\\n', 'Validation accuracy:\n0.5000', '\\n', 'Train loss: 0.6874', '\\n', 'Validation loss: 0.0000\\n', '\\n',\n'Dataset: loop', '\\n', 'Train accuracy: 0.0000', '\\n', 'Validation accuracy:\n0.0000', '\\n', 'Train loss: 1.0042', '\\n', 'Validation loss: 1.2961\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: random_negative (synthetic)', '\\n', 'EPOCHS = 10', '\\n', 'Train\naccuracy: 1.0000', '\\n', 'Validation accuracy: 0.7500', '\\n', 'Train loss:\n0.2536', '\\n', 'Validation loss: 0.6176', '\\n', 'EPOCHS = 30', '\\n', 'Train\naccuracy: 1.0000', '\\n', 'Validation accuracy: 1.0000', '\\n', 'Train loss:\n0.0000', '\\n', 'Validation loss: 0.0000', '\\n', 'EPOCHS = 50', '\\n', 'Train\naccuracy: 1.0000', '\\n', 'Validation accuracy: 1.0000', '\\n', 'Train loss:\n0.0000', '\\n', 'Validation loss: 0.0000', '\\n', '\\n', 'Dataset: hard_negative\n(synthetic)', '\\n', 'EPOCHS = 10', '\\n', 'Train accuracy: 1.0000', '\\n',\n'Validation accuracy: 0.7500', '\\n', 'Train loss: 0.4149', '\\n', 'Validation\nloss: 0.6794', '\\n', 'EPOCHS = 30', '\\n', 'Train accuracy: 1.0000', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Train loss: 0.0000', '\\n', 'Validation\nloss: 0.2762', '\\n', 'EPOCHS = 50', '\\n', 'Train accuracy: 1.0000', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Train loss: 0.0000', '\\n', 'Validation\nloss: 0.3422', '\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "", "['Dataset: synthetic', '\\n', 'Margin: 0.1', '\\n', 'Train accuracy: 1.0000',\n'\\n', 'Validation accuracy: 1.0000', '\\n', 'Train loss: 0.0000', '\\n',\n'Validation loss: 0.0000', '\\n', 'Margin: 0.5', '\\n', 'Train accuracy: 1.0000',\n'\\n', 'Validation accuracy: 1.0000', '\\n', 'Train loss: 0.0000', '\\n',\n'Validation loss: 0.0000', '\\n', 'Margin: 1.0', '\\n', 'Train accuracy: 1.0000',\n'\\n', 'Validation accuracy: 1.0000', '\\n', 'Train loss: 0.0000', '\\n',\n'Validation loss: 0.0000', '\\n', 'Margin: 2.0', '\\n', 'Train accuracy: 1.0000',\n'\\n', 'Validation accuracy: 1.0000', '\\n', 'Train loss: 0.0000', '\\n',\n'Validation loss: 0.0000', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "", "['Dataset: synthetic', '\\n', 'Model Variant: lstm, Epochs: 10', '\\n', 'train\naccuracy: 1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'training loss:\n0.3032', '\\n', 'validation loss: 0.4970', '\\n', 'training gap: 0.4227', '\\n',\n'validation gap: 0.3250', '\\n', 'Model Variant: lstm, Epochs: 30', '\\n', 'train\naccuracy: 1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'training loss:\n0.0000', '\\n', 'validation loss: 0.0000', '\\n', 'training gap: 0.4282', '\\n',\n'validation gap: 0.4213', '\\n', 'Model Variant: lstm, Epochs: 50', '\\n', 'train\naccuracy: 1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'training loss:\n0.0000', '\\n', 'validation loss: 0.0587', '\\n', 'training gap: 0.5251', '\\n',\n'validation gap: 0.3865', '\\n', '\\n', 'Dataset: synthetic', '\\n', 'Model\nVariant: mean_pool, Epochs: 10', '\\n', 'train accuracy: 1.0000', '\\n',\n'validation accuracy: 1.0000', '\\n', 'training loss: 0.4119', '\\n', 'validation\nloss: 0.4066', '\\n', 'training gap: 0.0247', '\\n', 'validation gap: 0.0253',\n'\\n', 'Model Variant: mean_pool, Epochs: 30', '\\n', 'train accuracy: 1.0000',\n'\\n', 'validation accuracy: 1.0000', '\\n', 'training loss: 0.3877', '\\n',\n'validation loss: 0.4406', '\\n', 'training gap: 0.0387', '\\n', 'validation gap:\n0.0318', '\\n', 'Model Variant: mean_pool, Epochs: 50', '\\n', 'train accuracy:\n1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'training loss: 0.3854',\n'\\n', 'validation loss: 0.4192', '\\n', 'training gap: 0.0283', '\\n', 'validation\ngap: 0.0240', '\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Dataset: synthetic', '\\n', 'Model: lstm, Epochs: 10', '\\n', '  train accuracy:\n1.0000', '\\n', '  validation accuracy: 1.0000', '\\n', '  train loss: 0.2733',\n'\\n', '  validation loss: 0.4235', '\\n', '  training alignment gap: 0.4130',\n'\\n', '  validation alignment gap: 0.3914', '\\n', 'Model: lstm, Epochs: 30',\n'\\n', '  train accuracy: 1.0000', '\\n', '  validation accuracy: 1.0000', '\\n', '\ntrain loss: 0.0000', '\\n', '  validation loss: 0.0000', '\\n', '  training\nalignment gap: 0.6701', '\\n', '  validation alignment gap: 0.6533', '\\n',\n'Model: lstm, Epochs: 50', '\\n', '  train accuracy: 1.0000', '\\n', '  validation\naccuracy: 1.0000', '\\n', '  train loss: 0.0000', '\\n', '  validation loss:\n0.0000', '\\n', '  training alignment gap: 0.5166', '\\n', '  validation alignment\ngap: 0.4839', '\\n', 'Dataset: synthetic', '\\n', 'Model: mean_pool, Epochs: 10',\n'\\n', '  train accuracy: 1.0000', '\\n', '  validation accuracy: 1.0000', '\\n', '\ntrain loss: 0.3902', '\\n', '  validation loss: 0.4398', '\\n', '  training\nalignment gap: 0.0302', '\\n', '  validation alignment gap: 0.0252', '\\n',\n'Model: mean_pool, Epochs: 30', '\\n', '  train accuracy: 1.0000', '\\n', '\nvalidation accuracy: 1.0000', '\\n', '  train loss: 0.4390', '\\n', '  validation\nloss: 0.4747', '\\n', '  training alignment gap: 0.0201', '\\n', '  validation\nalignment gap: 0.0160', '\\n', 'Model: mean_pool, Epochs: 50', '\\n', '  train\naccuracy: 1.0000', '\\n', '  validation accuracy: 1.0000', '\\n', '  train loss:\n0.3479', '\\n', '  validation loss: 0.3764', '\\n', '  training alignment gap:\n0.0278', '\\n', '  validation alignment gap: 0.0251', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Model: lstm, Epochs: 10', '\\n', 'train accuracy:\n1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'train loss:\n0.3032', '\\n', 'validation loss:     0.4970\\n', '\\n', 'Model: lstm, Epochs: 30',\n'\\n', 'train accuracy:      1.0000', '\\n', 'validation accuracy: 1.0000', '\\n',\n'train loss:          0.0000', '\\n', 'validation loss:     0.0000\\n', '\\n',\n'Model: lstm, Epochs: 50', '\\n', 'train accuracy:      1.0000', '\\n',\n'validation accuracy: 1.0000', '\\n', 'train loss:          0.0000', '\\n',\n'validation loss:     0.0587\\n', '\\n', 'Dataset: synthetic', '\\n', 'Model:\nmean_pool, Epochs: 10', '\\n', 'train accuracy:      1.0000', '\\n', 'validation\naccuracy: 1.0000', '\\n', 'train loss:          0.4119', '\\n', 'validation loss:\n0.4066\\n', '\\n', 'Model: mean_pool, Epochs: 30', '\\n', 'train accuracy:\n1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'train loss:\n0.3877', '\\n', 'validation loss:     0.4406\\n', '\\n', 'Model: mean_pool, Epochs:\n50', '\\n', 'train accuracy:      1.0000', '\\n', 'validation accuracy: 1.0000',\n'\\n', 'train loss:          0.3854', '\\n', 'validation loss:     0.4192\\n',\n'\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Tokenization scheme: char, Epochs: 10', '\\n', '\ntrain accuracy: 1.0000', '\\n', '  validation accuracy: 0.7500', '\\n', '  train\nloss: 0.1725', '\\n', '  validation loss: 0.1942', '\\n', 'Tokenization scheme:\nchar, Epochs: 30', '\\n', '  train accuracy: 1.0000', '\\n', '  validation\naccuracy: 1.0000', '\\n', '  train loss: 0.0000', '\\n', '  validation loss:\n0.0000', '\\n', 'Tokenization scheme: char, Epochs: 50', '\\n', '  train accuracy:\n1.0000', '\\n', '  validation accuracy: 1.0000', '\\n', '  train loss: 0.0000',\n'\\n', '  validation loss: 0.0000', '\\n', 'Tokenization scheme: subword, Epochs:\n10', '\\n', '  train accuracy: 1.0000', '\\n', '  validation accuracy: 1.0000',\n'\\n', '  train loss: 0.1035', '\\n', '  validation loss: 0.5102', '\\n',\n'Tokenization scheme: subword, Epochs: 30', '\\n', '  train accuracy: 1.0000',\n'\\n', '  validation accuracy: 1.0000', '\\n', '  train loss: 0.0000', '\\n', '\nvalidation loss: 0.0192', '\\n', 'Tokenization scheme: subword, Epochs: 50',\n'\\n', '  train accuracy: 1.0000', '\\n', '  validation accuracy: 1.0000', '\\n', '\ntrain loss: 0.0000', '\\n', '  validation loss: 0.0700', '\\n', 'Tokenization\nscheme: ast, Epochs: 10', '\\n', '  train accuracy: 0.0000', '\\n', '  validation\naccuracy: 0.0000', '\\n', '  train loss: 1.0502', '\\n', '  validation loss:\n1.0488', '\\n', 'Tokenization scheme: ast, Epochs: 30', '\\n', '  train accuracy:\n0.0000', '\\n', '  validation accuracy: 0.0000', '\\n', '  train loss: 1.0028',\n'\\n', '  validation loss: 1.0000', '\\n', 'Tokenization scheme: ast, Epochs: 50',\n'\\n', '  train accuracy: 0.0000', '\\n', '  validation accuracy: 0.0000', '\\n', '\ntrain loss: 1.0105', '\\n', '  validation loss: 1.0074', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['Dataset: synthetic, Ablation: euclidean, Epochs: 10', '\\n', 'Final train\naccuracy: 1.0000', '\\n', 'Final validation accuracy: 0.7500', '\\n', 'Final train\nloss: 0.1305', '\\n', 'Final validation loss: 0.3857', '\\n', '\\n', 'Dataset:\nsynthetic, Ablation: euclidean, Epochs: 30', '\\n', 'Final train accuracy:\n1.0000', '\\n', 'Final validation accuracy: 1.0000', '\\n', 'Final train loss:\n0.0000', '\\n', 'Final validation loss: 0.2652', '\\n', '\\n', 'Dataset: synthetic,\nAblation: euclidean, Epochs: 50', '\\n', 'Final train accuracy: 1.0000', '\\n',\n'Final validation accuracy: 1.0000', '\\n', 'Final train loss: 0.0000', '\\n',\n'Final validation loss: 0.1913', '\\n', '\\n', 'Dataset: synthetic, Ablation:\ncosine, Epochs: 10', '\\n', 'Final train accuracy: 1.0000', '\\n', 'Final\nvalidation accuracy: 0.7500', '\\n', 'Final train loss: 0.1678', '\\n', 'Final\nvalidation loss: 0.2616', '\\n', '\\n', 'Dataset: synthetic, Ablation: cosine,\nEpochs: 30', '\\n', 'Final train accuracy: 1.0000', '\\n', 'Final validation\naccuracy: 1.0000', '\\n', 'Final train loss: 0.0592', '\\n', 'Final validation\nloss: 0.1290', '\\n', '\\n', 'Dataset: synthetic, Ablation: cosine, Epochs: 50',\n'\\n', 'Final train accuracy: 1.0000', '\\n', 'Final validation accuracy: 1.0000',\n'\\n', 'Final train loss: 0.0004', '\\n', 'Final validation loss: 0.1711', '\\n',\n'\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: synthetic (Dimension: 16)', '\\n', 'train accuracy: 0.8750', '\\n',\n'validation accuracy: 0.7500', '\\n', 'training loss: 0.5840', '\\n', 'validation\nloss: 0.7061', '\\n', '\\n', 'Dataset: synthetic (Dimension: 32)', '\\n', 'train\naccuracy: 1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'training loss:\n0.0683', '\\n', 'validation loss: 0.2551', '\\n', '\\n', 'Dataset: synthetic\n(Dimension: 64)', '\\n', 'train accuracy: 1.0000', '\\n', 'validation accuracy:\n1.0000', '\\n', 'training loss: 0.0000', '\\n', 'validation loss: 0.0000', '\\n',\n'\\n', 'Dataset: synthetic (Dimension: 128)', '\\n', 'train accuracy: 1.0000',\n'\\n', 'validation accuracy: 1.0000', '\\n', 'training loss: 0.0000', '\\n',\n'validation loss: 0.0000', '\\n', '\\n', 'Dataset: synthetic (Dimension: 256)',\n'\\n', 'train accuracy: 1.0000', '\\n', 'validation accuracy: 1.0000', '\\n',\n'training loss: 0.0000', '\\n', 'validation loss: 0.0000', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Epochs: 10', '\\n', 'Train accuracy: 1.0000', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Renaming accuracy: 0.0000', '\\n', 'Train\nloss: 0.8100', '\\n', 'Validation loss: 0.7737', '\\n', '\\n', 'Epochs: 30', '\\n',\n'Train accuracy: 0.9375', '\\n', 'Validation accuracy: 1.0000', '\\n', 'Renaming\naccuracy: 0.2500', '\\n', 'Train loss: 0.0947', '\\n', 'Validation loss: 0.0305',\n'\\n', '\\n', 'Epochs: 50', '\\n', 'Train accuracy: 1.0000', '\\n', 'Validation\naccuracy: 1.0000', '\\n', 'Renaming accuracy: 0.0000', '\\n', 'Train loss:\n0.0290', '\\n', 'Validation loss: 0.0000', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Variant: unidirectional, Epochs: 10', '\\n', '\ntrain accuracy: 1.0000', '\\n', '  validation accuracy: 1.0000', '\\n', '  train\nloss: 0.2233', '\\n', '  validation loss: 0.2086', '\\n', 'Variant:\nunidirectional, Epochs: 30', '\\n', '  train accuracy: 1.0000', '\\n', '\nvalidation accuracy: 1.0000', '\\n', '  train loss: 0.0000', '\\n', '  validation\nloss: 0.0000', '\\n', 'Variant: unidirectional, Epochs: 50', '\\n', '  train\naccuracy: 1.0000', '\\n', '  validation accuracy: 1.0000', '\\n', '  train loss:\n0.0000', '\\n', '  validation loss: 0.1431', '\\n', 'Variant: bidirectional,\nEpochs: 10', '\\n', '  train accuracy: 1.0000', '\\n', '  validation accuracy:\n0.7500', '\\n', '  train loss: 0.4307', '\\n', '  validation loss: 0.5075', '\\n',\n'Variant: bidirectional, Epochs: 30', '\\n', '  train accuracy: 1.0000', '\\n', '\nvalidation accuracy: 1.0000', '\\n', '  train loss: 0.0238', '\\n', '  validation\nloss: 0.0778', '\\n', 'Variant: bidirectional, Epochs: 50', '\\n', '  train\naccuracy: 1.0000', '\\n', '  validation accuracy: 1.0000', '\\n', '  train loss:\n0.0009', '\\n', '  validation loss: 0.0454', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['Dataset: synthetic_clean', '\\n', 'Training Accuracy: 1.0000', '\\n',\n'Validation Accuracy: 1.0000', '\\n', 'Training Loss: 0.0000', '\\n', 'Validation\nLoss: 0.0000', '\\n', 'Dataset: synthetic_injected', '\\n', 'Training Accuracy:\n1.0000', '\\n', 'Validation Accuracy: 0.5000', '\\n', 'Training Loss: 0.0000',\n'\\n', 'Validation Loss: 0.3423', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "['Dataset: synthetic', '\\n', '\\nTokenization scheme: char, Epochs: 10', '\\n',\n'train accuracy: 1.0000', '\\n', 'validation accuracy: 0.7500', '\\n', 'train\nalignment gap: 0.5769', '\\n', 'validation alignment gap: 0.5831', '\\n', 'final\ntraining loss: 0.1007', '\\n', 'final validation loss: 0.1815', '\\n',\n'\\nTokenization scheme: char, Epochs: 30', '\\n', 'train accuracy: 1.0000', '\\n',\n'validation accuracy: 1.0000', '\\n', 'train alignment gap: 0.4348', '\\n',\n'validation alignment gap: 0.3450', '\\n', 'final training loss: 0.0000', '\\n',\n'final validation loss: 0.0000', '\\n', '\\nTokenization scheme: char, Epochs:\n50', '\\n', 'train accuracy: 1.0000', '\\n', 'validation accuracy: 1.0000', '\\n',\n'train alignment gap: 0.2954', '\\n', 'validation alignment gap: 0.2313', '\\n',\n'final training loss: 0.0000', '\\n', 'final validation loss: 0.0000', '\\n',\n'\\nTokenization scheme: subword, Epochs: 10', '\\n', 'train accuracy: 1.0000',\n'\\n', 'validation accuracy: 0.7500', '\\n', 'train alignment gap: 0.6198', '\\n',\n'validation alignment gap: 0.4914', '\\n', 'final training loss: 0.1536', '\\n',\n'final validation loss: 0.2808', '\\n', '\\nTokenization scheme: subword, Epochs:\n30', '\\n', 'train accuracy: 1.0000', '\\n', 'validation accuracy: 1.0000', '\\n',\n'train alignment gap: 0.8391', '\\n', 'validation alignment gap: 0.5932', '\\n',\n'final training loss: 0.0000', '\\n', 'final validation loss: 0.0168', '\\n',\n'\\nTokenization scheme: subword, Epochs: 50', '\\n', 'train accuracy: 1.0000',\n'\\n', 'validation accuracy: 1.0000', '\\n', 'train alignment gap: 0.7534', '\\n',\n'validation alignment gap: 0.5321', '\\n', 'final training loss: 0.0000', '\\n',\n'final validation loss: 0.0342', '\\n', '\\nTokenization scheme: ast, Epochs: 10',\n'\\n', 'train accuracy: 1.0000', '\\n', 'validation accuracy: 1.0000', '\\n',\n'train alignment gap: 0.7167', '\\n', 'validation alignment gap: 0.4090', '\\n',\n'final training loss: 0.0881', '\\n', 'final validation loss: 0.5382', '\\n',\n'\\nTokenization scheme: ast, Epochs: 30', '\\n', 'train accuracy: 1.0000', '\\n',\n'validation accuracy: 1.0000', '\\n', 'train alignment gap: 0.7775', '\\n',\n'validation alignment gap: 0.6876', '\\n', 'final training loss: 0.0000', '\\n',\n'final validation loss: 0.0751', '\\n', '\\nTokenization scheme: ast, Epochs: 50',\n'\\n', 'train accuracy: 1.0000', '\\n', 'validation accuracy: 1.0000', '\\n',\n'train alignment gap: 0.6334', '\\n', 'validation alignment gap: 0.5980', '\\n',\n'final training loss: 0.0000', '\\n', 'final validation loss: 0.0000', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Projection head setting: head_0, Epochs: 10',\n'\\n', '  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.1722', '\\n', '  Validation loss: 0.4165', '\\n', '\\n', 'Projection\nhead setting: head_0, Epochs: 30', '\\n', '  Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 1.0000', '\\n', '  Train loss: 0.0000', '\\n', '  Validation\nloss: 0.0000', '\\n', '\\n', 'Projection head setting: head_0, Epochs: 50', '\\n',\n'  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0000', '\\n', '  Validation loss: 0.0000', '\\n', '\\n', 'Projection\nhead setting: head_1, Epochs: 10', '\\n', '  Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 1.0000', '\\n', '  Train loss: 0.3379', '\\n', '  Validation\nloss: 0.4535', '\\n', '\\n', 'Projection head setting: head_1, Epochs: 30', '\\n',\n'  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0000', '\\n', '  Validation loss: 0.0109', '\\n', '\\n', 'Projection\nhead setting: head_1, Epochs: 50', '\\n', '  Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 1.0000', '\\n', '  Train loss: 0.0000', '\\n', '  Validation\nloss: 0.0000', '\\n', '\\n', 'Projection head setting: head_2, Epochs: 10', '\\n',\n'  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.7093', '\\n', '  Validation loss: 0.8796', '\\n', '\\n', 'Projection\nhead setting: head_2, Epochs: 30', '\\n', '  Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 1.0000', '\\n', '  Train loss: 0.0239', '\\n', '  Validation\nloss: 0.0310', '\\n', '\\n', 'Projection head setting: head_2, Epochs: 50', '\\n',\n'  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0000', '\\n', '  Validation loss: 0.0000', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', '  Epochs: 10', '\\n', '    train accuracy: 1.0000',\n'\\n', '    validation accuracy: 0.7500', '\\n', '    train loss: 0.0187', '\\n', '\nvalidation loss: 0.4631', '\\n', '  Epochs: 30', '\\n', '    train accuracy:\n1.0000', '\\n', '    validation accuracy: 1.0000', '\\n', '    train loss:\n0.0000', '\\n', '    validation loss: 0.3217', '\\n', '  Epochs: 50', '\\n', '\ntrain accuracy: 1.0000', '\\n', '    validation accuracy: 1.0000', '\\n', '\ntrain loss: 0.0000', '\\n', '    validation loss: 0.1519', '\\n', 'Execution time:\na moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Projection head setting: head_0, Epochs: 10',\n'\\n', '  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 0.7500', '\\n', '\nTrain loss: 0.2674', '\\n', '  Validation loss: 0.5151', '\\n', '\\n', 'Projection\nhead setting: head_0, Epochs: 30', '\\n', '  Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 1.0000', '\\n', '  Train loss: 0.0039', '\\n', '  Validation\nloss: 0.1083', '\\n', '\\n', 'Projection head setting: head_0, Epochs: 50', '\\n',\n'  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0000', '\\n', '  Validation loss: 0.1380', '\\n', '\\n', 'Projection\nhead setting: head_1, Epochs: 10', '\\n', '  Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 0.7500', '\\n', '  Train loss: 0.5004', '\\n', '  Validation\nloss: 0.4879', '\\n', '\\n', 'Projection head setting: head_1, Epochs: 30', '\\n',\n'  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0000', '\\n', '  Validation loss: 0.0000', '\\n', '\\n', 'Projection\nhead setting: head_1, Epochs: 50', '\\n', '  Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 0.7500', '\\n', '  Train loss: 0.0006', '\\n', '  Validation\nloss: 0.1814', '\\n', '\\n', 'Projection head setting: head_2, Epochs: 10', '\\n',\n'  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 0.5000', '\\n', '\nTrain loss: 0.6689', '\\n', '  Validation loss: 0.8643', '\\n', '\\n', 'Projection\nhead setting: head_2, Epochs: 30', '\\n', '  Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 1.0000', '\\n', '  Train loss: 0.0000', '\\n', '  Validation\nloss: 0.0672', '\\n', '\\n', 'Projection head setting: head_2, Epochs: 50', '\\n',\n'  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0000', '\\n', '  Validation loss: 0.0000', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Projection head setting: head_0, Epochs: 10',\n'\\n', '  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.1324', '\\n', '  Validation loss: 0.2506', '\\n', '\\n', 'Projection\nhead setting: head_0, Epochs: 30', '\\n', '  Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 1.0000', '\\n', '  Train loss: 0.0000', '\\n', '  Validation\nloss: 0.0000', '\\n', '\\n', 'Projection head setting: head_0, Epochs: 50', '\\n',\n'  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0000', '\\n', '  Validation loss: 0.0208', '\\n', '\\n', 'Projection\nhead setting: head_1, Epochs: 10', '\\n', '  Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 0.7500', '\\n', '  Train loss: 0.3989', '\\n', '  Validation\nloss: 0.6261', '\\n', '\\n', 'Projection head setting: head_1, Epochs: 30', '\\n',\n'  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0000', '\\n', '  Validation loss: 0.0000', '\\n', '\\n', 'Projection\nhead setting: head_1, Epochs: 50', '\\n', '  Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 1.0000', '\\n', '  Train loss: 0.0000', '\\n', '  Validation\nloss: 0.0000', '\\n', '\\n', 'Projection head setting: head_2, Epochs: 10', '\\n',\n'  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.6400', '\\n', '  Validation loss: 0.4752', '\\n', '\\n', 'Projection\nhead setting: head_2, Epochs: 30', '\\n', '  Train accuracy: 0.8750', '\\n', '\nValidation accuracy: 0.5000', '\\n', '  Train loss: 0.0000', '\\n', '  Validation\nloss: 0.0591', '\\n', '\\n', 'Projection head setting: head_2, Epochs: 50', '\\n',\n'  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0000', '\\n', '  Validation loss: 0.0000', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Projection head setting: head_0, Epochs: 10',\n'\\n', '  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.1324', '\\n', '  Validation loss: 0.2506', '\\n', '\\n', 'Projection\nhead setting: head_0, Epochs: 30', '\\n', '  Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 1.0000', '\\n', '  Train loss: 0.0000', '\\n', '  Validation\nloss: 0.0000', '\\n', '\\n', 'Projection head setting: head_0, Epochs: 50', '\\n',\n'  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0000', '\\n', '  Validation loss: 0.0208', '\\n', '\\n', 'Projection\nhead setting: head_1, Epochs: 10', '\\n', '  Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 0.7500', '\\n', '  Train loss: 0.3989', '\\n', '  Validation\nloss: 0.6261', '\\n', '\\n', 'Projection head setting: head_1, Epochs: 30', '\\n',\n'  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0000', '\\n', '  Validation loss: 0.0000', '\\n', '\\n', 'Projection\nhead setting: head_1, Epochs: 50', '\\n', '  Train accuracy: 1.0000', '\\n', '\nValidation accuracy: 1.0000', '\\n', '  Train loss: 0.0000', '\\n', '  Validation\nloss: 0.0000', '\\n', '\\n', 'Projection head setting: head_2, Epochs: 10', '\\n',\n'  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.6400', '\\n', '  Validation loss: 0.4752', '\\n', '\\n', 'Projection\nhead setting: head_2, Epochs: 30', '\\n', '  Train accuracy: 0.8750', '\\n', '\nValidation accuracy: 0.5000', '\\n', '  Train loss: 0.0000', '\\n', '  Validation\nloss: 0.0591', '\\n', '\\n', 'Projection head setting: head_2, Epochs: 50', '\\n',\n'  Train accuracy: 1.0000', '\\n', '  Validation accuracy: 1.0000', '\\n', '\nTrain loss: 0.0000', '\\n', '  Validation loss: 0.0000', '\\n', '\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3", "Stage_4"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>

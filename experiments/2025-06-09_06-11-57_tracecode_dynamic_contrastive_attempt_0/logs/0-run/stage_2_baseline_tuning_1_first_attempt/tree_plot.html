<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script>
      // Check if we're running under Live Server
      if (window.location.hostname === '127.0.0.1' || window.location.hostname === 'localhost') {
          let lastModified = '';

          // Check for file changes every second
          setInterval(async () => {
              try {
                  const response = await fetch(window.location.href, { method: 'HEAD' });
                  // get a timestamp that shows when the file was last changed
                  const currentModified = response.headers.get('last-modified');

                  if (lastModified && lastModified !== currentModified) {
                      window.location.reload();
                  }

                  lastModified = currentModified;
              } catch (e) {
                  console.error('Error checking for updates:', e);
              }
          }, 1000);
      }
  </script>
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#FFFFFF";
const accentCol = "#1a439e";

hljs.initHighlightingOnLoad();

// Function to update background color globally
function updateBackgroundColor(color) {
  // Update the JS variable
  window.bgColCurrent = color;

  // Update body background
  document.body.style.backgroundColor = color;

  // Update canvas container background
  const canvasContainer = document.getElementById('canvas-container');
  if (canvasContainer) {
    canvasContainer.style.backgroundColor = color;
  }
}

// Store tree data for each stage
const stageData = {
  Stage_1: null,
  Stage_2: null,
  Stage_3: null,
  Stage_4: null
};

// Keep track of current selected stage
let currentStage = null;
let currentSketch = null;
let availableStages = [];

// Class definitions for nodes and edges
class Node {
  constructor(x, y, id, isRoot = false) {
    this.x = x;
    this.y = y;
    this.id = id;
    this.visible = isRoot; // Only root nodes are visible initially
    this.appearProgress = 0;
    this.popEffect = 0;
    this.selected = false;
    this.isRootNode = isRoot;
  }

  update() {
    if (this.visible) {
      // Handle the main appearance animation
      if (this.appearProgress < 1) {
        this.appearProgress += 0.06;

        // When we reach full size, trigger the pop effect
        if (this.appearProgress >= 1) {
          this.appearProgress = 1; // Cap at 1
          this.popEffect = 1; // Start the pop effect
        }
      }

      // Handle the pop effect animation
      if (this.popEffect > 0) {
        this.popEffect -= 0.15; // Control how quickly it shrinks back
        if (this.popEffect < 0) this.popEffect = 0; // Don't go negative
      }
    }
  }

  startAnimation() {
    this.visible = true;
  }

  color() {
    if (this.selected) {
      return accentCol; // Use the global accent color variable for selected node
    }
    return '#4263eb'; // Default blue color
  }

  render(p5) {
    if (this.visible) {
      const popBonus = this.popEffect * 0.1;
      const nodeScale = p5.map(this.appearProgress, 0, 1, 0, 1) + popBonus;
      const alpha = p5.map(this.appearProgress, 0, 1, 0, 255);

      p5.push();
      p5.translate(this.x, this.y);

      // Shadow effect
      p5.noStroke();
      p5.rectMode(p5.CENTER);

      for (let i = 1; i <= 4; i++) {
        p5.fill(0, 0, 0, alpha * 0.06);
        p5.rect(i, i, 30 * nodeScale, 30 * nodeScale, 10);
      }

      // Main square - use node's color with alpha
      let nodeColor = p5.color(this.color());
      nodeColor.setAlpha(alpha);
      p5.fill(nodeColor);
      p5.rect(0, 0, 30 * nodeScale, 30 * nodeScale, 10);

      // Draw checkmark icon if the node is selected
      if (this.selected && this.appearProgress >= 1) {
        p5.stroke(255);
        p5.strokeWeight(2 * nodeScale);
        p5.noFill();
        // Draw checkmark
        p5.beginShape();
        p5.vertex(-8, 0);
        p5.vertex(-3, 5);
        p5.vertex(8, -6);
        p5.endShape();
      }

      p5.pop();
    }
  }

  isMouseOver(p5) {
    return this.visible &&
           p5.mouseX > this.x - 15 &&
           p5.mouseX < this.x + 15 &&
           p5.mouseY > this.y - 15 &&
           p5.mouseY < this.y + 15;
  }

  // Connect this node to a child node
  child(childNode) {
    // Create an edge from this node to the child
    let isLeft = childNode.x < this.x;
    let isRight = childNode.x > this.x;
    let edge = new Edge(this, childNode, isLeft, isRight);
    return edge;
  }
}

class Edge {
  constructor(parent, child, isLeft, isRight) {
    this.parent = parent;
    this.child = child;
    this.isLeft = isLeft;
    this.isRight = isRight;
    this.progress = 0;

    // Calculate the midpoint where branching occurs
    this.midY = parent.y + (child.y - parent.y) * 0.6;

    // Use the actual child x-coordinate
    // This ensures the edge will connect directly to the child node
    this.branchX = child.x;
  }

  update() {
    if (this.parent.visible && this.progress < 1) {
      this.progress += 0.01; // Adjust animation speed
    }
    if (this.progress >= 1) {
      this.child.visible = true;
    }
  }

  color() {
    return this.child.color();
  }

  render(p5) {
    if (!this.parent.visible) return;

    // Calculate path lengths
    const verticalDist1 = this.midY - this.parent.y;
    const horizontalDist = Math.abs(this.branchX - this.parent.x);
    const verticalDist2 = this.child.y - this.midY;
    const totalLength = verticalDist1 + horizontalDist + verticalDist2;

    // Calculate how much of each segment to draw
    const currentLength = totalLength * this.progress;

    p5.stroke(180, 190, 205);
    p5.strokeWeight(1.5);
    p5.noFill();

    // Always draw the first vertical segment from parent
    if (currentLength > 0) {
      const firstSegmentLength = Math.min(currentLength, verticalDist1);
      const currentMidY = p5.lerp(this.parent.y, this.midY, firstSegmentLength / verticalDist1);
      p5.line(this.parent.x, this.parent.y, this.parent.x, currentMidY);
    }

    if (currentLength > verticalDist1) {
      // Draw second segment (horizontal)
      const secondSegmentLength = Math.min(currentLength - verticalDist1, horizontalDist);
      const currentBranchX = p5.lerp(this.parent.x, this.branchX, secondSegmentLength / horizontalDist);
      p5.line(this.parent.x, this.midY, currentBranchX, this.midY);

      if (currentLength > verticalDist1 + horizontalDist) {
        // Draw third segment (vertical to child)
        const thirdSegmentLength = currentLength - verticalDist1 - horizontalDist;
        const currentChildY = p5.lerp(this.midY, this.child.y, thirdSegmentLength / verticalDist2);
        p5.line(this.branchX, this.midY, this.branchX, currentChildY);
      }
    }
  }
}

// Create a modified sketch for each stage
function createTreeSketch(stageId) {
  return function(p5) {
    let nodes = [];
    let edges = [];
    let treeData = stageData[stageId];

    p5.setup = function() {
      const canvas = p5.createCanvas(p5.windowWidth * 0.4, p5.windowHeight);
      canvas.parent('canvas-container');
      p5.smooth();
      p5.frameRate(60);

      if (treeData) {
        createTreeFromData(treeData);
      }
    };

    p5.windowResized = function() {
      p5.resizeCanvas(p5.windowWidth * 0.4, p5.windowHeight);
    };

    function createTreeFromData(data) {
      // Clear existing nodes and edges
      nodes = [];
      edges = [];

      // Add defensive checks to prevent errors
      if (!data || !data.layout || !Array.isArray(data.layout) || !data.edges || !Array.isArray(data.edges)) {
        console.error("Invalid tree data format:", data);
        return; // Exit if data structure is invalid
      }

      // Find all parent nodes in edges
      const parentNodes = new Set();
      for (const [parentId, childId] of data.edges) {
        parentNodes.add(parentId);
      }

      // Create nodes
      for (let i = 0; i < data.layout.length; i++) {
        const [nx, ny] = data.layout[i];
        // A node is a root if it's a parent and not a child in any edge
        const isRoot = parentNodes.has(i) && data.edges.every(edge => edge[1] !== i);

        const node = new Node(
          nx * p5.width * 0.8 + p5.width * 0.1,
          ny * p5.height * 0.8 + p5.height * 0.1,
          i,
          isRoot
        );
        nodes.push(node);
      }

      // If no root was found, make the first parent node visible
      if (!nodes.some(node => node.visible) && parentNodes.size > 0) {
        // Get the first parent node
        const firstParentId = [...parentNodes][0];
        if (nodes[firstParentId]) {
          nodes[firstParentId].visible = true;
        }
      }

      // Create edges
      for (const [parentId, childId] of data.edges) {
        const parent = nodes[parentId];
        const child = nodes[childId];
        if (parent && child) { // Verify both nodes exist
          const isLeft = child.x < parent.x;
          const isRight = child.x > parent.x;
          edges.push(new Edge(parent, child, isLeft, isRight));
        }
      }

      // Select the first node by default
      if (nodes.length > 0) {
        nodes[0].selected = true;
        updateNodeInfo(0);
      }
    }

    p5.draw = function() {
      // Use the global background color if available, otherwise use the default bgCol
      const currentBgColor = window.bgColCurrent || bgCol;
      p5.background(currentBgColor);

      // Update and render edges
      for (const edge of edges) {
        edge.update();
        edge.render(p5);
      }

      // Update and render nodes
      for (const node of nodes) {
        node.update();
        node.render(p5);
      }

      // Handle mouse hover
      p5.cursor(p5.ARROW);
      for (const node of nodes) {
        if (node.isMouseOver(p5)) {
          p5.cursor(p5.HAND);
        }
      }
    };

    p5.mousePressed = function() {
      // Check if any node was clicked
      for (let i = 0; i < nodes.length; i++) {
        if (nodes[i].visible && nodes[i].isMouseOver(p5)) {
          // Deselect all nodes
          nodes.forEach(n => n.selected = false);
          // Select the clicked node
          nodes[i].selected = true;
          // Update the right panel with node info
          updateNodeInfo(i);
          break;
        }
      }
    };

    function updateNodeInfo(nodeIndex) {
      if (treeData) {
        setNodeInfo(
          treeData.code[nodeIndex],
          treeData.plan[nodeIndex],
          treeData.plot_code?.[nodeIndex],
          treeData.plot_plan?.[nodeIndex],
          treeData.metrics?.[nodeIndex],
          treeData.exc_type?.[nodeIndex] || '',
          treeData.exc_info?.[nodeIndex]?.args?.[0] || '',
          treeData.exc_stack?.[nodeIndex] || [],
          treeData.plots?.[nodeIndex] || [],
          treeData.plot_analyses?.[nodeIndex] || [],
          treeData.vlm_feedback_summary?.[nodeIndex] || '',
          treeData.datasets_successfully_tested?.[nodeIndex] || [],
          treeData.exec_time_feedback?.[nodeIndex] || '',
          treeData.exec_time?.[nodeIndex] || ''
        );
      }
    }
  };
}

// Start a new p5 sketch for the given stage
function startSketch(stageId) {
  if (currentSketch) {
    currentSketch.remove();
  }

  if (stageData[stageId]) {
    currentSketch = new p5(createTreeSketch(stageId));

    // Update stage info
    const stageNumber = stageId.split('_')[1];
    let stageDesc = '';
    switch(stageId) {
      case 'Stage_1': stageDesc = 'Preliminary Investigation'; break;
      case 'Stage_2': stageDesc = 'Baseline Tuning'; break;
      case 'Stage_3': stageDesc = 'Research Agenda Execution'; break;
      case 'Stage_4': stageDesc = 'Ablation Studies'; break;
    }

    document.getElementById('stage-info').innerHTML =
      `<strong>Current Stage: ${stageNumber} - ${stageDesc}</strong>`;
  }
}

// Handle tab selection
function selectStage(stageId) {
  if (!stageData[stageId] || !availableStages.includes(stageId)) {
    return; // Don't allow selection of unavailable stages
  }

  // Update active tab styles
  document.querySelectorAll('.tab').forEach(tab => {
    tab.classList.remove('active');
  });
  document.querySelector(`.tab[data-stage="${stageId}"]`).classList.add('active');

  // Start the new sketch
  currentStage = stageId;
  startSketch(stageId);
}

// Function to load the tree data for all stages
async function loadAllStageData(baseTreeData) {
  console.log("Loading stage data with base data:", baseTreeData);

  // The base tree data is for the current stage
  const currentStageId = baseTreeData.current_stage || 'Stage_1';

  // Ensure base tree data is valid and has required properties
  if (baseTreeData && baseTreeData.layout && baseTreeData.edges) {
    stageData[currentStageId] = baseTreeData;
    availableStages.push(currentStageId);
    console.log(`Added current stage ${currentStageId} to available stages`);
  } else {
    console.warn(`Current stage ${currentStageId} data is invalid:`, baseTreeData);
  }

  // Use relative path to load other stage trees
  const logDirPath = baseTreeData.log_dir_path || '.';
  console.log("Log directory path:", logDirPath);

  // Load data for each stage if available
  const stageNames = ['Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'];
  const stageNames2actualNames = {
    'Stage_1': 'stage_1_initial_implementation_1_preliminary',
    'Stage_2': 'stage_2_baseline_tuning_1_first_attempt',
    'Stage_3': 'stage_3_creative_research_1_first_attempt',
    'Stage_4': 'stage_4_ablation_studies_1_first_attempt'
    }

  for (const stage of stageNames) {

    if (baseTreeData.completed_stages && baseTreeData.completed_stages.includes(stage)) {
      try {
        console.log(`Attempting to load data for ${stage} from ${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);
        const response = await fetch(`${logDirPath}/${stageNames2actualNames[stage]}/tree_data.json`);

        if (response.ok) {
          const data = await response.json();

          // Validate the loaded data
          if (data && data.layout && data.edges) {
            stageData[stage] = data;
            availableStages.push(stage);
            console.log(`Successfully loaded and validated data for ${stage}`);
          } else {
            console.warn(`Loaded data for ${stage} is invalid:`, data);
          }
        } else {
          console.warn(`Failed to load data for ${stage} - HTTP status ${response.status}`);
        }
      } catch (error) {
        console.error(`Error loading data for ${stage}:`, error);
      }
    } else {
      console.log(`Skipping stage ${stage} - not in completed stages list:`, baseTreeData.completed_stages);
    }
  }

  // Update tab visibility based on available stages
  updateTabVisibility();

  // Start with the first available stage
  if (availableStages.length > 0) {
    selectStage(availableStages[0]);
  } else {
    console.warn("No stages available to display");
    // Display a message in the canvas area
    document.getElementById('canvas-container').innerHTML =
      '<div style="padding: 20px; color: #333; text-align: center;"><h3>No valid tree data available to display</h3></div>';
  }
}

// Update tab visibility based on available stages
function updateTabVisibility() {
  const tabs = document.querySelectorAll('.tab');
  tabs.forEach(tab => {
    const stageId = tab.getAttribute('data-stage');
    if (availableStages.includes(stageId)) {
      tab.classList.remove('disabled');
    } else {
      tab.classList.add('disabled');
    }
  });
}

// Utility function to set the node info in the right panel
const setNodeInfo = (code, plan, plot_code, plot_plan, metrics = null, exc_type = '', exc_info = '',
    exc_stack = [], plots = [], plot_analyses = [], vlm_feedback_summary = '',
    datasets_successfully_tested = [], exec_time_feedback = '', exec_time = '') => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    if (code) {
      codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
    } else {
      codeElm.innerHTML = '<p>No code available</p>';
    }
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    if (plan) {
      planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
    } else {
      planElm.innerHTML = '<p>No plan available</p>';
    }
  }

  const plot_codeElm = document.getElementById("plot_code");
  if (plot_codeElm) {
    if (plot_code) {
      plot_codeElm.innerHTML = hljs.highlight(plot_code, { language: "python" }).value;
    } else {
      plot_codeElm.innerHTML = '<p>No plot code available</p>';
    }
  }

  const plot_planElm = document.getElementById("plot_plan");
  if (plot_planElm) {
    if (plot_plan) {
      plot_planElm.innerHTML = hljs.highlight(plot_plan, { language: "plaintext" }).value;
    } else {
      plot_planElm.innerHTML = '<p>No plot plan available</p>';
    }
  }

  const metricsElm = document.getElementById("metrics");
  if (metricsElm) {
      let metricsContent = `<h3>Metrics:</h3>`;
      if (metrics && metrics.metric_names) {
          for (const metric of metrics.metric_names) {
              metricsContent += `<div class="metric-group">`;
              metricsContent += `<h4>${metric.metric_name}</h4>`;
              metricsContent += `<p><strong>Description:</strong> ${metric.description || 'N/A'}</p>`;
              metricsContent += `<p><strong>Optimization:</strong> ${metric.lower_is_better ? 'Minimize' : 'Maximize'}</p>`;

              // Create table for dataset values
              metricsContent += `<table class="metric-table">
                  <tr>
                      <th>Dataset</th>
                      <th>Final Value</th>
                      <th>Best Value</th>
                  </tr>`;

              for (const dataPoint of metric.data) {
                  metricsContent += `<tr>
                      <td>${dataPoint.dataset_name}</td>
                      <td>${dataPoint.final_value?.toFixed(4) || 'N/A'}</td>
                      <td>${dataPoint.best_value?.toFixed(4) || 'N/A'}</td>
                  </tr>`;
              }

              metricsContent += `</table></div>`;
          }
      } else if (metrics === null) {
          metricsContent += `<p>No metrics available</p>`;
      }
      metricsElm.innerHTML = metricsContent;
  }

  // Add plots display
  const plotsElm = document.getElementById("plots");
  if (plotsElm) {
      if (plots && plots.length > 0) {
          let plotsContent = '';
          plots.forEach(plotPath => {
              plotsContent += `
                  <div class="plot-item">
                      <img src="${plotPath}" alt="Experiment Plot" onerror="console.error('Failed to load plot:', this.src)"/>
                  </div>`;
          });
          plotsElm.innerHTML = plotsContent;
      } else {
          plotsElm.innerHTML = '';
      }
  }

  // Add error info display
  const errorElm = document.getElementById("exc_info");
  if (errorElm) {
    if (exc_type) {
      let errorContent = `<h3 style="color: #ff5555">Exception Information:</h3>
                          <p><strong>Type:</strong> ${exc_type}</p>`;

      if (exc_info) {
        errorContent += `<p><strong>Details:</strong> <pre>${JSON.stringify(exc_info, null, 2)}</pre></p>`;
      }

      if (exc_stack) {
        errorContent += `<p><strong>Stack Trace:</strong> <pre>${exc_stack.join('\n')}</pre></p>`;
      }

      errorElm.innerHTML = errorContent;
    } else {
      errorElm.innerHTML = "No exception info available";
    }
  }

  const exec_timeElm = document.getElementById("exec_time");
  if (exec_timeElm) {
    let exec_timeContent = '<div id="exec_time"><h3>Execution Time (in seconds):</h3><p>' + exec_time + '</p></div>';
    exec_timeElm.innerHTML = exec_timeContent;
  }

  const exec_time_feedbackElm = document.getElementById("exec_time_feedback");
  if (exec_time_feedbackElm) {
    let exec_time_feedbackContent = '<div id="exec_time_feedback_content">'
    exec_time_feedbackContent += '<h3>Execution Time Feedback:</h3>'
    exec_time_feedbackContent += '<p>' + exec_time_feedback + '</p>'
    exec_time_feedbackContent += '</div>';
    exec_time_feedbackElm.innerHTML = exec_time_feedbackContent;
  }

  const vlm_feedbackElm = document.getElementById("vlm_feedback");
  if (vlm_feedbackElm) {
      let vlm_feedbackContent = '';

      if (plot_analyses && plot_analyses.length > 0) {
          vlm_feedbackContent += `<h3>Plot Analysis:</h3>`;
          plot_analyses.forEach(analysis => {
              if (analysis && analysis.plot_path) {  // Add null check
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <h4>Analysis for ${analysis.plot_path.split('/').pop()}</h4>
                          <p>${analysis.analysis || 'No analysis available'}</p>
                          <ul class="key-findings">
                              ${(analysis.key_findings || []).map(finding => `<li>${finding}</li>`).join('')}
                          </ul>
                      </div>`;
              } else {
                  console.warn('Received invalid plot analysis:', analysis);
                  vlm_feedbackContent += `
                      <div class="plot-analysis">
                          <p>Invalid plot analysis data received</p>
                      </div>`;
              }
          });
      }

      // Add actionable insights if available
      if (vlm_feedback_summary && typeof vlm_feedback_summary === 'string') {
          vlm_feedbackContent += `
              <div class="vlm_feedback">
                  <h3>VLM Feedback Summary:</h3>
                  <p>${vlm_feedback_summary}</p>
              </div>`;
      }

      console.log("Datasets successfully tested:", datasets_successfully_tested);
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          vlm_feedbackContent += `
              <div id="datasets_successfully_tested">
                  <h3>Datasets Successfully Tested:</h3>
                  <p>${datasets_successfully_tested.join(', ')}</p>
              </div>`;
      }

      if (!vlm_feedbackContent) {
          vlm_feedbackContent = '<p>No insights available for this experiment.</p>';
      }

      vlm_feedbackElm.innerHTML = vlm_feedbackContent;
  }

  const datasets_successfully_testedElm = document.getElementById("datasets_successfully_tested");
  if (datasets_successfully_testedElm) {
      let datasets_successfully_testedContent = '';
      if (datasets_successfully_tested && datasets_successfully_tested.length > 0) {
          datasets_successfully_testedContent = `<h3>Datasets Successfully Tested:</h3><ul>`;
          datasets_successfully_tested.forEach(dataset => {
              datasets_successfully_testedContent += `<li>${dataset}</li>`;
          });
          datasets_successfully_testedContent += `</ul>`;
      } else {
          datasets_successfully_testedContent = '<p>No datasets tested yet</p>';
      }
      datasets_successfully_testedElm.innerHTML = datasets_successfully_testedContent;
  }
};

// Initialize with the provided tree data
const treeStructData = {"edges": [[0, 9], [0, 12], [0, 8], [0, 1], [0, 7], [0, 3], [0, 11], [0, 10], [0, 2], [0, 4], [1, 13], [1, 15], [1, 14], [1, 16], [2, 5], [2, 6]], "layout": [[0.6240000000000001, 0.0], [0.12, 0.5], [0.36, 0.5], [0.44, 0.5], [0.52, 0.5], [0.32, 1.0], [0.4, 1.0], [0.6, 0.5], [0.68, 0.5], [0.76, 0.5], [0.84, 0.5], [0.92, 0.5], [1.0, 0.5], [0.0, 1.0], [0.08, 1.0], [0.16, 1.0], [0.24, 1.0]], "plan": ["In this preliminary implementation, we synthetically generate pairs of simple\nPython functions with equivalent dynamic traces (e.g. addition with swapped\noperands) by executing each snippet on random inputs and grouping them by output\ntraces. We then char\u2010tokenize each code snippet, pad to uniform length, and\nbuild a small retrieval dataset of positive and negative pairs for contrastive\ntraining. A lightweight LSTM\u2010based encoder maps padded sequences to fixed\u2010size\nembeddings. We train with a Triplet Margin Loss, sampling, for each anchor, a\npositive variant from the same trace group and a negative from a different\ngroup. During training, we log the average training and validation loss per\nepoch, and compute Top\u20101 Trace\u2010Equivalence retrieval accuracy on held\u2010out groups\nby ranking each snippet\u2019s nearest neighbor in embedding space. All metrics,\nlosses, predictions, and ground truth labels are stored in experiment_data and\nsaved as a numpy file in the working directory. Finally, we print per\u2010epoch\nvalidation losses and the final retrieval accuracy. This simple baseline sets up\nthe pipeline from code preparation to contrastive training and evaluation.", "Hyperparam tuning name: EPOCHS. I will loop over different epoch settings (e.g.,\n10, 30, 50), reinitialize and train the LSTM encoder from scratch for each\nsetting, and record per\u2010epoch losses and retrieval accuracies as well as final\nvalidation predictions. All results are organized under the hyperparameter\ntuning key \u201cEPOCHS\u201d and saved to a single NumPy file for later plotting or\nanalysis.", "Hyperparam tuning name: learning_rate. We perform a grid search over learning\nrates \\[1e-4, 5e-4, 1e-3, 5e-3, 1e-2\\], reinitializing the model and optimizer\nfor each rate and training on the synthetic trace-equivalence task for a fixed\nnumber of epochs. For each rate we record epoch\u2010wise training/validation losses\nand retrieval accuracies, then compute final top-1 predictions and ground truth\non the validation set. All results are stored in a nested `experiment_data` dict\nkeyed by `\"learning_rate\"` and the stringified rate, and finally saved as\n`experiment_data.npy` for downstream analysis.", "Hyperparam tuning name: BATCH_SIZE. Below is an outline followed by the full\nsingle\u2010file implementation that sweeps over batch sizes of 4, 16, and 32. For\neach batch size it re-creates the data loaders, encoder, and optimizer; then\ntrains for a fixed number of epochs while collecting per-epoch train/validation\nlosses and retrieval accuracies. After training each model, it computes the\nfinal top-1 retrieval predictions and ground truth on the validation set. All of\nthese results\u2014along with the corresponding batch size\u2014are stored in a nested\ndictionary under the key \u201cbatch_size\u201d and saved as \u201cexperiment_data.npy\u201d.", "Hyperparam tuning name: triplet_margin. I will loop over a list of margin\nvalues, and for each I reinitialize the encoder, optimizer, and\nTripletMarginLoss with that margin, train for a fixed number of epochs, and\nrecord per\u2010epoch train/val losses and retrieval accuracies. After each margin\nrun, I perform final retrieval on the validation set to collect predictions and\nground truth. All runs\u2019 metrics, losses, predictions, ground_truth, and the list\nof margins are stored in a hierarchical `experiment_data` dictionary under\n`\"triplet_margin_sweep\"`. Finally, the complete `experiment_data` is saved as a\nnumpy file.", "We restrict sampling to within each split by adding an `allowed_idx` list to the\ndataset so that positives and negatives are drawn only from the current\npartition, avoiding any leakage of validation codes into training negatives. We\neliminate the use of `Subset` and instead build two `CodeDataset` instances\u2014one\nfor training and one for validation\u2014each unaware of the other's indices. The\nrest of the training loop and metric computations remain unchanged, ensuring\nclean separation of data.", "We expand hyperparameter tuning to also explore batch sizes and extend the\nnumber of epochs for more robust search. A new Cross\u2010Trace Embedding Consistency\n(CTEC) metric is computed at each epoch by averaging cosine similarities among\nembeddings of semantically equivalent functions. After tuning on the synthetic\ndata, we evaluate the best model on two HuggingFace CodeXGLUE clone\u2010detection\ndatasets (Python and Java), recording accuracy, predictions, and ground truths.\nAll losses, metrics, predictions, and ground truths are aggregated under a\nsingle `experiment_data` dict and saved to `working/experiment_data.npy`.", "Hyperparam tuning name: embed_dim. We will extend the baseline to loop over\nembedding dimensions [32, 64, 128, 256], re\u2010initializing the encoder and\noptimizer for each setting. For each run, we train for a fixed number of epochs,\ncollecting train/val losses and retrieval accuracies per epoch as well as final\ntop\u20101 predictions on the validation split. All hyperparameter values and their\nassociated metrics, losses, predictions, and ground truth are stored in a nested\n`experiment_data` dict under `'embed_dim'->'synthetic'` and saved to\n`experiment_data.npy`.", "Hyperparam tuning name: lstm_dropout. I will extend the `CodeEncoder` to support\na configurable LSTM dropout between two layers, then sweep over several dropout\nrates (0.1, 0.3, 0.5). For each rate, I instantiate a fresh encoder, train it\nfor a fixed number of epochs on the synthetic trace dataset, and record both\ntriplet losses and high-level retrieval accuracy on train/validation splits per\nepoch. After each run I store the final top-1 retrieval predictions alongside\nthe ground truth. All of this data is organized under a single\n`experiment_data['lstm_dropout']` entry and saved to `experiment_data.npy`.", "Hyperparam tuning name: weight_decay. We extend the baseline by introducing L2\nregularization via the `weight_decay` parameter in Adam and sweeping over `[0,\n1e-5, 1e-4, 1e-3, 1e-2]`. For each weight decay, we reinitialize the encoder,\ntrain for a fixed number of epochs, collect train/val losses and retrieval\naccuracies per epoch, and finally compute validation predictions and ground\ntruth. All results are stored in a nested `experiment_data` dict under\n`weight_decay_sweep` and saved to `\"working/experiment_data.npy\"`. The code is a\nsingle-file, self-contained Python script.", "Hyperparam tuning name: HARD_NEGATIVE_POOL_SIZE. Here we extend the dataset to\nsample a pool of negatives per anchor, then during each batch we embed all\nnegatives, compute cosine similarity to the anchor, pick the hardest and feed\nthat into the triplet margin loss. We loop over different\n`hard_negative_pool_size` values, re\u2010initialize model/optimizer per run, and\ncollect per-epoch train/val losses and retrieval metrics, as well as final\npredictions vs ground truth. All results are stored in a nested\n`experiment_data` dict and saved with `np.save(\"experiment_data.npy\")`.", "Hyperparam tuning name: lstm_num_layers. Below is a script that wraps the\nexisting training loop in a simple grid search over 1, 2, and 3 LSTM layers.\nFor each setting it instantiates a fresh encoder, trains for a fixed number of\nepochs, computes triplet\u2010loss and retrieval accuracy on train/val each epoch,\nthen records final per\u2010example val predictions and ground\u2010truth.  All losses,\nmetrics, predictions and labels are stored in a nested dict under\n`experiment_data['lstm_num_layers']['synthetic'][str(num_layers)]` and saved as\n\u201cexperiment_data.npy\u201d at the end.", "Hyperparam tuning name: lstm_bidirectional. We loop over the two boolean\nsettings for `lstm_bidirectional`, augmenting our `CodeEncoder` to accept this\nflag and adjust the LSTM size/directions. For each setting we train for\n`EPOCHS`, record per\u2010epoch train/val losses and retrieval accuracies, and then\ncompute top\u20101 predictions on the validation set. All data (metrics, losses,\npredictions, ground truth) are stored in\n`experiment_data['lstm_bidirectional'][str(flag)]['synthetic']`. Finally, we\nsave the full nested dictionary via `np.save` to `experiment_data.npy`.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic code snippets\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\n\n# generate dynamic traces\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: ids for gid, ids in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# encode as char tokens\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\n# split groups into train/val\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# define encoder\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\nmodel = CodeEncoder(len(stoi), 64, 64).to(device)\noptimizer = Adam(model.parameters(), lr=1e-3)\nloss_fn = nn.TripletMarginLoss(margin=1.0)\n\nexperiment_data = {\n    \"synthetic\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n    # training\n    model.train()\n    train_loss = 0.0\n    for a, p, n in train_loader:\n        a, p, n = a.to(device), p.to(device), n.to(device)\n        emb_a = model(a)\n        emb_p = model(p)\n        emb_n = model(n)\n        loss = loss_fn(emb_a, emb_p, emb_n)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    train_loss /= len(train_loader)\n    experiment_data[\"synthetic\"][\"losses\"][\"train\"].append(train_loss)\n    # validation loss\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for a, p, n in val_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a = model(a)\n            emb_p = model(p)\n            emb_n = model(n)\n            val_loss += loss_fn(emb_a, emb_p, emb_n).item()\n    val_loss /= len(val_loader)\n    experiment_data[\"synthetic\"][\"losses\"][\"val\"].append(val_loss)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n    # retrieval accuracy on train and val\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n\n        def compute_acc(indices):\n            correct = 0\n            for i in indices:\n                sim = sims[i].clone()\n                sim[i] = -float(\"inf\")\n                top1 = torch.argmax(sim).item()\n                if index_to_gid[top1] == index_to_gid[i]:\n                    correct += 1\n            return correct / len(indices) if indices else 0\n\n        train_acc = compute_acc(train_indices)\n        val_acc = compute_acc(val_indices)\n    experiment_data[\"synthetic\"][\"metrics\"][\"train\"].append(train_acc)\n    experiment_data[\"synthetic\"][\"metrics\"][\"val\"].append(val_acc)\n\n# store final predictions and ground truth for val\nwith torch.no_grad():\n    emb_all = model(encoded.to(device))\n    emb_norm = F.normalize(emb_all, dim=1)\n    sims = emb_norm @ emb_norm.T\n    for i in val_indices:\n        sim = sims[i].clone()\n        sim[i] = -float(\"inf\")\n        top1 = torch.argmax(sim).item()\n        experiment_data[\"synthetic\"][\"predictions\"].append(index_to_gid[top1])\n        experiment_data[\"synthetic\"][\"ground_truth\"].append(index_to_gid[i])\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(f\"Top-1 Trace-Equivalence Retrieval Accuracy: {val_acc:.4f}\")\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# generate synthetic code snippets and traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: ids for gid, ids in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# encode snippets as character sequences\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\n# dataset & loaders\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# model definition\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# hyperparameter tuning over different epoch counts\nEPOCH_LIST = [10, 30, 50]\nexperiment_data = {\"EPOCHS\": {\"synthetic\": {}}}\n\nfor E in EPOCH_LIST:\n    # reinitialize model & optimizer\n    model = CodeEncoder(len(stoi), 64, 64).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.TripletMarginLoss(margin=1.0)\n\n    # containers for this setting\n    data = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    # training + validation\n    for epoch in range(E):\n        model.train()\n        total_train_loss = 0.0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a, emb_p, emb_n = model(a), model(p), model(n)\n            loss = loss_fn(emb_a, emb_p, emb_n)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_train_loss += loss.item()\n        avg_train_loss = total_train_loss / len(train_loader)\n        data[\"losses\"][\"train\"].append(avg_train_loss)\n\n        model.eval()\n        total_val_loss = 0.0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                total_val_loss += loss_fn(model(a), model(p), model(n)).item()\n        avg_val_loss = total_val_loss / len(val_loader)\n        data[\"losses\"][\"val\"].append(avg_val_loss)\n\n        # compute retrieval accuracies\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n\n            def compute_acc(indices):\n                correct = 0\n                for i in indices:\n                    sim = sims[i].clone()\n                    sim[i] = -float(\"inf\")\n                    top1 = torch.argmax(sim).item()\n                    if index_to_gid[top1] == index_to_gid[i]:\n                        correct += 1\n                return correct / len(indices)\n\n            train_acc = compute_acc(train_indices)\n            val_acc = compute_acc(val_indices)\n        data[\"metrics\"][\"train\"].append(train_acc)\n        data[\"metrics\"][\"val\"].append(val_acc)\n\n    # final predictions on validation\n    model.eval()\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n        for i in val_indices:\n            sim = sims[i].clone()\n            sim[i] = -float(\"inf\")\n            top1 = torch.argmax(sim).item()\n            data[\"predictions\"].append(index_to_gid[top1])\n            data[\"ground_truth\"].append(index_to_gid[i])\n\n    experiment_data[\"EPOCHS\"][\"synthetic\"][E] = data\n    print(f\"Finished EPOCHS={E}: final val_acc={val_acc:.4f}\")\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os, random, numpy as np, torch\nimport torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# synthetic code and traces\ncodes = []\nfor c in range(1, 11):\n    codes += [f\"def f(x): return x+{c}\", f\"def f(x): return {c}+x\"]\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\ntrace_to_indices = {}\nfor i, t in enumerate(traces):\n    trace_to_indices.setdefault(t, []).append(i)\ngroup_to_indices = {g: idxs for g, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor g, idxs in group_to_indices.items():\n    for i in idxs:\n        index_to_gid[i] = g\n\n# encoding\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = torch.LongTensor(\n    [[stoi[c] for c in s] + [0] * (max_len - len(s)) for s in codes]\n)\n\n\n# dataset and loaders\nclass CodeDataset(Dataset):\n    def __init__(self, enc, grp2idx, idx2gid):\n        self.enc, self.grp2idx, self.idx2gid = enc, grp2idx, idx2gid\n\n    def __len__(self):\n        return len(self.idx2gid)\n\n    def __getitem__(self, i):\n        anchor = self.enc[i]\n        g = self.idx2gid[i]\n        pos = i\n        while pos == i:\n            pos = random.choice(self.grp2idx[g])\n        ng = random.choice([x for x in self.grp2idx if x != g])\n        neg = random.choice(self.grp2idx[ng])\n        return anchor, self.enc[pos], self.enc[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_g, val_g = all_gids[:split], all_gids[split:]\ntrain_idx = [i for g in train_g for i in group_to_indices[g]]\nval_idx = [i for g in val_g for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_idx), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_idx), batch_size=8, shuffle=False)\n\n\n# model\nclass CodeEncoder(nn.Module):\n    def __init__(self, vs, ed=64, hd=64):\n        super().__init__()\n        self.embed = nn.Embedding(vs, ed, padding_idx=0)\n        self.lstm = nn.LSTM(ed, hd, batch_first=True)\n\n    def forward(self, x):\n        emb, _ = self.lstm(self.embed(x))\n        return emb[:, -1, :]\n\n\n# hyperparameter tuning over learning rates\nlrs = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\nEPOCHS = 10\nexperiment_data = {\"learning_rate\": {}}\n\nfor lr in lrs:\n    model = CodeEncoder(len(stoi), 64, 64).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n    loss_fn = nn.TripletMarginLoss(margin=1.0)\n    res = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    for ep in range(EPOCHS):\n        model.train()\n        tr_loss = 0.0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            la = loss_fn(model(a), model(p), model(n))\n            optimizer.zero_grad()\n            la.backward()\n            optimizer.step()\n            tr_loss += la.item()\n        tr_loss /= len(train_loader)\n        res[\"losses\"][\"train\"].append(tr_loss)\n        model.eval()\n        vl = 0.0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                vl += loss_fn(model(a), model(p), model(n)).item()\n        vl /= len(val_loader)\n        res[\"losses\"][\"val\"].append(vl)\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            norm = F.normalize(emb_all, dim=1)\n            sims = norm @ norm.T\n\n            def acc(ix):\n                c = 0\n                for i in ix:\n                    v = sims[i].clone()\n                    v[i] = -float(\"inf\")\n                    if index_to_gid[v.argmax().item()] == index_to_gid[i]:\n                        c += 1\n                return c / len(ix) if ix else 0\n\n            tr_acc, va_acc = acc(train_idx), acc(val_idx)\n        res[\"metrics\"][\"train\"].append(tr_acc)\n        res[\"metrics\"][\"val\"].append(va_acc)\n        print(f\"lr={lr:.1e} ep={ep} val_loss={vl:.4f} val_acc={va_acc:.4f}\")\n    # final predictions\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        norm = F.normalize(emb_all, dim=1)\n        sims = norm @ norm.T\n        for i in val_idx:\n            v = sims[i].clone()\n            v[i] = -float(\"inf\")\n            res[\"predictions\"].append(index_to_gid[v.argmax().item()])\n            res[\"ground_truth\"].append(index_to_gid[i])\n    experiment_data[\"learning_rate\"][str(lr)] = res\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# build synthetic code snippets and traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: idxs for gid, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for i in idxs:\n        index_to_gid[i] = gid\n\n# encode snippets as character tokens\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = torch.LongTensor(\n    [[stoi[c] for c in s] + [0] * (max_len - len(s)) for s in codes]\n)\n\n\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\n\n\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# hyperparameter sweep over batch sizes\nbatch_sizes = [4, 16, 32]\nEPOCHS = 10\nexperiment_data = {\"batch_size\": {\"synthetic\": []}}\n\nfor bs in batch_sizes:\n    print(f\"\\n=== Training with batch size = {bs} ===\")\n    train_loader = DataLoader(\n        Subset(dataset, train_indices), batch_size=bs, shuffle=True\n    )\n    val_loader = DataLoader(Subset(dataset, val_indices), batch_size=bs, shuffle=False)\n    model = CodeEncoder(len(stoi), 64, 64).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.TripletMarginLoss(margin=1.0)\n\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n\n    # training loop\n    for epoch in range(EPOCHS):\n        model.train()\n        tloss = 0.0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a = model(a)\n            emb_p = model(p)\n            emb_n = model(n)\n            loss = loss_fn(emb_a, emb_p, emb_n)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            tloss += loss.item()\n        tloss /= len(train_loader)\n        train_losses.append(tloss)\n\n        model.eval()\n        vloss = 0.0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                emb_a = model(a)\n                emb_p = model(p)\n                emb_n = model(n)\n                vloss += loss_fn(emb_a, emb_p, emb_n).item()\n        vloss /= len(val_loader)\n        val_losses.append(vloss)\n        print(f\"Epoch {epoch+1}/{EPOCHS}: train_loss={tloss:.4f}, val_loss={vloss:.4f}\")\n\n        # retrieval accuracy\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n\n            def comp_acc(idxs):\n                correct = 0\n                for i in idxs:\n                    row = sims[i].clone()\n                    row[i] = -float(\"inf\")\n                    if index_to_gid[int(torch.argmax(row))] == index_to_gid[i]:\n                        correct += 1\n                return correct / len(idxs)\n\n            train_accs.append(comp_acc(train_indices))\n            val_accs.append(comp_acc(val_indices))\n        print(f\"  acc -> train: {train_accs[-1]:.4f}, val: {val_accs[-1]:.4f}\")\n\n    # final predictions & ground truth\n    predictions, ground_truth = [], []\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n        for i in val_indices:\n            row = sims[i].clone()\n            row[i] = -float(\"inf\")\n            predictions.append(index_to_gid[int(torch.argmax(row))])\n            ground_truth.append(index_to_gid[i])\n\n    # store results for this batch size\n    experiment_data[\"batch_size\"][\"synthetic\"].append(\n        {\n            \"batch_size\": bs,\n            \"metrics\": {\"train\": train_accs, \"val\": val_accs},\n            \"losses\": {\"train\": train_losses, \"val\": val_losses},\n            \"predictions\": predictions,\n            \"ground_truth\": ground_truth,\n        }\n    )\n\n# save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment data to:\", os.path.join(working_dir, \"experiment_data.npy\"))\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# reproducibility\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic code snippets\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\n\n# generate dynamic traces\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: ids for gid, ids in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# encode as char tokens\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = torch.LongTensor(\n    [[stoi[c] for c in s] + [0] * (max_len - len(s)) for s in codes]\n)\n\n\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# hyperparameter sweep over margin\nmargin_values = [0.2, 0.5, 1.0, 1.5, 2.0]\nEPOCHS = 10\n\nexperiment_data = {\n    \"triplet_margin_sweep\": {\n        \"synthetic\": {\n            \"margins\": margin_values,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor margin in margin_values:\n    # initialize model, optimizer, loss\n    model = CodeEncoder(len(stoi), 64, 64).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.TripletMarginLoss(margin=margin)\n    run_train_metrics, run_val_metrics = [], []\n    run_train_losses, run_val_losses = [], []\n    # training loop\n    for epoch in range(EPOCHS):\n        model.train()\n        train_loss = 0.0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a, emb_p, emb_n = model(a), model(p), model(n)\n            loss = loss_fn(emb_a, emb_p, emb_n)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        train_loss /= len(train_loader)\n        run_train_losses.append(train_loss)\n        # validation loss\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                val_loss += loss_fn(model(a), model(p), model(n)).item()\n        val_loss /= len(val_loader)\n        run_val_losses.append(val_loss)\n        # retrieval accuracy\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n\n            def compute_acc(indices):\n                correct = 0\n                for i in indices:\n                    sim = sims[i].clone()\n                    sim[i] = -float(\"inf\")\n                    top1 = torch.argmax(sim).item()\n                    if index_to_gid[top1] == index_to_gid[i]:\n                        correct += 1\n                return correct / len(indices) if indices else 0\n\n            train_acc = compute_acc(train_indices)\n            val_acc = compute_acc(val_indices)\n        run_train_metrics.append(train_acc)\n        run_val_metrics.append(val_acc)\n    # final retrieval predictions\n    preds, gts = [], []\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n        for i in val_indices:\n            sim = sims[i].clone()\n            sim[i] = -float(\"inf\")\n            top1 = torch.argmax(sim).item()\n            preds.append(index_to_gid[top1])\n            gts.append(index_to_gid[i])\n    # record run data\n    exp = experiment_data[\"triplet_margin_sweep\"][\"synthetic\"]\n    exp[\"metrics\"][\"train\"].append(run_train_metrics)\n    exp[\"metrics\"][\"val\"].append(run_val_metrics)\n    exp[\"losses\"][\"train\"].append(run_train_losses)\n    exp[\"losses\"][\"val\"].append(run_val_losses)\n    exp[\"predictions\"].append(preds)\n    exp[\"ground_truth\"].append(gts)\n\n# save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os, random, numpy as np, torch\nimport torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import Adam\n\n# setup working dir and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# synthetic code and dynamic traces\ncodes = []\nfor c in range(1, 11):\n    codes += [f\"def f(x): return x+{c}\", f\"def f(x): return {c}+x\"]\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group by trace\ntrace_to_indices = {}\nfor i, t in enumerate(traces):\n    trace_to_indices.setdefault(t, []).append(i)\ngroup_to_indices = {g: idxs for g, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor g, idxs in group_to_indices.items():\n    for i in idxs:\n        index_to_gid[i] = g\n\n# char\u2010level encoding\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = torch.LongTensor(\n    [[stoi.get(c, 0) for c in s] + [0] * (max_len - len(s)) for s in codes]\n)\n\n# train/val split at group level\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_g, val_g = all_gids[:split], all_gids[split:]\ntrain_idx = [i for g in train_g for i in group_to_indices[g]]\nval_idx = [i for g in val_g for i in group_to_indices[g]]\n\n\n# dataset with restricted sampling\nclass CodeDataset(Dataset):\n    def __init__(self, enc, grp2idx, idx2gid, allowed_idx):\n        self.enc = enc\n        self.grp2idx = grp2idx\n        self.idx2gid = idx2gid\n        self.allowed = set(allowed_idx)\n\n    def __len__(self):\n        return len(self.allowed)\n\n    def __getitem__(self, local_i):\n        # map local index to global code index\n        idx = list(self.allowed)[local_i]\n        anchor = self.enc[idx]\n        g = self.idx2gid[idx]\n        # positives within same group and same split\n        pos_cands = [j for j in self.grp2idx[g] if j in self.allowed and j != idx]\n        pos = random.choice(pos_cands) if pos_cands else idx\n        # negatives from other groups but within split\n        neg_gs = [gid for gid in self.grp2idx if gid != g]\n        neg_g = random.choice(neg_gs)\n        neg_cands = [j for j in self.grp2idx[neg_g] if j in self.allowed]\n        neg = (\n            random.choice(neg_cands) if neg_cands else random.choice(list(self.allowed))\n        )\n        return anchor, self.enc[pos], self.enc[neg]\n\n\n# create datasets and loaders\ntrain_dataset = CodeDataset(encoded, group_to_indices, index_to_gid, train_idx)\nval_dataset = CodeDataset(encoded, group_to_indices, index_to_gid, val_idx)\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n\n\n# encoder model\nclass CodeEncoder(nn.Module):\n    def __init__(self, vs, ed=64, hd=64):\n        super().__init__()\n        self.embed = nn.Embedding(vs, ed, padding_idx=0)\n        self.lstm = nn.LSTM(ed, hd, batch_first=True)\n\n    def forward(self, x):\n        emb, _ = self.lstm(self.embed(x))\n        return emb[:, -1, :]\n\n\n# hyperparameter tuning\nlrs = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\nEPOCHS = 10\nexperiment_data = {\"learning_rate\": {}}\n\nfor lr in lrs:\n    model = CodeEncoder(len(stoi), 64, 64).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n    loss_fn = nn.TripletMarginLoss(margin=1.0)\n    res = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    for ep in range(EPOCHS):\n        # training\n        model.train()\n        tr_loss = 0.0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            optimizer.zero_grad()\n            la = loss_fn(model(a), model(p), model(n))\n            la.backward()\n            optimizer.step()\n            tr_loss += la.item()\n        tr_loss /= len(train_loader)\n        res[\"losses\"][\"train\"].append(tr_loss)\n        # validation loss\n        model.eval()\n        vl = 0.0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                vl += loss_fn(model(a), model(p), model(n)).item()\n        vl /= len(val_loader)\n        res[\"losses\"][\"val\"].append(vl)\n        # nearest\u2010neighbor accuracy\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            norm = F.normalize(emb_all, dim=1)\n            sims = norm @ norm.T\n\n            def acc(ix):\n                if not ix:\n                    return 0\n                c = 0\n                for i in ix:\n                    v = sims[i].clone()\n                    v[i] = -float(\"inf\")\n                    if index_to_gid[v.argmax().item()] == index_to_gid[i]:\n                        c += 1\n                return c / len(ix)\n\n            tr_acc, va_acc = acc(train_idx), acc(val_idx)\n        res[\"metrics\"][\"train\"].append(tr_acc)\n        res[\"metrics\"][\"val\"].append(va_acc)\n        print(f\"lr={lr:.1e} ep={ep} val_loss={vl:.4f} val_acc={va_acc:.4f}\")\n    # final predictions\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        norm = F.normalize(emb_all, dim=1)\n        sims = norm @ norm.T\n        for i in val_idx:\n            v = sims[i].clone()\n            v[i] = -float(\"inf\")\n            res[\"predictions\"].append(index_to_gid[v.argmax().item()])\n            res[\"ground_truth\"].append(index_to_gid[i])\n    experiment_data[\"learning_rate\"][str(lr)] = res\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nimport random, numpy as np, torch\nimport torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\nfrom datasets import load_dataset\nfrom sklearn.metrics import accuracy_score\n\n# device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic code and traces\ncodes = []\nfor c in range(1, 11):\n    codes += [f\"def f(x): return x+{c}\", f\"def f(x): return {c}+x\"]\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\ntrace_to_indices = {}\nfor i, t in enumerate(traces):\n    trace_to_indices.setdefault(t, []).append(i)\ngroup_to_indices = {g: idxs for g, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor g, idxs in group_to_indices.items():\n    for i in idxs:\n        index_to_gid[i] = g\n\n# encoding setup\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = torch.LongTensor(\n    [[stoi[c] for c in s] + [0] * (max_len - len(s)) for s in codes]\n).to(device)\n\n\n# dataset class\nclass CodeDataset(Dataset):\n    def __init__(self, enc, grp2idx, idx2gid):\n        self.enc, self.grp2idx, self.idx2gid = enc, grp2idx, idx2gid\n\n    def __len__(self):\n        return len(self.idx2gid)\n\n    def __getitem__(self, i):\n        anchor = self.enc[i]\n        g = self.idx2gid[i]\n        pos = i\n        while pos == i:\n            pos = random.choice(self.grp2idx[g])\n        ng = random.choice([x for x in self.grp2idx if x != g])\n        neg = random.choice(self.grp2idx[ng])\n        return anchor, self.enc[pos], self.enc[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_g, val_g = all_gids[:split], all_gids[split:]\ntrain_idx = [i for g in train_g for i in group_to_indices[g]]\nval_idx = [i for g in val_g for i in group_to_indices[g]]\n\n\n# model\nclass CodeEncoder(nn.Module):\n    def __init__(self, vs, ed=64, hd=64):\n        super().__init__()\n        self.embed = nn.Embedding(vs, ed, padding_idx=0)\n        self.lstm = nn.LSTM(ed, hd, batch_first=True)\n\n    def forward(self, x):\n        emb, _ = self.lstm(self.embed(x))\n        return emb[:, -1, :]\n\n\n# hyperparams\nlearning_rates = [1e-4, 5e-4, 1e-3]\nbatch_sizes = [8, 16]\nEPOCHS = 15\n\nexperiment_data = {\n    \"synthetic\": {},\n    \"clone_python\": {\"metrics\": [], \"predictions\": [], \"ground_truth\": []},\n    \"clone_java\": {\"metrics\": [], \"predictions\": [], \"ground_truth\": []},\n}\n\n# hyperparameter tuning\nfor bs in batch_sizes:\n    train_loader = DataLoader(Subset(dataset, train_idx), batch_size=bs, shuffle=True)\n    val_loader = DataLoader(Subset(dataset, val_idx), batch_size=bs, shuffle=False)\n    for lr in learning_rates:\n        key = f\"lr_{lr}_bs_{bs}\"\n        res = {\n            \"metrics\": {\"train\": [], \"val\": [], \"ctec\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n        model = CodeEncoder(len(stoi), 64, 64).to(device)\n        optimizer = Adam(model.parameters(), lr=lr)\n        loss_fn = nn.TripletMarginLoss(margin=1.0)\n        for ep in range(EPOCHS):\n            # train\n            model.train()\n            tr_loss = 0.0\n            for a, p, n in train_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                optimizer.zero_grad()\n                loss = loss_fn(model(a), model(p), model(n))\n                loss.backward()\n                optimizer.step()\n                tr_loss += loss.item()\n            tr_loss /= len(train_loader)\n            res[\"losses\"][\"train\"].append(tr_loss)\n            # eval\n            model.eval()\n            vl = 0.0\n            with torch.no_grad():\n                for a, p, n in val_loader:\n                    a, p, n = a.to(device), p.to(device), n.to(device)\n                    vl += loss_fn(model(a), model(p), model(n)).item()\n                vl /= len(val_loader)\n                # full embeddings\n                emb_all = model(encoded)\n                norm = F.normalize(emb_all, dim=1)\n                sims = norm @ norm.T\n\n                # accuracy\n                def acc(idx):\n                    c = 0\n                    for i in idx:\n                        v = sims[i].clone()\n                        v[i] = -float(\"inf\")\n                        if index_to_gid[v.argmax().item()] == index_to_gid[i]:\n                            c += 1\n                    return c / len(idx) if idx else 0\n\n                tr_acc = acc(train_idx)\n                va_acc = acc(val_idx)\n                # CTEC: average cos between same-gid pairs\n                csum, count = 0.0, 0\n                for i in range(len(index_to_gid)):\n                    for j in range(i + 1, len(index_to_gid)):\n                        if index_to_gid[i] == index_to_gid[j]:\n                            csum += (norm[i] * norm[j]).sum().item()\n                            count += 1\n                ctec = csum / count if count > 0 else 0.0\n            res[\"losses\"][\"val\"].append(vl)\n            res[\"metrics\"][\"train\"].append(tr_acc)\n            res[\"metrics\"][\"val\"].append(va_acc)\n            res[\"metrics\"][\"ctec\"].append(ctec)\n            print(\n                f\"lr={lr:.1e} bs={bs} ep={ep} val_loss={vl:.4f} val_acc={va_acc:.4f} ctec={ctec:.4f}\"\n            )\n        # final synthetic preds\n        with torch.no_grad():\n            emb_all = model(encoded)\n            norm = F.normalize(emb_all, dim=1)\n            sims = norm @ norm.T\n            for i in val_idx:\n                v = sims[i].clone()\n                v[i] = -float(\"inf\")\n                res[\"predictions\"].append(index_to_gid[v.argmax().item()])\n                res[\"ground_truth\"].append(index_to_gid[i])\n        experiment_data[\"synthetic\"][key] = res\n\n\n# evaluate on HuggingFace CodeXGLUE clone detection (python/java)\ndef encode_list(lst):\n    return torch.LongTensor(\n        [\n            [stoi.get(c, 0) for c in s[:max_len]] + [0] * (max_len - len(s[:max_len]))\n            for s in lst\n        ]\n    ).to(device)\n\n\nfor lang_key, cfg in [(\"clone_python\", \"python\"), (\"clone_java\", \"java\")]:\n    ds = (\n        load_dataset(\"code_x_glue_cc_clone_detection\", cfg, split=\"test\")\n        .shuffle(seed=42)\n        .select(range(1000))\n    )\n    c0, c1, lbs = ds[\"code0\"], ds[\"code1\"], ds[\"label\"]\n    enc0 = encode_list(c0)\n    enc1 = encode_list(c1)\n    with torch.no_grad():\n        emb0 = F.normalize(model(enc0), dim=1)\n        emb1 = F.normalize(model(enc1), dim=1)\n        sims = (emb0 * emb1).sum(dim=1).cpu().numpy()\n    preds = (sims > 0.5).astype(int)\n    acc = accuracy_score(lbs, preds)\n    experiment_data[lang_key][\"metrics\"].append(acc)\n    experiment_data[lang_key][\"predictions\"].extend(preds.tolist())\n    experiment_data[lang_key][\"ground_truth\"].extend(lbs)\n\n# save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n# synthetic code and traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\ntrace_to_indices = {}\nfor idx, tr in enumerate(traces):\n    trace_to_indices.setdefault(tr, []).append(idx)\ngroup_to_indices = {g: ids for g, ids in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor g, ids in group_to_indices.items():\n    for i in ids:\n        index_to_gid[i] = g\n\n# token encoding\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = torch.LongTensor(\n    [[stoi[c] for c in s] + [0] * (max_len - len(s)) for s in codes]\n)\n\n\n# Dataset\nclass CodeDataset(Dataset):\n    def __init__(self, enc, g2i, i2g):\n        self.encoded = enc\n        self.group_to_indices = g2i\n        self.index_to_gid = i2g\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\n# train/val split by groups\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# encoder\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# hyperparam sweep\nembed_dims = [32, 64, 128, 256]\nEPOCHS = 10\nexperiment_data = {\n    \"embed_dim\": {\n        \"synthetic\": {\n            \"values\": embed_dims,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor ed in embed_dims:\n    model = CodeEncoder(len(stoi), embed_dim=ed, hidden=64).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.TripletMarginLoss(margin=1.0)\n    run_metrics_train, run_metrics_val = [], []\n    run_losses_train, run_losses_val = [], []\n\n    for epoch in range(EPOCHS):\n        # train\n        model.train()\n        tot_loss = 0.0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a, emb_p, emb_n = model(a), model(p), model(n)\n            loss = loss_fn(emb_a, emb_p, emb_n)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            tot_loss += loss.item()\n        avg_train_loss = tot_loss / len(train_loader)\n        run_losses_train.append(avg_train_loss)\n\n        # val loss\n        model.eval()\n        tot_vloss = 0.0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                emb_a, emb_p, emb_n = model(a), model(p), model(n)\n                tot_vloss += loss_fn(emb_a, emb_p, emb_n).item()\n        run_losses_val.append(tot_vloss / len(val_loader))\n\n        # retrieval acc\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n\n            def comp_acc(indices):\n                c = 0\n                for i in indices:\n                    sim = sims[i].clone()\n                    sim[i] = -float(\"inf\")\n                    top1 = torch.argmax(sim).item()\n                    if index_to_gid[top1] == index_to_gid[i]:\n                        c += 1\n                return c / len(indices)\n\n            run_metrics_train.append(comp_acc(train_indices))\n            run_metrics_val.append(comp_acc(val_indices))\n\n    # final preds and gt\n    preds, gts = [], []\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n        for i in val_indices:\n            sim = sims[i].clone()\n            sim[i] = -float(\"inf\")\n            top1 = torch.argmax(sim).item()\n            preds.append(index_to_gid[top1])\n            gts.append(index_to_gid[i])\n\n    # collect\n    d = experiment_data[\"embed_dim\"][\"synthetic\"]\n    d[\"metrics\"][\"train\"].append(run_metrics_train)\n    d[\"metrics\"][\"val\"].append(run_metrics_val)\n    d[\"losses\"][\"train\"].append(run_losses_train)\n    d[\"losses\"][\"val\"].append(run_losses_val)\n    d[\"predictions\"].append(preds)\n    d[\"ground_truth\"].append(gts)\n    print(f\"Embed_dim={ed} | final val acc={run_metrics_val[-1]:.4f}\")\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# generate synthetic code snippets and dynamic traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n# group by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: ids for gid, ids in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# char-level encoding\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = torch.LongTensor(\n    [[stoi[c] for c in s] + [0] * (max_len - len(s)) for s in codes]\n)\n\n\n# dataset and splits\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# encoder with configurable LSTM dropout (applied between num_layers>1)\nclass CodeEncoder(nn.Module):\n    def __init__(\n        self, vocab_size, embed_dim=64, hidden=64, num_layers=2, lstm_dropout=0.0\n    ):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(\n            embed_dim,\n            hidden,\n            num_layers=num_layers,\n            dropout=lstm_dropout,\n            batch_first=True,\n        )\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h[-1]\n\n\n# hyperparameter sweep over dropout rates\ndropout_rates = [0.1, 0.3, 0.5]\nEPOCHS = 10\nlr = 1e-3\n\nexperiment_data = {\n    \"lstm_dropout\": {\n        \"synthetic\": {\n            \"dropout_rates\": dropout_rates,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor dr in dropout_rates:\n    print(f\"\\n=== Training with LSTM dropout = {dr} ===\")\n    model = CodeEncoder(\n        len(stoi), embed_dim=64, hidden=64, num_layers=2, lstm_dropout=dr\n    ).to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n    loss_fn = nn.TripletMarginLoss(margin=1.0)\n\n    train_metrics, val_metrics = [], []\n    train_losses, val_losses = [], []\n\n    for epoch in range(EPOCHS):\n        # training\n        model.train()\n        total_train_loss = 0.0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a, emb_p, emb_n = model(a), model(p), model(n)\n            loss = loss_fn(emb_a, emb_p, emb_n)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_train_loss += loss.item()\n        avg_train_loss = total_train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n\n        # validation loss\n        model.eval()\n        total_val_loss = 0.0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                total_val_loss += loss_fn(model(a), model(p), model(n)).item()\n        avg_val_loss = total_val_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n\n        # retrieval accuracy\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n\n            def compute_acc(indices):\n                correct = 0\n                for i in indices:\n                    sim = sims[i].clone()\n                    sim[i] = -float(\"inf\")\n                    top1 = torch.argmax(sim).item()\n                    if index_to_gid[top1] == index_to_gid[i]:\n                        correct += 1\n                return correct / len(indices) if indices else 0\n\n            train_acc = compute_acc(train_indices)\n            val_acc = compute_acc(val_indices)\n        train_metrics.append(train_acc)\n        val_metrics.append(val_acc)\n\n        print(\n            f\"Dropout {dr} Epoch {epoch+1}/{EPOCHS} \"\n            f\"train_loss={avg_train_loss:.4f}, val_loss={avg_val_loss:.4f}, \"\n            f\"val_acc={val_acc:.4f}\"\n        )\n\n    # record per-rate curves\n    exp = experiment_data[\"lstm_dropout\"][\"synthetic\"]\n    exp[\"metrics\"][\"train\"].append(train_metrics)\n    exp[\"metrics\"][\"val\"].append(val_metrics)\n    exp[\"losses\"][\"train\"].append(train_losses)\n    exp[\"losses\"][\"val\"].append(val_losses)\n\n    # final predictions & ground truth on validation\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n        preds, gts = [], []\n        for i in val_indices:\n            sim = sims[i].clone()\n            sim[i] = -float(\"inf\")\n            top1 = torch.argmax(sim).item()\n            preds.append(index_to_gid[top1])\n            gts.append(index_to_gid[i])\n    exp[\"predictions\"].append(preds)\n    exp[\"ground_truth\"].append(gts)\n\n# save all collected data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nHyperparameter tuning complete. Data saved to experiment_data.npy\")\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic code snippets\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\n\n# generate dynamic traces\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: idxs for gid, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [0] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for i in idxs:\n        index_to_gid[i] = gid\n\n# encode as char tokens\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = torch.LongTensor(\n    [[stoi[c] for c in s] + [0] * (max_len - len(s)) for s in codes]\n)\n\n\n# dataset and loaders\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# encoder model\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# helper for retrieval accuracy\ndef compute_acc(sims, indices):\n    correct = 0\n    for i in indices:\n        v = sims[i].clone()\n        v[i] = -float(\"inf\")\n        top1 = torch.argmax(v).item()\n        if index_to_gid[top1] == index_to_gid[i]:\n            correct += 1\n    return correct / len(indices) if indices else 0\n\n\n# hyperparameter sweep for weight_decay\nweight_decay_values = [0, 1e-5, 1e-4, 1e-3, 1e-2]\nEPOCHS = 10\nexperiment_data = {\n    \"weight_decay_sweep\": {\n        \"synthetic\": {\n            \"params\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor wd in weight_decay_values:\n    print(f\"\\n--- Training with weight_decay = {wd} ---\")\n    experiment_data[\"weight_decay_sweep\"][\"synthetic\"][\"params\"].append(wd)\n    # init model, optimizer, loss\n    model = CodeEncoder(len(stoi), 64, 64).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n    loss_fn = nn.TripletMarginLoss(margin=1.0)\n    # storage per run\n    run_train_losses, run_val_losses = [], []\n    run_train_acc, run_val_acc = [], []\n    # training loop\n    for epoch in range(EPOCHS):\n        model.train()\n        tloss = 0.0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a, emb_p, emb_n = model(a), model(p), model(n)\n            loss = loss_fn(emb_a, emb_p, emb_n)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            tloss += loss.item()\n        tloss /= len(train_loader)\n        run_train_losses.append(tloss)\n        # validation loss\n        model.eval()\n        vloss = 0.0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                vloss += loss_fn(model(a), model(p), model(n)).item()\n        vloss /= len(val_loader)\n        run_val_losses.append(vloss)\n        # retrieval metrics\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n            tra = compute_acc(sims, train_indices)\n            va = compute_acc(sims, val_indices)\n        run_train_acc.append(tra)\n        run_val_acc.append(va)\n        print(\n            f\"WD {wd} Epoch {epoch}: train_loss={tloss:.4f}, val_loss={vloss:.4f}, train_acc={tra:.4f}, val_acc={va:.4f}\"\n        )\n    # record run data\n    sd = experiment_data[\"weight_decay_sweep\"][\"synthetic\"]\n    sd[\"losses\"][\"train\"].append(run_train_losses)\n    sd[\"losses\"][\"val\"].append(run_val_losses)\n    sd[\"metrics\"][\"train\"].append(run_train_acc)\n    sd[\"metrics\"][\"val\"].append(run_val_acc)\n    # predictions and ground truth\n    preds, gts = [], []\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n        for i in val_indices:\n            s = sims[i].clone()\n            s[i] = -float(\"inf\")\n            top1 = torch.argmax(s).item()\n            preds.append(index_to_gid[top1])\n            gts.append(index_to_gid[i])\n    sd[\"predictions\"].append(preds)\n    sd[\"ground_truth\"].append(gts)\n\n# save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment_data.npy with weight_decay results.\")\n", "import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# synthetic code snippets\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\n# dynamic traces\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: idxs for gid, idxs in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for i in idxs:\n        index_to_gid[i] = gid\n# encode as char tokens\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\nclass CodeDataset(Dataset):\n    def __init__(\n        self, encoded, group_to_indices, index_to_gid, hard_negative_pool_size=1\n    ):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n        self.pool_size = hard_negative_pool_size\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        # positive\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        # negatives pool\n        negs = []\n        for _ in range(self.pool_size):\n            neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n            neg_idx = random.choice(self.group_to_indices[neg_gid])\n            negs.append(self.encoded[neg_idx])\n        negs = torch.stack(negs, dim=0)  # [pool_size, max_len]\n        return anchor, self.encoded[pos], negs\n\n\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# train/val split by group\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\n\n# hyperparam grid\npool_sizes = [1, 2, 5, 10]\nexperiment_data = {\n    \"hard_negative_pool_size\": {\n        \"synthetic\": {\n            \"pool_sizes\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nEPOCHS = 10\nfor pool_size in pool_sizes:\n    # new dataset, loaders, model, optimizer per run\n    dataset = CodeDataset(\n        encoded, group_to_indices, index_to_gid, hard_negative_pool_size=pool_size\n    )\n    train_loader = DataLoader(\n        Subset(dataset, train_indices), batch_size=8, shuffle=True\n    )\n    val_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n    model = CodeEncoder(len(stoi), 64, 64).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.TripletMarginLoss(margin=1.0)\n\n    losses_train_run, losses_val_run = [], []\n    metrics_train_run, metrics_val_run = [], []\n\n    for epoch in range(EPOCHS):\n        # training\n        model.train()\n        train_loss = 0.0\n        for a, p, ns in train_loader:\n            a, p, ns = a.to(device), p.to(device), ns.to(device)\n            emb_a = model(a)\n            emb_p = model(p)\n            B, K, L = ns.size()\n            emb_ns = model(ns.view(-1, L)).view(B, K, -1)\n            sim = F.cosine_similarity(emb_a.unsqueeze(1), emb_ns, dim=2)  # [B,K]\n            hard_idx = torch.argmax(sim, dim=1)\n            emb_n = emb_ns[torch.arange(B), hard_idx]\n            loss = loss_fn(emb_a, emb_p, emb_n)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        train_loss /= len(train_loader)\n        losses_train_run.append(train_loss)\n\n        # validation loss\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for a, p, ns in val_loader:\n                a, p, ns = a.to(device), p.to(device), ns.to(device)\n                emb_a = model(a)\n                emb_p = model(p)\n                B, K, L = ns.size()\n                emb_ns = model(ns.view(-1, L)).view(B, K, -1)\n                sim = F.cosine_similarity(emb_a.unsqueeze(1), emb_ns, dim=2)\n                hard_idx = torch.argmax(sim, dim=1)\n                emb_n = emb_ns[torch.arange(B), hard_idx]\n                val_loss += loss_fn(emb_a, emb_p, emb_n).item()\n        val_loss /= len(val_loader)\n        losses_val_run.append(val_loss)\n\n        # retrieval accuracy\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n\n            def compute_acc(indices):\n                correct = 0\n                for i in indices:\n                    sim_i = sims[i].clone()\n                    sim_i[i] = -float(\"inf\")\n                    top1 = torch.argmax(sim_i).item()\n                    if index_to_gid[top1] == index_to_gid[i]:\n                        correct += 1\n                return correct / len(indices) if indices else 0\n\n            train_acc = compute_acc(train_indices)\n            val_acc = compute_acc(val_indices)\n        metrics_train_run.append(train_acc)\n        metrics_val_run.append(val_acc)\n\n        print(\n            f\"Pool {pool_size} | Epoch {epoch} | val_loss {val_loss:.4f} | train_acc {train_acc:.4f} | val_acc {val_acc:.4f}\"\n        )\n\n    # record run results\n    exp = experiment_data[\"hard_negative_pool_size\"][\"synthetic\"]\n    exp[\"pool_sizes\"].append(pool_size)\n    exp[\"losses\"][\"train\"].append(losses_train_run)\n    exp[\"losses\"][\"val\"].append(losses_val_run)\n    exp[\"metrics\"][\"train\"].append(metrics_train_run)\n    exp[\"metrics\"][\"val\"].append(metrics_val_run)\n\n    # final predictions & ground truth\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n        preds, gts = [], []\n        for i in val_indices:\n            sim_i = sims[i].clone()\n            sim_i[i] = -float(\"inf\")\n            top1 = torch.argmax(sim_i).item()\n            preds.append(index_to_gid[top1])\n            gts.append(index_to_gid[i])\n    exp[\"predictions\"].append(preds)\n    exp[\"ground_truth\"].append(gts)\n\n# save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# prepare storage for hyperparameter tuning\nexperiment_data = {\"lstm_num_layers\": {\"synthetic\": {}}}\n\n# synthetic code snippets\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\n\n# generate dynamic traces\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: inds for gid, inds in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, inds in group_to_indices.items():\n    for idx in inds:\n        index_to_gid[idx] = gid\n\n# encode as char tokens\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = torch.LongTensor(\n    [[stoi[c] for c in s] + [0] * (max_len - len(s)) for s in codes]\n)\n\n\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\n# build dataset and split\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\n\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# flexible encoder with variable LSTM depth\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64, num_layers=1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True, num_layers=num_layers)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        # take top layer's hidden state\n        return h[-1]\n\n\n# hyperparameter grid\nlayer_options = [1, 2, 3]\nEPOCHS = 10\nfor num_layers in layer_options:\n    print(f\"\\n=== Training with {num_layers} LSTM layer(s) ===\")\n    model = CodeEncoder(len(stoi), embed_dim=64, hidden=64, num_layers=num_layers).to(\n        device\n    )\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.TripletMarginLoss(margin=1.0)\n\n    losses_train, losses_val = [], []\n    metrics_train, metrics_val = [], []\n\n    for epoch in range(EPOCHS):\n        model.train()\n        running_loss = 0.0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a, emb_p, emb_n = model(a), model(p), model(n)\n            loss = loss_fn(emb_a, emb_p, emb_n)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        losses_train.append(running_loss / len(train_loader))\n\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                emb_a, emb_p, emb_n = model(a), model(p), model(n)\n                val_loss += loss_fn(emb_a, emb_p, emb_n).item()\n        losses_val.append(val_loss / len(val_loader))\n        print(\n            f\"Layer {num_layers} Epoch {epoch}: train_loss={losses_train[-1]:.4f}, val_loss={losses_val[-1]:.4f}\"\n        )\n\n        # compute retrieval accuracy\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n\n            def compute_acc(indices):\n                correct = 0\n                for i in indices:\n                    s = sims[i].clone()\n                    s[i] = -float(\"inf\")\n                    top1 = torch.argmax(s).item()\n                    if index_to_gid[top1] == index_to_gid[i]:\n                        correct += 1\n                return correct / len(indices) if indices else 0\n\n            metrics_train.append(compute_acc(train_indices))\n            metrics_val.append(compute_acc(val_indices))\n\n    # final predictions & ground truth for validation\n    preds, gts = [], []\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n        for i in val_indices:\n            s = sims[i].clone()\n            s[i] = -float(\"inf\")\n            top1 = torch.argmax(s).item()\n            preds.append(index_to_gid[top1])\n            gts.append(index_to_gid[i])\n\n    # record everything\n    experiment_data[\"lstm_num_layers\"][\"synthetic\"][str(num_layers)] = {\n        \"metrics\": {\"train\": metrics_train, \"val\": metrics_val},\n        \"losses\": {\"train\": losses_train, \"val\": losses_val},\n        \"predictions\": preds,\n        \"ground_truth\": gts,\n    }\n\n# save all data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"\\nSaved experiment_data.npy with results for lstm_num_layers =\", layer_options)\n", "import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# synthetic code snippets\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\n\n# generate dynamic traces\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: ids for gid, ids in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# encode as char tokens\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\n# dataset\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# encoder with bidirectional toggle\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64, bidirectional=False):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.bidirectional = bidirectional\n        if self.bidirectional:\n            assert hidden % 2 == 0, \"hidden must be even for bidirectional\"\n            self.lstm = nn.LSTM(\n                embed_dim, hidden // 2, batch_first=True, bidirectional=True\n            )\n            self.hidden_dim = hidden\n        else:\n            self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n            self.hidden_dim = hidden\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        if self.bidirectional:\n            # h: (2, batch, hidden//2)\n            h_f, h_b = h[0], h[1]\n            return torch.cat((h_f, h_b), dim=1)\n        else:\n            return h.squeeze(0)\n\n\n# hyperparam tuning\nEPOCHS = 10\nexperiment_data = {\"lstm_bidirectional\": {}}\nfor bidirectional in [False, True]:\n    key = str(bidirectional)\n    experiment_data[\"lstm_bidirectional\"][key] = {\n        \"synthetic\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n    # model & optimizer\n    model = CodeEncoder(\n        len(stoi), embed_dim=64, hidden=64, bidirectional=bidirectional\n    ).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.TripletMarginLoss(margin=1.0)\n\n    # training loop\n    for epoch in range(EPOCHS):\n        # train\n        model.train()\n        t_loss = 0.0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a, emb_p, emb_n = model(a), model(p), model(n)\n            loss = loss_fn(emb_a, emb_p, emb_n)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            t_loss += loss.item()\n        t_loss /= len(train_loader)\n        experiment_data[\"lstm_bidirectional\"][key][\"synthetic\"][\"losses\"][\n            \"train\"\n        ].append(t_loss)\n\n        # val loss\n        model.eval()\n        v_loss = 0.0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                v_loss += loss_fn(model(a), model(p), model(n)).item()\n        v_loss /= len(val_loader)\n        experiment_data[\"lstm_bidirectional\"][key][\"synthetic\"][\"losses\"][\"val\"].append(\n            v_loss\n        )\n\n        # retrieval accuracy\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n\n            def compute_acc(indices):\n                correct = 0\n                for i in indices:\n                    sim = sims[i].clone()\n                    sim[i] = -float(\"inf\")\n                    top1 = torch.argmax(sim).item()\n                    if index_to_gid[top1] == index_to_gid[i]:\n                        correct += 1\n                return correct / len(indices) if indices else 0\n\n            tr_acc = compute_acc(train_indices)\n            va_acc = compute_acc(val_indices)\n        experiment_data[\"lstm_bidirectional\"][key][\"synthetic\"][\"metrics\"][\n            \"train\"\n        ].append(tr_acc)\n        experiment_data[\"lstm_bidirectional\"][key][\"synthetic\"][\"metrics\"][\n            \"val\"\n        ].append(va_acc)\n        print(\n            f\"bidirectional={bidirectional} epoch={epoch} val_loss={v_loss:.4f} val_acc={va_acc:.4f}\"\n        )\n\n    # final predictions & ground truth on val\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n        for i in val_indices:\n            sim = sims[i].clone()\n            sim[i] = -float(\"inf\")\n            top1 = torch.argmax(sim).item()\n            experiment_data[\"lstm_bidirectional\"][key][\"synthetic\"][\n                \"predictions\"\n            ].append(index_to_gid[top1])\n            experiment_data[\"lstm_bidirectional\"][key][\"synthetic\"][\n                \"ground_truth\"\n            ].append(index_to_gid[i])\n\n# save\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# generate synthetic code snippets and traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: ids for gid, ids in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# encode snippets as character sequences\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\n# dataset & loaders\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# model definition\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# hyperparameter tuning over different epoch counts\nEPOCH_LIST = [10, 30, 50]\nexperiment_data = {\"EPOCHS\": {\"synthetic\": {}}}\n\nfor E in EPOCH_LIST:\n    # reinitialize model & optimizer\n    model = CodeEncoder(len(stoi), 64, 64).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.TripletMarginLoss(margin=1.0)\n\n    # containers for this setting\n    data = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    # training + validation\n    for epoch in range(E):\n        model.train()\n        total_train_loss = 0.0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a, emb_p, emb_n = model(a), model(p), model(n)\n            loss = loss_fn(emb_a, emb_p, emb_n)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_train_loss += loss.item()\n        avg_train_loss = total_train_loss / len(train_loader)\n        data[\"losses\"][\"train\"].append(avg_train_loss)\n\n        model.eval()\n        total_val_loss = 0.0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                total_val_loss += loss_fn(model(a), model(p), model(n)).item()\n        avg_val_loss = total_val_loss / len(val_loader)\n        data[\"losses\"][\"val\"].append(avg_val_loss)\n\n        # compute retrieval accuracies\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n\n            def compute_acc(indices):\n                correct = 0\n                for i in indices:\n                    sim = sims[i].clone()\n                    sim[i] = -float(\"inf\")\n                    top1 = torch.argmax(sim).item()\n                    if index_to_gid[top1] == index_to_gid[i]:\n                        correct += 1\n                return correct / len(indices)\n\n            train_acc = compute_acc(train_indices)\n            val_acc = compute_acc(val_indices)\n        data[\"metrics\"][\"train\"].append(train_acc)\n        data[\"metrics\"][\"val\"].append(val_acc)\n\n    # final predictions on validation\n    model.eval()\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n        for i in val_indices:\n            sim = sims[i].clone()\n            sim[i] = -float(\"inf\")\n            top1 = torch.argmax(sim).item()\n            data[\"predictions\"].append(index_to_gid[top1])\n            data[\"ground_truth\"].append(index_to_gid[i])\n\n    experiment_data[\"EPOCHS\"][\"synthetic\"][E] = data\n    print(f\"Finished EPOCHS={E}: final val_acc={val_acc:.4f}\")\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# generate synthetic code snippets and traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: ids for gid, ids in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# encode snippets as character sequences\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\n# dataset & loaders\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# model definition\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# hyperparameter tuning over different epoch counts\nEPOCH_LIST = [10, 30, 50]\nexperiment_data = {\"EPOCHS\": {\"synthetic\": {}}}\n\nfor E in EPOCH_LIST:\n    # reinitialize model & optimizer\n    model = CodeEncoder(len(stoi), 64, 64).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.TripletMarginLoss(margin=1.0)\n\n    # containers for this setting\n    data = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    # training + validation\n    for epoch in range(E):\n        model.train()\n        total_train_loss = 0.0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a, emb_p, emb_n = model(a), model(p), model(n)\n            loss = loss_fn(emb_a, emb_p, emb_n)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_train_loss += loss.item()\n        avg_train_loss = total_train_loss / len(train_loader)\n        data[\"losses\"][\"train\"].append(avg_train_loss)\n\n        model.eval()\n        total_val_loss = 0.0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                total_val_loss += loss_fn(model(a), model(p), model(n)).item()\n        avg_val_loss = total_val_loss / len(val_loader)\n        data[\"losses\"][\"val\"].append(avg_val_loss)\n\n        # compute retrieval accuracies\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n\n            def compute_acc(indices):\n                correct = 0\n                for i in indices:\n                    sim = sims[i].clone()\n                    sim[i] = -float(\"inf\")\n                    top1 = torch.argmax(sim).item()\n                    if index_to_gid[top1] == index_to_gid[i]:\n                        correct += 1\n                return correct / len(indices)\n\n            train_acc = compute_acc(train_indices)\n            val_acc = compute_acc(val_indices)\n        data[\"metrics\"][\"train\"].append(train_acc)\n        data[\"metrics\"][\"val\"].append(val_acc)\n\n    # final predictions on validation\n    model.eval()\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n        for i in val_indices:\n            sim = sims[i].clone()\n            sim[i] = -float(\"inf\")\n            top1 = torch.argmax(sim).item()\n            data[\"predictions\"].append(index_to_gid[top1])\n            data[\"ground_truth\"].append(index_to_gid[i])\n\n    experiment_data[\"EPOCHS\"][\"synthetic\"][E] = data\n    print(f\"Finished EPOCHS={E}: final val_acc={val_acc:.4f}\")\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim import Adam\n\n# set up working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# generate synthetic code snippets and traces\ncodes = []\nfor c in range(1, 11):\n    codes.append(f\"def f(x): return x+{c}\")\n    codes.append(f\"def f(x): return {c}+x\")\ninput_set = np.random.randint(-10, 10, size=100)\ntraces = []\nfor code in codes:\n    env = {}\n    exec(code, env)\n    f = env[\"f\"]\n    traces.append(tuple(f(int(x)) for x in input_set))\n\n# group snippets by trace\ntrace_to_indices = {}\nfor idx, trace in enumerate(traces):\n    trace_to_indices.setdefault(trace, []).append(idx)\ngroup_to_indices = {gid: ids for gid, ids in enumerate(trace_to_indices.values())}\nindex_to_gid = [None] * len(codes)\nfor gid, idxs in group_to_indices.items():\n    for idx in idxs:\n        index_to_gid[idx] = gid\n\n# encode snippets as character sequences\nvocab = sorted(set(\"\".join(codes)))\nstoi = {c: i + 1 for i, c in enumerate(vocab)}\nstoi[\"PAD\"] = 0\nmax_len = max(len(s) for s in codes)\nencoded = []\nfor s in codes:\n    seq = [stoi[c] for c in s] + [0] * (max_len - len(s))\n    encoded.append(seq)\nencoded = torch.LongTensor(encoded)\n\n\n# dataset & loaders\nclass CodeDataset(Dataset):\n    def __init__(self, encoded, group_to_indices, index_to_gid):\n        self.encoded = encoded\n        self.group_to_indices = group_to_indices\n        self.index_to_gid = index_to_gid\n\n    def __len__(self):\n        return len(self.index_to_gid)\n\n    def __getitem__(self, idx):\n        anchor = self.encoded[idx]\n        gid = self.index_to_gid[idx]\n        pos = idx\n        while pos == idx:\n            pos = random.choice(self.group_to_indices[gid])\n        neg_gid = random.choice([g for g in self.group_to_indices if g != gid])\n        neg = random.choice(self.group_to_indices[neg_gid])\n        return anchor, self.encoded[pos], self.encoded[neg]\n\n\ndataset = CodeDataset(encoded, group_to_indices, index_to_gid)\nall_gids = list(group_to_indices.keys())\nrandom.shuffle(all_gids)\nsplit = int(0.8 * len(all_gids))\ntrain_gids, val_gids = all_gids[:split], all_gids[split:]\ntrain_indices = [i for g in train_gids for i in group_to_indices[g]]\nval_indices = [i for g in val_gids for i in group_to_indices[g]]\ntrain_loader = DataLoader(Subset(dataset, train_indices), batch_size=8, shuffle=True)\nval_loader = DataLoader(Subset(dataset, val_indices), batch_size=8, shuffle=False)\n\n\n# model definition\nclass CodeEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden=64):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        _, (h, _) = self.lstm(x)\n        return h.squeeze(0)\n\n\n# hyperparameter tuning over different epoch counts\nEPOCH_LIST = [10, 30, 50]\nexperiment_data = {\"EPOCHS\": {\"synthetic\": {}}}\n\nfor E in EPOCH_LIST:\n    # reinitialize model & optimizer\n    model = CodeEncoder(len(stoi), 64, 64).to(device)\n    optimizer = Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.TripletMarginLoss(margin=1.0)\n\n    # containers for this setting\n    data = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n\n    # training + validation\n    for epoch in range(E):\n        model.train()\n        total_train_loss = 0.0\n        for a, p, n in train_loader:\n            a, p, n = a.to(device), p.to(device), n.to(device)\n            emb_a, emb_p, emb_n = model(a), model(p), model(n)\n            loss = loss_fn(emb_a, emb_p, emb_n)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_train_loss += loss.item()\n        avg_train_loss = total_train_loss / len(train_loader)\n        data[\"losses\"][\"train\"].append(avg_train_loss)\n\n        model.eval()\n        total_val_loss = 0.0\n        with torch.no_grad():\n            for a, p, n in val_loader:\n                a, p, n = a.to(device), p.to(device), n.to(device)\n                total_val_loss += loss_fn(model(a), model(p), model(n)).item()\n        avg_val_loss = total_val_loss / len(val_loader)\n        data[\"losses\"][\"val\"].append(avg_val_loss)\n\n        # compute retrieval accuracies\n        with torch.no_grad():\n            emb_all = model(encoded.to(device))\n            emb_norm = F.normalize(emb_all, dim=1)\n            sims = emb_norm @ emb_norm.T\n\n            def compute_acc(indices):\n                correct = 0\n                for i in indices:\n                    sim = sims[i].clone()\n                    sim[i] = -float(\"inf\")\n                    top1 = torch.argmax(sim).item()\n                    if index_to_gid[top1] == index_to_gid[i]:\n                        correct += 1\n                return correct / len(indices)\n\n            train_acc = compute_acc(train_indices)\n            val_acc = compute_acc(val_indices)\n        data[\"metrics\"][\"train\"].append(train_acc)\n        data[\"metrics\"][\"val\"].append(val_acc)\n\n    # final predictions on validation\n    model.eval()\n    with torch.no_grad():\n        emb_all = model(encoded.to(device))\n        emb_norm = F.normalize(emb_all, dim=1)\n        sims = emb_norm @ emb_norm.T\n        for i in val_indices:\n            sim = sims[i].clone()\n            sim[i] = -float(\"inf\")\n            top1 = torch.argmax(sim).item()\n            data[\"predictions\"].append(index_to_gid[top1])\n            data[\"ground_truth\"].append(index_to_gid[i])\n\n    experiment_data[\"EPOCHS\"][\"synthetic\"][E] = data\n    print(f\"Finished EPOCHS={E}: final val_acc={val_acc:.4f}\")\n\n# save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Epoch 0: validation_loss = 0.9259', '\\n', 'Epoch\n1: validation_loss = 0.8038', '\\n', 'Epoch 2: validation_loss = 0.7574', '\\n',\n'Epoch 3: validation_loss = 0.6785', '\\n', 'Epoch 4: validation_loss = 0.6791',\n'\\n', 'Epoch 5: validation_loss = 0.6019', '\\n', 'Epoch 6: validation_loss =\n0.6162', '\\n', 'Epoch 7: validation_loss = 0.3668', '\\n', 'Epoch 8:\nvalidation_loss = 0.1523', '\\n', 'Epoch 9: validation_loss = 0.2121', '\\n',\n'Top-1 Trace-Equivalence Retrieval Accuracy: 1.0000', '\\n', 'Execution time: 2\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished EPOCHS=10: final val_acc=1.0000', '\\n',\n'Finished EPOCHS=30: final val_acc=1.0000', '\\n', 'Finished EPOCHS=50: final\nval_acc=1.0000', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 3\nseconds seconds (time limit is an hour).']", "['lr=1.0e-04 ep=0 val_loss=0.9360 val_acc=0.5000', '\\n', 'lr=1.0e-04 ep=1\nval_loss=0.8000 val_acc=0.5000', '\\n', 'lr=1.0e-04 ep=2 val_loss=0.9730\nval_acc=0.5000', '\\n', 'lr=1.0e-04 ep=3 val_loss=0.8358 val_acc=0.5000', '\\n',\n'lr=1.0e-04 ep=4 val_loss=0.8840 val_acc=0.5000', '\\n', 'lr=1.0e-04 ep=5\nval_loss=0.8843 val_acc=0.5000', '\\n', 'lr=1.0e-04 ep=6 val_loss=0.8290\nval_acc=0.5000', '\\n', 'lr=1.0e-04 ep=7 val_loss=0.9119 val_acc=0.5000', '\\n',\n'lr=1.0e-04 ep=8 val_loss=0.7110 val_acc=0.5000', '\\n', 'lr=1.0e-04 ep=9\nval_loss=0.7781 val_acc=0.5000', '\\n', 'lr=5.0e-04 ep=0 val_loss=0.9137\nval_acc=0.5000', '\\n', 'lr=5.0e-04 ep=1 val_loss=0.8284 val_acc=0.5000', '\\n',\n'lr=5.0e-04 ep=2 val_loss=0.9228 val_acc=0.5000', '\\n', 'lr=5.0e-04 ep=3\nval_loss=0.8127 val_acc=0.5000', '\\n', 'lr=5.0e-04 ep=4 val_loss=0.9465\nval_acc=0.5000', '\\n', 'lr=5.0e-04 ep=5 val_loss=0.9177 val_acc=0.5000', '\\n',\n'lr=5.0e-04 ep=6 val_loss=0.8187 val_acc=0.5000', '\\n', 'lr=5.0e-04 ep=7\nval_loss=0.5332 val_acc=0.5000', '\\n', 'lr=5.0e-04 ep=8 val_loss=0.6829\nval_acc=0.7500', '\\n', 'lr=5.0e-04 ep=9 val_loss=0.7524 val_acc=0.7500', '\\n',\n'lr=1.0e-03 ep=0 val_loss=0.8665 val_acc=0.5000', '\\n', 'lr=1.0e-03 ep=1\nval_loss=0.9324 val_acc=0.5000', '\\n', 'lr=1.0e-03 ep=2 val_loss=0.7994\nval_acc=0.5000', '\\n', 'lr=1.0e-03 ep=3 val_loss=0.7660 val_acc=0.5000', '\\n',\n'lr=1.0e-03 ep=4 val_loss=0.6553 val_acc=0.7500', '\\n', 'lr=1.0e-03 ep=5\nval_loss=0.7134 val_acc=1.0000', '\\n', 'lr=1.0e-03 ep=6 val_loss=0.5892\nval_acc=1.0000', '\\n', 'lr=1.0e-03 ep=7 val_loss=0.2969 val_acc=1.0000', '\\n',\n'lr=1.0e-03 ep=8 val_loss=0.4023 val_acc=1.0000', '\\n', 'lr=1.0e-03 ep=9\nval_loss=0.2715 val_acc=1.0000', '\\n', 'lr=5.0e-03 ep=0 val_loss=0.6051\nval_acc=0.5000', '\\n', 'lr=5.0e-03 ep=1 val_loss=0.3830 val_acc=1.0000', '\\n',\n'lr=5.0e-03 ep=2 val_loss=0.2358 val_acc=1.0000', '\\n', 'lr=5.0e-03 ep=3\nval_loss=0.0350 val_acc=1.0000', '\\n', 'lr=5.0e-03 ep=4 val_loss=0.0000\nval_acc=1.0000', '\\n', 'lr=5.0e-03 ep=5 val_loss=0.0000 val_acc=1.0000', '\\n',\n'lr=5.0e-03 ep=6 val_loss=0.0351 val_acc=1.0000', '\\n', 'lr=5.0e-03 ep=7\nval_loss=0.0109 val_acc=1.0000', '\\n', 'lr=5.0e-03 ep=8 val_loss=0.0000\nval_acc=1.0000', '\\n', 'lr=5.0e-03 ep=9 val_loss=0.0000 val_acc=1.0000', '\\n',\n'lr=1.0e-02 ep=0 val_loss=0.5458 val_acc=0.7500', '\\n', 'lr=1.0e-02 ep=1\nval_loss=0.0678 val_acc=1.0000', '\\n', 'lr=1.0e-02 ep=2 val_loss=0.0000\nval_acc=1.0000', '\\n', 'lr=1.0e-02 ep=3 val_loss=0.0000 val_acc=1.0000', '\\n',\n'lr=1.0e-02 ep=4 val_loss=0.0000 val_acc=1.0000', '\\n', 'lr=1.0e-02 ep=5\nval_loss=0.0039 val_acc=1.0000', '\\n', 'lr=1.0e-02 ep=6 val_loss=0.0000\nval_acc=1.0000', '\\n', 'lr=1.0e-02 ep=7 val_loss=0.0000 val_acc=1.0000', '\\n',\n'lr=1.0e-02 ep=8 val_loss=0.0000 val_acc=1.0000', '\\n', 'lr=1.0e-02 ep=9\nval_loss=0.0000 val_acc=1.0000', '\\n', 'Saved experiment_data.npy', '\\n',\n'Execution time: 3 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Training with batch size = 4 ===', '\\n',\n'Epoch 1/10: train_loss=0.9010, val_loss=0.8006', '\\n', '  acc -> train: 0.4375,\nval: 0.5000', '\\n', 'Epoch 2/10: train_loss=0.8060, val_loss=0.9256', '\\n', '\nacc -> train: 0.5000, val: 0.5000', '\\n', 'Epoch 3/10: train_loss=0.5611,\nval_loss=0.7950', '\\n', '  acc -> train: 0.8750, val: 0.5000', '\\n', 'Epoch\n4/10: train_loss=0.5336, val_loss=0.5545', '\\n', '  acc -> train: 0.9375, val:\n0.5000', '\\n', 'Epoch 5/10: train_loss=0.3106, val_loss=0.6721', '\\n', '  acc ->\ntrain: 1.0000, val: 1.0000', '\\n', 'Epoch 6/10: train_loss=0.1764,\nval_loss=0.2627', '\\n', '  acc -> train: 1.0000, val: 1.0000', '\\n', 'Epoch\n7/10: train_loss=0.1254, val_loss=0.1910', '\\n', '  acc -> train: 1.0000, val:\n1.0000', '\\n', 'Epoch 8/10: train_loss=0.0561, val_loss=0.0000', '\\n', '  acc ->\ntrain: 1.0000, val: 1.0000', '\\n', 'Epoch 9/10: train_loss=0.0000,\nval_loss=0.0816', '\\n', '  acc -> train: 1.0000, val: 1.0000', '\\n', 'Epoch\n10/10: train_loss=0.0139, val_loss=0.0000', '\\n', '  acc -> train: 1.0000, val:\n1.0000', '\\n', '\\n=== Training with batch size = 16 ===', '\\n', 'Epoch 1/10:\ntrain_loss=0.9683, val_loss=0.8511', '\\n', '  acc -> train: 0.3750, val:\n0.5000', '\\n', 'Epoch 2/10: train_loss=0.9303, val_loss=1.0397', '\\n', '  acc ->\ntrain: 0.5000, val: 0.5000', '\\n', 'Epoch 3/10: train_loss=0.8969,\nval_loss=0.5693', '\\n', '  acc -> train: 0.5000, val: 0.5000', '\\n', 'Epoch\n4/10: train_loss=0.8469, val_loss=0.8716', '\\n', '  acc -> train: 0.5000, val:\n0.5000', '\\n', 'Epoch 5/10: train_loss=0.8079, val_loss=0.7802', '\\n', '  acc ->\ntrain: 0.5625, val: 0.5000', '\\n', 'Epoch 6/10: train_loss=0.7587,\nval_loss=0.9284', '\\n', '  acc -> train: 0.6875, val: 0.5000', '\\n', 'Epoch\n7/10: train_loss=0.6936, val_loss=0.8511', '\\n', '  acc -> train: 0.8125, val:\n0.5000', '\\n', 'Epoch 8/10: train_loss=0.6024, val_loss=0.7283', '\\n', '  acc ->\ntrain: 0.8750, val: 0.5000', '\\n', 'Epoch 9/10: train_loss=0.6228,\nval_loss=0.8116', '\\n', '  acc -> train: 0.9375, val: 0.7500', '\\n', 'Epoch\n10/10: train_loss=0.4996, val_loss=0.6038', '\\n', '  acc -> train: 1.0000, val:\n0.7500', '\\n', '\\n=== Training with batch size = 32 ===', '\\n', 'Epoch 1/10:\ntrain_loss=0.9387, val_loss=1.0692', '\\n', '  acc -> train: 0.4375, val:\n0.5000', '\\n', 'Epoch 2/10: train_loss=0.9623, val_loss=0.8395', '\\n', '  acc ->\ntrain: 0.4375, val: 0.5000', '\\n', 'Epoch 3/10: train_loss=0.9303,\nval_loss=0.9062', '\\n', '  acc -> train: 0.4375, val: 0.5000', '\\n', 'Epoch\n4/10: train_loss=0.8973, val_loss=0.7671', '\\n', '  acc -> train: 0.4375, val:\n0.5000', '\\n', 'Epoch 5/10: train_loss=0.9141, val_loss=0.7592', '\\n', '  acc ->\ntrain: 0.5000, val: 0.5000', '\\n', 'Epoch 6/10: train_loss=0.7834,\nval_loss=0.7983', '\\n', '  acc -> train: 0.5000, val: 0.5000', '\\n', 'Epoch\n7/10: train_loss=0.7888, val_loss=0.7271', '\\n', '  acc -> train: 0.5625, val:\n0.5000', '\\n', 'Epoch 8/10: train_loss=0.7383, val_loss=0.7565', '\\n', '  acc ->\ntrain: 0.8125, val: 0.5000', '\\n', 'Epoch 9/10: train_loss=0.6755,\nval_loss=0.6634', '\\n', '  acc -> train: 0.8750, val: 0.5000', '\\n', 'Epoch\n10/10: train_loss=0.5431, val_loss=0.7879', '\\n', '  acc -> train: 1.0000, val:\n0.5000', '\\n', '\\nSaved experiment data to:', ' ', '/data/chenhui/AI-Scientist-\nv2/experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/0-\nrun/process_ForkProcess-7/working/experiment_data.npy', '\\n', 'Execution time: 2\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time:\n3 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'lr=1.0e-04 ep=0 val_loss=1.1939 val_acc=0.5000',\n'\\n', 'lr=1.0e-04 ep=1 val_loss=1.1107 val_acc=0.5000', '\\n', 'lr=1.0e-04 ep=2\nval_loss=1.0529 val_acc=0.5000', '\\n', 'lr=1.0e-04 ep=3 val_loss=0.9470\nval_acc=0.5000', '\\n', 'lr=1.0e-04 ep=4 val_loss=1.0956 val_acc=0.5000', '\\n',\n'lr=1.0e-04 ep=5 val_loss=0.8670 val_acc=0.5000', '\\n', 'lr=1.0e-04 ep=6\nval_loss=0.9553 val_acc=0.5000', '\\n', 'lr=1.0e-04 ep=7 val_loss=1.1264\nval_acc=0.5000', '\\n', 'lr=1.0e-04 ep=8 val_loss=0.9887 val_acc=0.5000', '\\n',\n'lr=1.0e-04 ep=9 val_loss=1.1217 val_acc=0.5000', '\\n', 'lr=5.0e-04 ep=0\nval_loss=0.7979 val_acc=0.5000', '\\n', 'lr=5.0e-04 ep=1 val_loss=1.0059\nval_acc=0.5000', '\\n', 'lr=5.0e-04 ep=2 val_loss=1.0603 val_acc=0.5000', '\\n',\n'lr=5.0e-04 ep=3 val_loss=1.0549 val_acc=0.5000', '\\n', 'lr=5.0e-04 ep=4\nval_loss=1.0408 val_acc=0.5000', '\\n', 'lr=5.0e-04 ep=5 val_loss=0.8246\nval_acc=0.5000', '\\n', 'lr=5.0e-04 ep=6 val_loss=0.9876 val_acc=0.5000', '\\n',\n'lr=5.0e-04 ep=7 val_loss=1.2458 val_acc=0.5000', '\\n', 'lr=5.0e-04 ep=8\nval_loss=0.7046 val_acc=0.5000', '\\n', 'lr=5.0e-04 ep=9 val_loss=1.2857\nval_acc=0.5000', '\\n', 'lr=1.0e-03 ep=0 val_loss=1.1405 val_acc=0.5000', '\\n',\n'lr=1.0e-03 ep=1 val_loss=1.2074 val_acc=0.5000', '\\n', 'lr=1.0e-03 ep=2\nval_loss=1.0614 val_acc=0.5000', '\\n', 'lr=1.0e-03 ep=3 val_loss=1.1688\nval_acc=0.5000', '\\n', 'lr=1.0e-03 ep=4 val_loss=1.1132 val_acc=0.5000', '\\n',\n'lr=1.0e-03 ep=5 val_loss=1.0471 val_acc=0.7500', '\\n', 'lr=1.0e-03 ep=6\nval_loss=1.2007 val_acc=0.7500', '\\n', 'lr=1.0e-03 ep=7 val_loss=1.0003\nval_acc=0.7500', '\\n', 'lr=1.0e-03 ep=8 val_loss=1.1478 val_acc=0.7500', '\\n',\n'lr=1.0e-03 ep=9 val_loss=0.6893 val_acc=1.0000', '\\n', 'lr=5.0e-03 ep=0\nval_loss=0.8269 val_acc=0.5000', '\\n', 'lr=5.0e-03 ep=1 val_loss=1.2515\nval_acc=1.0000', '\\n', 'lr=5.0e-03 ep=2 val_loss=0.7941 val_acc=1.0000', '\\n',\n'lr=5.0e-03 ep=3 val_loss=0.4362 val_acc=1.0000', '\\n', 'lr=5.0e-03 ep=4\nval_loss=0.3760 val_acc=1.0000', '\\n', 'lr=5.0e-03 ep=5 val_loss=0.5811\nval_acc=1.0000', '\\n', 'lr=5.0e-03 ep=6 val_loss=0.6989 val_acc=1.0000', '\\n',\n'lr=5.0e-03 ep=7 val_loss=0.8403 val_acc=1.0000', '\\n', 'lr=5.0e-03 ep=8\nval_loss=0.7285 val_acc=1.0000', '\\n', 'lr=5.0e-03 ep=9 val_loss=0.2500\nval_acc=1.0000', '\\n', 'lr=1.0e-02 ep=0 val_loss=0.6501 val_acc=0.7500', '\\n',\n'lr=1.0e-02 ep=1 val_loss=0.5000 val_acc=1.0000', '\\n', 'lr=1.0e-02 ep=2\nval_loss=0.8367 val_acc=1.0000', '\\n', 'lr=1.0e-02 ep=3 val_loss=0.3516\nval_acc=1.0000', '\\n', 'lr=1.0e-02 ep=4 val_loss=0.5731 val_acc=1.0000', '\\n',\n'lr=1.0e-02 ep=5 val_loss=0.2500 val_acc=1.0000', '\\n', 'lr=1.0e-02 ep=6\nval_loss=0.8830 val_acc=1.0000', '\\n', 'lr=1.0e-02 ep=7 val_loss=0.5682\nval_acc=1.0000', '\\n', 'lr=1.0e-02 ep=8 val_loss=0.2500 val_acc=1.0000', '\\n',\n'lr=1.0e-02 ep=9 val_loss=0.8123 val_acc=1.0000', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 3 seconds seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'lr=1.0e-04 bs=8 ep=0 val_loss=0.8277\nval_acc=0.5000 ctec=0.7001', '\\n', 'lr=1.0e-04 bs=8 ep=1 val_loss=0.9268\nval_acc=0.5000 ctec=0.7050', '\\n', 'lr=1.0e-04 bs=8 ep=2 val_loss=0.8437\nval_acc=0.5000 ctec=0.7100', '\\n', 'lr=1.0e-04 bs=8 ep=3 val_loss=0.7762\nval_acc=0.5000 ctec=0.7146', '\\n', 'lr=1.0e-04 bs=8 ep=4 val_loss=0.9405\nval_acc=0.5000 ctec=0.7190', '\\n', 'lr=1.0e-04 bs=8 ep=5 val_loss=0.9336\nval_acc=0.5000 ctec=0.7234', '\\n', 'lr=1.0e-04 bs=8 ep=6 val_loss=0.9945\nval_acc=0.5000 ctec=0.7276', '\\n', 'lr=1.0e-04 bs=8 ep=7 val_loss=0.8911\nval_acc=0.5000 ctec=0.7317', '\\n', 'lr=1.0e-04 bs=8 ep=8 val_loss=0.9099\nval_acc=0.5000 ctec=0.7358', '\\n', 'lr=1.0e-04 bs=8 ep=9 val_loss=0.9594\nval_acc=0.5000 ctec=0.7400', '\\n', 'lr=1.0e-04 bs=8 ep=10 val_loss=0.9144\nval_acc=0.5000 ctec=0.7442', '\\n', 'lr=1.0e-04 bs=8 ep=11 val_loss=0.8075\nval_acc=0.5000 ctec=0.7482', '\\n', 'lr=1.0e-04 bs=8 ep=12 val_loss=0.8148\nval_acc=0.5000 ctec=0.7523', '\\n', 'lr=1.0e-04 bs=8 ep=13 val_loss=1.0343\nval_acc=0.5000 ctec=0.7565', '\\n', 'lr=1.0e-04 bs=8 ep=14 val_loss=0.9119\nval_acc=0.5000 ctec=0.7607', '\\n', 'lr=5.0e-04 bs=8 ep=0 val_loss=0.9593\nval_acc=0.2500 ctec=0.6437', '\\n', 'lr=5.0e-04 bs=8 ep=1 val_loss=1.0683\nval_acc=0.2500 ctec=0.6718', '\\n', 'lr=5.0e-04 bs=8 ep=2 val_loss=1.0496\nval_acc=0.2500 ctec=0.7012', '\\n', 'lr=5.0e-04 bs=8 ep=3 val_loss=0.6409\nval_acc=0.5000 ctec=0.7300', '\\n', 'lr=5.0e-04 bs=8 ep=4 val_loss=0.7729\nval_acc=0.5000 ctec=0.7555', '\\n', 'lr=5.0e-04 bs=8 ep=5 val_loss=0.9681\nval_acc=0.5000 ctec=0.7793', '\\n', 'lr=5.0e-04 bs=8 ep=6 val_loss=0.8592\nval_acc=0.5000 ctec=0.8008', '\\n', 'lr=5.0e-04 bs=8 ep=7 val_loss=0.7541\nval_acc=0.5000 ctec=0.8208', '\\n', 'lr=5.0e-04 bs=8 ep=8 val_loss=0.5066\nval_acc=0.5000 ctec=0.8391', '\\n', 'lr=5.0e-04 bs=8 ep=9 val_loss=0.8312\nval_acc=0.5000 ctec=0.8558', '\\n', 'lr=5.0e-04 bs=8 ep=10 val_loss=0.4938\nval_acc=0.5000 ctec=0.8700', '\\n', 'lr=5.0e-04 bs=8 ep=11 val_loss=0.8185\nval_acc=0.5000 ctec=0.8826', '\\n', 'lr=5.0e-04 bs=8 ep=12 val_loss=0.6780\nval_acc=0.7500 ctec=0.8942', '\\n', 'lr=5.0e-04 bs=8 ep=13 val_loss=0.5580\nval_acc=0.7500 ctec=0.9049', '\\n', 'lr=5.0e-04 bs=8 ep=14 val_loss=0.7376\nval_acc=0.7500 ctec=0.9149', '\\n', 'lr=1.0e-03 bs=8 ep=0 val_loss=0.8326\nval_acc=0.5000 ctec=0.8099', '\\n', 'lr=1.0e-03 bs=8 ep=1 val_loss=0.7262\nval_acc=0.5000 ctec=0.8501', '\\n', 'lr=1.0e-03 bs=8 ep=2 val_loss=0.7923\nval_acc=0.5000 ctec=0.8765', '\\n', 'lr=1.0e-03 bs=8 ep=3 val_loss=0.7064\nval_acc=0.5000 ctec=0.8968', '\\n', 'lr=1.0e-03 bs=8 ep=4 val_loss=0.4300\nval_acc=0.5000 ctec=0.9135', '\\n', 'lr=1.0e-03 bs=8 ep=5 val_loss=0.7197\nval_acc=0.7500 ctec=0.9276', '\\n', 'lr=1.0e-03 bs=8 ep=6 val_loss=0.6897\nval_acc=1.0000 ctec=0.9402', '\\n', 'lr=1.0e-03 bs=8 ep=7 val_loss=0.6721\nval_acc=1.0000 ctec=0.9505', '\\n', 'lr=1.0e-03 bs=8 ep=8 val_loss=0.3675\nval_acc=1.0000 ctec=0.9586', '\\n', 'lr=1.0e-03 bs=8 ep=9 val_loss=0.3400\nval_acc=1.0000 ctec=0.9655', '\\n', 'lr=1.0e-03 bs=8 ep=10 val_loss=0.1094\nval_acc=1.0000 ctec=0.9709', '\\n', 'lr=1.0e-03 bs=8 ep=11 val_loss=0.1173\nval_acc=1.0000 ctec=0.9751', '\\n', 'lr=1.0e-03 bs=8 ep=12 val_loss=0.1007\nval_acc=1.0000 ctec=0.9788', '\\n', 'lr=1.0e-03 bs=8 ep=13 val_loss=0.0508\nval_acc=1.0000 ctec=0.9811', '\\n', 'lr=1.0e-03 bs=8 ep=14 val_loss=0.1694\nval_acc=1.0000 ctec=0.9825', '\\n', 'lr=1.0e-04 bs=16 ep=0 val_loss=0.7580\nval_acc=0.0000 ctec=0.5560', '\\n', 'lr=1.0e-04 bs=16 ep=1 val_loss=1.1663\nval_acc=0.0000 ctec=0.5597', '\\n', 'lr=1.0e-04 bs=16 ep=2 val_loss=0.8573\nval_acc=0.0000 ctec=0.5637', '\\n', 'lr=1.0e-04 bs=16 ep=3 val_loss=1.1333\nval_acc=0.2500 ctec=0.5676', '\\n', 'lr=1.0e-04 bs=16 ep=4 val_loss=1.0421\nval_acc=0.5000 ctec=0.5714', '\\n', 'lr=1.0e-04 bs=16 ep=5 val_loss=1.0428\nval_acc=0.5000 ctec=0.5752', '\\n', 'lr=1.0e-04 bs=16 ep=6 val_loss=0.9978\nval_acc=0.5000 ctec=0.5789', '\\n', 'lr=1.0e-04 bs=16 ep=7 val_loss=0.8885\nval_acc=0.5000 ctec=0.5827', '\\n', 'lr=1.0e-04 bs=16 ep=8 val_loss=0.7574\nval_acc=0.5000 ctec=0.5864', '\\n', 'lr=1.0e-04 bs=16 ep=9 val_loss=0.8472\nval_acc=0.5000 ctec=0.5900', '\\n', 'lr=1.0e-04 bs=16 ep=10 val_loss=0.9066\nval_acc=0.5000 ctec=0.5937', '\\n', 'lr=1.0e-04 bs=16 ep=11 val_loss=0.8588\nval_acc=0.5000 ctec=0.5974', '\\n', 'lr=1.0e-04 bs=16 ep=12 val_loss=0.9551\nval_acc=0.5000 ctec=0.6010', '\\n', 'lr=1.0e-04 bs=16 ep=13 val_loss=0.7577\nval_acc=0.5000 ctec=0.6046', '\\n', 'lr=1.0e-04 bs=16 ep=14 val_loss=1.0661\nval_acc=0.5000 ctec=0.6082', '\\n', 'lr=5.0e-04 bs=16 ep=0 val_loss=1.0009\nval_acc=0.5000 ctec=0.7610', '\\n', 'lr=5.0e-04 bs=16 ep=1 val_loss=0.9676\nval_acc=0.5000 ctec=0.7711', '\\n', 'lr=5.0e-04 bs=16 ep=2 val_loss=0.7270\nval_acc=0.5000 ctec=0.7811', '\\n', 'lr=5.0e-04 bs=16 ep=3 val_loss=0.8926\nval_acc=0.5000 ctec=0.7912', '\\n', 'lr=5.0e-04 bs=16 ep=4 val_loss=0.8783\nval_acc=0.5000 ctec=0.8006', '\\n', 'lr=5.0e-04 bs=16 ep=5 val_loss=1.0030\nval_acc=0.5000 ctec=0.8095', '\\n', 'lr=5.0e-04 bs=16 ep=6 val_loss=0.7883\nval_acc=0.5000 ctec=0.8182', '\\n', 'lr=5.0e-04 bs=16 ep=7 val_loss=0.5948\nval_acc=0.5000 ctec=0.8266', '\\n', 'lr=5.0e-04 bs=16 ep=8 val_loss=0.9832\nval_acc=0.5000 ctec=0.8348', '\\n', 'lr=5.0e-04 bs=16 ep=9 val_loss=0.7567\nval_acc=0.5000 ctec=0.8424', '\\n', 'lr=5.0e-04 bs=16 ep=10 val_loss=0.6445\nval_acc=0.5000 ctec=0.8495', '\\n', 'lr=5.0e-04 bs=16 ep=11 val_loss=0.7152\nval_acc=0.5000 ctec=0.8565', '\\n', 'lr=5.0e-04 bs=16 ep=12 val_loss=0.7893\nval_acc=0.5000 ctec=0.8630', '\\n', 'lr=5.0e-04 bs=16 ep=13 val_loss=0.8316\nval_acc=0.5000 ctec=0.8692', '\\n', 'lr=5.0e-04 bs=16 ep=14 val_loss=0.8716\nval_acc=0.5000 ctec=0.8752', '\\n', 'lr=1.0e-03 bs=16 ep=0 val_loss=1.0805\nval_acc=0.5000 ctec=0.5554', '\\n', 'lr=1.0e-03 bs=16 ep=1 val_loss=0.9643\nval_acc=0.5000 ctec=0.5991', '\\n', 'lr=1.0e-03 bs=16 ep=2 val_loss=0.7985\nval_acc=0.5000 ctec=0.6420', '\\n', 'lr=1.0e-03 bs=16 ep=3 val_loss=0.7325\nval_acc=0.5000 ctec=0.6835', '\\n', 'lr=1.0e-03 bs=16 ep=4 val_loss=0.8462\nval_acc=0.5000 ctec=0.7218', '\\n', 'lr=1.0e-03 bs=16 ep=5 val_loss=0.7351\nval_acc=0.5000 ctec=0.7557', '\\n', 'lr=1.0e-03 bs=16 ep=6 val_loss=0.8331\nval_acc=0.5000 ctec=0.7867', '\\n', 'lr=1.0e-03 bs=16 ep=7 val_loss=0.6941\nval_acc=0.5000 ctec=0.8133', '\\n', 'lr=1.0e-03 bs=16 ep=8 val_loss=0.8505\nval_acc=0.5000 ctec=0.8355', '\\n', 'lr=1.0e-03 bs=16 ep=9 val_loss=0.5716\nval_acc=1.0000 ctec=0.8543', '\\n', 'lr=1.0e-03 bs=16 ep=10 val_loss=0.7496\nval_acc=1.0000 ctec=0.8703', '\\n', 'lr=1.0e-03 bs=16 ep=11 val_loss=0.7136\nval_acc=1.0000 ctec=0.8827', '\\n', 'lr=1.0e-03 bs=16 ep=12 val_loss=0.4806\nval_acc=1.0000 ctec=0.8937', '\\n', 'lr=1.0e-03 bs=16 ep=13 val_loss=0.5829\nval_acc=1.0000 ctec=0.9042', '\\n', 'lr=1.0e-03 bs=16 ep=14 val_loss=0.6592\nval_acc=1.0000 ctec=0.9136', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 190, in <module>\\n\nload_dataset(\"code_x_glue_cc_clone_detection\", cfg, split=\"test\")\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1652, in dataset_module_factory\\n    raise e1\nfrom None\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1578, in dataset_module_factory\\n    raise\nDatasetNotFoundError(f\"Dataset \\'{path}\\' doesn\\'t exist on the Hub or cannot be\naccessed.\") from e\\ndatasets.exceptions.DatasetNotFoundError: Dataset\n\\'code_x_glue_cc_clone_detection\\' doesn\\'t exist on the Hub or cannot be\naccessed.\\n', 'Execution time: 4 seconds seconds (time limit is an hour).']", "['Embed_dim=32 | final val acc=0.7500', '\\n', 'Embed_dim=64 | final val\nacc=0.7500', '\\n', 'Embed_dim=128 | final val acc=0.7500', '\\n', 'Embed_dim=256\n| final val acc=0.7500', '\\n', 'Execution time: 2 seconds seconds (time limit is\nan hour).']", "['Using device: cuda', '\\n', '\\n=== Training with LSTM dropout = 0.1 ===', '\\n',\n'Dropout 0.1 Epoch 1/10 train_loss=0.9315, val_loss=0.9265, val_acc=0.2500',\n'\\n', 'Dropout 0.1 Epoch 2/10 train_loss=0.8874, val_loss=0.8105,\nval_acc=1.0000', '\\n', 'Dropout 0.1 Epoch 3/10 train_loss=0.8489,\nval_loss=0.7906, val_acc=1.0000', '\\n', 'Dropout 0.1 Epoch 4/10\ntrain_loss=0.8028, val_loss=0.7657, val_acc=1.0000', '\\n', 'Dropout 0.1 Epoch\n5/10 train_loss=0.7515, val_loss=0.6469, val_acc=1.0000', '\\n', 'Dropout 0.1\nEpoch 6/10 train_loss=0.6855, val_loss=0.4120, val_acc=1.0000', '\\n', 'Dropout\n0.1 Epoch 7/10 train_loss=0.5409, val_loss=0.4498, val_acc=1.0000', '\\n',\n'Dropout 0.1 Epoch 8/10 train_loss=0.4792, val_loss=0.1568, val_acc=0.7500',\n'\\n', 'Dropout 0.1 Epoch 9/10 train_loss=0.3359, val_loss=0.7374,\nval_acc=0.5000', '\\n', 'Dropout 0.1 Epoch 10/10 train_loss=0.3688,\nval_loss=0.2889, val_acc=0.5000', '\\n', '\\n=== Training with LSTM dropout = 0.3\n===', '\\n', 'Dropout 0.3 Epoch 1/10 train_loss=0.9691, val_loss=0.9386,\nval_acc=0.5000', '\\n', 'Dropout 0.3 Epoch 2/10 train_loss=0.9432,\nval_loss=0.8912, val_acc=0.7500', '\\n', 'Dropout 0.3 Epoch 3/10\ntrain_loss=0.9401, val_loss=0.8821, val_acc=0.7500', '\\n', 'Dropout 0.3 Epoch\n4/10 train_loss=0.9058, val_loss=0.9314, val_acc=0.7500', '\\n', 'Dropout 0.3\nEpoch 5/10 train_loss=0.8387, val_loss=0.8646, val_acc=1.0000', '\\n', 'Dropout\n0.3 Epoch 6/10 train_loss=0.8471, val_loss=0.6940, val_acc=1.0000', '\\n',\n'Dropout 0.3 Epoch 7/10 train_loss=0.7824, val_loss=0.6223, val_acc=1.0000',\n'\\n', 'Dropout 0.3 Epoch 8/10 train_loss=0.6961, val_loss=0.6311,\nval_acc=1.0000', '\\n', 'Dropout 0.3 Epoch 9/10 train_loss=0.6260,\nval_loss=0.4613, val_acc=1.0000', '\\n', 'Dropout 0.3 Epoch 10/10\ntrain_loss=0.5033, val_loss=0.1825, val_acc=1.0000', '\\n', '\\n=== Training with\nLSTM dropout = 0.5 ===', '\\n', 'Dropout 0.5 Epoch 1/10 train_loss=0.9547,\nval_loss=0.8550, val_acc=0.7500', '\\n', 'Dropout 0.5 Epoch 2/10\ntrain_loss=0.9631, val_loss=0.8705, val_acc=0.7500', '\\n', 'Dropout 0.5 Epoch\n3/10 train_loss=0.9048, val_loss=0.9123, val_acc=1.0000', '\\n', 'Dropout 0.5\nEpoch 4/10 train_loss=0.8905, val_loss=0.8433, val_acc=1.0000', '\\n', 'Dropout\n0.5 Epoch 5/10 train_loss=0.9025, val_loss=0.7689, val_acc=1.0000', '\\n',\n'Dropout 0.5 Epoch 6/10 train_loss=0.8902, val_loss=0.7976, val_acc=1.0000',\n'\\n', 'Dropout 0.5 Epoch 7/10 train_loss=0.8175, val_loss=0.7475,\nval_acc=1.0000', '\\n', 'Dropout 0.5 Epoch 8/10 train_loss=0.8111,\nval_loss=0.6772, val_acc=1.0000', '\\n', 'Dropout 0.5 Epoch 9/10\ntrain_loss=0.7077, val_loss=0.6452, val_acc=1.0000', '\\n', 'Dropout 0.5 Epoch\n10/10 train_loss=0.7647, val_loss=0.5913, val_acc=1.0000', '\\n',\n'\\nHyperparameter tuning complete. Data saved to experiment_data.npy', '\\n',\n'Execution time: 2 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n--- Training with weight_decay = 0 ---', '\\n',\n'WD 0 Epoch 0: train_loss=0.8682, val_loss=1.0856, train_acc=0.5000,\nval_acc=0.2500', '\\n', 'WD 0 Epoch 1: train_loss=0.8266, val_loss=1.0073,\ntrain_acc=0.5000, val_acc=0.2500', '\\n', 'WD 0 Epoch 2: train_loss=0.7911,\nval_loss=0.9785, train_acc=0.5000, val_acc=0.2500', '\\n', 'WD 0 Epoch 3:\ntrain_loss=0.7120, val_loss=0.9229, train_acc=0.6250, val_acc=0.5000', '\\n', 'WD\n0 Epoch 4: train_loss=0.6650, val_loss=0.8559, train_acc=0.7500,\nval_acc=0.5000', '\\n', 'WD 0 Epoch 5: train_loss=0.5861, val_loss=0.8470,\ntrain_acc=0.8750, val_acc=0.5000', '\\n', 'WD 0 Epoch 6: train_loss=0.4864,\nval_loss=0.4496, train_acc=0.9375, val_acc=0.5000', '\\n', 'WD 0 Epoch 7:\ntrain_loss=0.3538, val_loss=0.6824, train_acc=1.0000, val_acc=0.7500', '\\n', 'WD\n0 Epoch 8: train_loss=0.2626, val_loss=0.4960, train_acc=1.0000,\nval_acc=0.7500', '\\n', 'WD 0 Epoch 9: train_loss=0.1176, val_loss=0.4806,\ntrain_acc=1.0000, val_acc=0.7500', '\\n', '\\n--- Training with weight_decay =\n1e-05 ---', '\\n', 'WD 1e-05 Epoch 0: train_loss=0.9178, val_loss=0.9577,\ntrain_acc=0.5000, val_acc=0.2500', '\\n', 'WD 1e-05 Epoch 1: train_loss=0.9156,\nval_loss=0.8959, train_acc=0.5000, val_acc=0.2500', '\\n', 'WD 1e-05 Epoch 2:\ntrain_loss=0.8080, val_loss=0.8031, train_acc=0.5625, val_acc=0.5000', '\\n', 'WD\n1e-05 Epoch 3: train_loss=0.6953, val_loss=0.7836, train_acc=0.7500,\nval_acc=0.5000', '\\n', 'WD 1e-05 Epoch 4: train_loss=0.7195, val_loss=0.5829,\ntrain_acc=0.9375, val_acc=0.7500', '\\n', 'WD 1e-05 Epoch 5: train_loss=0.5674,\nval_loss=0.6844, train_acc=1.0000, val_acc=0.7500', '\\n', 'WD 1e-05 Epoch 6:\ntrain_loss=0.4851, val_loss=0.6063, train_acc=1.0000, val_acc=1.0000', '\\n', 'WD\n1e-05 Epoch 7: train_loss=0.4528, val_loss=0.6609, train_acc=1.0000,\nval_acc=1.0000', '\\n', 'WD 1e-05 Epoch 8: train_loss=0.2896, val_loss=0.4240,\ntrain_acc=1.0000, val_acc=1.0000', '\\n', 'WD 1e-05 Epoch 9: train_loss=0.2386,\nval_loss=0.1815, train_acc=1.0000, val_acc=1.0000', '\\n', '\\n--- Training with\nweight_decay = 0.0001 ---', '\\n', 'WD 0.0001 Epoch 0: train_loss=0.9315,\nval_loss=1.1355, train_acc=0.5000, val_acc=0.2500', '\\n', 'WD 0.0001 Epoch 1:\ntrain_loss=0.8928, val_loss=1.1516, train_acc=0.5000, val_acc=0.2500', '\\n', 'WD\n0.0001 Epoch 2: train_loss=0.9186, val_loss=1.0991, train_acc=0.5000,\nval_acc=0.2500', '\\n', 'WD 0.0001 Epoch 3: train_loss=0.7728, val_loss=1.0543,\ntrain_acc=0.6250, val_acc=0.2500', '\\n', 'WD 0.0001 Epoch 4: train_loss=0.7173,\nval_loss=0.8755, train_acc=1.0000, val_acc=0.2500', '\\n', 'WD 0.0001 Epoch 5:\ntrain_loss=0.5855, val_loss=0.9326, train_acc=1.0000, val_acc=0.2500', '\\n', 'WD\n0.0001 Epoch 6: train_loss=0.5630, val_loss=0.8559, train_acc=1.0000,\nval_acc=0.5000', '\\n', 'WD 0.0001 Epoch 7: train_loss=0.3675, val_loss=0.7818,\ntrain_acc=1.0000, val_acc=0.5000', '\\n', 'WD 0.0001 Epoch 8: train_loss=0.3429,\nval_loss=0.8621, train_acc=1.0000, val_acc=0.5000', '\\n', 'WD 0.0001 Epoch 9:\ntrain_loss=0.2445, val_loss=0.5442, train_acc=1.0000, val_acc=0.5000', '\\n',\n'\\n--- Training with weight_decay = 0.001 ---', '\\n', 'WD 0.001 Epoch 0:\ntrain_loss=1.0119, val_loss=1.0609, train_acc=0.5000, val_acc=0.5000', '\\n', 'WD\n0.001 Epoch 1: train_loss=0.8937, val_loss=0.9789, train_acc=0.5000,\nval_acc=0.5000', '\\n', 'WD 0.001 Epoch 2: train_loss=0.8336, val_loss=0.8802,\ntrain_acc=0.5000, val_acc=0.5000', '\\n', 'WD 0.001 Epoch 3: train_loss=0.7586,\nval_loss=0.8098, train_acc=0.5000, val_acc=0.5000', '\\n', 'WD 0.001 Epoch 4:\ntrain_loss=0.7276, val_loss=0.9033, train_acc=0.6250, val_acc=0.5000', '\\n', 'WD\n0.001 Epoch 5: train_loss=0.6019, val_loss=0.7604, train_acc=0.8750,\nval_acc=0.7500', '\\n', 'WD 0.001 Epoch 6: train_loss=0.6134, val_loss=0.7788,\ntrain_acc=1.0000, val_acc=0.7500', '\\n', 'WD 0.001 Epoch 7: train_loss=0.4901,\nval_loss=0.7470, train_acc=1.0000, val_acc=0.7500', '\\n', 'WD 0.001 Epoch 8:\ntrain_loss=0.4510, val_loss=0.6070, train_acc=1.0000, val_acc=1.0000', '\\n', 'WD\n0.001 Epoch 9: train_loss=0.2965, val_loss=0.5921, train_acc=1.0000,\nval_acc=1.0000', '\\n', '\\n--- Training with weight_decay = 0.01 ---', '\\n', 'WD\n0.01 Epoch 0: train_loss=0.9659, val_loss=1.0599, train_acc=0.5000,\nval_acc=0.2500', '\\n', 'WD 0.01 Epoch 1: train_loss=0.8860, val_loss=1.0273,\ntrain_acc=0.5000, val_acc=0.2500', '\\n', 'WD 0.01 Epoch 2: train_loss=0.8351,\nval_loss=0.9407, train_acc=0.5625, val_acc=0.5000', '\\n', 'WD 0.01 Epoch 3:\ntrain_loss=0.7835, val_loss=0.9471, train_acc=0.6250, val_acc=0.5000', '\\n', 'WD\n0.01 Epoch 4: train_loss=0.7496, val_loss=0.8552, train_acc=0.6875,\nval_acc=0.5000', '\\n', 'WD 0.01 Epoch 5: train_loss=0.7122, val_loss=0.8490,\ntrain_acc=0.8750, val_acc=0.7500', '\\n', 'WD 0.01 Epoch 6: train_loss=0.6228,\nval_loss=0.8246, train_acc=0.8750, val_acc=0.7500', '\\n', 'WD 0.01 Epoch 7:\ntrain_loss=0.5685, val_loss=0.7416, train_acc=0.8750, val_acc=0.7500', '\\n', 'WD\n0.01 Epoch 8: train_loss=0.4972, val_loss=0.7393, train_acc=0.8750,\nval_acc=0.7500', '\\n', 'WD 0.01 Epoch 9: train_loss=0.4641, val_loss=0.5831,\ntrain_acc=1.0000, val_acc=0.7500', '\\n', '\\nSaved experiment_data.npy with\nweight_decay results.', '\\n', 'Execution time: 3 seconds seconds (time limit is\nan hour).']", "['Using device: cuda', '\\n', 'Pool 1 | Epoch 0 | val_loss 0.9482 | train_acc\n0.5000 | val_acc 0.2500', '\\n', 'Pool 1 | Epoch 1 | val_loss 0.9323 | train_acc\n0.5000 | val_acc 0.2500', '\\n', 'Pool 1 | Epoch 2 | val_loss 0.9798 | train_acc\n0.5000 | val_acc 0.2500', '\\n', 'Pool 1 | Epoch 3 | val_loss 0.8201 | train_acc\n0.6250 | val_acc 0.2500', '\\n', 'Pool 1 | Epoch 4 | val_loss 0.8158 | train_acc\n0.9375 | val_acc 0.2500', '\\n', 'Pool 1 | Epoch 5 | val_loss 0.7680 | train_acc\n0.9375 | val_acc 0.7500', '\\n', 'Pool 1 | Epoch 6 | val_loss 0.5994 | train_acc\n1.0000 | val_acc 0.7500', '\\n', 'Pool 1 | Epoch 7 | val_loss 0.6801 | train_acc\n1.0000 | val_acc 0.7500', '\\n', 'Pool 1 | Epoch 8 | val_loss 0.4698 | train_acc\n1.0000 | val_acc 0.7500', '\\n', 'Pool 1 | Epoch 9 | val_loss 0.3034 | train_acc\n1.0000 | val_acc 0.7500', '\\n', 'Pool 2 | Epoch 0 | val_loss 1.1246 | train_acc\n0.5000 | val_acc 0.2500', '\\n', 'Pool 2 | Epoch 1 | val_loss 1.1172 | train_acc\n0.5000 | val_acc 0.2500', '\\n', 'Pool 2 | Epoch 2 | val_loss 0.9249 | train_acc\n0.5625 | val_acc 0.5000', '\\n', 'Pool 2 | Epoch 3 | val_loss 0.9630 | train_acc\n0.8750 | val_acc 0.5000', '\\n', 'Pool 2 | Epoch 4 | val_loss 0.9113 | train_acc\n1.0000 | val_acc 0.5000', '\\n', 'Pool 2 | Epoch 5 | val_loss 0.7696 | train_acc\n1.0000 | val_acc 0.5000', '\\n', 'Pool 2 | Epoch 6 | val_loss 0.7240 | train_acc\n1.0000 | val_acc 1.0000', '\\n', 'Pool 2 | Epoch 7 | val_loss 0.3305 | train_acc\n1.0000 | val_acc 1.0000', '\\n', 'Pool 2 | Epoch 8 | val_loss 0.4820 | train_acc\n1.0000 | val_acc 1.0000', '\\n', 'Pool 2 | Epoch 9 | val_loss 0.4166 | train_acc\n1.0000 | val_acc 1.0000', '\\n', 'Pool 5 | Epoch 0 | val_loss 1.2378 | train_acc\n0.5000 | val_acc 0.2500', '\\n', 'Pool 5 | Epoch 1 | val_loss 1.1895 | train_acc\n0.5000 | val_acc 0.2500', '\\n', 'Pool 5 | Epoch 2 | val_loss 1.0527 | train_acc\n0.5625 | val_acc 0.2500', '\\n', 'Pool 5 | Epoch 3 | val_loss 1.0396 | train_acc\n0.8750 | val_acc 0.2500', '\\n', 'Pool 5 | Epoch 4 | val_loss 0.9809 | train_acc\n1.0000 | val_acc 0.5000', '\\n', 'Pool 5 | Epoch 5 | val_loss 0.9252 | train_acc\n1.0000 | val_acc 0.5000', '\\n', 'Pool 5 | Epoch 6 | val_loss 0.9229 | train_acc\n1.0000 | val_acc 0.5000', '\\n', 'Pool 5 | Epoch 7 | val_loss 0.7693 | train_acc\n1.0000 | val_acc 0.7500', '\\n', 'Pool 5 | Epoch 8 | val_loss 0.7221 | train_acc\n1.0000 | val_acc 0.7500', '\\n', 'Pool 5 | Epoch 9 | val_loss 0.5927 | train_acc\n1.0000 | val_acc 0.7500', '\\n', 'Pool 10 | Epoch 0 | val_loss 1.1545 | train_acc\n0.5000 | val_acc 0.5000', '\\n', 'Pool 10 | Epoch 1 | val_loss 1.1042 | train_acc\n0.5000 | val_acc 0.5000', '\\n', 'Pool 10 | Epoch 2 | val_loss 1.0547 | train_acc\n0.5000 | val_acc 0.5000', '\\n', 'Pool 10 | Epoch 3 | val_loss 1.0079 | train_acc\n0.7500 | val_acc 0.5000', '\\n', 'Pool 10 | Epoch 4 | val_loss 0.9541 | train_acc\n0.9375 | val_acc 0.5000', '\\n', 'Pool 10 | Epoch 5 | val_loss 0.9392 | train_acc\n1.0000 | val_acc 0.7500', '\\n', 'Pool 10 | Epoch 6 | val_loss 0.8900 | train_acc\n1.0000 | val_acc 0.7500', '\\n', 'Pool 10 | Epoch 7 | val_loss 0.8414 | train_acc\n1.0000 | val_acc 1.0000', '\\n', 'Pool 10 | Epoch 8 | val_loss 0.7705 | train_acc\n1.0000 | val_acc 1.0000', '\\n', 'Pool 10 | Epoch 9 | val_loss 0.6821 | train_acc\n1.0000 | val_acc 1.0000', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution\ntime: 2 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Training with 1 LSTM layer(s) ===', '\\n',\n'Layer 1 Epoch 0: train_loss=0.8593, val_loss=1.0267', '\\n', 'Layer 1 Epoch 1:\ntrain_loss=0.8862, val_loss=0.9879', '\\n', 'Layer 1 Epoch 2: train_loss=0.7211,\nval_loss=0.9845', '\\n', 'Layer 1 Epoch 3: train_loss=0.5698, val_loss=0.8823',\n'\\n', 'Layer 1 Epoch 4: train_loss=0.5925, val_loss=0.8719', '\\n', 'Layer 1\nEpoch 5: train_loss=0.6618, val_loss=0.7869', '\\n', 'Layer 1 Epoch 6:\ntrain_loss=0.4801, val_loss=0.6790', '\\n', 'Layer 1 Epoch 7: train_loss=0.3276,\nval_loss=0.5344', '\\n', 'Layer 1 Epoch 8: train_loss=0.3415, val_loss=0.4827',\n'\\n', 'Layer 1 Epoch 9: train_loss=0.1354, val_loss=0.5408', '\\n', '\\n===\nTraining with 2 LSTM layer(s) ===', '\\n', 'Layer 2 Epoch 0: train_loss=0.9374,\nval_loss=0.9176', '\\n', 'Layer 2 Epoch 1: train_loss=0.8890, val_loss=0.8891',\n'\\n', 'Layer 2 Epoch 2: train_loss=0.8196, val_loss=0.8349', '\\n', 'Layer 2\nEpoch 3: train_loss=0.7916, val_loss=0.7672', '\\n', 'Layer 2 Epoch 4:\ntrain_loss=0.7221, val_loss=0.8028', '\\n', 'Layer 2 Epoch 5: train_loss=0.6815,\nval_loss=0.5542', '\\n', 'Layer 2 Epoch 6: train_loss=0.5856, val_loss=0.5480',\n'\\n', 'Layer 2 Epoch 7: train_loss=0.4978, val_loss=0.5541', '\\n', 'Layer 2\nEpoch 8: train_loss=0.3650, val_loss=0.2037', '\\n', 'Layer 2 Epoch 9:\ntrain_loss=0.3229, val_loss=0.0849', '\\n', '\\n=== Training with 3 LSTM layer(s)\n===', '\\n', 'Layer 3 Epoch 0: train_loss=0.9733, val_loss=0.9678', '\\n', 'Layer\n3 Epoch 1: train_loss=0.9545, val_loss=0.9536', '\\n', 'Layer 3 Epoch 2:\ntrain_loss=0.9306, val_loss=0.9623', '\\n', 'Layer 3 Epoch 3: train_loss=0.9062,\nval_loss=0.9056', '\\n', 'Layer 3 Epoch 4: train_loss=0.8621, val_loss=0.8930',\n'\\n', 'Layer 3 Epoch 5: train_loss=0.8384, val_loss=0.8901', '\\n', 'Layer 3\nEpoch 6: train_loss=0.8125, val_loss=0.8170', '\\n', 'Layer 3 Epoch 7:\ntrain_loss=0.7644, val_loss=0.8146', '\\n', 'Layer 3 Epoch 8: train_loss=0.6985,\nval_loss=0.8257', '\\n', 'Layer 3 Epoch 9: train_loss=0.5719, val_loss=0.7202',\n'\\n', '\\nSaved experiment_data.npy with results for lstm_num_layers =', ' ',\n'[1, 2, 3]', '\\n', 'Execution time: 2 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'bidirectional=False epoch=0 val_loss=0.9474\nval_acc=0.5000', '\\n', 'bidirectional=False epoch=1 val_loss=0.5846\nval_acc=0.5000', '\\n', 'bidirectional=False epoch=2 val_loss=0.7915\nval_acc=0.5000', '\\n', 'bidirectional=False epoch=3 val_loss=0.6438\nval_acc=0.5000', '\\n', 'bidirectional=False epoch=4 val_loss=0.6687\nval_acc=0.5000', '\\n', 'bidirectional=False epoch=5 val_loss=0.6491\nval_acc=0.7500', '\\n', 'bidirectional=False epoch=6 val_loss=0.4976\nval_acc=1.0000', '\\n', 'bidirectional=False epoch=7 val_loss=0.6073\nval_acc=1.0000', '\\n', 'bidirectional=False epoch=8 val_loss=0.4626\nval_acc=1.0000', '\\n', 'bidirectional=False epoch=9 val_loss=0.4970\nval_acc=1.0000', '\\n', 'bidirectional=True epoch=0 val_loss=0.9477\nval_acc=0.2500', '\\n', 'bidirectional=True epoch=1 val_loss=0.9466\nval_acc=0.5000', '\\n', 'bidirectional=True epoch=2 val_loss=1.0758\nval_acc=0.5000', '\\n', 'bidirectional=True epoch=3 val_loss=0.8893\nval_acc=0.5000', '\\n', 'bidirectional=True epoch=4 val_loss=0.8709\nval_acc=0.5000', '\\n', 'bidirectional=True epoch=5 val_loss=0.9487\nval_acc=0.5000', '\\n', 'bidirectional=True epoch=6 val_loss=0.8429\nval_acc=0.5000', '\\n', 'bidirectional=True epoch=7 val_loss=0.8097\nval_acc=0.5000', '\\n', 'bidirectional=True epoch=8 val_loss=0.9252\nval_acc=0.5000', '\\n', 'bidirectional=True epoch=9 val_loss=0.6593\nval_acc=0.5000', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 2\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished EPOCHS=10: final val_acc=1.0000', '\\n',\n'Finished EPOCHS=30: final val_acc=1.0000', '\\n', 'Finished EPOCHS=50: final\nval_acc=1.0000', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 3\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished EPOCHS=10: final val_acc=1.0000', '\\n',\n'Finished EPOCHS=30: final val_acc=1.0000', '\\n', 'Finished EPOCHS=50: final\nval_acc=1.0000', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 4\nseconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Finished EPOCHS=10: final val_acc=1.0000', '\\n',\n'Finished EPOCHS=30: final val_acc=1.0000', '\\n', 'Finished EPOCHS=50: final\nval_acc=1.0000', '\\n', 'Saved experiment_data.npy', '\\n', 'Execution time: 4\nseconds seconds (time limit is an hour).']", ""], "analysis": ["", "", "The script executes without errors but does not fulfill the sub-stage\nrequirement of introducing two new HuggingFace datasets for testing. It only\nuses the synthetic data and never loads or evaluates on any HuggingFace\ndatasets. Proposed fix: Load two appropriate HuggingFace code datasets (e.g.,\n'code_search_net' Python subset for clone detection and\n'code_x_glue_ct_code_to_text' for summarization) using the `datasets` library.\nPreprocess these datasets into the same triplet format (anchor, positive,\nnegative), create DataLoaders for them, and incorporate their evaluation\nalongside the synthetic data in the training loop.", "", "", "", "The script crashes at the Hugging Face evaluation step because it tries to load\na non\u2010existent dataset 'code_x_glue_cc_clone_detection'. To fix this, use the\ncorrect dataset identifier (e.g., 'code_x_glue_cc_clone' or prefix with\n'microsoft/') so that load_dataset can successfully retrieve the dataset.", "", "", "", "", "The implementation did not include two HuggingFace datasets as required by the\nsub-stage; it only uses synthetic data for experiments. To fix this, integrate\nat least two real code datasets from HuggingFace (e.g., CodeSearchNet and\nCodeXGLUE tasks) by loading them via the `datasets` library, generate dynamic\ntraces for those code samples, and add corresponding DataLoaders. Additionally,\nprint the computed retrieval accuracy metrics during training to monitor\nperformance more transparently.", "", "", "The code executed without errors and achieved perfect validation accuracy (1.0)\nacross all epoch settings (10, 30, 50) on the synthetic dataset. Experiment\noutputs (experiment_data.npy) were saved successfully. No runtime bugs were\ndetected.", "", ""], "exc_type": [null, null, null, null, null, null, "DatasetNotFoundError", null, null, null, null, null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null, {"args": ["Dataset 'code_x_glue_cc_clone_detection' doesn't exist on the Hub or cannot be accessed."]}, null, null, null, null, null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null, [["/data/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 190, "<module>", "load_dataset(\"code_x_glue_cc_clone_detection\", cfg, split=\"test\")"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1652, "dataset_module_factory", "raise e1 from None"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1578, "dataset_module_factory", "raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e"]], null, null, null, null, null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "test accuracy", "lower_is_better": false, "description": "Test accuracy on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training loss on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.1601, "best_value": 0.1601}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss on the synthetic dataset", "data": [{"dataset_name": "synthetic", "final_value": 0.2121, "best_value": 0.2121}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training set", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "accuracy", "lower_is_better": false, "description": "Classification accuracy", "data": [{"dataset_name": "train", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "validation", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "loss", "lower_is_better": true, "description": "Cross-entropy loss", "data": [{"dataset_name": "train", "final_value": 0.0255, "best_value": 0.0}, {"dataset_name": "validation", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training dataset", "data": [{"dataset_name": "synthetic (batch size 4)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (batch size 16)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (batch size 32)", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation dataset", "data": [{"dataset_name": "synthetic (batch size 4)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (batch size 16)", "final_value": 0.75, "best_value": 0.75}, {"dataset_name": "synthetic (batch size 32)", "final_value": 0.5, "best_value": 0.5}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training dataset", "data": [{"dataset_name": "synthetic (batch size 4)", "final_value": 0.0139, "best_value": 0.0139}, {"dataset_name": "synthetic (batch size 16)", "final_value": 0.4996, "best_value": 0.4996}, {"dataset_name": "synthetic (batch size 32)", "final_value": 0.5431, "best_value": 0.5431}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation dataset", "data": [{"dataset_name": "synthetic (batch size 4)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic (batch size 16)", "final_value": 0.6038, "best_value": 0.6038}, {"dataset_name": "synthetic (batch size 32)", "final_value": 0.7879, "best_value": 0.7879}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training set", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.75, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "synthetic", "final_value": 1.2626, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 1.3805, "best_value": 0.0032}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Proportion of correct predictions on the in-sample dataset.", "data": [{"dataset_name": "In-sample", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the in-sample dataset.", "data": [{"dataset_name": "In-sample", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Proportion of correct predictions on the out-of-sample dataset.", "data": [{"dataset_name": "Out-of-sample", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the out-of-sample dataset.", "data": [{"dataset_name": "Out-of-sample", "final_value": 0.8123, "best_value": 0.25}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the training set", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.75, "best_value": 0.75}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.0387, "best_value": 0.0387}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training set accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation set accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training set loss", "data": [{"dataset_name": "synthetic", "final_value": 0.3688, "best_value": 0.3688}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation set loss", "data": [{"dataset_name": "synthetic", "final_value": 0.1825, "best_value": 0.1825}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "synthetic", "final_value": 0.4641, "best_value": 0.4641}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "synthetic", "final_value": 0.5831, "best_value": 0.5831}]}, {"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "synthetic", "final_value": 0.75, "best_value": 0.75}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training set classification accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation set classification accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Training set loss", "data": [{"dataset_name": "synthetic", "final_value": 0.1419, "best_value": 0.1419}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation set loss", "data": [{"dataset_name": "synthetic", "final_value": 0.3034, "best_value": 0.3034}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy", "data": [{"dataset_name": "synthetic", "final_value": 0.9375, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 1.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on training set", "data": [{"dataset_name": "synthetic (bidirectional=False)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (bidirectional=True)", "final_value": 0.8125, "best_value": 0.8125}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on validation set", "data": [{"dataset_name": "synthetic (bidirectional=False)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic (bidirectional=True)", "final_value": 0.5, "best_value": 0.5}]}, {"metric_name": "train loss", "lower_is_better": true, "description": "Loss on training set", "data": [{"dataset_name": "synthetic (bidirectional=False)", "final_value": 0.3032, "best_value": 0.3032}, {"dataset_name": "synthetic (bidirectional=True)", "final_value": 0.6463, "best_value": 0.6463}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on validation set", "data": [{"dataset_name": "synthetic (bidirectional=False)", "final_value": 0.497, "best_value": 0.497}, {"dataset_name": "synthetic (bidirectional=True)", "final_value": 0.6593, "best_value": 0.6593}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Training accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Validation accuracy", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "synthetic", "final_value": 0.0208, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy of the model on the training set", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy of the model on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss of the model on the training set", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss of the model on the validation set", "data": [{"dataset_name": "synthetic", "final_value": 0.0208, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train accuracy", "lower_is_better": false, "description": "Accuracy on the synthetic dataset during training", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "validation accuracy", "lower_is_better": false, "description": "Accuracy on the synthetic dataset during validation", "data": [{"dataset_name": "synthetic", "final_value": 1.0, "best_value": 1.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the synthetic dataset during training", "data": [{"dataset_name": "synthetic", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the synthetic dataset during validation", "data": [{"dataset_name": "synthetic", "final_value": 0.0208, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_confusion_matrix.png", "../../logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_loss_curve.png"], ["../../logs/0-run/experiment_results/experiment_52c025f14d09471881bea2a75607be65_proc_385044/synthetic_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_52c025f14d09471881bea2a75607be65_proc_385044/synthetic_loss_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_fbe88c9058a04005be5c2c7d64c1b3c6_proc_385046/synthetic_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_fbe88c9058a04005be5c2c7d64c1b3c6_proc_385046/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_219001b288c34fbdb49fed084d7879cb_proc_385045/triplet_margin_sweep_synthetic_accuracy.png", "../../logs/0-run/experiment_results/experiment_219001b288c34fbdb49fed084d7879cb_proc_385045/triplet_margin_sweep_synthetic_losses.png"], ["../../logs/0-run/experiment_results/experiment_9ba2f17c50f740b1b377075dc3ef57b8_proc_385046/code_dataset_loss_curves.png", "../../logs/0-run/experiment_results/experiment_9ba2f17c50f740b1b377075dc3ef57b8_proc_385046/code_dataset_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_9ba2f17c50f740b1b377075dc3ef57b8_proc_385046/code_dataset_final_val_accuracy.png"], [], ["../../logs/0-run/experiment_results/experiment_6470a51e4d59451b907d85605f56e4f4_proc_385044/synthetic_final_val_acc_vs_embed_dim.png", "../../logs/0-run/experiment_results/experiment_6470a51e4d59451b907d85605f56e4f4_proc_385044/synthetic_accuracy_vs_epochs.png", "../../logs/0-run/experiment_results/experiment_6470a51e4d59451b907d85605f56e4f4_proc_385044/synthetic_loss_vs_epochs.png"], ["../../logs/0-run/experiment_results/experiment_9c5ffbd6b7ee44f685bb3b311260816f_proc_385045/synthetic_accuracy_curves_lstm_dropout.png", "../../logs/0-run/experiment_results/experiment_9c5ffbd6b7ee44f685bb3b311260816f_proc_385045/synthetic_loss_curves_lstm_dropout.png", "../../logs/0-run/experiment_results/experiment_9c5ffbd6b7ee44f685bb3b311260816f_proc_385045/synthetic_final_val_accuracy_lstm_dropout.png"], ["../../logs/0-run/experiment_results/experiment_55a7dc89d453480c8f3243b06b131b1e_proc_385046/synthetic_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_55a7dc89d453480c8f3243b06b131b1e_proc_385046/synthetic_final_val_acc.png", "../../logs/0-run/experiment_results/experiment_55a7dc89d453480c8f3243b06b131b1e_proc_385046/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_10ec16f50e464b7795916e1bc62c4866_proc_385044/synthetic_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_10ec16f50e464b7795916e1bc62c4866_proc_385044/synthetic_loss_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_b96d6deaae18406cb106d8355de1ea36_proc_385046/lstm_bidirectional_synthetic_accuracy_curve.png", "../../logs/0-run/experiment_results/experiment_b96d6deaae18406cb106d8355de1ea36_proc_385046/lstm_bidirectional_synthetic_loss_curve.png", "../../logs/0-run/experiment_results/experiment_b96d6deaae18406cb106d8355de1ea36_proc_385046/lstm_bidirectional_synthetic_final_val_accuracy.png"], ["../../logs/0-run/experiment_results/experiment_c5b491aafe1947f39e9c2308fe06a617_proc_385045/synthetic_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_c5b491aafe1947f39e9c2308fe06a617_proc_385045/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_1b2ea6531bed4bfb8a350a13214f6289_proc_385046/synthetic_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_1b2ea6531bed4bfb8a350a13214f6289_proc_385046/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_0175b29beba44f91b947647cb2b6688f_proc_385044/synthetic_accuracy_curves.png", "../../logs/0-run/experiment_results/experiment_0175b29beba44f91b947647cb2b6688f_proc_385044/synthetic_loss_curves.png"], ["../../logs/0-run/experiment_results/seed_aggregation_d1e0f81b04594f3b81989d223f7108f9/synthetic_mean_sem_accuracy.png", "../../logs/0-run/experiment_results/seed_aggregation_d1e0f81b04594f3b81989d223f7108f9/synthetic_mean_sem_loss.png"]], "plot_paths": [["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_accuracy_curve.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_confusion_matrix.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_loss_curve.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_52c025f14d09471881bea2a75607be65_proc_385044/synthetic_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_52c025f14d09471881bea2a75607be65_proc_385044/synthetic_loss_curves.png"], [], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_fbe88c9058a04005be5c2c7d64c1b3c6_proc_385046/synthetic_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_fbe88c9058a04005be5c2c7d64c1b3c6_proc_385046/synthetic_loss_curves.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_219001b288c34fbdb49fed084d7879cb_proc_385045/triplet_margin_sweep_synthetic_accuracy.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_219001b288c34fbdb49fed084d7879cb_proc_385045/triplet_margin_sweep_synthetic_losses.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_9ba2f17c50f740b1b377075dc3ef57b8_proc_385046/code_dataset_loss_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_9ba2f17c50f740b1b377075dc3ef57b8_proc_385046/code_dataset_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_9ba2f17c50f740b1b377075dc3ef57b8_proc_385046/code_dataset_final_val_accuracy.png"], [], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_6470a51e4d59451b907d85605f56e4f4_proc_385044/synthetic_final_val_acc_vs_embed_dim.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_6470a51e4d59451b907d85605f56e4f4_proc_385044/synthetic_accuracy_vs_epochs.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_6470a51e4d59451b907d85605f56e4f4_proc_385044/synthetic_loss_vs_epochs.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_9c5ffbd6b7ee44f685bb3b311260816f_proc_385045/synthetic_accuracy_curves_lstm_dropout.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_9c5ffbd6b7ee44f685bb3b311260816f_proc_385045/synthetic_loss_curves_lstm_dropout.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_9c5ffbd6b7ee44f685bb3b311260816f_proc_385045/synthetic_final_val_accuracy_lstm_dropout.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_55a7dc89d453480c8f3243b06b131b1e_proc_385046/synthetic_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_55a7dc89d453480c8f3243b06b131b1e_proc_385046/synthetic_final_val_acc.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_55a7dc89d453480c8f3243b06b131b1e_proc_385046/synthetic_loss_curves.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_10ec16f50e464b7795916e1bc62c4866_proc_385044/synthetic_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_10ec16f50e464b7795916e1bc62c4866_proc_385044/synthetic_loss_curves.png"], [], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_b96d6deaae18406cb106d8355de1ea36_proc_385046/lstm_bidirectional_synthetic_accuracy_curve.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_b96d6deaae18406cb106d8355de1ea36_proc_385046/lstm_bidirectional_synthetic_loss_curve.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_b96d6deaae18406cb106d8355de1ea36_proc_385046/lstm_bidirectional_synthetic_final_val_accuracy.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_c5b491aafe1947f39e9c2308fe06a617_proc_385045/synthetic_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_c5b491aafe1947f39e9c2308fe06a617_proc_385045/synthetic_loss_curves.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_1b2ea6531bed4bfb8a350a13214f6289_proc_385046/synthetic_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_1b2ea6531bed4bfb8a350a13214f6289_proc_385046/synthetic_loss_curves.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_0175b29beba44f91b947647cb2b6688f_proc_385044/synthetic_accuracy_curves.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_0175b29beba44f91b947647cb2b6688f_proc_385044/synthetic_loss_curves.png"], ["experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/seed_aggregation_d1e0f81b04594f3b81989d223f7108f9/synthetic_mean_sem_accuracy.png", "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/seed_aggregation_d1e0f81b04594f3b81989d223f7108f9/synthetic_mean_sem_loss.png"]], "plot_analyses": [[{"analysis": "Train accuracy starts below chance and steadily climbs to perfect (1.0) by epoch 7. Validation accuracy, however, remains at chance (0.5) until epoch 7, then abruptly jumps to perfect by epoch 8. Such a sudden leap suggests an issue in the evaluation pipeline (e.g., data leakage or mislabeled validation batches) rather than genuine learning of runtime semantics. The plateau at chance for seven epochs indicates the model wasn\u2019t capturing useful patterns until an unexpected event\u2014likely code or data handling bug\u2014triggered perfect scores.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_accuracy_curve.png"}, {"analysis": "The confusion matrix on the synthetic dataset shows nonzero counts only for True Group IDs 6 and 7 (two correct predictions for class 6, one for class 7) with all other classes absent. This implies the model only ever outputs two of the eight possible classes, effectively ignoring six classes completely. The mismatch between the observed perfect validation accuracy and this highly skewed confusion matrix highlights an evaluation or indexing error\u2014labels or predictions are likely being remapped or filtered incorrectly.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_confusion_matrix.png"}, {"analysis": "Loss curves decrease smoothly from around 0.95 (train) and 0.93 (val) to below 0.2. A minor bump around epoch 5 suggests transient optimization instability, but overall the loss drop is consistent with overfitting on a tiny synthetic set. Combined with the accuracy and confusion results, the loss dynamics further point to correct gradient flow but flawed validation logic causing misleading metrics.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_efc9317a06fc4724955f705527cd8f70_proc_382493/synthetic_loss_curve.png"}], [{"analysis": "Plot 1 shows that all three epoch budgets (10, 30, 50) eventually reach perfect training accuracy, but their validation curves tell different stories. With only 10 epochs, validation peaks around 65\u201370%, indicating underfitting on the held\u2010out set before training halts. Allowing 30 epochs pushes validation up to about 75%, but it plateaus quickly\u2014suggesting the model learns faster but still fails to fully generalize. At 50 epochs, training accuracy climbs more gradually, and validation remains stuck at 50% until roughly epoch 12, when it suddenly jumps to near 100%. This abrupt transition hints at a late learning regime change (e.g., a schedule step or warm\u2010up ending) rather than smooth generalization improvements.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_52c025f14d09471881bea2a75607be65_proc_385044/synthetic_accuracy_curves.png"}, {"analysis": "Plot 2 confirms that training loss falls swiftly to near zero in all settings, but validation loss behaves differently across epoch choices. For 10 epochs, validation loss drops smoothly to around 0.1 and then stops. At 30 epochs, loss also falls fast but shows small oscillations around zero toward the end, reflecting some overfitting. Under the 50\u2010epoch run, validation loss oscillates more noticeably even after hitting near\u2010zero, indicating sensitivity to noise and potential overtraining. The sharper spikes correspond temporally to the delayed accuracy jump seen in the first plot, reinforcing the idea of a late training schedule event.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_52c025f14d09471881bea2a75607be65_proc_385044/synthetic_loss_curves.png"}], [], [{"analysis": "Synthetic dataset accuracy curves show that smaller batch sizes converge much faster. For batch size 4, both training and validation accuracy climb sharply: training accuracy surpasses 0.85 by epoch 3 and reaches 1.0 by epoch 5, with validation matching at epoch 5 and remaining at 1.0 thereafter. Batch size 16 exhibits a steadier train accuracy rise, crossing 0.80 around epoch 7 and hitting 1.0 by epoch 10; validation accuracy stays at 0.50 until epoch 5, then jumps to 1.0 and holds. Batch size 32 displays the slowest progress: training accuracy only exceeds 0.80 at epoch 9 and reaches 1.0 at epoch 10, while validation accuracy remains stuck at 0.50 until a brief rise to 0.75 at epoch 10. This suggests large batches underutilize the signal in early epochs, causing delayed convergence and underfitting on validation early on.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_fbe88c9058a04005be5c2c7d64c1b3c6_proc_385046/synthetic_accuracy_curves.png"}, {"analysis": "Loss curves complement these findings. With batch size 4, training loss falls steeply from ~0.90 to near 0.0 by epoch 10; validation loss follows the downward trend with minor fluctuations, hitting ~0.05 by the end. Batch size 16 shows moderate loss reduction: training loss declines from ~0.95 to 0.50 over ten epochs, and validation loss oscillates between 0.75 and 0.80, indicating some instability before epoch 5 and a steady decrease thereafter. Batch size 32 experiences the slowest loss descent: training loss moves from ~0.95 to ~0.55 by epoch 10, while validation loss remains around 0.80 with only small dips. The high and flat loss for larger batches confirms underfitting and suggests that gradient estimates are too coarse with bs=32.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_fbe88c9058a04005be5c2c7d64c1b3c6_proc_385046/synthetic_loss_curves.png"}], [{"analysis": "Margins of 0.5 and 1.0 yield the fastest convergence in retrieval accuracy. For m=0.5, training accuracy jumps from 0.5 to near 1.0 by epoch 5, with validation closely following by epoch 6. The m=1.0 sweep similarly climbs quickly, hitting 0.94 training and 0.88 validation by epoch 6. Very small margin (m=0.2) saturates early at 0.75 with minimal gap between train/val but fails to reach top performance. Large margins (m=1.5, 2.0) delay both train and val accuracy gains until epochs 5\u20137, and even at the end plateau below optimal levels. This suggests a sweet spot around 0.5\u20131.0 for this synthetic retrieval task.", "order": 1, "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_219001b288c34fbdb49fed084d7879cb_proc_385045/triplet_margin_sweep_synthetic_accuracy.png"}, {"analysis": "Loss curves mirror accuracy trends: smallest margin (m=0.2) yields quickest loss reduction to near zero by epoch 5 but plateaus with suboptimal retrieval. m=0.5 and m=1.0 show steady decline in both train and validation losses, with validation loss dropping to ~0.05 by epoch 10. Oversized margins (m=1.5, 2.0) sustain higher losses (>0.5) deep into training, reflecting difficulty in pushing dissimilar pairs apart when margin is too large. Validation losses for these higher margins also fluctuate more, indicating less stable convergence.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_219001b288c34fbdb49fed084d7879cb_proc_385045/triplet_margin_sweep_synthetic_losses.png"}], [{"analysis": "Training loss curves show that very high learning rates (0.005 and 0.01) drive loss rapidly toward zero within a few epochs, but validation loss for these settings oscillates heavily and spikes, indicating severe overfitting and instability. The mid\u2010range rate (0.001) yields a steady decline in training loss without collapse and its validation loss gently decreases and plateaus, suggesting a better balance between learning speed and generalization. Lower rates (0.0005 and 0.0001) produce slow or negligible decreases in training loss, and their validation losses remain high and flat, implying underfitting and insufficient learning at those scales.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_9ba2f17c50f740b1b377075dc3ef57b8_proc_385046/code_dataset_loss_curves.png"}, {"analysis": "Training accuracy curves reveal that the very high rates achieve perfect accuracy almost immediately, again hinting at over\u2010capacity and memorization. The 0.001 rate climbs gradually to full accuracy by epoch 5, reflecting consistent learning. Rates 0.0005 and 0.0001 fail to reach high training accuracy by the end of the schedule, confirming that they are too low to capture semantic structures efficiently. On validation, high rates also hit 100% but in light of their unstable loss behavior, this likely reflects over\u2010fitting to the validation set. The 0.001 rate steadily improves validation accuracy mid\u2010training and reaches 100% by the final epoch, aligned with its smoother validation loss trajectory. Very low rates remain stuck at random\u2010guess levels.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_9ba2f17c50f740b1b377075dc3ef57b8_proc_385046/code_dataset_accuracy_curves.png"}, {"analysis": "Bar chart of final validation accuracy confirms that learning rates of 0.001 and above achieve perfect validation accuracy, whereas 0.0005 and 0.0001 are stuck at chance (50%). Given the earlier loss\u2010stability observations, 0.001 emerges as the strongest candidate: it converges quickly, generalizes reliably, and avoids the erratic loss spikes seen at higher rates.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_9ba2f17c50f740b1b377075dc3ef57b8_proc_385046/code_dataset_final_val_accuracy.png"}], [], [{"analysis": "Validation accuracy increases with embedding dimension up to a point: 32 dims achieve roughly 0.70, 64 dims ~0.72, 128 dims peak around 0.75, and 256 dims slightly below (around 0.74). This suggests diminishing returns beyond 128 dimensions, with a small drop-off or plateau at 256 dims that may indicate overparameterization without further generalization gains.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_6470a51e4d59451b907d85605f56e4f4_proc_385044/synthetic_final_val_acc_vs_embed_dim.png"}, {"analysis": "Training retrieval accuracy rises faster as embedding dimension grows: 256-dim model reaches perfect retrieval by epoch 4, 128-dim by epoch 5, 64-dim by epoch 7, and 32-dim only by epoch 10. Validation retrieval for all models climbs to ~0.75 and then plateaus\u201432-dim and 64-dim reach 0.75 around epochs 6 and 5 respectively, while 128-dim and 256-dim hit 0.75 by epochs 4 and 3. Beyond that point, no further improvement is seen, indicating that larger embeddings speed convergence but do not push validation retrieval above ~0.75 under the current setup.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_6470a51e4d59451b907d85605f56e4f4_proc_385044/synthetic_accuracy_vs_epochs.png"}, {"analysis": "Triplet loss curves confirm faster convergence and lower final loss for larger embeddings. On the training side, the 256-dim model\u2019s loss drops from ~0.75 to near 0 by epoch 8, followed by 128-dim (to ~0.1) and 64-dim (~0.35), with 32-dim still at ~0.55 by epoch 10. Validation loss trends mirror training: 256-dim and 128-dim yield the lowest final losses (~0.05 and ~0.1), whereas 64-dim and 32-dim remain higher (~0.35 and ~0.8). This indicates that higher-dimensional embeddings better fit the contrastive objective and generalize more effectively on the synthetic data.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_6470a51e4d59451b907d85605f56e4f4_proc_385044/synthetic_loss_vs_epochs.png"}], [], [{"analysis": "Accuracy curves show that moderate weight decay values (1e-5 and 0.001) yield the fastest convergence and highest validation accuracy. With weight decay = 1e-5, validation climbs from 0.5 at epoch 1 to 0.95 by epoch 5 and reaches 1.0 by epoch 7. Weight decay = 0.001 follows a similar trend, hitting 1.0 on validation around epoch 7\u20138. Zero weight decay improves training steadily but plateaus validation at 0.5 throughout, indicating overfitting. Very small weight decay (0.0001) barely moves training or validation above 0.6, suggesting under-regularization or poor optimization. The largest weight decay (0.01) slows both train and val curves, topping out around 0.9 train and 0.75 validation by epoch 10, hinting at underfitting from too strong regularization.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_55a7dc89d453480c8f3243b06b131b1e_proc_385046/synthetic_accuracy_curves.png"}, {"analysis": "Bar chart of final validation accuracy reveals a U-shaped relationship: extreme low (0) and extreme high (0.01) weight decay both yield 0.75, the worst moderate setting (0.0001) gives only 0.50, whereas mid-range settings (1e-5 and 0.001) both achieve perfect 1.00. This confirms an optimal regularization sweet spot around 1e-5 to 1e-3.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_55a7dc89d453480c8f3243b06b131b1e_proc_385046/synthetic_final_val_acc.png"}, {"analysis": "Loss curves confirm the above: for weight decay = 1e-5 and 0.001, training and validation losses drop rapidly\u2014validation loss falls below 0.5 by epoch 6 and continues downward, indicating good generalization. Zero weight decay yields the lowest training loss (below 0.2 by epoch 10) but validation loss stalls above 0.6, showing overfitting. Weight decay = 0.0001 leads to slow loss decrease and validation loss never drops below 0.85. The strongest decay (0.01) achieves moderate training loss (\u22480.45 by epoch 10) but validation loss remains high (\u22480.75), corroborating underfitting.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_55a7dc89d453480c8f3243b06b131b1e_proc_385046/synthetic_loss_curves.png"}], [], [], [{"analysis": "For the synthetic retrieval task, unidirectional (bidirectional=False) training accuracy climbs from 50% to 100% by epoch five, and its validation accuracy follows closely, hitting 100% by epoch six. In contrast, the bidirectional=True variant only moves training accuracy from ~30% to ~81% by epoch nine and keeps validation stuck at 50%, indicating it fails to learn or generalize under current settings.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_b96d6deaae18406cb106d8355de1ea36_proc_385046/lstm_bidirectional_synthetic_accuracy_curve.png"}, {"analysis": "Unidirectional loss steadily falls from about 0.92 to 0.30 on train and from 0.95 to 0.49 on validation, showing smooth, stable convergence. The bidirectional model starts higher (~1.07), decreases more slowly to ~0.65, and exhibits larger fluctuations\u2014signaling optimization difficulties and possibly ill-suited hyperparameters for its more complex encoder.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_b96d6deaae18406cb106d8355de1ea36_proc_385046/lstm_bidirectional_synthetic_loss_curve.png"}, {"analysis": "The final bar chart underscores the stark gap: bidirectional=False achieves perfect retrieval accuracy (1.0) on validation, while bidirectional=True remains at 0.5. This confirms that unidirectional pre-training is far more effective on this synthetic task under the current hyperparameter configuration.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_b96d6deaae18406cb106d8355de1ea36_proc_385046/lstm_bidirectional_synthetic_final_val_accuracy.png"}], [{"analysis": "On the Synthetic Dataset retrieval accuracy curves, across all three hyperparameter settings (E=10, 30, 50):\n- Training accuracy reaches near-perfect (1.0) by about epoch 8 for E=10 and within 10\u201312 epochs for E=30 and E=50.\n- Validation accuracy closely follows training, indicating minimal overfitting.  E=30 shows a temporary dip around epoch 7 to ~0.75 before snapping to 1.0 by epoch 10; E=50 dips to ~0.50 at epoch 7, recovers to ~0.75 at epoch 8, then reaches 1.0 by epoch 12.  These transient fluctuations suggest some sensitivity to noisy examples or learning rate schedule but eventual convergence is strong.\n- Extending beyond ~15 epochs yields no further gains in accuracy, as all curves saturate at 1.0 by epoch ~12.\n\nOn the Synthetic Dataset loss curves:\n- All training loss curves (E=10, E=30, E=50) decline steeply from ~1.0 at epoch 1 to near 0.0 by epoch 10\u201312, consistent with fast convergence on the synthetic task.\n- Validation loss mirrors the training trend but exhibits noisier behavior\u2014especially for E=50 past epoch 10\u2014indicating occasional gradient-induced fluctuations, yet it still approaches zero by epoch ~15.\n- No evidence of divergence or sustained overfitting; both training and validation losses stabilize near zero.\n\nOverall, the model under all tested hyperparameter regimes quickly learns the synthetic retrieval task with high fidelity and generalization. The transient validation dips for larger epoch budgets hint at potential noise sensitivity or learning rate decay misalignment, suggesting room to tune the learning-rate schedule or introduce regularization when moving to more complex real-world code datasets.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_c5b491aafe1947f39e9c2308fe06a617_proc_385045/synthetic_accuracy_curves.png"}], [{"analysis": "Synthetic Dataset Train vs Validation Retrieval Accuracy curves show that all three hyperparameter settings eventually reach perfect retrieval accuracy. The setting with 30 epochs yields the fastest validation accuracy spike, reaching near 100% around epoch 6. The 10-epoch configuration plateaus earlier at about 75% on validation, indicating underfitting with insufficient training time. The 50-epoch run converges more slowly on validation, suggesting that its learning rate or batch size may be too conservative, delaying generalization despite achieving final accuracy parity.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_1b2ea6531bed4bfb8a350a13214f6289_proc_385046/synthetic_accuracy_curves.png"}, {"analysis": "Synthetic Dataset Train vs Validation Loss curves illustrate that training losses for all settings drop rapidly to zero by epoch ~12. Validation loss for the 30-epoch setting decreases the fastest, stabilizing at zero by epoch ~13. The 10-epoch configuration\u2019s validation loss plateaus around 0.2 by epoch 5 before gradually decreasing, reflecting limited training capacity. The 50-epoch run exhibits higher variance in validation loss and only reaches zero around epoch 45, confirming slower convergence under that hyperparameter choice.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_1b2ea6531bed4bfb8a350a13214f6289_proc_385046/synthetic_loss_curves.png"}], [{"analysis": "Training and validation retrieval accuracy climbs steeply, reaching near-perfect performance by around epoch 10 across all three epoch budgets. The E=10 run achieves 100% accuracy on both splits slightly earlier than E=30 and E=50, which both converge by epoch 12. The E=30 validation curve shows a brief dip at epoch 8 before recovering, and E=50 exhibits a small plateau between epochs 8 and 12, suggesting some instability when training beyond 10 epochs without further regularization.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_0175b29beba44f91b947647cb2b6688f_proc_385044/synthetic_accuracy_curves.png"}, {"analysis": "Loss curves for training and validation all descend rapidly from an initial value above 1.0 to near zero by epoch 15. The E=10 setting cleanly reaches minimal loss around epoch 12 with very smooth validation behavior. The E=30 and E=50 settings also converge by epoch 15 but display more fluctuation in validation loss, particularly spikes for E=50 between epochs 10\u201320. This indicates that longer training schedules offer no added benefit in reducing loss and may introduce overfitting or noisy validation estimates.", "plot_path": "experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_0175b29beba44f91b947647cb2b6688f_proc_385044/synthetic_loss_curves.png"}], []], "vlm_feedback_summary": ["Results show steady training loss reduction and perfect final train/val scores,\nbut validation accuracy remains at chance for many epochs and then spikes,\ncoupled with a confusion matrix that only covers two of eight classes. This\nstrongly indicates bugs in the evaluation, label mapping, or data splitting\nrather than true model performance. Recommend auditing the data loader and\nevaluation code for misindexed labels, ensuring no train\u2013val leakage, and\nrerunning with a properly shuffled synthetic dataset.", "Validation performance under short runs (10 epochs) suffers from underfitting,\nwhile medium runs (30 epochs) strike a better balance but still plateau early.\nVery long runs (50 epochs) introduce unstable behavior until a late jump in\nmetrics, suggesting a learning rate or warm\u2010up schedule impact. To improve,\nconsider fine\u2010tuning the learning\u2010rate schedule and early stopping, and add\nregularization (dropout or weight decay). Proposed additional test datasets from\nHuggingFace: \"mbpp\" for Python problem generalization and\n\"code_x_glue_cc_code_clone_detection\" for adversarial clone detection.", "[]", "Smaller batch sizes (bs=4) yield rapid convergence and strong generalization on\nthe synthetic task, while larger batches slow both train and validation progress\nand risk underfitting. To improve performance at larger batch sizes, consider\nincreasing the learning rate, using a learning\u00ad-rate warm\u00adup or cyclic\nscheduler, or increasing the number of epochs. Applying weight decay or gradient\naccumulation could help stabilize larger\u2010batch training. Additionally, to\nfurther test the model in realistic code tasks, incorporate HuggingFace datasets\nsuch as \u2018code_x_glue_cc_defect_detection\u2019 for vulnerability classification and\n\u2018code_x_glue_cc_code_to_text\u2019 for code summarization.", "Optimal triplet margin lies between 0.5 and 1.0: it balances rapid convergence,\nstable validation performance, and lowest loss. Small margin underfits\nsemantically distinct pairs; large margins over-constrain training and slow down\nlearning. Next steps: focus tuning around m=0.5\u20131.0, potentially refining batch\ncomposition or mining strategy. Introduce two HuggingFace benchmarks for broader\nevaluation: \u201cmicrosoft/CodeXGLUE-python-clone-detection\u201d for code clone\nretrieval robustness, and \u201cmicrosoft/CodeXGLUE-code-to-docstring\u201d for transfer\nin code summarization under limited data.", "Learning rate 0.001 provides the optimal trade\u2010off between fast convergence and\nstable generalization. Rates above 0.001 lead to overfitting and loss\ninstability, while rates below 0.001 underfit and learn too slowly.", "[]", "Larger embedding dimensions accelerate training and yield lower triplet loss,\nbut validation retrieval saturates around 0.75 across dimensions. The 128-dim\nmodel offers the best trade-off between capacity and generalization on these\nsynthetic tasks. Consider 128 as a default for further tuning.", "[]", "Mid-range weight decay (1e-5, 0.001) produces optimal tradeoff between fitting\nand generalization; zero decay overfits; too large decay underfits. Recommend\ntuning weight decay within this range and possibly adjusting learning rate or\nbatch size for further gains.", "[]", "[]", "The unidirectional setup yields perfect accuracy and smooth convergence, whereas\nthe bidirectional encoder severely underperforms, likely due to suboptimal\nlearning rate, insufficient epochs, or lack of regularization. To improve, keep\nthe architecture fixed but tune hyperparameters: reduce the learning rate or add\na warmup schedule for the bidirectional model, experiment with more epochs,\nadjust batch size, and consider gradient clipping or weight decay to stabilize\ntraining. For broader evaluation, add two HuggingFace code datasets: the\nCodeSearchNet (python and java) retrieval subsets to test cross-language clone\nretrieval, and the CodeXGLUE code-to-text summarization dataset (Python) to\nassess how trace-augmented representations transfer to summarization tasks\nwithout architecture changes.", "Hyperparameter changes (longer training budgets) did not improve final\nperformance beyond fast convergence. Transient validation fluctuations under\nhigher epoch settings imply schedule or noise sensitivity. Model saturates\nearly, indicating the synthetic dataset might be too simplistic; propose testing\non two additional HuggingFace code datasets (e.g., code_search_net and conala)\nto assess real-world generalization.", "The 30-epoch hyperparameter configuration offers the best trade-off between fast\nconvergence and robust generalization. It should be adopted as the new baseline.\nTo further evaluate TraceCode\u2019s dynamic-trace benefits, test on two additional\nHuggingFace datasets: \u2022 code_x_glue_cc_clone_detection for adversarial code-\nclone detection across real-world Java function pairs \u2022 zhoushuxin/devign for\nC-language vulnerability detection tasks", "All experiments converge rapidly and reach optimal retrieval accuracy within\n10\u201312 epochs, with no meaningful gains from extending training to 30 or 50\nepochs. Validation loss fluctuations at higher epoch counts point to diminishing\nreturns and potential overfitting. Early stopping around epoch 10\u201315 is\nrecommended for efficiency. For further evaluation, test the tuned model on\ndiverse real-world code benchmarks from HuggingFace such as CodeSearchNet and\nDevign to assess generalization.", "[]"], "exec_time": [2.0860533714294434, 3.924856662750244, 3.112076759338379, 2.548266887664795, 3.006809949874878, 3.0813939571380615, 4.870294570922852, 2.8020517826080322, 2.7616167068481445, 3.0331287384033203, 2.957202672958374, 2.845320701599121, 2.4009737968444824, 3.9445207118988037, 4.017588376998901, 4.0068933963775635, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [[""], ["[\"synthetic\"]"], [], ["synthetic"], ["[\"synthetic\"]"], ["[Code Dataset]"], [], ["['synthetic']"], [], ["['synthetic']"], [], [], ["[\"synthetic\"]"], ["['synthetic']"], ["[\"Synthetic Dataset\"]"], ["[synthetic]"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot loss curves\ntry:\n    losses = experiment_data[\"synthetic\"][\"losses\"]\n    epochs = np.arange(1, len(losses[\"train\"]) + 1)\n    plt.figure()\n    plt.plot(epochs, losses[\"train\"], label=\"Train Loss\")\n    plt.plot(epochs, losses[\"val\"], label=\"Val Loss\")\n    plt.title(\"Loss Curves for synthetic dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# Plot accuracy curves\ntry:\n    acc = experiment_data[\"synthetic\"][\"metrics\"]\n    epochs = np.arange(1, len(acc[\"train\"]) + 1)\n    plt.figure()\n    plt.plot(epochs, acc[\"train\"], label=\"Train Acc\")\n    plt.plot(epochs, acc[\"val\"], label=\"Val Acc\")\n    plt.title(\"Accuracy Curves for synthetic dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Retrieval Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve plot: {e}\")\n    plt.close()\n\n# Plot confusion matrix\ntry:\n    preds = np.array(experiment_data[\"synthetic\"][\"predictions\"])\n    truths = np.array(experiment_data[\"synthetic\"][\"ground_truth\"])\n    num_classes = int(max(preds.max(), truths.max()) + 1)\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    for t, p in zip(truths, preds):\n        cm[t, p] += 1\n    plt.figure()\n    plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n    plt.title(\"Confusion Matrix for synthetic dataset\")\n    plt.xlabel(\"Predicted Group ID\")\n    plt.ylabel(\"True Group ID\")\n    plt.colorbar()\n    plt.savefig(os.path.join(working_dir, \"synthetic_confusion_matrix.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating confusion matrix plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    synthetic_data = experiment_data[\"EPOCHS\"][\"synthetic\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    synthetic_data = {}\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for E, data in synthetic_data.items():\n        epochs = list(range(1, len(data[\"losses\"][\"train\"]) + 1))\n        plt.plot(epochs, data[\"losses\"][\"train\"], label=f\"Train E={E}\")\n        plt.plot(epochs, data[\"losses\"][\"val\"], linestyle=\"--\", label=f\"Val E={E}\")\n    plt.suptitle(\"Synthetic Dataset\")\n    plt.title(\"Train vs Validation Loss Curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Plot accuracy curves\ntry:\n    plt.figure()\n    for E, data in synthetic_data.items():\n        epochs = list(range(1, len(data[\"metrics\"][\"train\"]) + 1))\n        plt.plot(epochs, data[\"metrics\"][\"train\"], label=f\"Train E={E}\")\n        plt.plot(epochs, data[\"metrics\"][\"val\"], linestyle=\"--\", label=f\"Val E={E}\")\n    plt.suptitle(\"Synthetic Dataset\")\n    plt.title(\"Train vs Validation Retrieval Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves plot: {e}\")\n    plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\ndata_list = experiment_data.get(\"batch_size\", {}).get(\"synthetic\", [])\n\ntry:\n    plt.figure()\n    for entry in data_list:\n        bs = entry[\"batch_size\"]\n        tl = entry[\"losses\"][\"train\"]\n        vl = entry[\"losses\"][\"val\"]\n        epochs = range(1, len(tl) + 1)\n        plt.plot(epochs, tl, label=f\"Train Loss bs={bs}\")\n        plt.plot(epochs, vl, label=f\"Val Loss bs={bs}\", linestyle=\"--\")\n    plt.title(\"Synthetic Dataset Loss Curves: Solid=Train, Dashed=Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    for entry in data_list:\n        bs = entry[\"batch_size\"]\n        ta = entry[\"metrics\"][\"train\"]\n        va = entry[\"metrics\"][\"val\"]\n        epochs = range(1, len(ta) + 1)\n        plt.plot(epochs, ta, label=f\"Train Acc bs={bs}\")\n        plt.plot(epochs, va, label=f\"Val Acc bs={bs}\", linestyle=\"--\")\n    plt.title(\"Synthetic Dataset Accuracy Curves: Solid=Train, Dashed=Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Loss curves\ntry:\n    data = experiment_data[\"triplet_margin_sweep\"][\"synthetic\"]\n    margins = data[\"margins\"]\n    train_losses = data[\"losses\"][\"train\"]\n    val_losses = data[\"losses\"][\"val\"]\n    epochs = range(1, len(train_losses[0]) + 1)\n    plt.figure()\n    for i, m in enumerate(margins):\n        plt.plot(epochs, train_losses[i], label=f\"Train m={m}\")\n        plt.plot(epochs, val_losses[i], \"--\", label=f\"Val m={m}\")\n    plt.title(\"Triplet Margin Sweep on Synthetic Dataset - Losses\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"triplet_margin_sweep_synthetic_losses.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Retrieval accuracy curves\ntry:\n    train_acc = data[\"metrics\"][\"train\"]\n    val_acc = data[\"metrics\"][\"val\"]\n    plt.figure()\n    for i, m in enumerate(margins):\n        plt.plot(epochs, train_acc[i], label=f\"Train m={m}\")\n        plt.plot(epochs, val_acc[i], \"--\", label=f\"Val m={m}\")\n    plt.title(\"Triplet Margin Sweep on Synthetic Dataset - Retrieval Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(\n        os.path.join(working_dir, \"triplet_margin_sweep_synthetic_accuracy.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Sort learning rates\nlr_strs = list(experiment_data.get(\"learning_rate\", {}).keys())\nlrs = sorted(lr_strs, key=lambda x: float(x))\n\n# Plot loss curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    for lr in lrs:\n        res = experiment_data[\"learning_rate\"][lr]\n        axes[0].plot(res[\"losses\"][\"train\"], label=lr)\n        axes[1].plot(res[\"losses\"][\"val\"], label=lr)\n    axes[0].set_title(\"Left: Training Loss - Code Dataset\")\n    axes[1].set_title(\"Right: Validation Loss - Code Dataset\")\n    fig.suptitle(\"Loss Curves per Learning Rate\")\n    for ax in axes:\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Loss\")\n        ax.legend(title=\"LR\")\n    fig.savefig(os.path.join(working_dir, \"code_dataset_loss_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close(\"all\")\n\n# Plot accuracy curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    for lr in lrs:\n        res = experiment_data[\"learning_rate\"][lr]\n        axes[0].plot(res[\"metrics\"][\"train\"], label=lr)\n        axes[1].plot(res[\"metrics\"][\"val\"], label=lr)\n    axes[0].set_title(\"Left: Training Accuracy - Code Dataset\")\n    axes[1].set_title(\"Right: Validation Accuracy - Code Dataset\")\n    fig.suptitle(\"Accuracy Curves per Learning Rate\")\n    for ax in axes:\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Accuracy\")\n        ax.legend(title=\"LR\")\n    fig.savefig(os.path.join(working_dir, \"code_dataset_accuracy_curves.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating accuracy curves plot: {e}\")\n    plt.close(\"all\")\n\n# Plot final validation accuracy bar chart\ntry:\n    final_acc = [\n        experiment_data[\"learning_rate\"][lr][\"metrics\"][\"val\"][-1] for lr in lrs\n    ]\n    fig = plt.figure(figsize=(8, 5))\n    plt.bar(lrs, final_acc, color=\"skyblue\")\n    plt.title(\"Final Validation Accuracy per Learning Rate - Code Dataset\")\n    plt.xlabel(\"Learning Rate\")\n    plt.ylabel(\"Accuracy\")\n    plt.xticks(rotation=45)\n    fig.tight_layout()\n    fig.savefig(os.path.join(working_dir, \"code_dataset_final_val_accuracy.png\"))\n    plt.close(fig)\nexcept Exception as e:\n    print(f\"Error creating final accuracy bar chart: {e}\")\n    plt.close(\"all\")\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# load data\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    syn = data[\"embed_dim\"][\"synthetic\"]\n    dims = syn[\"values\"]\n    acc_train = syn[\"metrics\"][\"train\"]\n    acc_val = syn[\"metrics\"][\"val\"]\n    loss_train = syn[\"losses\"][\"train\"]\n    loss_val = syn[\"losses\"][\"val\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    raise\n\n# Plot accuracy vs epochs\ntry:\n    plt.figure()\n    for d, tr, vl in zip(dims, acc_train, acc_val):\n        epochs = range(1, len(tr) + 1)\n        plt.plot(epochs, tr, label=f\"{d} Train\")\n        plt.plot(epochs, vl, linestyle=\"--\", label=f\"{d} Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Retrieval Accuracy\")\n    plt.title(\"Synthetic: Retrieval Accuracy vs Epochs\\nTrain (solid) and Val (dashed)\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_vs_epochs.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy plot: {e}\")\n    plt.close()\n\n# Plot loss vs epochs\ntry:\n    plt.figure()\n    for d, tr, vl in zip(dims, loss_train, loss_val):\n        epochs = range(1, len(tr) + 1)\n        plt.plot(epochs, tr, label=f\"{d} Train\")\n        plt.plot(epochs, vl, linestyle=\"--\", label=f\"{d} Val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Triplet Loss\")\n    plt.title(\n        \"Synthetic: Training and Validation Loss vs Epochs\\nTrain (solid) and Val (dashed)\"\n    )\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_vs_epochs.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Plot final validation accuracy vs embed_dim\ntry:\n    final_acc = [v[-1] for v in acc_val]\n    plt.figure()\n    plt.bar(dims, final_acc, color=\"skyblue\")\n    plt.xlabel(\"Embedding Dimension\")\n    plt.ylabel(\"Final Validation Accuracy\")\n    plt.title(\"Synthetic: Final Validation Accuracy vs Embedding Dimension\")\n    plt.xticks(dims)\n    plt.savefig(os.path.join(working_dir, \"synthetic_final_val_acc_vs_embed_dim.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating summary bar chart: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    exp_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    exp_data = {}\n\nexp = exp_data.get(\"lstm_dropout\", {}).get(\"synthetic\", {})\ndropouts = exp.get(\"dropout_rates\", [])\nloss_tr = exp.get(\"losses\", {}).get(\"train\", [])\nloss_val = exp.get(\"losses\", {}).get(\"val\", [])\nacc_tr = exp.get(\"metrics\", {}).get(\"train\", [])\nacc_val = exp.get(\"metrics\", {}).get(\"val\", [])\n\n# 1: Loss curves\ntry:\n    plt.figure()\n    for d, lt, lv in zip(dropouts, loss_tr, loss_val):\n        plt.plot(lt, linestyle=\"-\", label=f\"train dr={d}\")\n        plt.plot(lv, linestyle=\"--\", label=f\"val dr={d}\")\n    plt.title(\"Loss Curves (synthetic)\\nTrain: solid; Val: dashed\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves_lstm_dropout.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot1: {e}\")\n    plt.close()\n\n# 2: Accuracy curves\ntry:\n    plt.figure()\n    for d, at, av in zip(dropouts, acc_tr, acc_val):\n        plt.plot(at, linestyle=\"-\", label=f\"train dr={d}\")\n        plt.plot(av, linestyle=\"--\", label=f\"val dr={d}\")\n    plt.title(\"Accuracy Curves (synthetic)\\nTrain: solid; Val: dashed\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curves_lstm_dropout.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot2: {e}\")\n    plt.close()\n\n# 3: Final validation accuracy bar chart\ntry:\n    final_acc = [av[-1] for av in acc_val]\n    plt.figure()\n    plt.bar([str(d) for d in dropouts], final_acc)\n    plt.title(\n        \"Final Validation Accuracy by Dropout Rate (synthetic)\\nBar height = val accuracy\"\n    )\n    plt.xlabel(\"LSTM Dropout Rate\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.savefig(\n        os.path.join(working_dir, \"synthetic_final_val_accuracy_lstm_dropout.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating plot3: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\ndata = experiment_data[\"weight_decay_sweep\"][\"synthetic\"]\nwds = data[\"params\"]\ntrain_losses = data[\"losses\"][\"train\"]\nval_losses = data[\"losses\"][\"val\"]\ntrain_accs = data[\"metrics\"][\"train\"]\nval_accs = data[\"metrics\"][\"val\"]\nepochs = range(1, len(train_losses[0]) + 1)\n\n# Plot 1: Loss curves\ntry:\n    plt.figure()\n    for wd, tloss, vloss in zip(wds, train_losses, val_losses):\n        plt.plot(epochs, tloss, label=f\"WD {wd} train\")\n        plt.plot(epochs, vloss, \"--\", label=f\"WD {wd} val\")\n    plt.title(\"Synthetic dataset - Loss curves\\nTrain (solid) vs Validation (dashed)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Plot 2: Accuracy curves\ntry:\n    plt.figure()\n    for wd, tac, vac in zip(wds, train_accs, val_accs):\n        plt.plot(epochs, tac, label=f\"WD {wd} train\")\n        plt.plot(epochs, vac, \"--\", label=f\"WD {wd} val\")\n    plt.title(\n        \"Synthetic dataset - Accuracy curves\\nTrain (solid) vs Validation (dashed)\"\n    )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves plot: {e}\")\n    plt.close()\n\n# Plot 3: Final Validation Accuracy vs Weight Decay\ntry:\n    plt.figure()\n    final_vals = [accs[-1] for accs in val_accs]\n    plt.bar([str(w) for w in wds], final_vals)\n    plt.title(\"Synthetic dataset - Final Validation Accuracy vs Weight Decay\")\n    plt.xlabel(\"Weight Decay\")\n    plt.ylabel(\"Final Validation Accuracy\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_final_val_acc.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final val accuracy bar plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    data = {}\nexp = data.get(\"hard_negative_pool_size\", {}).get(\"synthetic\", {})\npool_sizes = exp.get(\"pool_sizes\", [])\nloss_train = exp.get(\"losses\", {}).get(\"train\", [])\nloss_val = exp.get(\"losses\", {}).get(\"val\", [])\nmetrics_train = exp.get(\"metrics\", {}).get(\"train\", [])\nmetrics_val = exp.get(\"metrics\", {}).get(\"val\", [])\nepochs = list(range(len(loss_train[0]))) if loss_train else []\n\n# Plot loss curves\ntry:\n    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n    for ps, lt in zip(pool_sizes, loss_train):\n        axs[0].plot(epochs, lt, label=f\"pool {ps}\")\n    axs[0].set_title(\"Left: Training Loss (synthetic)\")\n    axs[0].set_xlabel(\"Epoch\")\n    axs[0].set_ylabel(\"Loss\")\n    axs[0].legend()\n    for ps, lv in zip(pool_sizes, loss_val):\n        axs[1].plot(epochs, lv, label=f\"pool {ps}\")\n    axs[1].set_title(\"Right: Validation Loss (synthetic)\")\n    axs[1].set_xlabel(\"Epoch\")\n    axs[1].set_ylabel(\"Loss\")\n    axs[1].legend()\n    fig.suptitle(\"Loss Curves - Synthetic dataset\")\n    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n    fig.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Plot accuracy curves\ntry:\n    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n    for ps, mt in zip(pool_sizes, metrics_train):\n        axs[0].plot(epochs, mt, label=f\"pool {ps}\")\n    axs[0].set_title(\"Left: Training Accuracy (synthetic)\")\n    axs[0].set_xlabel(\"Epoch\")\n    axs[0].set_ylabel(\"Accuracy\")\n    axs[0].legend()\n    for ps, mv in zip(pool_sizes, metrics_val):\n        axs[1].plot(epochs, mv, label=f\"pool {ps}\")\n    axs[1].set_title(\"Right: Validation Accuracy (synthetic)\")\n    axs[1].set_xlabel(\"Epoch\")\n    axs[1].set_ylabel(\"Accuracy\")\n    axs[1].legend()\n    fig.suptitle(\"Retrieval Accuracy Curves - Synthetic dataset\")\n    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n    fig.savefig(os.path.join(working_dir, \"synthetic_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves plot: {e}\")\n    plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot loss curves\ntry:\n    plt.figure()\n    data = experiment_data[\"lstm_bidirectional\"]\n    for key, val in data.items():\n        losses = val[\"synthetic\"][\"losses\"]\n        plt.plot(losses[\"train\"], label=f\"train bidirectional={key}\")\n        plt.plot(losses[\"val\"], label=f\"val bidirectional={key}\")\n    plt.title(\"Training & Validation Loss Curves\\nSynthetic dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(\n        os.path.join(working_dir, \"lstm_bidirectional_synthetic_loss_curve.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curve plot: {e}\")\n    plt.close()\n\n# Plot accuracy curves\ntry:\n    plt.figure()\n    for key, val in data.items():\n        metrics = val[\"synthetic\"][\"metrics\"]\n        plt.plot(metrics[\"train\"], label=f\"train bidirectional={key}\")\n        plt.plot(metrics[\"val\"], label=f\"val bidirectional={key}\")\n    plt.title(\"Training & Validation Retrieval Accuracy\\nSynthetic dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(\n        os.path.join(working_dir, \"lstm_bidirectional_synthetic_accuracy_curve.png\")\n    )\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curve plot: {e}\")\n    plt.close()\n\n# Plot final validation accuracy bar chart\ntry:\n    keys = list(data.keys())\n    final_accs = [data[k][\"synthetic\"][\"metrics\"][\"val\"][-1] for k in keys]\n    plt.figure()\n    plt.bar([f\"bidirectional={k}\" for k in keys], final_accs, color=[\"C0\", \"C1\"])\n    plt.title(\"Final Validation Retrieval Accuracy\\nSynthetic dataset\")\n    plt.xlabel(\"Model Setting\")\n    plt.ylabel(\"Accuracy\")\n    plt.savefig(\n        os.path.join(working_dir, \"lstm_bidirectional_synthetic_final_val_accuracy.png\")\n    )\n    plt.close()\n    print(\"Final validation accuracies:\", dict(zip(keys, final_accs)))\nexcept Exception as e:\n    print(f\"Error creating final val accuracy bar chart: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    synthetic_data = experiment_data[\"EPOCHS\"][\"synthetic\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    synthetic_data = {}\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for E, data in synthetic_data.items():\n        epochs = list(range(1, len(data[\"losses\"][\"train\"]) + 1))\n        plt.plot(epochs, data[\"losses\"][\"train\"], label=f\"Train E={E}\")\n        plt.plot(epochs, data[\"losses\"][\"val\"], linestyle=\"--\", label=f\"Val E={E}\")\n    plt.suptitle(\"Synthetic Dataset\")\n    plt.title(\"Train vs Validation Loss Curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Plot accuracy curves\ntry:\n    plt.figure()\n    for E, data in synthetic_data.items():\n        epochs = list(range(1, len(data[\"metrics\"][\"train\"]) + 1))\n        plt.plot(epochs, data[\"metrics\"][\"train\"], label=f\"Train E={E}\")\n        plt.plot(epochs, data[\"metrics\"][\"val\"], linestyle=\"--\", label=f\"Val E={E}\")\n    plt.suptitle(\"Synthetic Dataset\")\n    plt.title(\"Train vs Validation Retrieval Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    synthetic_data = experiment_data[\"EPOCHS\"][\"synthetic\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    synthetic_data = {}\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for E, data in synthetic_data.items():\n        epochs = list(range(1, len(data[\"losses\"][\"train\"]) + 1))\n        plt.plot(epochs, data[\"losses\"][\"train\"], label=f\"Train E={E}\")\n        plt.plot(epochs, data[\"losses\"][\"val\"], linestyle=\"--\", label=f\"Val E={E}\")\n    plt.suptitle(\"Synthetic Dataset\")\n    plt.title(\"Train vs Validation Loss Curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Plot accuracy curves\ntry:\n    plt.figure()\n    for E, data in synthetic_data.items():\n        epochs = list(range(1, len(data[\"metrics\"][\"train\"]) + 1))\n        plt.plot(epochs, data[\"metrics\"][\"train\"], label=f\"Train E={E}\")\n        plt.plot(epochs, data[\"metrics\"][\"val\"], linestyle=\"--\", label=f\"Val E={E}\")\n    plt.suptitle(\"Synthetic Dataset\")\n    plt.title(\"Train vs Validation Retrieval Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    synthetic_data = experiment_data[\"EPOCHS\"][\"synthetic\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    synthetic_data = {}\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for E, data in synthetic_data.items():\n        epochs = list(range(1, len(data[\"losses\"][\"train\"]) + 1))\n        plt.plot(epochs, data[\"losses\"][\"train\"], label=f\"Train E={E}\")\n        plt.plot(epochs, data[\"losses\"][\"val\"], linestyle=\"--\", label=f\"Val E={E}\")\n    plt.suptitle(\"Synthetic Dataset\")\n    plt.title(\"Train vs Validation Loss Curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Plot accuracy curves\ntry:\n    plt.figure()\n    for E, data in synthetic_data.items():\n        epochs = list(range(1, len(data[\"metrics\"][\"train\"]) + 1))\n        plt.plot(epochs, data[\"metrics\"][\"train\"], label=f\"Train E={E}\")\n        plt.plot(epochs, data[\"metrics\"][\"val\"], linestyle=\"--\", label=f\"Val E={E}\")\n    plt.suptitle(\"Synthetic Dataset\")\n    plt.title(\"Train vs Validation Retrieval Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_accuracy_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy curves plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load all experiment data\ntry:\n    experiment_data_path_list = [\n        \"experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_c5b491aafe1947f39e9c2308fe06a617_proc_385045/experiment_data.npy\",\n        \"experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_1b2ea6531bed4bfb8a350a13214f6289_proc_385046/experiment_data.npy\",\n        \"experiments/2025-06-09_06-11-57_tracecode_dynamic_contrastive_attempt_0/logs/0-run/experiment_results/experiment_0175b29beba44f91b947647cb2b6688f_proc_385044/experiment_data.npy\",\n    ]\n    all_experiment_data = []\n    for rel_path in experiment_data_path_list:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\"), rel_path)\n        exp = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(exp)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_experiment_data = []\n\n# Prepare aggregated synthetic runs\nsynthetic_runs = []\nfor exp in all_experiment_data:\n    try:\n        synthetic_runs.append(exp[\"EPOCHS\"][\"synthetic\"])\n    except:\n        continue\n\ncommon_Es = (\n    set.intersection(*[set(run.keys()) for run in synthetic_runs])\n    if synthetic_runs\n    else set()\n)\n\n# Plot mean \u00b1 SEM loss curves\ntry:\n    plt.figure()\n    for E in sorted(common_Es):\n        train_curves = [run[E][\"losses\"][\"train\"] for run in synthetic_runs]\n        val_curves = [run[E][\"losses\"][\"val\"] for run in synthetic_runs]\n        train_arr = np.array(train_curves)\n        val_arr = np.array(val_curves)\n        epochs = np.arange(1, train_arr.shape[1] + 1)\n        mean_train = train_arr.mean(axis=0)\n        sem_train = train_arr.std(axis=0, ddof=1) / np.sqrt(train_arr.shape[0])\n        mean_val = val_arr.mean(axis=0)\n        sem_val = val_arr.std(axis=0, ddof=1) / np.sqrt(val_arr.shape[0])\n        plt.errorbar(\n            epochs, mean_train, yerr=sem_train, label=f\"Train E={E}\", capsize=3\n        )\n        plt.errorbar(\n            epochs,\n            mean_val,\n            yerr=sem_val,\n            linestyle=\"--\",\n            label=f\"Val E={E}\",\n            capsize=3,\n        )\n    plt.suptitle(\"Synthetic Dataset\")\n    plt.title(\"Mean Train vs Validation Loss with SEM\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_mean_sem_loss.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss summary plot: {e}\")\n    plt.close()\n\n# Plot mean \u00b1 SEM accuracy curves and print final metrics\ntry:\n    plt.figure()\n    for E in sorted(common_Es):\n        train_acc = [run[E][\"metrics\"][\"train\"] for run in synthetic_runs]\n        val_acc = [run[E][\"metrics\"][\"val\"] for run in synthetic_runs]\n        train_arr = np.array(train_acc)\n        val_arr = np.array(val_acc)\n        epochs = np.arange(1, train_arr.shape[1] + 1)\n        mean_train = train_arr.mean(axis=0)\n        sem_train = train_arr.std(axis=0, ddof=1) / np.sqrt(train_arr.shape[0])\n        mean_val = val_arr.mean(axis=0)\n        sem_val = val_arr.std(axis=0, ddof=1) / np.sqrt(val_arr.shape[0])\n        plt.errorbar(\n            epochs, mean_train, yerr=sem_train, label=f\"Train E={E}\", capsize=3\n        )\n        plt.errorbar(\n            epochs,\n            mean_val,\n            yerr=sem_val,\n            linestyle=\"--\",\n            label=f\"Val E={E}\",\n            capsize=3,\n        )\n        print(\n            f\"E={E} Final Accuracy -> Train: {mean_train[-1]:.4f} \u00b1 {sem_train[-1]:.4f}, Val: {mean_val[-1]:.4f} \u00b1 {sem_val[-1]:.4f}\"\n        )\n    plt.suptitle(\"Synthetic Dataset\")\n    plt.title(\"Mean Train vs Validation Accuracy with SEM\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_mean_sem_accuracy.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy summary plot: {e}\")\n    plt.close()\n"], "plot_plan": ["We start by loading the saved experiment data from the working directory using numpy. In the first try\u2010except block we extract training and validation loss lists and plot them together in one figure, labeling axes and saving as \u201csynthetic_loss_curve.png\u201d. The second block similarly plots retrieval accuracy curves for both train and validation sets, saving \u201csynthetic_accuracy_curve.png\u201d. In a third block we compute a confusion matrix between the final ground truth and predicted group IDs, visualize it as a heatmap with true vs predicted axes, and save it as \u201csynthetic_confusion_matrix.png\u201d. Each figure includes a clear title specifying the \u201csynthetic\u201d dataset and appropriate axis labels. We handle any plotting errors gracefully by printing an error message and ensuring figures are always closed. All plots are saved into the same working directory.", "The code below loads the saved experiment data for the synthetic dataset, then creates two summary plots: one showing train vs. validation loss curves for each epoch setting, and another showing train vs. validation retrieval accuracy curves. Each plotting block is wrapped in a try-except to handle potential errors, uses clear titles and subtitles specifying the synthetic dataset, labels axes, includes a legend, and saves to the working directory with descriptive filenames while always closing the figure.", null, "We load the saved experiment data from the working directory and extract the synthetic batch\u2010size entries. We collect training and validation losses and accuracies across epochs for each batch size. We then generate a combined loss\u2010curve figure plotting both train and val losses per epoch for every batch size. A similar figure is created for training and validation accuracy curves. Each plot is wrapped in a try\u2010except to handle errors gracefully and always closes the figure. Figures are saved to the working directory with filenames that include the dataset and plot type for clarity. Legends distinguish batch sizes, and titles specify the synthetic dataset and curve types.", "First, load the experiment data from the working directory. Extract the synthetic dataset\u2019s margin sweep results including train/validation losses and accuracies. In the first try-except block, plot solid lines for train and dashed lines for validation losses over epochs for each margin, add a descriptive title and axis labels, and save the figure. In the second try-except, similarly plot retrieval accuracy curves. Wrap each plotting section to catch errors and always close the figure to release resources. Ensure filenames include the dataset and plot type for clarity. This produces two clear visualizations of model behavior under different margin settings.", "We will load the saved experiment data and extract learning\u2010rate keys sorted numerically.  \nNext, we will plot training and validation loss curves in a two\u2010panel figure, with \u201cLeft: Training Loss \u2013 Code Dataset\u201d and \u201cRight: Validation Loss \u2013 Code Dataset\u201d subtitles, saving to working_dir.  \nSimilarly, we will create a two\u2010panel accuracy curves figure and save it with a descriptive filename.  \nFinally, we will generate a bar chart of final validation accuracy per learning rate to compare hyperparameter effects.  \nEach plot is wrapped in its own try\u2010except block, uses only real data from experiment_data.npy, and closes figures after saving.", null, "We import matplotlib, numpy, and os, then set working_dir. We load experiment_data.npy. We extract embed_dims, metrics, and losses for the synthetic dataset. First, we plot retrieval accuracy vs epochs, with solid train and dashed val lines per embed_dim. Second, we plot training and validation loss curves. Third, we plot a bar chart of final validation accuracy vs embed_dim. Each plot lives in its own try-except, saves to working_dir with descriptive names, includes titles specifying the synthetic dataset and clear subtitles, and always closes the figure.", "I will load the saved experiment data and extract the synthetic dataset\u2019s dropout rates along with its per-rate training/validation loss and accuracy trajectories. Then I\u2019ll produce three clean matplotlib figures, each inside its own try\u2010except block to catch errors and ensure the figure is closed properly. First, I\u2019ll overlay all dropout\u2010rate loss curves (solid for training, dashed for validation) with a clear multiline title indicating the synthetic dataset and line styles. Second, I\u2019ll similarly plot the accuracy curves with the same conventions. Finally, I\u2019ll summarize final validation accuracies as a bar chart keyed by dropout rate. Each plot is saved to the working directory under descriptive filenames that include the dataset name and plot type. This approach sticks to at most five summary figures, uses only actual experiment data, and follows the specified plotting guidelines.", "I will load the saved experiment data for the synthetic dataset and extract the weight decay sweep parameters along with their training/validation losses and accuracies across epochs. Then I'll generate three plots wrapped in try-except blocks: (1) combined loss curves per weight decay (solid for training, dashed for validation); (2) combined accuracy curves per weight decay; and (3) a bar chart of the final validation accuracy versus weight decay. Each plot will have a clear title referencing the synthetic dataset, axis labels, and a legend where appropriate. All figures will be saved into the working directory with descriptive filenames and closed after saving. Now let's implement this.", "I will load the saved experiment_data.npy from the working directory and extract the 'synthetic' results under 'hard_negative_pool_size'. Then I will prepare two figures: one for training and validation loss curves and another for training and validation retrieval accuracy curves, each across all pool sizes over epochs. I will structure each figure with two subplots, labeling the left and right axes as 'Left: Training Loss/Accuracy' and 'Right: Validation Loss/Accuracy' respectively, and include the dataset name in the titles. I will plot a separate line for each pool size in both subplots and add legends and axis labels for clarity. Each plotting operation will be wrapped in a try-except block to catch and report any exceptions without halting. After saving each figure to the working directory with descriptive filenames, I will always close the figure to free resources. I will not generate or simulate any extra data, and only plot arrays directly from the loaded numpy object. The code will rely solely on basic matplotlib and numpy calls and adhere to the given guidelines for plot saving. Overall, this approach provides a concise visualization of how pool size affects loss and retrieval accuracy over training.", null, "We will set the working directory and load the saved experiment data.  \nThen, in a try-except block, we will create a single plot of training and validation loss curves for both bidirectional settings, label each line accordingly, include a two-line title specifying the synthetic dataset, and save the figure with a descriptive filename.  \nNext, in a separate try-except block, we will plot training and validation retrieval accuracy curves following the same conventions and save with an accuracy plot filename.  \nFinally, in a third try-except block, we will extract the final validation accuracies for each bidirectional toggle, draw a bar chart comparing them, label axes, include a dataset subtitle, save under a clear filename, and print out the final accuracy values.  \nEach plot block will always close its figure even on exceptions to avoid resource leakage.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "EPOCHS", "learning_rate", "BATCH_SIZE", "triplet_margin", null, null, "embed_dim", "lstm_dropout", "weight_decay", "HARD_NEGATIVE_POOL_SIZE", "lstm_num_layers", "lstm_bidirectional", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script first locates and loads the saved experiment data from the working\ndirectory. It then iterates over each dataset entry, extracting the final epoch\nvalues for training and validation accuracy and loss. If prediction and\nground\u2010truth arrays are present, it computes and prints the overall test\naccuracy. All outputs are clearly labeled and printed directly at the global\nscope.", "Here\u2019s a script that loads the saved `experiment_data.npy` from the `working`\ndirectory, iterates over each dataset (here just \"synthetic\") and each epoch\nsetting, and prints out the final training/validation accuracy and\ntraining/validation loss with precise labels. The dataset name is printed before\nits metrics, and each metric is clearly labeled. No plotting is performed and\nthere is no `if __name__ == \"__main__\":` block.", "The following script immediately loads the saved experimental results, iterates\nover each learning rate, and for both the training and validation datasets\nextracts the final accuracy and loss values.  It prints each learning rate, then\nclearly labels the dataset (\u201ctrain\u201d or \u201cvalidation\u201d) before printing \u201ctrain\naccuracy\u201d, \u201ctrain loss\u201d, \u201cvalidation accuracy\u201d and \u201cvalidation loss\u201d with\nformatted values.  All code executes at the global scope and does not use plots\nor an entry\u2010point guard.", "Below is a script that loads the saved `experiment_data.npy` from the working\ndirectory, retrieves the dictionary, and iterates over each dataset under the\n`\"batch_size\"` key. For each dataset (e.g. `\"synthetic\"`) and each batch\u2010size\nrun, it extracts the final train accuracy, validation accuracy, train loss, and\nvalidation loss, then prints the dataset name first followed by each metric with\nclear labels. All execution occurs at the global scope.", "I will load the saved `experiment_data.npy` file from the `working` directory,\nthen iterate over each sweep and dataset, extracting the final\u2010epoch values of\ntrain accuracy, validation accuracy, train loss, and validation loss for each\nmargin. Each dataset name and metric label is printed clearly according to the\ninstructions. The script runs immediately at the global scope and requires no\nspecial entry point.", "Below is a simple script that loads the saved experiment data, iterates through\neach learning\u2010rate entry, and for each split (train and validation) prints the\ndataset name followed by the final epoch values for accuracy and loss using\nclear labels.", "", "Below is a brief outline and the corresponding Python script. The script loads\nthe saved `experiment_data.npy`, iterates over the hyperparameter sweep (here\n`embed_dim`) and the `synthetic` dataset, and prints for each embedding\ndimension the final train accuracy, validation accuracy, training loss, and\nvalidation loss with clear labels.", "I will load the saved experiment data from the working directory, then iterate\nover each experiment type and dataset to extract and print the final\ntrain/validation accuracy and loss per dropout rate. Each dataset name is\nprinted before its metrics, and each metric is labeled explicitly (e.g., \"train\naccuracy\", \"validation loss\"). No plots are generated, and the script executes\nimmediately without any `if __name__ == \"__main__\":` guard.", "Here\u2019s a script that immediately loads the saved experiment data from the\nworking directory, extracts the final train/validation loss and accuracy for\neach dataset in the weight\u2010decay sweep, and prints them with clear labels:", "Below is a script that loads the experiment data from the specified working\ndirectory, iterates over each dataset and hyperparameter setting, and prints out\nthe final train and validation accuracy as well as the final train and\nvalidation loss with clear labels. The code runs at the global scope and\nrequires no special entry points or plotting.", "The script builds the working directory path, loads the saved\n`experiment_data.npy` file, and retrieves the nested results under the\n`'lstm_num_layers'` key. It then iterates through each dataset (here, just\n\u201csynthetic\u201d), prints its name, and for each number of LSTM layers reports the\nfinal epoch\u2019s train and validation accuracy with clear labels. The code runs\nimmediately at the global scope without any `__main__` guard.", "The following script loads the saved experiment data from the `working`\ndirectory and iterates over both bidirectional settings. For each setting it\nextracts and prints the final epoch\u2019s train accuracy, validation accuracy, train\nloss, and validation loss for the synthetic dataset, labeling each metric\nclearly. The script runs immediately upon execution and prints the dataset name\nalong with whether the encoder was bidirectional.", "Here\u2019s a script that loads the saved `experiment_data.npy` from the `working`\ndirectory, iterates over each dataset (here just \"synthetic\") and each epoch\nsetting, and prints out the final training/validation accuracy and\ntraining/validation loss with precise labels. The dataset name is printed before\nits metrics, and each metric is clearly labeled. No plotting is performed and\nthere is no `if __name__ == \"__main__\":` block.", "Here\u2019s a script that loads the saved `experiment_data.npy` from the `working`\ndirectory, iterates over each dataset (here just \"synthetic\") and each epoch\nsetting, and prints out the final training/validation accuracy and\ntraining/validation loss with precise labels. The dataset name is printed before\nits metrics, and each metric is clearly labeled. No plotting is performed and\nthere is no `if __name__ == \"__main__\":` block.", "Here\u2019s a script that loads the saved `experiment_data.npy` from the `working`\ndirectory, iterates over each dataset (here just \"synthetic\") and each epoch\nsetting, and prints out the final training/validation accuracy and\ntraining/validation loss with precise labels. The dataset name is printed before\nits metrics, and each metric is clearly labeled. No plotting is performed and\nthere is no `if __name__ == \"__main__\":` block.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, dataset in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n    # Extract and print final accuracies\n    metrics = dataset.get(\"metrics\", {})\n    train_accs = metrics.get(\"train\", [])\n    val_accs = metrics.get(\"val\", [])\n    if train_accs:\n        print(f\"Train accuracy: {train_accs[-1]:.4f}\")\n    if val_accs:\n        print(f\"Validation accuracy: {val_accs[-1]:.4f}\")\n    # Extract and print final losses\n    losses = dataset.get(\"losses\", {})\n    train_losses = losses.get(\"train\", [])\n    val_losses = losses.get(\"val\", [])\n    if train_losses:\n        print(f\"Train loss: {train_losses[-1]:.4f}\")\n    if val_losses:\n        print(f\"Validation loss: {val_losses[-1]:.4f}\")\n    # Compute and print test accuracy if predictions are available\n    preds = dataset.get(\"predictions\", [])\n    gts = dataset.get(\"ground_truth\", [])\n    if preds and gts and len(preds) == len(gts):\n        test_acc = sum(p == g for p, g in zip(preds, gts)) / len(gts)\n        print(f\"Test accuracy: {test_acc:.4f}\")\n    print()  # Blank line between datasets\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over each dataset under the \"EPOCHS\" key\nfor dataset_name, epoch_dict in experiment_data.get(\"EPOCHS\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    # Sort by epoch count for consistent ordering\n    for E in sorted(epoch_dict):\n        data = epoch_dict[E]\n        train_acc = data[\"metrics\"][\"train\"][-1]\n        val_acc = data[\"metrics\"][\"val\"][-1]\n        train_loss = data[\"losses\"][\"train\"][-1]\n        val_loss = data[\"losses\"][\"val\"][-1]\n        print(f\"EPOCHS = {E}\")\n        print(f\"train accuracy: {train_acc:.4f}\")\n        print(f\"validation accuracy: {val_acc:.4f}\")\n        print(f\"training loss: {train_loss:.4f}\")\n        print(f\"validation loss: {val_loss:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each learning rate and print the final metrics\nlr_dict = experiment_data.get(\"learning_rate\", {})\nfor lr_str in sorted(lr_dict, key=float):\n    res = lr_dict[lr_str]\n    train_acc = res[\"metrics\"][\"train\"][-1]\n    val_acc = res[\"metrics\"][\"val\"][-1]\n    train_loss = res[\"losses\"][\"train\"][-1]\n    val_loss = res[\"losses\"][\"val\"][-1]\n\n    print(f\"Learning rate: {float(lr_str):.1e}\")\n    print(\"Dataset: train\")\n    print(f\"train accuracy: {train_acc:.4f}\")\n    print(f\"train loss: {train_loss:.4f}\")\n    print(\"Dataset: validation\")\n    print(f\"validation accuracy: {val_acc:.4f}\")\n    print(f\"validation loss: {val_loss:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nfile_path = os.path.join(os.getcwd(), \"working\", \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each dataset under the \"batch_size\" section\nbatch_results = experiment_data.get(\"batch_size\", {})\nfor dataset_name, runs in batch_results.items():\n    print(f\"Dataset: {dataset_name}\")\n    for run in runs:\n        bs = run.get(\"batch_size\")\n        train_acc = run.get(\"metrics\", {}).get(\"train\", [])[-1]\n        val_acc = run.get(\"metrics\", {}).get(\"val\", [])[-1]\n        train_loss = run.get(\"losses\", {}).get(\"train\", [])[-1]\n        val_loss = run.get(\"losses\", {}).get(\"val\", [])[-1]\n        print(f\"Batch size: {bs}\")\n        print(f\"train accuracy: {train_acc:.4f}\")\n        print(f\"validation accuracy: {val_acc:.4f}\")\n        print(f\"train loss: {train_loss:.4f}\")\n        print(f\"validation loss: {val_loss:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# 0. Working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# 1. Load the experiment data\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# 2. Extract and print metrics for each dataset\nfor sweep_name, sweep_data in experiment_data.items():\n    for dataset_name, data in sweep_data.items():\n        print(f\"Dataset: {dataset_name}\")\n        margins = data.get(\"margins\", [])\n        train_metrics = data[\"metrics\"].get(\"train\", [])\n        val_metrics = data[\"metrics\"].get(\"val\", [])\n        train_losses = data[\"losses\"].get(\"train\", [])\n        val_losses = data[\"losses\"].get(\"val\", [])\n\n        # 5. Print only the final value for each metric per margin\n        for margin, tm, vm, tl, vl in zip(\n            margins, train_metrics, val_metrics, train_losses, val_losses\n        ):\n            final_train_acc = tm[-1] if tm else None\n            final_val_acc = vm[-1] if vm else None\n            final_train_loss = tl[-1] if tl else None\n            final_val_loss = vl[-1] if vl else None\n\n            print(f\"Triplet margin: {margin}\")\n            if final_train_acc is not None:\n                print(f\"Train accuracy: {final_train_acc:.4f}\")\n            if final_val_acc is not None:\n                print(f\"Validation accuracy: {final_val_acc:.4f}\")\n            if final_train_loss is not None:\n                print(f\"Train loss: {final_train_loss:.4f}\")\n            if final_val_loss is not None:\n                print(f\"Validation loss: {final_val_loss:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each learning rate experiment\nfor lr_str, results in experiment_data[\"learning_rate\"].items():\n    print(f\"\\nLearning rate: {lr_str}\")\n    metrics = results[\"metrics\"]\n    losses = results[\"losses\"]\n\n    # Train split\n    train_acc_final = metrics[\"train\"][-1]\n    train_loss_final = losses[\"train\"][-1]\n    print(\"Dataset: Train\")\n    print(f\"Train accuracy: {train_acc_final:.4f}\")\n    print(f\"Training loss: {train_loss_final:.4f}\")\n\n    # Validation split\n    val_acc_final = metrics[\"val\"][-1]\n    val_loss_final = losses[\"val\"][-1]\n    print(\"Dataset: Validation\")\n    print(f\"Validation accuracy: {val_acc_final:.4f}\")\n    print(f\"Validation loss: {val_loss_final:.4f}\")\n", "", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over the hyperparameter sweep and datasets\nfor sweep_param, sweep_dict in experiment_data.items():\n    for dataset_name, dataset_data in sweep_dict.items():\n        print(f\"Dataset: {dataset_name}\")\n        values = dataset_data[\"values\"]\n        train_accs = dataset_data[\"metrics\"][\"train\"]\n        val_accs = dataset_data[\"metrics\"][\"val\"]\n        train_losses = dataset_data[\"losses\"][\"train\"]\n        val_losses = dataset_data[\"losses\"][\"val\"]\n\n        # For each hyperparameter value, print final metrics\n        for val, tr_acc, v_acc, tr_loss, v_loss in zip(\n            values, train_accs, val_accs, train_losses, val_losses\n        ):\n            final_train_acc = tr_acc[-1]\n            final_val_acc = v_acc[-1]\n            final_train_loss = tr_loss[-1]\n            final_val_loss = v_loss[-1]\n\n            print(f\"{sweep_param} = {val}\")\n            print(f\"train accuracy: {final_train_acc:.4f}\")\n            print(f\"validation accuracy: {final_val_acc:.4f}\")\n            print(f\"training loss: {final_train_loss:.4f}\")\n            print(f\"validation loss: {final_val_loss:.4f}\")\n            print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate and print metrics\nfor exp_name, exp_results in experiment_data.items():\n    for dataset_name, ds_results in exp_results.items():\n        print(f\"Dataset: {dataset_name}\")\n        dropout_rates = ds_results.get(\"dropout_rates\", [])\n        train_metrics = ds_results[\"metrics\"][\"train\"]\n        val_metrics = ds_results[\"metrics\"][\"val\"]\n        train_losses = ds_results[\"losses\"][\"train\"]\n        val_losses = ds_results[\"losses\"][\"val\"]\n\n        for dr, tr_acc, vl_acc, tr_loss, vl_loss in zip(\n            dropout_rates, train_metrics, val_metrics, train_losses, val_losses\n        ):\n            # final epoch metrics\n            final_train_acc = tr_acc[-1]\n            final_val_acc = vl_acc[-1]\n            final_train_loss = tr_loss[-1]\n            final_val_loss = vl_loss[-1]\n\n            print(f\"Dropout rate: {dr}\")\n            print(f\"train accuracy: {final_train_acc:.4f}\")\n            print(f\"validation accuracy: {final_val_acc:.4f}\")\n            print(f\"train loss: {final_train_loss:.4f}\")\n            print(f\"validation loss: {final_val_loss:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Extract and print final metrics for each dataset in the weight_decay sweep\nweight_sweep = experiment_data.get(\"weight_decay_sweep\", {})\nfor dataset_name, dataset_data in weight_sweep.items():\n    # Get lists of per-epoch values for each weight decay run\n    train_losses = dataset_data[\"losses\"][\"train\"]\n    val_losses = dataset_data[\"losses\"][\"val\"]\n    train_accs = dataset_data[\"metrics\"][\"train\"]\n    val_accs = dataset_data[\"metrics\"][\"val\"]\n\n    # Take the final epoch from the last run\n    final_train_loss = train_losses[-1][-1]\n    final_val_loss = val_losses[-1][-1]\n    final_train_acc = train_accs[-1][-1]\n    final_val_acc = val_accs[-1][-1]\n\n    # Print results\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"Final train loss: {final_train_loss:.4f}\")\n    print(f\"Final validation loss: {final_val_loss:.4f}\")\n    print(f\"Final train accuracy: {final_train_acc:.4f}\")\n    print(f\"Final validation accuracy: {final_val_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Print metrics for each dataset and hyperparameter run\nfor experiment_name, datasets in experiment_data.items():\n    for dataset_name, dataset_data in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        pool_sizes = dataset_data[\"pool_sizes\"]\n        train_losses = dataset_data[\"losses\"][\"train\"]\n        val_losses = dataset_data[\"losses\"][\"val\"]\n        train_metrics = dataset_data[\"metrics\"][\"train\"]\n        val_metrics = dataset_data[\"metrics\"][\"val\"]\n\n        for idx, pool_size in enumerate(pool_sizes):\n            final_train_acc = train_metrics[idx][-1]\n            final_val_acc = val_metrics[idx][-1]\n            final_train_loss = train_losses[idx][-1]\n            final_val_loss = val_losses[idx][-1]\n\n            print(f\"  hard_negative_pool_size = {pool_size}\")\n            print(f\"    train accuracy: {final_train_acc:.4f}\")\n            print(f\"    validation accuracy: {final_val_acc:.4f}\")\n            print(f\"    train loss: {final_train_loss:.4f}\")\n            print(f\"    validation loss: {final_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# 0. Construct working directory path and load data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# 1. Navigate into the hyperparameter grouping\nlstm_experiments = experiment_data.get(\"lstm_num_layers\", {})\n\n# 2. Iterate over each dataset (e.g., \"synthetic\")\nfor dataset_name, results in lstm_experiments.items():\n    print(f\"Dataset: {dataset_name}\")\n    # 3. For each LSTM layer count, extract the final train & validation accuracy\n    for num_layers in sorted(results.keys(), key=int):\n        metrics = results[num_layers][\"metrics\"]\n        final_train_acc = metrics[\"train\"][-1]\n        final_val_acc = metrics[\"val\"][-1]\n        print(f\"  LSTM Layers = {num_layers}\")\n        print(f\"    train accuracy = {final_train_acc:.4f}\")\n        print(f\"    validation accuracy = {final_val_acc:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over the two bidirectional settings\nfor bidir_flag, settings in experiment_data[\"lstm_bidirectional\"].items():\n    # Extract results for the synthetic dataset\n    synthetic = settings[\"synthetic\"]\n    metrics = synthetic[\"metrics\"]\n    losses = synthetic[\"losses\"]\n\n    # Get final epoch values\n    final_train_acc = metrics[\"train\"][-1] if metrics[\"train\"] else None\n    final_val_acc = metrics[\"val\"][-1] if metrics[\"val\"] else None\n    final_train_loss = losses[\"train\"][-1] if losses[\"train\"] else None\n    final_val_loss = losses[\"val\"][-1] if losses[\"val\"] else None\n\n    # Print results\n    print(f\"Dataset: synthetic (bidirectional={bidir_flag})\")\n    if final_train_acc is not None:\n        print(f\"Train accuracy: {final_train_acc:.4f}\")\n    if final_val_acc is not None:\n        print(f\"Validation accuracy: {final_val_acc:.4f}\")\n    if final_train_loss is not None:\n        print(f\"Train loss: {final_train_loss:.4f}\")\n    if final_val_loss is not None:\n        print(f\"Validation loss: {final_val_loss:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over each dataset under the \"EPOCHS\" key\nfor dataset_name, epoch_dict in experiment_data.get(\"EPOCHS\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    # Sort by epoch count for consistent ordering\n    for E in sorted(epoch_dict):\n        data = epoch_dict[E]\n        train_acc = data[\"metrics\"][\"train\"][-1]\n        val_acc = data[\"metrics\"][\"val\"][-1]\n        train_loss = data[\"losses\"][\"train\"][-1]\n        val_loss = data[\"losses\"][\"val\"][-1]\n        print(f\"EPOCHS = {E}\")\n        print(f\"train accuracy: {train_acc:.4f}\")\n        print(f\"validation accuracy: {val_acc:.4f}\")\n        print(f\"training loss: {train_loss:.4f}\")\n        print(f\"validation loss: {val_loss:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over each dataset under the \"EPOCHS\" key\nfor dataset_name, epoch_dict in experiment_data.get(\"EPOCHS\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    # Sort by epoch count for consistent ordering\n    for E in sorted(epoch_dict):\n        data = epoch_dict[E]\n        train_acc = data[\"metrics\"][\"train\"][-1]\n        val_acc = data[\"metrics\"][\"val\"][-1]\n        train_loss = data[\"losses\"][\"train\"][-1]\n        val_loss = data[\"losses\"][\"val\"][-1]\n        print(f\"EPOCHS = {E}\")\n        print(f\"train accuracy: {train_acc:.4f}\")\n        print(f\"validation accuracy: {val_acc:.4f}\")\n        print(f\"training loss: {train_loss:.4f}\")\n        print(f\"validation loss: {val_loss:.4f}\")\n        print()\n", "import os\nimport numpy as np\n\n# Load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate over each dataset under the \"EPOCHS\" key\nfor dataset_name, epoch_dict in experiment_data.get(\"EPOCHS\", {}).items():\n    print(f\"Dataset: {dataset_name}\")\n    # Sort by epoch count for consistent ordering\n    for E in sorted(epoch_dict):\n        data = epoch_dict[E]\n        train_acc = data[\"metrics\"][\"train\"][-1]\n        val_acc = data[\"metrics\"][\"val\"][-1]\n        train_loss = data[\"losses\"][\"train\"][-1]\n        val_loss = data[\"losses\"][\"val\"][-1]\n        print(f\"EPOCHS = {E}\")\n        print(f\"train accuracy: {train_acc:.4f}\")\n        print(f\"validation accuracy: {val_acc:.4f}\")\n        print(f\"training loss: {train_loss:.4f}\")\n        print(f\"validation loss: {val_loss:.4f}\")\n        print()\n", ""], "parse_term_out": ["['Dataset: synthetic', '\\n', 'Train accuracy: 1.0000', '\\n', 'Validation\naccuracy: 1.0000', '\\n', 'Train loss: 0.1601', '\\n', 'Validation loss: 0.2121',\n'\\n', 'Test accuracy: 1.0000', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Dataset: synthetic', '\\n', 'EPOCHS = 10', '\\n', 'train accuracy: 1.0000',\n'\\n', 'validation accuracy: 1.0000', '\\n', 'training loss: 0.2750', '\\n',\n'validation loss: 0.4489', '\\n', '\\n', 'EPOCHS = 30', '\\n', 'train accuracy:\n1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'training loss: 0.0000',\n'\\n', 'validation loss: 0.0000', '\\n', '\\n', 'EPOCHS = 50', '\\n', 'train\naccuracy: 1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'training loss:\n0.0000', '\\n', 'validation loss: 0.0000', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Learning rate: 1.0e-04', '\\n', 'Dataset: train', '\\n', 'train accuracy:\n0.4375', '\\n', 'train loss: 0.9816', '\\n', 'Dataset: validation', '\\n',\n'validation accuracy: 0.5000', '\\n', 'validation loss: 0.7781', '\\n', '\\n',\n'Learning rate: 5.0e-04', '\\n', 'Dataset: train', '\\n', 'train accuracy:\n0.8750', '\\n', 'train loss: 0.5391', '\\n', 'Dataset: validation', '\\n',\n'validation accuracy: 0.7500', '\\n', 'validation loss: 0.7524', '\\n', '\\n',\n'Learning rate: 1.0e-03', '\\n', 'Dataset: train', '\\n', 'train accuracy:\n1.0000', '\\n', 'train loss: 0.2099', '\\n', 'Dataset: validation', '\\n',\n'validation accuracy: 1.0000', '\\n', 'validation loss: 0.2715', '\\n', '\\n',\n'Learning rate: 5.0e-03', '\\n', 'Dataset: train', '\\n', 'train accuracy:\n1.0000', '\\n', 'train loss: 0.0000', '\\n', 'Dataset: validation', '\\n',\n'validation accuracy: 1.0000', '\\n', 'validation loss: 0.0000', '\\n', '\\n',\n'Learning rate: 1.0e-02', '\\n', 'Dataset: train', '\\n', 'train accuracy:\n1.0000', '\\n', 'train loss: 0.0255', '\\n', 'Dataset: validation', '\\n',\n'validation accuracy: 1.0000', '\\n', 'validation loss: 0.0000', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Batch size: 4', '\\n', 'train accuracy: 1.0000',\n'\\n', 'validation accuracy: 1.0000', '\\n', 'train loss: 0.0139', '\\n',\n'validation loss: 0.0000\\n', '\\n', 'Batch size: 16', '\\n', 'train accuracy:\n1.0000', '\\n', 'validation accuracy: 0.7500', '\\n', 'train loss: 0.4996', '\\n',\n'validation loss: 0.6038\\n', '\\n', 'Batch size: 32', '\\n', 'train accuracy:\n1.0000', '\\n', 'validation accuracy: 0.5000', '\\n', 'train loss: 0.5431', '\\n',\n'validation loss: 0.7879\\n', '\\n', 'Execution time: a moment seconds (time limit\nis an hour).']", "['Dataset: synthetic', '\\n', 'Triplet margin: 0.2', '\\n', 'Train accuracy:\n1.0000', '\\n', 'Validation accuracy: 0.7500', '\\n', 'Train loss: 0.0000', '\\n',\n'Validation loss: 0.0032', '\\n', 'Triplet margin: 0.5', '\\n', 'Train accuracy:\n1.0000', '\\n', 'Validation accuracy: 1.0000', '\\n', 'Train loss: 0.0000', '\\n',\n'Validation loss: 0.0086', '\\n', 'Triplet margin: 1.0', '\\n', 'Train accuracy:\n1.0000', '\\n', 'Validation accuracy: 0.7500', '\\n', 'Train loss: 0.3330', '\\n',\n'Validation loss: 0.5107', '\\n', 'Triplet margin: 1.5', '\\n', 'Train accuracy:\n1.0000', '\\n', 'Validation accuracy: 0.5000', '\\n', 'Train loss: 0.6189', '\\n',\n'Validation loss: 0.8136', '\\n', 'Triplet margin: 2.0', '\\n', 'Train accuracy:\n1.0000', '\\n', 'Validation accuracy: 0.7500', '\\n', 'Train loss: 1.2626', '\\n',\n'Validation loss: 1.3805', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "['\\nLearning rate: 0.0001', '\\n', 'Dataset: Train', '\\n', 'Train accuracy:\n0.5000', '\\n', 'Training loss: 0.9026', '\\n', 'Dataset: Validation', '\\n',\n'Validation accuracy: 0.5000', '\\n', 'Validation loss: 1.1217', '\\n',\n'\\nLearning rate: 0.0005', '\\n', 'Dataset: Train', '\\n', 'Train accuracy:\n0.8125', '\\n', 'Training loss: 0.7512', '\\n', 'Dataset: Validation', '\\n',\n'Validation accuracy: 0.5000', '\\n', 'Validation loss: 1.2857', '\\n',\n'\\nLearning rate: 0.001', '\\n', 'Dataset: Train', '\\n', 'Train accuracy:\n1.0000', '\\n', 'Training loss: 0.1801', '\\n', 'Dataset: Validation', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Validation loss: 0.6893', '\\n',\n'\\nLearning rate: 0.005', '\\n', 'Dataset: Train', '\\n', 'Train accuracy:\n1.0000', '\\n', 'Training loss: 0.0103', '\\n', 'Dataset: Validation', '\\n',\n'Validation accuracy: 1.0000', '\\n', 'Validation loss: 0.2500', '\\n',\n'\\nLearning rate: 0.01', '\\n', 'Dataset: Train', '\\n', 'Train accuracy: 1.0000',\n'\\n', 'Training loss: 0.0000', '\\n', 'Dataset: Validation', '\\n', 'Validation\naccuracy: 1.0000', '\\n', 'Validation loss: 0.8123', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "", "['Dataset: synthetic', '\\n', 'embed_dim = 32', '\\n', 'train accuracy: 1.0000',\n'\\n', 'validation accuracy: 0.7500', '\\n', 'training loss: 0.5282', '\\n',\n'validation loss: 0.8205', '\\n', '\\n', 'embed_dim = 64', '\\n', 'train accuracy:\n1.0000', '\\n', 'validation accuracy: 0.7500', '\\n', 'training loss: 0.3474',\n'\\n', 'validation loss: 0.5664', '\\n', '\\n', 'embed_dim = 128', '\\n', 'train\naccuracy: 1.0000', '\\n', 'validation accuracy: 0.7500', '\\n', 'training loss:\n0.0039', '\\n', 'validation loss: 0.0871', '\\n', '\\n', 'embed_dim = 256', '\\n',\n'train accuracy: 1.0000', '\\n', 'validation accuracy: 0.7500', '\\n', 'training\nloss: 0.0000', '\\n', 'validation loss: 0.0387', '\\n', '\\n', 'Execution time: a\nmoment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'Dropout rate: 0.1', '\\n', 'train accuracy:\n0.8750', '\\n', 'validation accuracy: 0.5000', '\\n', 'train loss: 0.3688', '\\n',\n'validation loss: 0.2889', '\\n', 'Dropout rate: 0.3', '\\n', 'train accuracy:\n1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'train loss: 0.5033', '\\n',\n'validation loss: 0.1825', '\\n', 'Dropout rate: 0.5', '\\n', 'train accuracy:\n1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'train loss: 0.7647', '\\n',\n'validation loss: 0.5913', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "['Dataset: synthetic', '\\n', 'Final train loss: 0.4641', '\\n', 'Final validation\nloss: 0.5831', '\\n', 'Final train accuracy: 1.0000', '\\n', 'Final validation\naccuracy: 0.7500', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "['Dataset: synthetic', '\\n', '  hard_negative_pool_size = 1', '\\n', '    train\naccuracy: 1.0000', '\\n', '    validation accuracy: 0.7500', '\\n', '    train\nloss: 0.1419', '\\n', '    validation loss: 0.3034', '\\n', '\nhard_negative_pool_size = 2', '\\n', '    train accuracy: 1.0000', '\\n', '\nvalidation accuracy: 1.0000', '\\n', '    train loss: 0.2659', '\\n', '\nvalidation loss: 0.4166', '\\n', '  hard_negative_pool_size = 5', '\\n', '\ntrain accuracy: 1.0000', '\\n', '    validation accuracy: 0.7500', '\\n', '\ntrain loss: 0.3228', '\\n', '    validation loss: 0.5927', '\\n', '\nhard_negative_pool_size = 10', '\\n', '    train accuracy: 1.0000', '\\n', '\nvalidation accuracy: 1.0000', '\\n', '    train loss: 0.4892', '\\n', '\nvalidation loss: 0.6821', '\\n', 'Execution time: a moment seconds (time limit is\nan hour).']", "['Dataset: synthetic', '\\n', '  LSTM Layers = 1', '\\n', '    train accuracy =\n1.0000', '\\n', '    validation accuracy = 0.5000', '\\n', '  LSTM Layers = 2',\n'\\n', '    train accuracy = 1.0000', '\\n', '    validation accuracy = 1.0000',\n'\\n', '  LSTM Layers = 3', '\\n', '    train accuracy = 0.9375', '\\n', '\nvalidation accuracy = 0.0000', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "['Dataset: synthetic (bidirectional=False)', '\\n', 'Train accuracy: 1.0000',\n'\\n', 'Validation accuracy: 1.0000', '\\n', 'Train loss: 0.3032', '\\n',\n'Validation loss: 0.4970', '\\n', '\\n', 'Dataset: synthetic\n(bidirectional=True)', '\\n', 'Train accuracy: 0.8125', '\\n', 'Validation\naccuracy: 0.5000', '\\n', 'Train loss: 0.6463', '\\n', 'Validation loss: 0.6593',\n'\\n', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'EPOCHS = 10', '\\n', 'train accuracy: 1.0000',\n'\\n', 'validation accuracy: 1.0000', '\\n', 'training loss: 0.1324', '\\n',\n'validation loss: 0.2506', '\\n', '\\n', 'EPOCHS = 30', '\\n', 'train accuracy:\n1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'training loss: 0.0000',\n'\\n', 'validation loss: 0.0000', '\\n', '\\n', 'EPOCHS = 50', '\\n', 'train\naccuracy: 1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'training loss:\n0.0000', '\\n', 'validation loss: 0.0208', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'EPOCHS = 10', '\\n', 'train accuracy: 1.0000',\n'\\n', 'validation accuracy: 1.0000', '\\n', 'training loss: 0.1324', '\\n',\n'validation loss: 0.2506', '\\n', '\\n', 'EPOCHS = 30', '\\n', 'train accuracy:\n1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'training loss: 0.0000',\n'\\n', 'validation loss: 0.0000', '\\n', '\\n', 'EPOCHS = 50', '\\n', 'train\naccuracy: 1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'training loss:\n0.0000', '\\n', 'validation loss: 0.0208', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Dataset: synthetic', '\\n', 'EPOCHS = 10', '\\n', 'train accuracy: 1.0000',\n'\\n', 'validation accuracy: 1.0000', '\\n', 'training loss: 0.1324', '\\n',\n'validation loss: 0.2506', '\\n', '\\n', 'EPOCHS = 30', '\\n', 'train accuracy:\n1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'training loss: 0.0000',\n'\\n', 'validation loss: 0.0000', '\\n', '\\n', 'EPOCHS = 50', '\\n', 'train\naccuracy: 1.0000', '\\n', 'validation accuracy: 1.0000', '\\n', 'training loss:\n0.0000', '\\n', 'validation loss: 0.0208', '\\n', '\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]};

// Add log directory path and stage info to the tree data
treeStructData.log_dir_path = window.location.pathname.split('/').slice(0, -1).join('/');
treeStructData.current_stage = window.location.pathname.includes('stage_')
  ? window.location.pathname.split('stage_')[1].split('/')[0]
  : 'Stage_1';

// Initialize background color
window.bgColCurrent = bgCol;

// Function to set background color that can be called from the console
function setBackgroundColor(color) {
  // Update the global color
  updateBackgroundColor(color);

  // Refresh the current sketch to apply the new background color
  if (currentStage) {
    startSketch(currentStage);
  }
}

// Load all stage data and initialize the visualization
loadAllStageData(treeStructData);

    </script>
    <title>AI Scientist-v2 Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background-color: #ffffff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      }
      #canvas-container {
        position: absolute;
        left: 0;
        top: 0;
        width: 40vw;
        height: 100vh;
        background-color: inherit;
        padding-top: 40px;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 100vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
        overflow: auto;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
      #plot_plan {
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
        white-space: pre-wrap;
      }
      #exec_time_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exec_time {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #exc_info {
        margin-top: 20px;
        padding: 10px;
        background-color: #2c1f1f;
        border-left: 3px solid #ff5555;
        color: #f2f0e7;
      }
      #metrics {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
      }
      #vlm_feedback {
        margin-top: 20px;
        padding: 10px;
        background-color: #1f2c2f;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      #vlm_feedback p {
        margin: 0.5em 0;
        white-space: pre-wrap;
      }
      .datasets_successfully_tested {
        margin-top: 20px;
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        border-left: 3px solid #55ff55;
      }
      .plots-container {
        float: right;
        width: 50vw;
        padding: 1rem;
        background-color: #282c34;
        margin-top: 1rem;
      }

      .plot-item {
        flex: 1 1 300px;
        max-width: 100%;
        margin-bottom: 1rem;
        white-space: pre-wrap;
      }

      .plot-item img {
        width: 100%;
        height: auto;
        border-radius: 4px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        display: block;
      }

      .metric-group {
        margin-bottom: 20px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 4px;
      }

      .metric-table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 10px;
      }

      .metric-table th,
      .metric-table td {
        padding: 8px;
        text-align: left;
        border: 1px solid #ddd;
      }

      .metric-table th {
        background-color: #363b44;
      }

      /* Styles for tabs */
      .tabs-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 49vw;
        background-color: #000000;
        z-index: 10;
        display: flex;
        padding: 0;
      }

      .tab {
        cursor: pointer;
        padding: 10px 15px;
        background-color: #333;
        color: #f2f0e7;
        border: none;
        outline: none;
        transition: background-color 0.3s;
        flex: 1;
        text-align: center;
      }

      .tab:hover {
        background-color: #444;
      }

      .tab.active {
        background-color: #4c76af;
        font-weight: bold;
      }

      .tab.disabled {
        opacity: 0.5;
        cursor: not-allowed;
        background-color: #282c34;
      }

      .tab-content {
        display: none;
        padding-top: 40px; /* Space for tabs */
      }

      .tab-content.active {
        display: block;
      }

      .stage-info {
        padding: 10px;
        background-color: #282c34;
        color: #f2f0e7;
        margin-bottom: 10px;
        font-size: 0.9em;
      }

      .stage-status {
        display: inline-block;
        padding: 3px 6px;
        border-radius: 3px;
        margin-left: 8px;
        font-size: 0.8em;
      }

      .stage-status.completed {
        background-color: #4caf50;
      }

      .stage-status.in-progress {
        background-color: #2196f3;
      }

      .stage-status.not-started {
        background-color: #9e9e9e;
      }
    </style>
  </head>
  <body>
    <div class="tabs-container" id="stage-tabs">
      <button class="tab" data-stage="Stage_1" onclick="selectStage('Stage_1')">Stage 1</button>
      <button class="tab" data-stage="Stage_2" onclick="selectStage('Stage_2')">Stage 2</button>
      <button class="tab" data-stage="Stage_3" onclick="selectStage('Stage_3')">Stage 3</button>
      <button class="tab" data-stage="Stage_4" onclick="selectStage('Stage_4')">Stage 4</button>
    </div>

    <div id="canvas-container"></div>

    <pre id="text-container">
        <div id="stage-info" class="stage-info"></div>
        <div id="plan"></div>
        <hr>
        <div id="exc_info"></div>
        <hr>
        <div id="exec_time"></div>
        <hr>
        <div id="exec_time_feedback"></div>
        <hr>
        <div id="metrics"></div>
        <hr>
        <div id="plot_plan"></div>
        <hr>
        <div class="plots-container" id="plots"></div>
        <hr>
        <div id="vlm_feedback"></div>
        <hr>
        <div id="datasets_successfully_tested"></div>
        <hr>
        <code id="code" class="language-python"></code>
        <hr>
        <code id="plot_code" class="language-python"></code>
    </pre>
  </body>
</html>

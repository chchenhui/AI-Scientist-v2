\begin{thebibliography}{14}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Gal \& Ghahramani(2015)Gal and Ghahramani]{gal2015dropoutaa}
Y.~Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock pp.\  1050--1059, 2015.

\bibitem[Guu et~al.(2020)Guu, Lee, Tung, Pasupat, and Chang]{guu2020realmrl}
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.
\newblock Realm: Retrieval-augmented language model pre-training.
\newblock \emph{ArXiv}, abs/2002.08909, 2020.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and
  Zettlemoyer]{joshi2017triviaqaal}
Mandar Joshi, Eunsol Choi, Daniel~S. Weld, and Luke Zettlemoyer.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for
  reading comprehension.
\newblock \emph{ArXiv}, abs/1705.03551, 2017.

\bibitem[Karpukhin et~al.(2020)Karpukhin, O{\u{g}}uz, Min, Lewis, Wu, Edunov,
  Chen, and tau Yih]{karpukhin2020densepr}
Vladimir Karpukhin, Barlas O{\u{g}}uz, Sewon Min, Patrick Lewis, Ledell~Yu Wu,
  Sergey Edunov, Danqi Chen, and Wen tau Yih.
\newblock Dense passage retrieval for open-domain question answering.
\newblock \emph{ArXiv}, abs/2004.04906, 2020.

\bibitem[Lee et~al.(2023)Lee, Kim, Lee, Lee, Park, Lee, and
  Jung]{lee2023askingcq}
Dongryeol Lee, Segwang Kim, Minwoo Lee, Hwanhee Lee, Joonsuk Park, Sang-Woo
  Lee, and Kyomin Jung.
\newblock Asking clarification questions to handle ambiguity in open-domain qa.
\newblock \emph{ArXiv}, abs/2305.13808, 2023.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal,
  Kuttler, Lewis, tau Yih, Rockt{\"a}schel, Riedel, and
  Kiela]{lewis2020retrievalaugmentedgf}
Patrick Lewis, Ethan Perez, Aleksandara Piktus, F.~Petroni, Vladimir Karpukhin,
  Naman Goyal, Heinrich Kuttler, M.~Lewis, Wen tau Yih, Tim Rockt{\"a}schel,
  Sebastian Riedel, and Douwe Kiela.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock \emph{ArXiv}, abs/2005.11401, 2020.

\bibitem[Lin et~al.(2021)Lin, Hilton, and Evans]{lin2021truthfulqamh}
Stephanie~C. Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock pp.\  3214--3252, 2021.

\bibitem[Min et~al.(2020)Min, Michael, Hajishirzi, and
  Zettlemoyer]{min2020ambigqaaa}
Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer.
\newblock Ambigqa: Answering ambiguous open-domain questions.
\newblock pp.\  5783--5797, 2020.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang]{rajpurkar2016squad1q}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock pp.\  2383--2392, 2016.

\bibitem[Robertson \& Zaragoza(2009)Robertson and Zaragoza]{robertson2009thepr}
S.~Robertson and H.~Zaragoza.
\newblock The probabilistic relevance framework: Bm25 and beyond.
\newblock \emph{Found. Trends Inf. Retr.}, 3:\penalty0 333--389, 2009.

\bibitem[Tix(2024)]{tix2024followupqi}
Bernadette Tix.
\newblock Follow-up questions improve documents generated by large language
  models.
\newblock \emph{ArXiv}, abs/2407.12017, 2024.

\bibitem[Wang et~al.(2023)Wang, Li, Sun, and Liu]{wang2023selfknowledgegr}
Yile Wang, Peng Li, Maosong Sun, and Yang Liu.
\newblock Self-knowledge guided retrieval augmentation for large language
  models.
\newblock pp.\  10303--10315, 2023.

\bibitem[Zhao et~al.(2024)Zhao, Dou, and Zhou]{zhao2024generatingic}
Ziliang Zhao, Zhicheng Dou, and Yujia Zhou.
\newblock \emph{Generating Intent-aware Clarifying Questions in Conversational
  Information Retrieval Systems}.
\newblock 2024.

\bibitem[Zubkova et~al.(2025)Zubkova, Park, and Lee]{zubkova2025sugarlc}
Hanna Zubkova, Ji-Hoon Park, and Seong-Whan Lee.
\newblock Sugar: Leveraging contextual confidence for smarter retrieval.
\newblock \emph{ArXiv}, abs/2501.04899, 2025.

\end{thebibliography}

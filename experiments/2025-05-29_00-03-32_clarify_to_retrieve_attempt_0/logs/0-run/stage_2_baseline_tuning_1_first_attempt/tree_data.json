{"edges": [[0, 12], [0, 4], [0, 3], [0, 9], [0, 2], [0, 7], [0, 5], [0, 1], [1, 6], [3, 8], [7, 10], [7, 11], [9, 15], [9, 13], [9, 14], [9, 16]], "layout": [[0.42105263157894735, 0.0], [0.0, 0.5], [0.10526315789473684, 0.5], [0.21052631578947367, 0.5], [0.3157894736842105, 0.5], [0.42105263157894735, 0.5], [0.0, 1.0], [0.5263157894736842, 0.5], [0.21052631578947367, 1.0], [0.8421052631578947, 0.5], [0.47368421052631576, 1.0], [0.5789473684210527, 1.0], [0.9473684210526315, 0.5], [0.6842105263157895, 1.0], [0.7894736842105263, 1.0], [0.8947368421052632, 1.0], [1.0, 1.0]], "plan": ["We simulate a synthetic XOR classification task where each sample has two\nfeatures but the second feature is hidden until a clarification is triggered. A\nsmall two-layer MLP with dropout is trained on the full features to learn the\nXOR mapping. During evaluation, we mask the second feature to mimic query\nambiguity and perform MC\u2010dropout to estimate per\u2010sample uncertainty. If the\nvariance of softmax outputs across MC passes exceeds a fixed threshold, we\nsimulate a clarification by revealing the hidden feature and re\u2010predict. We\ntrack baseline and clarified accuracies, count the total number of\nclarifications, and compute the Clarification Efficiency Score (CES) as the\naccuracy improvement per clarification turn. This process is run on both\ntraining and validation sets every epoch. We print validation loss each epoch,\nrecord CES and losses in an experiment_data structure, and store predictions\nplus ground truths. The script runs on GPU if available and saves all results to\n'./working/experiment_data.npy'.", "Hyperparam tuning name: epochs. Below is a single\u2010file Python script that sweeps\nover epoch counts [10, 50, 100], training a fresh MLP for each setting. For\nevery run we log per\u2010epoch training/validation losses and CES metrics, and save\nthe final validation predictions and ground truths. A helper `compute_CES`\nfunction encapsulates the MC\u2010dropout clarity score computation, keeping the main\nloop concise. All results are aggregated in a nested `experiment_data` dict and\nsaved as `experiment_data.npy`.", "Hyperparam tuning name: learning_rate. I will sweep over a grid of learning\nrates by reinitializing the model and optimizer for each rate, running the full\ntraining and CES\u2010evaluation loop, and collecting per\u2010epoch losses, CES metrics,\npredictions, and ground truths. Each learning rate\u2019s results are stored under a\ntop\u2010level \u201clearning_rate_sweep\u201d key, nested by the stringified lr value. After\ncompleting all runs, the entire `experiment_data` dictionary is saved as\n`experiment_data.npy` in the working directory. The script is self\u2010contained and\ncan be executed as\u2010is.", "Hyperparam tuning name: batch_size. We loop over a predefined list of batch\nsizes, reinitializing the data loaders, model, and optimizer for each setting.\nDuring each experiment, we log per-epoch train/validation losses, CES metrics,\nand full validation predictions/ground truths. All results are appended into a\nstructured `experiment_data` dictionary under the key `batch_size_tuning` with\nnested `synthetic_xor` data. Finally, we save the entire `experiment_data` as\n`'experiment_data.npy'` for downstream analysis.", "Hyperparam tuning name: dropout_rate. We extend the MLP to accept a variable\ndropout rate and loop over a list of rates to reinitialize and train from\nscratch. For each rate we run the full 10\u2010epoch training, compute train/val\nlosses and CES metrics, and collect predictions and ground truth per epoch. All\nresults are organized into a nested experiment_data dict under\n\u201cdropout_rate_tuning\u2192synthetic_xor,\u201d including a dropout_rates list, then saved\nvia np.save at the end.", "Hyperparam tuning name: threshold. Below is a single\u2010file script that sweeps\nover several MC\u2010dropout variance thresholds, retrains a fresh XOR MLP for each,\nand records per\u2010epoch train/val losses and CES metrics as well as validation\npredictions and ground truths. All results are stored under\n`experiment_data['hyperparam_tuning_type_1']['synthetic_xor']` and finally saved\nto `working/experiment_data.npy`. You can adjust the `thresholds`, `epochs`, or\nother settings as desired.", "I\u2019ll detach the softmax outputs before calling `.numpy()` to prevent the\n\u201crequires grad\u201d error. Specifically, in `compute_CES`, change", "Hyperparam tuning name: weight_decay. Below is a sweep over four weight_decay\nsettings: for each L2 penalty we re-init the MLP and Adam optimizer, run the\n10-epoch train/MC-dropout evaluate cycle collecting per-epoch CES, losses, and\nval predictions/ground truths. All runs are stored under a top-level\n\"weight_decay\" key in `experiment_data` with matching hyperparams, then saved to\n`experiment_data.npy`.", "We introduce the variance threshold as a hyperparameter to ensure that MC-\ndropout triggers clarifications appropriately (avoiding prolonged\nzero\u2010clarification runs), and we also add learning\u2010rate tuning. Two new\nHuggingFace tabular datasets (Titanic and Palmer Penguins) are loaded and\nfiltered to create binary tasks on two numeric features; features are\nstandardized per training split to guarantee proper model inputs. We retain the\n2-input MLP architecture and scan batch sizes, learning rates, and thresholds,\nrecording losses, CES metrics, predictions, ground-truth labels, and\nhyperparameters for every run. Finally, we save the full `experiment_data` dict\nto disk as a NumPy file.", "Hyperparam tuning name: hidden_layer_size. We introduce an outer loop over\nhidden_layer_sizes = [4,8,16,32,64] and for each size we instantiate an MLP with\nthat hidden size, train for a fixed number of epochs, compute train/validation\nlosses and CES metrics, and collect predictions and ground truth each epoch. We\naccumulate these values in nested lists under\nexperiment_data['hidden_layer_size']['synthetic_xor'], then save the entire dict\nas a numpy file. The MLP class now accepts a hidden_size parameter, and all\nexperiment data (metrics, losses, predictions, ground truth) is saved in\n`working/experiment_data.npy` at the end.", "We refactor the uncertainty\u2010driven query clarification metric to perform\nMC\u2010dropout in a fully vectorized way per batch, toggling the model to train only\nfor sampling and back to eval for deterministic predictions, which fixes the\nzero\u2010metric bug caused by per\u2010sample mode switches. The new\n`compute_accuracy_gain` computes base and clarified predictions in bulk,\naggregates variance, and computes \u201caccuracy gain per clarification\u201d correctly.\nIn the training loop we move all tensors/models to the device, track and print\nvalidation loss and the new metric each epoch, and save results at the end.", "We tune the training recipe by increasing epochs to 15, raising the batch size\nto 128, and lowering the learning rate to 5e-4 while still sweeping over weight\ndecay.  We add two HuggingFace benchmarks: a binary iris subset (setosa vs.\nversicolor, using sepal measurements) and a sampled Adult income dataset\n(predicting >50K from age and hours-per-week), each reduced to two numeric\nfeatures and split into train/val sets.  All data loading, preprocessing, model\ninstantiation, loss, and CES evaluation use the same consistency logic and move\ntensors/models to GPU when available.  We collect per-epoch CES, losses,\npredictions, and labels for each dataset\u00d7hyperparam setting and save them in a\nsingle numpy file.", "Hyperparam tuning name: mc_T. We will loop over different mc_T values,\nreinitialize and train a fresh model for each, and record per-epoch train/val\nlosses, CES metrics, predictions, and ground truth. Each run\u2019s data will be\nstored under experiment_data['mc_T_tuning']['synthetic_xor'][mc_T] and in the\nend saved via np.save to working/experiment_data.npy. This single-file script is\nself-contained and executable as-is.", "Seed node", "Seed node", "Seed node", "Aggregate results from multiple seeds"], "code": ["import os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# GPU/CPU setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic XOR data\ndef make_xor(n):\n    X = np.random.rand(n, 2)\n    y = ((X[:, 0] > 0.5) ^ (X[:, 1] > 0.5)).astype(int)\n    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n\n\ntrain_X, train_y = make_xor(2000)\nval_X, val_y = make_xor(500)\ntrain_ds = TensorDataset(train_X, train_y)\nval_ds = TensorDataset(val_X, val_y)\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=64)\n\n\n# Model\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 10)\n        self.drop = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(10, 2)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.drop(x)\n        return self.fc2(x)\n\n\nmodel = MLP().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# Experiment logging\nexperiment_data = {\n    \"synthetic_xor\": {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n}\n\n# Training + evaluation\nepochs = 10\nmc_T = 5\nthreshold = 0.02\nfor epoch in range(1, epochs + 1):\n    # Training\n    model.train()\n    total_loss, total_corr = 0.0, 0\n    for Xb, yb in train_loader:\n        Xb, yb = Xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        out = model(Xb)\n        loss = criterion(out, yb)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * Xb.size(0)\n        total_corr += (out.argmax(1) == yb).sum().item()\n    train_loss = total_loss / len(train_ds)\n    experiment_data[\"synthetic_xor\"][\"losses\"][\"train\"].append(train_loss)\n\n    # Compute CES on train\n    model.eval()\n    base_corr, clar_corr, clar_count = 0, 0, 0\n    with torch.no_grad():\n        for Xb, yb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            X_mask = Xb.clone()\n            X_mask[:, 1] = 0\n            out_base = model(X_mask)\n            preds_base = out_base.argmax(1)\n            base_corr += (preds_base == yb).sum().item()\n            for i in range(Xb.size(0)):\n                xi = X_mask[i : i + 1]\n                # MC-dropout\n                model.train()\n                ps = []\n                for _ in range(mc_T):\n                    p = torch.softmax(model(xi), dim=1)\n                    ps.append(p.cpu().numpy())\n                ps = np.stack(ps, 0)\n                var = ps.var(0).sum()\n                model.eval()\n                if var > threshold:\n                    clar_count += 1\n                    out_c = model(Xb[i : i + 1])\n                    clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                else:\n                    clar_corr += (preds_base[i] == yb[i]).item()\n    base_acc_tr = base_corr / len(train_ds)\n    clar_acc_tr = clar_corr / len(train_ds)\n    avg_ct_tr = clar_count / len(train_ds) if len(train_ds) else 0\n    CES_tr = (clar_acc_tr - base_acc_tr) / avg_ct_tr if avg_ct_tr > 0 else 0.0\n    experiment_data[\"synthetic_xor\"][\"metrics\"][\"train\"].append(CES_tr)\n\n    # Validation loss\n    model.eval()\n    val_loss, _ = 0.0, 0\n    with torch.no_grad():\n        for Xb, yb in val_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            out = model(Xb)\n            val_loss += criterion(out, yb).item() * Xb.size(0)\n    val_loss /= len(val_ds)\n    experiment_data[\"synthetic_xor\"][\"losses\"][\"val\"].append(val_loss)\n    print(f\"Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n    # Compute CES on val\n    base_corr, clar_corr, clar_count = 0, 0, 0\n    with torch.no_grad():\n        for Xb, yb in val_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            X_mask = Xb.clone()\n            X_mask[:, 1] = 0\n            out_base = model(X_mask)\n            preds_base = out_base.argmax(1)\n            base_corr += (preds_base == yb).sum().item()\n            for i in range(Xb.size(0)):\n                xi = X_mask[i : i + 1]\n                model.train()\n                ps = []\n                for _ in range(mc_T):\n                    p = torch.softmax(model(xi), dim=1)\n                    ps.append(p.cpu().numpy())\n                ps = np.stack(ps, 0)\n                var = ps.var(0).sum()\n                model.eval()\n                if var > threshold:\n                    clar_count += 1\n                    out_c = model(Xb[i : i + 1])\n                    clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                else:\n                    clar_corr += (preds_base[i] == yb[i]).item()\n    base_acc_val = base_corr / len(val_ds)\n    clar_acc_val = clar_corr / len(val_ds)\n    avg_ct_val = clar_count / len(val_ds) if len(val_ds) else 0\n    CES_val = (clar_acc_val - base_acc_val) / avg_ct_val if avg_ct_val > 0 else 0.0\n    experiment_data[\"synthetic_xor\"][\"metrics\"][\"val\"].append(CES_val)\n\n    # Save predictions and ground truth for this epoch\n    preds_list, gts_list = [], []\n    with torch.no_grad():\n        for Xb, yb in val_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            out = model(Xb)\n            preds_list.append(out.argmax(1).cpu().numpy())\n            gts_list.append(yb.cpu().numpy())\n    experiment_data[\"synthetic_xor\"][\"predictions\"].append(np.concatenate(preds_list))\n    experiment_data[\"synthetic_xor\"][\"ground_truth\"].append(np.concatenate(gts_list))\n\n    print(f\"Epoch {epoch}: train_CES = {CES_tr:.4f}, val_CES = {CES_val:.4f}\")\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# setup working directory and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# synthetic XOR dataset\ndef make_xor(n):\n    X = np.random.rand(n, 2)\n    y = ((X[:, 0] > 0.5) ^ (X[:, 1] > 0.5)).astype(int)\n    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n\n\ntrain_X, train_y = make_xor(2000)\nval_X, val_y = make_xor(500)\ntrain_loader = DataLoader(TensorDataset(train_X, train_y), batch_size=64, shuffle=True)\nval_loader = DataLoader(TensorDataset(val_X, val_y), batch_size=64)\n\n\n# model definition\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 10)\n        self.drop = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(10, 2)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.drop(x)\n        return self.fc2(x)\n\n\n# loss, MC\u2010drop parameters\ncriterion = nn.CrossEntropyLoss()\nmc_T, threshold = 5, 0.02\n\n\n# helper to compute CES score over a loader\ndef compute_CES(loader, model):\n    base_corr = clar_corr = clar_count = 0\n    total = len(loader.dataset)\n    for Xb, yb in loader:\n        Xb, yb = Xb.to(device), yb.to(device)\n        X_mask = Xb.clone()\n        X_mask[:, 1] = 0\n        out_base = model(X_mask)\n        preds_base = out_base.argmax(1)\n        base_corr += (preds_base == yb).sum().item()\n        for i in range(Xb.size(0)):\n            xi = X_mask[i : i + 1]\n            # MC\u2010dropout sampling\n            model.train()\n            ps = [torch.softmax(model(xi), dim=1).cpu().numpy() for _ in range(mc_T)]\n            model.eval()\n            var = np.stack(ps, 0).var(0).sum()\n            if var > threshold:\n                clar_count += 1\n                out_c = model(Xb[i : i + 1])\n                clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n            else:\n                clar_corr += (preds_base[i] == yb[i]).item()\n    base_acc = base_corr / total\n    clar_acc = clar_corr / total\n    avg_ct = clar_count / total if total > 0 else 0\n    return (clar_acc - base_acc) / avg_ct if avg_ct > 0 else 0.0\n\n\n# hyperparameter sweep over number of epochs\nepoch_list = [10, 50, 100]\nexperiment_data = {\n    \"epoch_sweep\": {\n        \"synthetic_xor\": {\n            \"epoch_settings\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor epochs in epoch_list:\n    experiment_data[\"epoch_sweep\"][\"synthetic_xor\"][\"epoch_settings\"].append(epochs)\n    model = MLP().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    run_tr_losses, run_val_losses = [], []\n    run_tr_CES, run_val_CES = [], []\n    for epoch in range(1, epochs + 1):\n        # training\n        model.train()\n        tot_loss = 0\n        for Xb, yb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            optimizer.step()\n            tot_loss += loss.item() * Xb.size(0)\n        train_loss = tot_loss / len(train_loader.dataset)\n        run_tr_losses.append(train_loss)\n        model.eval()\n        CES_tr = compute_CES(train_loader, model)\n        run_tr_CES.append(CES_tr)\n        # validation\n        val_loss = 0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                val_loss += criterion(out, yb).item() * Xb.size(0)\n        val_loss /= len(val_loader.dataset)\n        run_val_losses.append(val_loss)\n        CES_val = compute_CES(val_loader, model)\n        run_val_CES.append(CES_val)\n        print(\n            f\"Run={epochs} Epoch={epoch} train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n            f\"train_CES={CES_tr:.4f} val_CES={CES_val:.4f}\"\n        )\n    d = experiment_data[\"epoch_sweep\"][\"synthetic_xor\"]\n    d[\"losses\"][\"train\"].append(run_tr_losses)\n    d[\"losses\"][\"val\"].append(run_val_losses)\n    d[\"metrics\"][\"train\"].append(run_tr_CES)\n    d[\"metrics\"][\"val\"].append(run_val_CES)\n    # final run predictions and ground truth\n    preds, gts = [], []\n    model.eval()\n    with torch.no_grad():\n        for Xb, yb in val_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            preds.append(model(Xb).argmax(1).cpu().numpy())\n            gts.append(yb.cpu().numpy())\n    d[\"predictions\"].append(np.concatenate(preds))\n    d[\"ground_truth\"].append(np.concatenate(gts))\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic XOR data\ndef make_xor(n):\n    X = np.random.rand(n, 2)\n    y = ((X[:, 0] > 0.5) ^ (X[:, 1] > 0.5)).astype(int)\n    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n\n\ntrain_X, train_y = make_xor(2000)\nval_X, val_y = make_xor(500)\ntrain_loader = DataLoader(TensorDataset(train_X, train_y), batch_size=64, shuffle=True)\nval_loader = DataLoader(TensorDataset(val_X, val_y), batch_size=64)\n\n\n# Model definition\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 10)\n        self.drop = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(10, 2)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.drop(x)\n        return self.fc2(x)\n\n\n# Hyperparameter sweep setup\nlrs = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\nexperiment_data = {\"learning_rate_sweep\": {\"synthetic_xor\": {}}}\n\n# Common training settings\ncriterion = nn.CrossEntropyLoss()\nepochs = 10\nmc_T = 5\nthreshold = 0.02\n\n# Sweep loop\nfor lr in lrs:\n    key = f\"lr_{lr}\"\n    print(f\"\\nStarting run with learning rate = {lr}\")\n    # initialize storage for this run\n    run_data = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    # build model & optimizer\n    model = MLP().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    # run epochs\n    for epoch in range(1, epochs + 1):\n        # train\n        model.train()\n        total_loss, total_corr = 0.0, 0\n        for Xb, yb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * Xb.size(0)\n            total_corr += (out.argmax(1) == yb).sum().item()\n        train_loss = total_loss / len(train_loader.dataset)\n        run_data[\"losses\"][\"train\"].append(train_loss)\n        # compute CES on train\n        model.eval()\n        base_corr, clar_corr, clar_count = 0, 0, 0\n        with torch.no_grad():\n            for Xb, yb in train_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    var = np.stack(ps, 0).var(0).sum()\n                    model.eval()\n                    if var > threshold:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += (preds_base[i] == yb[i]).item()\n        base_acc_tr = base_corr / len(train_loader.dataset)\n        clar_acc_tr = clar_corr / len(train_loader.dataset)\n        avg_ct_tr = (\n            clar_count / len(train_loader.dataset) if len(train_loader.dataset) else 0\n        )\n        CES_tr = (clar_acc_tr - base_acc_tr) / avg_ct_tr if avg_ct_tr > 0 else 0.0\n        run_data[\"metrics\"][\"train\"].append(CES_tr)\n\n        # val loss\n        val_loss = 0.0\n        model.eval()\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                val_loss += criterion(out, yb).item() * Xb.size(0)\n        val_loss /= len(val_loader.dataset)\n        run_data[\"losses\"][\"val\"].append(val_loss)\n\n        # compute CES on val\n        base_corr, clar_corr, clar_count = 0, 0, 0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    var = np.stack(ps, 0).var(0).sum()\n                    model.eval()\n                    if var > threshold:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += (preds_base[i] == yb[i]).item()\n        base_acc_val = base_corr / len(val_loader.dataset)\n        clar_acc_val = clar_corr / len(val_loader.dataset)\n        avg_ct_val = (\n            clar_count / len(val_loader.dataset) if len(val_loader.dataset) else 0\n        )\n        CES_val = (clar_acc_val - base_acc_val) / avg_ct_val if avg_ct_val > 0 else 0.0\n        run_data[\"metrics\"][\"val\"].append(CES_val)\n\n        # save preds & gts\n        preds_list, gts_list = [], []\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                preds_list.append(model(Xb).argmax(1).cpu().numpy())\n                gts_list.append(yb.cpu().numpy())\n        run_data[\"predictions\"].append(np.concatenate(preds_list))\n        run_data[\"ground_truth\"].append(np.concatenate(gts_list))\n\n        print(\n            f\"LR {lr:.0e} E{epoch}: val_loss={val_loss:.4f} train_CES={CES_tr:.4f} val_CES={CES_val:.4f}\"\n        )\n\n    # store run data\n    experiment_data[\"learning_rate_sweep\"][\"synthetic_xor\"][key] = run_data\n\n# save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Experiment data structure\nexperiment_data = {\n    \"batch_size_tuning\": {\n        \"synthetic_xor\": {\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n            \"batch_sizes\": [],\n        }\n    }\n}\n\n# Hyperparameters\nbatch_sizes = [16, 32, 64, 128, 256]\nepochs = 10\nmc_T = 5\nthreshold = 0.02\n\n\n# Synthetic XOR data generator\ndef make_xor(n):\n    X = np.random.rand(n, 2)\n    y = ((X[:, 0] > 0.5) ^ (X[:, 1] > 0.5)).astype(int)\n    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n\n\n# Prepare dataset once\ntrain_X, train_y = make_xor(2000)\nval_X, val_y = make_xor(500)\ntrain_ds = TensorDataset(train_X, train_y)\nval_ds = TensorDataset(val_X, val_y)\n\n\n# Model definition\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 10)\n        self.drop = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(10, 2)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.drop(x)\n        return self.fc2(x)\n\n\n# Loop over batch sizes\nfor bs in batch_sizes:\n    experiment_data[\"batch_size_tuning\"][\"synthetic_xor\"][\"batch_sizes\"].append(bs)\n    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=bs)\n\n    # Initialize model, loss, optimizer\n    model = MLP().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # Local logs for this batch size\n    bs_train_losses, bs_val_losses = [], []\n    bs_train_metrics, bs_val_metrics = [], []\n    bs_preds_epochs, bs_gts_epochs = [], []\n\n    for epoch in range(1, epochs + 1):\n        # Training\n        model.train()\n        total_loss, total_corr = 0.0, 0\n        for Xb, yb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * Xb.size(0)\n            total_corr += (out.argmax(1) == yb).sum().item()\n        train_loss = total_loss / len(train_ds)\n        bs_train_losses.append(train_loss)\n\n        # CES on train\n        model.eval()\n        base_corr, clar_corr, clar_count = 0, 0, 0\n        with torch.no_grad():\n            for Xb, yb in train_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    ps = np.stack(ps, 0)\n                    var = ps.var(0).sum()\n                    model.eval()\n                    if var > threshold:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += (preds_base[i] == yb[i]).item()\n        base_acc_tr = base_corr / len(train_ds)\n        clar_acc_tr = clar_corr / len(train_ds)\n        avg_ct_tr = clar_count / len(train_ds) if len(train_ds) else 0\n        CES_tr = (clar_acc_tr - base_acc_tr) / avg_ct_tr if avg_ct_tr > 0 else 0.0\n        bs_train_metrics.append(CES_tr)\n\n        # Validation loss\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                val_loss += criterion(out, yb).item() * Xb.size(0)\n        val_loss /= len(val_ds)\n        bs_val_losses.append(val_loss)\n        print(f\"Batch {bs} Epoch {epoch}: val_loss = {val_loss:.4f}\")\n\n        # CES on val\n        base_corr, clar_corr, clar_count = 0, 0, 0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    ps = np.stack(ps, 0)\n                    var = ps.var(0).sum()\n                    model.eval()\n                    if var > threshold:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += (preds_base[i] == yb[i]).item()\n        base_acc_val = base_corr / len(val_ds)\n        clar_acc_val = clar_corr / len(val_ds)\n        avg_ct_val = clar_count / len(val_ds) if len(val_ds) else 0\n        CES_val = (clar_acc_val - base_acc_val) / avg_ct_val if avg_ct_val > 0 else 0.0\n        bs_val_metrics.append(CES_val)\n\n        # Predictions & ground truth\n        preds, gts = [], []\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                preds.append(out.argmax(1).cpu().numpy())\n                gts.append(yb.cpu().numpy())\n        bs_preds_epochs.append(np.concatenate(preds))\n        bs_gts_epochs.append(np.concatenate(gts))\n\n        print(\n            f\"Batch {bs} Epoch {epoch}: train_CES = {CES_tr:.4f}, val_CES = {CES_val:.4f}\"\n        )\n\n    # Append to main experiment data\n    exp = experiment_data[\"batch_size_tuning\"][\"synthetic_xor\"]\n    exp[\"losses\"][\"train\"].append(bs_train_losses)\n    exp[\"losses\"][\"val\"].append(bs_val_losses)\n    exp[\"metrics\"][\"train\"].append(bs_train_metrics)\n    exp[\"metrics\"][\"val\"].append(bs_val_metrics)\n    exp[\"predictions\"].append(bs_preds_epochs)\n    exp[\"ground_truth\"].append(bs_gts_epochs)\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic XOR data\ndef make_xor(n):\n    X = np.random.rand(n, 2)\n    y = ((X[:, 0] > 0.5) ^ (X[:, 1] > 0.5)).astype(int)\n    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n\n\ntrain_X, train_y = make_xor(2000)\nval_X, val_y = make_xor(500)\ntrain_ds = TensorDataset(train_X, train_y)\nval_ds = TensorDataset(val_X, val_y)\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=64)\n\n\n# MLP with variable dropout\nclass MLP(nn.Module):\n    def __init__(self, dropout_rate):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 10)\n        self.drop = nn.Dropout(dropout_rate)\n        self.fc2 = nn.Linear(10, 2)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.drop(x)\n        return self.fc2(x)\n\n\n# Hyperparams\ndropout_rates = [0.0, 0.1, 0.2, 0.3, 0.5, 0.7]\nepochs = 10\nmc_T = 5\nthreshold = 0.02\n\n# Experiment logging structure\nexperiment_data = {\n    \"dropout_rate_tuning\": {\n        \"synthetic_xor\": {\n            \"dropout_rates\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Sweep dropout rates\nfor dr in dropout_rates:\n    print(f\"\\n=== Sweeping dropout_rate = {dr} ===\")\n    # Init model, loss, optimizer\n    model = MLP(dropout_rate=dr).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    # Per-rate logs\n    loss_tr_list, loss_val_list = [], []\n    ces_tr_list, ces_val_list = [], []\n    preds_epoch_list, gts_epoch_list = [], []\n    # Train + eval\n    for epoch in range(1, epochs + 1):\n        # -- Train loss\n        model.train()\n        total_loss, total_corr = 0.0, 0\n        for Xb, yb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * Xb.size(0)\n            total_corr += (out.argmax(1) == yb).sum().item()\n        train_loss = total_loss / len(train_ds)\n        loss_tr_list.append(train_loss)\n        # -- Train CES\n        model.eval()\n        base_corr, clar_corr, clar_count = 0, 0, 0\n        with torch.no_grad():\n            for Xb, yb in train_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    var = np.stack(ps, 0).var(0).sum()\n                    model.eval()\n                    if var > threshold:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += (preds_base[i] == yb[i]).item()\n        base_acc_tr = base_corr / len(train_ds)\n        clar_acc_tr = clar_corr / len(train_ds)\n        avg_ct_tr = clar_count / len(train_ds) if len(train_ds) else 0\n        CES_tr = (clar_acc_tr - base_acc_tr) / avg_ct_tr if avg_ct_tr > 0 else 0.0\n        ces_tr_list.append(CES_tr)\n        # -- Validation loss\n        model.eval()\n        total_vloss = 0.0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                total_vloss += criterion(out, yb).item() * Xb.size(0)\n        val_loss = total_vloss / len(val_ds)\n        loss_val_list.append(val_loss)\n        # -- Validation CES\n        base_corr, clar_corr, clar_count = 0, 0, 0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    var = np.stack(ps, 0).var(0).sum()\n                    model.eval()\n                    if var > threshold:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += (preds_base[i] == yb[i]).item()\n        base_acc_val = base_corr / len(val_ds)\n        clar_acc_val = clar_corr / len(val_ds)\n        avg_ct_val = clar_count / len(val_ds) if len(val_ds) else 0\n        CES_val = (clar_acc_val - base_acc_val) / avg_ct_val if avg_ct_val > 0 else 0.0\n        ces_val_list.append(CES_val)\n        # -- Save predictions & gts for this epoch\n        preds_chunks, gts_chunks = [], []\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                preds_chunks.append(out.argmax(1).cpu().numpy())\n                gts_chunks.append(yb.cpu().numpy())\n        preds_epoch_list.append(np.concatenate(preds_chunks))\n        gts_epoch_list.append(np.concatenate(gts_chunks))\n        print(\n            f\"dr={dr} epoch={epoch}: tr_loss={train_loss:.4f}, val_loss={val_loss:.4f}, tr_CES={CES_tr:.4f}, val_CES={CES_val:.4f}\"\n        )\n    # Append per-rate results\n    ed = experiment_data[\"dropout_rate_tuning\"][\"synthetic_xor\"]\n    ed[\"dropout_rates\"].append(dr)\n    ed[\"losses\"][\"train\"].append(loss_tr_list)\n    ed[\"losses\"][\"val\"].append(loss_val_list)\n    ed[\"metrics\"][\"train\"].append(ces_tr_list)\n    ed[\"metrics\"][\"val\"].append(ces_val_list)\n    ed[\"predictions\"].append(preds_epoch_list)\n    ed[\"ground_truth\"].append(gts_epoch_list)\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic XOR data\ndef make_xor(n):\n    X = np.random.rand(n, 2)\n    y = ((X[:, 0] > 0.5) ^ (X[:, 1] > 0.5)).astype(int)\n    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n\n\ntrain_X, train_y = make_xor(2000)\nval_X, val_y = make_xor(500)\ntrain_ds = TensorDataset(train_X, train_y)\nval_ds = TensorDataset(val_X, val_y)\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=64)\n\n\n# Model definition\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 10)\n        self.drop = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(10, 2)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.drop(x)\n        return self.fc2(x)\n\n\n# Hyperparameters\nepochs = 10\nmc_T = 5\nthresholds = [0.005, 0.01, 0.02, 0.05, 0.1]\n\n# Prepare experiment_data structure\nexperiment_data = {\n    \"hyperparam_tuning_type_1\": {\n        \"synthetic_xor\": {\n            \"thresholds\": thresholds,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Sweep thresholds\nfor thr in thresholds:\n    print(f\"\\n=== Threshold = {thr} ===\")\n    # per-threshold containers\n    met_tr, met_val = [], []\n    los_tr, los_val = [], []\n    preds_epochs, gts_epochs = [], []\n\n    # new model & optimizer each threshold\n    model = MLP().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    for epoch in range(1, epochs + 1):\n        # Training\n        model.train()\n        total_loss, total_corr = 0.0, 0\n        for Xb, yb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * Xb.size(0)\n            total_corr += (out.argmax(1) == yb).sum().item()\n        train_loss = total_loss / len(train_ds)\n        los_tr.append(train_loss)\n\n        # Train CES\n        model.eval()\n        base_corr, clar_corr, clar_count = 0, 0, 0\n        with torch.no_grad():\n            for Xb, yb in train_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    var = np.stack(ps, 0).var(0).sum()\n                    model.eval()\n                    if var > thr:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += int(preds_base[i] == yb[i])\n        base_acc = base_corr / len(train_ds)\n        clar_acc = clar_corr / len(train_ds)\n        avg_ct = clar_count / len(train_ds) if len(train_ds) > 0 else 0\n        CES_tr = (clar_acc - base_acc) / avg_ct if avg_ct > 0 else 0.0\n        met_tr.append(CES_tr)\n\n        # Validation loss\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                val_loss += criterion(out, yb).item() * Xb.size(0)\n        val_loss /= len(val_ds)\n        los_val.append(val_loss)\n\n        # Val CES\n        base_corr, clar_corr, clar_count = 0, 0, 0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    var = np.stack(ps, 0).var(0).sum()\n                    model.eval()\n                    if var > thr:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += int(preds_base[i] == yb[i])\n        base_acc = base_corr / len(val_ds)\n        clar_acc = clar_corr / len(val_ds)\n        avg_ct = clar_count / len(val_ds) if len(val_ds) > 0 else 0\n        CES_val = (clar_acc - base_acc) / avg_ct if avg_ct > 0 else 0.0\n        met_val.append(CES_val)\n\n        # record predictions & gts\n        preds_list, gts_list = [], []\n        model.eval()\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb = Xb.to(device)\n                out = model(Xb)\n                preds_list.append(out.argmax(1).cpu().numpy())\n                gts_list.append(yb.numpy())\n        preds_epochs.append(np.concatenate(preds_list))\n        gts_epochs.append(np.concatenate(gts_list))\n\n        print(\n            f\"Thr {thr:.3f} Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, train_CES={CES_tr:.4f}, val_CES={CES_val:.4f}\"\n        )\n\n    # store per-threshold results\n    d = experiment_data[\"hyperparam_tuning_type_1\"][\"synthetic_xor\"]\n    d[\"metrics\"][\"train\"].append(met_tr)\n    d[\"metrics\"][\"val\"].append(met_val)\n    d[\"losses\"][\"train\"].append(los_tr)\n    d[\"losses\"][\"val\"].append(los_val)\n    d[\"predictions\"].append(preds_epochs)\n    d[\"ground_truth\"].append(gts_epochs)\n\n# Save all experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "ps = [torch.softmax(model(xi), dim=1).cpu().numpy() for _ in range(mc_T)]\n\n\nps = [torch.softmax(model(xi), dim=1).detach().cpu().numpy() for _ in range(mc_T)]\n\n\nimport os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# setup working directory and device\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# synthetic XOR dataset\ndef make_xor(n):\n    X = np.random.rand(n, 2)\n    y = ((X[:, 0] > 0.5) ^ (X[:, 1] > 0.5)).astype(int)\n    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n\n\ntrain_X, train_y = make_xor(2000)\nval_X, val_y = make_xor(500)\ntrain_loader = DataLoader(TensorDataset(train_X, train_y), batch_size=64, shuffle=True)\nval_loader = DataLoader(TensorDataset(val_X, val_y), batch_size=64)\n\n\n# model definition\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 10)\n        self.drop = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(10, 2)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.drop(x)\n        return self.fc2(x)\n\n\n# loss, MC\u2010drop parameters\ncriterion = nn.CrossEntropyLoss()\nmc_T, threshold = 5, 0.02\n\n\n# helper to compute CES score over a loader\ndef compute_CES(loader, model):\n    base_corr = clar_corr = clar_count = 0\n    total = len(loader.dataset)\n    for Xb, yb in loader:\n        Xb, yb = Xb.to(device), yb.to(device)\n        X_mask = Xb.clone()\n        X_mask[:, 1] = 0\n        out_base = model(X_mask)\n        preds_base = out_base.argmax(1)\n        base_corr += (preds_base == yb).sum().item()\n        for i in range(Xb.size(0)):\n            xi = X_mask[i : i + 1]\n            model.train()\n            ps = [\n                torch.softmax(model(xi), dim=1).detach().cpu().numpy()\n                for _ in range(mc_T)\n            ]\n            model.eval()\n            var = np.stack(ps, 0).var(0).sum()\n            if var > threshold:\n                clar_count += 1\n                out_c = model(Xb[i : i + 1])\n                clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n            else:\n                clar_corr += int(preds_base[i].item() == yb[i].item())\n    base_acc = base_corr / total\n    clar_acc = clar_corr / total\n    avg_ct = clar_count / total if total > 0 else 0\n    return (clar_acc - base_acc) / avg_ct if avg_ct > 0 else 0.0\n\n\n# hyperparameter sweep over number of epochs\nepoch_list = [10, 50, 100]\nexperiment_data = {\n    \"epoch_sweep\": {\n        \"synthetic_xor\": {\n            \"epoch_settings\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\nfor epochs in epoch_list:\n    experiment_data[\"epoch_sweep\"][\"synthetic_xor\"][\"epoch_settings\"].append(epochs)\n    model = MLP().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    run_tr_losses, run_val_losses = [], []\n    run_tr_CES, run_val_CES = [], []\n    for epoch in range(1, epochs + 1):\n        # training\n        model.train()\n        tot_loss = 0\n        for Xb, yb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            optimizer.step()\n            tot_loss += loss.item() * Xb.size(0)\n        train_loss = tot_loss / len(train_loader.dataset)\n        run_tr_losses.append(train_loss)\n\n        model.eval()\n        CES_tr = compute_CES(train_loader, model)\n        run_tr_CES.append(CES_tr)\n\n        # validation\n        val_loss = 0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                val_loss += criterion(out, yb).item() * Xb.size(0)\n        val_loss /= len(val_loader.dataset)\n        run_val_losses.append(val_loss)\n\n        CES_val = compute_CES(val_loader, model)\n        run_val_CES.append(CES_val)\n\n        print(\n            f\"Run={epochs} Epoch={epoch} train_loss={train_loss:.4f} \"\n            f\"val_loss={val_loss:.4f} train_CES={CES_tr:.4f} val_CES={CES_val:.4f}\"\n        )\n\n    d = experiment_data[\"epoch_sweep\"][\"synthetic_xor\"]\n    d[\"losses\"][\"train\"].append(run_tr_losses)\n    d[\"losses\"][\"val\"].append(run_val_losses)\n    d[\"metrics\"][\"train\"].append(run_tr_CES)\n    d[\"metrics\"][\"val\"].append(run_val_CES)\n\n    # final run predictions and ground truth\n    preds, gts = [], []\n    model.eval()\n    with torch.no_grad():\n        for Xb, yb in val_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            preds.append(model(Xb).argmax(1).cpu().numpy())\n            gts.append(yb.cpu().numpy())\n    d[\"predictions\"].append(np.concatenate(preds))\n    d[\"ground_truth\"].append(np.concatenate(gts))\n\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic XOR data\ndef make_xor(n):\n    X = np.random.rand(n, 2)\n    y = ((X[:, 0] > 0.5) ^ (X[:, 1] > 0.5)).astype(int)\n    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n\n\ntrain_X, train_y = make_xor(2000)\nval_X, val_y = make_xor(500)\ntrain_ds = TensorDataset(train_X, train_y)\nval_ds = TensorDataset(val_X, val_y)\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=64)\n\n\n# Model definition\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 10)\n        self.drop = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(10, 2)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.drop(x)\n        return self.fc2(x)\n\n\n# Hyperparameter sweep settings\nweight_decays = [0, 1e-5, 1e-4, 1e-3]\nepochs = 10\nmc_T = 5\nthreshold = 0.02\n\n# Experiment data container\nexperiment_data = {\n    \"weight_decay\": {\n        \"synthetic_xor\": {\n            \"hyperparams\": [],\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n\n# Helper for CES computation\ndef compute_ces(loader):\n    model.eval()\n    base_corr = clar_corr = clar_count = 0\n    for Xb, yb in loader:\n        Xb, yb = Xb.to(device), yb.to(device)\n        X_mask = Xb.clone()\n        X_mask[:, 1] = 0\n        out_base = model(X_mask)\n        preds_base = out_base.argmax(1)\n        base_corr += (preds_base == yb).sum().item()\n        for i in range(Xb.size(0)):\n            xi = X_mask[i : i + 1]\n            model.train()\n            ps = []\n            with torch.no_grad():\n                for _ in range(mc_T):\n                    p = torch.softmax(model(xi), dim=1)\n                    ps.append(p.cpu().numpy())\n            ps = np.stack(ps, 0)\n            var = ps.var(0).sum()\n            model.eval()\n            if var > threshold:\n                clar_count += 1\n                out_c = model(Xb[i : i + 1])\n                clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n            else:\n                clar_corr += (preds_base[i] == yb[i]).item()\n    N = len(loader.dataset)\n    base_acc = base_corr / N\n    clar_acc = clar_corr / N\n    avg_ct = clar_count / N if N > 0 else 0\n    return (clar_acc - base_acc) / avg_ct if avg_ct > 0 else 0.0\n\n\n# Main hyperparameter tuning loop\nfor wd in weight_decays:\n    print(f\"Running weight_decay = {wd}\")\n    data = experiment_data[\"weight_decay\"][\"synthetic_xor\"]\n    data[\"hyperparams\"].append(wd)\n    metrics_tr, metrics_val = [], []\n    losses_tr, losses_val = [], []\n    preds_all, gts_all = [], []\n\n    model = MLP().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n\n    for epoch in range(1, epochs + 1):\n        # Training\n        model.train()\n        total_loss = 0.0\n        for Xb, yb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * Xb.size(0)\n        train_loss = total_loss / len(train_ds)\n        losses_tr.append(train_loss)\n\n        # CES on train\n        CES_tr = compute_ces(train_loader)\n        metrics_tr.append(CES_tr)\n\n        # Validation loss\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                val_loss += criterion(out, yb).item() * Xb.size(0)\n        val_loss /= len(val_ds)\n        losses_val.append(val_loss)\n\n        # CES on val\n        CES_val = compute_ces(val_loader)\n        metrics_val.append(CES_val)\n\n        # Predictions and ground truth\n        model.eval()\n        preds_list, gts_list = [], []\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                preds_list.append(out.argmax(1).cpu().numpy())\n                gts_list.append(yb.cpu().numpy())\n        preds_all.append(np.concatenate(preds_list))\n        gts_all.append(np.concatenate(gts_list))\n\n        print(\n            f\"WD {wd} Epoch {epoch}: val_loss={val_loss:.4f}, train_CES={CES_tr:.4f}, val_CES={CES_val:.4f}\"\n        )\n\n    data[\"metrics\"][\"train\"].append(metrics_tr)\n    data[\"metrics\"][\"val\"].append(metrics_val)\n    data[\"losses\"][\"train\"].append(losses_tr)\n    data[\"losses\"][\"val\"].append(losses_val)\n    data[\"predictions\"].append(preds_all)\n    data[\"ground_truth\"].append(gts_all)\n\n# Save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\n\n# Working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic XOR generator\ndef make_xor(n):\n    X = np.random.rand(n, 2)\n    y = ((X[:, 0] > 0.5) ^ (X[:, 1] > 0.5)).astype(int)\n    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n\n\n# Load and preprocess Titanic\ntitanic = load_dataset(\"titanic\")[\"train\"]\ntitanic = titanic.filter(lambda x: x[\"age\"] is not None and x[\"fare\"] is not None)\nt_split = titanic.train_test_split(test_size=0.2, seed=42)\n\n\ndef prep_tab(ds, feats, label):\n    X = np.stack([[x[f] for f in feats] for x in ds], axis=0).astype(np.float32)\n    y = np.array(ds[label], dtype=np.int64)\n    return X, y\n\n\nX_tr_t, y_tr_t = prep_tab(t_split[\"train\"], [\"age\", \"fare\"], \"survived\")\nX_val_t, y_val_t = prep_tab(t_split[\"test\"], [\"age\", \"fare\"], \"survived\")\nmu_t, std_t = X_tr_t.mean(0), X_tr_t.std(0) + 1e-8\nX_tr_t = (X_tr_t - mu_t) / std_t\nX_val_t = (X_val_t - mu_t) / std_t\ntrain_X_t, train_y_t = torch.tensor(X_tr_t), torch.tensor(y_tr_t)\nval_X_t, val_y_t = torch.tensor(X_val_t), torch.tensor(y_val_t)\n\n# Load and preprocess Penguins (Adelie vs Chinstrap)\npeng = load_dataset(\"palmerpenguins\", \"penguins\")[\"train\"]\npeng = peng.filter(\n    lambda x: x[\"species\"] in [\"Adelie\", \"Chinstrap\"]\n    and x[\"bill_length_mm\"] is not None\n    and x[\"bill_depth_mm\"] is not None\n)\np_split = peng.train_test_split(test_size=0.2, seed=42)\n\n\ndef prep_peng(ds):\n    X = np.stack(\n        [[x[\"bill_length_mm\"], x[\"bill_depth_mm\"]] for x in ds], axis=0\n    ).astype(np.float32)\n    y = np.array([0 if x[\"species\"] == \"Adelie\" else 1 for x in ds], dtype=np.int64)\n    return X, y\n\n\nX_tr_p, y_tr_p = prep_peng(p_split[\"train\"])\nX_val_p, y_val_p = prep_peng(p_split[\"test\"])\nmu_p, std_p = X_tr_p.mean(0), X_tr_p.std(0) + 1e-8\nX_tr_p = (X_tr_p - mu_p) / std_p\nX_val_p = (X_val_p - mu_p) / std_p\ntrain_X_p, train_y_p = torch.tensor(X_tr_p), torch.tensor(y_tr_p)\nval_X_p, val_y_p = torch.tensor(X_val_p), torch.tensor(y_val_p)\n\n# Bundle datasets\ndatasets = {\n    \"synthetic_xor\": (*make_xor(2000), *make_xor(500)),\n    \"titanic\": (train_X_t, train_y_t, val_X_t, val_y_t),\n    \"penguins\": (train_X_p, train_y_p, val_X_p, val_y_p),\n}\n\n# Hyperparameters to scan\nbatch_sizes = [16, 32, 64, 128, 256]\nlearning_rates = [1e-3, 5e-4]\nthresholds = [0.01, 0.02]\nepochs = 10\nmc_T = 5\n\n# Experiment data\nexperiment_data = {}\nfor name in datasets:\n    experiment_data[name] = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n        \"hyperparams\": [],\n    }\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 10)\n        self.drop = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(10, 2)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.drop(x)\n        return self.fc2(x)\n\n\n# Run experiments\nfor name, (trX, trY, valX, valY) in datasets.items():\n    for bs in batch_sizes:\n        for lr in learning_rates:\n            for thr in thresholds:\n                experiment_data[name][\"hyperparams\"].append(\n                    {\"batch_size\": bs, \"lr\": lr, \"threshold\": thr}\n                )\n                # prepare loaders\n                train_ds = TensorDataset(trX, trY)\n                val_ds = TensorDataset(valX, valY)\n                train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n                val_loader = DataLoader(val_ds, batch_size=bs)\n                # model, loss, optim\n                model = MLP().to(device)\n                criterion = nn.CrossEntropyLoss()\n                optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n                bs_tr_losses, bs_val_losses = [], []\n                bs_tr_metrics, bs_val_metrics = [], []\n                # training loop\n                for epoch in range(1, epochs + 1):\n                    model.train()\n                    total_loss, total_corr = 0.0, 0\n                    for Xb, yb in train_loader:\n                        Xb, yb = Xb.to(device), yb.to(device)\n                        optimizer.zero_grad()\n                        out = model(Xb)\n                        loss = criterion(out, yb)\n                        loss.backward()\n                        optimizer.step()\n                        total_loss += loss.item() * Xb.size(0)\n                        total_corr += (out.argmax(1) == yb).sum().item()\n                    train_loss = total_loss / len(train_ds)\n                    bs_tr_losses.append(train_loss)\n\n                    # compute CES on train set\n                    model.eval()\n                    base_corr, clar_corr, clar_count = 0, 0, 0\n                    with torch.no_grad():\n                        for Xb, yb in train_loader:\n                            Xb, yb = Xb.to(device), yb.to(device)\n                            X_mask = Xb.clone()\n                            X_mask[:, 1] = 0\n                            out_base = model(X_mask)\n                            preds_base = out_base.argmax(1)\n                            base_corr += (preds_base == yb).sum().item()\n                            for i in range(Xb.size(0)):\n                                xi = X_mask[i : i + 1]\n                                model.train()\n                                ps = []\n                                for _ in range(mc_T):\n                                    p = torch.softmax(model(xi), dim=1)\n                                    ps.append(p.cpu().numpy())\n                                ps = np.stack(ps, 0)\n                                var = ps.var(0).sum()\n                                model.eval()\n                                if var > thr:\n                                    clar_count += 1\n                                    out_c = model(Xb[i : i + 1])\n                                    clar_corr += (\n                                        (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                                    )\n                                else:\n                                    clar_corr += int(preds_base[i] == yb[i])\n                    base_acc_tr = base_corr / len(train_ds)\n                    clar_acc_tr = clar_corr / len(train_ds)\n                    avg_ct_tr = clar_count / len(train_ds) if clar_count > 0 else 0\n                    CES_tr = (\n                        (clar_acc_tr - base_acc_tr) / avg_ct_tr\n                        if avg_ct_tr > 0\n                        else 0.0\n                    )\n                    bs_tr_metrics.append(CES_tr)\n\n                    # validation loss\n                    model.eval()\n                    vloss = 0.0\n                    with torch.no_grad():\n                        for Xb, yb in val_loader:\n                            Xb, yb = Xb.to(device), yb.to(device)\n                            vloss += criterion(model(Xb), yb).item() * Xb.size(0)\n                    val_loss = vloss / len(val_ds)\n                    bs_val_losses.append(val_loss)\n                    print(\n                        f\"Dataset {name}, bs {bs}, lr {lr}, thr {thr}, Epoch {epoch}: validation_loss = {val_loss:.4f}\"\n                    )\n\n                    # compute CES on val set\n                    base_corr, clar_corr, clar_count = 0, 0, 0\n                    with torch.no_grad():\n                        for Xb, yb in val_loader:\n                            Xb, yb = Xb.to(device), yb.to(device)\n                            X_mask = Xb.clone()\n                            X_mask[:, 1] = 0\n                            out_base = model(X_mask)\n                            preds_base = out_base.argmax(1)\n                            base_corr += (preds_base == yb).sum().item()\n                            for i in range(Xb.size(0)):\n                                xi = X_mask[i : i + 1]\n                                model.train()\n                                ps = []\n                                for _ in range(mc_T):\n                                    p = torch.softmax(model(xi), dim=1)\n                                    ps.append(p.cpu().numpy())\n                                ps = np.stack(ps, 0)\n                                var = ps.var(0).sum()\n                                model.eval()\n                                if var > thr:\n                                    clar_count += 1\n                                    out_c = model(Xb[i : i + 1])\n                                    clar_corr += (\n                                        (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                                    )\n                                else:\n                                    clar_corr += int(preds_base[i] == yb[i])\n                    base_acc_val = base_corr / len(val_ds)\n                    clar_acc_val = clar_corr / len(val_ds)\n                    avg_ct_val = clar_count / len(val_ds) if clar_count > 0 else 0\n                    CES_val = (\n                        (clar_acc_val - base_acc_val) / avg_ct_val\n                        if avg_ct_val > 0\n                        else 0.0\n                    )\n                    bs_val_metrics.append(CES_val)\n                    print(\n                        f\"Dataset {name}, bs {bs}, lr {lr}, thr {thr}, Epoch {epoch}: train_CES = {CES_tr:.4f}, val_CES = {CES_val:.4f}\"\n                    )\n\n                # record run data\n                ed = experiment_data[name]\n                ed[\"losses\"][\"train\"].append(bs_tr_losses)\n                ed[\"losses\"][\"val\"].append(bs_val_losses)\n                ed[\"metrics\"][\"train\"].append(bs_tr_metrics)\n                ed[\"metrics\"][\"val\"].append(bs_val_metrics)\n                # final predictions & ground truth\n                preds, gts = [], []\n                with torch.no_grad():\n                    for Xb, yb in val_loader:\n                        Xb, yb = Xb.to(device), yb.to(device)\n                        preds.append(model(Xb).argmax(1).cpu().numpy())\n                        gts.append(yb.cpu().numpy())\n                ed[\"predictions\"].append(np.concatenate(preds))\n                ed[\"ground_truth\"].append(np.concatenate(gts))\n\n# Save data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic XOR data\ndef make_xor(n):\n    X = np.random.rand(n, 2)\n    y = ((X[:, 0] > 0.5) ^ (X[:, 1] > 0.5)).astype(int)\n    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n\n\ntrain_X, train_y = make_xor(2000)\nval_X, val_y = make_xor(500)\ntrain_ds = TensorDataset(train_X, train_y)\nval_ds = TensorDataset(val_X, val_y)\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=64)\n\n\n# MLP with variable hidden size\nclass MLP(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.fc1 = nn.Linear(2, hidden_size)\n        self.drop = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(hidden_size, 2)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.drop(x)\n        return self.fc2(x)\n\n\n# Hyperparameters\nhidden_layer_sizes = [4, 8, 16, 32, 64]\nepochs = 10\nmc_T = 5\nthreshold = 0.02\n\n# Experiment data container\nexperiment_data = {\n    \"hidden_layer_size\": {\n        \"synthetic_xor\": {\n            \"sizes\": hidden_layer_sizes,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Loop over hidden sizes\nfor size in hidden_layer_sizes:\n    print(f\"\\nStarting training with hidden_layer_size = {size}\")\n    # Initialize storage for this hyperparam\n    mets_tr, mets_val = [], []\n    losses_tr, losses_val = [], []\n    preds_all, gts_all = [], []\n\n    # Model, loss, optimizer\n    model = MLP(size).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # Training epochs\n    for epoch in range(1, epochs + 1):\n        # Train\n        model.train()\n        total_loss, total_corr = 0.0, 0\n        for Xb, yb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * Xb.size(0)\n            total_corr += (out.argmax(1) == yb).sum().item()\n        train_loss = total_loss / len(train_ds)\n        losses_tr.append(train_loss)\n\n        # CES on train\n        model.eval()\n        base_corr, clar_corr, clar_count = 0, 0, 0\n        with torch.no_grad():\n            for Xb, yb in train_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    ps = np.stack(ps, 0)\n                    model.eval()\n                    var = ps.var(0).sum()\n                    if var > threshold:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += (preds_base[i] == yb[i]).item()\n        base_acc_tr = base_corr / len(train_ds)\n        clar_acc_tr = clar_corr / len(train_ds)\n        avg_ct_tr = clar_count / len(train_ds) if len(train_ds) else 0\n        ces_tr = (clar_acc_tr - base_acc_tr) / avg_ct_tr if avg_ct_tr > 0 else 0.0\n        mets_tr.append(ces_tr)\n\n        # Validation loss\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                val_loss += criterion(out, yb).item() * Xb.size(0)\n        val_loss /= len(val_ds)\n        losses_val.append(val_loss)\n\n        # CES on val\n        base_corr, clar_corr, clar_count = 0, 0, 0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    ps = np.stack(ps, 0)\n                    model.eval()\n                    var = ps.var(0).sum()\n                    if var > threshold:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += (preds_base[i] == yb[i]).item()\n        base_acc_val = base_corr / len(val_ds)\n        clar_acc_val = clar_corr / len(val_ds)\n        avg_ct_val = clar_count / len(val_ds) if len(val_ds) else 0\n        ces_val = (clar_acc_val - base_acc_val) / avg_ct_val if avg_ct_val > 0 else 0.0\n        mets_val.append(ces_val)\n\n        # Store predictions & ground truth\n        preds_list, gts_list = [], []\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                preds_list.append(out.argmax(1).cpu().numpy())\n                gts_list.append(yb.cpu().numpy())\n        preds_all.append(np.concatenate(preds_list))\n        gts_all.append(np.concatenate(gts_list))\n\n        print(\n            f\"Size {size}, Epoch {epoch}: train_loss={train_loss:.4f}, train_CES={ces_tr:.4f}, val_loss={val_loss:.4f}, val_CES={ces_val:.4f}\"\n        )\n\n    # Append results for this hidden size\n    exp = experiment_data[\"hidden_layer_size\"][\"synthetic_xor\"]\n    exp[\"metrics\"][\"train\"].append(mets_tr)\n    exp[\"metrics\"][\"val\"].append(mets_val)\n    exp[\"losses\"][\"train\"].append(losses_tr)\n    exp[\"losses\"][\"val\"].append(losses_val)\n    exp[\"predictions\"].append(preds_all)\n    exp[\"ground_truth\"].append(gts_all)\n    print(f\"Completed hidden_layer_size = {size}\")\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "import os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ndef make_xor(n):\n    X = torch.rand(n, 2)\n    y = ((X[:, 0] > 0.5) ^ (X[:, 1] > 0.5)).long()\n    return X, y\n\n\n# Prepare datasets\ntrain_X, train_y = make_xor(2000)\nval_X, val_y = make_xor(500)\ntrain_loader = DataLoader(TensorDataset(train_X, train_y), batch_size=64, shuffle=True)\nval_loader = DataLoader(TensorDataset(val_X, val_y), batch_size=64)\n\n\n# Model definition\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 10)\n        self.drop = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(10, 2)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.drop(x)\n        return self.fc2(x)\n\n\n# Vectorized compute of accuracy gain per clarification\ndef compute_accuracy_gain(loader, model, threshold, mc_T):\n    model.eval()\n    base_corr = clar_corr = clar_count = total_N = 0\n    for Xb, yb in loader:\n        Xb, yb = Xb.to(device), yb.to(device)\n        # mask ambiguous feature\n        X_mask = Xb.clone()\n        X_mask[:, 1] = 0\n        # base prediction\n        with torch.no_grad():\n            out_base = model(X_mask)\n            preds_base = out_base.argmax(dim=1)\n        base_corr += (preds_base == yb).sum().item()\n        # MC dropout sampling\n        model.train()\n        ps = []\n        with torch.no_grad():\n            for _ in range(mc_T):\n                p = torch.softmax(model(X_mask), dim=1)\n                ps.append(p)\n        ps = torch.stack(ps, dim=0)  # [T, batch, classes]\n        var = ps.var(dim=0).sum(dim=1)  # [batch]\n        clar_mask = var > threshold\n        clar_count += clar_mask.sum().item()\n        # clarified prediction\n        model.eval()\n        with torch.no_grad():\n            out_clar = model(Xb)\n            preds_clar = out_clar.argmax(dim=1)\n        # combine correctness\n        correct = torch.where(clar_mask, preds_clar == yb, preds_base == yb)\n        clar_corr += correct.sum().item()\n        total_N += Xb.size(0)\n    base_acc = base_corr / total_N\n    clar_acc = clar_corr / total_N\n    avg_ct = clar_count / total_N if total_N > 0 else 0\n    return (clar_acc - base_acc) / avg_ct if avg_ct > 0 else 0.0\n\n\n# Hyperparameters\nweight_decays = [0, 1e-5, 1e-4, 1e-3]\nepochs = 10\nthreshold = 0.02\nmc_T = 5\n\n# Data container\nexperiment_data = {\n    \"accuracy_gain_per_clarification\": {\"train\": [], \"val\": []},\n    \"losses\": {\"train\": [], \"val\": []},\n}\n\n# Main loop\nfor wd in weight_decays:\n    print(f\"Running weight_decay = {wd}\")\n    model = MLP().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=wd)\n    tr_metrics, val_metrics = [], []\n    tr_losses, val_losses = [], []\n\n    for epoch in range(1, epochs + 1):\n        # Training\n        model.train()\n        total_loss = 0\n        for Xb, yb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * Xb.size(0)\n        train_loss = total_loss / len(train_loader.dataset)\n        tr_losses.append(train_loss)\n\n        # Compute accuracy gain on train\n        ag_train = compute_accuracy_gain(train_loader, model, threshold, mc_T)\n        tr_metrics.append(ag_train)\n\n        # Validation\n        model.eval()\n        total_val_loss = 0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                total_val_loss += criterion(out, yb).item() * Xb.size(0)\n        val_loss = total_val_loss / len(val_loader.dataset)\n        val_losses.append(val_loss)\n\n        # Compute accuracy gain on val\n        ag_val = compute_accuracy_gain(val_loader, model, threshold, mc_T)\n        val_metrics.append(ag_val)\n\n        # Print metrics\n        print(\n            f\"Epoch {epoch}: validation_loss = {val_loss:.4f}, \"\n            + f\"accuracy_gain_per_clarification_train = {ag_train:.4f}, \"\n            + f\"accuracy_gain_per_clarification_val = {ag_val:.4f}\"\n        )\n\n    experiment_data[\"accuracy_gain_per_clarification\"][\"train\"].append(tr_metrics)\n    experiment_data[\"accuracy_gain_per_clarification\"][\"val\"].append(val_metrics)\n    experiment_data[\"losses\"][\"train\"].append(tr_losses)\n    experiment_data[\"losses\"][\"val\"].append(val_losses)\n\n# Save results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datasets import load_dataset\n\n# Setup\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic XOR data\ndef make_xor(n):\n    X = np.random.rand(n, 2)\n    y = ((X[:, 0] > 0.5) ^ (X[:, 1] > 0.5)).astype(int)\n    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n\n\n# Model definition (unchanged)\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 10)\n        self.drop = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(10, 2)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.drop(x)\n        return self.fc2(x)\n\n\n# CES computation helper\nmc_T = 5\nthreshold = 0.1\n\n\ndef compute_ces(loader):\n    model.eval()\n    base_corr = clar_corr = clar_count = 0\n    for Xb, yb in loader:\n        Xb, yb = Xb.to(device), yb.to(device)\n        # mask second feature\n        X_mask = Xb.clone()\n        X_mask[:, 1] = 0\n        # baseline prediction\n        out_base = model(X_mask)\n        preds_base = out_base.argmax(1)\n        base_corr += (preds_base == yb).sum().item()\n        # sample-by-sample MC\u2010dropout\n        for i in range(Xb.size(0)):\n            xi = X_mask[i : i + 1]\n            model.train()\n            ps = []\n            with torch.no_grad():\n                for _ in range(mc_T):\n                    p = torch.softmax(model(xi), dim=1)\n                    ps.append(p.cpu().numpy())\n            var = np.stack(ps, 0).var(0).sum()\n            model.eval()\n            if var > threshold:\n                clar_count += 1\n                out_c = model(Xb[i : i + 1])\n                clar_corr += (out_c.argmax(1) == yb[i]).sum().item()\n            else:\n                clar_corr += int(preds_base[i].item() == yb[i].item())\n    N = len(loader.dataset)\n    base_acc = base_corr / N\n    clar_acc = clar_corr / N\n    avg_ct = clar_count / N if N > 0 else 0\n    return (clar_acc - base_acc) / avg_ct if avg_ct > 0 else 0.0\n\n\n# Hyperparameters\nweight_decays = [0, 1e-5, 1e-4, 1e-3]\nlr = 5e-4\nepochs = 15\nbatch_size = 128\n\n# Experiment container\nexperiment_data = {\n    \"synthetic_xor\": {\n        \"hyperparams\": [],\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    },\n    \"iris_2class\": {\n        \"hyperparams\": [],\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    },\n    \"adult_2class\": {\n        \"hyperparams\": [],\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    },\n}\n\n# Prepare HuggingFace datasets\n# 1) Iris binary (setosa vs versicolor)\niris_ds = load_dataset(\"iris\", split=\"train\")\niris_ds = iris_ds.filter(lambda ex: ex[\"species\"] != \"virginica\")\nfeat_iris = np.array([[ex[\"sepal_length\"], ex[\"sepal_width\"]] for ex in iris_ds])\nlab_iris = np.array([0 if ex[\"species\"] == \"setosa\" else 1 for ex in iris_ds])\nperm = np.random.permutation(len(lab_iris))\ncut = int(0.8 * len(lab_iris))\niris_train_X = torch.tensor(feat_iris[perm[:cut]], dtype=torch.float32)\niris_train_y = torch.tensor(lab_iris[perm[:cut]], dtype=torch.long)\niris_val_X = torch.tensor(feat_iris[perm[cut:]], dtype=torch.float32)\niris_val_y = torch.tensor(lab_iris[perm[cut:]], dtype=torch.long)\n\n# 2) Adult income >50K from two numeric features (sampled)\nadult = load_dataset(\"adult\")\ntrain_ad = adult[\"train\"]\ntest_ad = adult[\"test\"]\nfeat_ad_train = np.array([[ex[\"age\"], ex[\"hours-per-week\"]] for ex in train_ad])\nlab_ad_train = np.array(\n    [1 if ex[\"salary\"].strip().startswith(\">\") else 0 for ex in train_ad]\n)\nfeat_ad_test = np.array([[ex[\"age\"], ex[\"hours-per-week\"]] for ex in test_ad])\nlab_ad_test = np.array(\n    [1 if ex[\"salary\"].strip().startswith(\">\") else 0 for ex in test_ad]\n)\n# random subsample to speed up CES\nidx_tr = np.random.permutation(len(lab_ad_train))[:5000]\nidx_va = np.random.permutation(len(lab_ad_test))[:1000]\nadult_train_X = torch.tensor(feat_ad_train[idx_tr], dtype=torch.float32)\nadult_train_y = torch.tensor(lab_ad_train[idx_tr], dtype=torch.long)\nadult_val_X = torch.tensor(feat_ad_test[idx_va], dtype=torch.float32)\nadult_val_y = torch.tensor(lab_ad_test[idx_va], dtype=torch.long)\n\n# Loop over datasets\nfor dname in [\"synthetic_xor\", \"iris_2class\", \"adult_2class\"]:\n    # Prepare loaders\n    if dname == \"synthetic_xor\":\n        trX, trY = make_xor(2000)\n        vX, vY = make_xor(500)\n    elif dname == \"iris_2class\":\n        trX, trY, vX, vY = iris_train_X, iris_train_y, iris_val_X, iris_val_y\n    else:  # adult\n        trX, trY, vX, vY = adult_train_X, adult_train_y, adult_val_X, adult_val_y\n\n    train_loader = DataLoader(\n        TensorDataset(trX, trY), batch_size=batch_size, shuffle=True\n    )\n    val_loader = DataLoader(TensorDataset(vX, vY), batch_size=batch_size)\n\n    for wd in weight_decays:\n        exp = experiment_data[dname]\n        exp[\"hyperparams\"].append({\"weight_decay\": wd, \"lr\": lr})\n        metrics_tr, metrics_val = [], []\n        losses_tr, losses_val = [], []\n        preds_all, gts_all = [], []\n\n        model = MLP().to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n\n        for epoch in range(1, epochs + 1):\n            # training\n            model.train()\n            tot_loss = 0.0\n            for Xb, yb in train_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                optimizer.zero_grad()\n                out = model(Xb)\n                loss = criterion(out, yb)\n                loss.backward()\n                optimizer.step()\n                tot_loss += loss.item() * Xb.size(0)\n            train_loss = tot_loss / len(train_loader.dataset)\n            losses_tr.append(train_loss)\n            train_ces = compute_ces(train_loader)\n            metrics_tr.append(train_ces)\n\n            # validation\n            model.eval()\n            totvl = 0.0\n            with torch.no_grad():\n                for Xb, yb in val_loader:\n                    Xb, yb = Xb.to(device), yb.to(device)\n                    out = model(Xb)\n                    totvl += criterion(out, yb).item() * Xb.size(0)\n            val_loss = totvl / len(val_loader.dataset)\n            losses_val.append(val_loss)\n            val_ces = compute_ces(val_loader)\n            metrics_val.append(val_ces)\n\n            # predictions\n            preds, gts = [], []\n            with torch.no_grad():\n                for Xb, yb in val_loader:\n                    Xb, yb = Xb.to(device), yb.to(device)\n                    p = model(Xb).argmax(1).cpu().numpy()\n                    preds.append(p)\n                    gts.append(yb.cpu().numpy())\n            preds_all.append(np.concatenate(preds))\n            gts_all.append(np.concatenate(gts))\n\n            print(\n                f\"{dname} WD={wd} E{epoch}: val_loss={val_loss:.4f}, tr_CES={train_ces:.4f}, val_CES={val_ces:.4f}\"\n            )\n\n        exp[\"metrics\"][\"train\"].append(metrics_tr)\n        exp[\"metrics\"][\"val\"].append(metrics_val)\n        exp[\"losses\"][\"train\"].append(losses_tr)\n        exp[\"losses\"][\"val\"].append(losses_val)\n        exp[\"predictions\"].append(preds_all)\n        exp[\"ground_truth\"].append(gts_all)\n\n# Save all results\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "import os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic XOR data\ndef make_xor(n):\n    X = np.random.rand(n, 2)\n    y = ((X[:, 0] > 0.5) ^ (X[:, 1] > 0.5)).astype(int)\n    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n\n\ntrain_X, train_y = make_xor(2000)\nval_X, val_y = make_xor(500)\ntrain_ds = TensorDataset(train_X, train_y)\nval_ds = TensorDataset(val_X, val_y)\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=64)\n\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 10)\n        self.drop = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(10, 2)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.drop(x)\n        return self.fc2(x)\n\n\n# Hyperparameter tuning settings\nmc_T_values = [2, 5, 10, 20]\nepochs = 10\nthreshold = 0.02\n\n# Prepare experiment_data\nexperiment_data = {\"mc_T_tuning\": {\"synthetic_xor\": {}}}\n\nfor mc_T in mc_T_values:\n    print(f\"Starting run for mc_T = {mc_T}\")\n    # initialize model, loss, optimizer\n    model = MLP().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    # storage for this run\n    run_data = {\n        \"metrics\": {\"train\": [], \"val\": []},\n        \"losses\": {\"train\": [], \"val\": []},\n        \"predictions\": [],\n        \"ground_truth\": [],\n    }\n    for epoch in range(1, epochs + 1):\n        # Training\n        model.train()\n        total_loss = 0.0\n        for Xb, yb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * Xb.size(0)\n        train_loss = total_loss / len(train_ds)\n        run_data[\"losses\"][\"train\"].append(train_loss)\n\n        # CES on train\n        model.eval()\n        base_corr = clar_corr = clar_count = 0\n        with torch.no_grad():\n            for Xb, yb in train_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    # MC-dropout\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    ps = np.stack(ps, 0)\n                    var = ps.var(0).sum()\n                    model.eval()\n                    if var > threshold:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += (preds_base[i] == yb[i]).item()\n        base_acc_tr = base_corr / len(train_ds)\n        clar_acc_tr = clar_corr / len(train_ds)\n        avg_ct_tr = clar_count / len(train_ds) if len(train_ds) else 0\n        CES_tr = (clar_acc_tr - base_acc_tr) / avg_ct_tr if avg_ct_tr > 0 else 0.0\n        run_data[\"metrics\"][\"train\"].append(CES_tr)\n\n        # Validation loss\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                val_loss += criterion(out, yb).item() * Xb.size(0)\n        val_loss /= len(val_ds)\n        run_data[\"losses\"][\"val\"].append(val_loss)\n        print(f\"mc_T={mc_T} Epoch {epoch}: validation_loss = {val_loss:.4f}\")\n\n        # CES on val\n        base_corr = clar_corr = clar_count = 0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    ps = np.stack(ps, 0)\n                    var = ps.var(0).sum()\n                    model.eval()\n                    if var > threshold:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += (preds_base[i] == yb[i]).item()\n        base_acc_val = base_corr / len(val_ds)\n        clar_acc_val = clar_corr / len(val_ds)\n        avg_ct_val = clar_count / len(val_ds) if len(val_ds) else 0\n        CES_val = (clar_acc_val - base_acc_val) / avg_ct_val if avg_ct_val > 0 else 0.0\n        run_data[\"metrics\"][\"val\"].append(CES_val)\n\n        # Save predictions and ground truth\n        preds_list = []\n        gts_list = []\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                preds_list.append(out.argmax(1).cpu().numpy())\n                gts_list.append(yb.cpu().numpy())\n        run_data[\"predictions\"].append(np.concatenate(preds_list))\n        run_data[\"ground_truth\"].append(np.concatenate(gts_list))\n\n        print(\n            f\"mc_T={mc_T} Epoch {epoch}: train_CES = {CES_tr:.4f}, val_CES = {CES_val:.4f}\"\n        )\n\n    # store this run\n    experiment_data[\"mc_T_tuning\"][\"synthetic_xor\"][str(mc_T)] = run_data\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic XOR data\ndef make_xor(n):\n    X = np.random.rand(n, 2)\n    y = ((X[:, 0] > 0.5) ^ (X[:, 1] > 0.5)).astype(int)\n    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n\n\ntrain_X, train_y = make_xor(2000)\nval_X, val_y = make_xor(500)\ntrain_ds = TensorDataset(train_X, train_y)\nval_ds = TensorDataset(val_X, val_y)\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=64)\n\n\n# MLP with variable hidden size\nclass MLP(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.fc1 = nn.Linear(2, hidden_size)\n        self.drop = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(hidden_size, 2)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.drop(x)\n        return self.fc2(x)\n\n\n# Hyperparameters\nhidden_layer_sizes = [4, 8, 16, 32, 64]\nepochs = 10\nmc_T = 5\nthreshold = 0.02\n\n# Experiment data container\nexperiment_data = {\n    \"hidden_layer_size\": {\n        \"synthetic_xor\": {\n            \"sizes\": hidden_layer_sizes,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Loop over hidden sizes\nfor size in hidden_layer_sizes:\n    print(f\"\\nStarting training with hidden_layer_size = {size}\")\n    # Initialize storage for this hyperparam\n    mets_tr, mets_val = [], []\n    losses_tr, losses_val = [], []\n    preds_all, gts_all = [], []\n\n    # Model, loss, optimizer\n    model = MLP(size).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # Training epochs\n    for epoch in range(1, epochs + 1):\n        # Train\n        model.train()\n        total_loss, total_corr = 0.0, 0\n        for Xb, yb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * Xb.size(0)\n            total_corr += (out.argmax(1) == yb).sum().item()\n        train_loss = total_loss / len(train_ds)\n        losses_tr.append(train_loss)\n\n        # CES on train\n        model.eval()\n        base_corr, clar_corr, clar_count = 0, 0, 0\n        with torch.no_grad():\n            for Xb, yb in train_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    ps = np.stack(ps, 0)\n                    model.eval()\n                    var = ps.var(0).sum()\n                    if var > threshold:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += (preds_base[i] == yb[i]).item()\n        base_acc_tr = base_corr / len(train_ds)\n        clar_acc_tr = clar_corr / len(train_ds)\n        avg_ct_tr = clar_count / len(train_ds) if len(train_ds) else 0\n        ces_tr = (clar_acc_tr - base_acc_tr) / avg_ct_tr if avg_ct_tr > 0 else 0.0\n        mets_tr.append(ces_tr)\n\n        # Validation loss\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                val_loss += criterion(out, yb).item() * Xb.size(0)\n        val_loss /= len(val_ds)\n        losses_val.append(val_loss)\n\n        # CES on val\n        base_corr, clar_corr, clar_count = 0, 0, 0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    ps = np.stack(ps, 0)\n                    model.eval()\n                    var = ps.var(0).sum()\n                    if var > threshold:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += (preds_base[i] == yb[i]).item()\n        base_acc_val = base_corr / len(val_ds)\n        clar_acc_val = clar_corr / len(val_ds)\n        avg_ct_val = clar_count / len(val_ds) if len(val_ds) else 0\n        ces_val = (clar_acc_val - base_acc_val) / avg_ct_val if avg_ct_val > 0 else 0.0\n        mets_val.append(ces_val)\n\n        # Store predictions & ground truth\n        preds_list, gts_list = [], []\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                preds_list.append(out.argmax(1).cpu().numpy())\n                gts_list.append(yb.cpu().numpy())\n        preds_all.append(np.concatenate(preds_list))\n        gts_all.append(np.concatenate(gts_list))\n\n        print(\n            f\"Size {size}, Epoch {epoch}: train_loss={train_loss:.4f}, train_CES={ces_tr:.4f}, val_loss={val_loss:.4f}, val_CES={ces_val:.4f}\"\n        )\n\n    # Append results for this hidden size\n    exp = experiment_data[\"hidden_layer_size\"][\"synthetic_xor\"]\n    exp[\"metrics\"][\"train\"].append(mets_tr)\n    exp[\"metrics\"][\"val\"].append(mets_val)\n    exp[\"losses\"][\"train\"].append(losses_tr)\n    exp[\"losses\"][\"val\"].append(losses_val)\n    exp[\"predictions\"].append(preds_all)\n    exp[\"ground_truth\"].append(gts_all)\n    print(f\"Completed hidden_layer_size = {size}\")\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic XOR data\ndef make_xor(n):\n    X = np.random.rand(n, 2)\n    y = ((X[:, 0] > 0.5) ^ (X[:, 1] > 0.5)).astype(int)\n    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n\n\ntrain_X, train_y = make_xor(2000)\nval_X, val_y = make_xor(500)\ntrain_ds = TensorDataset(train_X, train_y)\nval_ds = TensorDataset(val_X, val_y)\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=64)\n\n\n# MLP with variable hidden size\nclass MLP(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.fc1 = nn.Linear(2, hidden_size)\n        self.drop = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(hidden_size, 2)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.drop(x)\n        return self.fc2(x)\n\n\n# Hyperparameters\nhidden_layer_sizes = [4, 8, 16, 32, 64]\nepochs = 10\nmc_T = 5\nthreshold = 0.02\n\n# Experiment data container\nexperiment_data = {\n    \"hidden_layer_size\": {\n        \"synthetic_xor\": {\n            \"sizes\": hidden_layer_sizes,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Loop over hidden sizes\nfor size in hidden_layer_sizes:\n    print(f\"\\nStarting training with hidden_layer_size = {size}\")\n    # Initialize storage for this hyperparam\n    mets_tr, mets_val = [], []\n    losses_tr, losses_val = [], []\n    preds_all, gts_all = [], []\n\n    # Model, loss, optimizer\n    model = MLP(size).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # Training epochs\n    for epoch in range(1, epochs + 1):\n        # Train\n        model.train()\n        total_loss, total_corr = 0.0, 0\n        for Xb, yb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * Xb.size(0)\n            total_corr += (out.argmax(1) == yb).sum().item()\n        train_loss = total_loss / len(train_ds)\n        losses_tr.append(train_loss)\n\n        # CES on train\n        model.eval()\n        base_corr, clar_corr, clar_count = 0, 0, 0\n        with torch.no_grad():\n            for Xb, yb in train_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    ps = np.stack(ps, 0)\n                    model.eval()\n                    var = ps.var(0).sum()\n                    if var > threshold:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += (preds_base[i] == yb[i]).item()\n        base_acc_tr = base_corr / len(train_ds)\n        clar_acc_tr = clar_corr / len(train_ds)\n        avg_ct_tr = clar_count / len(train_ds) if len(train_ds) else 0\n        ces_tr = (clar_acc_tr - base_acc_tr) / avg_ct_tr if avg_ct_tr > 0 else 0.0\n        mets_tr.append(ces_tr)\n\n        # Validation loss\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                val_loss += criterion(out, yb).item() * Xb.size(0)\n        val_loss /= len(val_ds)\n        losses_val.append(val_loss)\n\n        # CES on val\n        base_corr, clar_corr, clar_count = 0, 0, 0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    ps = np.stack(ps, 0)\n                    model.eval()\n                    var = ps.var(0).sum()\n                    if var > threshold:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += (preds_base[i] == yb[i]).item()\n        base_acc_val = base_corr / len(val_ds)\n        clar_acc_val = clar_corr / len(val_ds)\n        avg_ct_val = clar_count / len(val_ds) if len(val_ds) else 0\n        ces_val = (clar_acc_val - base_acc_val) / avg_ct_val if avg_ct_val > 0 else 0.0\n        mets_val.append(ces_val)\n\n        # Store predictions & ground truth\n        preds_list, gts_list = [], []\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                preds_list.append(out.argmax(1).cpu().numpy())\n                gts_list.append(yb.cpu().numpy())\n        preds_all.append(np.concatenate(preds_list))\n        gts_all.append(np.concatenate(gts_list))\n\n        print(\n            f\"Size {size}, Epoch {epoch}: train_loss={train_loss:.4f}, train_CES={ces_tr:.4f}, val_loss={val_loss:.4f}, val_CES={ces_val:.4f}\"\n        )\n\n    # Append results for this hidden size\n    exp = experiment_data[\"hidden_layer_size\"][\"synthetic_xor\"]\n    exp[\"metrics\"][\"train\"].append(mets_tr)\n    exp[\"metrics\"][\"val\"].append(mets_val)\n    exp[\"losses\"][\"train\"].append(losses_tr)\n    exp[\"losses\"][\"val\"].append(losses_val)\n    exp[\"predictions\"].append(preds_all)\n    exp[\"ground_truth\"].append(gts_all)\n    print(f\"Completed hidden_layer_size = {size}\")\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 2\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Setup working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# Synthetic XOR data\ndef make_xor(n):\n    X = np.random.rand(n, 2)\n    y = ((X[:, 0] > 0.5) ^ (X[:, 1] > 0.5)).astype(int)\n    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n\n\ntrain_X, train_y = make_xor(2000)\nval_X, val_y = make_xor(500)\ntrain_ds = TensorDataset(train_X, train_y)\nval_ds = TensorDataset(val_X, val_y)\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=64)\n\n\n# MLP with variable hidden size\nclass MLP(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.fc1 = nn.Linear(2, hidden_size)\n        self.drop = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(hidden_size, 2)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.drop(x)\n        return self.fc2(x)\n\n\n# Hyperparameters\nhidden_layer_sizes = [4, 8, 16, 32, 64]\nepochs = 10\nmc_T = 5\nthreshold = 0.02\n\n# Experiment data container\nexperiment_data = {\n    \"hidden_layer_size\": {\n        \"synthetic_xor\": {\n            \"sizes\": hidden_layer_sizes,\n            \"metrics\": {\"train\": [], \"val\": []},\n            \"losses\": {\"train\": [], \"val\": []},\n            \"predictions\": [],\n            \"ground_truth\": [],\n        }\n    }\n}\n\n# Loop over hidden sizes\nfor size in hidden_layer_sizes:\n    print(f\"\\nStarting training with hidden_layer_size = {size}\")\n    # Initialize storage for this hyperparam\n    mets_tr, mets_val = [], []\n    losses_tr, losses_val = [], []\n    preds_all, gts_all = [], []\n\n    # Model, loss, optimizer\n    model = MLP(size).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # Training epochs\n    for epoch in range(1, epochs + 1):\n        # Train\n        model.train()\n        total_loss, total_corr = 0.0, 0\n        for Xb, yb in train_loader:\n            Xb, yb = Xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            out = model(Xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * Xb.size(0)\n            total_corr += (out.argmax(1) == yb).sum().item()\n        train_loss = total_loss / len(train_ds)\n        losses_tr.append(train_loss)\n\n        # CES on train\n        model.eval()\n        base_corr, clar_corr, clar_count = 0, 0, 0\n        with torch.no_grad():\n            for Xb, yb in train_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    ps = np.stack(ps, 0)\n                    model.eval()\n                    var = ps.var(0).sum()\n                    if var > threshold:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += (preds_base[i] == yb[i]).item()\n        base_acc_tr = base_corr / len(train_ds)\n        clar_acc_tr = clar_corr / len(train_ds)\n        avg_ct_tr = clar_count / len(train_ds) if len(train_ds) else 0\n        ces_tr = (clar_acc_tr - base_acc_tr) / avg_ct_tr if avg_ct_tr > 0 else 0.0\n        mets_tr.append(ces_tr)\n\n        # Validation loss\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                val_loss += criterion(out, yb).item() * Xb.size(0)\n        val_loss /= len(val_ds)\n        losses_val.append(val_loss)\n\n        # CES on val\n        base_corr, clar_corr, clar_count = 0, 0, 0\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                X_mask = Xb.clone()\n                X_mask[:, 1] = 0\n                out_base = model(X_mask)\n                preds_base = out_base.argmax(1)\n                base_corr += (preds_base == yb).sum().item()\n                for i in range(Xb.size(0)):\n                    xi = X_mask[i : i + 1]\n                    model.train()\n                    ps = []\n                    for _ in range(mc_T):\n                        p = torch.softmax(model(xi), dim=1)\n                        ps.append(p.cpu().numpy())\n                    ps = np.stack(ps, 0)\n                    model.eval()\n                    var = ps.var(0).sum()\n                    if var > threshold:\n                        clar_count += 1\n                        out_c = model(Xb[i : i + 1])\n                        clar_corr += (out_c.argmax(1) == yb[i : i + 1]).sum().item()\n                    else:\n                        clar_corr += (preds_base[i] == yb[i]).item()\n        base_acc_val = base_corr / len(val_ds)\n        clar_acc_val = clar_corr / len(val_ds)\n        avg_ct_val = clar_count / len(val_ds) if len(val_ds) else 0\n        ces_val = (clar_acc_val - base_acc_val) / avg_ct_val if avg_ct_val > 0 else 0.0\n        mets_val.append(ces_val)\n\n        # Store predictions & ground truth\n        preds_list, gts_list = [], []\n        with torch.no_grad():\n            for Xb, yb in val_loader:\n                Xb, yb = Xb.to(device), yb.to(device)\n                out = model(Xb)\n                preds_list.append(out.argmax(1).cpu().numpy())\n                gts_list.append(yb.cpu().numpy())\n        preds_all.append(np.concatenate(preds_list))\n        gts_all.append(np.concatenate(gts_list))\n\n        print(\n            f\"Size {size}, Epoch {epoch}: train_loss={train_loss:.4f}, train_CES={ces_tr:.4f}, val_loss={val_loss:.4f}, val_CES={ces_val:.4f}\"\n        )\n\n    # Append results for this hidden size\n    exp = experiment_data[\"hidden_layer_size\"][\"synthetic_xor\"]\n    exp[\"metrics\"][\"train\"].append(mets_tr)\n    exp[\"metrics\"][\"val\"].append(mets_val)\n    exp[\"losses\"][\"train\"].append(losses_tr)\n    exp[\"losses\"][\"val\"].append(losses_val)\n    exp[\"predictions\"].append(preds_all)\n    exp[\"ground_truth\"].append(gts_all)\n    print(f\"Completed hidden_layer_size = {size}\")\n\n# Save experiment data\nnp.save(os.path.join(working_dir, \"experiment_data.npy\"), experiment_data)\nprint(\"Saved experiment_data.npy\")\n", "# plotting aggregation code"], "term_out": ["['Using device: cuda', '\\n', 'Epoch 1: validation_loss = 0.6815', '\\n', 'Epoch\n1: train_CES = 0.2954, val_CES = 0.2478', '\\n', 'Epoch 2: validation_loss =\n0.6793', '\\n', 'Epoch 2: train_CES = 0.3246, val_CES = 0.2195', '\\n', 'Epoch 3:\nvalidation_loss = 0.6766', '\\n', 'Epoch 3: train_CES = 0.3472, val_CES =\n0.2143', '\\n', 'Epoch 4: validation_loss = 0.6740', '\\n', 'Epoch 4: train_CES =\n0.3895, val_CES = 0.3258', '\\n', 'Epoch 5: validation_loss = 0.6710', '\\n',\n'Epoch 5: train_CES = 0.3846, val_CES = 0.2453', '\\n', 'Epoch 6: validation_loss\n= 0.6681', '\\n', 'Epoch 6: train_CES = 0.4512, val_CES = 0.4754', '\\n', 'Epoch\n7: validation_loss = 0.6652', '\\n', 'Epoch 7: train_CES = 0.3910, val_CES =\n0.5000', '\\n', 'Epoch 8: validation_loss = 0.6619', '\\n', 'Epoch 8: train_CES =\n0.4628, val_CES = 0.4222', '\\n', 'Epoch 9: validation_loss = 0.6586', '\\n',\n'Epoch 9: train_CES = 0.4800, val_CES = 0.4706', '\\n', 'Epoch 10:\nvalidation_loss = 0.6550', '\\n', 'Epoch 10: train_CES = 0.4358, val_CES =\n0.4792', '\\n', 'Execution time: 21 seconds seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 111, in <module>\\n    CES_tr = compute_CES(train_loader,\nmodel)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\", line\n61, in compute_CES\\n    ps = [torch.softmax(model(xi), dim=1).cpu().numpy() for\n_ in range(mc_T)]\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"runfile.py\", line 61, in <listcomp>\\n    ps = [torch.softmax(model(xi),\ndim=1).cpu().numpy() for _ in range(mc_T)]\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: Can\\'t call numpy()\non Tensor that requires grad. Use tensor.detach().numpy() instead.\\n',\n'Execution time: a second seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\nStarting run with learning rate = 0.0001', '\\n',\n'LR 1e-04 E1: val_loss=0.6837 train_CES=0.2552 val_CES=0.2420', '\\n', 'LR 1e-04\nE2: val_loss=0.6835 train_CES=0.2824 val_CES=0.2372', '\\n', 'LR 1e-04 E3:\nval_loss=0.6833 train_CES=0.2688 val_CES=0.1993', '\\n', 'LR 1e-04 E4:\nval_loss=0.6831 train_CES=0.2678 val_CES=0.2444', '\\n', 'LR 1e-04 E5:\nval_loss=0.6829 train_CES=0.2680 val_CES=0.2586', '\\n', 'LR 1e-04 E6:\nval_loss=0.6827 train_CES=0.2822 val_CES=0.2664', '\\n', 'LR 1e-04 E7:\nval_loss=0.6825 train_CES=0.3011 val_CES=0.2727', '\\n', 'LR 1e-04 E8:\nval_loss=0.6823 train_CES=0.2718 val_CES=0.2297', '\\n', 'LR 1e-04 E9:\nval_loss=0.6821 train_CES=0.2540 val_CES=0.2217', '\\n', 'LR 1e-04 E10:\nval_loss=0.6819 train_CES=0.2612 val_CES=0.2325', '\\n', '\\nStarting run with\nlearning rate = 0.0005', '\\n', 'LR 5e-04 E1: val_loss=0.6950 train_CES=0.0000\nval_CES=0.0000', '\\n', 'LR 5e-04 E2: val_loss=0.6941 train_CES=0.0000\nval_CES=1.0000', '\\n', 'LR 5e-04 E3: val_loss=0.6938 train_CES=0.0000\nval_CES=0.0000', '\\n', 'LR 5e-04 E4: val_loss=0.6938 train_CES=0.0000\nval_CES=0.0000', '\\n', 'LR 5e-04 E5: val_loss=0.6939 train_CES=0.0000\nval_CES=0.0000', '\\n', 'LR 5e-04 E6: val_loss=0.6937 train_CES=0.0000\nval_CES=0.0000', '\\n', 'LR 5e-04 E7: val_loss=0.6932 train_CES=0.0000\nval_CES=0.0000', '\\n', 'LR 5e-04 E8: val_loss=0.6922 train_CES=0.0000\nval_CES=0.0000', '\\n', 'LR 5e-04 E9: val_loss=0.6898 train_CES=0.0000\nval_CES=0.0000', '\\n', 'LR 5e-04 E10: val_loss=0.6873 train_CES=0.0000\nval_CES=0.0000', '\\n', '\\nStarting run with learning rate = 0.001', '\\n', 'LR\n1e-03 E1: val_loss=0.6970 train_CES=0.0000 val_CES=0.0000', '\\n', 'LR 1e-03 E2:\nval_loss=0.6884 train_CES=0.0000 val_CES=0.0000', '\\n', 'LR 1e-03 E3:\nval_loss=0.6837 train_CES=0.0000 val_CES=0.0000', '\\n', 'LR 1e-03 E4:\nval_loss=0.6790 train_CES=0.0000 val_CES=0.0000', '\\n', 'LR 1e-03 E5:\nval_loss=0.6743 train_CES=0.0000 val_CES=0.0000', '\\n', 'LR 1e-03 E6:\nval_loss=0.6695 train_CES=0.0000 val_CES=0.0000', '\\n', 'LR 1e-03 E7:\nval_loss=0.6648 train_CES=0.0000 val_CES=0.0000', '\\n', 'LR 1e-03 E8:\nval_loss=0.6599 train_CES=0.0000 val_CES=0.0000', '\\n', 'LR 1e-03 E9:\nval_loss=0.6560 train_CES=0.0000 val_CES=0.0000', '\\n', 'LR 1e-03 E10:\nval_loss=0.6508 train_CES=0.0000 val_CES=0.0000', '\\n', '\\nStarting run with\nlearning rate = 0.005', '\\n', 'LR 5e-03 E1: val_loss=0.6655 train_CES=0.0000\nval_CES=0.0000', '\\n', 'LR 5e-03 E2: val_loss=0.6287 train_CES=0.5588\nval_CES=0.4286', '\\n', 'LR 5e-03 E3: val_loss=0.5781 train_CES=0.4258\nval_CES=0.3765', '\\n', 'LR 5e-03 E4: val_loss=0.5340 train_CES=0.4016\nval_CES=0.3902', '\\n', 'LR 5e-03 E5: val_loss=0.4955 train_CES=0.4297\nval_CES=0.3795', '\\n', 'LR 5e-03 E6: val_loss=0.4609 train_CES=0.4107\nval_CES=0.3974', '\\n', 'LR 5e-03 E7: val_loss=0.4367 train_CES=0.4224\nval_CES=0.3918', '\\n', 'LR 5e-03 E8: val_loss=0.4145 train_CES=0.4397\nval_CES=0.3870', '\\n', 'LR 5e-03 E9: val_loss=0.3937 train_CES=0.4020\nval_CES=0.3611', '\\n', 'LR 5e-03 E10: val_loss=0.3787 train_CES=0.4642\nval_CES=0.3979', '\\n', '\\nStarting run with learning rate = 0.01', '\\n', 'LR\n1e-02 E1: val_loss=0.6937 train_CES=0.0000 val_CES=0.0000', '\\n', 'LR 1e-02 E2:\nval_loss=0.6947 train_CES=0.0000 val_CES=0.0000', '\\n', 'LR 1e-02 E3:\nval_loss=0.6927 train_CES=0.0000 val_CES=0.0000', '\\n', 'LR 1e-02 E4:\nval_loss=0.6770 train_CES=0.0000 val_CES=0.0000', '\\n', 'LR 1e-02 E5:\nval_loss=0.6451 train_CES=0.3271 val_CES=0.3562', '\\n', 'LR 1e-02 E6:\nval_loss=0.6335 train_CES=0.2332 val_CES=0.2178', '\\n', 'LR 1e-02 E7:\nval_loss=0.6155 train_CES=0.2513 val_CES=0.2379', '\\n', 'LR 1e-02 E8:\nval_loss=0.5929 train_CES=0.2838 val_CES=0.2667', '\\n', 'LR 1e-02 E9:\nval_loss=0.5380 train_CES=0.3820 val_CES=0.3540', '\\n', 'LR 1e-02 E10:\nval_loss=0.4940 train_CES=0.3954 val_CES=0.3579', '\\n', 'Execution time: a\nminute seconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Batch 16 Epoch 1: val_loss = 0.6787', '\\n', 'Batch\n16 Epoch 1: train_CES = 0.2600, val_CES = 0.2674', '\\n', 'Batch 16 Epoch 2:\nval_loss = 0.6676', '\\n', 'Batch 16 Epoch 2: train_CES = 0.4000, val_CES =\n0.3562', '\\n', 'Batch 16 Epoch 3: val_loss = 0.6531', '\\n', 'Batch 16 Epoch 3:\ntrain_CES = 0.3367, val_CES = 0.4286', '\\n', 'Batch 16 Epoch 4: val_loss =\n0.6391', '\\n', 'Batch 16 Epoch 4: train_CES = 0.3879, val_CES = 0.4000', '\\n',\n'Batch 16 Epoch 5: val_loss = 0.6245', '\\n', 'Batch 16 Epoch 5: train_CES =\n0.4305, val_CES = 0.4375', '\\n', 'Batch 16 Epoch 6: val_loss = 0.6110', '\\n',\n'Batch 16 Epoch 6: train_CES = 0.3761, val_CES = 0.4191', '\\n', 'Batch 16 Epoch\n7: val_loss = 0.5986', '\\n', 'Batch 16 Epoch 7: train_CES = 0.4534, val_CES =\n0.4892', '\\n', 'Batch 16 Epoch 8: val_loss = 0.5859', '\\n', 'Batch 16 Epoch 8:\ntrain_CES = 0.4162, val_CES = 0.4402', '\\n', 'Batch 16 Epoch 9: val_loss =\n0.5749', '\\n', 'Batch 16 Epoch 9: train_CES = 0.3815, val_CES = 0.3991', '\\n',\n'Batch 16 Epoch 10: val_loss = 0.5656', '\\n', 'Batch 16 Epoch 10: train_CES =\n0.3878, val_CES = 0.3621', '\\n', 'Batch 32 Epoch 1: val_loss = 0.6940', '\\n',\n'Batch 32 Epoch 1: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'Batch 32 Epoch\n2: val_loss = 0.6935', '\\n', 'Batch 32 Epoch 2: train_CES = 0.0000, val_CES =\n0.0000', '\\n', 'Batch 32 Epoch 3: val_loss = 0.6899', '\\n', 'Batch 32 Epoch 3:\ntrain_CES = 0.0000, val_CES = 0.0000', '\\n', 'Batch 32 Epoch 4: val_loss =\n0.6846', '\\n', 'Batch 32 Epoch 4: train_CES = 0.0000, val_CES = 0.0000', '\\n',\n'Batch 32 Epoch 5: val_loss = 0.6808', '\\n', 'Batch 32 Epoch 5: train_CES =\n0.0000, val_CES = 0.0000', '\\n', 'Batch 32 Epoch 6: val_loss = 0.6761', '\\n',\n'Batch 32 Epoch 6: train_CES = 0.1975, val_CES = 0.1250', '\\n', 'Batch 32 Epoch\n7: val_loss = 0.6715', '\\n', 'Batch 32 Epoch 7: train_CES = 0.3194, val_CES =\n0.3469', '\\n', 'Batch 32 Epoch 8: val_loss = 0.6668', '\\n', 'Batch 32 Epoch 8:\ntrain_CES = 0.3754, val_CES = 0.4507', '\\n', 'Batch 32 Epoch 9: val_loss =\n0.6620', '\\n', 'Batch 32 Epoch 9: train_CES = 0.4265, val_CES = 0.3774', '\\n',\n'Batch 32 Epoch 10: val_loss = 0.6568', '\\n', 'Batch 32 Epoch 10: train_CES =\n0.4200, val_CES = 0.3456', '\\n', 'Batch 64 Epoch 1: val_loss = 0.6971', '\\n',\n'Batch 64 Epoch 1: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'Batch 64 Epoch\n2: val_loss = 0.6882', '\\n', 'Batch 64 Epoch 2: train_CES = 0.0000, val_CES =\n0.0000', '\\n', 'Batch 64 Epoch 3: val_loss = 0.6832', '\\n', 'Batch 64 Epoch 3:\ntrain_CES = 0.0000, val_CES = 0.0000', '\\n', 'Batch 64 Epoch 4: val_loss =\n0.6789', '\\n', 'Batch 64 Epoch 4: train_CES = 0.0000, val_CES = 0.0000', '\\n',\n'Batch 64 Epoch 5: val_loss = 0.6743', '\\n', 'Batch 64 Epoch 5: train_CES =\n0.0000, val_CES = 0.0000', '\\n', 'Batch 64 Epoch 6: val_loss = 0.6700', '\\n',\n'Batch 64 Epoch 6: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'Batch 64 Epoch\n7: val_loss = 0.6653', '\\n', 'Batch 64 Epoch 7: train_CES = 0.0000, val_CES =\n0.0000', '\\n', 'Batch 64 Epoch 8: val_loss = 0.6598', '\\n', 'Batch 64 Epoch 8:\ntrain_CES = 0.0000, val_CES = 0.0000', '\\n', 'Batch 64 Epoch 9: val_loss =\n0.6549', '\\n', 'Batch 64 Epoch 9: train_CES = 0.0000, val_CES = 0.0000', '\\n',\n'Batch 64 Epoch 10: val_loss = 0.6494', '\\n', 'Batch 64 Epoch 10: train_CES =\n0.0000, val_CES = 0.0000', '\\n', 'Batch 128 Epoch 1: val_loss = 0.6860', '\\n',\n'Batch 128 Epoch 1: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'Batch 128\nEpoch 2: val_loss = 0.6825', '\\n', 'Batch 128 Epoch 2: train_CES = 0.0000,\nval_CES = 0.0000', '\\n', 'Batch 128 Epoch 3: val_loss = 0.6794', '\\n', 'Batch\n128 Epoch 3: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'Batch 128 Epoch 4:\nval_loss = 0.6767', '\\n', 'Batch 128 Epoch 4: train_CES = 0.0000, val_CES =\n0.0000', '\\n', 'Batch 128 Epoch 5: val_loss = 0.6739', '\\n', 'Batch 128 Epoch 5:\ntrain_CES = 0.0000, val_CES = 0.0000', '\\n', 'Batch 128 Epoch 6: val_loss =\n0.6708', '\\n', 'Batch 128 Epoch 6: train_CES = 0.0000, val_CES = 0.0000', '\\n',\n'Batch 128 Epoch 7: val_loss = 0.6675', '\\n', 'Batch 128 Epoch 7: train_CES =\n0.0000, val_CES = 0.0000', '\\n', 'Batch 128 Epoch 8: val_loss = 0.6638', '\\n',\n'Batch 128 Epoch 8: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'Batch 128\nEpoch 9: val_loss = 0.6599', '\\n', 'Batch 128 Epoch 9: train_CES = 0.0000,\nval_CES = 0.0000', '\\n', 'Batch 128 Epoch 10: val_loss = 0.6552', '\\n', 'Batch\n128 Epoch 10: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'Batch 256 Epoch 1:\nval_loss = 0.7188', '\\n', 'Batch 256 Epoch 1: train_CES = 0.0000, val_CES =\n0.0000', '\\n', 'Batch 256 Epoch 2: val_loss = 0.7123', '\\n', 'Batch 256 Epoch 2:\ntrain_CES = 0.0000, val_CES = 0.0000', '\\n', 'Batch 256 Epoch 3: val_loss =\n0.7069', '\\n', 'Batch 256 Epoch 3: train_CES = 0.0000, val_CES = 0.0000', '\\n',\n'Batch 256 Epoch 4: val_loss = 0.7029', '\\n', 'Batch 256 Epoch 4: train_CES =\n0.0000, val_CES = 0.0000', '\\n', 'Batch 256 Epoch 5: val_loss = 0.6995', '\\n',\n'Batch 256 Epoch 5: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'Batch 256\nEpoch 6: val_loss = 0.6972', '\\n', 'Batch 256 Epoch 6: train_CES = 0.0000,\nval_CES = 0.0000', '\\n', 'Batch 256 Epoch 7: val_loss = 0.6953', '\\n', 'Batch\n256 Epoch 7: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'Batch 256 Epoch 8:\nval_loss = 0.6941', '\\n', 'Batch 256 Epoch 8: train_CES = 0.0000, val_CES =\n0.0000', '\\n', 'Batch 256 Epoch 9: val_loss = 0.6933', '\\n', 'Batch 256 Epoch 9:\ntrain_CES = 0.0000, val_CES = 0.0000', '\\n', 'Batch 256 Epoch 10: val_loss =\n0.6929', '\\n', 'Batch 256 Epoch 10: train_CES = 0.0000, val_CES = 0.0000', '\\n',\n'Execution time: a minute seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Sweeping dropout_rate = 0.0 ===', '\\n',\n'dr=0.0 epoch=1: tr_loss=0.6863, val_loss=0.6818, tr_CES=0.0000,\nval_CES=0.0000', '\\n', 'dr=0.0 epoch=2: tr_loss=0.6825, val_loss=0.6796,\ntr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.0 epoch=3: tr_loss=0.6787,\nval_loss=0.6749, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.0 epoch=4:\ntr_loss=0.6730, val_loss=0.6699, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.0\nepoch=5: tr_loss=0.6663, val_loss=0.6634, tr_CES=0.0000, val_CES=0.0000', '\\n',\n'dr=0.0 epoch=6: tr_loss=0.6598, val_loss=0.6575, tr_CES=0.0000,\nval_CES=0.0000', '\\n', 'dr=0.0 epoch=7: tr_loss=0.6530, val_loss=0.6508,\ntr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.0 epoch=8: tr_loss=0.6451,\nval_loss=0.6439, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.0 epoch=9:\ntr_loss=0.6376, val_loss=0.6357, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.0\nepoch=10: tr_loss=0.6295, val_loss=0.6282, tr_CES=0.0000, val_CES=0.0000', '\\n',\n'\\n=== Sweeping dropout_rate = 0.1 ===', '\\n', 'dr=0.1 epoch=1: tr_loss=0.7014,\nval_loss=0.6937, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.1 epoch=2:\ntr_loss=0.6980, val_loss=0.6932, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.1\nepoch=3: tr_loss=0.6933, val_loss=0.6924, tr_CES=0.0000, val_CES=0.0000', '\\n',\n'dr=0.1 epoch=4: tr_loss=0.6923, val_loss=0.6907, tr_CES=0.0000,\nval_CES=0.0000', '\\n', 'dr=0.1 epoch=5: tr_loss=0.6919, val_loss=0.6869,\ntr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.1 epoch=6: tr_loss=0.6856,\nval_loss=0.6822, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.1 epoch=7:\ntr_loss=0.6814, val_loss=0.6781, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.1\nepoch=8: tr_loss=0.6772, val_loss=0.6742, tr_CES=0.0000, val_CES=0.0000', '\\n',\n'dr=0.1 epoch=9: tr_loss=0.6718, val_loss=0.6701, tr_CES=0.0000,\nval_CES=0.0000', '\\n', 'dr=0.1 epoch=10: tr_loss=0.6706, val_loss=0.6661,\ntr_CES=1.0000, val_CES=0.0000', '\\n', '\\n=== Sweeping dropout_rate = 0.2 ===',\n'\\n', 'dr=0.2 epoch=1: tr_loss=0.6953, val_loss=0.6967, tr_CES=0.0000,\nval_CES=0.0000', '\\n', 'dr=0.2 epoch=2: tr_loss=0.6868, val_loss=0.6875,\ntr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.2 epoch=3: tr_loss=0.6808,\nval_loss=0.6825, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.2 epoch=4:\ntr_loss=0.6758, val_loss=0.6777, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.2\nepoch=5: tr_loss=0.6705, val_loss=0.6724, tr_CES=0.0000, val_CES=0.0000', '\\n',\n'dr=0.2 epoch=6: tr_loss=0.6657, val_loss=0.6670, tr_CES=0.0000,\nval_CES=0.0000', '\\n', 'dr=0.2 epoch=7: tr_loss=0.6590, val_loss=0.6610,\ntr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.2 epoch=8: tr_loss=0.6531,\nval_loss=0.6552, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.2 epoch=9:\ntr_loss=0.6482, val_loss=0.6498, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.2\nepoch=10: tr_loss=0.6403, val_loss=0.6438, tr_CES=0.0000, val_CES=0.0000', '\\n',\n'\\n=== Sweeping dropout_rate = 0.3 ===', '\\n', 'dr=0.3 epoch=1: tr_loss=0.6925,\nval_loss=0.6827, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.3 epoch=2:\ntr_loss=0.6826, val_loss=0.6772, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.3\nepoch=3: tr_loss=0.6752, val_loss=0.6718, tr_CES=0.0000, val_CES=0.0000', '\\n',\n'dr=0.3 epoch=4: tr_loss=0.6695, val_loss=0.6654, tr_CES=0.0000,\nval_CES=0.0000', '\\n', 'dr=0.3 epoch=5: tr_loss=0.6614, val_loss=0.6577,\ntr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.3 epoch=6: tr_loss=0.6542,\nval_loss=0.6482, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.3 epoch=7:\ntr_loss=0.6424, val_loss=0.6377, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.3\nepoch=8: tr_loss=0.6322, val_loss=0.6265, tr_CES=0.0000, val_CES=0.0000', '\\n',\n'dr=0.3 epoch=9: tr_loss=0.6226, val_loss=0.6157, tr_CES=0.0000,\nval_CES=0.0000', '\\n', 'dr=0.3 epoch=10: tr_loss=0.6108, val_loss=0.6040,\ntr_CES=0.0000, val_CES=0.0000', '\\n', '\\n=== Sweeping dropout_rate = 0.5 ===',\n'\\n', 'dr=0.5 epoch=1: tr_loss=0.7286, val_loss=0.7044, tr_CES=0.0000,\nval_CES=0.0000', '\\n', 'dr=0.5 epoch=2: tr_loss=0.7140, val_loss=0.6956,\ntr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.5 epoch=3: tr_loss=0.7044,\nval_loss=0.6928, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.5 epoch=4:\ntr_loss=0.6991, val_loss=0.6925, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.5\nepoch=5: tr_loss=0.6959, val_loss=0.6926, tr_CES=0.0000, val_CES=0.0000', '\\n',\n'dr=0.5 epoch=6: tr_loss=0.6957, val_loss=0.6925, tr_CES=0.0000,\nval_CES=0.0000', '\\n', 'dr=0.5 epoch=7: tr_loss=0.6996, val_loss=0.6930,\ntr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.5 epoch=8: tr_loss=0.6974,\nval_loss=0.6927, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.5 epoch=9:\ntr_loss=0.6937, val_loss=0.6928, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.5\nepoch=10: tr_loss=0.6944, val_loss=0.6929, tr_CES=0.0000, val_CES=0.0000', '\\n',\n'\\n=== Sweeping dropout_rate = 0.7 ===', '\\n', 'dr=0.7 epoch=1: tr_loss=0.7323,\nval_loss=0.6947, tr_CES=-0.0743, val_CES=-0.0750', '\\n', 'dr=0.7 epoch=2:\ntr_loss=0.7202, val_loss=0.6922, tr_CES=-0.2917, val_CES=-0.1667', '\\n', 'dr=0.7\nepoch=3: tr_loss=0.7133, val_loss=0.6906, tr_CES=0.0000, val_CES=0.0000', '\\n',\n'dr=0.7 epoch=4: tr_loss=0.7011, val_loss=0.6887, tr_CES=0.0000,\nval_CES=0.0000', '\\n', 'dr=0.7 epoch=5: tr_loss=0.7033, val_loss=0.6877,\ntr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.7 epoch=6: tr_loss=0.6977,\nval_loss=0.6869, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.7 epoch=7:\ntr_loss=0.6923, val_loss=0.6858, tr_CES=0.0000, val_CES=0.0000', '\\n', 'dr=0.7\nepoch=8: tr_loss=0.6937, val_loss=0.6845, tr_CES=0.0000, val_CES=0.0000', '\\n',\n'dr=0.7 epoch=9: tr_loss=0.6912, val_loss=0.6831, tr_CES=0.0000,\nval_CES=0.0000', '\\n', 'dr=0.7 epoch=10: tr_loss=0.6893, val_loss=0.6820,\ntr_CES=0.0000, val_CES=0.0000', '\\n', 'Execution time: a minute seconds (time\nlimit is an hour).']", "['Using device: cuda', '\\n', '\\n=== Threshold = 0.005 ===', '\\n', 'Thr 0.005\nEpoch 1: train_loss=0.7287, val_loss=0.6817, train_CES=0.1705, val_CES=0.1706',\n'\\n', 'Thr 0.005 Epoch 2: train_loss=0.7133, val_loss=0.6796, train_CES=0.1697,\nval_CES=0.1792', '\\n', 'Thr 0.005 Epoch 3: train_loss=0.7206, val_loss=0.6769,\ntrain_CES=0.1923, val_CES=0.2165', '\\n', 'Thr 0.005 Epoch 4: train_loss=0.6952,\nval_loss=0.6735, train_CES=0.1400, val_CES=0.1359', '\\n', 'Thr 0.005 Epoch 5:\ntrain_loss=0.6953, val_loss=0.6685, train_CES=0.2498, val_CES=0.2480', '\\n',\n'Thr 0.005 Epoch 6: train_loss=0.6775, val_loss=0.6639, train_CES=0.2396,\nval_CES=0.2451', '\\n', 'Thr 0.005 Epoch 7: train_loss=0.6824, val_loss=0.6587,\ntrain_CES=0.2387, val_CES=0.2426', '\\n', 'Thr 0.005 Epoch 8: train_loss=0.6684,\nval_loss=0.6536, train_CES=0.2471, val_CES=0.2544', '\\n', 'Thr 0.005 Epoch 9:\ntrain_loss=0.6626, val_loss=0.6480, train_CES=0.2889, val_CES=0.2951', '\\n',\n'Thr 0.005 Epoch 10: train_loss=0.6547, val_loss=0.6427, train_CES=0.2982,\nval_CES=0.2750', '\\n', '\\n=== Threshold = 0.01 ===', '\\n', 'Thr 0.010 Epoch 1:\ntrain_loss=0.7144, val_loss=0.6938, train_CES=0.4487, val_CES=0.3571', '\\n',\n'Thr 0.010 Epoch 2: train_loss=0.7032, val_loss=0.6935, train_CES=0.1316,\nval_CES=0.1818', '\\n', 'Thr 0.010 Epoch 3: train_loss=0.7045, val_loss=0.6931,\ntrain_CES=0.1304, val_CES=0.0000', '\\n', 'Thr 0.010 Epoch 4: train_loss=0.6984,\nval_loss=0.6911, train_CES=0.0000, val_CES=0.0000', '\\n', 'Thr 0.010 Epoch 5:\ntrain_loss=0.6964, val_loss=0.6872, train_CES=0.0000, val_CES=0.0000', '\\n',\n'Thr 0.010 Epoch 6: train_loss=0.6911, val_loss=0.6839, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'Thr 0.010 Epoch 7: train_loss=0.6860, val_loss=0.6810,\ntrain_CES=0.2381, val_CES=0.1591', '\\n', 'Thr 0.010 Epoch 8: train_loss=0.6854,\nval_loss=0.6781, train_CES=0.3498, val_CES=0.3506', '\\n', 'Thr 0.010 Epoch 9:\ntrain_loss=0.6820, val_loss=0.6758, train_CES=0.4255, val_CES=0.3592', '\\n',\n'Thr 0.010 Epoch 10: train_loss=0.6826, val_loss=0.6733, train_CES=0.3582,\nval_CES=0.3600', '\\n', '\\n=== Threshold = 0.02 ===', '\\n', 'Thr 0.020 Epoch 1:\ntrain_loss=0.6962, val_loss=0.6968, train_CES=0.0000, val_CES=0.0000', '\\n',\n'Thr 0.020 Epoch 2: train_loss=0.6900, val_loss=0.6885, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'Thr 0.020 Epoch 3: train_loss=0.6845, val_loss=0.6844,\ntrain_CES=0.0000, val_CES=0.0000', '\\n', 'Thr 0.020 Epoch 4: train_loss=0.6783,\nval_loss=0.6798, train_CES=0.0000, val_CES=0.0000', '\\n', 'Thr 0.020 Epoch 5:\ntrain_loss=0.6759, val_loss=0.6753, train_CES=0.0000, val_CES=0.0000', '\\n',\n'Thr 0.020 Epoch 6: train_loss=0.6703, val_loss=0.6708, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'Thr 0.020 Epoch 7: train_loss=0.6660, val_loss=0.6655,\ntrain_CES=0.0000, val_CES=0.0000', '\\n', 'Thr 0.020 Epoch 8: train_loss=0.6626,\nval_loss=0.6608, train_CES=0.0000, val_CES=0.0000', '\\n', 'Thr 0.020 Epoch 9:\ntrain_loss=0.6568, val_loss=0.6560, train_CES=0.0000, val_CES=0.0000', '\\n',\n'Thr 0.020 Epoch 10: train_loss=0.6535, val_loss=0.6513, train_CES=0.0000,\nval_CES=0.0000', '\\n', '\\n=== Threshold = 0.05 ===', '\\n', 'Thr 0.050 Epoch 1:\ntrain_loss=0.6919, val_loss=0.6827, train_CES=0.0000, val_CES=0.0000', '\\n',\n'Thr 0.050 Epoch 2: train_loss=0.6836, val_loss=0.6776, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'Thr 0.050 Epoch 3: train_loss=0.6767, val_loss=0.6726,\ntrain_CES=0.0000, val_CES=0.0000', '\\n', 'Thr 0.050 Epoch 4: train_loss=0.6719,\nval_loss=0.6671, train_CES=0.0000, val_CES=0.0000', '\\n', 'Thr 0.050 Epoch 5:\ntrain_loss=0.6656, val_loss=0.6606, train_CES=0.0000, val_CES=0.0000', '\\n',\n'Thr 0.050 Epoch 6: train_loss=0.6590, val_loss=0.6527, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'Thr 0.050 Epoch 7: train_loss=0.6495, val_loss=0.6428,\ntrain_CES=0.0000, val_CES=0.0000', '\\n', 'Thr 0.050 Epoch 8: train_loss=0.6396,\nval_loss=0.6328, train_CES=0.0000, val_CES=0.0000', '\\n', 'Thr 0.050 Epoch 9:\ntrain_loss=0.6282, val_loss=0.6227, train_CES=0.0000, val_CES=0.0000', '\\n',\n'Thr 0.050 Epoch 10: train_loss=0.6235, val_loss=0.6123, train_CES=0.0000,\nval_CES=0.0000', '\\n', '\\n=== Threshold = 0.1 ===', '\\n', 'Thr 0.100 Epoch 1:\ntrain_loss=0.7268, val_loss=0.7033, train_CES=0.0000, val_CES=0.0000', '\\n',\n'Thr 0.100 Epoch 2: train_loss=0.7113, val_loss=0.6950, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'Thr 0.100 Epoch 3: train_loss=0.7012, val_loss=0.6926,\ntrain_CES=0.0000, val_CES=0.0000', '\\n', 'Thr 0.100 Epoch 4: train_loss=0.7029,\nval_loss=0.6924, train_CES=0.0000, val_CES=0.0000', '\\n', 'Thr 0.100 Epoch 5:\ntrain_loss=0.6964, val_loss=0.6926, train_CES=0.0000, val_CES=0.0000', '\\n',\n'Thr 0.100 Epoch 6: train_loss=0.6957, val_loss=0.6925, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'Thr 0.100 Epoch 7: train_loss=0.6974, val_loss=0.6931,\ntrain_CES=0.0000, val_CES=0.0000', '\\n', 'Thr 0.100 Epoch 8: train_loss=0.6946,\nval_loss=0.6928, train_CES=0.0000, val_CES=0.0000', '\\n', 'Thr 0.100 Epoch 9:\ntrain_loss=0.6966, val_loss=0.6929, train_CES=0.0000, val_CES=0.0000', '\\n',\n'Thr 0.100 Epoch 10: train_loss=0.6968, val_loss=0.6928, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'Execution time: a minute seconds (time limit is an\nhour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 1, in <module>\\n\nps = [torch.softmax(model(xi), dim=1).cpu().numpy() for _ in range(mc_T)]\\n\n^^^^\\nNameError: name \\'mc_T\\' is not defined\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Running weight_decay = 0', '\\n', 'WD 0 Epoch 1:\nval_loss=0.6818, train_CES=0.2848, val_CES=0.2773', '\\n', 'WD 0 Epoch 2:\nval_loss=0.6796, train_CES=0.2221, val_CES=0.2372', '\\n', 'WD 0 Epoch 3:\nval_loss=0.6765, train_CES=0.2873, val_CES=0.3789', '\\n', 'WD 0 Epoch 4:\nval_loss=0.6730, train_CES=0.3765, val_CES=0.3218', '\\n', 'WD 0 Epoch 5:\nval_loss=0.6682, train_CES=0.3476, val_CES=0.4035', '\\n', 'WD 0 Epoch 6:\nval_loss=0.6640, train_CES=0.3962, val_CES=0.4048', '\\n', 'WD 0 Epoch 7:\nval_loss=0.6592, train_CES=0.4000, val_CES=0.4595', '\\n', 'WD 0 Epoch 8:\nval_loss=0.6543, train_CES=0.4747, val_CES=0.4634', '\\n', 'WD 0 Epoch 9:\nval_loss=0.6487, train_CES=0.4056, val_CES=0.4242', '\\n', 'WD 0 Epoch 10:\nval_loss=0.6436, train_CES=0.3961, val_CES=0.5625', '\\n', 'Running weight_decay\n= 1e-05', '\\n', 'WD 1e-05 Epoch 1: val_loss=0.6939, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'WD 1e-05 Epoch 2: val_loss=0.6936, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'WD 1e-05 Epoch 3: val_loss=0.6931, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'WD 1e-05 Epoch 4: val_loss=0.6902, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'WD 1e-05 Epoch 5: val_loss=0.6873, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'WD 1e-05 Epoch 6: val_loss=0.6844, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'WD 1e-05 Epoch 7: val_loss=0.6816, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'WD 1e-05 Epoch 8: val_loss=0.6787, train_CES=0.3250,\nval_CES=0.0833', '\\n', 'WD 1e-05 Epoch 9: val_loss=0.6757, train_CES=0.3625,\nval_CES=0.1739', '\\n', 'WD 1e-05 Epoch 10: val_loss=0.6729, train_CES=0.3692,\nval_CES=0.3659', '\\n', 'Running weight_decay = 0.0001', '\\n', 'WD 0.0001 Epoch\n1: val_loss=0.6970, train_CES=0.0000, val_CES=0.0000', '\\n', 'WD 0.0001 Epoch 2:\nval_loss=0.6889, train_CES=0.0000, val_CES=0.0000', '\\n', 'WD 0.0001 Epoch 3:\nval_loss=0.6841, train_CES=0.0000, val_CES=0.0000', '\\n', 'WD 0.0001 Epoch 4:\nval_loss=0.6798, train_CES=0.0000, val_CES=0.0000', '\\n', 'WD 0.0001 Epoch 5:\nval_loss=0.6749, train_CES=0.0000, val_CES=0.0000', '\\n', 'WD 0.0001 Epoch 6:\nval_loss=0.6702, train_CES=0.0000, val_CES=0.0000', '\\n', 'WD 0.0001 Epoch 7:\nval_loss=0.6655, train_CES=0.0000, val_CES=0.0000', '\\n', 'WD 0.0001 Epoch 8:\nval_loss=0.6607, train_CES=0.0000, val_CES=0.0000', '\\n', 'WD 0.0001 Epoch 9:\nval_loss=0.6558, train_CES=0.0000, val_CES=0.0000', '\\n', 'WD 0.0001 Epoch 10:\nval_loss=0.6510, train_CES=0.0000, val_CES=0.0000', '\\n', 'Running weight_decay\n= 0.001', '\\n', 'WD 0.001 Epoch 1: val_loss=0.6830, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'WD 0.001 Epoch 2: val_loss=0.6780, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'WD 0.001 Epoch 3: val_loss=0.6734, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'WD 0.001 Epoch 4: val_loss=0.6680, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'WD 0.001 Epoch 5: val_loss=0.6620, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'WD 0.001 Epoch 6: val_loss=0.6539, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'WD 0.001 Epoch 7: val_loss=0.6452, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'WD 0.001 Epoch 8: val_loss=0.6358, train_CES=0.0000,\nval_CES=0.0000', '\\n', 'WD 0.001 Epoch 9: val_loss=0.6266, train_CES=0.7500,\nval_CES=1.0000', '\\n', 'WD 0.001 Epoch 10: val_loss=0.6172, train_CES=0.5673,\nval_CES=0.5455', '\\n', 'Execution time: a minute seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 25, in <module>\\n    titanic =\nload_dataset(\"titanic\")[\"train\"]\\n              ^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1652, in dataset_module_factory\\n    raise e1\nfrom None\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1578, in dataset_module_factory\\n    raise\nDatasetNotFoundError(f\"Dataset \\'{path}\\' doesn\\'t exist on the Hub or cannot be\naccessed.\") from e\\ndatasets.exceptions.DatasetNotFoundError: Dataset\n\\'titanic\\' doesn\\'t exist on the Hub or cannot be accessed.\\n', 'Execution\ntime: a moment seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\nStarting training with hidden_layer_size = 4',\n'\\n', 'Size 4, Epoch 1: train_loss=0.7064, train_CES=0.2231, val_loss=0.6987,\nval_CES=0.2917', '\\n', 'Size 4, Epoch 2: train_loss=0.7011, train_CES=0.1600,\nval_loss=0.6978, val_CES=0.0714', '\\n', 'Size 4, Epoch 3: train_loss=0.6980,\ntrain_CES=0.2000, val_loss=0.6966, val_CES=0.3333', '\\n', 'Size 4, Epoch 4:\ntrain_loss=0.6966, train_CES=0.0909, val_loss=0.6962, val_CES=0.0000', '\\n',\n'Size 4, Epoch 5: train_loss=0.6973, train_CES=0.0000, val_loss=0.6954,\nval_CES=0.0000', '\\n', 'Size 4, Epoch 6: train_loss=0.6989, train_CES=0.0000,\nval_loss=0.6952, val_CES=0.0000', '\\n', 'Size 4, Epoch 7: train_loss=0.6970,\ntrain_CES=0.0000, val_loss=0.6955, val_CES=0.0000', '\\n', 'Size 4, Epoch 8:\ntrain_loss=0.6947, train_CES=0.0000, val_loss=0.6948, val_CES=0.0000', '\\n',\n'Size 4, Epoch 9: train_loss=0.6935, train_CES=0.0000, val_loss=0.6942,\nval_CES=0.0000', '\\n', 'Size 4, Epoch 10: train_loss=0.6938, train_CES=0.0000,\nval_loss=0.6935, val_CES=0.0000', '\\n', 'Completed hidden_layer_size = 4', '\\n',\n'\\nStarting training with hidden_layer_size = 8', '\\n', 'Size 8, Epoch 1:\ntrain_loss=0.7120, train_CES=0.0000, val_loss=0.7021, val_CES=0.0000', '\\n',\n'Size 8, Epoch 2: train_loss=0.7006, train_CES=0.0000, val_loss=0.6971,\nval_CES=0.0000', '\\n', 'Size 8, Epoch 3: train_loss=0.6989, train_CES=0.0000,\nval_loss=0.6936, val_CES=0.0000', '\\n', 'Size 8, Epoch 4: train_loss=0.6958,\ntrain_CES=0.0000, val_loss=0.6905, val_CES=0.0000', '\\n', 'Size 8, Epoch 5:\ntrain_loss=0.6902, train_CES=0.0000, val_loss=0.6879, val_CES=0.0000', '\\n',\n'Size 8, Epoch 6: train_loss=0.6905, train_CES=0.0000, val_loss=0.6856,\nval_CES=0.0000', '\\n', 'Size 8, Epoch 7: train_loss=0.6855, train_CES=0.0000,\nval_loss=0.6837, val_CES=0.0000', '\\n', 'Size 8, Epoch 8: train_loss=0.6855,\ntrain_CES=0.0000, val_loss=0.6816, val_CES=0.0000', '\\n', 'Size 8, Epoch 9:\ntrain_loss=0.6834, train_CES=0.0000, val_loss=0.6801, val_CES=0.0000', '\\n',\n'Size 8, Epoch 10: train_loss=0.6835, train_CES=0.0000, val_loss=0.6784,\nval_CES=0.0000', '\\n', 'Completed hidden_layer_size = 8', '\\n', '\\nStarting\ntraining with hidden_layer_size = 16', '\\n', 'Size 16, Epoch 1:\ntrain_loss=0.7730, train_CES=0.0000, val_loss=0.7105, val_CES=0.0000', '\\n',\n'Size 16, Epoch 2: train_loss=0.7215, train_CES=0.0000, val_loss=0.6870,\nval_CES=0.0000', '\\n', 'Size 16, Epoch 3: train_loss=0.7011, train_CES=0.1389,\nval_loss=0.6781, val_CES=0.4286', '\\n', 'Size 16, Epoch 4: train_loss=0.6787,\ntrain_CES=0.3714, val_loss=0.6708, val_CES=0.5000', '\\n', 'Size 16, Epoch 5:\ntrain_loss=0.6776, train_CES=0.3077, val_loss=0.6634, val_CES=0.3000', '\\n',\n'Size 16, Epoch 6: train_loss=0.6737, train_CES=0.2375, val_loss=0.6575,\nval_CES=0.2632', '\\n', 'Size 16, Epoch 7: train_loss=0.6663, train_CES=0.3613,\nval_loss=0.6515, val_CES=0.2353', '\\n', 'Size 16, Epoch 8: train_loss=0.6560,\ntrain_CES=0.3929, val_loss=0.6430, val_CES=0.3143', '\\n', 'Size 16, Epoch 9:\ntrain_loss=0.6471, train_CES=0.3760, val_loss=0.6338, val_CES=0.3626', '\\n',\n'Size 16, Epoch 10: train_loss=0.6397, train_CES=0.3376, val_loss=0.6239,\nval_CES=0.2880', '\\n', 'Completed hidden_layer_size = 16', '\\n', '\\nStarting\ntraining with hidden_layer_size = 32', '\\n', 'Size 32, Epoch 1:\ntrain_loss=0.7062, train_CES=-0.0600, val_loss=0.6864, val_CES=0.0000', '\\n',\n'Size 32, Epoch 2: train_loss=0.6872, train_CES=0.0000, val_loss=0.6812,\nval_CES=0.0000', '\\n', 'Size 32, Epoch 3: train_loss=0.6812, train_CES=0.0000,\nval_loss=0.6745, val_CES=0.0000', '\\n', 'Size 32, Epoch 4: train_loss=0.6750,\ntrain_CES=0.0000, val_loss=0.6655, val_CES=0.0000', '\\n', 'Size 32, Epoch 5:\ntrain_loss=0.6595, train_CES=0.0000, val_loss=0.6553, val_CES=0.0000', '\\n',\n'Size 32, Epoch 6: train_loss=0.6502, train_CES=0.0000, val_loss=0.6442,\nval_CES=0.0000', '\\n', 'Size 32, Epoch 7: train_loss=0.6425, train_CES=0.0000,\nval_loss=0.6331, val_CES=0.0000', '\\n', 'Size 32, Epoch 8: train_loss=0.6332,\ntrain_CES=0.0000, val_loss=0.6210, val_CES=1.0000', '\\n', 'Size 32, Epoch 9:\ntrain_loss=0.6139, train_CES=0.0000, val_loss=0.6084, val_CES=0.0000', '\\n',\n'Size 32, Epoch 10: train_loss=0.6096, train_CES=0.8000, val_loss=0.5942,\nval_CES=0.0000', '\\n', 'Completed hidden_layer_size = 32', '\\n', '\\nStarting\ntraining with hidden_layer_size = 64', '\\n', 'Size 64, Epoch 1:\ntrain_loss=0.6908, train_CES=0.2667, val_loss=0.6733, val_CES=0.3333', '\\n',\n'Size 64, Epoch 2: train_loss=0.6804, train_CES=0.0714, val_loss=0.6586,\nval_CES=0.0000', '\\n', 'Size 64, Epoch 3: train_loss=0.6584, train_CES=0.6000,\nval_loss=0.6416, val_CES=0.6000', '\\n', 'Size 64, Epoch 4: train_loss=0.6406,\ntrain_CES=0.3810, val_loss=0.6207, val_CES=0.5556', '\\n', 'Size 64, Epoch 5:\ntrain_loss=0.6175, train_CES=0.5463, val_loss=0.5992, val_CES=0.4091', '\\n',\n'Size 64, Epoch 6: train_loss=0.5972, train_CES=0.4656, val_loss=0.5774,\nval_CES=0.4545', '\\n', 'Size 64, Epoch 7: train_loss=0.5762, train_CES=0.4651,\nval_loss=0.5548, val_CES=0.4667', '\\n', 'Size 64, Epoch 8: train_loss=0.5619,\ntrain_CES=0.5027, val_loss=0.5334, val_CES=0.4406', '\\n', 'Size 64, Epoch 9:\ntrain_loss=0.5410, train_CES=0.4524, val_loss=0.5111, val_CES=0.3867', '\\n',\n'Size 64, Epoch 10: train_loss=0.5240, train_CES=0.4799, val_loss=0.4889,\nval_CES=0.4649', '\\n', 'Completed hidden_layer_size = 64', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: a minute seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', 'Running weight_decay = 0', '\\n', 'Epoch 1:\nvalidation_loss = 0.7098, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 2: validation_loss =\n0.6998, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 3: validation_loss =\n0.6953, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 4: validation_loss =\n0.6929, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 5: validation_loss =\n0.6909, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = -1.0000', '\\n', 'Epoch 6: validation_loss\n= 0.6893, accuracy_gain_per_clarification_train = 1.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 7: validation_loss =\n0.6875, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 8: validation_loss =\n0.6852, accuracy_gain_per_clarification_train = 0.8000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 9: validation_loss =\n0.6833, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 10: validation_loss\n= 0.6813, accuracy_gain_per_clarification_train = 0.3846,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Running weight_decay =\n1e-05', '\\n', 'Epoch 1: validation_loss = 0.6854,\naccuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 2: validation_loss =\n0.6795, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 3: validation_loss =\n0.6743, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 4: validation_loss =\n0.6701, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 5: validation_loss =\n0.6651, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 6: validation_loss =\n0.6606, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 7: validation_loss =\n0.6559, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 8: validation_loss =\n0.6510, accuracy_gain_per_clarification_train = 0.4400,\naccuracy_gain_per_clarification_val = 1.0000', '\\n', 'Epoch 9: validation_loss =\n0.6458, accuracy_gain_per_clarification_train = 0.4896,\naccuracy_gain_per_clarification_val = 0.5417', '\\n', 'Epoch 10: validation_loss\n= 0.6399, accuracy_gain_per_clarification_train = 0.4890,\naccuracy_gain_per_clarification_val = 0.4091', '\\n', 'Running weight_decay =\n0.0001', '\\n', 'Epoch 1: validation_loss = 0.7107,\naccuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 2: validation_loss =\n0.6983, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 3: validation_loss =\n0.6923, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 4: validation_loss =\n0.6888, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 5: validation_loss =\n0.6859, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 6: validation_loss =\n0.6833, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 7: validation_loss =\n0.6808, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 8: validation_loss =\n0.6784, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 9: validation_loss =\n0.6764, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 10: validation_loss\n= 0.6748, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Running weight_decay =\n0.001', '\\n', 'Epoch 1: validation_loss = 0.7417,\naccuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 2: validation_loss =\n0.7196, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 3: validation_loss =\n0.7066, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 4: validation_loss =\n0.6962, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 5: validation_loss =\n0.6880, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 6: validation_loss =\n0.6813, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 7: validation_loss =\n0.6758, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 8: validation_loss =\n0.6713, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 9: validation_loss =\n0.6666, accuracy_gain_per_clarification_train = 0.0000,\naccuracy_gain_per_clarification_val = 0.0000', '\\n', 'Epoch 10: validation_loss\n= 0.6625, accuracy_gain_per_clarification_train = 0.1497,\naccuracy_gain_per_clarification_val = 0.1034', '\\n', 'Execution time: 5 seconds\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Traceback (most recent call last):\\n  File\n\"runfile.py\", line 110, in <module>\\n    iris_ds = load_dataset(\"iris\",\nsplit=\"train\")\\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 2062, in load_dataset\\n    builder_instance =\nload_dataset_builder(\\n                       ^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1782, in load_dataset_builder\\n\ndataset_module = dataset_module_factory(\\n\n^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1652, in dataset_module_factory\\n    raise e1\nfrom None\\n  File\n\"/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-\npackages/datasets/load.py\", line 1578, in dataset_module_factory\\n    raise\nDatasetNotFoundError(f\"Dataset \\'{path}\\' doesn\\'t exist on the Hub or cannot be\naccessed.\") from e\\ndatasets.exceptions.DatasetNotFoundError: Dataset \\'iris\\'\ndoesn\\'t exist on the Hub or cannot be accessed.\\n', 'Execution time: a moment\nseconds (time limit is an hour).']", "['Using device: cuda', '\\n', 'Starting run for mc_T = 2', '\\n', 'mc_T=2 Epoch 1:\nvalidation_loss = 0.6817', '\\n', 'mc_T=2 Epoch 1: train_CES = 0.2646, val_CES =\n0.2823', '\\n', 'mc_T=2 Epoch 2: validation_loss = 0.6795', '\\n', 'mc_T=2 Epoch\n2: train_CES = 0.2936, val_CES = 0.2759', '\\n', 'mc_T=2 Epoch 3: validation_loss\n= 0.6766', '\\n', 'mc_T=2 Epoch 3: train_CES = 0.3077, val_CES = 0.3833', '\\n',\n'mc_T=2 Epoch 4: validation_loss = 0.6726', '\\n', 'mc_T=2 Epoch 4: train_CES =\n0.3035, val_CES = 0.1395', '\\n', 'mc_T=2 Epoch 5: validation_loss = 0.6677',\n'\\n', 'mc_T=2 Epoch 5: train_CES = 0.3082, val_CES = 0.3571', '\\n', 'mc_T=2\nEpoch 6: validation_loss = 0.6631', '\\n', 'mc_T=2 Epoch 6: train_CES = 0.3893,\nval_CES = 0.3846', '\\n', 'mc_T=2 Epoch 7: validation_loss = 0.6586', '\\n',\n'mc_T=2 Epoch 7: train_CES = 0.4800, val_CES = 0.6667', '\\n', 'mc_T=2 Epoch 8:\nvalidation_loss = 0.6538', '\\n', 'mc_T=2 Epoch 8: train_CES = 0.4468, val_CES =\n0.2727', '\\n', 'mc_T=2 Epoch 9: validation_loss = 0.6483', '\\n', 'mc_T=2 Epoch\n9: train_CES = 0.3958, val_CES = 0.4615', '\\n', 'mc_T=2 Epoch 10:\nvalidation_loss = 0.6429', '\\n', 'mc_T=2 Epoch 10: train_CES = 0.3664, val_CES =\n0.3871', '\\n', 'Starting run for mc_T = 5', '\\n', 'mc_T=5 Epoch 1:\nvalidation_loss = 0.6941', '\\n', 'mc_T=5 Epoch 1: train_CES = 0.0000, val_CES =\n0.0000', '\\n', 'mc_T=5 Epoch 2: validation_loss = 0.6939', '\\n', 'mc_T=5 Epoch\n2: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'mc_T=5 Epoch 3: validation_loss\n= 0.6936', '\\n', 'mc_T=5 Epoch 3: train_CES = 0.0000, val_CES = 0.0000', '\\n',\n'mc_T=5 Epoch 4: validation_loss = 0.6916', '\\n', 'mc_T=5 Epoch 4: train_CES =\n0.0000, val_CES = 0.0000', '\\n', 'mc_T=5 Epoch 5: validation_loss = 0.6885',\n'\\n', 'mc_T=5 Epoch 5: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'mc_T=5\nEpoch 6: validation_loss = 0.6853', '\\n', 'mc_T=5 Epoch 6: train_CES = 0.0000,\nval_CES = 0.0000', '\\n', 'mc_T=5 Epoch 7: validation_loss = 0.6822', '\\n',\n'mc_T=5 Epoch 7: train_CES = 0.1818, val_CES = 0.0000', '\\n', 'mc_T=5 Epoch 8:\nvalidation_loss = 0.6795', '\\n', 'mc_T=5 Epoch 8: train_CES = 0.3000, val_CES =\n0.2857', '\\n', 'mc_T=5 Epoch 9: validation_loss = 0.6764', '\\n', 'mc_T=5 Epoch\n9: train_CES = 0.2222, val_CES = 0.2353', '\\n', 'mc_T=5 Epoch 10:\nvalidation_loss = 0.6731', '\\n', 'mc_T=5 Epoch 10: train_CES = 0.3486, val_CES =\n0.3721', '\\n', 'Starting run for mc_T = 10', '\\n', 'mc_T=10 Epoch 1:\nvalidation_loss = 0.6963', '\\n', 'mc_T=10 Epoch 1: train_CES = 0.0000, val_CES =\n0.0000', '\\n', 'mc_T=10 Epoch 2: validation_loss = 0.6879', '\\n', 'mc_T=10 Epoch\n2: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'mc_T=10 Epoch 3:\nvalidation_loss = 0.6833', '\\n', 'mc_T=10 Epoch 3: train_CES = 0.0000, val_CES =\n0.0000', '\\n', 'mc_T=10 Epoch 4: validation_loss = 0.6796', '\\n', 'mc_T=10 Epoch\n4: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'mc_T=10 Epoch 5:\nvalidation_loss = 0.6753', '\\n', 'mc_T=10 Epoch 5: train_CES = 0.0000, val_CES =\n0.0000', '\\n', 'mc_T=10 Epoch 6: validation_loss = 0.6707', '\\n', 'mc_T=10 Epoch\n6: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'mc_T=10 Epoch 7:\nvalidation_loss = 0.6651', '\\n', 'mc_T=10 Epoch 7: train_CES = 0.0000, val_CES =\n0.0000', '\\n', 'mc_T=10 Epoch 8: validation_loss = 0.6603', '\\n', 'mc_T=10 Epoch\n8: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'mc_T=10 Epoch 9:\nvalidation_loss = 0.6556', '\\n', 'mc_T=10 Epoch 9: train_CES = 0.0000, val_CES =\n0.0000', '\\n', 'mc_T=10 Epoch 10: validation_loss = 0.6508', '\\n', 'mc_T=10\nEpoch 10: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'Starting run for mc_T =\n20', '\\n', 'mc_T=20 Epoch 1: validation_loss = 0.6825', '\\n', 'mc_T=20 Epoch 1:\ntrain_CES = 0.0000, val_CES = 0.0000', '\\n', 'mc_T=20 Epoch 2: validation_loss =\n0.6770', '\\n', 'mc_T=20 Epoch 2: train_CES = 0.0000, val_CES = 0.0000', '\\n',\n'mc_T=20 Epoch 3: validation_loss = 0.6722', '\\n', 'mc_T=20 Epoch 3: train_CES =\n0.0000, val_CES = 0.0000', '\\n', 'mc_T=20 Epoch 4: validation_loss = 0.6667',\n'\\n', 'mc_T=20 Epoch 4: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'mc_T=20\nEpoch 5: validation_loss = 0.6605', '\\n', 'mc_T=20 Epoch 5: train_CES = 0.0000,\nval_CES = 0.0000', '\\n', 'mc_T=20 Epoch 6: validation_loss = 0.6530', '\\n',\n'mc_T=20 Epoch 6: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'mc_T=20 Epoch 7:\nvalidation_loss = 0.6436', '\\n', 'mc_T=20 Epoch 7: train_CES = 0.0000, val_CES =\n0.0000', '\\n', 'mc_T=20 Epoch 8: validation_loss = 0.6338', '\\n', 'mc_T=20 Epoch\n8: train_CES = 0.0000, val_CES = 0.0000', '\\n', 'mc_T=20 Epoch 9:\nvalidation_loss = 0.6235', '\\n', 'mc_T=20 Epoch 9: train_CES = 0.7778, val_CES =\n1.0000', '\\n', 'mc_T=20 Epoch 10: validation_loss = 0.6133', '\\n', 'mc_T=20\nEpoch 10: train_CES = 0.5255, val_CES = 0.4872', '\\n', 'Execution time: 2\nminutes seconds (time limit is an hour).']", "['Using device: cuda', '\\n', '\\nStarting training with hidden_layer_size = 4',\n'\\n', 'Size 4, Epoch 1: train_loss=0.7025, train_CES=0.0000, val_loss=0.6943,\nval_CES=0.0000', '\\n', 'Size 4, Epoch 2: train_loss=0.6987, train_CES=0.0000,\nval_loss=0.6920, val_CES=0.0000', '\\n', 'Size 4, Epoch 3: train_loss=0.6938,\ntrain_CES=0.0000, val_loss=0.6906, val_CES=0.0000', '\\n', 'Size 4, Epoch 4:\ntrain_loss=0.6931, train_CES=0.0000, val_loss=0.6902, val_CES=0.0000', '\\n',\n'Size 4, Epoch 5: train_loss=0.6916, train_CES=0.0000, val_loss=0.6893,\nval_CES=0.0000', '\\n', 'Size 4, Epoch 6: train_loss=0.6907, train_CES=0.0000,\nval_loss=0.6886, val_CES=0.0000', '\\n', 'Size 4, Epoch 7: train_loss=0.6893,\ntrain_CES=0.0000, val_loss=0.6876, val_CES=0.0000', '\\n', 'Size 4, Epoch 8:\ntrain_loss=0.6877, train_CES=0.0000, val_loss=0.6864, val_CES=0.0000', '\\n',\n'Size 4, Epoch 9: train_loss=0.6848, train_CES=0.0000, val_loss=0.6852,\nval_CES=0.0000', '\\n', 'Size 4, Epoch 10: train_loss=0.6866, train_CES=0.0000,\nval_loss=0.6845, val_CES=0.0000', '\\n', 'Completed hidden_layer_size = 4', '\\n',\n'\\nStarting training with hidden_layer_size = 8', '\\n', 'Size 8, Epoch 1:\ntrain_loss=0.7137, train_CES=0.0031, val_loss=0.6969, val_CES=0.0000', '\\n',\n'Size 8, Epoch 2: train_loss=0.7067, train_CES=0.0220, val_loss=0.6855,\nval_CES=0.0556', '\\n', 'Size 8, Epoch 3: train_loss=0.6891, train_CES=0.0698,\nval_loss=0.6798, val_CES=0.1250', '\\n', 'Size 8, Epoch 4: train_loss=0.6818,\ntrain_CES=0.2778, val_loss=0.6746, val_CES=0.4000', '\\n', 'Size 8, Epoch 5:\ntrain_loss=0.6814, train_CES=0.0435, val_loss=0.6704, val_CES=0.3333', '\\n',\n'Size 8, Epoch 6: train_loss=0.6802, train_CES=0.2424, val_loss=0.6663,\nval_CES=0.2222', '\\n', 'Size 8, Epoch 7: train_loss=0.6751, train_CES=0.3226,\nval_loss=0.6624, val_CES=0.6000', '\\n', 'Size 8, Epoch 8: train_loss=0.6676,\ntrain_CES=0.3051, val_loss=0.6581, val_CES=0.3077', '\\n', 'Size 8, Epoch 9:\ntrain_loss=0.6669, train_CES=0.3032, val_loss=0.6542, val_CES=0.5088', '\\n',\n'Size 8, Epoch 10: train_loss=0.6587, train_CES=0.3971, val_loss=0.6501,\nval_CES=0.3580', '\\n', 'Completed hidden_layer_size = 8', '\\n', '\\nStarting\ntraining with hidden_layer_size = 16', '\\n', 'Size 16, Epoch 1:\ntrain_loss=0.6955, train_CES=0.0000, val_loss=0.6945, val_CES=0.0000', '\\n',\n'Size 16, Epoch 2: train_loss=0.7053, train_CES=0.0000, val_loss=0.6914,\nval_CES=0.0000', '\\n', 'Size 16, Epoch 3: train_loss=0.6996, train_CES=0.0000,\nval_loss=0.6877, val_CES=0.0000', '\\n', 'Size 16, Epoch 4: train_loss=0.6891,\ntrain_CES=0.0000, val_loss=0.6845, val_CES=0.0000', '\\n', 'Size 16, Epoch 5:\ntrain_loss=0.6878, train_CES=0.0000, val_loss=0.6776, val_CES=0.0000', '\\n',\n'Size 16, Epoch 6: train_loss=0.6807, train_CES=0.1053, val_loss=0.6723,\nval_CES=0.0000', '\\n', 'Size 16, Epoch 7: train_loss=0.6751, train_CES=0.1875,\nval_loss=0.6670, val_CES=0.4444', '\\n', 'Size 16, Epoch 8: train_loss=0.6656,\ntrain_CES=0.1818, val_loss=0.6622, val_CES=0.1852', '\\n', 'Size 16, Epoch 9:\ntrain_loss=0.6643, train_CES=0.1975, val_loss=0.6578, val_CES=0.1500', '\\n',\n'Size 16, Epoch 10: train_loss=0.6581, train_CES=0.1790, val_loss=0.6520,\nval_CES=0.2075', '\\n', 'Completed hidden_layer_size = 16', '\\n', '\\nStarting\ntraining with hidden_layer_size = 32', '\\n', 'Size 32, Epoch 1:\ntrain_loss=0.7021, train_CES=0.0000, val_loss=0.6924, val_CES=0.0000', '\\n',\n'Size 32, Epoch 2: train_loss=0.6947, train_CES=0.0000, val_loss=0.6843,\nval_CES=0.0000', '\\n', 'Size 32, Epoch 3: train_loss=0.6833, train_CES=0.0000,\nval_loss=0.6747, val_CES=0.0000', '\\n', 'Size 32, Epoch 4: train_loss=0.6761,\ntrain_CES=0.0000, val_loss=0.6658, val_CES=0.0000', '\\n', 'Size 32, Epoch 5:\ntrain_loss=0.6621, train_CES=0.5000, val_loss=0.6563, val_CES=0.0000', '\\n',\n'Size 32, Epoch 6: train_loss=0.6563, train_CES=0.7778, val_loss=0.6469,\nval_CES=0.0000', '\\n', 'Size 32, Epoch 7: train_loss=0.6507, train_CES=0.1818,\nval_loss=0.6387, val_CES=0.1667', '\\n', 'Size 32, Epoch 8: train_loss=0.6353,\ntrain_CES=0.3043, val_loss=0.6279, val_CES=0.2143', '\\n', 'Size 32, Epoch 9:\ntrain_loss=0.6296, train_CES=0.3761, val_loss=0.6183, val_CES=0.3333', '\\n',\n'Size 32, Epoch 10: train_loss=0.6214, train_CES=0.3804, val_loss=0.6090,\nval_CES=0.3725', '\\n', 'Completed hidden_layer_size = 32', '\\n', '\\nStarting\ntraining with hidden_layer_size = 64', '\\n', 'Size 64, Epoch 1:\ntrain_loss=0.7060, train_CES=-0.2500, val_loss=0.6921, val_CES=-1.0000', '\\n',\n'Size 64, Epoch 2: train_loss=0.6903, train_CES=0.1667, val_loss=0.6782,\nval_CES=0.0000', '\\n', 'Size 64, Epoch 3: train_loss=0.6730, train_CES=0.6364,\nval_loss=0.6617, val_CES=0.6667', '\\n', 'Size 64, Epoch 4: train_loss=0.6528,\ntrain_CES=0.3704, val_loss=0.6396, val_CES=0.3333', '\\n', 'Size 64, Epoch 5:\ntrain_loss=0.6276, train_CES=0.4419, val_loss=0.6127, val_CES=0.4815', '\\n',\n'Size 64, Epoch 6: train_loss=0.6004, train_CES=0.4979, val_loss=0.5865,\nval_CES=0.4107', '\\n', 'Size 64, Epoch 7: train_loss=0.5769, train_CES=0.4063,\nval_loss=0.5613, val_CES=0.3953', '\\n', 'Size 64, Epoch 8: train_loss=0.5474,\ntrain_CES=0.4178, val_loss=0.5373, val_CES=0.4225', '\\n', 'Size 64, Epoch 9:\ntrain_loss=0.5280, train_CES=0.4102, val_loss=0.5162, val_CES=0.4068', '\\n',\n'Size 64, Epoch 10: train_loss=0.5134, train_CES=0.4249, val_loss=0.4964,\nval_CES=0.4010', '\\n', 'Completed hidden_layer_size = 64', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: a minute seconds (time limit is an\nhour).']", "['Using device: cuda', '\\n', '\\nStarting training with hidden_layer_size = 4',\n'\\n', 'Size 4, Epoch 1: train_loss=0.6855, train_CES=0.0000, val_loss=0.6827,\nval_CES=0.0000', '\\n', 'Size 4, Epoch 2: train_loss=0.6805, train_CES=0.0000,\nval_loss=0.6797, val_CES=0.0000', '\\n', 'Size 4, Epoch 3: train_loss=0.6777,\ntrain_CES=0.0000, val_loss=0.6769, val_CES=0.0000', '\\n', 'Size 4, Epoch 4:\ntrain_loss=0.6771, train_CES=0.0000, val_loss=0.6741, val_CES=0.0000', '\\n',\n'Size 4, Epoch 5: train_loss=0.6754, train_CES=0.0000, val_loss=0.6716,\nval_CES=0.0000', '\\n', 'Size 4, Epoch 6: train_loss=0.6722, train_CES=0.0000,\nval_loss=0.6691, val_CES=0.0000', '\\n', 'Size 4, Epoch 7: train_loss=0.6682,\ntrain_CES=0.0000, val_loss=0.6665, val_CES=0.0000', '\\n', 'Size 4, Epoch 8:\ntrain_loss=0.6656, train_CES=0.0000, val_loss=0.6635, val_CES=0.0000', '\\n',\n'Size 4, Epoch 9: train_loss=0.6638, train_CES=0.0000, val_loss=0.6612,\nval_CES=0.0000', '\\n', 'Size 4, Epoch 10: train_loss=0.6619, train_CES=0.0000,\nval_loss=0.6576, val_CES=0.0000', '\\n', 'Completed hidden_layer_size = 4', '\\n',\n'\\nStarting training with hidden_layer_size = 8', '\\n', 'Size 8, Epoch 1:\ntrain_loss=0.7378, train_CES=0.0000, val_loss=0.7293, val_CES=0.0000', '\\n',\n'Size 8, Epoch 2: train_loss=0.7215, train_CES=0.0000, val_loss=0.7142,\nval_CES=0.0000', '\\n', 'Size 8, Epoch 3: train_loss=0.7123, train_CES=0.0000,\nval_loss=0.7044, val_CES=0.0000', '\\n', 'Size 8, Epoch 4: train_loss=0.7002,\ntrain_CES=0.0000, val_loss=0.6989, val_CES=0.0000', '\\n', 'Size 8, Epoch 5:\ntrain_loss=0.7007, train_CES=0.0000, val_loss=0.6959, val_CES=0.0000', '\\n',\n'Size 8, Epoch 6: train_loss=0.6947, train_CES=0.0000, val_loss=0.6937,\nval_CES=0.0000', '\\n', 'Size 8, Epoch 7: train_loss=0.6932, train_CES=0.0000,\nval_loss=0.6927, val_CES=0.0000', '\\n', 'Size 8, Epoch 8: train_loss=0.6938,\ntrain_CES=0.0000, val_loss=0.6913, val_CES=0.0000', '\\n', 'Size 8, Epoch 9:\ntrain_loss=0.6905, train_CES=0.0000, val_loss=0.6897, val_CES=0.0000', '\\n',\n'Size 8, Epoch 10: train_loss=0.6902, train_CES=0.0000, val_loss=0.6883,\nval_CES=0.0000', '\\n', 'Completed hidden_layer_size = 8', '\\n', '\\nStarting\ntraining with hidden_layer_size = 16', '\\n', 'Size 16, Epoch 1:\ntrain_loss=0.7027, train_CES=0.0882, val_loss=0.6824, val_CES=0.0000', '\\n',\n'Size 16, Epoch 2: train_loss=0.6904, train_CES=0.2727, val_loss=0.6782,\nval_CES=0.5000', '\\n', 'Size 16, Epoch 3: train_loss=0.6863, train_CES=-1.0000,\nval_loss=0.6746, val_CES=0.0000', '\\n', 'Size 16, Epoch 4: train_loss=0.6784,\ntrain_CES=0.0000, val_loss=0.6693, val_CES=0.0000', '\\n', 'Size 16, Epoch 5:\ntrain_loss=0.6709, train_CES=0.0000, val_loss=0.6583, val_CES=0.0000', '\\n',\n'Size 16, Epoch 6: train_loss=0.6519, train_CES=0.3333, val_loss=0.6477,\nval_CES=0.5000', '\\n', 'Size 16, Epoch 7: train_loss=0.6511, train_CES=0.3333,\nval_loss=0.6395, val_CES=0.6000', '\\n', 'Size 16, Epoch 8: train_loss=0.6416,\ntrain_CES=0.4286, val_loss=0.6304, val_CES=0.5000', '\\n', 'Size 16, Epoch 9:\ntrain_loss=0.6334, train_CES=0.4286, val_loss=0.6215, val_CES=0.5000', '\\n',\n'Size 16, Epoch 10: train_loss=0.6245, train_CES=0.4298, val_loss=0.6130,\nval_CES=0.5111', '\\n', 'Completed hidden_layer_size = 16', '\\n', '\\nStarting\ntraining with hidden_layer_size = 32', '\\n', 'Size 32, Epoch 1:\ntrain_loss=0.7156, train_CES=0.0000, val_loss=0.6852, val_CES=0.0000', '\\n',\n'Size 32, Epoch 2: train_loss=0.6860, train_CES=0.0000, val_loss=0.6770,\nval_CES=0.0000', '\\n', 'Size 32, Epoch 3: train_loss=0.6762, train_CES=0.0000,\nval_loss=0.6686, val_CES=0.0000', '\\n', 'Size 32, Epoch 4: train_loss=0.6689,\ntrain_CES=0.0000, val_loss=0.6603, val_CES=0.0000', '\\n', 'Size 32, Epoch 5:\ntrain_loss=0.6580, train_CES=0.0000, val_loss=0.6505, val_CES=0.0000', '\\n',\n'Size 32, Epoch 6: train_loss=0.6509, train_CES=0.0000, val_loss=0.6407,\nval_CES=0.0000', '\\n', 'Size 32, Epoch 7: train_loss=0.6394, train_CES=0.0000,\nval_loss=0.6304, val_CES=0.0000', '\\n', 'Size 32, Epoch 8: train_loss=0.6291,\ntrain_CES=0.0000, val_loss=0.6196, val_CES=0.0000', '\\n', 'Size 32, Epoch 9:\ntrain_loss=0.6211, train_CES=0.0000, val_loss=0.6097, val_CES=0.0000', '\\n',\n'Size 32, Epoch 10: train_loss=0.6061, train_CES=0.6667, val_loss=0.5979,\nval_CES=0.6667', '\\n', 'Completed hidden_layer_size = 32', '\\n', '\\nStarting\ntraining with hidden_layer_size = 64', '\\n', 'Size 64, Epoch 1:\ntrain_loss=0.6952, train_CES=0.6000, val_loss=0.6742, val_CES=0.6667', '\\n',\n'Size 64, Epoch 2: train_loss=0.6798, train_CES=0.5217, val_loss=0.6572,\nval_CES=0.2000', '\\n', 'Size 64, Epoch 3: train_loss=0.6520, train_CES=0.6923,\nval_loss=0.6366, val_CES=0.4000', '\\n', 'Size 64, Epoch 4: train_loss=0.6339,\ntrain_CES=0.4878, val_loss=0.6135, val_CES=0.3333', '\\n', 'Size 64, Epoch 5:\ntrain_loss=0.6023, train_CES=0.4929, val_loss=0.5877, val_CES=0.4444', '\\n',\n'Size 64, Epoch 6: train_loss=0.5851, train_CES=0.4549, val_loss=0.5657,\nval_CES=0.4483', '\\n', 'Size 64, Epoch 7: train_loss=0.5629, train_CES=0.4701,\nval_loss=0.5442, val_CES=0.4532', '\\n', 'Size 64, Epoch 8: train_loss=0.5418,\ntrain_CES=0.4561, val_loss=0.5232, val_CES=0.4588', '\\n', 'Size 64, Epoch 9:\ntrain_loss=0.5160, train_CES=0.4583, val_loss=0.5032, val_CES=0.4444', '\\n',\n'Size 64, Epoch 10: train_loss=0.4969, train_CES=0.4513, val_loss=0.4830,\nval_CES=0.4275', '\\n', 'Completed hidden_layer_size = 64', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: 16 minutes seconds (time limit is\nan hour).']", "['Using device: cuda', '\\n', '\\nStarting training with hidden_layer_size = 4',\n'\\n', 'Size 4, Epoch 1: train_loss=0.7109, train_CES=0.0000, val_loss=0.7081,\nval_CES=0.0000', '\\n', 'Size 4, Epoch 2: train_loss=0.7048, train_CES=0.0000,\nval_loss=0.7033, val_CES=0.0000', '\\n', 'Size 4, Epoch 3: train_loss=0.7028,\ntrain_CES=0.0000, val_loss=0.6998, val_CES=0.0000', '\\n', 'Size 4, Epoch 4:\ntrain_loss=0.6972, train_CES=0.0000, val_loss=0.6976, val_CES=0.0000', '\\n',\n'Size 4, Epoch 5: train_loss=0.6985, train_CES=0.0000, val_loss=0.6962,\nval_CES=0.0000', '\\n', 'Size 4, Epoch 6: train_loss=0.6938, train_CES=0.0000,\nval_loss=0.6946, val_CES=0.0000', '\\n', 'Size 4, Epoch 7: train_loss=0.6923,\ntrain_CES=0.0000, val_loss=0.6934, val_CES=0.0000', '\\n', 'Size 4, Epoch 8:\ntrain_loss=0.6935, train_CES=0.0000, val_loss=0.6926, val_CES=0.0000', '\\n',\n'Size 4, Epoch 9: train_loss=0.6925, train_CES=0.0000, val_loss=0.6917,\nval_CES=0.0000', '\\n', 'Size 4, Epoch 10: train_loss=0.6915, train_CES=0.0000,\nval_loss=0.6906, val_CES=0.0000', '\\n', 'Completed hidden_layer_size = 4', '\\n',\n'\\nStarting training with hidden_layer_size = 8', '\\n', 'Size 8, Epoch 1:\ntrain_loss=0.7021, train_CES=0.7143, val_loss=0.6953, val_CES=0.0000', '\\n',\n'Size 8, Epoch 2: train_loss=0.7022, train_CES=0.0000, val_loss=0.6943,\nval_CES=0.0000', '\\n', 'Size 8, Epoch 3: train_loss=0.7006, train_CES=0.0000,\nval_loss=0.6939, val_CES=0.0000', '\\n', 'Size 8, Epoch 4: train_loss=0.6952,\ntrain_CES=0.0000, val_loss=0.6933, val_CES=0.0000', '\\n', 'Size 8, Epoch 5:\ntrain_loss=0.7011, train_CES=0.0000, val_loss=0.6928, val_CES=0.0000', '\\n',\n'Size 8, Epoch 6: train_loss=0.6978, train_CES=0.0000, val_loss=0.6919,\nval_CES=0.0000', '\\n', 'Size 8, Epoch 7: train_loss=0.6924, train_CES=0.0000,\nval_loss=0.6909, val_CES=0.0000', '\\n', 'Size 8, Epoch 8: train_loss=0.6934,\ntrain_CES=0.0000, val_loss=0.6900, val_CES=0.0000', '\\n', 'Size 8, Epoch 9:\ntrain_loss=0.6905, train_CES=0.0000, val_loss=0.6888, val_CES=0.0000', '\\n',\n'Size 8, Epoch 10: train_loss=0.6897, train_CES=0.0000, val_loss=0.6874,\nval_CES=0.0000', '\\n', 'Completed hidden_layer_size = 8', '\\n', '\\nStarting\ntraining with hidden_layer_size = 16', '\\n', 'Size 16, Epoch 1:\ntrain_loss=0.6993, train_CES=0.0000, val_loss=0.6895, val_CES=0.0000', '\\n',\n'Size 16, Epoch 2: train_loss=0.6896, train_CES=-1.0000, val_loss=0.6828,\nval_CES=0.0000', '\\n', 'Size 16, Epoch 3: train_loss=0.6838, train_CES=1.0000,\nval_loss=0.6774, val_CES=0.0000', '\\n', 'Size 16, Epoch 4: train_loss=0.6783,\ntrain_CES=0.2308, val_loss=0.6702, val_CES=0.0000', '\\n', 'Size 16, Epoch 5:\ntrain_loss=0.6710, train_CES=0.2200, val_loss=0.6626, val_CES=0.0000', '\\n',\n'Size 16, Epoch 6: train_loss=0.6666, train_CES=0.2698, val_loss=0.6553,\nval_CES=0.3043', '\\n', 'Size 16, Epoch 7: train_loss=0.6571, train_CES=0.3242,\nval_loss=0.6472, val_CES=0.2500', '\\n', 'Size 16, Epoch 8: train_loss=0.6518,\ntrain_CES=0.3485, val_loss=0.6398, val_CES=0.2540', '\\n', 'Size 16, Epoch 9:\ntrain_loss=0.6450, train_CES=0.3466, val_loss=0.6323, val_CES=0.3596', '\\n',\n'Size 16, Epoch 10: train_loss=0.6376, train_CES=0.3912, val_loss=0.6244,\nval_CES=0.3684', '\\n', 'Completed hidden_layer_size = 16', '\\n', '\\nStarting\ntraining with hidden_layer_size = 32', '\\n', 'Size 32, Epoch 1:\ntrain_loss=0.6888, train_CES=0.0000, val_loss=0.6763, val_CES=0.0000', '\\n',\n'Size 32, Epoch 2: train_loss=0.6786, train_CES=0.0000, val_loss=0.6647,\nval_CES=0.0000', '\\n', 'Size 32, Epoch 3: train_loss=0.6629, train_CES=0.0000,\nval_loss=0.6510, val_CES=0.0000', '\\n', 'Size 32, Epoch 4: train_loss=0.6461,\ntrain_CES=0.5000, val_loss=0.6369, val_CES=0.0000', '\\n', 'Size 32, Epoch 5:\ntrain_loss=0.6334, train_CES=0.4255, val_loss=0.6240, val_CES=0.5714', '\\n',\n'Size 32, Epoch 6: train_loss=0.6248, train_CES=0.4697, val_loss=0.6109,\nval_CES=0.3514', '\\n', 'Size 32, Epoch 7: train_loss=0.6113, train_CES=0.4656,\nval_loss=0.5980, val_CES=0.4727', '\\n', 'Size 32, Epoch 8: train_loss=0.6008,\ntrain_CES=0.4919, val_loss=0.5846, val_CES=0.4123', '\\n', 'Size 32, Epoch 9:\ntrain_loss=0.5912, train_CES=0.4784, val_loss=0.5714, val_CES=0.3822', '\\n',\n'Size 32, Epoch 10: train_loss=0.5787, train_CES=0.4704, val_loss=0.5581,\nval_CES=0.3990', '\\n', 'Completed hidden_layer_size = 32', '\\n', '\\nStarting\ntraining with hidden_layer_size = 64', '\\n', 'Size 64, Epoch 1:\ntrain_loss=0.7056, train_CES=0.3333, val_loss=0.6897, val_CES=0.5000', '\\n',\n'Size 64, Epoch 2: train_loss=0.6927, train_CES=0.2667, val_loss=0.6745,\nval_CES=0.6667', '\\n', 'Size 64, Epoch 3: train_loss=0.6768, train_CES=0.1765,\nval_loss=0.6558, val_CES=0.0000', '\\n', 'Size 64, Epoch 4: train_loss=0.6503,\ntrain_CES=0.3000, val_loss=0.6282, val_CES=0.2500', '\\n', 'Size 64, Epoch 5:\ntrain_loss=0.6216, train_CES=0.3077, val_loss=0.5969, val_CES=0.3103', '\\n',\n'Size 64, Epoch 6: train_loss=0.5958, train_CES=0.4404, val_loss=0.5683,\nval_CES=0.4259', '\\n', 'Size 64, Epoch 7: train_loss=0.5691, train_CES=0.4043,\nval_loss=0.5426, val_CES=0.3723', '\\n', 'Size 64, Epoch 8: train_loss=0.5515,\ntrain_CES=0.3767, val_loss=0.5184, val_CES=0.3562', '\\n', 'Size 64, Epoch 9:\ntrain_loss=0.5290, train_CES=0.3923, val_loss=0.4963, val_CES=0.4091', '\\n',\n'Size 64, Epoch 10: train_loss=0.5047, train_CES=0.4312, val_loss=0.4741,\nval_CES=0.4286', '\\n', 'Completed hidden_layer_size = 64', '\\n', 'Saved\nexperiment_data.npy', '\\n', 'Execution time: a minute seconds (time limit is an\nhour).']", ""], "analysis": ["", "RuntimeError occurs in compute_CES because code calls .numpy() on tensors that\nrequire grad. The model outputs during MC-dropout sampling still have\nrequires_grad=True. To fix, wrap the inference calls in a torch.no_grad() block\nor detach the output tensor before converting to numpy, e.g., use\n`torch.softmax(model(xi), dim=1).detach().cpu().numpy()`. This will resolve the\nnumpy() error.", "", "The script executed to completion without runtime errors but fails to meet the\nsub-stage requirements and shows undesirable behavior for larger batch sizes: 1.\nMissing Datasets: The code never loads or evaluates on the two additional\nHuggingFace datasets specified in Sub-stage 1. This violates the experiment plan\nand must be fixed by integrating e.g. `datasets.load_dataset('ag_news')` and\n`datasets.load_dataset('imdb')` (or other relevant tasks). 2. Zero\nClarifications at High Batch Sizes: For batch sizes \u226564, the uncertainty-based\nthreshold (0.02) is never exceeded, so no clarification steps are triggered and\nthe CES metric remains 0. This suggests the threshold or MC-dropout settings are\nnot robust across batch sizes. Proposed fix: lower the threshold or increase the\nnumber of MC samples (mc_T), and verify that the variance computation is correct\nto ensure clarifications occur as intended.", "", "", "The script references mc_T in the first two lines before mc_T is defined,\ncausing a NameError. Move the definition of mc_T (and threshold) above these\nlines or remove the extraneous initial ps assignments to fix the error.", "The script ran without crashes, but it did not fulfill the sub\u2010stage requirement\nto introduce two new HuggingFace datasets for testing. All experiments were only\non the synthetic XOR task. To fix this, the code should load two appropriate\nHuggingFace datasets (for instance, the \"iris\" numeric classification dataset\nand a second numeric dataset like \"wine-quality\"), wrap them in DataLoaders, and\nadd them under experiment_data (with their own hyperparameters, losses, metrics,\netc.) so that the model is evaluated on at least two additional benchmarks.", "The script fails because load_dataset(\"titanic\") isn\u2019t a valid HuggingFace\ndataset name. You need to specify a valid dataset loader, for example using the\nOpenML builder: `load_dataset(\"openml\", \"titanic\")` or refer to a hosted Titanic\ndataset (e.g., `user/titanic`) on the Hub. Alternatively, pick a different\navailable classification dataset (e.g., \u2018iris\u2019 or \u2018wine_quality\u2019).", "", "", "The script crashes at `load_dataset(\"iris\", split=\"train\")` with a\nDatasetNotFoundError because there is no publicly available Hugging Face dataset\nnamed exactly 'iris'. To fix this, replace the call with a valid dataset\nidentifier, for example:  - Use OpenML\u2019s iris dataset: `load_dataset(\"openml\",\ndata_id=61)` - Or employ scikit-learn\u2019s loader:    ```python   from\nsklearn.datasets import load_iris   data = load_iris()   feat_iris =\ndata.data[:,:2]   lab_iris = data.target   ```  Adjust subsequent code\naccordingly to use the loaded data.", "", "", "", "", ""], "exc_type": [null, "RuntimeError", null, null, null, null, "NameError", null, "DatasetNotFoundError", null, null, "DatasetNotFoundError", null, null, null, null, null], "exc_info": [null, {"args": ["Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."]}, null, null, null, null, {"args": ["name 'mc_T' is not defined"], "name": "mc_T"}, null, {"args": ["Dataset 'titanic' doesn't exist on the Hub or cannot be accessed."]}, null, null, {"args": ["Dataset 'iris' doesn't exist on the Hub or cannot be accessed."]}, null, null, null, null, null], "exc_stack": [null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 111, "<module>", "CES_tr = compute_CES(train_loader, model)"], ["runfile.py", 61, "compute_CES", "ps = [torch.softmax(model(xi), dim=1).cpu().numpy() for _ in range(mc_T)]"], ["runfile.py", 61, "<listcomp>", "ps = [torch.softmax(model(xi), dim=1).cpu().numpy() for _ in range(mc_T)]"]], null, null, null, null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 1, "<module>", "ps = [torch.softmax(model(xi), dim=1).cpu().numpy() for _ in range(mc_T)]"]], null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 25, "<module>", "titanic = load_dataset(\"titanic\")[\"train\"]"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1652, "dataset_module_factory", "raise e1 from None"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1578, "dataset_module_factory", "raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e"]], null, null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 110, "<module>", "iris_ds = load_dataset(\"iris\", split=\"train\")"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 2062, "load_dataset", "builder_instance = load_dataset_builder("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1782, "load_dataset_builder", "dataset_module = dataset_module_factory("], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1652, "dataset_module_factory", "raise e1 from None"], ["/home/chenhui/miniconda3/envs/ai_scientist/lib/python3.11/site-packages/datasets/load.py", 1578, "dataset_module_factory", "raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e"]], null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "training loss", "lower_is_better": true, "description": "Loss measured on the training set", "data": [{"dataset_name": "synthetic_xor", "final_value": 0.6727, "best_value": 0.6727}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss measured on the validation set", "data": [{"dataset_name": "synthetic_xor", "final_value": 0.655, "best_value": 0.655}]}, {"metric_name": "training CES", "lower_is_better": true, "description": "Cross-Entropy Score on the training set", "data": [{"dataset_name": "synthetic_xor", "final_value": 0.4358, "best_value": 0.4358}]}, {"metric_name": "validation CES", "lower_is_better": true, "description": "Cross-Entropy Score on the validation set", "data": [{"dataset_name": "synthetic_xor", "final_value": 0.4792, "best_value": 0.4792}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training CES", "lower_is_better": true, "description": "Cross-entropy score on the training set", "data": [{"dataset_name": "synthetic_xor", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation CES", "lower_is_better": true, "description": "Cross-entropy score on the validation set", "data": [{"dataset_name": "synthetic_xor", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "synthetic_xor", "final_value": 0.4548, "best_value": 0.4548}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "synthetic_xor", "final_value": 0.3787, "best_value": 0.3787}]}]}, {"metric_names": [{"metric_name": "train CES score", "lower_is_better": true, "description": "CES score on the training dataset; lower is better.", "data": [{"dataset_name": "synthetic_xor", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation CES score", "lower_is_better": true, "description": "CES score on the validation dataset; lower is better.", "data": [{"dataset_name": "synthetic_xor", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Training loss", "data": [{"dataset_name": "synthetic_xor (dropout=0.0)", "final_value": 0.6295, "best_value": 0.6295}, {"dataset_name": "synthetic_xor (dropout=0.1)", "final_value": 0.6706, "best_value": 0.6706}, {"dataset_name": "synthetic_xor (dropout=0.2)", "final_value": 0.6403, "best_value": 0.6403}, {"dataset_name": "synthetic_xor (dropout=0.3)", "final_value": 0.6108, "best_value": 0.6108}, {"dataset_name": "synthetic_xor (dropout=0.5)", "final_value": 0.6944, "best_value": 0.6944}, {"dataset_name": "synthetic_xor (dropout=0.7)", "final_value": 0.6893, "best_value": 0.6893}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss", "data": [{"dataset_name": "synthetic_xor (dropout=0.0)", "final_value": 0.6282, "best_value": 0.6282}, {"dataset_name": "synthetic_xor (dropout=0.1)", "final_value": 0.6661, "best_value": 0.6661}, {"dataset_name": "synthetic_xor (dropout=0.2)", "final_value": 0.6438, "best_value": 0.6438}, {"dataset_name": "synthetic_xor (dropout=0.3)", "final_value": 0.604, "best_value": 0.604}, {"dataset_name": "synthetic_xor (dropout=0.5)", "final_value": 0.6929, "best_value": 0.6929}, {"dataset_name": "synthetic_xor (dropout=0.7)", "final_value": 0.682, "best_value": 0.682}]}, {"metric_name": "train classification error score", "lower_is_better": true, "description": "Training classification error score (CES)", "data": [{"dataset_name": "synthetic_xor (dropout=0.0)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (dropout=0.1)", "final_value": 1.0, "best_value": 1.0}, {"dataset_name": "synthetic_xor (dropout=0.2)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (dropout=0.3)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (dropout=0.5)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (dropout=0.7)", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation classification error score", "lower_is_better": true, "description": "Validation classification error score (CES)", "data": [{"dataset_name": "synthetic_xor (dropout=0.0)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (dropout=0.1)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (dropout=0.2)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (dropout=0.3)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (dropout=0.5)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (dropout=0.7)", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train cross entropy score", "lower_is_better": true, "description": "Cross-entropy score on the training dataset for synthetic_xor", "data": [{"dataset_name": "synthetic_xor (threshold=0.005)", "final_value": 0.2982, "best_value": 0.2982}, {"dataset_name": "synthetic_xor (threshold=0.010)", "final_value": 0.3582, "best_value": 0.3582}, {"dataset_name": "synthetic_xor (threshold=0.020)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (threshold=0.050)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (threshold=0.100)", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation cross entropy score", "lower_is_better": true, "description": "Cross-entropy score on the validation dataset for synthetic_xor", "data": [{"dataset_name": "synthetic_xor (threshold=0.005)", "final_value": 0.275, "best_value": 0.275}, {"dataset_name": "synthetic_xor (threshold=0.010)", "final_value": 0.36, "best_value": 0.36}, {"dataset_name": "synthetic_xor (threshold=0.020)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (threshold=0.050)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (threshold=0.100)", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Training loss on the synthetic_xor dataset", "data": [{"dataset_name": "synthetic_xor (threshold=0.005)", "final_value": 0.6547, "best_value": 0.6547}, {"dataset_name": "synthetic_xor (threshold=0.010)", "final_value": 0.6826, "best_value": 0.6826}, {"dataset_name": "synthetic_xor (threshold=0.020)", "final_value": 0.6535, "best_value": 0.6535}, {"dataset_name": "synthetic_xor (threshold=0.050)", "final_value": 0.6235, "best_value": 0.6235}, {"dataset_name": "synthetic_xor (threshold=0.100)", "final_value": 0.6968, "best_value": 0.6968}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss on the synthetic_xor dataset", "data": [{"dataset_name": "synthetic_xor (threshold=0.005)", "final_value": 0.6427, "best_value": 0.6427}, {"dataset_name": "synthetic_xor (threshold=0.010)", "final_value": 0.6733, "best_value": 0.6733}, {"dataset_name": "synthetic_xor (threshold=0.020)", "final_value": 0.6513, "best_value": 0.6513}, {"dataset_name": "synthetic_xor (threshold=0.050)", "final_value": 0.6123, "best_value": 0.6123}, {"dataset_name": "synthetic_xor (threshold=0.100)", "final_value": 0.6928, "best_value": 0.6928}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training cross-entropy score", "lower_is_better": true, "description": "Cross-entropy score on the training set", "data": [{"dataset_name": "synthetic_xor (weight_decay=0)", "final_value": 0.3961, "best_value": 0.3961}, {"dataset_name": "synthetic_xor (weight_decay=1e-05)", "final_value": 0.3692, "best_value": 0.3692}, {"dataset_name": "synthetic_xor (weight_decay=0.0001)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (weight_decay=0.001)", "final_value": 0.5673, "best_value": 0.5673}]}, {"metric_name": "validation cross-entropy score", "lower_is_better": true, "description": "Cross-entropy score on the validation set", "data": [{"dataset_name": "synthetic_xor (weight_decay=0)", "final_value": 0.5625, "best_value": 0.5625}, {"dataset_name": "synthetic_xor (weight_decay=1e-05)", "final_value": 0.3659, "best_value": 0.3659}, {"dataset_name": "synthetic_xor (weight_decay=0.0001)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (weight_decay=0.001)", "final_value": 0.5455, "best_value": 0.5455}]}, {"metric_name": "training loss", "lower_is_better": true, "description": "Loss on the training set", "data": [{"dataset_name": "synthetic_xor (weight_decay=0)", "final_value": 0.6547, "best_value": 0.6547}, {"dataset_name": "synthetic_xor (weight_decay=1e-05)", "final_value": 0.6746, "best_value": 0.6746}, {"dataset_name": "synthetic_xor (weight_decay=0.0001)", "final_value": 0.6531, "best_value": 0.6531}, {"dataset_name": "synthetic_xor (weight_decay=0.001)", "final_value": 0.6269, "best_value": 0.6269}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set", "data": [{"dataset_name": "synthetic_xor (weight_decay=0)", "final_value": 0.6436, "best_value": 0.6436}, {"dataset_name": "synthetic_xor (weight_decay=1e-05)", "final_value": 0.6729, "best_value": 0.6729}, {"dataset_name": "synthetic_xor (weight_decay=0.0001)", "final_value": 0.651, "best_value": 0.651}, {"dataset_name": "synthetic_xor (weight_decay=0.001)", "final_value": 0.6172, "best_value": 0.6172}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "training cross-entropy loss", "lower_is_better": true, "description": "Final cross-entropy loss on the synthetic_xor training dataset", "data": [{"dataset_name": "synthetic_xor", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation cross-entropy loss", "lower_is_better": true, "description": "Final cross-entropy loss on the synthetic_xor validation dataset", "data": [{"dataset_name": "synthetic_xor", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "Training accuracy gain per clarification", "lower_is_better": false, "description": "Final training accuracy gain per clarification for each weight decay setting", "data": [{"dataset_name": "weight_decay=0", "final_value": 0.3846, "best_value": 0.3846}, {"dataset_name": "weight_decay=1e-05", "final_value": 0.489, "best_value": 0.489}, {"dataset_name": "weight_decay=0.0001", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "weight_decay=0.001", "final_value": 0.1497, "best_value": 0.1497}]}, {"metric_name": "Training loss", "lower_is_better": true, "description": "Final training loss for each weight decay setting", "data": [{"dataset_name": "weight_decay=0", "final_value": 0.688, "best_value": 0.688}, {"dataset_name": "weight_decay=1e-05", "final_value": 0.6443, "best_value": 0.6443}, {"dataset_name": "weight_decay=0.0001", "final_value": 0.6761, "best_value": 0.6761}, {"dataset_name": "weight_decay=0.001", "final_value": 0.6674, "best_value": 0.6674}]}, {"metric_name": "Validation accuracy gain per clarification", "lower_is_better": false, "description": "Final validation accuracy gain per clarification for each weight decay setting", "data": [{"dataset_name": "weight_decay=0", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "weight_decay=1e-05", "final_value": 0.4091, "best_value": 0.4091}, {"dataset_name": "weight_decay=0.0001", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "weight_decay=0.001", "final_value": 0.1034, "best_value": 0.1034}]}, {"metric_name": "Validation loss", "lower_is_better": true, "description": "Final validation loss for each weight decay setting", "data": [{"dataset_name": "weight_decay=0", "final_value": 0.6813, "best_value": 0.6813}, {"dataset_name": "weight_decay=1e-05", "final_value": 0.6399, "best_value": 0.6399}, {"dataset_name": "weight_decay=0.0001", "final_value": 0.6748, "best_value": 0.6748}, {"dataset_name": "weight_decay=0.001", "final_value": 0.6625, "best_value": 0.6625}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "Final Training Causal Effectiveness Score", "lower_is_better": false, "description": "Causal Effectiveness Score on the training set for different mc_T values on the synthetic_xor dataset.", "data": [{"dataset_name": "synthetic_xor (mc_T=2)", "final_value": 0.3664, "best_value": 0.3664}, {"dataset_name": "synthetic_xor (mc_T=5)", "final_value": 0.3486, "best_value": 0.3486}, {"dataset_name": "synthetic_xor (mc_T=10)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (mc_T=20)", "final_value": 0.5255, "best_value": 0.5255}]}, {"metric_name": "Final Validation Causal Effectiveness Score", "lower_is_better": false, "description": "Causal Effectiveness Score on the validation set for different mc_T values on the synthetic_xor dataset.", "data": [{"dataset_name": "synthetic_xor (mc_T=2)", "final_value": 0.3871, "best_value": 0.3871}, {"dataset_name": "synthetic_xor (mc_T=5)", "final_value": 0.3721, "best_value": 0.3721}, {"dataset_name": "synthetic_xor (mc_T=10)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (mc_T=20)", "final_value": 0.4872, "best_value": 0.4872}]}]}, {"metric_names": [{"metric_name": "train cross-entropy score", "lower_is_better": true, "description": "Cross-entropy score on the training set", "data": [{"dataset_name": "synthetic_xor", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation cross-entropy score", "lower_is_better": true, "description": "Cross-entropy score on the validation set", "data": [{"dataset_name": "synthetic_xor", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train CES", "lower_is_better": true, "description": "Final training cross entropy score", "data": [{"dataset_name": "synthetic_xor (hidden_layer_size=4)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (hidden_layer_size=8)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (hidden_layer_size=16)", "final_value": 0.4298, "best_value": 0.4298}, {"dataset_name": "synthetic_xor (hidden_layer_size=32)", "final_value": 0.6667, "best_value": 0.6667}, {"dataset_name": "synthetic_xor (hidden_layer_size=64)", "final_value": 0.4513, "best_value": 0.4513}]}, {"metric_name": "validation CES", "lower_is_better": true, "description": "Final validation cross entropy score", "data": [{"dataset_name": "synthetic_xor (hidden_layer_size=4)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (hidden_layer_size=8)", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "synthetic_xor (hidden_layer_size=16)", "final_value": 0.5111, "best_value": 0.5111}, {"dataset_name": "synthetic_xor (hidden_layer_size=32)", "final_value": 0.6667, "best_value": 0.6667}, {"dataset_name": "synthetic_xor (hidden_layer_size=64)", "final_value": 0.4275, "best_value": 0.4275}]}]}, {"metric_names": [{"metric_name": "training cross-entropy score", "lower_is_better": true, "description": "Cross-entropy score on the training split of the synthetic_xor dataset", "data": [{"dataset_name": "synthetic_xor", "final_value": 0.4312, "best_value": 0.4312}]}, {"metric_name": "validation cross-entropy score", "lower_is_better": true, "description": "Cross-entropy score on the validation split of the synthetic_xor dataset", "data": [{"dataset_name": "synthetic_xor", "final_value": 0.4286, "best_value": 0.4286}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}], "is_best_node": [false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false], "plots": [["../../logs/0-run/experiment_results/experiment_e6bb2ee42745453fb097f93ec5799692_proc_2375581/class_distribution_synthetic_xor.png", "../../logs/0-run/experiment_results/experiment_e6bb2ee42745453fb097f93ec5799692_proc_2375581/CES_curves_synthetic_xor.png", "../../logs/0-run/experiment_results/experiment_e6bb2ee42745453fb097f93ec5799692_proc_2375581/loss_curves_synthetic_xor.png"], [], ["../../logs/0-run/experiment_results/experiment_8c25fa26b5b7489da326a3024e9f3a5d_proc_2379937/synthetic_xor_ground_truth_vs_predictions.png", "../../logs/0-run/experiment_results/experiment_8c25fa26b5b7489da326a3024e9f3a5d_proc_2379937/synthetic_xor_CES_curves.png", "../../logs/0-run/experiment_results/experiment_8c25fa26b5b7489da326a3024e9f3a5d_proc_2379937/synthetic_xor_loss_curves.png"], [], ["../../logs/0-run/experiment_results/experiment_c1857ae07f9647a6933784b48c7e0cdf_proc_2379936/synthetic_xor_CES_vs_dropout.png", "../../logs/0-run/experiment_results/experiment_c1857ae07f9647a6933784b48c7e0cdf_proc_2379936/synthetic_xor_loss_curves.png", "../../logs/0-run/experiment_results/experiment_c1857ae07f9647a6933784b48c7e0cdf_proc_2379936/synthetic_xor_final_loss_vs_dropout.png"], ["../../logs/0-run/experiment_results/experiment_08e202d93d5347b980c3fb97b25b16d2_proc_2379938/synthetic_xor_CES_metrics.png", "../../logs/0-run/experiment_results/experiment_08e202d93d5347b980c3fb97b25b16d2_proc_2379938/synthetic_xor_loss_curves.png"], [], [], [], ["../../logs/0-run/experiment_results/experiment_3f33ddac87644ba2a1db2c8af87cae28_proc_2379938/synthetic_xor_loss_curve.png", "../../logs/0-run/experiment_results/experiment_3f33ddac87644ba2a1db2c8af87cae28_proc_2379938/synthetic_xor_CES_curve.png", "../../logs/0-run/experiment_results/experiment_3f33ddac87644ba2a1db2c8af87cae28_proc_2379938/synthetic_xor_final_CES_bar.png"], ["../../logs/0-run/experiment_results/experiment_5a2ab1e77fb74280bd8b78ab5608edbd_proc_2379936/XOR_loss_curves.png", "../../logs/0-run/experiment_results/experiment_5a2ab1e77fb74280bd8b78ab5608edbd_proc_2379936/XOR_accuracy_gain_per_clarification.png"], [], ["../../logs/0-run/experiment_results/experiment_212d996c3cc441228d2e61eb55136671_proc_2379938/synthetic_xor_CES_curves.png", "../../logs/0-run/experiment_results/experiment_212d996c3cc441228d2e61eb55136671_proc_2379938/synthetic_xor_loss_curves.png"], ["../../logs/0-run/experiment_results/experiment_1ca653a5762847979dd3440b11e776bd_proc_2379937/synthetic_xor_loss_curve.png", "../../logs/0-run/experiment_results/experiment_1ca653a5762847979dd3440b11e776bd_proc_2379937/synthetic_xor_CES_curve.png", "../../logs/0-run/experiment_results/experiment_1ca653a5762847979dd3440b11e776bd_proc_2379937/synthetic_xor_final_CES_bar.png"], ["../../logs/0-run/experiment_results/experiment_d55fa0d5edf8493ab1ab91886ff0b0f9_proc_2379936/synthetic_xor_loss_curve.png", "../../logs/0-run/experiment_results/experiment_d55fa0d5edf8493ab1ab91886ff0b0f9_proc_2379936/synthetic_xor_CES_curve.png", "../../logs/0-run/experiment_results/experiment_d55fa0d5edf8493ab1ab91886ff0b0f9_proc_2379936/synthetic_xor_final_CES_bar.png"], ["../../logs/0-run/experiment_results/experiment_0eece47f8b6b401ba4419b77a1ea3244_proc_2379938/synthetic_xor_loss_curve.png", "../../logs/0-run/experiment_results/experiment_0eece47f8b6b401ba4419b77a1ea3244_proc_2379938/synthetic_xor_CES_curve.png", "../../logs/0-run/experiment_results/experiment_0eece47f8b6b401ba4419b77a1ea3244_proc_2379938/synthetic_xor_final_CES_bar.png"], ["../../logs/0-run/experiment_results/seed_aggregation_65f2a3be93b2407ca123f01946009ed4/synthetic_xor_mean_loss_curve.png", "../../logs/0-run/experiment_results/seed_aggregation_65f2a3be93b2407ca123f01946009ed4/synthetic_xor_final_CES_bar_with_error.png", "../../logs/0-run/experiment_results/seed_aggregation_65f2a3be93b2407ca123f01946009ed4/synthetic_xor_mean_CES_curve.png"]], "plot_paths": [["experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_e6bb2ee42745453fb097f93ec5799692_proc_2375581/class_distribution_synthetic_xor.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_e6bb2ee42745453fb097f93ec5799692_proc_2375581/CES_curves_synthetic_xor.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_e6bb2ee42745453fb097f93ec5799692_proc_2375581/loss_curves_synthetic_xor.png"], [], ["experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_8c25fa26b5b7489da326a3024e9f3a5d_proc_2379937/synthetic_xor_ground_truth_vs_predictions.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_8c25fa26b5b7489da326a3024e9f3a5d_proc_2379937/synthetic_xor_CES_curves.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_8c25fa26b5b7489da326a3024e9f3a5d_proc_2379937/synthetic_xor_loss_curves.png"], [], ["experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_c1857ae07f9647a6933784b48c7e0cdf_proc_2379936/synthetic_xor_CES_vs_dropout.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_c1857ae07f9647a6933784b48c7e0cdf_proc_2379936/synthetic_xor_loss_curves.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_c1857ae07f9647a6933784b48c7e0cdf_proc_2379936/synthetic_xor_final_loss_vs_dropout.png"], ["experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_08e202d93d5347b980c3fb97b25b16d2_proc_2379938/synthetic_xor_CES_metrics.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_08e202d93d5347b980c3fb97b25b16d2_proc_2379938/synthetic_xor_loss_curves.png"], [], [], [], ["experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_3f33ddac87644ba2a1db2c8af87cae28_proc_2379938/synthetic_xor_loss_curve.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_3f33ddac87644ba2a1db2c8af87cae28_proc_2379938/synthetic_xor_CES_curve.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_3f33ddac87644ba2a1db2c8af87cae28_proc_2379938/synthetic_xor_final_CES_bar.png"], ["experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_5a2ab1e77fb74280bd8b78ab5608edbd_proc_2379936/XOR_loss_curves.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_5a2ab1e77fb74280bd8b78ab5608edbd_proc_2379936/XOR_accuracy_gain_per_clarification.png"], [], ["experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_212d996c3cc441228d2e61eb55136671_proc_2379938/synthetic_xor_CES_curves.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_212d996c3cc441228d2e61eb55136671_proc_2379938/synthetic_xor_loss_curves.png"], ["experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_1ca653a5762847979dd3440b11e776bd_proc_2379937/synthetic_xor_loss_curve.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_1ca653a5762847979dd3440b11e776bd_proc_2379937/synthetic_xor_CES_curve.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_1ca653a5762847979dd3440b11e776bd_proc_2379937/synthetic_xor_final_CES_bar.png"], ["experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_d55fa0d5edf8493ab1ab91886ff0b0f9_proc_2379936/synthetic_xor_loss_curve.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_d55fa0d5edf8493ab1ab91886ff0b0f9_proc_2379936/synthetic_xor_CES_curve.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_d55fa0d5edf8493ab1ab91886ff0b0f9_proc_2379936/synthetic_xor_final_CES_bar.png"], ["experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_0eece47f8b6b401ba4419b77a1ea3244_proc_2379938/synthetic_xor_loss_curve.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_0eece47f8b6b401ba4419b77a1ea3244_proc_2379938/synthetic_xor_CES_curve.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_0eece47f8b6b401ba4419b77a1ea3244_proc_2379938/synthetic_xor_final_CES_bar.png"], ["experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/seed_aggregation_65f2a3be93b2407ca123f01946009ed4/synthetic_xor_mean_loss_curve.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/seed_aggregation_65f2a3be93b2407ca123f01946009ed4/synthetic_xor_final_CES_bar_with_error.png", "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/seed_aggregation_65f2a3be93b2407ca123f01946009ed4/synthetic_xor_mean_CES_curve.png"]], "plot_analyses": [[{"analysis": "Class counts for label 0 are slightly overpredicted (ground truth \u2248258 vs. predictions \u2248268) while label 1 is underpredicted (ground truth \u2248242 vs. predictions \u2248232), indicating a minor bias toward the negative class and some misclassification around the decision boundary in this synthetic XOR task.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_e6bb2ee42745453fb097f93ec5799692_proc_2375581/class_distribution_synthetic_xor.png"}, {"analysis": "Training CES gradually increases from ~0.30 to ~0.44 over 10 epochs, and validation CES shows larger fluctuations, dipping to ~0.21 then spiking above ~0.47. This suggests calibration degrades as training progresses, with unstable calibration on the validation set, especially after epoch 5.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_e6bb2ee42745453fb097f93ec5799692_proc_2375581/CES_curves_synthetic_xor.png"}, {"analysis": "Train loss decreases from ~0.73 to ~0.67 (with a small bump at epoch 5) and validation loss steadily drops from ~0.68 to ~0.65. The model is improving in terms of cross-entropy loss, but the divergent trends between loss (improving) and CES (worsening) imply that higher confidence is not well aligned with correctness, pointing to calibration issues.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_e6bb2ee42745453fb097f93ec5799692_proc_2375581/loss_curves_synthetic_xor.png"}], [], [{"analysis": "Histograms of true class labels and model predictions on the synthetic_xor dataset reveal a slight class imbalance in predictions. True labels show 256 examples of class 0 and 244 examples of class 1, while predicted labels shift to 268 examples of class 0 and 232 examples of class 1. This indicates the model is biased toward predicting class 0, underestimating instances of class 1 by about 12 samples. The overall shape is similar, suggesting that the model largely captures the data distribution but exhibits a systematic tendency to overpredict the majority class.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_8c25fa26b5b7489da326a3024e9f3a5d_proc_2379937/synthetic_xor_ground_truth_vs_predictions.png"}, {"analysis": "Curves of the CES metric across multiple learning rates exhibit distinct patterns. At a very low rate (1e-4), training and validation CES remain flat around 0.25 and 0.23 respectively, indicating underfitting and minimal improvement. A slightly higher rate (5e-4) causes an early validation spike to ~0.9 at epoch 2, followed by collapse to zero, a sign of unstable training and possible numeric issues. A mid-range rate (1e-3) shows virtually no movement in CES, implying saturation. Rates of 5e-3 start with high CES around 0.9 but drop abruptly to near zero by epoch 3, again suggesting instability. The highest rate (1e-2) yields zero CES until epoch 4, then gradual increases in both train and validation curves up to ~0.4 by epoch 10, indicating eventual learning but slow convergence and risk of suboptimal performance. Overall, the CES metric highlights a narrow window of learning rates (around 1e-2) that allow gradual clarification behavior without the explosive dynamics seen at intermediate rates.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_8c25fa26b5b7489da326a3024e9f3a5d_proc_2379937/synthetic_xor_CES_curves.png"}, {"analysis": "Loss trajectories for training and validation sets underscore the hyperparameter sensitivities observed. The smallest rate (1e-4) keeps loss high (~0.73 train, 0.68 val) and nearly constant, confirming underfitting. A slightly larger rate (5e-4) yields modest decrease in training loss to ~0.69 but validation remains flat around 0.68, reflecting limited benefit. Moving to 1e-3 achieves minor gains with train loss decreasing to ~0.65 and validation to ~0.67. A rate of 5e-3 drives rapid training loss reduction to ~0.45 but validation plateaus around 0.68, revealing overfitting. The largest rate (1e-2) balances both: training loss falls to ~0.58 and validation to ~0.50 by epoch 10, showing steady improvement without dramatic divergence. These patterns align with CES analysis and suggest that 1e-2 strikes the best compromise between stable learning dynamics and generalization for this task.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_8c25fa26b5b7489da326a3024e9f3a5d_proc_2379937/synthetic_xor_loss_curves.png"}], [], [{"analysis": "CES vs Dropout Rate (synthetic_xor) shows that training calibration error (CES) remains at zero for most dropout rates except at 0.1, where it spikes to about 1.0. Validation CES stays pinned at zero across all settings. This suggests an anomalous calibration issue at a low dropout rate in training\u2014likely an instability in uncertainty estimation or thresholding artifact\u2014while validation predictions appear perfectly calibrated (or below the detection threshold) regardless of dropout.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_c1857ae07f9647a6933784b48c7e0cdf_proc_2379936/synthetic_xor_CES_vs_dropout.png"}], [{"analysis": "Thresholds at or above 0.05 (red and purple) produce zero CES on both training and validation, indicating the threshold is too coarse\u2014effectively eliminating the error metric rather than reducing it. Among the lower thresholds, 0.02 (green) yields the smallest and most stable CES throughout all epochs, plateauing around 0.12 on train and 0.15 on validation with minimal variance. The 0.005 (blue) and 0.010 (orange) settings start with similar CES but diverge: 0.01 drops sharply by epoch 4 before rebounding markedly after epoch 6, while 0.005 shows a steady upward drift over time. This suggests 0.02 is the sweet spot for minimizing combined error without metric collapse or late-stage degradation.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_08e202d93d5347b980c3fb97b25b16d2_proc_2379938/synthetic_xor_CES_metrics.png"}, {"analysis": "In training loss, threshold 0.05 (red) converges fastest to the lowest final value (\u22480.62), followed by 0.02 (green) at \u22480.65. Thresholds 0.005 (blue) and 0.010 (orange) finish higher (\u22480.65\u20130.68), while 0.10 (purple) lags (\u22480.70). Validation loss trends mirror training: 0.05 yields the lowest loss (\u22480.61), 0.02 next (\u22480.65), then 0.005, 0.01, and finally 0.10. The alignment of train and validation curves suggests these settings generalize well, and 0.05 maximizes loss reduction but, as seen in the CES plots, at the cost of metric collapse.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_08e202d93d5347b980c3fb97b25b16d2_proc_2379938/synthetic_xor_loss_curves.png"}], [], [], [], [{"analysis": "Synthetic XOR: Training vs Validation Loss reveals a clear underfitting-to-fitting spectrum as hidden layer size increases. Models with size=4 and size=8 linger around a loss of ~0.70 on both train and validation sets, plateauing after epoch 3. Size=16 and size=32 show modest steady decreases in both curves\u2014by epoch 10 they hit train/val losses of roughly 0.64/0.61 and 0.61/0.58 respectively\u2014but still remain well above random chance (0.5). By contrast, size=64 continues to improve throughout all 10 epochs, ending at a train loss of ~0.52 and validation loss of ~0.49. Across all sizes the train\u2013validation gap stays small, indicating low overfitting even at the largest capacity; instead, the dominant issue for small to mid-sized models is insufficient representational power to capture the XOR mapping.", "valid_plots_received": true, "vlm_feedback_summary": "Larger hidden sizes boost both training and generalization performance on XOR, but small models underfit. No signs of severe overfitting for any tested size.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_3f33ddac87644ba2a1db2c8af87cae28_proc_2379938/synthetic_xor_loss_curve.png"}, {"analysis": "Synthetic XOR: Training vs Validation CES (Calibration Error Score) paints a contrasting picture of confidence reliability. Small networks (sizes 4 and 8) drive both train and val CES to zero by epoch 3, indicating near-perfect calibration but underfit decision boundaries. Size 16 starts well calibrated but CES drifts upward, settling around 0.29 (train) and 0.25 (val) by epoch 10, betraying growing overconfidence as it fits the data more tightly. Size 32 exhibits volatile calibration\u2014sharp spikes on both train and val curves around epochs 4 and 8\u2014though it appears to self-correct by epoch 10. Size 64 never recovers perfect calibration: its validation CES rises steadily from ~0.25 at epoch 1 to ~0.47 at epoch 10, suggesting that the most powerful model yields the least trustworthy confidence estimates without explicit calibration strategies.", "valid_plots_received": true, "vlm_feedback_summary": "Increasing capacity improves accuracy but degrades reliability of confidence judgments, with mid- to large-sized networks showing unstable or high calibration error.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_3f33ddac87644ba2a1db2c8af87cae28_proc_2379938/synthetic_xor_CES_curve.png"}, {"analysis": "Synthetic XOR: Final Validation CES by Hidden Size underscores a non-monotonic calibration trend. Hidden sizes 4, 8, and 32 converge to nearly zero final calibration error, reflecting either persistent underfitting or late-stage self-correction. Size 16 ends at ~0.29 CES, while size 64 peaks at ~0.47, confirming that capacity beyond a certain point exacerbates miscalibrated confidence. This suggests a trade-off: while large networks reduce predictive error, they demand posthoc calibration (e.g., temperature scaling or mixup) to restore trustworthiness.", "valid_plots_received": true, "vlm_feedback_summary": "Calibration error varies non-monotonically with size; mid-sized nets can self-correct, but very large nets suffer the worst miscalibration.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_3f33ddac87644ba2a1db2c8af87cae28_proc_2379938/synthetic_xor_final_CES_bar.png"}], [{"analysis": "Loss curves reveal that adding even a small weight decay (1e-5) yields the strongest improvement in both training and validation loss, steadily decreasing from 0.69 down to 0.64 on the validation set over ten epochs. Zero weight decay shows slower convergence and a persistent gap between train and val losses, indicating mild overfitting. A moderate decay (1e-4) follows a similar trend but lags behind 1e-5, while heavy decay (1e-3) delays early progress and yields slightly higher final losses than 1e-5. Overall, weight decay at 1e-5 strikes the best balance between stable convergence and generalization on this toy XOR task.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_5a2ab1e77fb74280bd8b78ab5608edbd_proc_2379936/XOR_loss_curves.png"}, {"analysis": "The accuracy gain per clarification metric is noisy under no weight decay, with large positive spikes at epochs 6 and 8 but a sharp negative dip at epoch 5, suggesting erratic behavior. Weight decay of 1e-5 produces steadily increasing positive gains from epoch 8 onward, topping out around 0.5\u20130.6 points per clarification. Smaller decay (1e-4) yields modest gains (~0.3\u20130.4) late in training, while heavy decay (1e-3) stays near zero. These results indicate that clarification questions only become reliably useful once the model has sufficiently converged\u2014and that optimal regularization (1e-5) not only minimizes loss but also stabilizes interactive accuracy improvements.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_5a2ab1e77fb74280bd8b78ab5608edbd_proc_2379936/XOR_accuracy_gain_per_clarification.png"}], [], [{"analysis": "CES curves for the synthetic XOR dataset reveal that at low temperature (T=2), per-token uncertainty is nonzero from the start and remains moderate across epochs, with validation uncertainty peaking around epoch 7 before settling. As T increases to 5, the model shows essentially zero CES through early epochs and only begins to signal ambiguity around epoch 7, suggesting that mid-range temperatures suppress early uncertainty detection. At higher temperatures (T=10 and T=20), CES stays flat at zero until the very end of training (epochs 8\u20139), then spikes sharply\u2014validation uncertainty even reaching nearly 1.0\u2014before dropping back by epoch 10. This indicates that very high T values produce delayed but extreme uncertainty estimates, which could lead to very late or overly aggressive clarification in an interactive setting.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_212d996c3cc441228d2e61eb55136671_proc_2379938/synthetic_xor_CES_curves.png"}, {"analysis": "Loss curves for the same experimental configurations show that increasing temperature generally accelerates loss reduction and yields lower final training and validation losses. With T=2 and T=5, both train and val losses decrease steadily but remain higher at epoch 10 (train \u22480.65\u20130.68, val \u22480.64\u20130.67). For T=10, the training loss drops to around 0.645 by epoch 10, with validation loss around 0.645 as well. The highest temperature, T=20, achieves the lowest losses overall (train \u22480.62, val \u22480.635 at epoch 10), suggesting that while high temperatures delay uncertainty signals in CES, they improve raw training dynamics. The trends imply a trade-off between sharpness of uncertainty estimation (CES timing) and convergence speed / generalization (loss curves).", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_212d996c3cc441228d2e61eb55136671_proc_2379938/synthetic_xor_loss_curves.png"}], [{"analysis": "Training and validation loss curves indicate that smaller hidden sizes (4 and 8) barely improve over ten epochs, with training loss hovering near 0.70 and validation loss only dropping modestly. Medium size (16) shows a slight downward trend, while larger sizes (32 and especially 64) achieve pronounced loss reduction, with the size-64 model falling from ~0.70 to ~0.51 on training and from ~0.69 to ~0.50 on validation. This suggests that capacity is a key driver of convergence, but small architectures are under-parametrized for the XOR task under the current hyperparameters.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_1ca653a5762847979dd3440b11e776bd_proc_2379937/synthetic_xor_loss_curve.png"}, {"analysis": "Calibration Error Score (CES) trajectories reveal significant instability at larger sizes. Hidden size 4 and 8 show moderate CES fluctuations, but both remain relatively contained. Size 16 produces near-zero CES until mid-training before a modest rise, indicating better calibration. Sizes 32 and 64 exhibit large positive and negative swings (up to 0.75 and down to \u20130.25), signifying overconfidence or severe miscalibration despite good loss performance.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_1ca653a5762847979dd3440b11e776bd_proc_2379937/synthetic_xor_CES_curve.png"}, {"analysis": "Final bar chart of validation CES by hidden size confirms that hidden size 16 achieves the lowest miscalibration (~0.21). Hidden sizes 8 and 32 are worse (~0.36\u20130.37), and size 64 is the most miscalibrated (~0.40). This trade-off shows that maximal capacity yields best loss minimization but poorest calibration, whereas a moderate hidden size (16) balances accuracy and trustworthiness.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_1ca653a5762847979dd3440b11e776bd_proc_2379937/synthetic_xor_final_CES_bar.png"}], [{"analysis": "Training and validation losses consistently decrease faster and to a lower final value as hidden layer size increases. Models with hidden size 4 and 8 plateau early with relatively high losses around 0.66\u20130.69, indicating underfitting. Hidden sizes 16 and 32 reduce loss more substantially over epochs, and size 64 achieves the steepest decline, ending near 0.50 on training and 0.49 on validation. No clear overfitting is observed, since validation loss tracks training loss closely, but capacity below 16 is insufficient for this XOR task.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_d55fa0d5edf8493ab1ab91886ff0b0f9_proc_2379936/synthetic_xor_loss_curve.png"}, {"analysis": "Calibration Error Scores (CES) over epochs vary markedly with hidden size. Size 4 shows stable moderate miscalibration (train ~0.6, val ~0.4). Size 8 has high train miscalibration (~0.7) and a declining validation score from ~0.7 to ~0.6. Size 16 and 32 produce erratic CES curves\u2014often flat at zero with sudden spikes\u2014suggesting unstable probability estimates or numerical issues in calibration measurement. Size 64 yields the most consistent CES across training (~0.5 to 0.45) and validation (~0.65 to 0.43), indicating more reliable probability outputs and smoother calibration dynamics.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_d55fa0d5edf8493ab1ab91886ff0b0f9_proc_2379936/synthetic_xor_CES_curve.png"}, {"analysis": "Final validation CES by hidden size shows a non-monotonic trend: hidden size 32 yields the highest calibration error (~0.67), while size 16 sits in the middle (~0.51). Size 64 achieves the lowest CES (~0.43), confirming that larger capacity not only lowers loss but also enhances final calibration. The peak at size 32 suggests a calibration sweet spot requires sufficient capacity plus stabilization techniques; intermediate sizes may suffer from overconfidence without proper regularization.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_d55fa0d5edf8493ab1ab91886ff0b0f9_proc_2379936/synthetic_xor_final_CES_bar.png"}], [{"analysis": "Synthetic XOR: Training vs Validation Loss exhibits a clear trend across hidden sizes. Models with hidden size 4 and 8 hardly improve beyond an initial loss of ~0.70 on both train and validation, indicating severe underfitting at these capacities. Stepping up to size 16 yields gradual loss reduction, but plateaus around 0.64 (train) and 0.62 (val) by epoch 10, suggesting limited representational power. Size 32 further lowers losses to roughly 0.58 (train) and 0.57 (val), showing better fit and a small generalization gap (~0.01\u20130.02). Hidden size 64 achieves the most pronounced descent: train loss reaches ~0.50 and validation ~0.48 by epoch 10, with a similarly narrow gap, demonstrating both high capacity and effective generalization when given enough training steps.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_0eece47f8b6b401ba4419b77a1ea3244_proc_2379938/synthetic_xor_loss_curve.png"}, {"analysis": "Synthetic XOR: Training vs Validation CES (Calibration Error Score) reveals how well confidence aligns with correctness. Hidden sizes 4 and 8 maintain flat CES at zero throughout, indicating the model never exits random or uniform confidence\u2014a further signal of underfitting. Size 16 shows extreme instability early (overshooting negative CES at epoch 2), then gradually settles into ~0.25 on train and ~0.30 on val by epoch 10, reflecting initial calibration troubles. Size 32 achieves a more stable trajectory, hovering around 0.45 (train) and ~0.50 (val) across later epochs, while size 64 steadily climbs from ~0.18 to ~0.48 (train) and ~0.50 (val). Both larger models end with substantially higher CES, meaning confidence estimates become increasingly trustworthy as capacity grows.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_0eece47f8b6b401ba4419b77a1ea3244_proc_2379938/synthetic_xor_CES_curve.png"}, {"analysis": "Synthetic XOR: Final Validation CES by Hidden Size confirms a monotonic improvement in calibration with model size. Hidden size 16 yields a CES of ~0.37, size 32 roughly 0.40, and size 64 about 0.43. This pattern underlines that larger hidden layers not only fit the data more accurately (as seen in the loss curves) but also calibrate their output probabilities more reliably. The diminishing returns from 32\u219264 suggest that size 32 may already offer a strong balance of performance and efficiency.", "plot_path": "experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_0eece47f8b6b401ba4419b77a1ea3244_proc_2379938/synthetic_xor_final_CES_bar.png"}], []], "vlm_feedback_summary": ["Model learns the XOR pattern under decreasing loss, but exhibits a small class\nprediction bias and worsening calibration despite better loss metrics.\nCalibration needs targeted intervention.", "[]", "Hyperparameter tuning on synthetic_xor reveals a sweet spot at a learning rate\nof 1e-2, which yields consistent reductions in both loss and CES without the\noscillations or collapses observed at mid-range rates. Lower rates underfit\nwhile moderate rates cause unstable training. The model tends to overpredict the\nmajority class in its final predictions.", "[]", "CES plot indicates calibration breakdown at dropout=0.1 on training only; loss\ncurves and final-loss-vs-dropout plots reveal optimal regularization at dr=0.3,\nunderfitting at dr>=0.5, and slight over-regularization effects at dr=0.1.", "The plots indicate a threshold in the range 0.02\u20130.05 yields the best trade-off:\n0.05 drives losses lowest but collapses CES, while 0.02 offers stable,\nmeaningful CES with strong loss reduction. Recommend narrowing the search around\nthese values. To further evaluate interactive clarification, test on QuAC\n(conversational QA requiring follow-up questions) and AmbigQA (ambiguous-query\nQA) from HuggingFace.", "[]", "[]", "[]", "[]", "The XOR-based results demonstrate that a lightweight weight decay of 1e-5 is the\nmost effective hyperparameter for reducing loss and maximizing reliable accuracy\ngains per clarification. Zero or excessive decay either overfits or underfits,\nharming both raw loss and interactive benefits. To further refine performance,\nconsider lowering the learning rate slightly below the current value to smooth\nout the spike in accuracy gains, extend training beyond ten epochs to confirm\nplateau behavior, and experiment with small dropout rates. For more realistic\nevaluation, add two HuggingFace datasets that stress ambiguity and multi-hop\nretrieval: (1) microsoft/ambig_qa to benchmark clarification on genuinely\nambiguous queries, and (2) hotpot_qa for multi-step, retrieval-augmented\nreasoning.", "[]", "CES results highlight that temperature controls the timing and magnitude of\nquery-clarification signals, with low T producing persistent but moderate\nuncertainty and high T producing late, extreme spikes. Loss dynamics favor\nhigher T for faster convergence and lower final errors. Overall, an intermediate\ntemperature might balance early uncertainty detection with solid convergence.", "Increasing hidden size clearly accelerates loss reduction but degrades\ncalibration beyond a moderate point. To improve both metrics without changing\narchitecture: lower the learning rate for larger models (e.g., halving it for\nsizes \u226532), introduce a cosine or step decay schedule, and add early stopping\nbased on validation loss or CES. Experiment with batch size (smaller batches may\nstabilize gradient updates for capacity-rich models) and consider gradient\nclipping or weight decay to curb overconfidence. To assess generalization beyond\nsynthetic XOR, test on two HuggingFace datasets: \u201ctrivia_qa\u201d for open-domain QA\nand \u201csuper_glue/boolq\u201d for binary yes/no reasoning. These will probe retrieval-\naugmented performance and calibration under realistic, ambiguous queries.", "Larger hidden sizes improve both convergence and final calibration, with size 64\noutperforming smaller configurations in loss reduction and calibration\nstability. Intermediate capacity (size 32) shows degradation in calibration\ndespite lower loss, highlighting the need for post-training calibration or\nregularization at mid-sized models.", "Larger hidden sizes consistently improve both loss reduction and calibration,\nwith size 64 achieving the best metrics but slower convergence early. Size 32\nprovides a near\u2013optimal trade-off. Smaller models (\u22648) underfit and produce\nuninformative confidence. Consider tuning learning rate (e.g., lower LR or warm-\nup schedule for size \u226532), extending epochs, and possibly adding dropout to\nstabilize early CES fluctuations.", "[]"], "exec_time": [21.25241994857788, 1.4204809665679932, 103.84887552261353, 105.60217213630676, 118.50671935081482, 103.67301321029663, 0.0038678646087646484, 82.33982062339783, 0.5487158298492432, 100.5653920173645, 5.807505369186401, 0.6463031768798828, 130.10154056549072, 102.96215438842773, 993.8344557285309, 100.15335416793823, null], "exec_time_feedback": ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], "datasets_successfully_tested": [["\"\""], [], ["['synthetic_xor']"], [], ["[\"synthetic_xor\"]"], ["['synthetic_xor']"], [], [], [], [], ["[\"XOR\"]"], [], ["['synthetic_xor']"], ["['synthetic_xor']"], ["[\"synthetic_xor\"]"], ["['synthetic_xor']"], []], "plot_code": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nds = experiment_data.get(\"synthetic_xor\", {})\n\n# Plot loss curves\ntry:\n    loss_train = ds[\"losses\"][\"train\"]\n    loss_val = ds[\"losses\"][\"val\"]\n    epochs = range(1, len(loss_train) + 1)\n    plt.figure()\n    plt.plot(epochs, loss_train, label=\"Train Loss\")\n    plt.plot(epochs, loss_val, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss Curves - synthetic_xor\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"loss_curves_synthetic_xor.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves: {e}\")\n    plt.close()\n\n# Plot CES curves\ntry:\n    ces_train = ds[\"metrics\"][\"train\"]\n    ces_val = ds[\"metrics\"][\"val\"]\n    epochs = range(1, len(ces_train) + 1)\n    plt.figure()\n    plt.plot(epochs, ces_train, label=\"Train CES\")\n    plt.plot(epochs, ces_val, label=\"Val CES\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CES\")\n    plt.title(\"CES Curves - synthetic_xor\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"CES_curves_synthetic_xor.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CES curves: {e}\")\n    plt.close()\n\n# Plot class distribution on final validation epoch\ntry:\n    preds = ds[\"predictions\"][-1]\n    gts = ds[\"ground_truth\"][-1]\n    counts_true = np.bincount(gts, minlength=2)\n    counts_pred = np.bincount(preds, minlength=2)\n    classes = np.arange(len(counts_true))\n    width = 0.35\n    plt.figure()\n    plt.bar(classes - width / 2, counts_true, width, label=\"Ground Truth\")\n    plt.bar(classes + width / 2, counts_pred, width, label=\"Predictions\")\n    plt.xlabel(\"Class\")\n    plt.ylabel(\"Count\")\n    plt.title(\n        \"Class Distribution (Left: Ground Truth, Right: Predictions) - synthetic_xor\"\n    )\n    plt.xticks(classes)\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"class_distribution_synthetic_xor.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating class distribution plot: {e}\")\n    plt.close()\n\n# Print final metrics summary\ntry:\n    print(f\"Final Train Loss: {loss_train[-1]:.4f}, Final Val Loss: {loss_val[-1]:.4f}\")\n    print(f\"Final Train CES: {ces_train[-1]:.4f}, Final Val CES: {ces_val[-1]:.4f}\")\nexcept:\n    pass\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\nlr_sweep = experiment_data.get(\"learning_rate_sweep\", {}).get(\"synthetic_xor\", {})\nlr_keys = sorted(lr_sweep.keys(), key=lambda k: float(k.split(\"_\")[1]))\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for k in lr_keys:\n        tr = lr_sweep[k][\"losses\"][\"train\"]\n        va = lr_sweep[k][\"losses\"][\"val\"]\n        epochs = range(1, len(tr) + 1)\n        plt.plot(epochs, tr, label=f\"{k} train\")\n        plt.plot(epochs, va, \"--\", label=f\"{k} val\")\n    plt.title(\"Loss Curves (synthetic_xor)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Plot CES metric curves\ntry:\n    plt.figure()\n    for k in lr_keys:\n        trc = lr_sweep[k][\"metrics\"][\"train\"]\n        vac = lr_sweep[k][\"metrics\"][\"val\"]\n        epochs = range(1, len(trc) + 1)\n        plt.plot(epochs, trc, label=f\"{k} train CES\")\n        plt.plot(epochs, vac, \"--\", label=f\"{k} val CES\")\n    plt.title(\"CES Metric Curves (synthetic_xor)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CES\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_CES_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CES metric plot: {e}\")\n    plt.close()\n\n# Identify best LR by final val CES and print metrics\nbest_lr = None\nbest_val_ces = -np.inf\nfor k in lr_keys:\n    final_ces = lr_sweep[k][\"metrics\"][\"val\"][-1]\n    print(f\"LR {k}: final_val_CES={final_ces:.4f}\")\n    if final_ces > best_val_ces:\n        best_val_ces, best_lr = final_ces, k\nif best_lr:\n    print(f\"Best LR: {best_lr}, Final Val CES: {best_val_ces:.4f}\")\n\n# Plot ground truth vs predictions for best LR\ntry:\n    if best_lr:\n        gt = lr_sweep[best_lr][\"ground_truth\"][-1]\n        preds = lr_sweep[best_lr][\"predictions\"][-1]\n        plt.figure(figsize=(8, 4))\n        plt.subplot(1, 2, 1)\n        plt.hist(gt, bins=np.arange(3) - 0.5)\n        plt.title(\"Ground Truth\")\n        plt.subplot(1, 2, 2)\n        plt.hist(preds, bins=np.arange(3) - 0.5)\n        plt.title(\"Predictions\")\n        plt.suptitle(\n            \"Ground Truth vs Predictions (synthetic_xor) - Left: Ground Truth, Right: Predictions\"\n        )\n        plt.savefig(\n            os.path.join(working_dir, \"synthetic_xor_ground_truth_vs_predictions.png\")\n        )\n        plt.close()\nexcept Exception as e:\n    print(f\"Error creating ground truth vs predictions plot: {e}\")\n    plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = experiment_data[\"dropout_rate_tuning\"][\"synthetic_xor\"]\n    dropouts = data[\"dropout_rates\"]\n    loss_train = data[\"losses\"][\"train\"]\n    loss_val = data[\"losses\"][\"val\"]\n    ces_train = data[\"metrics\"][\"train\"]\n    ces_val = data[\"metrics\"][\"val\"]\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot CES vs Dropout Rate\ntry:\n    plt.figure()\n    plt.plot(dropouts, [c[-1] for c in ces_train], marker=\"o\", label=\"Train CES\")\n    plt.plot(dropouts, [c[-1] for c in ces_val], marker=\"o\", label=\"Val CES\")\n    plt.xlabel(\"Dropout Rate\")\n    plt.ylabel(\"CES\")\n    plt.title(\"CES vs Dropout Rate (synthetic_xor)\\nLeft: Train CES, Right: Val CES\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_CES_vs_dropout.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CES plot: {e}\")\n    plt.close()\n\n# Plot final Loss vs Dropout Rate\ntry:\n    plt.figure()\n    plt.plot(dropouts, [l[-1] for l in loss_train], marker=\"o\", label=\"Train Loss\")\n    plt.plot(dropouts, [l[-1] for l in loss_val], marker=\"o\", label=\"Val Loss\")\n    plt.xlabel(\"Dropout Rate\")\n    plt.ylabel(\"Loss\")\n    plt.title(\n        \"Final Loss vs Dropout Rate (synthetic_xor)\\nLeft: Train Loss, Right: Val Loss\"\n    )\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_final_loss_vs_dropout.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss vs dropout plot: {e}\")\n    plt.close()\n\n# Plot Loss Curves vs Epoch for up to 5 dropout rates\ntry:\n    plt.figure()\n    epochs = len(loss_train[0])\n    idxs = np.linspace(0, len(dropouts) - 1, min(len(dropouts), 5), dtype=int)\n    for i in idxs:\n        dr = dropouts[i]\n        plt.plot(range(1, epochs + 1), loss_train[i], label=f\"Train dr={dr}\")\n        plt.plot(\n            range(1, epochs + 1), loss_val[i], linestyle=\"--\", label=f\"Val dr={dr}\"\n        )\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss Curves (synthetic_xor)\\nSolid: Train, Dashed: Val\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\n# Print summary metrics\ntry:\n    print(\"Dropout Rates:\", dropouts)\n    print(\"Final Train CES:\", [c[-1] for c in ces_train])\n    print(\"Final Val CES:\", [c[-1] for c in ces_val])\nexcept Exception as e:\n    print(f\"Error printing summary metrics: {e}\")\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\n    data = experiment_data[\"hyperparam_tuning_type_1\"][\"synthetic_xor\"]\n    thresholds = data[\"thresholds\"]\n    losses_tr = np.array(data[\"losses\"][\"train\"])  # shape: (n_thr, epochs)\n    losses_val = np.array(data[\"losses\"][\"val\"])\n    ces_tr = np.array(data[\"metrics\"][\"train\"])\n    ces_val = np.array(data[\"metrics\"][\"val\"])\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\n# Plot loss curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for i, thr in enumerate(thresholds):\n        axes[0].plot(\n            np.arange(1, losses_tr.shape[1] + 1), losses_tr[i], label=f\"{thr:.3f}\"\n        )\n        axes[1].plot(\n            np.arange(1, losses_val.shape[1] + 1), losses_val[i], label=f\"{thr:.3f}\"\n        )\n    axes[0].set_title(\"Train Loss Curves\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[1].set_title(\"Validation Loss Curves\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Loss\")\n    axes[0].legend(title=\"Threshold\")\n    axes[1].legend(title=\"Threshold\")\n    fig.suptitle(\"Synthetic XOR Loss Curves\\nLeft: Train Loss, Right: Validation Loss\")\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Plot CES metric curves\ntry:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    for i, thr in enumerate(thresholds):\n        axes[0].plot(np.arange(1, ces_tr.shape[1] + 1), ces_tr[i], label=f\"{thr:.3f}\")\n        axes[1].plot(np.arange(1, ces_val.shape[1] + 1), ces_val[i], label=f\"{thr:.3f}\")\n    axes[0].set_title(\"Train CES Curves\")\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"CES\")\n    axes[1].set_title(\"Validation CES Curves\")\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"CES\")\n    axes[0].legend(title=\"Threshold\")\n    axes[1].legend(title=\"Threshold\")\n    fig.suptitle(\"Synthetic XOR CES Metrics\\nLeft: Train CES, Right: Validation CES\")\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_CES_metrics.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CES plot: {e}\")\n    plt.close()\n\n# Print best threshold based on final validation CES\ntry:\n    final_vals = ces_val[:, -1]\n    idx = np.argmax(final_vals)\n    print(\n        f\"Best threshold: {thresholds[idx]:.3f}, Final Validation CES: {final_vals[idx]:.4f}\"\n    )\nexcept Exception as e:\n    print(f\"Error computing best threshold: {e}\")\n", null, null, null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Extract synthetic XOR results\ndata = experiment_data.get(\"hidden_layer_size\", {}).get(\"synthetic_xor\", {})\nsizes = data.get(\"sizes\", [])\nloss_tr = data.get(\"losses\", {}).get(\"train\", [])\nloss_val = data.get(\"losses\", {}).get(\"val\", [])\nces_tr = data.get(\"metrics\", {}).get(\"train\", [])\nces_val = data.get(\"metrics\", {}).get(\"val\", [])\n\n# Print summary of final CES\nprint(\"Hidden sizes:\", sizes)\nprint(\"Final train CES per size:\", [m[-1] if m else None for m in ces_tr])\nprint(\"Final val CES per size:\", [m[-1] if m else None for m in ces_val])\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for sz, lt, lv in zip(sizes, loss_tr, loss_val):\n        plt.plot(range(1, len(lt) + 1), lt, label=f\"Train loss (size={sz})\")\n        plt.plot(range(1, len(lv) + 1), lv, \"--\", label=f\"Val loss (size={sz})\")\n    plt.title(\"Synthetic XOR: Training vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Plot CES curves\ntry:\n    plt.figure()\n    for sz, mt, mv in zip(sizes, ces_tr, ces_val):\n        plt.plot(range(1, len(mt) + 1), mt, label=f\"Train CES (size={sz})\")\n        plt.plot(range(1, len(mv) + 1), mv, \"--\", label=f\"Val CES (size={sz})\")\n    plt.title(\"Synthetic XOR: Training vs Validation CES\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CES\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_CES_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CES plot: {e}\")\n    plt.close()\n\n# Plot final CES bar chart\ntry:\n    plt.figure()\n    plt.bar([str(s) for s in sizes], [m[-1] if m else 0 for m in ces_val])\n    plt.title(\"Synthetic XOR: Final Validation CES by Hidden Size\")\n    plt.xlabel(\"Hidden Layer Size\")\n    plt.ylabel(\"CES\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_final_CES_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final CES bar plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\nweight_decays = [0, 1e-5, 1e-4, 1e-3]\n\ntry:\n    plt.figure()\n    for idx, wd in enumerate(weight_decays):\n        train_losses = experiment_data[\"losses\"][\"train\"][idx]\n        val_losses = experiment_data[\"losses\"][\"val\"][idx]\n        epochs = np.arange(1, len(train_losses) + 1)\n        plt.plot(epochs, train_losses, label=f\"Train wd={wd}\")\n        plt.plot(epochs, val_losses, linestyle=\"--\", label=f\"Val wd={wd}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss Curves (XOR Dataset)\\nSolid: Train, Dashed: Validation\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"XOR_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss curves plot: {e}\")\n    plt.close()\n\ntry:\n    plt.figure()\n    for idx, wd in enumerate(weight_decays):\n        train_ag = experiment_data[\"accuracy_gain_per_clarification\"][\"train\"][idx]\n        val_ag = experiment_data[\"accuracy_gain_per_clarification\"][\"val\"][idx]\n        epochs = np.arange(1, len(train_ag) + 1)\n        plt.plot(epochs, train_ag, label=f\"Train wd={wd}\")\n        plt.plot(epochs, val_ag, linestyle=\"--\", label=f\"Val wd={wd}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy Gain per Clarification\")\n    plt.title(\n        \"Accuracy Gain per Clarification (XOR Dataset)\\nSolid: Train, Dashed: Validation\"\n    )\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"XOR_accuracy_gain_per_clarification.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating accuracy gain plot: {e}\")\n    plt.close()\n", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n\ntry:\n    data = experiment_data[\"mc_T_tuning\"][\"synthetic_xor\"]\n    plt.figure()\n    for mc_T, run in data.items():\n        epochs = np.arange(1, len(run[\"losses\"][\"train\"]) + 1)\n        plt.plot(epochs, run[\"losses\"][\"train\"], label=f\"Train Loss T={mc_T}\")\n        plt.plot(epochs, run[\"losses\"][\"val\"], \"--\", label=f\"Val Loss T={mc_T}\")\n    plt.title(\"Loss Curves\\nSynthetic XOR Dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_loss_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\ntry:\n    data = experiment_data[\"mc_T_tuning\"][\"synthetic_xor\"]\n    plt.figure()\n    for mc_T, run in data.items():\n        epochs = np.arange(1, len(run[\"metrics\"][\"train\"]) + 1)\n        plt.plot(epochs, run[\"metrics\"][\"train\"], label=f\"Train CES T={mc_T}\")\n        plt.plot(epochs, run[\"metrics\"][\"val\"], \"--\", label=f\"Val CES T={mc_T}\")\n    plt.title(\"CES Curves\\nSynthetic XOR Dataset\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CES\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_CES_curves.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CES plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Extract synthetic XOR results\ndata = experiment_data.get(\"hidden_layer_size\", {}).get(\"synthetic_xor\", {})\nsizes = data.get(\"sizes\", [])\nloss_tr = data.get(\"losses\", {}).get(\"train\", [])\nloss_val = data.get(\"losses\", {}).get(\"val\", [])\nces_tr = data.get(\"metrics\", {}).get(\"train\", [])\nces_val = data.get(\"metrics\", {}).get(\"val\", [])\n\n# Print summary of final CES\nprint(\"Hidden sizes:\", sizes)\nprint(\"Final train CES per size:\", [m[-1] if m else None for m in ces_tr])\nprint(\"Final val CES per size:\", [m[-1] if m else None for m in ces_val])\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for sz, lt, lv in zip(sizes, loss_tr, loss_val):\n        plt.plot(range(1, len(lt) + 1), lt, label=f\"Train loss (size={sz})\")\n        plt.plot(range(1, len(lv) + 1), lv, \"--\", label=f\"Val loss (size={sz})\")\n    plt.title(\"Synthetic XOR: Training vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Plot CES curves\ntry:\n    plt.figure()\n    for sz, mt, mv in zip(sizes, ces_tr, ces_val):\n        plt.plot(range(1, len(mt) + 1), mt, label=f\"Train CES (size={sz})\")\n        plt.plot(range(1, len(mv) + 1), mv, \"--\", label=f\"Val CES (size={sz})\")\n    plt.title(\"Synthetic XOR: Training vs Validation CES\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CES\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_CES_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CES plot: {e}\")\n    plt.close()\n\n# Plot final CES bar chart\ntry:\n    plt.figure()\n    plt.bar([str(s) for s in sizes], [m[-1] if m else 0 for m in ces_val])\n    plt.title(\"Synthetic XOR: Final Validation CES by Hidden Size\")\n    plt.xlabel(\"Hidden Layer Size\")\n    plt.ylabel(\"CES\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_final_CES_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final CES bar plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Extract synthetic XOR results\ndata = experiment_data.get(\"hidden_layer_size\", {}).get(\"synthetic_xor\", {})\nsizes = data.get(\"sizes\", [])\nloss_tr = data.get(\"losses\", {}).get(\"train\", [])\nloss_val = data.get(\"losses\", {}).get(\"val\", [])\nces_tr = data.get(\"metrics\", {}).get(\"train\", [])\nces_val = data.get(\"metrics\", {}).get(\"val\", [])\n\n# Print summary of final CES\nprint(\"Hidden sizes:\", sizes)\nprint(\"Final train CES per size:\", [m[-1] if m else None for m in ces_tr])\nprint(\"Final val CES per size:\", [m[-1] if m else None for m in ces_val])\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for sz, lt, lv in zip(sizes, loss_tr, loss_val):\n        plt.plot(range(1, len(lt) + 1), lt, label=f\"Train loss (size={sz})\")\n        plt.plot(range(1, len(lv) + 1), lv, \"--\", label=f\"Val loss (size={sz})\")\n    plt.title(\"Synthetic XOR: Training vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Plot CES curves\ntry:\n    plt.figure()\n    for sz, mt, mv in zip(sizes, ces_tr, ces_val):\n        plt.plot(range(1, len(mt) + 1), mt, label=f\"Train CES (size={sz})\")\n        plt.plot(range(1, len(mv) + 1), mv, \"--\", label=f\"Val CES (size={sz})\")\n    plt.title(\"Synthetic XOR: Training vs Validation CES\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CES\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_CES_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CES plot: {e}\")\n    plt.close()\n\n# Plot final CES bar chart\ntry:\n    plt.figure()\n    plt.bar([str(s) for s in sizes], [m[-1] if m else 0 for m in ces_val])\n    plt.title(\"Synthetic XOR: Final Validation CES by Hidden Size\")\n    plt.xlabel(\"Hidden Layer Size\")\n    plt.ylabel(\"CES\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_final_CES_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final CES bar plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\n# Load experiment data\ntry:\n    experiment_data = np.load(\n        os.path.join(working_dir, \"experiment_data.npy\"), allow_pickle=True\n    ).item()\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    experiment_data = {}\n\n# Extract synthetic XOR results\ndata = experiment_data.get(\"hidden_layer_size\", {}).get(\"synthetic_xor\", {})\nsizes = data.get(\"sizes\", [])\nloss_tr = data.get(\"losses\", {}).get(\"train\", [])\nloss_val = data.get(\"losses\", {}).get(\"val\", [])\nces_tr = data.get(\"metrics\", {}).get(\"train\", [])\nces_val = data.get(\"metrics\", {}).get(\"val\", [])\n\n# Print summary of final CES\nprint(\"Hidden sizes:\", sizes)\nprint(\"Final train CES per size:\", [m[-1] if m else None for m in ces_tr])\nprint(\"Final val CES per size:\", [m[-1] if m else None for m in ces_val])\n\n# Plot loss curves\ntry:\n    plt.figure()\n    for sz, lt, lv in zip(sizes, loss_tr, loss_val):\n        plt.plot(range(1, len(lt) + 1), lt, label=f\"Train loss (size={sz})\")\n        plt.plot(range(1, len(lv) + 1), lv, \"--\", label=f\"Val loss (size={sz})\")\n    plt.title(\"Synthetic XOR: Training vs Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_loss_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating loss plot: {e}\")\n    plt.close()\n\n# Plot CES curves\ntry:\n    plt.figure()\n    for sz, mt, mv in zip(sizes, ces_tr, ces_val):\n        plt.plot(range(1, len(mt) + 1), mt, label=f\"Train CES (size={sz})\")\n        plt.plot(range(1, len(mv) + 1), mv, \"--\", label=f\"Val CES (size={sz})\")\n    plt.title(\"Synthetic XOR: Training vs Validation CES\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"CES\")\n    plt.legend()\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_CES_curve.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating CES plot: {e}\")\n    plt.close()\n\n# Plot final CES bar chart\ntry:\n    plt.figure()\n    plt.bar([str(s) for s in sizes], [m[-1] if m else 0 for m in ces_val])\n    plt.title(\"Synthetic XOR: Final Validation CES by Hidden Size\")\n    plt.xlabel(\"Hidden Layer Size\")\n    plt.ylabel(\"CES\")\n    plt.savefig(os.path.join(working_dir, \"synthetic_xor_final_CES_bar.png\"))\n    plt.close()\nexcept Exception as e:\n    print(f\"Error creating final CES bar plot: {e}\")\n    plt.close()\n", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nworking_dir = os.path.join(os.getcwd(), \"working\")\nos.makedirs(working_dir, exist_ok=True)\n\n# Load all experiment data\nexperiment_data_path_list = [\n    \"experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_1ca653a5762847979dd3440b11e776bd_proc_2379937/experiment_data.npy\",\n    \"experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_d55fa0d5edf8493ab1ab91886ff0b0f9_proc_2379936/experiment_data.npy\",\n    \"experiments/2025-05-29_00-03-32_clarify_to_retrieve_attempt_0/logs/0-run/experiment_results/experiment_0eece47f8b6b401ba4419b77a1ea3244_proc_2379938/experiment_data.npy\",\n]\nall_experiment_data = []\ntry:\n    for rel_path in experiment_data_path_list:\n        full_path = os.path.join(os.getenv(\"AI_SCIENTIST_ROOT\", \"\"), rel_path)\n        data = np.load(full_path, allow_pickle=True).item()\n        all_experiment_data.append(data)\nexcept Exception as e:\n    print(f\"Error loading experiment data: {e}\")\n    all_experiment_data = []\n\n# Extract synthetic XOR results per run\nall_runs = []\nfor exp in all_experiment_data:\n    d = exp.get(\"hidden_layer_size\", {}).get(\"synthetic_xor\", {})\n    if d:\n        all_runs.append(d)\nif not all_runs:\n    print(\"No synthetic_xor data found.\")\nelse:\n    sizes = all_runs[0].get(\"sizes\", [])\n\n    # Organize curves per run\n    loss_tr_runs = [run.get(\"losses\", {}).get(\"train\", []) for run in all_runs]\n    loss_val_runs = [run.get(\"losses\", {}).get(\"val\", []) for run in all_runs]\n    ces_tr_runs = [run.get(\"metrics\", {}).get(\"train\", []) for run in all_runs]\n    ces_val_runs = [run.get(\"metrics\", {}).get(\"val\", []) for run in all_runs]\n\n    # Plot mean loss curves with SEM\n    try:\n        plt.figure()\n        for idx, sz in enumerate(sizes):\n            # gather train curves across runs for this size\n            train_curves = [lr[idx] for lr in loss_tr_runs if len(lr) > idx and lr[idx]]\n            val_curves = [lv[idx] for lv in loss_val_runs if len(lv) > idx and lv[idx]]\n            if not train_curves or not val_curves:\n                continue\n            # align to shortest\n            L_tr = min(len(c) for c in train_curves)\n            L_val = min(len(c) for c in val_curves)\n            arr_tr = np.array([c[:L_tr] for c in train_curves])\n            arr_val = np.array([c[:L_val] for c in val_curves])\n            mean_tr = arr_tr.mean(axis=0)\n            mean_val = arr_val.mean(axis=0)\n            sem_tr = (\n                arr_tr.std(axis=0, ddof=1) / np.sqrt(arr_tr.shape[0])\n                if arr_tr.shape[0] > 1\n                else np.zeros_like(mean_tr)\n            )\n            sem_val = (\n                arr_val.std(axis=0, ddof=1) / np.sqrt(arr_val.shape[0])\n                if arr_val.shape[0] > 1\n                else np.zeros_like(mean_val)\n            )\n            epochs_tr = np.arange(1, L_tr + 1)\n            epochs_val = np.arange(1, L_val + 1)\n            plt.plot(epochs_tr, mean_tr, label=f\"Train loss mean (size={sz})\")\n            plt.fill_between(epochs_tr, mean_tr - sem_tr, mean_tr + sem_tr, alpha=0.2)\n            plt.plot(epochs_val, mean_val, \"--\", label=f\"Val loss mean (size={sz})\")\n            plt.fill_between(\n                epochs_val, mean_val - sem_val, mean_val + sem_val, alpha=0.2\n            )\n        plt.title(\"Synthetic XOR: Mean Training vs Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_xor_mean_loss_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating mean loss plot: {e}\")\n        plt.close()\n\n    # Plot mean CES curves with SEM\n    try:\n        plt.figure()\n        for idx, sz in enumerate(sizes):\n            train_curves = [cr[idx] for cr in ces_tr_runs if len(cr) > idx and cr[idx]]\n            val_curves = [cr[idx] for cr in ces_val_runs if len(cr) > idx and cr[idx]]\n            if not train_curves or not val_curves:\n                continue\n            L_tr = min(len(c) for c in train_curves)\n            L_val = min(len(c) for c in val_curves)\n            arr_tr = np.array([c[:L_tr] for c in train_curves])\n            arr_val = np.array([c[:L_val] for c in val_curves])\n            mean_tr = arr_tr.mean(axis=0)\n            mean_val = arr_val.mean(axis=0)\n            sem_tr = (\n                arr_tr.std(axis=0, ddof=1) / np.sqrt(arr_tr.shape[0])\n                if arr_tr.shape[0] > 1\n                else np.zeros_like(mean_tr)\n            )\n            sem_val = (\n                arr_val.std(axis=0, ddof=1) / np.sqrt(arr_val.shape[0])\n                if arr_val.shape[0] > 1\n                else np.zeros_like(mean_val)\n            )\n            epochs_tr = np.arange(1, L_tr + 1)\n            epochs_val = np.arange(1, L_val + 1)\n            plt.plot(epochs_tr, mean_tr, label=f\"Train CES mean (size={sz})\")\n            plt.fill_between(epochs_tr, mean_tr - sem_tr, mean_tr + sem_tr, alpha=0.2)\n            plt.plot(epochs_val, mean_val, \"--\", label=f\"Val CES mean (size={sz})\")\n            plt.fill_between(\n                epochs_val, mean_val - sem_val, mean_val + sem_val, alpha=0.2\n            )\n        plt.title(\"Synthetic XOR: Mean Training vs Validation CES\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"CES\")\n        plt.legend()\n        plt.savefig(os.path.join(working_dir, \"synthetic_xor_mean_CES_curve.png\"))\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating mean CES plot: {e}\")\n        plt.close()\n\n    # Plot final validation CES bar chart (mean \u00b1 SEM)\n    try:\n        means = []\n        sems = []\n        for idx, sz in enumerate(sizes):\n            finals = [c[idx][-1] for c in ces_val_runs if len(c) > idx and c[idx]]\n            if not finals:\n                means.append(0)\n                sems.append(0)\n            else:\n                arr = np.array(finals)\n                means.append(arr.mean())\n                sems.append(arr.std(ddof=1) / np.sqrt(len(arr)) if len(arr) > 1 else 0)\n        x = np.arange(len(sizes))\n        plt.figure()\n        plt.bar(x, means, yerr=sems, capsize=5, label=\"Final Val CES mean\")\n        plt.xticks(x, [str(s) for s in sizes])\n        plt.title(\"Synthetic XOR: Final Validation CES by Hidden Size (Mean \u00b1 SEM)\")\n        plt.xlabel(\"Hidden Layer Size\")\n        plt.ylabel(\"CES\")\n        plt.legend()\n        plt.savefig(\n            os.path.join(working_dir, \"synthetic_xor_final_CES_bar_with_error.png\")\n        )\n        plt.close()\n    except Exception as e:\n        print(f\"Error creating final CES bar plot: {e}\")\n        plt.close()\n"], "plot_plan": ["We will import matplotlib, numpy, and os, then locate our working directory and load the saved experiment data.  From the synthetic_xor entry we will extract training and validation loss and CES arrays.  The first plot will display both train and val loss curves versus epoch, saved as a png file with an informative title.  Next, we will similarly plot CES (Clarification Explained Score) over epochs for both training and validation sets.  Finally, we will compute the class distribution on the validation set from the final epoch by comparing ground truth and predicted counts, displaying them as a grouped bar chart.  Each visualization is encapsulated in its own try-except block to catch errors, saving each figure into the working directory and closing it afterward.  After creating all plots, we will print the final loss and CES metrics for both training and validation to summarize the experiment\u2019s performance.  This approach gives clear, reproducible insights into how the synthetic_xor model behaved over epochs.", null, "We will load the saved experiment_data.npy from the working directory and extract the synthetic_xor learning rate sweep results.  We then create two combined line plots: one for training vs validation loss over epochs across all learning rates, and one for train vs val CES metrics.  We sort the learning rates numerically for consistent legend ordering.  We identify the best learning rate based on the final validation CES score and print out the final CES of each run and the best LR.  Finally, for the best LR at the last epoch, we plot side\u2010by\u2010side histograms of ground truth vs predicted labels with a suptitle clarifying \u201cLeft: Ground Truth, Right: Predictions.\u201d  Each plot is wrapped in a try\u2010except with plt.close() to handle errors gracefully, and files are saved to working_dir with descriptive names.", null, "We load the saved experiment data from the working directory and extract the synthetic_xor results. We then prepare three separate figures wrapped in try-except blocks to avoid interruptions: 1) a CES vs Dropout Rate plot showing final epoch train and validation CES, 2) a Final Loss vs Dropout Rate plot showing final train and validation losses, and 3) Loss Curves vs Epoch for up to five evenly spaced dropout rates, with solid lines for training and dashed for validation, so as to include training/validation curves. Each plot includes a main title specifying the dataset and a subtitle describing the line semantics. We save each figure to the working directory under descriptive filenames, always closing the figure on completion or error. Finally, we print out the dropout rates alongside the final CES metrics for both training and validation as a summary.", "We will load the experiment data saved during training and extract thresholds, losses, and CES metrics for both training and validation sets. Then we\u2019ll plot the loss curves and CES metrics across epochs for each threshold as two separate figures, each with left/right subplots for train/val curves. Both figures will include clear titles and subtitles, and we\u2019ll save them into the working directory under descriptive filenames. We wrap each plotting section in its own try-except block to handle errors and always close figures afterward. Finally, we compute and print out the threshold that yields the highest final validation CES.", null, null, null, "First, the code loads the saved experiment data from the working directory. It extracts the synthetic XOR results under the hidden\u2010layer\u2010size sweep, retrieving hidden sizes, training/validation losses, and CES metrics across epochs. It then prints out the final CES values for each size to summarize performance numerically. Next, it creates three plots in separate try-except blocks: (1) a combined line plot of training vs. validation loss across epochs for each hidden size, (2) a similar line plot of training vs. validation CES curves, and (3) a bar chart of final validation CES versus hidden layer size. Each figure is titled with the dataset name and metric type, includes axis labels and legends, and is saved under a descriptive filename to the working directory. Figures are always closed, and any plotting errors are caught and reported.", "I will load the saved experiment data and safely handle loading errors. I\u2019ll manually define the weight decay values used during the experiment to label each curve. In the first try-except block, I\u2019ll plot training and validation loss curves across epochs for every weight decay, using solid lines for training and dashed for validation, add axis labels, a title specifying the XOR dataset, and a legend, then save and close the figure. In the second try-except block, I\u2019ll similarly plot accuracy gain per clarification curves for train and val, include appropriate labels and title, save with a descriptive filename, and close the figure. All plots will be saved under working_dir without simulating any new data.", null, "First, import matplotlib, numpy, and os, and set the working directory. Then attempt to load the saved `experiment_data.npy` file into a dictionary. Extract the `synthetic_xor` runs under the `mc_T_tuning` key. In the first try-except block, create a figure to plot training and validation losses for each `mc_T` value across epochs, add labels and a title/subtitle, save it to `working_dir`, and close the figure. In the second try-except block, create another figure to plot training and validation CES curves for each `mc_T` value, again adding labels, title/subtitle, saving, and closing the figure. Error messages are printed if loading or plotting fails, and figures are always closed even on error.", null, null, null, null], "ablation_name": [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null], "hyperparam_name": [null, "epochs", "learning_rate", "batch_size", "dropout_rate", "threshold", null, "weight_decay", null, "hidden_layer_size", null, null, "mc_T", null, null, null, null], "is_seed_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true], "is_seed_agg_node": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "parse_metrics_plan": ["The script imports `os` and `numpy`, loads the saved experiment data from the\n`working` subdirectory, and iterates over each dataset key. For each dataset it\nextracts the recorded training and validation losses as well as the CES metrics,\ntakes the final (last epoch) values, and prints them with clear, self\u2010describing\nlabels. The code runs at the top level and does not rely on any special entry\npoint or plotting.", "", "The script begins by constructing the path to the saved NumPy file in the\n\u201cworking\u201d directory and loading it into a Python dictionary. It then looks under\nthe `\"learning_rate_sweep\"` key to iterate over each dataset (here\n`\"synthetic_xor\"`) and each learning\u2010rate run, pulling out the final epoch\u2019s\nClarification Effectiveness Score (CES) and training/validation loss. Each value\nis printed with a clear label (e.g., \u201cFinal training CES\u201d, \u201cFinal validation\nloss\u201d) along with the dataset and learning\u2010rate identifiers. The code runs\nimmediately at import time without any `if __name__ == \"__main__\":` guard.", "I will load the saved NumPy file from the \u201cworking\u201d folder, then iterate over\nevery dataset under the `batch_size_tuning` key. For each dataset, I print its\nname and then for each batch size I extract the final epoch of both the training\nand validation CES metrics and display them with clear labels. All code runs\nimmediately at the global scope without any `if __name__ == \"__main__\":` guard.", "The following script loads the saved NumPy experiment file from the \u201cworking\u201d\ndirectory, extracts the synthetic_xor results under different dropout rates, and\nthen prints, for each dropout setting, the dataset name followed by the final\n(last\u2010epoch) train loss, validation loss, train CES, and validation CES with\nclear metric labels.", "We can load the saved NumPy object and then unpack its nested structure by\niterating over the hyperparameter tuning types and datasets. For each dataset we\nprint its name once, then loop over thresholds to extract and display the final\nepoch values of the key metrics (CES and losses), labeling each output clearly.\nAll of this is done at the global level so the script runs immediately when\nexecuted.", "", "The following script loads the saved experiment data and iterates over each\nhyperparameter sweep under the \u201cweight_decay\u201d group. For each dataset (here only\n\u201csynthetic_xor\u201d), it prints the dataset name once and then, for each tested\nweight decay value, reports the final epoch values of the train and validation\nCES as well as the final training and validation loss. All outputs use clear,\ndescriptive metric names and there are no plotting or guarded entry\u2010points.", "", "I will load the saved numpy dictionary, iterate over each hyperparameter and\ndataset, and then for each hidden size extract the training and validation CES\nvalues from the last epoch. The script will print the dataset name first, and\nthen for each hidden layer size it will print \u201ctrain CES (final)\u201d and\n\u201cvalidation CES (final)\u201d with four\u2010decimal precision. All code runs at global\nscope without any special entry points.", "Below is a simple script that loads the saved experiment data, iterates over\neach weight\u2010decay setting, and prints out the final accuracy gain per\nclarification and final loss for both the training and validation sets. All\noutputs are clearly labeled with dataset names and metric names. The script runs\nimmediately at the global scope without any `if __name__ == \"__main__\":` guard.", "Here\u2019s a simple script that loads the saved experiment data from the working\ndirectory, then iterates over each dataset to extract the\ncalibration\u2010effectiveness scores (CES). For each dataset, it finds the\nhyperparameter setting with the highest final validation CES, and prints the\ndataset name followed by the corresponding training and validation CES values.\nAll output uses clear metric labels (\u201ctraining calibration\u2010effectiveness score\u201d\nand \u201cvalidation calibration\u2010effectiveness score\u201d) and the script runs\nimmediately at the global scope.", "The following script sets the working directory, loads the saved NumPy file into\na Python dict, and then iterates through the \u201cmc_T_tuning\u201d section for each\ndataset. It prints the dataset name once, and for each Monte Carlo temperature\n(`mc_T`) setting, it extracts and displays the final epoch\u2019s training and\nvalidation Causal Effectiveness Score (CES) with explicit metric labels. The\ncode runs immediately upon execution without any special entry point checks.", "I will load the saved numpy dictionary, iterate over each hyperparameter and\ndataset, and then for each hidden size extract the training and validation CES\nvalues from the last epoch. The script will print the dataset name first, and\nthen for each hidden layer size it will print \u201ctrain CES (final)\u201d and\n\u201cvalidation CES (final)\u201d with four\u2010decimal precision. All code runs at global\nscope without any special entry points.", "I will load the saved numpy dictionary, iterate over each hyperparameter and\ndataset, and then for each hidden size extract the training and validation CES\nvalues from the last epoch. The script will print the dataset name first, and\nthen for each hidden layer size it will print \u201ctrain CES (final)\u201d and\n\u201cvalidation CES (final)\u201d with four\u2010decimal precision. All code runs at global\nscope without any special entry points.", "I will load the saved numpy dictionary, iterate over each hyperparameter and\ndataset, and then for each hidden size extract the training and validation CES\nvalues from the last epoch. The script will print the dataset name first, and\nthen for each hidden layer size it will print \u201ctrain CES (final)\u201d and\n\u201cvalidation CES (final)\u201d with four\u2010decimal precision. All code runs at global\nscope without any special entry points.", ""], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each dataset and print final metrics\nfor dataset_name, dataset_data in experiment_data.items():\n    print(f\"Dataset: {dataset_name}\")\n\n    # Extract losses\n    train_losses = dataset_data.get(\"losses\", {}).get(\"train\", [])\n    val_losses = dataset_data.get(\"losses\", {}).get(\"val\", [])\n\n    # Extract CES metrics\n    train_ces = dataset_data.get(\"metrics\", {}).get(\"train\", [])\n    val_ces = dataset_data.get(\"metrics\", {}).get(\"val\", [])\n\n    # Print final training loss\n    if train_losses:\n        final_train_loss = train_losses[-1]\n        print(f\"Final training loss: {final_train_loss:.4f}\")\n\n    # Print final validation loss\n    if val_losses:\n        final_val_loss = val_losses[-1]\n        print(f\"Final validation loss: {final_val_loss:.4f}\")\n\n    # Print final training CES\n    if train_ces:\n        final_train_ces = train_ces[-1]\n        print(f\"Final training CES: {final_train_ces:.4f}\")\n\n    # Print final validation CES\n    if val_ces:\n        final_val_ces = val_ces[-1]\n        print(f\"Final validation CES: {final_val_ces:.4f}\")\n", "", "import os\nimport numpy as np\n\n# Locate and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over the learning rate sweep for each dataset\nlr_sweep = experiment_data.get(\"learning_rate_sweep\", {})\nfor dataset_name, runs in lr_sweep.items():\n    print(f\"Dataset: {dataset_name}\")\n    for run_key, run_data in runs.items():\n        # Extract final epoch metrics and losses\n        final_train_ces = run_data[\"metrics\"][\"train\"][-1]\n        final_val_ces = run_data[\"metrics\"][\"val\"][-1]\n        final_train_loss = run_data[\"losses\"][\"train\"][-1]\n        final_val_loss = run_data[\"losses\"][\"val\"][-1]\n\n        # Print with clear labels\n        print(f\"\\nLearning rate: {run_key}\")\n        print(f\"Final training CES: {final_train_ces:.4f}\")\n        print(f\"Final validation CES: {final_val_ces:.4f}\")\n        print(f\"Final training loss: {final_train_loss:.4f}\")\n        print(f\"Final validation loss: {final_val_loss:.4f}\")\n", "import os\nimport numpy as np\n\n# Load the experiment data from the working directory\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each dataset in the batch_size_tuning section\nbatch_size_section = experiment_data.get(\"batch_size_tuning\", {})\nfor dataset_name, ds_info in batch_size_section.items():\n    print(f\"Dataset: {dataset_name}\")\n    batch_sizes = ds_info.get(\"batch_sizes\", [])\n    train_metrics = ds_info.get(\"metrics\", {}).get(\"train\", [])\n    val_metrics = ds_info.get(\"metrics\", {}).get(\"val\", [])\n\n    # For each batch size, print the final epoch\u2019s train and validation CES scores\n    for bs, train_list, val_list in zip(batch_sizes, train_metrics, val_metrics):\n        if train_list:\n            final_train = train_list[-1]\n            print(f\"Batch size {bs} - final train CES score: {final_train:.4f}\")\n        if val_list:\n            final_val = val_list[-1]\n            print(f\"Batch size {bs} - final validation CES score: {final_val:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Load experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each dataset under dropout_rate_tuning\nfor dataset_name, dataset_results in experiment_data[\"dropout_rate_tuning\"].items():\n    print(f\"Dataset: {dataset_name}\")\n    dropout_rates = dataset_results[\"dropout_rates\"]\n    train_losses = dataset_results[\"losses\"][\"train\"]\n    val_losses = dataset_results[\"losses\"][\"val\"]\n    train_ces = dataset_results[\"metrics\"][\"train\"]\n    val_ces = dataset_results[\"metrics\"][\"val\"]\n\n    # Print final metrics for each dropout rate\n    for dr, tr_loss_list, vl_loss_list, tr_ces_list, vl_ces_list in zip(\n        dropout_rates, train_losses, val_losses, train_ces, val_ces\n    ):\n        final_train_loss = tr_loss_list[-1]\n        final_val_loss = vl_loss_list[-1]\n        final_train_ces = tr_ces_list[-1]\n        final_val_ces = vl_ces_list[-1]\n\n        print(f\"Dropout rate: {dr}\")\n        print(f\"Train loss: {final_train_loss:.4f}\")\n        print(f\"Validation loss: {final_val_loss:.4f}\")\n        print(f\"Train CES: {final_train_ces:.4f}\")\n        print(f\"Validation CES: {final_val_ces:.4f}\")\n        print()  # blank line between settings\n", "import os\nimport numpy as np\n\n# Load the experiment data dictionary\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through each tuning type and dataset\nfor tuning_type, datasets in experiment_data.items():\n    for dataset_name, info in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        thresholds = info[\"thresholds\"]\n        train_ces_list = info[\"metrics\"][\"train\"]\n        val_ces_list = info[\"metrics\"][\"val\"]\n        train_loss_list = info[\"losses\"][\"train\"]\n        val_loss_list = info[\"losses\"][\"val\"]\n\n        # For each threshold, print the final-epoch values\n        for thr, train_ces, val_ces, train_loss, val_loss in zip(\n            thresholds, train_ces_list, val_ces_list, train_loss_list, val_loss_list\n        ):\n            final_train_ces = train_ces[-1]\n            final_val_ces = val_ces[-1]\n            final_train_loss = train_loss[-1]\n            final_val_loss = val_loss[-1]\n\n            print(f\"Threshold: {thr:.3f}\")\n            print(f\"  Final train CES: {final_train_ces:.4f}\")\n            print(f\"  Final validation CES: {final_val_ces:.4f}\")\n            print(f\"  Final training loss: {final_train_loss:.4f}\")\n            print(f\"  Final validation loss: {final_val_loss:.4f}\")\n            print()\n", "", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# Iterate through each hyperparameter group and dataset\nfor sweep_name, sweep_group in experiment_data.items():\n    # e.g., sweep_name == \"weight_decay\"\n    for dataset_name, data in sweep_group.items():\n        print(f\"Dataset: {dataset_name}\")\n        hyperparams = data[\"hyperparams\"]\n        train_ces_all = data[\"metrics\"][\"train\"]\n        val_ces_all = data[\"metrics\"][\"val\"]\n        train_loss_all = data[\"losses\"][\"train\"]\n        val_loss_all = data[\"losses\"][\"val\"]\n\n        # For each weight decay setting, print the final metrics\n        for idx, wd in enumerate(hyperparams):\n            final_train_ces = train_ces_all[idx][-1]\n            final_val_ces = val_ces_all[idx][-1]\n            final_train_loss = train_loss_all[idx][-1]\n            final_val_loss = val_loss_all[idx][-1]\n\n            print(f\"  Weight Decay: {wd}\")\n            print(f\"    Final train CES: {final_train_ces:.4f}\")\n            print(f\"    Final validation CES: {final_val_ces:.4f}\")\n            print(f\"    Final training loss: {final_train_loss:.4f}\")\n            print(f\"    Final validation loss: {final_val_loss:.4f}\")\n", "", "import os\nimport numpy as np\n\n# 1. Define the working directory and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# 2. Iterate through each hyperparameter and dataset to print final metrics\nfor param_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        sizes = data[\"sizes\"]\n        train_metrics = data[\"metrics\"][\"train\"]\n        val_metrics = data[\"metrics\"][\"val\"]\n        for size, train_vals, val_vals in zip(sizes, train_metrics, val_metrics):\n            print(f\"{param_name} = {size}\")\n            print(f\"train CES (final): {train_vals[-1]:.4f}\")\n            print(f\"validation CES (final): {val_vals[-1]:.4f}\")\n        print()  # Blank line between datasets\n", "import os\nimport numpy as np\n\n# Define the same weight decay settings used during the experiments\nweight_decays = [0, 1e-5, 1e-4, 1e-3]\n\n# Construct the path to the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\n\n# Load the saved dictionary\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Extract nested metrics\nacc_gain = experiment_data[\"accuracy_gain_per_clarification\"]\nlosses = experiment_data[\"losses\"]\n\n# Iterate through each weight decay and report final metrics\nfor idx, wd in enumerate(weight_decays):\n    print(f\"Weight decay = {wd}\")\n\n    # Final training metrics\n    final_acc_gain_train = acc_gain[\"train\"][idx][-1]\n    final_loss_train = losses[\"train\"][idx][-1]\n    print(\"Dataset: Training\")\n    print(\n        f\"  Training accuracy gain per clarification (final): {final_acc_gain_train:.4f}\"\n    )\n    print(f\"  Training loss (final): {final_loss_train:.4f}\")\n\n    # Final validation metrics\n    final_acc_gain_val = acc_gain[\"val\"][idx][-1]\n    final_loss_val = losses[\"val\"][idx][-1]\n    print(\"Dataset: Validation\")\n    print(\n        f\"  Validation accuracy gain per clarification (final): {final_acc_gain_val:.4f}\"\n    )\n    print(f\"  Validation loss (final): {final_loss_val:.4f}\")\n    print()\n", "import os\nimport numpy as np\n\n# Load the saved experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate over each dataset and print the best final metrics\nfor dataset_name, results in experiment_data.items():\n    # Extract lists of CES over epochs for each hyperparameter setting\n    train_ces_lists = results[\"metrics\"][\"train\"]\n    val_ces_lists = results[\"metrics\"][\"val\"]\n\n    # Find the index of the best final validation CES\n    best_val = -np.inf\n    best_idx = None\n    for idx, val_list in enumerate(val_ces_lists):\n        final_val = val_list[-1]\n        if final_val > best_val:\n            best_val = final_val\n            best_idx = idx\n\n    # Get the corresponding final training CES\n    best_train = train_ces_lists[best_idx][-1]\n\n    # Print the dataset and its best training/validation scores\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"Training calibration-effectiveness score: {best_train:.4f}\")\n    print(f\"Validation calibration-effectiveness score: {best_val:.4f}\\n\")\n", "import os\nimport numpy as np\n\n# Determine the working directory and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\ndata_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(data_path, allow_pickle=True).item()\n\n# Iterate through the hyperparameter tuning section\nmc_tuning = experiment_data.get(\"mc_T_tuning\", {})\nfor dataset_name, runs in mc_tuning.items():\n    print(f\"Dataset: {dataset_name}\")\n    # For each mc_T setting, print the final training/validation CES\n    for mc_T, run_data in runs.items():\n        train_ces = run_data[\"metrics\"][\"train\"][-1]\n        val_ces = run_data[\"metrics\"][\"val\"][-1]\n        print(\n            f\"  mc_T = {mc_T} - Final Training Causal Effectiveness Score: {train_ces:.4f}\"\n        )\n        print(\n            f\"  mc_T = {mc_T} - Final Validation Causal Effectiveness Score: {val_ces:.4f}\"\n        )\n    print()\n", "import os\nimport numpy as np\n\n# 1. Define the working directory and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# 2. Iterate through each hyperparameter and dataset to print final metrics\nfor param_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        sizes = data[\"sizes\"]\n        train_metrics = data[\"metrics\"][\"train\"]\n        val_metrics = data[\"metrics\"][\"val\"]\n        for size, train_vals, val_vals in zip(sizes, train_metrics, val_metrics):\n            print(f\"{param_name} = {size}\")\n            print(f\"train CES (final): {train_vals[-1]:.4f}\")\n            print(f\"validation CES (final): {val_vals[-1]:.4f}\")\n        print()  # Blank line between datasets\n", "import os\nimport numpy as np\n\n# 1. Define the working directory and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# 2. Iterate through each hyperparameter and dataset to print final metrics\nfor param_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        sizes = data[\"sizes\"]\n        train_metrics = data[\"metrics\"][\"train\"]\n        val_metrics = data[\"metrics\"][\"val\"]\n        for size, train_vals, val_vals in zip(sizes, train_metrics, val_metrics):\n            print(f\"{param_name} = {size}\")\n            print(f\"train CES (final): {train_vals[-1]:.4f}\")\n            print(f\"validation CES (final): {val_vals[-1]:.4f}\")\n        print()  # Blank line between datasets\n", "import os\nimport numpy as np\n\n# 1. Define the working directory and load the experiment data\nworking_dir = os.path.join(os.getcwd(), \"working\")\nfile_path = os.path.join(working_dir, \"experiment_data.npy\")\nexperiment_data = np.load(file_path, allow_pickle=True).item()\n\n# 2. Iterate through each hyperparameter and dataset to print final metrics\nfor param_name, datasets in experiment_data.items():\n    for dataset_name, data in datasets.items():\n        print(f\"Dataset: {dataset_name}\")\n        sizes = data[\"sizes\"]\n        train_metrics = data[\"metrics\"][\"train\"]\n        val_metrics = data[\"metrics\"][\"val\"]\n        for size, train_vals, val_vals in zip(sizes, train_metrics, val_metrics):\n            print(f\"{param_name} = {size}\")\n            print(f\"train CES (final): {train_vals[-1]:.4f}\")\n            print(f\"validation CES (final): {val_vals[-1]:.4f}\")\n        print()  # Blank line between datasets\n", ""], "parse_term_out": ["['Dataset: synthetic_xor', '\\n', 'Final training loss: 0.6727', '\\n', 'Final\nvalidation loss: 0.6550', '\\n', 'Final training CES: 0.4358', '\\n', 'Final\nvalidation CES: 0.4792', '\\n', 'Execution time: a moment seconds (time limit is\nan hour).']", "", "['Dataset: synthetic_xor', '\\n', '\\nLearning rate: lr_0.0001', '\\n', 'Final\ntraining CES: 0.2612', '\\n', 'Final validation CES: 0.2325', '\\n', 'Final\ntraining loss: 0.7295', '\\n', 'Final validation loss: 0.6819', '\\n', '\\nLearning\nrate: lr_0.0005', '\\n', 'Final training CES: 0.0000', '\\n', 'Final validation\nCES: 0.0000', '\\n', 'Final training loss: 0.6920', '\\n', 'Final validation loss:\n0.6873', '\\n', '\\nLearning rate: lr_0.001', '\\n', 'Final training CES: 0.0000',\n'\\n', 'Final validation CES: 0.0000', '\\n', 'Final training loss: 0.6527', '\\n',\n'Final validation loss: 0.6508', '\\n', '\\nLearning rate: lr_0.005', '\\n', 'Final\ntraining CES: 0.4642', '\\n', 'Final validation CES: 0.3979', '\\n', 'Final\ntraining loss: 0.4548', '\\n', 'Final validation loss: 0.3787', '\\n', '\\nLearning\nrate: lr_0.01', '\\n', 'Final training CES: 0.3954', '\\n', 'Final validation CES:\n0.3579', '\\n', 'Final training loss: 0.5812', '\\n', 'Final validation loss:\n0.4940', '\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: synthetic_xor', '\\n', 'Batch size 16 - final train CES score:\n0.3878', '\\n', 'Batch size 16 - final validation CES score: 0.3621', '\\n',\n'Batch size 32 - final train CES score: 0.4200', '\\n', 'Batch size 32 - final\nvalidation CES score: 0.3456', '\\n', 'Batch size 64 - final train CES score:\n0.0000', '\\n', 'Batch size 64 - final validation CES score: 0.0000', '\\n',\n'Batch size 128 - final train CES score: 0.0000', '\\n', 'Batch size 128 - final\nvalidation CES score: 0.0000', '\\n', 'Batch size 256 - final train CES score:\n0.0000', '\\n', 'Batch size 256 - final validation CES score: 0.0000', '\\n',\n'\\n', 'Execution time: a moment seconds (time limit is an hour).']", "['Dataset: synthetic_xor', '\\n', 'Dropout rate: 0.0', '\\n', 'Train loss:\n0.6295', '\\n', 'Validation loss: 0.6282', '\\n', 'Train CES: 0.0000', '\\n',\n'Validation CES: 0.0000', '\\n', '\\n', 'Dropout rate: 0.1', '\\n', 'Train loss:\n0.6706', '\\n', 'Validation loss: 0.6661', '\\n', 'Train CES: 1.0000', '\\n',\n'Validation CES: 0.0000', '\\n', '\\n', 'Dropout rate: 0.2', '\\n', 'Train loss:\n0.6403', '\\n', 'Validation loss: 0.6438', '\\n', 'Train CES: 0.0000', '\\n',\n'Validation CES: 0.0000', '\\n', '\\n', 'Dropout rate: 0.3', '\\n', 'Train loss:\n0.6108', '\\n', 'Validation loss: 0.6040', '\\n', 'Train CES: 0.0000', '\\n',\n'Validation CES: 0.0000', '\\n', '\\n', 'Dropout rate: 0.5', '\\n', 'Train loss:\n0.6944', '\\n', 'Validation loss: 0.6929', '\\n', 'Train CES: 0.0000', '\\n',\n'Validation CES: 0.0000', '\\n', '\\n', 'Dropout rate: 0.7', '\\n', 'Train loss:\n0.6893', '\\n', 'Validation loss: 0.6820', '\\n', 'Train CES: 0.0000', '\\n',\n'Validation CES: 0.0000', '\\n', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "['Dataset: synthetic_xor', '\\n', 'Threshold: 0.005', '\\n', '  Final train CES:\n0.2982', '\\n', '  Final validation CES: 0.2750', '\\n', '  Final training loss:\n0.6547', '\\n', '  Final validation loss: 0.6427', '\\n', '\\n', 'Threshold:\n0.010', '\\n', '  Final train CES: 0.3582', '\\n', '  Final validation CES:\n0.3600', '\\n', '  Final training loss: 0.6826', '\\n', '  Final validation loss:\n0.6733', '\\n', '\\n', 'Threshold: 0.020', '\\n', '  Final train CES: 0.0000',\n'\\n', '  Final validation CES: 0.0000', '\\n', '  Final training loss: 0.6535',\n'\\n', '  Final validation loss: 0.6513', '\\n', '\\n', 'Threshold: 0.050', '\\n', '\nFinal train CES: 0.0000', '\\n', '  Final validation CES: 0.0000', '\\n', '  Final\ntraining loss: 0.6235', '\\n', '  Final validation loss: 0.6123', '\\n', '\\n',\n'Threshold: 0.100', '\\n', '  Final train CES: 0.0000', '\\n', '  Final validation\nCES: 0.0000', '\\n', '  Final training loss: 0.6968', '\\n', '  Final validation\nloss: 0.6928', '\\n', '\\n', 'Execution time: a moment seconds (time limit is an\nhour).']", "", "['Dataset: synthetic_xor', '\\n', '  Weight Decay: 0', '\\n', '    Final train\nCES: 0.3961', '\\n', '    Final validation CES: 0.5625', '\\n', '    Final\ntraining loss: 0.6547', '\\n', '    Final validation loss: 0.6436', '\\n', '\nWeight Decay: 1e-05', '\\n', '    Final train CES: 0.3692', '\\n', '    Final\nvalidation CES: 0.3659', '\\n', '    Final training loss: 0.6746', '\\n', '\nFinal validation loss: 0.6729', '\\n', '  Weight Decay: 0.0001', '\\n', '    Final\ntrain CES: 0.0000', '\\n', '    Final validation CES: 0.0000', '\\n', '    Final\ntraining loss: 0.6531', '\\n', '    Final validation loss: 0.6510', '\\n', '\nWeight Decay: 0.001', '\\n', '    Final train CES: 0.5673', '\\n', '    Final\nvalidation CES: 0.5455', '\\n', '    Final training loss: 0.6269', '\\n', '\nFinal validation loss: 0.6172', '\\n', 'Execution time: a moment seconds (time\nlimit is an hour).']", "", "['Dataset: synthetic_xor', '\\n', 'hidden_layer_size = 4', '\\n', 'train CES\n(final): 0.0000', '\\n', 'validation CES (final): 0.0000', '\\n',\n'hidden_layer_size = 8', '\\n', 'train CES (final): 0.0000', '\\n', 'validation\nCES (final): 0.0000', '\\n', 'hidden_layer_size = 16', '\\n', 'train CES (final):\n0.3376', '\\n', 'validation CES (final): 0.2880', '\\n', 'hidden_layer_size = 32',\n'\\n', 'train CES (final): 0.8000', '\\n', 'validation CES (final): 0.0000', '\\n',\n'hidden_layer_size = 64', '\\n', 'train CES (final): 0.4799', '\\n', 'validation\nCES (final): 0.4649', '\\n', '\\n', 'Execution time: a moment seconds (time limit\nis an hour).']", "['Weight decay = 0', '\\n', 'Dataset: Training', '\\n', '  Training accuracy gain\nper clarification (final): 0.3846', '\\n', '  Training loss (final): 0.6880',\n'\\n', 'Dataset: Validation', '\\n', '  Validation accuracy gain per clarification\n(final): 0.0000', '\\n', '  Validation loss (final): 0.6813', '\\n', '\\n', 'Weight\ndecay = 1e-05', '\\n', 'Dataset: Training', '\\n', '  Training accuracy gain per\nclarification (final): 0.4890', '\\n', '  Training loss (final): 0.6443', '\\n',\n'Dataset: Validation', '\\n', '  Validation accuracy gain per clarification\n(final): 0.4091', '\\n', '  Validation loss (final): 0.6399', '\\n', '\\n', 'Weight\ndecay = 0.0001', '\\n', 'Dataset: Training', '\\n', '  Training accuracy gain per\nclarification (final): 0.0000', '\\n', '  Training loss (final): 0.6761', '\\n',\n'Dataset: Validation', '\\n', '  Validation accuracy gain per clarification\n(final): 0.0000', '\\n', '  Validation loss (final): 0.6748', '\\n', '\\n', 'Weight\ndecay = 0.001', '\\n', 'Dataset: Training', '\\n', '  Training accuracy gain per\nclarification (final): 0.1497', '\\n', '  Training loss (final): 0.6674', '\\n',\n'Dataset: Validation', '\\n', '  Validation accuracy gain per clarification\n(final): 0.1034', '\\n', '  Validation loss (final): 0.6625', '\\n', '\\n',\n'Execution time: a moment seconds (time limit is an hour).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 12, in\n<module>\\n    train_ces_lists = results[\"metrics\"][\"train\"]\\n\n~~~~~~~^^^^^^^^^^^\\nKeyError: \\'metrics\\'\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Dataset: synthetic_xor', '\\n', '  mc_T = 2 - Final Training Causal\nEffectiveness Score: 0.3664', '\\n', '  mc_T = 2 - Final Validation Causal\nEffectiveness Score: 0.3871', '\\n', '  mc_T = 5 - Final Training Causal\nEffectiveness Score: 0.3486', '\\n', '  mc_T = 5 - Final Validation Causal\nEffectiveness Score: 0.3721', '\\n', '  mc_T = 10 - Final Training Causal\nEffectiveness Score: 0.0000', '\\n', '  mc_T = 10 - Final Validation Causal\nEffectiveness Score: 0.0000', '\\n', '  mc_T = 20 - Final Training Causal\nEffectiveness Score: 0.5255', '\\n', '  mc_T = 20 - Final Validation Causal\nEffectiveness Score: 0.4872', '\\n', '\\n', 'Execution time: a moment seconds\n(time limit is an hour).']", "['Dataset: synthetic_xor', '\\n', 'hidden_layer_size = 4', '\\n', 'train CES\n(final): 0.0000', '\\n', 'validation CES (final): 0.0000', '\\n',\n'hidden_layer_size = 8', '\\n', 'train CES (final): 0.3971', '\\n', 'validation\nCES (final): 0.3580', '\\n', 'hidden_layer_size = 16', '\\n', 'train CES (final):\n0.1790', '\\n', 'validation CES (final): 0.2075', '\\n', 'hidden_layer_size = 32',\n'\\n', 'train CES (final): 0.3804', '\\n', 'validation CES (final): 0.3725', '\\n',\n'hidden_layer_size = 64', '\\n', 'train CES (final): 0.4249', '\\n', 'validation\nCES (final): 0.4010', '\\n', '\\n', 'Execution time: a moment seconds (time limit\nis an hour).']", "['Dataset: synthetic_xor', '\\n', 'hidden_layer_size = 4', '\\n', 'train CES\n(final): 0.0000', '\\n', 'validation CES (final): 0.0000', '\\n',\n'hidden_layer_size = 8', '\\n', 'train CES (final): 0.0000', '\\n', 'validation\nCES (final): 0.0000', '\\n', 'hidden_layer_size = 16', '\\n', 'train CES (final):\n0.4298', '\\n', 'validation CES (final): 0.5111', '\\n', 'hidden_layer_size = 32',\n'\\n', 'train CES (final): 0.6667', '\\n', 'validation CES (final): 0.6667', '\\n',\n'hidden_layer_size = 64', '\\n', 'train CES (final): 0.4513', '\\n', 'validation\nCES (final): 0.4275', '\\n', '\\n', 'Execution time: a moment seconds (time limit\nis an hour).']", "['Dataset: synthetic_xor', '\\n', 'hidden_layer_size = 4', '\\n', 'train CES\n(final): 0.0000', '\\n', 'validation CES (final): 0.0000', '\\n',\n'hidden_layer_size = 8', '\\n', 'train CES (final): 0.0000', '\\n', 'validation\nCES (final): 0.0000', '\\n', 'hidden_layer_size = 16', '\\n', 'train CES (final):\n0.3912', '\\n', 'validation CES (final): 0.3684', '\\n', 'hidden_layer_size = 32',\n'\\n', 'train CES (final): 0.4704', '\\n', 'validation CES (final): 0.3990', '\\n',\n'hidden_layer_size = 64', '\\n', 'train CES (final): 0.4312', '\\n', 'validation\nCES (final): 0.4286', '\\n', '\\n', 'Execution time: a moment seconds (time limit\nis an hour).']", ""], "parse_exc_type": [null, null, null, null, null, null, null, null, null, null, null, "KeyError", null, null, null, null, null], "parse_exc_info": [null, null, null, null, null, null, null, null, null, null, null, {"args": ["metrics"]}, null, null, null, null, null], "parse_exc_stack": [null, null, null, null, null, null, null, null, null, null, null, [["/home/chenhui/AI-Scientist-v2/ai_scientist/treesearch/interpreter.py", 144, "_run_session", "exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 12, "<module>", "train_ces_lists = results[\"metrics\"][\"train\"]"]], null, null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]}
\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{iyer2019learningtm}
\citation{nguyen2013semifixpr,bjrner2008satisfiabilitymb}
\citation{none}
\citation{wei2022chainot}
\citation{rosa2022zeroshotrc}
\citation{lehtosalo2011languagewa}
\citation{chen2021evaluatingll}
\citation{austin2021programsw}
\citation{iyer2019learningtm}
\citation{nguyen2013semifixpr}
\citation{bjrner2008satisfiabilitymb}
\citation{ernst2007theds}
\citation{none}
\citation{schick2020itsnj,wei2022chainot}
\citation{none}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{1}{section.3}\protected@file@percent }
\citation{chen2021evaluatingll}
\citation{austin2021programsw}
\citation{rosa2022zeroshotrc}
\citation{bjrner2008satisfiabilitymb}
\citation{nguyen2013semifixpr}
\bibdata{references}
\bibcite{austin2021programsw}{{1}{2021}{{Austin et~al.}}{{Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, and Sutton}}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{2}{section.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Pass@1 and runtime-error reduction ($\Delta _{\text  {err}}$) vs.\ GPT-J-6B.}}{2}{table.1}\protected@file@percent }
\newlabel{tab:main_results}{{1}{2}{Pass@1 and runtime-error reduction ($\Delta _{\text {err}}$) vs.\ GPT-J-6B}{table.1}{}}
\newlabel{tab:main_results@cref}{{[table][1][]1}{[1][2][]2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Synthetic Ablations}{2}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Qualitative Example}{2}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{2}{section.6}\protected@file@percent }
\bibcite{bjrner2008satisfiabilitymb}{{2}{2008}{{Bj{\o }rner et~al.}}{{Bj{\o }rner, Moura, and Tillmann}}}
\bibcite{chen2021evaluatingll}{{3}{2021}{{Chen et~al.}}{{Chen, Tworek, Jun, Yuan, Pond{\'e}, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Babuschkin, Balaji, Jain, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba}}}
\bibcite{none}{{4}{}{{Cousot \& Cousot}}{{Cousot and Cousot}}}
\bibcite{rosa2022zeroshotrc}{{5}{2022}{{de~la Rosa \& Fernandez}}{{de~la Rosa and Fernandez}}}
\bibcite{ernst2007theds}{{6}{2007}{{Ernst et~al.}}{{Ernst, Perkins, Guo, McCamant, Pacheco, Tschantz, and Xiao}}}
\bibcite{iyer2019learningtm}{{7}{2019}{{Iyer}}{{}}}
\bibcite{lehtosalo2011languagewa}{{8}{2011}{{Lehtosalo \& Greaves}}{{Lehtosalo and Greaves}}}
\bibcite{schick2020itsnj}{{9}{2020}{{Schick \& Schütze}}{{Schick and Schütze}}}
\bibcite{wei2022chainot}{{10}{2022}{{Wei et~al.}}{{Wei, Wang, Schuurmans, Bosma, Chi, Xia, Le, and Zhou}}}
\bibstyle{iclr2025}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Synthetic arithmetic ablations: solid/dashed lines are training/validation loss; flat lines show AICR reaching 1.0 immediately.}}{3}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {LR Sweep}}}{3}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Embedding Dim}}}{3}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Weight Decay}}}{3}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Fixed Embeds}}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:synthetic}{{1}{3}{Synthetic arithmetic ablations: solid/dashed lines are training/validation loss; flat lines show AICR reaching 1.0 immediately}{figure.1}{}}
\newlabel{fig:synthetic@cref}{{[figure][1][]1}{[1][2][]3}}
\gdef \@abspage@last{4}
